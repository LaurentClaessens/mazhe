% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014, 2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Other results}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Abstract Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

As before if we chose a basis $\{\varphi_1\ldots\varphi_l\}$ of $V$, we can consider a lexicographic ordering\index{lexicographic ordering} on $V$. A root is \defe{simple}{simple!abstract root} when it is positive and can't be written as as sum of two positive roots. As in a non abstract case, abstract simple root also have the following property:

\begin{proposition}
If $\dim V=l$, one has only $l$ simple roots $\alpha_1,\ldots,\alpha_l$; they are linearly independent and if $\beta\in\Phi$ expands into $\beta=\sum c_j\alpha_j$, the $c_j$'s all are integers and the non zero ones all have the same sign.
\end{proposition}

An ordering on $V$ gives a notion of simple roots. The $l\times l$ matrix whose entries are
\[
   A_{ij}=\frZ{\alpha_i}{\alpha_j}
\]
is the \defe{abstract Cartan matrix}{abstract!Cartan matrix}\index{Cartan!abstract matrix} of the abstract root system and the given ordering.

\begin{theorem}
    The main properties are
    \begin{enumerate}
        \item $A_{ij}\in\eZ$,
        \item $A_{ii}=2$,
        \item if $i\neq j$, then $A_{ij}\leq 0$ and $A_{ij}$ can only take the values $0,-1,-2$ or $-3$,
        \item if $i\neq j$, $A_{ij}A_{ji}<4$ (no sum),
        \item $A_{ij}=0$ is and only if $A_{ji}=0$,
        \item $\det A$ is integer and positive.
    \end{enumerate}
\end{theorem}

    \begin{proof}
    The last point is the only non immediate one. The matrix $A$ is the product of the diagonal matrix with entries $2/|\alpha_i|^2$ and the matrix whose entries are $(\alpha_i,\alpha_j)$. The fact that the latter is positive definite is a general property of linear algebra. If $\{e_i\}$ is a basis of a vector space $V$, the matrix whose entry $ij$ is given by $(e_i,e_i)$ is positive definite. Indeed one can consider an orthonormal basis $\{f_i\}$ and a nondegenerate change of basis $e_i=B_{ik}f_k$. Then $(e_i,e_j)=(BB^t)_{ij}$. It is easy to see that for all $v\in V$, we have $(BB^t)_{ij}v^iv^j=\sum_k(v^iB_{ik})^2>0$.

    The fact that the determinant is integer is simply the fact that this is a polynomial with integer variables.
\end{proof}

If we have an ordering on $V$ we define $\Phi^+$, the set of positive roots. From there, one can consider $\Pi$, the set of simple roots. Any element of $\Phi$ expands to a sum of elements of $\Pi$. Note that the knowledge of $\Pi$ is sufficient to find $\Phi^+$ back because $\alpha>0$ implies $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

We can make this reasoning backward. Let us consider $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a basis of $V$ such that any $\alpha\in\Phi$ expands as a sum of $\alpha_i$ with all coefficients of the same sign. Such a $\Pi$ is a \defe{simple system}{simple!system}. From such a $\Pi$, we can build a $\Phi^+$ as the set of elements of the form $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

\begin{proposition}
The so build $\Phi^+$ is the set of positive roots for a certain ordering.
\end{proposition}

\begin{proof}
If we consider on $V$ the lexicographic ordering with respect to the basis $\Pi$, a positive element $\alpha=\sum c_i\alpha_i$ has at least one positive coefficient among the $c_i$. If $\alpha\in\Phi$, we can say (by definition of $\Pi$) that in this case \emph{all} the coefficients are positive, then the positive roots exactly form the set $\Phi^+$.
\end{proof}

From now when we speak about a $\Phi^+$, it will always be with respect to a simple system. The advantage is the fact that there are no more implicit ordering.

\begin{lemma}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system and $\alpha\in\Phi^+$. Then
\[
   s_{\alpha_i}=\begin{cases}
                           -\alpha_i & \text{if }\alpha=\alpha_i\\
                    >0       & \text{if }\alpha\neq\alpha_i.
                          \end{cases}
\]
\end{lemma}

\begin{proof}
The first case is well know from a long time. For the second, compute
\begin{equation}
\begin{split}
  s_{\alpha_i}( \sum c_j\alpha_j )&=\sum_{j\neq i}c_j\alpha_j+c_i\alpha_i-2c_i\alpha_i
                                         -\sum_{j\neq i}\frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)\alpha_i\\
                              &=\sum_{j\neq i}+\left(
        -\sum_{i\neq j}       \frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)+c_i
                                    \right)\alpha_i.
\end{split}
\end{equation}
We see that between $\sum c_k\alpha_k$ and $s_{\alpha_i}(\sum c_k\alpha_k)$, there is just the coefficient of $\alpha_i$ which changes. Then if $\alpha\neq \alpha_i$, the positivity is conserved.

\end{proof}

\begin{proposition}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system. Then $W$ is generate by the $s_{\alpha_i}$'s. If $\alpha\in\Phi$, then there exists a $\alpha_i\in\Pi$ and $s\in W$ such that $s\alpha_j=\alpha$.
\end{proposition}

\begin{proof}
We denote by $W'$ the group generate by the $s_{\alpha_i}$'s; the purpose is to show that $W=W'$. We begin to show that if $\alpha>0$, then $\alpha=s\alpha_j$ for certain $s\in W'$ and $\alpha_j\in\Pi$. For this, we write $\alpha=\sum c_j\alpha_j$ and we make an induction with respect to $\niv(\alpha)=\sum c_j$. If $\niv(\alpha)=1$, then $\alpha-\alpha_j$ and $s=\id$ works.  Now we suppose that it works for $\niv<\niv(\alpha)$. We have
\[
   0<(\alpha,\alpha)=\sum c_i(\alpha,\alpha_i).
\]
Since all the $c_i$ are positive, it assures the existence of a $i_0$ such that $(\alpha,\alpha_{i_0)}>0$. Then from the lemma, $\beta=s_{\alpha_{i_0}}(\alpha)>0$ ($\alpha\neq \alpha_{i_0}$ because $\niv(\alpha)>1$). The root $\beta$ can be expanded as
\begin{equation}
\beta=\sum_{j \neq i_0}c_j\alpha_j+\left(  c_{i_0}-\sum_{j\neq i_0}\frac{c_j}{|\alpha_{i_0}|^2}(\alpha,\alpha_{i_0})   \right)\alpha_{i_0}.
\end{equation}
Since $(\alpha,\alpha_{i_0})>0$, it implies $\niv(\beta)<\niv(\alpha)$ and thus $\beta=s'\alpha_j$ for a certain $s'\in W'$. So $\alpha=s_{\alpha_{i_0}}s'\alpha_j$ with $s_{\alpha_{i_0}}s'\in W'$. This conclude the induction. For $\alpha<0$, the same result holds by writing $-\alpha=s\alpha_j$ and $\alpha=ss_{\alpha_j}\alpha_j$.

Now it remains to prove that $W'\subseteq W$. For a $\alpha\in\Phi$, we write $\alpha=s\alpha_j$ with $s\in W'$. Then
\[
   s_{\alpha}=ss_{\alpha_j}s^{-1}\in W'.
\]
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{About group representations}
%---------------------------------------------------------------------------------------------------------------------------

Let $\pi$ be a representation of a group $G$. The \defe{character}{character!of a representation} of $\pi$ is the function
\begin{equation}
    \begin{aligned}
        \chi_{\pi}\colon G&\to\eC \\
        g&\mapsto \tr\big( \pi(g) \big).
    \end{aligned}
\end{equation}
From the cyclic invariance of trace, it fulfils $\chi_{\pi}(gxg^{-1})=\chi_{\pi}(x)$, so that the character is a central function.

Let $G$ be a Lie group with Lie algebra $\lG$. We denote by $Z_{\pm}$ the subgroup of $G$ generated by $\lN^{\pm}$. The \defe{Cartan subgroup}{Cartan!subgroup} $D$ of $G$ is the maximal abelian subgroup of $G$ which has $\lH$ as Lie algebra.

A \defe{character}{character!of an abelian group} of an abelian group is a representation of dimension one.

 Let $T$ be a representation of $G$ on a complex vector space $V$. One say that $\xi\in V$ is a \defe{highest weight}{highest weight!for group representation} if
\begin{itemize}
\item $T(z)\xi=\xi$ for every $z\in Z_+$,
\item $T(g)\xi=\alpha(g)\xi$ for every $g\in D$.
\end{itemize}
The function $\alpha\colon D\to \eC$ is the \defe{highest weight}{highest weight} of the representation $T$.

\begin{lemma}
    The function $\alpha$ is a character of the group $D$.
\end{lemma}

\begin{proof}
    The number $\alpha(gg')$ is defined by $T(gg')\xi=\alpha(gg')\xi$. Using the fact that $T$ is a representation, one easily obtains $T(gg')\xi=\alpha(g)\alpha(g')\xi$.
\end{proof}

\subsection{Modules and reducibility}
%------------------------------------

As far as terminology is concerned, one can sometimes find the following definitions. A $\lG$-module is \defe{simple}{simple!module}\index{module!simple} when the only submodules are $\lG$ and $0$. It is \defe{semisimple}{semisimple!module}\index{module!semisimple} when it is isomorphic to a direct sum of simple modules. The module is \defe{indecomposable}{indecomposable module}\index{module!indecomposable} if it is not isomorphic to the direct sum of two non trivial submodules.

An vector space endomorphism $\dpt{a}{V}{V}$ is \defe{semisimple}{semisimple!endomorphism} if $V$ is semisimple as module for the associative algebra spanned by $A$. In this text, we will not use this terminology but the one in terms of reducibility. It is clear that $\lG$ is itself a $\lG$-module for the adjoint representation. From this point of view, a $\lG$-submodule is an ideal. Then a simple Lie algebra is an irreducible $\lG$-module and a semisimple Lie algebra is completely reducible by corollary~\ref{cor:decomp_ideal}. This explains the terminology correspondence

\begin{center}
\begin{tabular}{ccc}
\emph{simple} & $\leftrightarrow$ & \emph{irreducible} \\
\emph{semisimple}& $\leftrightarrow$ &\emph{completely reducible}.\\
\end{tabular}
\end{center}

\begin{theorem}[Weyl's theorem]
A representation of a semisimple Lie algebra is completely reducible.
\end{theorem}

\subsection{Weight and dual spaces}
%---------------------------------

In general, when $\dpt{T}{V}{V}$ is an endomorphism of the vector space $V$ and $\lambda\in\eK$ ($\eK$ is the base field of $V$), we define
\begin{equation}
V_\lambda=\{ v\in V\tq (T-\lambda\mtu)^nv=0\textrm{ for a $n\in\eN$} \}).
\end{equation}
If $V(\lambda)\neq0$, we say that $\lambda$ is a \defe{weight}{weight!for endomorphism} and $V(\lambda)$ is a weight space.

Let now particularize to the case where $\lG$ is a Lie algebra, and $\lG^*$ its dual space (the space of all the complex linear forms on $\lG$). Let $\rho$ be a representation of $\lG$ on a complex vector space $V$ (seen as a $\lG$-module\index{module}), and $\gamma\in\lG^*$. For each $x\in\lG$, we have $\dpt{\rho(x)}{V}{V}$ and $\gamma(x)\in\eC$; then it makes sense to speak about the operator $\dpt{\rho(x)-\gamma(x)}{V}{V}$ and to define
\begin{equation}
V_{\gamma}=\{ v\in V\tq\forall x\in\lG,\exists n\in\eN \tq\big(\rho(x)-\gamma(x)\big)^nv=0 \}.
\end{equation}
If $V_{\gamma}\neq 0$, we say that $\gamma$ is a \defe{weight}{weight!for representation} for the representation $\rho$ while $V\bgamma$ is the corresponding \defe{weight space}{weight!space}.

Notice that a root is a weight space for the adjoint representation, see definition~\ref{DefRootSpace}. We denote by $\Phi$ the set of non empty root spaces.

\begin{lemma}

Let $\End(V)$ be the algebra of linear endomorphism of a vector space $V$. Let $x_1,\ldots,x_k,y_1,\ldots,y_k\in\End(V)$ and
\[
    e=\sum_i[x_i,y_i].
\]
If $e$ commutes with all $x_i$, then it is nilpotent.
\label{lem:EndV_e}
\end{lemma}

A proof of this lemma can be found in \cite{Hochschild}

\begin{theorem}
Let $\lG$ be a Lie algebra of linear endomorphisms of a finite dimensional vector space $V$. We suppose that $V$ is a completely reducible $\lG$-module and we denote the center of $\lG$ by $\mZ$. Then
\begin{enumerate}
\item $[\lG,\lG]\cap\mZ=0$,
\item $L/\mZ$ has a non zero abelian ideal,
\item any element of $\mZ$ is a semisimple endomorphism.
\end{enumerate}
\end{theorem}

\begin{probleme}
    The following proof seems me to be quite wrong.
\end{probleme}

\begin{proof}
Let $A$ be the associative algebra spanned by $\lG$ and the identity on $V$. It is clear that the $A$-stable subspaces are exactly the $\lG$-stable ones. Then $V$ is a completely reducible $A$-module and it has no non zero nilpotent left ideal. Indeed let $B$ be a left ideal in $A$ such that $BB=0$. In this case, $B\cdot V$ is a $A$-submodule of $V$ (because $B$ is an ideal) and $V=B\cdot V\oplus W$ for a certain $A$-submodule $W$. Since $B\cdot V$ is a $A$-submodule,
\[
B\cdot W\subset(B\cdot V)\cap W
\]
(because $W$ is stable under $A$) which implies $B\cdot W=0$ and $B\cdot V=B\cdot(BV+W) 0$. Consequently, $B=0$.

Let $T$ be the center of $A$; this is an ideal, so that $T$ has no non zero nilpotent elements. To see it, consider a nilpotent element $z\in T$. Remark that $T=Az$  is a nilpotent ideal because $AzAz=Az^2A$. Now, we prove that $z$ is a semisimple linear endomorphism of $V$. There exists \( n\) such that $z^n(V)=z^{n+1}(V)$. Let $q=\sum_{v\in V}$  The space $V_s=z^n(V)$ is not zero because $z$ is not nilpotent. Let $W$ be the set of elements of $V$ which are annihilated by a certain power of $z$. Equation \eqref{eq:ApoplusW} makes $z$ semisimple because $V_s$ and $W$ are $z$-stables.

By lemma~\ref{lem:EndV_e}, any element of $[A,A]\cap T$ is nilpotent; but we just saw that it has no non zero nilpotent elements then $[A,A]\cap T=0$, so that
\[
[\lG,\lG]\cap\mZ=0.
\]
This proves the first point.

Now we consider  an ideal $J$ such that $[J,J]\subset\mZ$. Then $[J,J]=[J,J]\cap \mZ=0$. We looks at the abelian ideal $[\lG,J]$ of $\lG$. This is an ideal because $[[g,j],h]=-[[j,h],g]-[[h,g],j]$. By the lemma, the elements of $[\lG,J]$ are nilpotent and the associative algebra generated by $[G,J]$ is also nilpotent because $[\lG,J]$ is abelian.

The elements of $B$ are polynomials with respect to elements of $[\lG,J]$, then $AB\subset BA+B$ because $AB$ is made up with elements of the form $a(hj-jh)^n$ which itself is made up with elements $ah^kj^l$. By commutating $j^l$, we get
\[
j^lah^k+\text{elements of } [\lG,J],
\]
but $J$ is an ideal and $j^l\in\ J$. By induction,
\begin{equation}
(AB)^k\subset B^kA+B^k.
\end{equation}
Since $B$ is nilpotent, $AB$ is a nilpotent left ideal. Then $AB=0$ which in turn implies $B=0$. In particular $[\lG,J]=0$, so that $J\subset\mZ$. But any abelian ideal in $\lG/\mZ$ is the canonical projection of an ideal $J$ of $\lG$ such that $[J,J]\in\mZ$. We conclude that $\lG/\mZ$ has no non zero abelian ideal.

\end{proof}

Now we are able to prove a third version of Lie's theorem:
\begin{theorem}[Lie]\label{tho:Lie_trois}
If $\lG$ is a solvable ideal, then any completely reducible $\lG$-module  is annihilated by $[\lG,\lG]$.
\end{theorem}

\begin{proof}
Let $V$ be such a $\lG$-module, $\rho$ the representation of $\lG$ on $V$ and $\mA=\rho(\lG)\subset\End(V)$. By assumption, $\lA$ is a solvable subalgebra of $\End(V)$; let $\mZ$ be the center of $\lA$. It is clear that $\lA/\mZ$ is solvable, so that it has no non zero abelian ideal. But the fact that $\lA/\mZ$ is solvable makes one of the $\dD^k(\lA/\mZ)$ an abelian ideal. The conclusion is that $\lA/\mZ=0$, or $\lA=\mZ$. Clearly this makes $[\lA,\lA]=0$.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Dynkin diagram}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    If $\alpha$ and $\beta$ are simple roots, then the angle $\theta_{\alpha,\beta}$ can only take the values $90\degree$, $120\degree$, $135\degree$ or $150\degree$.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

In order to draw the \defe{Dynkin diagram}{Dynkin!diagram} of a Lie algebra, one draws a circle for each simple root, and one joins the roots with $1$, $2$ or $3$ lines, following that the value of the angle is $120\degree$, $135\degree$ or $150\degree$. If the roots are orthogonal (angle $90\degree$), they are not connected. If the length of a root is maximal, the circle is left empty. If not, it is filled.

One easily determines the number of lines between two roots by the following proposition.
\begin{proposition}         \label{PropProdNbLignes}
    If $\alpha$ and $\beta$ are two simple roots with $(\alpha,\alpha)\leq(\beta,\beta)$, then
    \begin{equation}
        \frac{ (\alpha,\alpha) }{ (\beta,\beta) }=
    \begin{cases}
        1   &\text{if }\theta_{\alpha,\beta}=120\degree\\
        2   &\text{if }\theta_{\alpha,\beta}=135\degree\\
        3   &\text{if }\theta_{\alpha,\beta}=150\degree.
    \end{cases}
    \end{equation}
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

If $M$ is a weight of a representation, its \defe{Dynkin coefficients}{Dynkin!coefficient} are
\begin{equation}
    M_i=\frac{ 2(M,\alpha_i) }{ (\alpha_i,\alpha_i) },
\end{equation}
and we can compute the Dynkin coefficients from one weight to another by the simple formula
\begin{equation}        \label{EqCofDynMmoisAlpha}
    (M-\alpha_j)_i=M_i-A_{ij}.
\end{equation}
A weight is \defe{dominant}{weight!dominant}\index{dominant weight} if all its Dynkin coefficients are strictly positive.

\begin{proposition}\label{prop:trois_poids}
    Let $\lG$ be a nilpotent complex Lie algebra and $\rho$, a representation of $\lG$ on a finite dimensional vector space $V$. Then
    \begin{enumerate}
        \item $\forall \gamma\in\lG^*$, the space $V_{\gamma}$ is a $\lG$-submodule of $V$,
        \item if $\gamma$ is a weight, then there exists a nonzero vector $v\in V_{\gamma}$ such that $\forall x\in\lG$, $x\cdot v=\gamma(x)v$,
        \label{prop:trois_poids:deux}
        \item\label{ItemVSEZlhviii} $V=\bigoplus_{\gamma}V_{\gamma}$ where the sum is taken over the set of weight.
    \end{enumerate}
\end{proposition}

\begin{proof}
Since $\rho$ is a representation,
\[
\big(\rho(x)-\gamma(x)\big)\rho(y)=\rho(y)\big( \rho(x)-\gamma(x) \big)+\rho([x,y]).
\]
Now let us suppose that $\big(\rho(x)-\gamma(x)\big)^m\rho(y)$ is a sum of endomorphism of the form
\[
\rho( (\ad x)^py )\rmg{x}^q
\]
with $p+q=m$. We just saw that it was true for $m=1$. Let us check for $m+1$:
\begin{equation}
\begin{split}
\rho(x)\rho( (\ad x)^py )\rmg{x}^q&=\rho( [x,(\ad x)^py] )\rmg{x}^q\\
&\quad+\rho((\ad x)^py)\rho(x)\rmg{x}^q.
\end{split}
\end{equation}
Then, since $\lG$ is nilpotent, the space $V_{\gamma}$ is a submodule of $V$ because for large enough $m$ and for all $y$, $\rmg{x}^m \rho(y)v=0$ if $v\in V_{\gamma}$.

Any nilpotent algebra is solvable, then from Lie theorem~\ref{tho:Lie_trois}, the restrictions of $\rho(x)$ (with $x\in\lG$) to irreducible submodules commute. By Schur's lemma~\ref{lem:Schur}, they are multiples of identity. But if all $\lG$ is the identity on an irreducible module, then the module has dimension one. In particular, \emph{any irreducible submodule of $V_{\gamma}$ has dimension one}\quext{Encore que soit pas bien clair pourquoi un tel module existerait... donc l'affiramation suivante ne me semble pas trop justifi\'ee}.

Then, in the weight space $V_{\gamma}$, there is a $v$ which fulfils  $\rho(x)v=\lambda(x)v$ for all $x\in\lG$. It is rather clear that it will only works for $\lambda=\gamma$. Our conclusion is that there exists a $v\in V_{\gamma}$ such that $\rho(x)v=\gamma(x)v$.

Now we consider $\gamma_1,\ldots,\gamma_k$, distinct weights. They are linear forms; then there exists a $x\in\lG$ such that $\gamma_1(x),\ldots,\gamma_k(x)$ are distinct numbers. In fact, the set $\{h\in\lH\tq \alpha_i(h)=\alpha_j(h)\text{ for a certain pair }(i,j)\}$ is a finite union of hyperplanes in $\lH$; then the complementary is non empty.


With this fact we can see that the sum $V_{\gamma_1}+\cdots+V_{\gamma_k}$ is direct. Indeed let $v\in V_{\gamma_i}\cap V_{\gamma_j}$; for the chosen $x\in\lG$ and for suitable $m$,
\begin{equation}
\big(  \rho(x)-\gamma_i(x) \big)^mv=\big(  \rho(x)-\gamma_j(x) \big)^mv=0
\end{equation}
which implies $\gamma_i(x)=\gamma_j(x)$ or $v=0$. In particular one has only a finitely many roots and we can suppose that our choice of $\gamma_i$ is complete.

For $a\in\eC$, we define $V_a$ as the set of elements in $V$ which are annihilated by some power of $\rho(x)-a$ with our famous $x$. By the first lines of the proof, $V_a$ is a $\lG$-submodule of $V$.

For the same reasons as before\quext{Celles que je n'ai pas bien comprises}, if $V_a\neq 0$, there exists a $v\in V_a$ and a weight $\gamma_i$ such that $\forall y\in\lG$,
\[
\rho(y)v=\gamma_i(y)v.
\]
But as $v$ is annihilated by a power of $\big( \rho(x)-a \big)$, it is clear that $a=\gamma_i(x)$, and some theory of linear endomorphism\quext{th\'eorie que je ne connais pas trop} shows that $V$ is the sum of the $V_a$'s:
\begin{equation}
V=\sum_{i=1}^kV_{\gamma_i(x)}.
\end{equation}
It remains to be proved that $V_{\gamma_i(x)}\subset V_{\gamma_i}$. Let $y\in\lG$ and
\[
V_{i,a}=\{ v\in V_{\gamma_i(x)}\tq\exists n: (\rho(y)-a)^nv=0 \}.
\]
As usual\quext{et comme d'hab, l'argument que je ne saisit pas} if $V_{i,a}\neq 0$, there exists a $v\in V_{i,a}$ and a weight $\gamma_j$ such that $\rho(z)v=\gamma_j(z)v$ for any $z\in\lG$. Then $a=\gamma_j(y)=\gamma_i(y)$. But $V_{\gamma_i(x)}$ being the sum of the $V_{i,a}$'s, we have $V_{\gamma_i(x)}=V_{i,\gamma_i(y)}$ for any $y\in\lG$. This makes $V_{\gamma_i(x)\subset V_{\gamma_i}}$.

\end{proof}

From proposition~\ref{prop:trois_poids}\ref{ItemVSEZlhviii}, an element $y\in\lG$ can be decomposed as
\begin{equation}\label{eq:decomp_racine}
    y=\sum_{\beta\in\Phi}y_{\beta}
\end{equation}
with $y_{\beta}\in\lG_{\beta}$.

\begin{theorem} \label{tho:Killing_Cartan}
    Let $\lG$ be a Lie algebra, $\lH$ a Cartan subalgebra of $\lG$ and $B$ the Killing\index{Killing!form} form of $\lG$. Then for all $x$, $y\in\lH$,
    \begin{equation}
    B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y)
    \end{equation}
    where $g_{\gamma}=\dim\lG\bgamma$.
\end{theorem}

\begin{proof}
    We are seeing $\lG$ as a $\lH$-module for the adjoint representation. In particular, proposition~\ref{prop:trois_poids} makes $\lG$ a direct sum of the $\lH$-submodules $\lG_{\gamma}$. Then
    \begin{equation}
    B(x,y)=\tr({\ad x}^2)\\
            =\sum_{\gamma\in\Phi}\tr(\ad x|_{\gamma}^2)
    \end{equation}
    where $\ad x|_{\gamma}$ means the restriction of $\ad x$ to $\lG\bgamma$. It is clear that $\ad x|\bgamma-\gamma(x)$ is nilpotent, then $\ad x|\bgamma^2-\gamma(x)^2$ is also nilpotent because
    \[
    \ad x|_{\gamma}^2-\gamma(x)^2=(\ad x|\bgamma+\gamma(x))(\ad x|\bgamma-\gamma(x))
    \]
    and the fact that these two terms commute. The trace of a nilpotent endomorphism is zero, then $\tr(\ad x|_{\gamma}^2-\gamma(x)^2)=0$ or for all $x\in\lG$,
    \begin{equation}\label{eq:Bxx}
    B(x,x)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)^2.
    \end{equation}
    on the other hand, we know that a quadratic form determines only one bilinear form. Here the form \eqref{eq:Bxx} gives
    \[
    B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y).
    \]
\end{proof}

\begin{theorem}\label{tho:six_Cartan}
    If $\alpha,\beta$ are roots of a semisimple Lie algebra $\lG$ with respect to a Cartan subalgebra $\lH$, then
    \begin{enumerate}
        \item if $x_{\alpha}\neq0\in\lG_{\alpha}$ fulfils $[h,x_{\alpha}]=\alpha(h)x_{\alpha}$ for all $h\in\lH$, then $\forall y\in\lG_{-\alpha}$
        \[
        [x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha},
        \]
        \item\label{ite:six_deux} $\alpha(h_{\alpha})$ is rational and positive. Moreover
        \[
        \alpha(h_{\alpha})\sum_{\gamma\in\Phi}(\gamma_{\alpha}-\gamma^{\alpha})^2=4,
        \]
        \item $2\beta(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$,
        \label{ite:six_trois}
        \item the forms $0,\alpha,-\alpha$ are the only integer multiples of $\alpha$ which are roots,
        \label{ite:six_quatre}
        \item $\dim\lG_{\alpha}$=1,
        \label{ite:six_cinq}
        \item any $k$ which makes $\beta+k\alpha$ a root lie between $-\beta_{\alpha}$ and $\beta^{\alpha}$. In other words, $\beta+k\alpha\in\Phi$ is only true with $-\beta_{\alpha}\leq k\leq\beta^{\alpha}$.
        \label{ite:six_six}
    \end{enumerate}

\end{theorem}

\begin{proof}
The fact that $y\in\lG_{-\alpha}$ and that $x\in\lG_{\alpha}$ make $[x,y]\in\lG_0=\lH$. Now we consider $h\in\lH$ and the invariance formula \eqref{eq:Killing_invariant}. We find:
\begin{equation}
B(h,[x_{\alpha},y])=-B([x_{\alpha},h],y)\\
        =\alpha(h)B(x_{\alpha},y)\\
        =B(h,h_{\alpha})B(x_{\alpha},y)\\
        =B(h,B(x_{\alpha},y)h_{\alpha}).
\end{equation}
Since it is true for any $h\in\lH$ and $B$ is nondegenerate on $\lH$ we find the first point. In order to prove~\ref{ite:six_deux}, we consider
\[
U=\bigoplus_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}\lG_{\beta+m\alpha}.
\]
By definition of $\alpha_{\beta}$ and $\alpha^{\beta}$, each term of the sum is a root space. If $z\in\lG_{\alpha}\oplus\lG_{-\alpha}$, then $U$ is stable under $\ad z$ because the terms in $\ad z U$ are of the form $[z,x_{\beta+m\alpha}]\in\lG_{\beta+m\alpha\pm\alpha}$. Note however that this $\ad z U$ is not \emph{equal} to $U$.

Let $x_{\alpha}\neq 0\in\lG_{\alpha}$. There exists a $y\in\lG_{-\alpha}$ such that $[x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha}$ (here we use semi-simplicity). By fitting the norm of $y$, we can choose it in order to get  $[x_{\alpha},y]=h_{\alpha}$, so that
\[
\ad h_{\alpha}=[\ad x_{\alpha},\ad y].
\]
Now we look at the restriction of $\ad h_{\alpha}$ to $U$:
\begin{equation}
\tr(\ad h_{\alpha})=\tr(\ad x_{\alpha}\circ\ad y)-\tr(\ad y\circ\ad x_{\alpha})=0.
\end{equation}
Since $h_{\alpha}\in\lH=\lG_0$, we have $\dpt{\ad h_{\alpha}}{U}{U}$, so that the annihilation of the trace of $\ad h_{\alpha}$ can be particularised to
\[
\tr(\ad h_{\alpha}|_U)=0.
\]
On the other hand, by definition $\ad h_{\alpha}-(\beta+m\alpha)(h_{\alpha})$ is nilpotent on $\lG_{\beta+m\alpha}$. Then it has a vanishing trace:
\[
\sum_m\tr( \ad h_{\alpha}-(\beta+m\alpha)h_{\alpha}  )=0.
\]
But we had yet seen that the term with $\ad h_{\alpha}$ is zero; then
\begin{equation}\label{eq:trace_U}
\sum_{-\beta_{\alpha}\leq m\leq \beta^{\alpha}} (\beta+m\alpha)h_{\alpha}\dim\lG_{\beta+m\alpha}=0.
\end{equation}
If we suppose that $\alpha(h_{\alpha})=0$ this gives $\beta(h_{\alpha})=0$. Since this conclusion is true for any root $\beta$, we find $B(h,h_{\alpha})=0$ for any $h\in\lH$. In other words, $\alpha(h)=0$ for any $h\in\lH$. This contradicts the assumption, so that we conclude $\alpha(h_{\alpha})\neq 0$.


Let $V=\lH+(x_{\alpha})+\sum_{m<0}\lG_{m\alpha}$ where $(x_{\alpha})$ is the one dimensional space spanned by $x_{\alpha}$. On the one hand,  from the definition of $x_{\alpha}$, $\ad x_{\alpha}\lH\subset(x_{\alpha})$ and $\ad x_{\alpha}\lG_{m\alpha}\subset\lG_{(m+1)\alpha}$. On the other hand, $y\in\lG_{-\alpha}$ is defined by the relation $[x_{\alpha},y]=h_{\alpha}$, then  $\ad y\lH\subset\lG_{-\alpha}\subset\sum_{m<0}\lG_{m\alpha}$, $\ad y(x_{\alpha})\subset\lG_0=\lH$ and $\ad y\sum_{m<0}\lG_{m\alpha}=\sum_{m<0}\lG_{(m-1)\alpha}$. All this make $V$ invariant under $\ad x_{\alpha}$ and $\ad y$.

Since $\ad h_{\alpha}=[\ad x_{\alpha},\ad y]$, the trace of $\ad h_{\alpha}$ is zero so that the invariance of $V$ gives
\[
\tr(\ad h_{\alpha}|_V)=0.
\]
By the definition of $x_{\alpha}$ particularised to $h\to h_{\alpha}$, we have $\tr(\ad h_{\alpha}|_{(x_{\alpha})})=\alpha(h_{\alpha})$. By the definition of $\lG_0$, for any $x\in\lH$ and $v\in\lG_0$, $\ad x$ is nilpotent on $v$. Taking $h_{\alpha}$ as $x$, we see that
$(\ad h_{\alpha})h$ don't contain ``$h$-component''. Then $\tr(\ad h_{\alpha}|_{\lH})=0$. Finally the operator $(\ad h_{\alpha}-m\alpha(h_{\alpha}))$ is nilpotent on $\lG_{m\alpha}$, so that $\tr(\ad h_{\alpha}|_{\lG_{m\alpha}})=\tr( m\alpha(\alpha)|_{\lG_{m\alpha}} )=m\alpha(h_{\alpha})\dim\lG_{m\alpha}$. All this gives
\begin{equation}
\alpha(h_{\alpha})\left( 1+\sum_{m<0}m\dim\lG_{m\alpha}  \right)=0.
\end{equation}
As we saw that $\alpha(h_{\alpha})\neq 0$, we conclude that $\dim\lG_{m\alpha}=0$ for $m<-1$ and $\dim\lG_{-\alpha}=1$. This proves~\ref{ite:six_cinq}.

This also prove~\ref{ite:six_quatre} in the particular case of \emph{integer} multiples. It is rather simple to get relations such that $0_{\alpha}=1$, $0^{\alpha}=1$, $\alpha_{\alpha}=2$, $(-\alpha)_{\alpha}=0$, and it is easy to check~\ref{ite:six_trois} in the cases $\beta=-\alpha,0,\alpha$. Now we turn our attention to the case in which $\beta$ is not an integer multiple of $\alpha$. By~\ref{ite:six_quatre} applied to $\alpha\to\beta+m\alpha$, we have $\dim\lG_{\beta+m\alpha}=1$ whenever $-\beta_{\alpha}\leq m\leq\beta^{\alpha}$.

From equation \eqref{eq:trace_U}, $\sum_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}( \beta(h_{\alpha})+m\alpha(h_{\alpha}) )=0$, then
\begin{equation}
(\beta_{\alpha}+\beta^{\alpha}+1)\beta(h_{\alpha})=(\sum_m m)\alpha(h_{\alpha})\\
                        =\left(
                    \frac{\beta_{\alpha}(\beta_{\alpha}+1)}{2}-\frac{(\beta^{\alpha}-1)\lbha}{2}
                        \right)\alpha(h_{\alpha}).
\end{equation}
This gives~\ref{ite:six_trois}. Now we consider the formula of theorem~\ref{tho:Killing_Cartan} in the case $x=y=h_{\alpha}$ and we use the fact that $B(h,h_{\alpha})=\alpha(h)$ in the case $h=h_{\alpha}$:
\begin{equation}
B(h_{\alpha},h_{\alpha})=\alpha(h_{\alpha})\\
            =\sum_{\gamma\in\Phi}\dim\lG_{\gamma}\gamma(h_{\alpha})^2\\
            =\sum_{\gamma\in\Phi}\gamma(h_{\alpha})^2.
\end{equation}
Since $\beta(h_{\alpha})=\frac{1}{2}(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, we find~\ref{ite:six_deux}. In order to prove~\ref{ite:six_quatre}, we consider $\beta=c\alpha$ for a $c\in\eC$. By~\ref{ite:six_trois}, $2c\alpha(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, so that $c$ is an half integer: $c=p/2$ with $p\in\eZ$. If $c$ is non zero, we can interchange $\alpha$ and $\beta$ and see that $\alpha=c^{-1}\beta$ implies $c^{-1}=q/2$ with $q\in\eZ$. It is clear the $pq=4$. But we had already discussed the case of integer multiples of $\alpha$, so that we can suppose that $p$ is odd. The only odd $p$ such that $pq=3$ with $q\in\eZ$ are $p=1,-1$, which are two excluded cases: they are $\alpha=\pm 2\beta$ which lies in the case of integer multiples.

It remains to prove~\ref{ite:six_six}. By definition of $\beta^{\alpha}$, the form $\beta+(\beta^{\alpha}+1)\alpha$ is not a root. But it remains possible that $\beta+(\beta^{\alpha}+2)\alpha$ is. We suppose that $k_1,\ldots,k_p$ are the $p$ positive integers such that $\beta+k_i\alpha\in\Phi$. We pose
\[
W=\bigoplus_{i=1}^p\lG_{\beta+k_i\alpha}.
\]
As usual we see that $W$ is stable under $\ad x_{\alpha}$ and $\ad y$ (because $k_i\geq\beta^{\alpha}+2$). The trace of $\ad g_{\alpha}$ on $W$ is zero, thus
\begin{equation}
0=\sum_{i=1}^p(\beta+k_i\alpha)(h_{\alpha}).
\end{equation}
By~\ref{ite:six_trois}, we find
\[
p(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})=2(k_1+\cdots+k_p)>p(\beta^{\alpha}+1).
\]
This is not possible because it would gives $-\beta^{\alpha}-\beta_{\alpha}>2$.
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Strings of roots}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Let $\alpha,\beta$ be two roots with respect to $\lH$ and suppose $\beta\neq 0$. We denote by $\alpha^{\beta}$ the largest integer $m$ such that $\alpha+m\beta$ is a root and by $\alpha_{\beta}$ the one such that $\alpha-m\beta$ is a root. Let $x\in\lG_{\alpha}$; since the Killing form is nondegenerate, there exists a $y\in\lG$ such that $B(x,y)\neq 0$. Using the root space decomposition \eqref{eq:decomp_racine} for $y$ and corollary~\ref{cor:Bxy_zero}, $B(x,y)=B(x,y_{-\alpha})$ . Then \label{pg:root_ss}
\[
\forall x\in\lG_{\alpha},\exists y\in\lG_{-\alpha}\textrm{ such that } B(x,y)\neq 0.
\]
In particular if $\alpha$ is a root, $-\alpha$ is also a root and the restriction of $B$ to $\lH\times\lH$ is nondegenerate because $\lH=\lG_0$. So
\[
\forall\mu\in\lH^*,\exists!h_{\mu}\in\lH\textrm{ such that } \forall h\in\lG, B(h,h_{\mu})=\mu(h).
\]
This is a general result about nondegenerate (here we use the semi-simplicity assumption) bilinear forms on a vector space. If $B(x,y)=B_{ij}x^iy^j$ and $a(x)=a_ix^i$, then a vector $v$ such that $B(x,v)=a(x)$ exists, is unique and is given by coordinates $v^k=B^{ki}a_i$ where the matrix $(B^{ij})$ is the inverse of $(B_{ij})$.

We will sometimes use the following notation if $\alpha$ and $\beta$ are roots:
\[
(\alpha,\beta)=B( h_{\alpha},\hbb),\qquad |\alpha|^2=(\alpha,\alpha).
\]

By proposition~\ref{tho:six_Cartan}, the roots come by pairs $(\alpha,-\alpha)$. For each of them, we choose $x_{\alpha}\in\lG_{\alpha}$. Our choice of $x_{-\alpha}$ is made as following. From discussion at page \pageref{pg:root_ss} we can find a $x_{-\alpha}\in\lG_{-\alpha}$ such that $B(x_{-\alpha},x_{\alpha})=1$. Note that this choice is unambigous: if we had chosen first $x_{-\alpha}\in\lG_{-\alpha}$, this construction would have given the same $x_{\alpha}$ than our starting point. Note also that $h_{-\alpha}=-h_{\alpha}$. These $ x_{\alpha}$ fulfil $[ x_{\alpha},\xbma]= h_{\alpha}$.

\begin{probleme}
    Here the notation \( \Delta\) does not follow our convention of subsection~\ref{SubsecNotationRootsDel}.
\end{probleme}

Let $\Delta$ be the set of non zero roots. We define an antisymmetric map $\dpt{c}{\Delta\times \Delta}{\eC}$ as following. If $\alpha,\beta\in S$ are such that $\alpha+\beta\notin\Delta$, we pose $c(\alpha,\beta)=0$. If $\alpha+\beta\in\Delta$,
\begin{equation}
[x_{\alpha},x_{\beta}]=c(\alpha,\beta)x_{\alpha+\beta}.
\end{equation}
It is easy to see that $c(\alpha,\beta)=-c(\beta,\alpha)$.

\begin{proposition}
If $\alpha,\beta,\alpha+\beta\in\Delta$, then

\begin{enumerate}
\item
\[
c(-\alpha,\alpha+\beta)=c(\alpha+\beta,-\beta)=c(-\beta,-\alpha),
\]
\label{enuai}
\item\label{enuaii} If $\alpha,\beta,\gamma,\delta\in\Delta$ and $\alpha+\beta+\gamma+\delta=0$ wile $\delta$ is neither $-\alpha$, nor $-\beta$ nor $-\gamma$, then
\begin{equation}\label{eq:enuaii}
c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta)=0,
\end{equation}

\item if $\beta\neq\alpha\neq -\beta$, then
\[
c(\alpha,\beta)+c(-\alpha,-\beta)=c(\alpha,-\beta)c(-\alpha,\beta)-B(h_{\alpha},h_{\beta}),
\]
\label{enuaiii}
\item\label{enuaiv} if $\alpha+\beta\neq0$ then
\begin{equation}\label{eq:enuaiv}
2c(\alpha,\beta)c(-\alpha,-\beta)=\lbha(1+\lbba)\alpha(h_{\alpha}).
\end{equation}

\end{enumerate}
\label{prop:enua}
\end{proposition}

\begin{proof}
From our choice of $ x_{\alpha}$, we find that $B(\xbb,\xbmb)=B(\xbma, x_{\alpha})=B(x_{\alpha+\beta},\xbmab)=1$, but
\begin{equation}
\begin{split}
  B\big(c(-\alpha,\alpha+\beta)\xbb,\xbmb\big)&=B\big(\xbma,c(\alpha+\beta,-\beta) x_{\alpha}\big)\\
                        &=B\big(x_{\alpha+\beta},c(-\beta,-\alpha)\xbmamb \big).
\end{split}
\end{equation}
This proves~\ref{enuai}. In order to prove~\ref{enuaii}, suppose that
\begin{equation}\label{eq:amontrer}
c(\alpha,\beta)c(\gamma,\delta)=B\Big(  \big[[ x_{\alpha},\xbb],\xbg\big] ,\xbd  \Big)
\end{equation}
Then the Jacobi identity gives the result:
\begin{equation}
\begin{split}
0&=B\Big( \big[[ x_{\alpha},\xbb],\xbg\big],\xbd \Big)+B\Big(\big[[\xbb,\xbg], x_{\alpha}\big],\xbd\Big)+B\Big(\big[[\xbg, x_{\alpha}],\xbb\big] ,\xbd\Big)\\
&=c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta),
\end{split}
\end{equation}
Here, we used the hypothesis $-\gamma\neq\delta\neq -\beta$ by supposing that \eqref{eq:amontrer} still hold after permutation of $\alpha,\beta,\gamma$.
Now we show the \eqref{eq:amontrer} is true. The assumptions imply  $\alpha+\beta=-(\gamma+\delta)\neq 0$, then
\begin{equation}
\begin{split}
B\big(\big[[ x_{\alpha},\xbb],\xbg \big],\xbd\big)&=B\big( [ x_{\alpha},\xbb],[\xbg,\xbd]  \big)\\
&=c(\alpha,\beta)c(\gamma,\delta)B(x_{\alpha+\beta},x_{\gamma+\delta})\\
&=c(\alpha,\beta)c(\gamma,\delta).
\end{split}
\end{equation}
Now we turn our attention to~\ref{enuaiii}. If $\alpha$ and $\beta$ fulfil the condition $\beta\neq\alpha\neq-\beta$, we can apply~\ref{enuaii} on the quadruple $(\alpha,\beta,-\alpha,-\beta)$ to get $c(\alpha,\beta)c(-\alpha,-\beta)=
-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]   \big)$.
If we replace $\beta$ by $-\beta$ and if we make the difference between the two expressions,
\begin{equation}
\begin{split}
c(\alpha,\beta)c(-\alpha,-\beta)&=-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]    \big)+B\big(   [ x_{\alpha},\xbmb],[\xbma,\xbb]    \big)\\
&=B\big( [ x_{\alpha},[\xbmb,\xbma]],\xbb \big)-B\big(
[\xbma,[ x_{\alpha},\xbmb]],\xbb \big)\\
&=-B\big( [\xbma, x_{\alpha}],[\xbmb,\xbb]  \big) \\
&=-B(h_{\alpha},h_{\beta}).
\end{split}
\end{equation}

In order to prove~\ref{enuaiv}, we consider $\alpha+\beta\neq0$ and we pose
\[
d(\alpha,\beta)=c(\alpha,\beta)c(-\alpha,-\beta)-\frac{1}{2}\lbha(1+\lbba)\alpha(h_{\alpha}).
\]
Our aim is to prove that it is zero. We will do it by induction on $\lbha$. First $\lbha=0$ means that $\beta+\alpha=0$, so that $c(\alpha,\beta)=0$ and $d(\alpha,\beta)=0$. Now we suppose that $\lbha>0$ and that~\ref{enuaiv} is yet checked for lower cases. Note that $\beta+\alpha\in \Delta$ and $(\beta+\alpha)+\alpha\neq 0$ because $-2\alpha$ is not a root. Then $\beta=2\alpha$ is not possible. From the fact that $(\beta+\alpha)^{\alpha}=\lbha-1$, we conclude $d(\alpha,\beta+\alpha)=0$. Then
\[
c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)=c(\alpha,-\alpha-\beta)c(-\alpha,\alpha+\beta)-B(h_{\alpha},h_{\alpha+\beta}).
\]
On the other hand,~\ref{enuai} and the antisymmetry of $c$ give
\begin{subequations}
\begin{align}
c(-\alpha,\alpha+\beta)=c(-\beta,-\alpha)=-c(-\alpha,-\beta)\\
\intertext{and}
c(\alpha,-\alpha-\beta)=c(\beta,\alpha)=-c(\alpha,\beta)
\end{align}
\end{subequations}
With all this
\begin{equation}
\begin{split}
d(\alpha,\beta+\alpha)&=c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)-\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})\\
&=c(\alpha,\beta)c(-\alpha,-\beta)-k(\alpha,\beta)
\end{split}
\end{equation}
where $k(\alpha,\beta)=B(h_{\alpha},h_{\alpha+\beta})+\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})$. But $h_{\alpha+\beta}$ is definied in order to have $B(h,h_{\alpha+\beta})=(\alpha+\beta)(h)$ for any $h\in\lH$. Then using $2\beta(h_{\alpha})=(\lbba-\lbha)\alpha(h_{\alpha})$, we find $k(\alpha,\beta)=\frac{1}{2}\alpha(h_{\alpha})\lbha(1+\lbba)$.
\end{proof}

\begin{proposition}\label{prop:lHeR}\index{real!form!of a vector space}
Let
\begin{equation}
\lHeR=\sum_{\alpha\in\Delta}\eR h_{\alpha}.
\end{equation}
Then:
\begin{enumerate}
    \item any root is real on $\lHeR$,
    \item the Killing form is real and strictly positive definite on $\lHeR$,
    \item $\lH=\lHeR\oplus i\lHeR$.
\end{enumerate}
\end{proposition}

The last item shows that $\lHeR$ is a real form of $\lH$. Remark also that $\lHeR$ can also be written as
\[
\lHeR=\{h\in\lH\tq \alpha(h)\in\eR\,\forall\alpha\in\Phi \}.
\]
\begin{proof}
Let $\beta\in\Delta$; we looks at $\beta( h_{\alpha})$. From~\ref{ite:six_deux} of theorem~\ref{tho:six_Cartan}, we know that $\alpha( h_{\alpha})$ is real and positive, and~\ref{ite:six_trois} makes $\beta( h_{\alpha})$ real. From the formula $B( h_{\alpha},\hbb)=\sum_{\gamma\in\Delta}\gamma( h_{\alpha})\gamma(\hbb)$, the Killing form is real and positive definite on $\lHeR\times\lHeR$. If $B(h,h)=0$ for a certain $h\in\lHeR$, we find $\alpha(h)=0$ for all $\alpha\in\Delta$. Then any $x=x^{\alpha} x_{\alpha}\in\lG$ commutes with $h$ because
\[
[h,x]=\sum_{\alpha\in\Phi}a^{\alpha}(\ad h) x_{\alpha}=\sum_{\alpha}a^{\alpha}\alpha(h)=0.
\]
So $h$ is in the center of $\lG$ and so $h=0$ be cause $\lG$ is semisimple. Thus the Killing form is strictly positive definite on $\lHeR\times\lHeR$.

Now we are going to show that $\lH=\lHeR\oplus i\lHeR$. If $h\in\lHeR\cap i\lHeR$, it can be written as $h=ih'$ with $h,h'\in\lHeR$. Then
\[
0<B(h,h)=B(ih',ih')=-B(h',h')<0,
\]
so that $h=0$ because $B$ is nondegenerate. This shows that $\lHeR\cap i\lHeR=0$. It is clear that $\sum_{\alpha\in\Delta}\eC h_{\alpha}\subset\lH$; thus it remains to be proved that $\lH\subset\sum_{\alpha\in\Delta}\eC h_{\alpha}$. It it is not, we can build a linear function $\dpt{\lambda}{\lH}{\eC}$ which is not identically zero but which is zero on the subspace $\sum_{\alpha\in\Delta}\eC h_{\alpha}$. Then there exists (only one) $h_{\lambda}\in\lH$ such that $B(h,h_{\lambda})=\lambda(h)$ for every $h\in\lH$. In particular, $\alpha(h_{\lambda})=0$ for every $\alpha\in\Delta$ because $\alpha(h_{\lambda})=B( h_{\alpha},h_{\lambda})=\lambda( h_{\alpha})$. This implies that $h_{\lambda}=0$, so that $\lambda\equiv 0$.

\end{proof}

One interest in the third point of this proposition is that we are now able to see $\Delta$ as a subset of $\lHeR^*$ because the definition of $\alpha\in\Delta$ on $\lHeR$ only is sufficient to define $\alpha$ on the whole $\lH$.

If $\{e_i\}$ is a basis of a vector space $V$, we say that $x=x^ie_i>y=y^ie_i$ if $x-y=a^ie_i$ and the first non zero $a^i$ is positive. This is the \defe{lexicographic order}{lexicographic ordering} on $V$. It is clear that it doesn't works on a complex vector space (because in this case we should first define $a^i>0$), but we can anyway get an order on $\Delta$ by seeing it as a subset of $\lHeR$.

\begin{lemma}
    If $\alpha-\beta$ are simple roots with $\alpha\neq\beta$, then $\beta-\alpha$ is not a root and $B( h_{\alpha},\hbb)\leq 0$.
\end{lemma}

\begin{proof}
    Define $\gamma=\beta-\alpha\in\Delta$ (and not $\Phi$ because $\alpha\neq\beta$). If $\gamma>0$, the fact that $\beta=\gamma+\alpha$ contradict the simplicity of $\beta$ while if $\gamma<0$, in the same way $\alpha=\beta-\alpha$ contradict the simplicity of $\alpha$.

    Since $\beta-\alpha$ is not a root, $\lbba=0$ and $\lbha\geq 0$ thus formula $2\beta( h_{\alpha})=(\lbba-\lbha)\alpha( h_{\alpha})$ gives
    \begin{equation}
        2B( h_{\alpha},\hbb)=\underbrace{(\lbba-\lbha)}_{\leq 0}B( h_{\alpha}, h_{\alpha}).
    \end{equation}
    Now proposition~\ref{prop:lHeR} gives the result.
\end{proof}

\begin{lemma}
    The simple roots are linearly independent.
\end{lemma}

\begin{proof}
    In the definition of a simple root, we need an order notion on $\Delta$ which is then seen as a subset of $\lHeR$. But the roots are real thereon. Then the right notion of ``independence''{} for the simple root is the independence with respect to \emph{real} combinations.

    If one has a combination $c^i\alpha_i=0$ (sum over $i$) with at least one non zero among the $c^i$'s  by putting the negative $c^i$'s at right, one can write
    \[
        a^i\alpha_i=b^j\alpha_j
    \]
    with $a^i,b^j\geq 0$. Let us consider $\gamma=a^i\alpha_i$ and $h_{\gamma}$. For every $h\in\lH$, we have
    \[
        B(h,h\gamma)=\gamma(h)=a^i\alpha_i(h_{\gamma}).
    \]
    but $h_{\gamma}=a^jh_{\alpha}$, then
    \begin{equation}
        B(h_{\gamma},h\bgamma)=a^ia^j\alpha_i(h_{\alpha_j})
                    =a^ia^jB(h_{\alpha_i},h_{\alpha_i}).
    \end{equation}
    Since the $\alpha_i$ are all simple roots, the right hand side is negative, but proposition~\ref{prop:lHeR} makes the left hand side positive. Thus $\gamma=0$.
\end{proof}

\begin{theorem}
    If $\{\alpha_1,\ldots,\alpha_r\}$ is the set of all the simple roots, then $\dim\lHeR=r$ and every $\beta\in\Phi$ can be decomposed as
    \[
        \beta=\sum_{i=1}^rn_i\alpha_i
    \]
    where the $n_i$ are integers either all positive either all negative.
\end{theorem}

\begin{proof}
    Let $\beta$ be a non simple positive root. Then it can be decomposed as $\beta=\gamma+\delta$ with $\gamma,\delta>0$.We can also separately decompose $\gamma$ and $\delta$ and continue so until we are left with simple roots. We have to see why the process stops. Since there are only a finite number of positive root, if the process does not stop, then the decomposition of (at least) one of the positive roots $\gamma$ contains $\gamma$ itself. So we have a situation $\gamma=\gamma+\alpha$ for a certain positive $\alpha$. This contradict the notion of order.

    In particular $\Span_{\eN}\{\alpha_i\}=\{\textrm{positive roots}\}$. Thus it is clear that
    \[
        \Span_{\eR}\{\alpha_i\}=\Phi.
    \]
\end{proof}


\begin{theorem}\label{tho:Phi_base}
The Cartan algebra of a complex semisimple Lie algebra is abelian and the dual is spanned by the roots: \( \Span\Phi=\lH^*\).
\end{theorem}

\begin{proof}
Let $\alpha$ be a non zero root; from the point~\ref{prop:trois_poids:deux} of proposition~\ref{prop:trois_poids}, there exists a $v\in\lG_{\alpha}$ such that for any $x\in\lH$, $[x,v]=\alpha(x)v$. Since $\dim\lG_{\alpha}=1$ it is in fact true for any $v\in\lG_{\alpha}$. In particular $\forall v\in\lG_{\alpha}$ and $h\in\lH$, we have $[h,x]=\alpha(h)x$.

Let $\lN\subset\lH$ be the set of elements which are annihilated by all the roots:
\begin{equation}
    \lN=\{ H\in\lH\tq\alpha(H)=0\,\forall \alpha\in\Phi \}.
\end{equation}
First remark that
\begin{equation}\label{eq:GNz}
[\lG_{\alpha},\lN]=0
\end{equation}
because for $x\in\lG_{\alpha}$ and $h\in\lN\subset\lH$, we have $[h,x]=\alpha(h)x=0$. An other property of $\lN$ is
\begin{equation}\label{eq:HHN}
[\lH,\lH]\subset\lN.
\end{equation}
Indeed consider a root $\alpha$ and $x\in\lG_{\alpha}$. We have
\begin{equation}
\begin{split}
-\alpha([h,h'])x&=[x,[h,h']]
=[h,[h',x]]+[h',[x,h]]
=\alpha(h)[h',x]+\alpha(h')[x,h]\\
&=\alpha(h)\alpha(h')-\alpha(h')\alpha(h)
=0.
\end{split}
\end{equation}
If $x\in\lG$ is decomposed as $x=\sum_{\alpha\in\Phi}x_{\alpha}$ and if $n\in\lN$, then
\[
[x,n]=\sum_{\alpha}[x_{\alpha},n]=\sum_{\alpha}\alpha(n)x_{\alpha}=0.
\]
In particular, $\lN$ is an ideal\quext{\c Ca me semble quand m\^eme fort de prouver que c'est le centralisateur pour dire que c'est un id\'eal. D'autant plus que je pourais directement dire que $\lN$ est centralisateur dans un semisimple et donc nulle.}. Moreover, the fact that $\lN\subset\lH$ makes $\lN$ a \emph{nilpotent} ideal in the semisimple Lie algebra $\lG$. Then $\lN=0$. Equation \eqref{eq:GNz} makes $\lH$ abelian while equation \eqref{eq:HHN} says that no element of $\lH$ is anihilated by all the roots. This implies that $\Span\Phi=\lH^*$. To see it more precisely, if $\Phi$ don't span a certain (dual) basis element $e_i^*$ of $\lH^*$, then a basis of $\Span\Phi$ is at most $\{e_j\}_{j\neq i}$. Then it is clear that $\alpha(e_i)=0$ for any root $\alpha$.
\end{proof}


The following important result is the fact that a complex semisimple Lie algebra is determined by its root system.
\begin{theorem}
Let $\lG$ and $\lG'$ be two semisimple complex Lie algebras; $\lH$ and $\lH'$, Cartan subalgebras. We suppose that we have a bijection $\Phi\to\Phi'$, $\alpha\to\alpha'$ which preserve the root system:

\begin{itemize}
\item $\alpha'+\beta'=0$ if and only if $\alpha+\beta=0$,
\item $\alpha'+\beta'$ is not a root if and only if $\alpha+\beta$ is also not a root,
\item $(\alpha+\beta)'=\alpha'+\beta'$ whenever $\alpha+\beta$ is a root.
\end{itemize}
Then we have a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$ such that $\eta(\lH)=\lH'$ and $\alpha'\circ\eta|_{\lH}=\alpha$.
\end{theorem}

\begin{proof}
From the assumptions, $\lbha=(\beta')^{\alpha'}$ and $\lbba=(\beta')_{\alpha'}$ and the point~\ref{ite:six_deux} of theorem~\ref{tho:six_Cartan} makes $\alpha'(h_{\alpha'})=\alpha(h_{\alpha})$. The fourth point of the same theorem then gives
\begin{equation}\label{eq:beta_h_beta}
\beta'(h_{}\alpha')=\beta(h_{\alpha}).
\end{equation}
Now we choose a maximally linearly independent set $(\alpha_1,\ldots,\alpha_R)$ of roots of $\lG$. Because of theorem~\ref{tho:Phi_base}, this is a basis of $\lH^*$. For notational convenience, we put $h_r=h_{\alpha_r}$ and naturally, $h'_r=h_{\alpha'_r}$. It is easy to see that the set of $h_r$ is a basis of $\lH$. Indeed if $a^rh_r=0$ (with sum over $r$), then $B(h,a^r,h_r)=a^r\alpha_r(h)=0$ which implies that $a^r\alpha_r|_{\lH}=0$ but it is impossible because the $\alpha_r$ are free in $\lH^*$.
\begin{equation*}
\begin{split}
\{ \alpha_1,\ldots,\alpha_r \}&\textrm{ is a basis of $\lH^*$},\\
\{ h_1,\ldots,h_r \}&\textrm{ is a basis of $\lH$}.
\end{split}
\end{equation*}
Then the matrix $(A_{ij})=\alpha_i(h_j)$ has non zero determinant. Since $\alpha'_i(h_j')=\alpha_i(h_j)$, the set $\{\alpha'_1,\ldots,\alpha'_r\}$ is free and $\{h'_1,\ldots,h'_r\}$ is a basis of $\lH'$.
\begin{equation*}
\begin{split}
\{ \alpha'_1,\ldots,\alpha'_r \}&\textrm{ is a basis of ${\lH'}^*$},\\
\{ h'_1,\ldots,h'_r \}&\textrm{ is a basis of $\lH'$}.
\end{split}
\end{equation*}
Then can define an isomorphism $\dpt{\etalH}{\lH}{\lH'}$ by $\etalH(h_i)=h'_i$. If $x\in\lH$ is decomposed as $x=a^rh_r$, from equation \eqref{eq:beta_h_beta} we have $(\alpha'_i\circ\etalH)(a^rh_r)=a^r\alpha'(h'_r)=\alpha_i(h_r)$. Then
\[
\alpha'_i\circ\etalH=\alpha_i.
\]

Let $\alpha\in\Phi$; we can write $\alpha=c_i\alpha_i$ and $\alpha'=c'_i\alpha'_i$ (with a sum over $i$). We have
\begin{equation}
c_i\alpha_i(h_k)=\alpha(h_j)
=\alpha'(h_j)
=c'_i\alpha_i(h_j).
\end{equation}
As the determinant of $(\alpha_i(h_j))$ is non zero, this implies $c_i=c'_i$, so that
\begin{equation}
\alpha'\circ\etalH=\alpha
\end{equation}
because $\alpha'\circ\etalH=c'_i(\alpha'_i\circ\etalH)=c_i\alpha_i=\alpha$. Now we ``just''{} have to extend $\etalH$ into a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$. As before for each $\alpha\in \Delta$ we choose $ x_{\alpha}\in\lG_{\alpha}$ such that $B( x_{\alpha},\xbma)=-1$ and $[\xbma, x_{\alpha}]= h_{\alpha}$. We naturally do the same for $x_{\alpha'}\in\lG'_{\alpha'}$. We also consider the function $c$ as before: $[ x_{\alpha},\xbb]=c(\alpha,\beta)x_{\alpha+\beta}$. Since $\lH=\lG_0$, these $ x_{\alpha}$ form a basis of $\lG\ominus\lH$ and $\eta$ can be defined by the date of $\eta( x_{\alpha})$. We set $\eta( x_{\alpha})=a_{\alpha}x_{\alpha'}$ (without sum).

The condition $\eta\big(  [ x_{\alpha},\xbb]=[\eta( x_{\alpha}),\eta(\xbb)]  \big)$ gives
\begin{subequations}
\begin{align}
\label{eq:ca_caa_a}
c(\alpha,\beta)a_{\alpha+\beta}&=c(\alpha',\beta')\aba\abb&\quad\text{if }\alpha+\beta\neq 0\\
\intertext{and}
\label{eq:ca_caa_b}
\aba\abma &=1                        &\quad\forall\alpha\in\Phi.
\end{align}
\end{subequations}
These two conditions are necessary and also sufficient. Indeed there are three cases of $[x,y]$ to check: $x$, $y\in\lH$, one of these two is out of $\lH$ or $x,y$ are booth out of $\lH$. In the third case, using \eqref{eq:ca_caa_a},
\begin{equation}
\eta([ x_{\alpha},\xbb])=c(\alpha,\beta)a_{\alpha+\beta}x_{\alpha'+\beta'}
            =x(\alpha',\beta')\aba\abb x_{\alpha'+\beta'}
            =\aba\abb[x_{\alpha'},\abbp]
            =[\eta( x_{\alpha}),\eta(\xbb)].
\end{equation}
If $x$, $y\in\lH$, then from theorem~\ref{tho:Phi_base}, $\eta([x,y])=0=[\eta(x),\eta(y)]$. Using the fact that $[h, x_{\alpha}]=\alpha(h) x_{\alpha}$, we find the third case:
\begin{equation}
\eta\big(  [\hbb, x_{\alpha}]  \big)=\eta\big(  \alpha( h_{\alpha}) x_{\alpha}  \big)
                =\eta\big(  \alpha'(h_{\beta'}) x_{\alpha}  \big)
                =\aba[h_{\beta'},x_{\alpha'}]
                =[\eta(h_{\beta}),\eta( x_{\alpha})].
\end{equation}
Now we are going to find some $\aba\in\eC$ such that
\begin{itemize}
\item $\aba\abma=1$ for any $\alpha$,
\item $c(\alpha,\beta)a_{\alpha+\beta}=c(\alpha',\beta')\aba\abb$ if $\alpha+\beta\neq 0$.
\end{itemize}
We consider the lexicagraphic order \index{lexicographic ordering} on $\Phi$: this is the order on $\Phi$ seen as a subset of $\lHeR$ on which we put the lexicagraphic order. For a root $\alpha>0$, we will fix the coefficient $\aba$ by an induction with respect to the order and put $\abma=\aba^{-1}$. Let us consider $\rho>0$ and suppose that $\aba$ is already defined for $-\rho<\alpha<\rho$ in such a manner that $\aba\abma=1$ and $c(\alpha,\beta)\abab=c(\alpha',\beta')\aba\abb$ for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are stricly between $-\rho$ and $\rho$. We have to define $\abr$ in such a way that if $\abmr=\abr^{-1}$, the second condition holds for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are no zero roots between $-\rho$ and $\rho$.

If such a pair $(\alpha,\beta)$ doesn't exist, there are no problem to put $\abr=\abmr=1$. Let us suppose that such a pair exists: $\alpha+\beta=\rho$. Then $\lbha\neq 0$ and the point~\ref{enuaiii} of proposition~\ref{prop:enua} shows that $c(\alpha,\beta)\neq 0$; in the same way, $(\beta')^{\alpha'}=\lbha\neq 0$ implies $c(\alpha',\beta')\neq 0$. We define
\begin{subequations}\label{eq:def_abr}
\begin{align}
\abr&=c(\alpha,\beta)^{-1} c(\alpha',\beta')\aba\abb,\\
\abmr&=\abr^{-1}.
\end{align}
\end{subequations}


Since the value of the right hand side of \eqref{eq:enuaiv} doesn't change under $\alpha\to\alpha'$ and $\beta\to\beta'$, it gives  $c(\alpha,\beta)c(-\alpha,-\beta)=c(\alpha',\beta')c(-\alpha',-\beta')$ and thus
\begin{equation}
\begin{split}
c(-\alpha,-\beta)\abmr&=c(-\alpha,-\beta)c(\alpha,\beta)c(\alpha',\beta')^{-1}\abma\abmb\\
&=c(\alpha',\beta')c(-\alpha',-\beta')c(\alpha',\beta')^{-1}\aba\abmb\\
&=c(-\alpha',-\beta')\abma\abmb.
\end{split}
\end{equation}
Thus the definition \eqref{eq:def_abr} fulfils the requirements for the pair $(\alpha,\beta)$. It should be shown whether that works as well with another pair $(\gamma,\delta)$ such that $-\rho\leq\gamma,\delta\leq\rho$ and $\gamma+\delta=\rho$. If this second pair is really different that $(\alpha,\beta)$, then $\delta$ is neither $\alpha$ nor $\beta$; it is allso clear that $\delta$ is not $-\gamma$. Then formula \eqref{eq:enuaii}  works with the quadruple $(-\alpha,-\beta,\gamma,\delta)$:
\begin{equation}\label{eq:c_un}
c(-\alpha,-\beta)c(\gamma,\delta)+c(-\beta,\gamma)c(-\alpha,\delta)+c(\gamma,-\alpha)c(-\beta,\delta)=0.
\end{equation}
If $\alpha<0$, the assumption $\alpha+\beta=\rho$ makes $\beta>\rho$, which is in contradiction with $-\rho\leq\beta\leq\rho$. Then $\alpha,\beta,\gamma,\delta>0$ and moreover, the difference of any two of them is strictly between $-\rho$ and $\rho$. Since $\delta-\alpha=-(\gamma-\beta)$, if $\gamma-\beta$ is a root, $\delta-\alpha$ is also a root and the induction hypothesis gives
\begin{subequations}\label{eq:c_deux_un}
\begin{align}
c(\gamma,-\beta)a_{\gamma-\beta}&=c(\gamma',-\beta')\abg\abmb,  \label{eq:c_deux_un_a}\\
c(-\alpha,\delta)a_{-\alpha+\delta}&=c(-\alpha',\delta')\abma\abd.\label{eq:c_deux_un_b}
\end{align}
\end{subequations}
If we take for the convention $a_{\mu}=1$ whenever $\mu$ is not a root, these relations still hold if $\gamma-\beta$ is not a root. In the same way,
\begin{subequations}\label{eq:c_deux_deux}
\begin{align}
c(\gamma,-\alpha)a_{\gamma-\alpha}&=c(\gamma',-\alpha')\abma\abg,\label{eq:c_deux_deux_a}\\
c(-\beta,\delta)a_{-\beta+\delta}&=c(-\beta',\delta')\abmb\abd.\label{eq:c_deux_deux_b}
\end{align}
\end{subequations}
As $\delta-\alpha=-(\gamma-\beta)$, we have $a_{\delta-\alpha}a_{\gamma-\beta}=1$ and in the same way, $a_{\gamma-\alpha}a_{\delta-\beta}=1$. Taking it into account and multiplicating \eqref{eq:c_deux_un_a} by \eqref{eq:c_deux_un_b} and \eqref{eq:c_deux_deux_a} by \eqref{eq:c_deux_deux_a}, we find:
\begin{subequations}\label{eq:c_deux_trois}
\begin{align}
c(-\beta,\gamma)c(-\alpha,\delta)&=c(-\beta',\gamma')c(-\alpha',\delta')\abma\abmb\abg\abd\label{eq:c_deux_trois_a}\\
c(\gamma,-\alpha)c(-\beta,\delta)&=c(\gamma',-\alpha')c(-\beta',\delta')\abma\abmb\abg\abd.
\end{align}
\end{subequations}
We can use it to rewrite equation \eqref{eq:c_un}. After multiplication by $\aba\abb\abmg\abmd$,
\begin{equation}
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd+c(-\beta',\gamma')c(-\alpha',\delta')+c(\gamma',-\alpha')c(-\beta',\delta')=0.
\end{equation}
But equation \eqref{eq:c_un} is also true for $(\alpha',\beta',\gamma',\delta')$ instead of $(\alpha,\beta,\gamma,\delta)$, so that the last two terms can be replaced by only one term to give
\[
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd-c(-\alpha',-\beta')c(\gamma',\delta')=0.
\]
Since the pair $(\alpha,\beta)$ fulfils $c(-\alpha,-\beta)a_{-\alpha-\beta}=c(-\alpha',-\beta')\abma\abmb$, using $\alpha+\beta=\gamma+\delta$, we find
\[
c(\gamma,\delta)a_{\gamma+\delta}=c(\gamma',\delta')\abg\abd.
\]

\end{proof}


\begin{corollary}
The elements $ x_{\alpha}\in\lG_{\alpha}$ can be chosen in order to satisfy

\begin{itemize}
\item $B( x_{\alpha},\xbma)=1,$
\item $[ x_{\alpha},\xbma]= h_{\alpha}$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}

\end{corollary}

These vectors $ x_{\alpha}\in\lG_{\alpha}$ are called \defe{root vectors}{root!vectors}.

\begin{proof}
We consider the isomorphism $\alpha\to\alpha$ from $\Phi$ to $\Phi$; by the theorem this induces an isomorphism $\dpt{\eta}{\lG}{\lG}$ given by some constants $c_{\alpha}$:
\[
\eta( x_{\alpha})=c_{-\alpha}\xbma
\]
without sum on $\alpha$, because of course $\eta( x_{\alpha})\in\lG_{-\alpha}$. We choose $a x_{\alpha}\in\eC$ in such a way that
\begin{subequations}
\begin{align}
a_{\alpha}^2&=-c_{-\alpha}\\
a_{\alpha} a_{-\alpha}&=1,
\end{align}
\end{subequations}
and then we put $y_{\alpha}=a_{\alpha} x_{\alpha}$. It is immediate that $B(y_{\alpha},y_{-\alpha})=1$, thus the redefinition $ x_{\alpha}\to y_{\alpha}$ doesn't change the obtained relations. Acting on $y_{\alpha}$, the isomorphism $\eta$ gives
\begin{equation}
\eta(y_{\alpha})=a_{\alpha} c_{-\alpha}\xbma
=-a_{-\alpha}\xbma
=-y_{-\alpha}.
\end{equation}

If $\alpha,\beta,\alpha+\beta\in\Delta$, we naturally define $c'(\alpha,\beta)$ by
\[
[y_{\alpha},y_{\beta}]=c'(\alpha,\beta)y_{\alpha,\beta}.
\]
Using the fact that $\eta$ is a Lie algebra automorphism of $\lG$ we have:
\begin{equation}
-c'(\alpha,\beta)y_{-(\alpha+\beta)}=\eta\big(  c'(\alpha,\beta)y_{\alpha+\beta}   \big)
            =[-y_{-\alpha},-y_{-\beta}]
            =c'(-\alpha,-\beta)y_{-(\alpha+\beta)}.
\end{equation}
\end{proof}

From now we always our $ x_{\alpha}$ in this way.

\begin{remark}
It is also possible to choice the $ x_{\alpha}$ in such a way that

\begin{itemize}
\item $B( x_{\alpha},\xbma)=-1$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}
This is the choice of the reference \cite{Hochschild}.
\end{remark}


Here is a characterization for Cartan subalgebras of semisimple Lie algebras. This is sometimes taken as the \emph{definition} of a Cartan subalgebra in books devoted to semisimple Lie algebras (for example in \cite{Helgason}).
\begin{proposition}     \label{PropCartanMaxAnel}
    A subalgebra $\lH$ of a semisimple Lie algebra $\lG$ is a Cartan subalgebra if and only if
    \begin{itemize}
        \item $\lH$ is maximally abelian in $\lG$,
        \item the endomorphism $\ad h$ is semisimple\footnote{If the base field is \( \eC\), this means ``diagonalizable''.} for every $h\in\lH$.
    \end{itemize}
\end{proposition}

\begin{proof}
\subdem{Necessary condition} We know from theorem~\ref{tho:Phi_base} that $\lH$ is abelian and from proposition~\ref{prop:Cartan_max_nil} that it is maximally nilpotent. Then it is maximally abelian. On the other hand, let $h\in\lH$; the endomorphism $\ad h$ is diagonalisable with respect to the decomposition $\lG=\lH\bigoplus_{\alpha\in\Delta}\lH_{\alpha}$.

\subdem{Sufficient condition}
Firstly it is clear that a maximal abelian subalgebra is nilpotent and the $\ad h_i$ are simultaneously diagonalisable for the different $h_i\in\lH$. Let $\{x_1,\ldots,x_n\}$ be a basis of $\lG$ which diagonalise all the $\ad h_i$. In this basis, if $(\ad h)_{ii}=0$ for any $h\in\lH$, then $x_i\in\lH$: if it was not, $\lH\cup\{x_i\}$ would be abelian.

Let $x\in\lG$ such that $(\ad h)x\in\lH$ for every $h\in\lH$. Suppose that $x$ has a $x_i$-component with $x_i\notin\lH$. There is a $h\in\lH$ with $(\ad h)_{ii}\neq 0$. Then $(\ad h)x$ has a $x_i$-component and can't lies in $\lH$.

\end{proof}

This characterization of Cartan subalgebras is used to prove the existence of Cartan subalgebra for any complex semisimple Lie algebra.



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl: other results}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
Two immediate properties of the Weyl group are

\begin{enumerate}
\item $W$ is a finite group of orthogonal transformations of $V$,
\item if $r$ is an orthogonal transformation of $V$, the $s_{r\alpha}=rs_{\alpha} r^{-1}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\subdem{First item} By definition of an abstract root system, $W$ leaves $\Delta$ invariant; since $V$ is spanned by $V$, it implies that $W$ also leaves $V$ invariant. From an easy computation, $(s_{\alpha}\varphi,s_{\alpha}\phi)=(\varphi,\phi)$. Since $\Delta$ is a finite set, there are only a finite number of common permutations of elements of $\Delta$ \emph{a fortiori} $W$ is finite.

\subdem{Second item}
It is easy to see that $s_{r\alpha}(r\varphi)=rs_{\alpha}\varphi$, then $s_{r\alpha}=r\circ s_{\alpha}\circ r^{-1}$.
\end{proof}


We introduce the \defe{root reflexion}{root!reflexion} $\dpt{s_{\alpha}}{\lHeR^*}{\lHeR^*}$ for $\alpha\in\Phi$ and $\varphi\in\lHeR^*$ by
\begin{equation}
s_{\alpha}(\varphi)=\varphi-\frac{2(\varphi,\alpha)}{|\alpha|^2}\alpha.
\end{equation}

\begin{proposition}
If $\alpha\in\Phi$, then $s_{\alpha}$ leaves $\Phi$ invariant.
\end{proposition}

\begin{proof}
If $\alpha$ or $\varphi$ is zero, then it is clear that $s_{\alpha}(\varphi)$ belongs to $\Phi$. Thus we can suppose that $\alpha\in\Delta$ and proof that $s_{\alpha}$ leaves $\Delta$ invariant. For, we use the theorem~\ref{tho:six_Cartan} to find
\begin{equation}
  s_{\alpha}\beta=\beta-\frac{2(\beta,\alpha)}{|\alpha|^2}\alpha
               =\beta-(\lbba-\lbha)\alpha.
\end{equation}
If $\lbba-\lbha>0$, we are in a case $\beta-n\alpha$ with $\lbba-\lbha<\lbba$, so that $s_{\alpha}\beta$ is a root. The case $\lbha>\lbba$ is treated in the same way. It just remains to check that  if $\alpha,\beta\in\Delta$, then $s_{\alpha}\beta\neq0$. The problem is to show that the equation (with a given $\alpha$ in $\Delta$)
\begin{equation}\label{eq:beta_frZ_alpha}
   \beta=\frZ{\alpha}{\beta}\alpha
\end{equation}
has no solution in $\Delta$ (the indeterminate is $\beta$). The only nonzero multiples of $\beta$ which are roots are $\pm\beta$, then if we set $\beta=r\alpha$, equation \eqref{eq:beta_frZ_alpha} gives $r=\pm\frac{1}{2}$, which is impossible.

\end{proof}

\begin{proposition}
    The Weyl group permutes simply transitively the simple systems.
\end{proposition}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Longest element}
%---------------------------------------------------------------------------------------------------------------------------

Let \( w\in W\). The \defe{length}{length!in Weyl group}\nomenclature[G]{\( l(w)\)}{length in the Weyl group} of \( w\) is the smallest \( k\) such that \( w\) can be written as a composition of \( k\) reflexions \( s_{\alpha_i}\). That is the smallest \( k\) such that
\begin{equation}
    w=s_{\alpha_{i_1}}s_{\alpha_{i_2}}\ldots s_{\alpha_{i_k}}.
\end{equation}

\begin{lemma}
    If \( w\) and \( w'\) are elements of the Weyl group,
    \begin{enumerate}
        \item
            \( l(w)=l(w^{-1})\),
        \item
            \( l(w)=0\) if and only if \( w=\id\),
        \item
            \( l(ww')\leq l(w)+l(w')\),
        \item
            \( l(ww')\geq l(w)-l(w')\),
        \item
            \( l(w)-1\leq l(ws_{\alpha_i})\leq l(w)+1\).
    \end{enumerate}
\end{lemma}
Let \( n(w)\) be the number of positive simple roots that are send to a negative root:
\begin{equation}
    n(w)=Card\,\Pi\cap w^{-1}(-\Pi).
\end{equation}

\begin{proposition}
    Let \( \Delta\) be a system of simple roots and \( \Pi\) the associated positive system. The following conditions on an element \( w\) of the Weyl group are equivalent:
    \begin{enumerate}
        \item
            \( w\Pi=\Pi\);
        \item
            \( w\Delta=\Delta\);
        \item
            \( l(w)=0\);
        \item
            \( n(w)=0\);
        \item
            \( w=\id\).
    \end{enumerate}
\end{proposition}

For a proof see page 15 in \cite{HumphreysCoxeter}.

\begin{theorem}
    If \( w\) is an element of the Weyl group,
    \begin{equation}
        l(w)=n(w).
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl group and representations}
%---------------------------------------------------------------------------------------------------------------------------

This subsection comes from \cite{Cornwell}.

\begin{theorem}     \label{Thoirrepllamifffmor}
    There exists an irreducible representation of highest weight $\Lambda$ if and only if
    \begin{equation}
        \Lambda_{\alpha}=\frac{ 2(\Lambda,\alpha) }{ (\alpha,\alpha) }\in\eN
    \end{equation}
    for every simple root $\alpha$. Moreover, if $\xi$ is a highest weight vector and if $\alpha$ is a simple root, then
    \begin{equation}
        E_{-\alpha}^k\xi
                \begin{cases}
                    \neq 0  &\text{if }k\leq\Lambda_{\alpha}\\
                    =0  &\text{if }k>\Lambda_{\alpha}.
                \end{cases}
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

\begin{example} \label{ExHESKimc}
    We already studied the irreducible representations if \( \so(3)\) in the subsection~\ref{subsecPJmtqrG}. The theorem~\ref{Thoirrepllamifffmor} allows to determine them in a much more synthetic way.

    In the case of $\so(3)$, the Cartan subalgebra is one dimensional, and one has only one simple root: $\alpha=J_{12}^*$. If $\Lambda=aJ_{12}^*$, one has $(\Lambda,\alpha)=a$, and theorem~\ref{Thoirrepllamifffmor} says that $\Lambda$ is highest weight of an irreducible representation if and only if $a\in \eN/2$.
\end{example}

\begin{theorem}     \label{ThoLOngestlowestrepres}
    If \( \Lambda\) is the highest weight of a representation and if \( w_0\) is the longest element of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{theorem}

\begin{probleme}
    It is still not clear for me how does the proof works. Questions to be answered:
    \begin{enumerate}
        \item
            existence, unicity
        \item
            \( w_0\) is the longest element of the Weyl group
        \item
            if \( \Lambda\) is the highest weight, then \( w_0\Lambda\) is the lowest.
    \end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Chevalley basis (deprecated)}
%---------------------------------------------------------------------------------------------------------------------------
See \cite{SSLA_Modave2005}.

Let $\Phi$\nomenclature{$\Phi$, $\Phi^+$}{Root system} be the finite set of roots of $\lG$. Then chose a positivity notion on $\lH^*$ and consider $\Phi^+$, the positive subset of $\Phi$. We also take $\Delta$\nomenclature{$\Delta$}{Basis of the roots}, a basis of the roots. An element of $\Phi^+$ is a \defe{simple root}{simple!root}\index{root!simple} if it cannot be written under the form of a sum of two elements of $\Phi^+$. Every positive root is a sum of simple roots.

Let
\begin{equation}
    \{ \alpha_1,\ldots,\alpha_l \}
\end{equation}
be a basis of $\lH^*$ made of simple roots and
\begin{equation}
    \{ h_1,\ldots,h_l \},
\end{equation}
the dual basis. One can choose the $\alpha_i$ in such a way that $\{ h_1,\ldots,h_l \}$ is orthogonal with respect to the Killing form\quext{Why?}. One consequence of that is that
\begin{equation}            \label{EqBhihalphaih}
    B(h_i,h)=\alpha_i(h)
\end{equation}
for every $h\in\lH$. Indeed, $h$ can be written, in the basis, as $h=h^jh_j$ where $h^j=B(h_j,h)$. Thus one has
\begin{equation}
    B(h_i,h)=h^i=h^j\delta_{ij}=\alpha_i(h^jh_j)=\alpha_i(h).
\end{equation}
We consider $\{ \alpha_1,\ldots,\alpha_m \}$, the positive roots (the roots $\alpha_1$,\ldots,$\alpha_l$ are some of them). One knows that $\lG_{\alpha_i}$ is one dimensional, so one take $e_i\in\lG_{\alpha_i}$ and $f_i\in\lG_{-\alpha_i}$ as basis of their respective spaces. If we denote by $\lN^+=\Span\{ e_1,\ldots, e_m \}$ and $\lN^-=\Span\{ f_1,\ldots,f_m \}$, we have the decomposition
\begin{equation}
    \lG=\lN^-\oplus\lH\oplus\lN^+.
\end{equation}


% The proposition about SL(2,R) was here. 198631779

It $\{ \alpha_i \}$ are the simple roots, we consider the following new basis for $\lH$:
\begin{equation}
    H_{\alpha_i}=\frac{ 2\alpha_i^* }{ (\alpha_i,\alpha_i) }
\end{equation}
where $\alpha_i^*$ is the dual of $\alpha_i$ with respect to the inner product on \( \lH^*\), this means
\begin{equation}
    \alpha_j(\alpha_i^*)=(\alpha_i,\alpha_j).
\end{equation}
Since \( \lH\) is abelian (proposition~\ref{PropCartanMaxAnel}), we have
\begin{equation}
    [H_{\alpha_i},H_{\alpha_j}]=0.
\end{equation}
Each root is a combination of the simple roots. If $\beta=\sum_{i=1}^lk_i\alpha_i$, we generalise the definition of $H_{\alpha_i}$ to
\begin{equation}
    H_{\beta}=\frac{ 2\beta^* }{ (\beta,\beta) }=\sum_i k_i\frac{ (\alpha_i,\alpha_i) }{ (\beta,\beta) }H_{\alpha_i}.
\end{equation}
The element $H_{\beta}$ is the \defe{co-weight}{co-weight} associated with the weight $\beta$.

Using the inner product $(.,.)$, we have the decomposition $\beta=\sum_i(\beta,\alpha_i)\alpha_i$ of the roots. An immediate consequence is that
\begin{equation}
    \beta(\alpha_i^*)=(\alpha_i,\beta).
\end{equation}
If $\beta$ is any root, we denote by $\beta_i$ the result of $\beta$ on $H_{\alpha_i}$:
\begin{equation}            \label{EqbetaialphaiH}
    \beta_i=\beta(H_{\alpha_i})=\frac{ 2(\alpha_i,\beta) }{ (\alpha_i,\alpha_i) }.
\end{equation}

\begin{theorem}[Chevalley basis]\index{Chevalley!basis}
    For each root $\beta$, one can found an eigenvector $E_{\beta}$ of $\ad(H_{\beta})$ such that
    \begin{equation}            \label{EqChevalleyBasis}
        \begin{aligned}[]
            [H_{\beta},H_{\gamma}]      &=  0\\
            [E_{\beta},E_{-\beta}]      &=  H_{\beta}\\
            [E_{\beta},E_{\gamma}]      &=
                                \begin{cases}
                                    \pm(p+1)E_{\beta+\gamma}    &\text{if }\beta+\gamma\\
                                    0               &\text{otherwise}\\
                                \end{cases}\\
            [H_{\beta},E_{\gamma}]      &=2\frac{ (\beta,\gamma) }{ (\beta,\beta) }E_{\gamma}
        \end{aligned}
    \end{equation}
    where $p$ is the biggest integer $j$ such that $\gamma+j\beta$ is a root. Moreover, if $\alpha_i$ and $\alpha_j$ are simple roots, the latter becomes
    \begin{equation}
        [H_{\alpha_i},E_{\pm\alpha_j}]=\pm A_{ij}E_{\pm\alpha_j}
    \end{equation}
    where $A$ is the Cartan matrix.
\end{theorem}

An important point to notice is that, for each positive root $\alpha$, the algebra generated by $\{ H_{\alpha},E_{\alpha},E_{-\alpha} \}$ is $\gsl(2)$. This is the reason why the representation theory of $\lG$ reduces to the representation theory of $\gsl(2)$.
