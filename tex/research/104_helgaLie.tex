% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014,2016-2018
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{Representations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

References for Lie algebras and their modules are \cite{SternLieAlgebra,SamelsonNotesLieAlg,DirkEnvFiniteDimNilLieAlg,Wybourne,zelobenko,rncahn,SSLA_Modave2005}.

Since $\lH$ is abelian, the operators $H_{\alpha_j}$ ($j=1,\ldots,l$) are simultaneously diagonalisable. In that basis of the representation space $W$, the basis vectors are denoted by $\ket{u_{\Lambda}}$ and have the property
\begin{equation}
    H_{\alpha_i}\ket{u_{\Lambda}}=\Lambda(H_{\alpha_i})\ket{u_{\Lambda}},
\end{equation}
and, as notation, we note $\Lambda_i=\Lambda(H_{\alpha_i})$. The root $\Lambda$ is a \defe{weight}{weight of a vector} of the vector $\ket{u_{\Lambda}}$. The vector $E_{\beta}\ket{u_{\Lambda}}$ is of weight $\beta+\Lambda$, indeed,
\begin{equation}
        H_{\alpha_i}E_{\beta}\ket{u_{\Lambda}}  =\big( [H_{\alpha_i},E_{\beta}]+E_{\beta}H_{\alpha_i} \big)\ket{u_{\Lambda}}
                            =\left( \frac{ 2(\alpha_i,\beta) }{ (\alpha_i,\alpha_i) }+\Lambda_i \right)E_{\beta}\ket{u_{\Lambda}}.
\end{equation}
Thus the eigenvalue of $E_{\beta}\ket{u_{\Lambda}}$ for $H_{\alpha_i}$ is, according to the relation, \eqref{EqbetaialphaiH},  $\beta(H_{\alpha_i})+\Lambda(H_{\alpha_i})$.

We suppose that the roots $\alpha_i$ are given in increasing order:
\begin{equation}
    \alpha_1\geq\alpha_2\geq\ldots\geq\alpha_l,
\end{equation}
and one says that a weight is \defe{positive}{positive!weight} if its first non vanishing component is positive. Then one choose a basis of $W$
\begin{equation}
    \ket{u_{\Lambda^{(1)}}},\ldots,\ket{u_{\Lambda^{(N)}}}
\end{equation}
of weight vectors. One say that this basis is \defe{canonical}{canonical!basis of a representation} if
\begin{equation}
    \Lambda^{(1)}\geq\ldots\geq\Lambda^{(N)}.
\end{equation}

\begin{theorem}
    A vector if weight $\Lambda$ which is a combination of vectors of weight $\Lambda^{(k)}$ all different of $\Lambda$ vanishes.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
A consequence of that theorem is that, if $W$ is a representation of dimension $N$ of $\lG$, there are at most $N$ different weights. When several vectors have the same weight, the number of linearly independent such vectors is the \defe{multiplicity}{multiplicity of a weight} of the weight. A weight who has only one weight vector is \defe{simple}{simple!weight}.

\begin{proposition}
    The weights $\Lambda$ and $\Lambda-2\alpha(\Lambda,\alpha)/(\alpha,\alpha)$ have the same multiplicity for every root $\alpha$.
\end{proposition}

\begin{theorem}
    Two representation are equivalent when they have the same highest weight.
\end{theorem}

\begin{proposition}     \label{PropMakalMmoinsinten}
For any weight $M$ and root $\alpha$,
\begin{equation}
    \frac{ 2(M,\alpha) }{ (\alpha,\alpha) }\in\eZ,
\end{equation}
and
\begin{equation}
    M-\frac{ 2(M,\alpha) }{ (\alpha,\alpha) }\alpha
\end{equation}
is a weight.
\end{proposition}
Notice, in particular,  that for every weight $M$, the root $-M$ is also a weight.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{List of the weights of a representation}
%---------------------------------------------------------------------------------------------------------------------------

We consider a representation of highest weight $\Lambda$. For each weight $M$, we define
\begin{equation}
    \delta(M)=2\sum_{\alpha_i\in\Pi}M_{\alpha_i}
\end{equation}
where, as usual, $M_{\alpha}=2(M,\alpha)/(\alpha,\alpha)$. For any root $\alpha$, we define
\begin{equation}
    \gamma(\alpha)=\frac{ 1 }{2}\big( \delta(\Lambda)-\delta(\alpha) \big).
\end{equation}
Proposition~\ref{PropMakalMmoinsinten} shows in particular that $\gamma(\alpha)$ is an integer.

\begin{proposition}     \label{PropgammasubMLLweight}
When $M$ is a weight, $\gamma(M)$ is the number of simple roots that have to be subtracted from the highest weight $\Lambda$ in order to get $M$.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

Let us consider the sets
\begin{equation}
    \Delta_{\phi}^k=\{ M\tq\gamma(M)=k \}.
\end{equation}
That set is the \defe{layer}{layer} of order $k$. Of course, there exists a $T(\phi)$ such that
\begin{equation}
    \Delta_{\phi}=\Delta_{\phi}^0\cup\Delta_{\phi}^1\cup\ldots\cup\Delta_{\phi}^{T(\phi)}.
\end{equation}
That $T(\phi)$ is the \defe{height}{height of a representation} of the representation $\phi$. If $\Lambda$ is the highest weight and $\Lambda'$ is the lowest weight, then we have $\gamma(\Lambda)=0$ and $\gamma(\Lambda')=T(\phi)$.

A corollary of proposition~\ref{PropgammasubMLLweight} is that, if $M\in\Delta_{\phi}^r$ and if $\alpha$ is a simple root, then $M+\alpha\in\Delta_{\phi}^{r-1}$, and $M-\alpha\in\Delta_{\phi}^{r+1}$.

Let us denote by $S_k(\phi)$ the multiplicity of the layer of order $k$; we have
\begin{equation}
    S_0+S_1+\cdots+S_T=N,
\end{equation}
where $N$ is the dimension of the representation $\phi$. The number
\begin{equation}
    III(\phi)=\max S_k(\phi)
\end{equation}
is the \defe{width}{width!of a representation} of the representation.

\begin{lemma}
    If $\Lambda$ is the highest weight and $\Lambda'$ is the lowest weight, then $\delta(\Lambda)+\delta(\Lambda')=0$.
\end{lemma}
\begin{proof}
    No proof.
\end{proof}
From that lemma and the definition of $\gamma(M)$, we deduce that $\delta(\Lambda)-\delta(\Lambda')=2\gamma(\Lambda')=T(\phi)$, so that $\delta(\Lambda)=T(\phi)$ and
\begin{equation}
     \delta(M)=T(\phi)-2\gamma(M).
\end{equation}
In particular, $\delta(M)$ has a fixed parity for a given representation $\phi$. It is the \defe{parity}{parity!of a representation} (even or odd) of the representation.

\begin{theorem}     \label{ThoLLralphatablefo}
    If $\Lambda$ is the highest weight of the irreducible representation $\phi$, then
    \begin{equation}
        T(\phi)=\sum_{\alpha_i\in\Pi}r_{\alpha_i}\Lambda_{\alpha}
    \end{equation}
    where the coefficients $r_{\alpha_i}$ only depend on the algebra, and in particular not on the representation.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
The coefficients $r_{\alpha_i}$ are known for all the simple Lie algebra, see for example page 105 of \cite{Wybourne}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Finding all the weights of a representation}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The following can be found in \cite{rncahn,Wybourne}.

\begin{theorem}
    If $\Delta_{\phi}$ is the weight system of the irreducible representation $\phi$, then
    \begin{equation}
        S_k=S_{T-k}
    \end{equation}
    and
    \begin{equation}
        S_r\geq S_{r-1}\geq\ldots\geq S_2\geq S_1
    \end{equation}
    where $r=\frac{ T }{ 2 }+1$.
\end{theorem}
The theorem says that when $T(\phi)$ is even (let us say $T(\phi)=2r$), then $III(\phi)=S_r(\phi)$ and when $T(\phi)$ is odd (let us say $T(\phi)=2r+1$), then
\begin{equation}
    III(\phi)=S_r(\phi)=S_{r+1}(\phi).
\end{equation}

Let $\alpha$ be a root. The \defe{$\alpha$-series}{series of weight}\index{$\alpha$-series of weight} trough the weight $M$ is the sequence of weights
\begin{equation}
    M-r\alpha,\ldots,M+q\alpha
\end{equation}
such that $M-(r+1)\alpha$ and $M+(q+1)\alpha$ do not belong to $\Delta_{\phi}$.

\begin{proposition}     \label{PropweightCondprstring}
Let $M$ be a weight of the representation $\phi$ and $\alpha$, any root of $\lG$. If the $\alpha$-series trough $M$ begins at $M-r\alpha$ and ends at $M+q\alpha$, then
\begin{equation}
    \frac{ 2(M,\alpha) }{ (\alpha,\alpha) }=r-q,
\end{equation}
or, more compactly, $M_{\alpha}+q=r$.
\end{proposition}
Notice that, in that proposition, $q$ and $r$ are well defined functions of $M$ and $\alpha$.

We are now able to determine all the weights of the representation $\phi$. Let us suppose that we already know all the layers $\Delta_{\phi}^0,\ldots,\Delta_{\phi}^{r-1}$. We are going to determine the weights in the layer $\Delta_{\phi}^r$.

An element of $\Delta_{\phi}^r$ has the form $M-\alpha$ with $M\in\Delta_{\phi}^{r-1}$ and $\alpha$, a root. Thus, in order to determine $\Delta_{\phi}^r$, we have to test if $M-\alpha$ is a weight for each choice of $M\in\Delta_{\phi}^{r-1}$ and $\alpha\in\Pi$. Using proposition~\ref{PropweightCondprstring}, if\quext{At page 104 of \cite{Wybourne}, that condition is (I think) wrongly written $M_{\alpha}+q\geq 0$; that mistake is repeated in the example of page 106.}
\begin{equation}
    M_{\alpha}+q \geq 1,
\end{equation}
then $M-\alpha\in\Delta_{\phi}$. The number $M_{\alpha}-q(M,\alpha)$ is the \defe{lucky number}{lucky number} of the root $M-\alpha$. The root is a weight if its lucky number is bigger or equal to $1$. Notice that $q(M,\alpha)$ depends on the representation we are looking at.

Since $M+k\alpha\in\Delta_{\phi}^{r-k}$, the value of $q$ is known when one knows the ``lower'' layers. We are thus able to determine, by induction, all the layers from $\Delta^0_{\phi}$ which only contains the highest weight. For this one, by definition, we always have $q=0$.

The Dynkin coefficients of one weights can be more easily computed using the following formula, which is a direct consequence of definition of the Cartan matrix:
\begin{equation}        \label{EqCoefDynkMalpha}
    (M-\alpha_j)_i=M_i-A_{ji}.
\end{equation}

As example, let us determine the weights of the representation \input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks} of \( \gsu(3)\).
%\input{image_su3Dynkin(-1).pstricks}
%The result is on figure~\ref{LabelFigDynkinpWjUbE}. % From file DynkinpWjUbE
%\newcommand{\CaptionFigDynkinpWjUbE}{<+Type your caption here+>}
The algebra $\gsu(3)$ has two simple roots $\alpha$ and $\beta$ whose inner products are $(\alpha,\alpha)=(\beta,\beta)=1$ and $(\alpha,\beta)=-1/2$. The highest weight of $\phi=$
\input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}
is $\Lambda=(\alpha+2\beta)/3$.

We first test if $\Lambda-\alpha$ is a weight. Easy computations show that  $\Lambda_{\alpha}=0$ wile $q=0$; thus $\Lambda-\alpha$ is not a weight. The same kind of computations show that $\Lambda_{\beta}=1$, so that $\Lambda_{\beta}=q(\Lambda,\beta)=1$. That shows that $\Delta_{\phi}^1=\{ \Lambda-\alpha \}$.

Let now $M=\Lambda-\beta=(\alpha-\beta)/3$. Since $M+\alpha\notin\Delta_{\phi}$, we have $q(M,\alpha)=0$. On the other hand, $M_{\alpha}=1$, so that $M-\alpha\in\Delta_{\phi}^2$. The last one to have to be tested is $M-\beta$. Since $M+\beta=\Lambda$, we have $q(M,\beta)=1$, but $M_{\beta}=-1$. Thus $M_{\beta}+q(M,\beta)=0$ and $M-\beta$ is not a weight.

We can obviously continue in that way up to find $\Delta_{\phi}^r=0$, but there is an escape to be more rapid. Indeed, using theorem~\ref{ThoLLralphatablefo} with coefficients $r_{\alpha}$ that can be found in tables (for example in \cite{Wybourne}), we find
\begin{equation}
    T(\phi)=2\Lambda_{\alpha}+3\Lambda_{\beta}=2,
\end{equation}
thus we immediately know that $\Delta^3_{\phi}$ does not exist.

On the other hand, one knows the width $III(\phi)=\max S_k(\phi)$ because (since $T(\phi)=2r$, with $r=1$), we have $III(\phi)=S_1(\phi)$. Thus, once $\Delta^1(\phi)$ is determined, we know that the next ones will never have more elements.

In the example, when we know that $M-\alpha$ is a weight, we do not have to test $M-\beta$.

\label{LeTravail}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Tensor product of representations}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Tensor and weight}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Let $\phi$ and $\phi'$ be representations of $\lG$ on the vector spaces $R$ and $R'$ of dimensions $n$ and $m$. If $A\in\eM_n(R)$ and $B\in\eM_m(R')$, the \defe{tensor product}{tensor product!of matrices}, also know as the \defe{Kronecker product}{Kronecker product} of $A$ and $B$ is the matrix $A\otimes B\in\eM_{mn}(R\otimes R')$ whose elements are given by
\begin{equation}
    C_{ik,jl}=A_{ij}B_{kl}.
\end{equation}
The principal properties of that product are
\begin{subequations}
    \begin{align}
        (A_1A_2)\otimes(B_1B_2)&=(A_1\otimes B_1)(A_2\otimes B_2)\\
        (A\otimes B)^{-1}   &= A^{-1}\otimes B^{-1}\\
        \mtu_{R}\otimes\mtu_{R'}&=\mtu_{R\otimes R'}
    \end{align}
\end{subequations}
If $\varphi_1$ and $\varphi_2$ are two representations of a group $G$, the \defe{tensor product}{tensor product!of group representations} is defined by
\begin{equation}
    (\varphi_1\otimes\varphi_2)(g)=\varphi_1(g)\otimes\varphi_2(g).
\end{equation}
If $\phi$ and $\phi'$ are two representations of a Lie algebra $\lG$, the \defe{tensor product}{tensor product!of Lie algebra representations} representation is defined by
\begin{equation}
    (\phi\otimes\phi')(X)(v\otimes v')=\big( \phi(X)v\big)\otimes v'+v\otimes\big( \phi'(X)v' \big).
\end{equation}
If $\{ \phi_k \}$ are the irreducible representations, a natural question that arise is to determine the coefficients $\Gamma$ which decompose $\phi\otimes\phi'$ into irreducible representations:
\begin{equation}
    \phi\otimes\phi'=\sum_k\Gamma_k(\phi,\phi')\phi_k
\end{equation}

Let $W$ and $W'$ be the representation spaces and consider the following decompositions in weight spaces:
\begin{align}
    W&=\bigoplus_{\Lambda\in\Delta_1}W_{\Lambda},&      W'&=\bigoplus_{\Lambda\in\Delta_2}W'_{\Lambda}.
\end{align}
By definition,
\begin{equation}
    (W\otimes W')_{\alpha}=\{ v\otimes v'\tq (\phi\otimes\phi')(h)(v\otimes v')=\alpha(h)(v\otimes v') \}.
\end{equation}
If $\big( \phi(h)v \big)\otimes v'+v\otimes\big( \phi'(h)v' \big)$ is a multiple of $v\otimes v'$, one requires that
\begin{subequations}
    \begin{align}
        \phi(h)v    &=\alpha_1(h)v,\\
        \phi'(h)v   &=\alpha_2(h)v'
    \end{align}
\end{subequations}
for the weights $\alpha_1$ and $\alpha_2$ of $\phi$ and $\phi'$. Thus we have
\begin{equation}
    (W\otimes W')_{\alpha_1+\alpha_2}=W_{\alpha_1}\otimes W_{\alpha_2}.
\end{equation}

We have in particular that the simple root system $\Delta_{\phi\otimes\phi'}$ of the representation $\phi\otimes\phi'$ is given by
\begin{equation}        \label{EqDeldelDElphitens}
    \Delta_{\phi\otimes\phi'}= \Delta_{\phi}+\Delta_{\phi'}.
\end{equation}

What we proved is\quext{The second part is not proved.}
\begin{proposition} \label{Propphihwrepplullllam}
    If $\phi$ is a representation of highest weight $\Lambda$ and $\phi'$ is a representation of highest weight $\Lambda'$, then $\phi\otimes\phi'$ is a representation of height weight $\Lambda+\Lambda'$.

    If, moreover, $\phi$ and $\phi'$ are irreducible, then $\phi\otimes\phi'$ is irreducible.
\end{proposition}

An irreducible representation that cannot be written under the form of a tensor product of irreducible representations is a \defe{basic representation}{basic!representation}.

\begin{lemma}
    A representation is basic if and only if its highest weight $\Lambda$ is such that the $\Lambda_{\alpha_i}$ are all zero but one which is $1$.
\end{lemma}
The basic representations of $\so(10)$ are given by the Dynkin diagrams of figure~\ref{LabelFigDynkinNUtPJx}. All the irreducible representations are obtained by tensor products of the basic ones. An \defe{elementary}{elementary!representation} is a basic representation which has his ``1'' on a terminal point of the Dynkin diagram.
\newcommand{\CaptionFigDynkinNUtPJx}{Basic representations of $\so(10)$}
\input{auto/pictures_tex/Fig_DynkinNUtPJx.pstricks}
%L'autre truc est~\ref{LabelFigDynkinNUtPJx}. % From file DynkinNUtPJx

% La ligne suivante était l'ancien fichier contenant la figure.
%\input{fig_basic_sodix.pstricks}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Decomposition of tensor products of representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


%This is the picture of
%                    1
%                    o-----o
%i.e. the old picture image_su3Dynkin(1-).pstricks
%The result is on figure~\ref{LabelFigDynkinrjbHIu}. % From file DynkinrjbHIu
%\newcommand{\CaptionFigDynkinrjbHIu}{<+Type your caption here+>}
%\input{auto/pictures_tex/Fig_DynkinrjbHIu.pstricks}

% This is the picture of
%                          1
%                    o-----o
%The result is on figure~\ref{LabelFigDynkinpWjUbE}. % From file DynkinpWjUbE
%\newcommand{\CaptionFigDynkinpWjUbE}{<+Type your caption here+>}
%\input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}

%This is the picture of
%                          2
%                    o-----o
%i.e. the old picture \input{image_su3Dynkin(-2)
%The result is on figure~\ref{LabelFigDynkinqlgIQl}. % From file DynkinqlgIQl
%\newcommand{\CaptionFigDynkinqlgIQl}{<+Type your caption here+>}
%\input{auto/pictures_tex/Fig_DynkinqlgIQl.pstricks}


Proposition~\ref{Propphihwrepplullllam} allows us to decompose a tensor product of representations into irreducible representations. Let us do it on a simple example in $\gsu(3)$. We consider the representations $\phi=$\input{auto/pictures_tex/Fig_DynkinrjbHIu.pstricks} and $\phi'$=\input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}. The first representation has weights
\begin{equation}
    \Delta_{\phi}=\left\{ \frac{ \alpha+2\beta }{ 3 },\frac{ \alpha-\beta }{ 3 },\frac{ -(2\alpha+\beta) }{ 3 } \right\},
\end{equation}
and the second one has
\begin{equation}
    \Delta_{\phi'}=\left\{ \frac{ \alpha+2\beta }{ 3 },\frac{ \alpha-\beta }{ 3 },\frac{ -(2\alpha+\beta) }{ 3 } \right\}.
\end{equation}

According to equation \eqref{EqDeldelDElphitens}, we have $9$ weights in the representation $\phi\otimes\phi'$ (all the sums of one element of $\Delta_{\phi}$ with a one of $\Delta_{\phi'}$). The highest one is
\[
    \frac{ 2\alpha+4\beta }{ 3 },
\]
which is the double of the highest weight in \input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}, so $\phi\otimes\phi'$ contains the representation \input{auto/pictures_tex/Fig_DynkinqlgIQl.pstricks}. Now, we remove from the list of weights of $\phi\otimes\phi'$ the list of weight of \input{auto/pictures_tex/Fig_DynkinqlgIQl.pstricks}; the result is
\begin{equation}
    \frac{ 2\alpha+\beta }{ 3 },\frac{ -(\alpha-\beta) }{ 3 },\frac{ -(\alpha+2\beta) }{ 3 },
\end{equation}
which are the weights of \input{auto/pictures_tex/Fig_DynkinrjbHIu.pstricks}. The conclusion is that\quext{I guess the following line is a typo and should be 1--o times o--1.}
\begin{equation}
    \text{\input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}}\otimes\text{\input{auto/pictures_tex/Fig_DynkinpWjUbE.pstricks}}=\text{\input{auto/pictures_tex/Fig_DynkinqlgIQl.pstricks}}\oplus\text{\input{auto/pictures_tex/Fig_DynkinrjbHIu.pstricks}}.
\end{equation}
That procedure of decomposition is quite long because it requires to compute the complete set of weights for some intermediate representations.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Symmetrization and anti symmetrization}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

                    Let $\phi$ be a irreducible representation. We want to compute the symmetric and antisymmetric parts of the representation $\phi^{\otimes k}=\underbrace{\phi\otimes\ldots\otimes\phi}_{k \text{ times }}$. These symmetric and antisymmetric parts are denoted by $\phi^{\otimes k}_s$ and $\phi^{\otimes k}_a$ respectively.

\begin{proposition}
    If $\{ \xi_1,\ldots,\xi_N \}$ is a canonical basis of $\phi$ and if we denote by $\Lambda_i$ the weight of the vector $\xi_i$, the followings hold:
    \begin{enumerate}
        \item the weight system of $\phi^{\otimes k}_a$ is
            \begin{equation}
                \Lambda_{i_1}+\Lambda_{i_2}+\cdots+\Lambda_{i_k}
            \end{equation}
            with $i_k>\ldots>i_2>i_1$, and the highest weight is
            \begin{equation}
                \Lambda_1+\cdots+\Lambda_k.
            \end{equation}
            The dimension of the representation $\phi^{\otimes k}_a$ is
            \begin{equation}
                N\big( \phi^{\otimes k}_a \big)= {n\choose k}.
            \end{equation}

        \item The weight system of the representation $\phi^{\otimes k}_s$ is
            \begin{equation}
                \Lambda_{i_1}+\Lambda_{i_2}+\cdots+\Lambda_{i_k}
            \end{equation}
            with $i_k\geq \ldots\geq i_2\geq i_1$, and the highest weight is
            \begin{equation}
                k\Lambda_1
            \end{equation}
            The dimension of the representation $\phi^{\otimes k}_s$ is
            \begin{equation}
                N\big( \phi^{\otimes k}_s \big)= {n+k\choose k}.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    No proof.
\end{proof}
The representations $\phi^{\otimes k}_a$ and $\phi^{\otimes k}_s$ might be decomposable and we denote by $\phi^{\otimes k}_{s>}$ and $\phi^{\otimes k}_{a>}$ their highest weight parts.


Let $\alpha$ be a terminal point in a Dynkin diagram. The \defe{branch}{branch!in a Dynkin diagram} of $\alpha$ is the sequence of points of the Dynkin diagram $\alpha=\alpha_1,\alpha_2,\ldots,\alpha_k$ defined by the following properties.
\begin{itemize}
    \item The point $\alpha_i$ is connected with (and only with) the points $\alpha_{i-1}$ and $\alpha_{i+1}$,
    \item the connexion between $\alpha_i$ and $\alpha_{i+1}$ is of one of the following forms
        \begin{subequations}
            \begin{align}
            \input{auto/pictures_tex/Fig_ADUGmRRA.pstricks}\\
   \input{auto/pictures_tex/Fig_ADUGmRRB.pstricks}\\
   \input{auto/pictures_tex/Fig_ADUGmRRC.pstricks}
            \end{align}
        \end{subequations}
    \item the sequence $\alpha_1,\ldots,\alpha_k$ is maximal in the sense that no $\alpha_{k+1}$ can be added without violating one of the two first rules.
\end{itemize}

\begin{proposition}
    Let $\alpha$ be a terminal point in a Dynkin diagram and $\alpha_1,\ldots,\alpha_k$ be the corresponding branch. Then we have
    \begin{equation}
        \phi_{\alpha_r}\simeq \phi^{\otimes r}_{\alpha\,a>}
    \end{equation}
    for every $r=1,2,\ldots,k$.
\end{proposition}



\section{Semi-direct product}
%++++++++++++++++++++++++++++

\subsection{From Lie algebra point of view}\label{subsec:semi_Lie}
%----------------------------------------

Here, the matter comes from \cite{Knapp,newfromold}. When $\mfa$ and $\mfb$ are Lie algebras, one can consider $\mfg=\mfa\oplus\mfb$ as vector space, and define a Lie algebra structure on $\mfg$ by
\[
     [ (a,b),(a',b') ]=( [a,a'],[b,b'] ).
\]
This is the \defe{direct sum}{direct!sum of Lie algebras} of $\mfa$ and $\mfb$.

An endomorphism $\mD$ of the Lie algebra $\mfa$ is a \defe{derivation}{derivation!of a Lie algebra} when
\[
   \mD[X,Y]=[\mD X,Y]+[X,\mD Y].
\]
The set of the derivations of $\mfa$ is written $\Der\mfa$.

\begin{proposition}\label{prop:Lie_derr}
    Let $\mfa$ be a Lie algebra
    \begin{enumerate}
        \item $\Der\mfa$ is a Lie algebra for the usual commutator,
        \item $\dpt{\ad}{\mfa}{\Der\mfa\subseteq\End\mfa}$ is a Lie algebra homomorphism.
    \end{enumerate}
\end{proposition}

\begin{proof}
    For the first statement, we just have to compute to see that if $\mD,\mE\in\Der\mfa$,
    \[
        [\mD,\mE][X,Y]=(\mD\mE-\mE\mD)[X,Y]=[ [\mD,\mE]X,Y ]+[ X,[\mD,\mE]Y ].
    \]

    The second comes from the fact that $\ad X\in\Der\mfa$ for any $X\in\mfa$ and
    $\ad[X,Y]=\ad X\ad Y-\ad Y\ad X$.
\end{proof}

Let us now consider the vector space direct sum $\mfg=\mfa\oplus\mfb$. Let us suppose moreover that $\mfa$ is a Lie subalgebra of $\mfg$ and that $\mfb$ is an idea in $\mfg$. So we have that
\[
   \ad|_{\mfb}\in\Der\mfb.
\]
By proposition~\ref{prop:Lie_derr}, we have a homomorphism $\dpt{\pi}{\mfa}{\Der\mfb}$, $\pi(A)=\ad A|_{\mfb}$. So if $A\in\mfa$ and $B\in\mfb$, $[A,B]=\pi(A)B$. The conclusion is that the Lie algebra structure of $\mfg$ is given by $\mfa$, $\mfb$ and $\pi$. In this case, we write   $\mfg=\mfa\oplus_{\pi}\mfb$,
and we say that $\mfg$ is the semidirect product of $\mfa$ and $\mfb$. The following theorem gives the general definition of semidirect product.

\begin{theorem}
    Let $\mfa$ and $\mfb$ be two Lie algebras, and $\dpt{\pi}{\mfa}{\Der\mfb}$, a Lie algebra homomorphism. There exists an unique Lie algebra structure on the vector space $\mfg=\mfa\oplus\mfb$ such that
    \begin{itemize}
    \item the commutators on $\mfa$ and $\mfb$ are the old ones,
    \item $[A,B]=\pi(A)B$ for any $A\in\mfa$ and $B\in\mfb$.
    \end{itemize}
    In this case, in the so defined Lie algebra $\mfg$, $\mfa$ is a subalgebra and $\mfb$ is an ideal.
\end{theorem}

The vector space $\mfg=\mfa\oplus\mfb$ endowed with this Lie algebra structure is the
\defe{semidirect product}{semi-direct product!of Lie algebras} of $\mfa$ and $\mfb$, it is denoted by
\[
  \mfg=\mfa\oplus_{\pi}\mfb
\]
One also often speak about \defe{split extension}{split!extension} of $\lA$ by $\lB$, with the splitting map $\pi$.

\begin{proof}
    The unicity part is clear: the Lie algebra structure is completely defined by the two conditions and the condition of antisymmetry. The matter is just to see that this structure is a Lie algebra structure: we have to check Jacobi. If in $[[X,Y],Z]$, $X,Y,Z$ are all three in $\mfa$ or $\mfb$, it is trivial. The two other cases are:
    \begin{itemize}
    \item $X$, $Y\in\mfa$ and $Z\in\mfb$. In this case, we use $\pi([X,Y])=\pi(X)\pi(Y)-\pi(Y)\pi(x)$ (because $\pi$ is a Lie algebra homomorphism) to find
    \[
    [[X,Y],Z]=\pi([X,Y])Z=-[[Y,Z],X]-[[Z,X],Y].
    \]

    \item The second case is $X$, $Y\in\mfb$ and $Z\in\mfa$. Here, we use the fact that $\pi(Z)$ is a derivation of $\mfb$. The computation is also direct.
    \end{itemize}

    It is clear that $\mfb$ is an ideal because for any $A\in\mfa$ and $B\in\mfb$, $[B,A]=-[A,B]=-\pi(A)B\in\mfb$.

\end{proof}

The theory of split extension is often used in the following sense. We have a Lie algebra $\mfg$ which decomposes (as vector space) into a direct sum $\mfa\oplus\mfb$. If in $\mfg$ the map $a\mapsto \ad(a)$ is an action of $\mfa$ on $\mfb$, we say that $\mfg$ is a split extension
\[
  \mfg=\mfa\oplus_{\ad}\mfb.
\]
This way to use split extensions is used for example in the proof of proposition~\ref{PropRsurSglobgroup}.


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Example: extensions of the Heisenberg algebra}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let $\pH(V,\Omega)=V\oplus\eR E$ be the Heisenberg algebra. A derivation is a map $D\colon \pH \to \pH$ such that
\begin{equation}        \label{EqderCOndHeis}
    D[X,Y]=[DX,YT]+[X,DY].
\end{equation}
Let us look at the derivations under the form
\begin{equation}
    D=\begin{pmatrix}
        X   &   v   \\
        \xi &   a
    \end{pmatrix}
\end{equation}
where $a\in\eR$, $X\in\End(V)$, $v\in V$ and $\xi\in V^*$. The left hand side of the condition \eqref{EqderCOndHeis} reads
\begin{equation}    \label{EqderCOndHeisA}
    D[w+zE,w'+z'E]=D\big( \Omega(w,w')E \big)=\Omega(w,w')(v+aE).
\end{equation}
Now, using $Dw=Xw+\xi(w)E$ and $D(zE)=v+aE$, the right hand side is
\begin{equation}    \label{EqderCOndHeisB}
    \big( \Omega(Xw,v')+\Omega(zv,v')+\Omega(w,Xw')+\Omega(w,z'v) \big)E.
\end{equation}
Equating \eqref{EqderCOndHeisA} and \eqref{EqderCOndHeisB} we find $v=0$ and
\begin{equation}        \label{EqPourEtreDerHein}
    \Omega(Xw,w')+\Omega(w,Xw')=a\Omega(w,w').
\end{equation}
If we write it as matrices, we find
\begin{equation}
    X^t\Omega+\Omega X=a\Omega.
\end{equation}

The derivations with $a=0$ form the algebra
\begin{equation}
    \Der(\pH)_0=\gsp(\Omega,V)\times V^*.
\end{equation}

If $a\neq 0$, we find the symplectic conform group
\begin{equation}
    \Conf(V,\Omega)=\{ A\colon V\to V\tq \Omega(Av,Aw)=\lambda\Omega(v,w)\text{ with }\lambda\in\eR_0^+ \}.
\end{equation}
Taking the derivative of the group condition, we find
\begin{equation}
    \Dsdd{ \Omega\big( A(t)v,A(t)w \big) }{t}{0}=\Dsdd{ \lambda(t)\Omega(v,w) }{t}{0},
\end{equation}
which produces the condition \eqref{EqPourEtreDerHein} with $X=\dot A$ and $a=\dot \lambda$.

\begin{enumerate}

    \item
        If $X=\id$ and $\xi=0$, then we must have $a=2$ and we have the derivation
        \begin{equation}
            H=\id|_V\oplus 2\id|_{\eR E}.
        \end{equation}
    \item
        If $\xi=0$, $a=0$ and $X$ if exchange the Lagrangian in the decomposition $V=W\oplus \bar W$.
\end{enumerate}

\subsection{Group algebra}
%-------------------------

Let $\mA$ and $\mB$ be abelian algebras and $\dpt{\rho}{\mA}{\Der\mB}$ be a homomorphism. We want to put a group structure on the set $\mA\times\mB$ in such a way that the Lie algebra of $\mA\times\mB$ has Lie bracket given by
\begin{equation}
\big[ (A+B),(A'+B')  \big]=[A,B']+[B,A']=\rho(A)B'-\rho(A')B.
\end{equation}
We claim that the group law should be
\begin{equation}
(a,b)(a',b')=(a+a',e^{\rho(a)}b'+b)
\end{equation}
whose inverse is
\begin{equation}
(a,b)^{-1}=(-a,-e^{-\rho(a)b})
\end{equation}
Indeed, the general form of the commutator is
\[
  [X,Y]=\DDsdd{ \AD(X(t))Y(s) }{t}{0}{s}{0}
\]
with respect to the group law. A path in $\mA\times\mB$ with tangent vector $(a,b)$ is $(at,bt)$. Then
\begin{equation}
\begin{split}
\big[ (a,b),(a',b') \big]&=\DDsdd{ (at,bt)(a's,b's)(at,bt)^{-1} }{t}{0}{s}{0}\\
            &=\big(0,-\rho(a)b+\rho(a)b'\big).
\end{split}
\end{equation}
