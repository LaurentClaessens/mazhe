% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014, 2019-2020, 2022-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{Explicit choices in \texorpdfstring{$\so(2,l-1)$}{so2l-1}}
%\label{app_calc}
%+++++++++++++++++++++++++++++++++++++++++++

In the case of $AdS_4$ the matrices are $5\times 5$. We will write them down, but the general form are entirely similar. Our choice of Iwasawa decomposition is
\begin{subequations}
	\begin{align}
		\sN & =\{W_i,V_j,M,L\} \\
		\sA & =\{ J_1, J_2\}.
	\end{align}
\end{subequations}


The basis of $\sodn$ in which we want to decompose all our elements is the root space one:
\begin{equation}
	\sG=\Span\{J_1,q_1,X,Y,V,W,M,N,F,L\}
\end{equation}
note in particular that $\sG_{(0,0)}=\Span\{J_1,q_1\}$ and $W,J_1\in\sH$.
\begin{equation}
	\frac{1}{2}(W-Y)=
	\begin{pmatrix}
		  & 0             \\
		0 & 0 & 0 & 0 & 1 \\
		  & 0             \\
		  & 0             \\
		  & 1
	\end{pmatrix},
	\qquad
	\frac{1}{2}(V+X)=
	\begin{pmatrix}
		  &   &    &   & 0 \\
		  &   &    &   & 0 \\
		  &   &    &   & 1 \\
		  &   &    &   & 0 \\
		0 & 0 & -1 & 0 & 0
	\end{pmatrix},
\end{equation}



\begin{equation}
	\frac{1}{2}(W+Y)=
	\begin{pmatrix}
		  &   &   &    & 0 \\
		  &   &   &    & 0 \\
		  &   &   &    & 0 \\
		  &   &   &    & 1 \\
		0 & 0 & 0 & -1 & 0
	\end{pmatrix},
	\qquad
	q_3=\frac{1}{2}(V-X)=
	\begin{pmatrix}
		0 & 0 & 0 & 0 & 1 \\
		0                 \\
		0                 \\
		0                 \\
		1
	\end{pmatrix}.
\end{equation}


\subsection{Decompositions and commutators for \texorpdfstring{$\sQ$}{Q}}
%-------------------------------------------------------------------------

First, the root space decomposition of the basis $\{q_i\}$ of $\sQ$:
\begin{subequations}
	\begin{align}
		q_0 & =\frac{1}{ 4 }(M+N+L+F)                            & q_2 & =\frac{1}{ 4 }(N+F-M-L)                            \\
		    & =\frac{1}{ 4 }(X_{11}+X_{1,-1}+X_{-1,1}+X_{-1,-1}) &     & =\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1}) \\
		q_1 & =q_1=J_2                                           & q_3 & =\frac{1}{2}(V-X)                                  \\
		    &                                                    &     & =\frac{ 1 }{2}(X_{01}-X_{0,-1}).
	\end{align}
\end{subequations}
The commutators:
\begin{subequations}
	\begin{align}
		[q_0,q_1] & =\us{4}(L+F-M-N)                                   & [q_1,q_2] & =\us{4}(L+N-F-M)                                  \\\
		          & =\frac{1}{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1}) &           & =\frac{1}{ 4 }(X_{1,-1}+X_{-1,1}-X_{-1,1}-X_{11}) \\
		[q_0,q_2] & =-J_1                                              & [q_1,q_3] & =\frac{1}{2}(V+X)                                 \\
		[q_0,q_3] & =\frac{1}{2}(Y-W)                                  & [q_2,q_3] & =\frac{1}{2}(W+Y)
	\end{align}
\end{subequations}

\subsection{Commutators between root spaces and \texorpdfstring{$\sQ$}{Q}}

\begin{subequations}
	\begin{align}
		[q_0,J_1] & =\frac{1}{4}(N+F-M-L)                                & [q_1,J_1] & =0  & [q_2,J_1] & =q_0               \\
		          & =\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})                                                      \\
		          & =q_2                                                                                                    \\
		[q_0,q_1] & =\frac{1}{4}(L+F-M-N)                                &           &     & [q_2,q_1] & =\us{4}(F+M-L-N)   \\
		          & =\frac{ 1 }{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})                                                    \\
		[q_0,X]   & =\frac{1}{2}(W-Y)                                    & [q_1,X]   & =-X & [q_2,X]   & =-\frac{1}{2}(W+Y) \\
		[q_0,Y]   & =\frac{1}{2}(X-V)                                    & [q_1,Y]   & =0  & [q_2,Y]   & =\frac{1}{2}(V-X)  \\
		[q_0,V]   & =\frac{1}{2}(Y-W)                                    & [q_1,V]   & =V  & [q_2,V]   & =\frac{1}{2}(W+Y)  \\
		[q_0,W]   & =\frac{1}{2}(V-X)                                    & [q_1,W]   & =0  & [q_2,W]   & =\frac{1}{2}(V+X)  \\
		[q_0,M]   & =q_1+J_1                                             & [q_1,M]   & =M  & [q_2,M]   & =q_1+J_1           \\
		[q_0,N]   & =q_1-J_1                                             & [q_1,N]   & =N  & [q_2,N]   & =-q_1+J_1          \\
		[q_0,L]   & =-q_1+J_1                                            & [q_1,L]   & =-L & [q_2,L]   & =-q_1+J_1          \\
		[q_0,F]   & =-q_1-J_1                                            & [q_1,F]   & =-F & [q_2,F]   & =q_1+J_1
	\end{align}
\end{subequations}

\begin{subequations}
	\begin{align}
		[q_3,J_1] & =0                 & [q_3,M] & =W  & [q_3,V] & =-q_1              \\
		[q_3,q_1] & =-\frac{1}{2}(V+X) & [q_3,N] & =-Y & [q_3,W] & =\frac{1}{2}(M+L)  \\
		          &                    & [q_3,L] & =W  & [q_3,X] & =-q_1              \\
		          &                    & [q_3,F] & =-Y & [q_3,Y] & =-\frac{1}{2}(N+F)
	\end{align}
\end{subequations}

\subsection{Commutators in the root spaces}

\begin{subequations}
	\begin{align}
		[J_1,q_1] & =0                                  \\
		[J_1,X]   & =0  & [q_1,X] & =-X                 \\
		[J_1,Y]   & =-Y & [q_1,Y] & =0  & [X,Y] & =F    \\
		[J_1,V]   & =0  & [q_1,V] & =V  & [X,V] & =2q_1 \\
		[J_1,W]   & =W  & [q_1,W] & =0  & [X,W] & =-L   \\
		[J_1,M]   & =M  & [q_1,M] & =M  & [X,M] & =-2W  \\
		[J_1,N]   & =-N & [q_1,N] & =N  & [X,N] & =2Y   \\
		[J_1,F]   & =-F & [q_1,F] & =-F & [X,F] & =0    \\
		[J_1,L]   & =L  & [q_1,L] & =-L & [X,L] & =0
	\end{align}
\end{subequations}

\begin{subequations}
	\begin{align}
		[V,W] & =M                                        \\
		[V,M] & =0   & [W,M] & =0                         \\
		[V,N] & =0   & [W,N] & =-2V & [M,N] & =0          \\
		[V,F] & =-2Y & [W,F] & =2X  & [M,F] & =-4q_1-4J_1 \\
		[V,L] & =2W  & [W,L] & =0   & [M,L] & =0
	\end{align}
\end{subequations}


\begin{subequations}
	\begin{align}
		[N,F] & =0                       \\
		[N,L] & =-4q_1+4J_1 & [F,L] & =0
	\end{align}
\end{subequations}

\subsection{Killing form}
%++++++++++++++++++++
The adopted definition is $B(x,y)=\tr(\ad x\circ\ad y)$ with no one half or such coefficient.
\begin{equation}
	\begin{aligned}
		B(J_1,q_1) & =0   & B(V,X) & =-12 \\
		B(J_1,J_1) & =6   & B(N,L) & =-24 \\
		B(W,Y)     & =-12 & B(M,F) & =-24
	\end{aligned}
\end{equation}
Some easy computations show that for $g\in \SO(2)$,
\[
	\begin{split}
		dL_gq_0&=
		\begin{pmatrix}
			-\sin u & \cos u \\
			-\cos u & \sin u
		\end{pmatrix},
		\quad
		dL_g q_1=
		\begin{pmatrix}
			0 & 0 & \cos u  \\
			0 &   & -\sin u \\
			1
		\end{pmatrix}\\
		dL_g H_1&=
		\begin{pmatrix}
			0 & 0 & \sin u \\
			0 &   & \cos u \\
			0 & 1
		\end{pmatrix}\\
		dR_g J_1&=
		\begin{pmatrix}
			0                        \\
			0       & 0      & 0 & 1 \\
			0                        \\
			-\sin u & \cos u
		\end{pmatrix},
		\quad
		dR_g J_2=
		\begin{pmatrix}
			0      & 0      & 1 \\
			0                   \\
			\cos u & \sin u
		\end{pmatrix}
	\end{split}
\]
So
\begin{subequations}
	\begin{align}
		dR_g J_1 & =-\sin u\, dL_g q_2+\cos u\, dL_g H_2 \\
		dR_g J_2 & =\sin u\, dL_g H_1+\cos u\, dL_g q_1.
	\end{align}
\end{subequations}
and
\begin{subequations}
	\begin{align}
		B_{[g]}(J_1^*,J_1^*) & =6\sin^2 u  \\
		B_{[g]}(J_2^*,J_2^*) & =6\cos^2 u.
	\end{align}
\end{subequations}


\section{Iwasawa decomposition for \texorpdfstring{$\gsl(2,\eC)$}{sl2C}}		\label{SecIwasldeuxC}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{Iwasawa!decomposition!of $\SL(2,\eC)$}

Matrices of $\gsl(2,\eC)$ are acting on $\eC^2$ as
\[
	\begin{split}
		\begin{pmatrix}
			\alpha & \beta \\\gamma&-\alpha
		\end{pmatrix}&
		\begin{pmatrix}
			a+bi \\c+di
		\end{pmatrix}\\
		&=
		\begin{pmatrix}
			(\alpha_1a-\alpha_2b+\beta_1c-\beta_2d)+i(\alpha_2a+\alpha_1b+\beta_2c+\beta_1d) \\
			(\gamma_1a-\gamma_2b-\alpha_1c+\alpha_2d)+i(\gamma_2a+\gamma_1b-\alpha_2c-\alpha_1d)
		\end{pmatrix}
	\end{split}
\]
if $\alpha=\alpha_1+i\alpha_2$.  Our aim is to embed $\SL(2,\eC)$ in $\SP(2,\eR)$ (see sections~\ref{SecSympleGp} and~\ref{SecDirADs}), so that we want a four dimensional realization of $\gsl(2,\eC)$. It is easy to rewrite the previous action under the form of $\begin{pmatrix}
		\alpha & \beta \\\gamma&-\alpha
	\end{pmatrix}$ acting of the vertical four component vector $(a,b,c,d)$. The result is that a general matrix of $\gsl(2,\eC)$ reads
\begin{equation}		\label{EqGenslMatr}
	\gsl(2,\eC)\leadsto
	\begin{pmatrix}
		\boxed{
			\begin{array}{cc}
				\alpha_1 & -\alpha_2 \\
				\alpha_2 & \alpha_1
			\end{array}
		}                       &
		\begin{array}{cc}
			\beta_1 & -\beta_2 \\
			\beta_2 & \beta_1
		\end{array}        \\
		\begin{array}{cc}
			\gamma_1 & -\gamma_2 \\
			\gamma_2 & \gamma_1
		\end{array} &
		\boxed{
			\begin{array}{cc}
				-\alpha_1 & \alpha_2  \\
				-\alpha_2 & -\alpha_1
			\end{array}
		}
	\end{pmatrix}.
\end{equation}
The boxes are drawn for visual convenience.  Using the Cartan involution $\theta(X)=-X^t$, we find the following Cartan decomposition:
\begin{equation}
	\begin{split}
		\iK_{\gsl(2,\eC)}&\leadsto
		\begin{pmatrix}
			\boxed{
				\begin{array}{cc}
					0        & -\alpha_2 \\
					\alpha_2 & 0
				\end{array}
			}                      &
			\begin{array}{cc}
				\beta_1 & -\beta_2 \\
				\beta_2 & \beta_1
			\end{array}       \\
			\begin{array}{cc}
				-\beta_1 & -\beta_2 \\
				\beta_2  & -\beta_1
			\end{array} &
			\boxed{
				\begin{array}{cc}
					0         & \alpha_2 \\
					-\alpha_2 & 0
				\end{array}
			}
		\end{pmatrix},\\
		\iP_{\gsl(2,\eC)}&\leadsto
		\begin{pmatrix}
			\boxed{
				\begin{array}{cc}
					\alpha_1 & 0        \\
					0        & \alpha_1
				\end{array}
			}                      &
			\begin{array}{cc}
				\beta_1 & -\beta_2 \\
				\beta_2 & \beta_1
			\end{array}       \\
			\begin{array}{cc}
				-\beta_1 & -\beta_2 \\
				\beta_2  & -\beta_1
			\end{array} &
			\boxed{
				\begin{array}{cc}
					0         & \alpha_2 \\
					-\alpha_2 & 0
				\end{array}
			}
		\end{pmatrix}.
	\end{split}
\end{equation}
We have $\dim\iP_{\gsl(2,\eC)}=3$ and $\dim\iP_{\gsl(2,\eC)}=3$. A maximal abelian subalgebra of $\iP_{\gsl(2,\eC)}$ is the one dimensional algebra generated by
\[
	A_1=
	\begin{pmatrix}
		1 \\&1\\&&-1\\&&&-1
	\end{pmatrix}.
\]
The corresponding root spaces are
\begin{itemize}
	\item $\gsl(2,\eC)_0$:
	      \[
		      I_1=
		      \begin{pmatrix}
			      1 \\&1\\&&-1\\&&&-1
		      \end{pmatrix},\quad
		      I_2=
		      \begin{pmatrix}
			      0 & -1          \\
			      1 & 0           \\
			        &    & 0  & 1 \\
			        &    & -1 & 0
		      \end{pmatrix}
	      \]
	\item $\gsl(2,\eC)_2$:
	      \[
		      D_1=\begin{pmatrix}
			        &   & 1 & 0 \\
			        &   & 0 & 1 \\
			      0 & 0         \\
			      0 & 0
		      \end{pmatrix},\quad
		      D_2=
		      \begin{pmatrix}
			        &   & 0 & -1 \\
			        &   & 1 & 0  \\
			      0 & 0 &        \\
			      0 & 0 &
		      \end{pmatrix}
	      \]
	\item $\gsl(2,\eC)_{-2}$
	      \[
		      C_1=\begin{pmatrix}
			        &   & 0 & 0 \\
			        &   & 0 & 0 \\
			      1 & 0         \\
			      0 & 1
		      \end{pmatrix},\quad
		      C_2=\begin{pmatrix}
			        &    & 0 & 0 \\
			        &    & 0 & 0 \\
			      0 & -1         \\
			      1 & 0
		      \end{pmatrix}.
	      \]
\end{itemize}
It is natural to choose $\gsl(2,\eC)_2$ as positive root space system. In this case, $\iN_{\gsl(2,\eC)}=\{ D_1,D_2 \}$, $\iA_{\gsl(2,\eC)}=\{ I_1 \}$ and the table of $\iA\oplus\iN$ is
\begin{align}
	[I_1,D_1] & =2D_1 & [D_1,D_2] & =0 \\
	[I_1,D_2] & =2D_2 &
\end{align}

The full table is
\begin{align}
	[I_1,D_1] & =2D_1  & [I_2,D_1] & =2D_2  & [D_1,D_2] & =0     \\
	[I_1,D_2] & =2D_2  & [I_2,D_2] & =-2D_1 & [D_1,C_1] & =I_1   \\
	[I_1,C_1] & =-2C_1 & [I_2,C_1] & =-2C_2 & [D_1,C_2] & =I_2   \\
	[I_1,C_2] & =-2C_2 & [I_2,C_2] & =2C_1  & [D_2,C_1] & =I_2   \\
	          &        &           &        & [D_2,C_2] & =-I_1.
\end{align}
\section{Symplectic group}		\label{SecSympleGp}
%+++++++++++++++++++++++++

\subsection{Iwasawa decomposition}
%-----------------------------
\index{Iwasawa!decomposition!of $\SP(2,\eR)$}

A simple computation shows that $4\times 4$ matrices subject to $A^t\Omega+\Omega A=0$ are given by
\[
	\begin{pmatrix}
		A & B    \\
		C & -A^t
	\end{pmatrix}
\]
where $A$ is any $2\times 2$ matrix while $B$ and $C$ are symmetric matrices. Looking at general form \eqref{EqGenslMatr}, we see that the operation to invert the two last column and then to invert the two last lines provides a homomorphism $\phi\colon \gsl(2,\eC)\to \gsp(2,\eR)$. The aim is now to build an Iwasawa decomposition of $\gsp(2,\eR)$ which ``contains'' the one of $\gsl(2,\eC)$.

Using the Cartan involution $\theta(X)=-X^t$, we find the Cartan decomposition
\begin{align}
	\iK_{\gsp(2,\eR)} & \leadsto
	\begin{pmatrix}
		A & S \\-S&A
	\end{pmatrix},
	                  & \iP_{\gsp(2,\eR)} & \leadsto
	\begin{pmatrix}
		S & S' \\S'&-S
	\end{pmatrix}
\end{align}
where $S$ and $S'$ are any symmetric matrices while $A$ is a skew-symmetric one. As far as the dimensions are concerned, we have $\dim\iK_{\gsp(2,\eR)}=4$ and $\dim\iP_{\gsp(2,\eR)}=6$. It turns out that $\phi(\iK_{\gsl(2,\eC)})\subset\iK_{\gsp(2,\eR)}$ and $\phi(\iP_{\gsl(2,\eC)})\subset \iP_{\gsp(2,\eR)}$. A maximal abelian subalgebra of $\iP_{\gsp(2,\eR)}$ is spanned by the matrices $A'_1$ and $A'_2$ listed below and the corresponding root spaces are:
\begin{itemize}
	\item $\gsp(2,\eR)_{(0,0)}$:
	      \[
		      A'_1=
		      \begin{pmatrix}
			      1 & 0           \\
			      0 & 1           \\
			        &   & -1 & 0  \\
			        &   & 0  & -1
		      \end{pmatrix},
		      \quad
		      A'_2=
		      \begin{pmatrix}
			      0 & 1           \\
			      1 & 0           \\
			        &   & 0  & -1 \\
			        &   & -1 & 0
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(0,2)}$:
	      \[
		      X'= \begin{pmatrix}
			      1 & -1 &         \\
			      1 & -1 &         \\
			        &    & -1 & -1 \\
			        &    & 1  & -1
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(0,-2)}$:
	      \[
		      V'= \begin{pmatrix}
			      1  & 1           \\
			      -1 & -1          \\
			         &    & -1 & 1 \\
			         &    & -1 & 1
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(2,0)}$:
	      \[
		      W'= \begin{pmatrix}
			        &   & 1 & 0  \\
			        &   & 0 & -1 \\
			      0 & 0          \\0&0
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(2,2)}$:
	      \[
		      L'=
		      \begin{pmatrix}
			        &   & 1 & 1 \\
			        &   & 1 & 1 \\
			      0 & 0         \\0&0
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(2,-2)}$:
	      \[
		      M'=
		      \begin{pmatrix}
			        &   & 1  & -1 \\
			        &   & -1 & 1  \\
			      0 & 0           \\0&0
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(-2,0)}$
	      \[
		      Y'=
		      \begin{pmatrix}
			        &   & 0 & 0 \\&&0&0\\
			      1 & 0         \\0&-1
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(-2,2)}$:
	      \[
		      N'=\begin{pmatrix}
			         &    & 0 & 0 \\&&0&0\\
			      1  & -1         \\
			      -1 & 1
		      \end{pmatrix}
	      \]
	\item $\gsp(2,\eR)_{(-2,-2)}$:
	      \[
		      F'=\begin{pmatrix}
			        &   & 0 & 0 \\
			        &   & 0 & 0 \\
			      1 & 1         \\
			      1 & 1
		      \end{pmatrix}
	      \]
\end{itemize}
It is important to notice how do the root spaces of $\gsl(2,\eC)$ embed:
\begin{align}
	\phi(I_1) & =A'_1              & \phi(I_2) & =\frac{ V'-X' }{ 2 } \\
	\phi(D_1) & =\frac{ L'-M' }{2} & \phi(D_2) & =-W'                 \\
	\phi(C_1) & =\frac{ F'-N' }{2} & \phi(C_2) & =Y'.
\end{align}
So $\iN_{\gsp(2,\eR)}$ must at least contain the elements $L'$, $M'$ and $W'$. We complete the notion of positivity by $V'$. The Iwasawa algebra reads
\[
	\begin{split}
		\iA_{\gsp(2,\eR)}&=\{ B_1,B_2 \}\\
		\iN_{\gsp(2,\eR)}&=\{ L',M',W',V' \}
	\end{split}
\]
with
\begin{align*}
	[L',V']   & =-4W' & [W',V']   & =-2M' \\
	[B_1',L'] & =2L'  & [B'_2,M'] & =2M'  \\
	[B_1',W'] & =W'   & [B_2',W'] & =W'   \\
	[B_1',V'] & =-V'  & [B_2',V'] & =V'
\end{align*}
where $B'_1=\frac{ 1 }{2}(A'_1+A'_2)$ and $B_2=\frac{ 1 }{2}(A_1'-A_2')$. The generators of $\iK_{\gsp(2,\eR)}$ are
\begin{align*}
	K'_t & =
	\begin{pmatrix}
		   &    & 1 & 0 \\
		   &    & 0 & 1 \\
		-1 & 0          \\
		0  & -1
	\end{pmatrix}
	     & K'_1 & =
	\begin{pmatrix}
		0 & 1          \\-1&0\\
		  &   & 0  & 1 \\
		  &   & -1 & 0
	\end{pmatrix} \\
	K'_2 & =
	\begin{pmatrix}
		 &  & 0 & 1 \\&&1&0\\0&-1\\-1&0
	\end{pmatrix}
	     & K'_3 & =
	\begin{pmatrix}
		   &   & 1 & 0  \\
		   &   & 0 & -1 \\
		-1 & 0          \\
		0  & 1
	\end{pmatrix}.
\end{align*}
Notice that $[K'_t,K'_i]=0$ for $i=1$, $2$, $3$.


\subsection{Isomorphism}		\label{SubSecIsosp}
%-----------------------

The following provides an isomorphism $\psi\colon \so(2,3)\to \gsp(2,\eR)$:
\begin{align*}
	\psi(H_i) & =B'_i            & \psi(u)   & =K'_t              \\
	\psi(W)   & =W'              & \psi(R_1) & =\frac{ 1 }{2}K'_1 \\
	\psi(M)   & =M'              & \psi(R_2) & =\frac{ 1 }{2}K'_2 \\
	\psi(L)   & =L'              & \psi(R_3) & =\frac{ 1 }{2}K'_3 \\
	\psi(V)   & =\frac{ 1 }{2}V'
\end{align*}
where the $R_i$'s are the generators of the $\so(3)$ part of $\sK_{\so(2,3)}$ satisfying the relations $[R_i,R_j]=\epsilon_{ijk}R_k$. It is now easy to check that the image of the embedding $\phi\colon \gsl(2,\eC) \to \gsp(2,\eR)$ is exactly $\so(1,3)$, so that
\begin{equation}
	\psi^{-1}\circ\phi\colon \gsl(2,\eC)\to \sH
\end{equation}
is an isomorphism which realises $\sH$ as subalgebra of $\gsp(2,\eR)$. This circumstance will be useful in defining a spin structure on $AdS_4$.

One can prove that the kernel of the adjoint representation of $\SP(2,\eR)$ on its Lie algebra is $\pm\mtu$, in other words, $\Ad(a)=\id$ if and only if $a=\pm\mtu$. We define a bijective map $h\colon \SO(2,3)\to \SP(2,\eR)/\eZ_2$ by the requirement that
\begin{equation}		\label{Eqdefhspsl}
	\psi\big( \Ad(g)X \big)=\Ad\big( h(g) \big)\psi(X)
\end{equation}
for every $X\in\so(2,3)$. The following is true for all $\psi(X)$:
\[
	\begin{split}
		\Ad\big(h(gg'\big)) \psi(X)&=\psi\Big( \Ad(g)\big( \Ad(g')X \big) \Big)\\
		&=\Ad\big( h(g) \big)\psi\big( \Ad(g')X \big)\\
		&=\Ad\big( h(g)h(g') \big)\psi(X),
	\end{split}
\]
the map $h$ is therefore a homomorphism. If an element $a\in \SP(2,\eR)$ reads $a= e^{X_A} e^{X_N} e^{X_K}$ in the Iwasawa decomposition, the property $\Ad(a)\psi(X)=\psi\big( \Ad(g)X \big)$ holds for the element\label{PgSolhpsiSP} $g= e^{\psi^{-1}X_A} e^{\psi^{-1}X_N} e^{\psi^{-1}X_K}$ of $\SO(2,3)$. This shows that $h$ is surjective.

\subsection{Reductive structure on the symplectic group}		\label{SubSecRedspT}
%-------------------------------------------------------

A lot of structure of $\so(2,3)$, such as the reductive homogeneous space decomposition as $\sQ\oplus\sH$, can be immediately transported from $\so(2,3)$ to $\gsp(2,\eR)$. Indeed, let $\mT=\psi(\sQ)$ and $\mI=\phi\big( \gsl(2,\eC) \big)$. We have the direct sum decomposition
\[
	\gsp(2,\eR)=\mT\oplus\mI.
\]
Let $X\in\mT\cap\mI$, then $\psi^{-1}X$ belongs to $\sQ\cap\sH$ which only contains $0$. The fact that $\psi$ is an isomorphism yields that $X=0$. Since $\psi$ preserves linear independence, a simple dimension counting shows that the sum actually spans the whole space.

Putting $g=h^{-1}(a)$ in the definition \eqref{Eqdefhspsl} of $h$, we find
\[
	\psi\left( \Ad\big( h^{-1}(a) \big)X \right)=\Ad(a)\psi(X).
\]
Considering a path $a(t)$ with $a(0)=e$, we differentiate this expression with respect to $t$ at $t=0$ we find
\[
	\ad(dh^{-1}\dot a)X=d\psi^{-1}\big( \ad(\dot a)\psi(X) \big)=\ad(d\psi^{-1}\dot a)(d\psi^{-1}\psi X),
\]
but $d\psi=\psi$ because $\psi$ is linear, hence $[dh^{-1}\dot a,X]=[\psi^{-1}\dot a,X]$ for all $X\in \so(2,3)$ and $\dot a\in \gsp(2,\eR)$. We deduce that $(dh^{-1})_e=\psi^{-1}$. We define
\begin{align*}
	\theta_{\gsp} & =\id|_{\iK_{\gsp}}\oplus(-\id)|_{\iP_{\gsp}} \\
	\sigma_{\gsp} & =\id|_{\mT}\oplus(-\id)|_{\mI}.
\end{align*}
We can check that $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$ and $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$. Then it is clear that
\[
	[\sigma_{\gsp},\theta_{\gsp}]=0
\]
using the corresponding vanishing commutator in $\so(2,3)$. We denote $\mT_a=dL_a\mT$ and the fact that $dp= d\pi\circ dh^{-1}= d\pi\circ \psi^{-1}$ shows that $dp(\mT_a)$ is a basis of $T_{p(a)}(G/H)$. So we consider the basis $t_i=\psi(q_i)$ of $\mT$ and the corresponding left invariant vector fields $\tilde t_i(a)=dL_at_i$.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Heisenberg group and algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Let $V$ be a symplectic vector space with the symplectic form $\Omega$. The \hypertarget{HyperHeisenberg}{Heisenberg algebra} build on $V$ is the vector space
\begin{equation}
	\pH(V,\Omega)=V\oplus \eR E
\end{equation}
endowed with the bracket defined by
\begin{enumerate}

	\item
	      $[\pH(V,\Omega),E]=0$,
	\item
	      $[v,w]=\Omega(v,w)E$ for every $v,w\in V$.

\end{enumerate}
The first conditions makes $E$ central in $\pH$.

The Heisenberg group is, as set, the same as the algebra: $H=V\oplus\eR E$ with the product
\begin{equation}		\label{EqProduitHeisenbergGp}
	g_1\cdot g_2=g_1+g_2+\frac{ 1 }{2}[g_1,g_2]
\end{equation}
where the bracket is the one in the Lie algebra. Direct computations show that this product is associative, the neutral is $(0,0)$ and that the inverse is given by
\begin{equation}
	g^{-1}=-g.
\end{equation}
We are now going to prove that the Lie algebra of that group actually is $\pH(V,\Omega)$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exponential mapping}
%---------------------------------------------------------------------------------------------------------------------------

Let us build the exponential map between the Heisenberg algebra and its group. Let $(x,\tau)\in T_eH$ and consider $g(s)= e^{s(x,\tau)}=\big( v(s),h(s) \big)$.  This map is subject to the following three relations:
\begin{enumerate}

	\item
	      $g(s)g(t)=g(s+t)$,
	\item
	      $g(0)=0$,
	\item
	      $g'(01)=(x,t)$.

\end{enumerate}
Taking the derivative of the first one with respect to $s$ and taking into account $v'(0)=x$ and $h'(0)=\tau$, we find
\begin{subequations}
	\begin{align}
		\Dsdd{g(s)+g(t)+\frac{ 1 }{2}\big[ g(s),g(t) \big]  }{s}{0}                 & =\Dsdd{ \big( v(s+t),h(s+t) \big) }{s}{0} \\
		\Dsdd{ v(s)+v(t),h(s)+h(t)+\frac{ 1 }{2}\Omega\big( v(s),v(t) \big) }{s}{0} & =\big( v'(t),h'(t) \big)                  \\
		\Big( x,\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big) \Big)                    & =\big( v'(t),h'(t) \big)
	\end{align}
\end{subequations}
We deduce that $v'(t)=x$ and $h'(t)=\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big)$, so that $v(t)=tx$ and $h(t)=t\tau$. The exponential mapping is thus given by the identity:
\begin{equation}
	\exp(x,\tau)=(x,\tau).
\end{equation}

In order to prove that the law \eqref{EqProduitHeisenbergGp} accepts the Heisenberg algebra as Lie algebra, we need to compute the adjoint action.
\begin{equation}
	\begin{aligned}[]
		\Ad( e^{t(x,\tau)})(x',\tau') & =\Dsdd{ \AD( e^{t(x,\tau)}) e^{s(x',\tau')} }{s}{0}                                 \\
		                              & =\Dsdd{ (tx,t\tau)(sx',s\tau')(-tx,-t\tau) }{s}{0}                                  \\
		                              & =\Dsdd{ \big(tx+sx',t\tau+s\tau'+\frac{ ts }{2}\Omega(x,x')\big)(-tx,t\tau) }{s}{0} \\
		                              & =\big( x',\tau'+t\Omega(x,x') \big).
	\end{aligned}
\end{equation}
Now, the Lie algebra bracket is given by
\begin{equation}
	\begin{aligned}[]
		\big[ (x,\tau),(x',\tau') \big] & =\Dsdd{ \Ad( e^{t(x,\tau)})(x',\tau') }{t}{0} \\
		                                & =\big( 0,\Omega(x,x') \big)                   \\
		                                & =\Omega(x,x')E,
	\end{aligned}
\end{equation}
which is the bracket of $\pH(V,\Omega)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Hermitian conjugate, unitary operators}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}
	Let \( \hH\) be an Hilbert space. Let \( A\) be a linear continuous operator on \( \hH\). The map
	\begin{equation}
		\begin{aligned}
			S_A\colon \hH' & \to \hH'              \\
			\alpha         & \mapsto \alpha\circ A
		\end{aligned}
	\end{equation}
	is well defined and continuous.
\end{lemma}

\begin{proof}
	We have to prove two elements: firstly \( \alpha\circ A\) must be continuous, so that \( S_A\) takes its values in \( \hH'\), and secondly we want \( S_A\) itself to be continuous.

	The fact that \( \alpha\circ A\) is continuous is simply the fact that \( \alpha\) and \( A\) are continuous.

	Now we compute the norm of \( S_A\); first we have
	\begin{equation}
		\| S_A \|=\sup_{\alpha\in \hH'}\frac{ \| S_A(\alpha) \| }{ \| \alpha \| }.
	\end{equation}
	Then we compute
	\begin{equation}
		\| S_A(\alpha) \|=\sup_{\| u \|=1}\| S_A(\alpha)u \|=\sup_{\| u \|=1}\| \alpha(Au) \|\leq \| \alpha \|\| A \|\| u \|=\| \alpha \|\| A \|.
	\end{equation}
	Then we have
	\begin{equation}
		\| S_A \|=\sup_{\alpha\in \hH'}\frac{ \| S_A(\alpha) \| }{ \| \alpha \| }\leq \sup_{\alpha\in \hH'}\frac{ \| \alpha \|\| A \| }{ \| \alpha \| }=\| A \|<\infty.
	\end{equation}
	The proposition \ref{PROPooQZYVooYJVlBd} shows that \( S_A \) is continuous because it is bounded.
\end{proof}

We will use the map
\begin{equation}
	\begin{aligned}
		\Phi\colon \hH & \to \hH'       \\
		y              & \mapsto \Phi_y
	\end{aligned}
\end{equation}
where \( \Phi_y\) is defined by \( \Phi_y(x)=\langle x, y\rangle \). From the Riesz representation theorem \ref{ThoQgTovL} we know that \( \Phi\) is a bijective isometry. For the sake of notational convenience we will write \( \Phi(u)\) for \( \Phi_u\).

Notice the following formula:
\begin{equation}        \label{EQooHWQPooNeYokT}
	\langle u, \Phi^{-1}(\alpha)\rangle =\alpha(u)
\end{equation}
for every \( u\in \hH\) and \( \alpha\in\hH'\).

\begin{propositionDef}[\cite{MonCerveau}]       \label{DEFooAAKCooZJCHPS}
	Let \( \hH\) be an Hilbert space over \( \eC\). We consider a continuous \( A\colon \hH\to \hH\). There exists an unique linear operator \( B\colon \hH\to \hH\) such that
	\begin{equation}
		\langle Au, v\rangle =\langle u, Bv\rangle
	\end{equation}
	for every \( u,v\in \hH\).

	The so-defined operator \( B\) is the \defe{hermitian conjugate of \( A\)}{hermitian conjugate} and is denoted \( A^{\dag}\). In other words, \( A^{\dag}\) is defined by the equality
	\begin{equation}        \label{EQooPTUWooPCbNxA}
		\langle Au, v\rangle =\langle u, A^{\dag}v\rangle
	\end{equation}
	for every \( u,v\in\hH\).
\end{propositionDef}

\begin{proof}
	In two parts.
	\begin{subproof}
		\spitem[Unicity]
		We have to prove that, \( v\) being given, there exists an unique \( w\) such that
		\begin{equation}        \label{EQooVBJXooOtdmlQ}
			\langle Au, v\rangle =\langle u,w, \rangle
		\end{equation}
		for every \( u\). Be clear: the same \( w\) must works for every \( u\). The condition \eqref{EQooVBJXooOtdmlQ} can be written as
		\begin{subequations}
			\begin{align}
				\Phi(v)Au               & =\Phi(w)u \\
				S_A\big( \Phi(v) \big)u & =\Phi(w)u \\
				S_A\big( \Phi(v) \big)  & =\Phi(w),
			\end{align}
		\end{subequations}
		and finally
		\begin{equation}
			w=\Phi^{-1}\Big( S_A\big( \Phi(v) \big) \Big).      \label{EQooPRVGooUfIELg}
		\end{equation}
		This proves the unicity: \( Bv\) must be given by the expression \eqref{EQooPRVGooUfIELg}.
		\spitem[Existence]
		We check that the formula
		\begin{equation}        \label{EQooOIROooXUjCWL}
			A^{\dag}=\Phi^{-1}\circ S_A\circ \Phi.
		\end{equation}
		satisfy the properties. Using the formula \eqref{EQooHWQPooNeYokT} we have:
		\begin{subequations}
			\begin{align}
				\langle u, (\Phi^{-1}\circ S_A\circ \Phi)(v)\rangle & =(S_A\circ\Phi)(v)u     \\
				                                                    & =\Phi(v)Au              \\
				                                                    & =\langle Au, v\rangle ,
			\end{align}
		\end{subequations}
		so that we see that the operator given by \eqref{EQooOIROooXUjCWL} makes the work.
	\end{subproof}
\end{proof}

\begin{definition}[Unitary, hermitian]      \label{DEFooOKGXooFCzCHu}
	An operator \( A\colon \hH\to \hH\) is \defe{unitary}{unitary operator} if it satisfies
	\begin{equation}
		A^{\dag}A=AA^{\dag}=\id.
	\end{equation}
	An operator \( A\colon \hH\to \hH\) is \defe{hermitian}{hermitian operator} if it satisfies
	\begin{equation}
		A^{\dag}=A.
	\end{equation}
	This was already the definition \ref{DEFooKEBHooWwCKRK}.
\end{definition}

\begin{lemma}
	An unitary operator is an isometry: it preserves the hermitian product on an Hilbert space.
\end{lemma}

\begin{proof}
	Let \( u,v\in \hH\) and \( A\) be an unitary operator on the Hilbert space \( \hH\). We have
	\begin{equation}
		\langle Au, Au\rangle =\langle A^{\dag}Au, v\rangle =\langle u,v, \rangle .
	\end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooJYGRooPTMZwY}
	The hermitian conjugation\footnote{Definition \ref{DEFooAAKCooZJCHPS}.} satisfies:
	\begin{enumerate}
		\item
		      \( \langle A^{\dag}u, v\rangle =\langle u, Av\rangle \) for every \( u,v\in \hH\)
		\item
		      \( (A^{\dag})^{\dag}=A\).
		\item       \label{ITEMooULJLooAqnbHI}
		      \( (AB)^{\dag}=B^{\dag}A^{\dag}\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	We have
	\begin{equation}
		\langle A^{\dag}u, v\rangle =\overline{ \langle v, A^{\dag}u\rangle  }=\overline{ \langle Av, u\rangle  }=\langle u, Av\rangle .
	\end{equation}
	This proves the first point.

	For the second point, \( (A^{\dag})^{\dag}\) is the unique operator satisfying
	\begin{equation}
		\langle A^{\dag}u, v\rangle =\langle u, (A^{\dag})^{\dag}v\rangle
	\end{equation}
	for every \( u,v\in \hH\). Using the first point,
	\begin{equation}
		\langle u, (A^{\dag})^{\dag} v\rangle =\langle u, Av\rangle .
	\end{equation}
	This shows that \( \Phi\big( (A^{\dag})^{\dag}v \big)=\Phi(Av)\). Since \( \Phi\) is bijective, \( (A^{\dag})^{\dag}v=Av\) for every \( v\in \hH\).

	For \ref{ITEMooULJLooAqnbHI}, for every \( x,y\) we have
	\begin{equation}
		\langle (AB)x, y\rangle =\langle Bx, A^{\dag}y\rangle =\langle x, B^{\dag}A^{\dag} y\rangle .
	\end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooZILJooFfDEAF}
	The map
	\begin{equation}
		\begin{aligned}
			f\colon \GL(n,\eC) & \to \GL(n,\eC)   \\
			U                  & \mapsto U^{\dag}
		\end{aligned}
	\end{equation}
	is continuous.
\end{lemma}

\begin{proof}
	Let \( U\in\GL(n,\eC)\) and a sequence \( U_k\stackrel{\GL(n,\eC)}{\longrightarrow}U\). We want to prove that \( U_k^{\dag}\stackrel{\GL(n,\eC)}{\longrightarrow}U^{\dag}\). Let \( x,y\in \eC^n\); from the definition \ref{DEFooAAKCooZJCHPS} we have
	\begin{equation}
		\langle (U_k^{\dag}-U^{\dag})(x),y \rangle =\langle x, (U_k-U)y \rangle .
	\end{equation}
	Since \( U_k\to U\), we have
	\begin{equation}
		\langle (U_k^{\dag}-U^{\dag})(x),y \rangle \stackrel{\eR}{\longrightarrow}0.
	\end{equation}
	Since the scalar product is non degenerate, this implies \( U_k^{\dag}-U^{\dag}\stackrel{\GL(n,\eC)}{\longrightarrow}0\). Notice that we used the fact that \( (U_k-U)^{\dag}=U_k^{\dag}-U^{\dag}\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{The group \texorpdfstring{$ \GL(V,W)$}{GL(V)} of invertible operators}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


The group \( \GL(V,W)\) is defined in \ref{DEFooTLQUooJvknvi}. This is the group of isomorphisms between \( V\) and \( W\). Here, \( \GL(n,\eR)\) is a shortcut for \( \GL(\eR^n, \eR^n)\).

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooQJANooMaqLvV}
	Let \( V\) be a finite dimensional vector space.

	\begin{enumerate}
		\item

		      The group \( \GL(V)\) is a topological manifold with the unique map
		      \begin{equation}
			      \begin{aligned}
				      \varphi\colon U & \to \GL(V) \\
				      a               & \mapsto a
			      \end{aligned}
		      \end{equation}
		      where\footnote{The part \( U\) is \( \GL(V) \) itself, but I prefer to see it as a part of \( \End(V)\). The difference is that \( \End(V) \) is a vector space while \( \GL(V)\) is not.}
		      \begin{equation}
			      U=\{ \text{\( a\) invertible in \( \End(V)\)} \}.
		      \end{equation}
		\item
		      The topological manifold \( \GL(V)\) is smooth.
		\item		\label{ITEMooZWFDooWPiScd}
		      The topology of the manifold \( \big( \GL(V),\varphi \big) \) is the operator norm topology.
		\item		\label{ITEMooBSXTooIrGFco}
		      The smooth manifold \( \GL(V)\) is analytic.
	\end{enumerate}
\end{proposition}

\begin{proof}
	We check the conditions of theorem \ref{THOooFIHIooLiSUxH}. Here, \( \Lambda\) contains only one element and the unique chart is the identity. All Simple.

	\begin{subproof}
		\spitem[Hypothesis \ref{ITEMooDWSWooWdcDdI}]
		%-----------------------------------------------------------
		The part \( U\) is open by proposition \ref{PROPooPNMXooLaJDGc}.
		\spitem[Hypothesis \ref{ITEMooPEXDooNuJBKH}]
		%-----------------------------------------------------------
		The identity is a bijection between \( U\) and \( \GL(V)\) because \( U=\GL(V)\).
		\spitem[Hypothesis \ref{ITEMooSRPQooNUPzlj}]
		%-----------------------------------------------------------
		The identity is a bijection between \( U\) and \( \GL(V)\) because \( U=\GL(V)\).
		\spitem[Hypothesis \ref{ITEMooWFAWooAqQfzZ}]
		%-----------------------------------------------------------
		Since \( \varphi\) is the identity, we have
		\begin{equation}
			\varphi^{-1}\Big(   \varphi\big( \End(V) \big)\cap\varphi\big( \End(V) \big)   \Big)=\End(V),
		\end{equation}
		is open in \( \End(V)\).
		\spitem[Hypothesis \ref{ITEMooZHLXooBpWSXr}]
		%-----------------------------------------------------------
		The identity on a finite dimensional normed vector space is continuous.
	\end{subproof}
	Now \( \GL(V)\) is a topological manifold.

	For \ref{ITEMooZWFDooWPiScd}. The topology of the manifold \( \GL(V)\) is generated by \eqref{EQooHMWQooUdceMk}. Since there is only one map, the topology of \( \GL(V)\) is given by the parts \( \{ \varphi(A) \}_{A\in \tau_{\End(V)}}\). Since \( \varphi\) is the identity, the open sets in \( \GL(V)\) as manifold are the same as the open parts of \( \GL(V)\) as \( U\).

	For \ref{ITEMooBSXTooIrGFco}. Following the definition \ref{DEFooVMWRooGQYJwl}, it is a smooth manifold since the unique chart is the identity which is smooth. Since the identity is analytic, the manifold is analytic by \ref{THOooSQVCooCyEPOS}.
\end{proof}


\begin{proposition}		\label{PROPooPZABooXxQkFi}
	The group \( \GL(n,\eR)\) is an analytic Lie group.
\end{proposition}

\begin{proof}
	This is a particular case of proposition \ref{PROPooQJANooMaqLvV} with \( V=\eR^n\).
\end{proof}

\begin{proposition}     \label{PROPooWRVKooLfqLfV}
	The group \( \GL(n,\eC)\) is an analytic Lie group.
\end{proposition}

\begin{proof}
	This is a particular case of proposition \ref{PROPooQJANooMaqLvV} with \( V=\eC^n\).
\end{proof}


In the remaining we will always consider analytic charts on \( \GL(n,\eC)\).

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooRRNXooXJZYSB}
	Let \( V\) and \( W\) be finite dimensional vector spaces. Let \( S\) be open in the manifold \( M\). We have \( A\in C^p\big( S,\End(V,W) \big) \) if an only if \( A_{ij}\in C^p(S,\eR)\) for every \( i,j\).
\end{proposition}
\noproof

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooLANVooKPiLuu}
	Let \( S\) be open in the manifold \( M\). If \( A\in C^{p}\big( S,\GL(\eR,k) \big)\) and \( B\in C^{p}\big( S,\GL(\eR,l) \big)\), then\footnote{Definition \ref{PROPooLFHFooRmTAjY} for the tensor product of linear maps.}
	\begin{equation}
		A\otimes B\in C^{p}\big( S,\GL(\eR^k\otimes \eR^l) \big).
	\end{equation}
\end{proposition}
\noproof

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{The group \texorpdfstring{$ \SU(n)$}{SUn} of special unitary operators}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooKFQOooBVtyoW}
	The part \( \SU(n)\) is closed in \( \GL(n,\eC)\).
\end{lemma}

\begin{proof}
	The definition of the hermitian adjoint is in the proposition \ref{DEFooAAKCooZJCHPS}. The map \( U\to U^{\dag}\) is continuous by lemma \ref{LEMooZILJooFfDEAF}. Since \( (U,U^{\dag})\mapsto UU^{\dag}\) is continuous too, the set of operators satisfying \( UU^{\dag}\neq\mtu\) is open. By complementarity, \( \SU(n)\) is closed.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Some settings}
%---------------------------------------------------------------------------------------------------------------------------

We particularize ourself to the finite dimensional Hilbert space \( \hH=\eC^n\). The vector space \( \eC^n\) has a canonical basis; so we use it, and the linear maps \( \eC^n\to \eC^n\) are identified with their matrices in that very basis. Using the conventions described in the proposition \ref{PROPooGXDBooHfKRrv} we have
\begin{equation}
	A(e_j)=\sum_iA(e_j)_ie_i=\sum_iA_{ij}e_i
\end{equation}
where \( \{ e_i \}_{i=1,\ldots, n}\) is the canonical basis of \( \eC^n\).

When \( u\in \eC^n\), we denote by \( u_k\in \eC\) the \( k\)\ieme\ component of \( u\) with respect to the canonical basis. The vector space \( \eC^n\) has an hermitian product\footnote{Definition \ref{DefMZQxmQ}.} given by
\begin{equation}
	\langle u, v\rangle =\sum_{k=1}^nu_k\bar v_k.
\end{equation}

\begin{lemma}       \label{LEMooKEUZooUjQVmp}
	Let \( A\) be a linear continuous operator on the Hilbert space \( \hH\). We have
	\begin{equation}
		\det(A^{\dag})=\overline{ \det(A) }
	\end{equation}
	where the bar stands for the complex conjugate.

	If \( A\) is unitary, then \( \det(A)\in S^1\) where \( S^1\) is the set of elements of norm \( 1\) in \( \eC\).
\end{lemma}

\begin{proof}
	We use for the determinant the formula given by lemma \ref{LEMooEZFIooXyYybe}:
	\begin{equation}
		\det(A^{\dag})=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\langle e_{\sigma(i)}, A^{\dag}e_i\rangle =  \sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n \langle Ae_{\sigma(i)}, e_i\rangle = \sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\overline{ e_i,Ae_{\sigma(i)} }.
	\end{equation}
	At this point we re-index the product and we make a change of variable $\sigma\to \sigma^{-1}$ for the sum (as in the proof for the transposed matrix, lemma \ref{LEMooCEQYooYAbctZ}).

	If the operator is unitary,
	\begin{equation}
		1=\det(AA^{\dag})=\det(A)\det(A^{\dag})=| \det(A) |^2.
	\end{equation}
	Thus \( | \det(A) |=1\).
\end{proof}

Recall: the hermitian conjugate is definition \ref{DEFooAAKCooZJCHPS}.
\begin{lemma}
	As for the matrices,
	\begin{equation}
		(U^{\dag})_{ij}=\overline{ U_{ji} }.
	\end{equation}
\end{lemma}

\begin{proof}
	The matrix convention are summarized in \ref{SECooBTTTooZZABWA}. We write the definition \eqref{EQooPTUWooPCbNxA} for the basis vectors:
	\begin{equation}
		\langle Ue_i, e_j\rangle =\langle e_i, U^{\dag}e_j\rangle .
	\end{equation}
	This means \( U_{ji}=\overline{ \langle U^{\dag}e_j, e_i\rangle  }=\overline{ U^{\dag}_{ij} }\).
\end{proof}

\begin{definition}[\cite{BIBooUXTFooXTeMOn}]        \label{DEFooVIQUooQbnYMu}
	The group of the unitary operators with determinant \( 1\) is \( \SU(n)\). More explicitly,
	\begin{equation}
		\SU(n)=\{U\in \GL(n,\eC)\tq U^{\dag}U=\id,\det(U)=1\}.
	\end{equation}
\end{definition}

\begin{proposition}     \label{PROPooYXPRooBgikdE}
	A unitary endomorphism of \( \eC^n\) is diagonalizable by a unitary operator.
\end{proposition}

\begin{proof}
	A unitary operator \( U\) is normal (definition \ref{DefWQNooKEeJzv}), so that the spectral theorem \ref{ThogammwA} provides an unitary matrice \( V\) such that \( V^{\dag}UV\) is diagonal.
\end{proof}

\begin{proposition}     \label{PROPooZBJSooEIguXR}
	A special unitary matrix is the exponential of a skew-hermitian matrix with vanishing trace.
\end{proposition}

\begin{proof}
	Let \( U\in \SU(n)\). We prove that here exists an hermitian operator \( H\) with \( U= e^{iH}\) and \( \tr(H)=0\). Then \( iH\) is the requested skew-hermitian operator.

	By mean of the proposition \ref{PROPooYXPRooBgikdE}, we diagonalize it with the unitary operator \( S\). We have
	\begin{equation}
		U=SDS^{\dag},
	\end{equation}
	and
	\begin{equation}
		\det(U)=1,
	\end{equation}
	and
	\begin{equation}
		D=\begin{pmatrix}
			e^{i\varphi_1} &        &                \\
			               & \ddots &                \\
			               &        & e^{i\varphi_n}
		\end{pmatrix}.
	\end{equation}
	Since \( U\in\SU(n)\) we have
	\begin{equation}
		1=\det(U)=\det(S)\det(D)\det(S^{\dag})=| \det(S) |^2\det(D)=\det(D).
	\end{equation}
	Thus \( \det(D)=1\) and we deduce
	\begin{equation}
		\sum_{i=1}^n\varphi_i=2k\pi
	\end{equation}
	for some \( k\in \eZ\). We consider the two following  operators:
	\begin{equation}
		\Delta=\begin{pmatrix}
			\varphi_1 &        &           \\
			          & \ddots &           \\
			          &        & \varphi_n
		\end{pmatrix},
	\end{equation}
	and \( H=S\Delta S^{\dag}\). Since \( \Delta^{\dag}=\Delta\), we have \( H^{\dag}=H\), so that \( H\) is hermitian. We also have \( U= e^{iH}\). Indeed, since \( (S\Delta S^{\dag})^k=S\Delta^kS^{\dag}\) we have
	\begin{equation}
		e^{iH}=\sum_{k=0}^{\infty}\frac{ (iH)^k }{ k! }=\sum_k\frac{ i^k }{ k! }(S\Delta S^{\dag})^k=\sum_k\frac{ i^k }{ k! }S\Delta^kS^{\dag}=S e^{i\Delta}S^{\dag}=SDS^{\dag}=U.
	\end{equation}
	We also have \( \tr(H)=2k\pi\).

	This means that \( H\) is almost the operator we are searching for. It is easy to modify \( H\) in order to get our answer. We set
	\begin{equation}
		\Delta'=\begin{pmatrix}
			\varphi_1 &        &                 \\
			          & \ddots &                 \\
			          &        & \varphi_n-2k\pi
		\end{pmatrix}.
	\end{equation}
	This matrix satisfies \(   e^{i\Delta'}= e^{i\Delta}\) and \( \tr(\Delta')=0\). If we set \( H'=S\Delta'S^{\dag}\) we still have \( \tr(H')=\tr(\Delta')=0\) because of the cyclic invariance of the trace. And finally the operator \( H'\) satisfies
	\begin{equation}
		e^{iH'}=S e^{i\Delta'}S^{\dag}= e^{iH}=U.
	\end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{The center of \texorpdfstring{$ \SU(n)$}{SUn} }
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooLMGHooKrKpsa}
	The center\footnote{The set of elements which commute with all the elements, see definition \ref{defGroupeCentre}.} of the group \( \SU(n)\), \( \mZ\big( \SU(n) \big)\), is the subgroup of the element of the form
	\begin{equation}
		e^{i\varphi}\id.
	\end{equation}
	In particular, the center of \( \SU(2)\) is \( \{ \id,-\id \}\).
\end{proposition}

\begin{proof}
	Let \( Z\in\mZ\big( \SU(n) \big)\). It can be diagonalized by an unitary matrix \( S\):
	\begin{equation}
		SZS^{\dag}=D.
	\end{equation}
	The operator \( S\) belongs to \( \gU(n)\) while \( Z\) only commutes with the elements of \( \SU(n)\). Thus we cannot immediately deduce \( Z=D\).  However, the operator \( S_0 = S/\det(S)\) diagonalizes \( Z\) as well as \( S\): \( S_0ZS_0^{\dag}=D\). But since \( S_0\) is in \( \SU(n)\) we can deduce \( Z=D\).

	Long story short, the elements of \( \mZ\big( \SU(n) \big)\) are diagonal. Let \( Ze_i=\lambda_ie_i\). We consider the operator \( A\) of \( \SU(n)\) given by the formula
	\begin{subequations}
		\begin{align}
			Ae_1 & =-e_2                       \\
			Ae_2 & =e_1                        \\
			Ae_k & =e_k\qquad\text{otherwise.}
		\end{align}
	\end{subequations}
	This is the element of \( \SU(n)\) which permutes \( e_1\) with \( e_2\) (with a sign for the sake of the determinant). Since \( ZA=AZ\), we have
	\begin{equation}
		AZe_1=\lambda_1Ae_1=-\lambda_1e_1
	\end{equation}
	and
	\begin{equation}
		ZAe_1=Z(-)e_2=-\lambda_2e_2.
	\end{equation}
	So we have \( \lambda_1=\lambda_2\). The same being true not only for \( 1\) and \( 2\) but for the other ones, we know that the numbers \( \lambda_i\) are all equal.

	So far, \( Z=\lambda\id\). The value of \( \lambda\) is not arbitrary: we must impose \( Z\in\SU(n)\). The determinant condition provides
	\begin{equation}        \label{EQooZZPJooCeKPDD}
		1=\det(Z)=\lambda^n
	\end{equation}
	and the unitary condition imposes
	\begin{equation}
		1=\langle e_i, e_i\rangle =\langle Ze_i, Ze_i\rangle =| \lambda |^2,
	\end{equation}
	so that \( | \lambda |^2=1\). This conditions imposes \( \lambda= e^{i\varphi}\) for some \( \varphi\in \eR\) while the condition \eqref{EQooZZPJooCeKPDD} furnishes
	\begin{equation}
		e^{in\varphi}=1.
	\end{equation}
	This shows the existence of \( k\in \eZ\) such that \( in\varphi=2ki\pi\) and finally
	\begin{equation}
		\varphi=\frac{ 2k\pi }{ n }.
	\end{equation}

	In the case \( n=2\) we have \( \varphi=k\pi\) and \(  e^{i\varphi}=\pm 1\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Lie group}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Lie group structure for \( \SU(n)\)\cite{MonCerveau}]        \label{PROPooPXRJooLNnMFn}
	There exists a unique analytic manifold structure on \( \SU(n)\) such that :
	\begin{enumerate}
		\item       \label{ITEMooHZQRooFGCVjP}
		      The group \( \SU(n)\) is Lie subgroup\footnote{Definition \ref{DEFooGCHDooHUMSju}.} of \( \GL(n,\eC)\).
		\item
		      There exists an atlas in which \( \SU(n)\) is an analytic Lie group.
		\item
		      With such an atlas, the inclusion map \( \iota\colon \SU(n)\to \GL(n,\eC)\) is analytic.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Since \( \SU(n)\) is closed in \( \GL(n,\eC)\) (lemma \ref{LEMooKFQOooBVtyoW}), the Cartan theorem \ref{THOooDEJHooVKJYBL} provides us a unique smooth manifold structure on \( \SU(n)\) such that the inclusion map \( \iota\colon \SU(n)\to \GL(n,\eC)\) is a smooth embedding. The Cartan theorem adds that \( \SU(n)\) is then a Lie subgroup of \( \GL(n,\eC)\)

	Theorem \ref{THOooSQVCooCyEPOS} allows us to choose an atlas for \( \SU(n)\) in which it becomes analytic.
	\begin{probleme}
		Thus proof is not finished: lacks of the analytic statement. I may have to prove an analytic version of the Cartan theorem.
	\end{probleme}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representations of \texorpdfstring{$ U(1)$}{U(1)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}
	Let \( V\) be a complex vector space of dimension \( 1\). Let \( \xi\neq 0\in V\). If \( \langle ., .\rangle \) is an hermitian product on \( V\), there exists \( m\in \eR\) such that
	\begin{equation}
		\langle z_1\xi, z_2\xi\rangle =mz_1\bar z_2.
	\end{equation}
\end{proposition}

\begin{proof}
	Every element of \( V\) can be written under the form \( z\xi\) for some \( z\in \eC\). The properties of an hermitian product say
	\begin{equation}
		\langle z_1\xi, z_2\xi\rangle =z_1\bar z_2\langle \xi, \xi\rangle
	\end{equation}
	and \( \langle \xi, \xi\rangle \in \eR\).
\end{proof}

\begin{normaltext}
	What we write \( S^1\) and \( \gU(1)\) are the same thing. When we write \( S^1\) we have in mind the geometrical object (with a measure) made of the complex numbers of norm \( 1\); when we write \( \gU(1)\) we have in mind the group structure. But it's the same.

	However the generalizations \( S^2\) and \( \gU(2)\) are not the same.
\end{normaltext}

\begin{normaltext}[\cite{MonCerveau}]
	We are searching now for the irreducible representations of \( U(1)\). More precisely we will determine the irreducible continuous representations of \( U(1)\). Here the fact to be «continuous» means that \( \rho\colon U(1)\to \GL(V)\) is continuous; in particular, \( V\) has to be a topological vector space.

	This is not a restriction because the theorem \ref{THOooFFJGooCekFQc} shows that every irreducible representation of \( U(1)\) has dimension \( 1\).
\end{normaltext}

\begin{proposition}[Irreducible representations of \( U(1)\)]       \label{PROPooLWWEooUmqbRA}
	Let \( m\in \eZ\). We consider
	\begin{equation}        \label{EQooXPXKooJasMyY}
		\begin{aligned}
			T_m\colon U(1) & \to \GL(\eC) \\
			T_m(g)z        & =g^mz.
		\end{aligned}
	\end{equation}
	\begin{enumerate}
		\item
		      The formula \eqref{EQooXPXKooJasMyY} defines a representation of \( U(1)\).
		\item
		      The representation \( T_m\) is irreducible.
		\item
		      The representation \( T_m\) is continuous.
		\item
		      If \( m\neq l\), then the representations \( T_m\) and \( T_l\) are not equivalent.
		\item       \label{ITEMooUPVQooQddQOJ}
		      Every continuous\footnote{For the norm of proposition \ref{PROPooNTCFooEcwZwt} on \( V\) and the corresponding operator norm one on \( \GL(V)\), definition \ref{DefNFYUooBZCPTr}.} irreducible representation of \( U(1)\) is equivalent to one of them.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Several points.
	\begin{subproof}
		\spitem[It is a representation]
		Since \( U(1)\) is abelian, \( (g_1g_2)^m=g_1^mg_2^m\).
		\spitem[Irreducible]
		The representation \( T_m\) is irreducible because the vector space is \( \eC\) which has dimension \( 1\).
		\spitem[Continuous]
		Let \( g_k\stackrel{U(1)}{\longrightarrow}g\). We have
		\begin{equation}
			\| T_m(g_k)-T_m(g) \|=\sup_{| z |1=1}| g_k^m-g^mz |=| g_k^m-g^m |\to 0.
		\end{equation}
		This shows that \( T_m\) is continuous.
		\spitem[Non equivalence]
		Let \( \psi\colon \eC\to \eC\) be a linear map such that \( T_m(g)\circ\psi=\psi\circ T_l(g)\) for every \( g\in U(1)\). This implies
		\begin{equation}
			g^m\psi(z)=\psi(g^lz),
		\end{equation}
		hence
		\begin{equation}
			g^m\psi(z)=g^l\psi(z).
		\end{equation}
		Taking \( z\) such that \( \psi(z)\neq 0\) we have \( g^m=g^l\) for every \( g\in U(1)\), or \( g^{l-m}=1\). Writing \( g= e^{ix}\) we have
		\begin{equation}
			e^{i(l-m)x=1}
		\end{equation}
		for every \( x\in \eR\). We conclude \( l-m=2k \pi\). Since \( l-m\in \eZ\) the only solution is \( l-m=0\).

	\end{subproof}
	At this point, it ``remains'' to prove the point \ref{ITEMooUPVQooQddQOJ}. Let \( (\rho, V)\) be a continuous irreducible representation of \( U(1)\).

	\begin{subproof}
		\spitem[The function \( \lambda\)]
		Since \( U(1)\) is abelian, \( \dim(V)=1\) (theorem \ref{THOooFFJGooCekFQc}). So there exist a function \( \lambda\colon U(1)\to \eC\) such that
		\begin{equation}
			\rho(g)=\lambda(g)\id.
		\end{equation}

		\spitem[\( \lambda\) is continuous]

		The spaces \( V\), \( \GL(V)\) and \( \eC\) are metric, thanks to the restriction we imposed on the topology of \( V\). Let \( g_k\to g\) in \( U(1)\). Since \( \rho\) is continuous we have \( \rho(g_k)\stackrel{\GL(V)}{\longrightarrow}\rho(g)\). From the definition of the operator norm, that implies, for each \( v\in V\) that
		\begin{equation}
			\rho(g_k)\stackrel{V}{\longrightarrow}\rho(g)v,
		\end{equation}
		which means
		\begin{equation}        \label{EQooTPAXooAPSgxP}
			\lambda(g_k)v\stackrel{V}{\longrightarrow}\lambda(g)v.
		\end{equation}
		Using the definition of the topology on \( V\),
		\begin{equation}
			\| \lambda(g_k)v-\lambda(g)v \|=\|\big( \lambda(g_k)-\lambda(g) \big)v \|=\| \lambda(g_k)-\lambda(g) \|\| v \|.
		\end{equation}
		The convergence \eqref{EQooTPAXooAPSgxP} means
		\begin{equation}
			| \lambda(g_k)-\lambda(g) |\| v \|\stackrel{\eR}{\longrightarrow}0,
		\end{equation}
		which implies the convergence \( \lambda(g_k)\to \lambda(g)\), hence the continuity of \( \lambda\).

		\spitem[\( \lambda\) takes values in \( S^1\)]

		We show that \( |\lambda(g)|=1\) pour tout \( g\in U(1)\). An element of \( U(1)\) reads \( g= e^{2\pi i x}\) with \( x\in \eR\)\footnote{Proposition \ref{PROPooXELTooYKjDav}\ref{ITEMooOHRHooRXvxrL}.}.

		If \( x\in \eZ\) we have \( g=1\), so that \( \lambda(g)=1\). If \( x=1/n\) (\( n\in \eZ\)) we have \( g^n=1\), but
		\begin{equation}
			\lambda(g^n)=\lambda(g)^n,
		\end{equation}
		so that \( | \lambda(g) |^n=1\) which proves that \( | \lambda(g) |=1\).

		From here we know that \( |\lambda( e^{2\pi i q})|=1\) for every \( q\in \eQ\).

		Since \( \lambda\) is continuous, the function \( x\mapsto | \lambda( e^{2\pi i x}) |\) is continuous. A continuous function whose value is \( 1\) over \( \eQ\) is constant.

		\spitem[Functional equation]
		For \( x,y\in \eR\) we have  \( \rho( e^{ix})\rho( e^{it})=\rho( e^{ix} e^{iy})=\rho( e^{i(x+y)})\). We introduce the notation
		\begin{equation}
			\alpha=\lambda\circ\varphi
		\end{equation}
		where \( \varphi\colon \eR\to U(1)\) is \( \varphi(x)= e^{ix}\). The function
		\begin{equation}
			\begin{aligned}
				\alpha\colon \eR & \to S^1                  \\
				x                & \mapsto \lambda( e^{ix})
			\end{aligned}
		\end{equation}
		satisfies
		\begin{subequations}
			\begin{numcases}{}
				\alpha(0)=1\\
				\alpha(x+y)=\alpha(x)\alpha(y).
			\end{numcases}
		\end{subequations}
		The proposition \ref{PROPooVJLYooOzfWCd} shows that there exists \( m\in \eR\) such that
		\begin{equation}
			\alpha(x)= e^{imx}.
		\end{equation}

		Since \( \alpha(2\pi)=\alpha(0)=1\) we have \(  e^{2\pi im}=1\) and we conclude that \( m\in \eZ\).

		\spitem[The value of \( \lambda\)]

		We have defined \( \alpha=\lambda\circ \varphi\). Since \( \varphi\) is not a bijection, we cannot write \( \lambda=\alpha\circ \varphi^{-1}\). However for every \( x\in \eR\) we have
		\begin{equation}
			\lambda( e^{ix})=\alpha(x)= e^{imx}=( e^{ix})^m.
		\end{equation}
		So
		\begin{equation}
			\lambda(g)=g^m.
		\end{equation}

		\spitem[Equivalence]

		We prove that \( \rho\) is equivalent to the representations \( T_m\). Let \( \{ v \}\) be a basis of \( V\); we consider the linear map
		\begin{equation}
			\begin{aligned}
				\psi\colon V & \to \eC    \\
				sv           & \mapsto s.
			\end{aligned}
		\end{equation}
		We have
		\begin{equation}
			\big( T_m(g)\circ \psi \big)(sv)=T_m(g)s=g^ms
		\end{equation}
		while
		\begin{equation}
			\big( \psi\circ\rho(g) \big)(sv)=\psi\big( g^msv \big)=g^ms\psi(v)=g^ms.
		\end{equation}
		So we have
		\begin{equation}
			T_m(g)\circ \psi=\psi\circ\rho(g)
		\end{equation}
		which proves that \( T_m\) and \( \rho\) are equivalent.
	\end{subproof}
\end{proof}
