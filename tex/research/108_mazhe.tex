% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014, 2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{Explicit choices in \texorpdfstring{$\so(2,l-1)$}{so2l-1}}
%\label{app_calc}
%+++++++++++++++++++++++++++++++++++++++++++

In the case of $AdS_4$ the matrices are $5\times 5$. We will write them down, but the general form are entirely similar. Our choice of Iwasawa decomposition is
\begin{subequations}
\begin{align}
\sN&=\{W_i,V_j,M,L\}\\
\sA&=\{ J_1, J_2\}.
\end{align}
\end{subequations}


The basis of $\sodn$ in which we want to decompose all our elements is the root space one:
\begin{equation}
\sG=\Span\{J_1,q_1,X,Y,V,W,M,N,F,L\}
\end{equation}
note in particular that $\sG_{(0,0)}=\Span\{J_1,q_1\}$ and $W,J_1\in\sH$.
\begin{equation}
\frac{1}{2}(W-Y)=
\begin{pmatrix}
&0\\
0&0&0&0&1\\
&0\\
&0\\
&1
\end{pmatrix},
\qquad
\frac{1}{2}(V+X)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&1\\
&&&&0\\
0&0&-1&0&0
\end{pmatrix},
\end{equation}



\begin{equation}
\frac{1}{2}(W+Y)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&0\\
&&&&1\\
0&0&0&-1&0
\end{pmatrix},
\qquad
q_3=\frac{1}{2}(V-X)=
\begin{pmatrix}
0&0&0&0&1\\
0\\
0\\
0\\
1
\end{pmatrix}.
\end{equation}


\subsection{Decompositions and commutators for \texorpdfstring{$\sQ$}{Q}}
%-------------------------------------------------------------------------

First, the root space decomposition of the basis $\{q_i\}$ of $\sQ$:
\begin{subequations}
\begin{align}
	q_0	&=\frac{1}{ 4 }(M+N+L+F)				&	q_2	&=\frac{1}{ 4 }(N+F-M-L)\\
		&=\frac{1}{ 4 }(X_{11}+X_{1,-1}+X_{-1,1}+X_{-1,-1})	&		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})\\
	q_1	&=q_1=J_2						&	q_3	&=\frac{1}{2}(V-X)\\
		&							&		&=\frac{ 1 }{2}(X_{01}-X_{0,-1}).
\end{align}
\end{subequations}
The commutators:
\begin{subequations}
\begin{align}
[q_0,q_1]&=\us{4}(L+F-M-N) &[q_1,q_2]&=\us{4}(L+N-F-M)\\\
&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})	&&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,1}-X_{-1,1}-X_{11})\\
[q_0,q_2]&=-J_1            &[q_1,q_3]&=\frac{1}{2}(V+X)\\
[q_0,q_3]&=\frac{1}{2}(Y-W)      &[q_2,q_3]&=\frac{1}{2}(W+Y)
\end{align}
\end{subequations}

\subsection{Commutators between root spaces and \texorpdfstring{$\sQ$}{Q}}

\begin{subequations}
\begin{align}
[q_0,J_1]	&=\frac{1}{4}(N+F-M-L)					&[q_1,J_1]&=0	&[q_2,J_1]&=q_0\\
		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})					\\
		&=q_2\\
[q_0,q_1]	&=\frac{1}{4}(L+F-M-N)&       				&    &[q_2,q_1]&=\us{4}(F+M-L-N)\\
		&=\frac{ 1 }{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})\\
[q_0,X]  &=\frac{1}{2}(W-Y)          &[q_1,X]  &=-X &[q_2,X]&=-\frac{1}{2}(W+Y)\\
[q_0,Y]  &=\frac{1}{2}(X-V)          &[q_1,Y]  &=0  &[q_2,Y]&=\frac{1}{2}(V-X)\\
[q_0,V]  &=\frac{1}{2}(Y-W)          &[q_1,V]  &=V  &[q_2,V]&=\frac{1}{2}(W+Y)\\
[q_0,W]  &=\frac{1}{2}(V-X)          &[q_1,W]  &=0  &[q_2,W]&=\frac{1}{2}(V+X)\\
[q_0,M]  &=q_1+J_1             &[q_1,M]  &=M  &[q_2,M]&=q_1+J_1\\
[q_0,N]  &=q_1-J_1             &[q_1,N]  &=N  &[q_2,N]&=-q_1+J_1\\
[q_0,L]  &=-q_1+J_1            &[q_1,L]  &=-L &[q_2,L]&=-q_1+J_1\\
[q_0,F]  &=-q_1-J_1            &[q_1,F]  &=-F &[q_2,F]&=q_1+J_1
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
 [q_3,J_1]&=0           &[q_3,M]&=W   &[q_3,V]&=-q_1\\
 [q_3,q_1]&=-\frac{1}{2}(V+X) &[q_3,N]&=-Y  &[q_3,W]&=\frac{1}{2}(M+L)\\
          &             &[q_3,L]&=W   &[q_3,X]&=-q_1\\
          &             &[q_3,F]&=-Y  &[q_3,Y]&=-\frac{1}{2}(N+F)
\end{align}
\end{subequations}

\subsection{Commutators in the root spaces}

\begin{subequations}
\begin{align}
[J_1,q_1]&=0\\
[J_1,X]&=0&[q_1,X]&=-X\\
[J_1,Y]&=-Y&[q_1,Y]&=0&[X,Y]&=F\\
[J_1,V]&=0&[q_1,V]&=V&[X,V]&=2q_1\\
[J_1,W]&=W&[q_1,W]&=0&[X,W]&=-L\\
[J_1,M]&=M&[q_1,M]&=M&[X,M]&=-2W\\
[J_1,N]&=-N&[q_1,N]&=N&[X,N]&=2Y\\
[J_1,F]&=-F&[q_1,F]&=-F&[X,F]&=0\\
[J_1,L]&=L&[q_1,L]&=-L&[X,L]&=0
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
[V,W]&=M\\
[V,M]&=0&[W,M]&=0\\
[V,N]&=0&[W,N]&=-2V&[M,N]&=0\\
[V,F]&=-2Y&[W,F]&=2X&[M,F]&=-4q_1-4J_1\\
[V,L]&=2W&[W,L]&=0&[M,L]&=0
\end{align}
\end{subequations}


\begin{subequations}
\begin{align}
[N,F]&=0\\
[N,L]&=-4q_1+4J_1&[F,L]&=0
\end{align}
\end{subequations}

\subsection{Killing form}
%++++++++++++++++++++
The adopted definition is $B(x,y)=\tr(\ad x\circ\ad y)$ with no one half or such coefficient.
\begin{equation}
\begin{aligned}
B(J_1,q_1)&=0	&B(V,X)&=-12\\
B(J_1,J_1)&=6	&B(N,L)&=-24\\
B(W,Y)&=-12	&B(M,F)&=-24
\end{aligned}
\end{equation}
Some easy computations show that for $g\in \SO(2)$,
\[
\begin{split}
dL_gq_0&=
\begin{pmatrix}
-\sin u&\cos u\\
-\cos u&\sin u
\end{pmatrix},
\quad
dL_g q_1=
\begin{pmatrix}
0&0&\cos u\\
0&&-\sin u\\
1
\end{pmatrix}\\
dL_g H_1&=
\begin{pmatrix}
0&0&\sin u\\
0&&\cos u\\
0&1
\end{pmatrix}\\
dR_g J_1&=
\begin{pmatrix}
0\\
0&0&0&1\\
0\\
-\sin u&\cos u
\end{pmatrix},
\quad
dR_g J_2=
\begin{pmatrix}
0&0&1\\
0\\
\cos u&\sin u
\end{pmatrix}
\end{split}
\]
So
\begin{subequations}
\begin{align}
dR_g J_1&=-\sin u\, dL_g q_2+\cos u\, dL_g H_2\\
dR_g J_2&=\sin u\, dL_g H_1+\cos u\, dL_g q_1.
\end{align}
\end{subequations}
and
\begin{subequations}
\begin{align}
  B_{[g]}(J_1^*,J_1^*)&=6\sin^2 u\\
B_{[g]}(J_2^*,J_2^*)&=6\cos^2 u.
\end{align}
\end{subequations}


\section{Iwasawa decomposition for \texorpdfstring{$\gsl(2,\eC)$}{sl2C}}		\label{SecIwasldeuxC}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{Iwasawa!decomposition!of $\SL(2,\eC)$}

Matrices of $\gsl(2,\eC)$ are acting on $\eC^2$ as
\[
\begin{split}
  \begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}&
\begin{pmatrix}
a+bi\\c+di
\end{pmatrix}\\
&=
\begin{pmatrix}
(\alpha_1a-\alpha_2b+\beta_1c-\beta_2d)+i(\alpha_2a+\alpha_1b+\beta_2c+\beta_1d)\\
(\gamma_1a-\gamma_2b-\alpha_1c+\alpha_2d)+i(\gamma_2a+\gamma_1b-\alpha_2c-\alpha_1d)
\end{pmatrix}
\end{split}
\]
if $\alpha=\alpha_1+i\alpha_2$.  Our aim is to embed $\SL(2,\eC)$ in $\SP(2,\eR)$ (see sections~\ref{SecSympleGp} and~\ref{SecDirADs}), so that we want a four dimensional realization of $\gsl(2,\eC)$. It is easy to rewrite the previous action under the form of $\begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}$ acting of the vertical four component vector $(a,b,c,d)$. The result is that a general matrix of $\gsl(2,\eC)$ reads
\begin{equation}		\label{EqGenslMatr}
\gsl(2,\eC)\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&-\alpha_2\\
\alpha_2&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
\gamma_1&-\gamma_2\\
\gamma_2&\gamma_1
\end{array}&
\boxed{
\begin{array}{cc}
-\alpha_1&\alpha_2\\
-\alpha_2&-\alpha_1
\end{array}
}
\end{pmatrix}.
\end{equation}
The boxes are drawn for visual convenience.  Using the Cartan involution $\theta(X)=-X^t$, we find the following Cartan decomposition:
\begin{equation}
\begin{split}
\iK_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
0&-\alpha_2\\
\alpha_2&0
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix},\\
\iP_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&0\\
0&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix}.
\end{split}
\end{equation}
We have $\dim\iP_{\gsl(2,\eC)}=3$ and $\dim\iP_{\gsl(2,\eC)}=3$. A maximal abelian subalgebra of $\iP_{\gsl(2,\eC)}$ is the one dimensional algebra generated by
\[
  A_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix}.
\]
The corresponding root spaces are
\begin{itemize}
\item $\gsl(2,\eC)_0$:
\[
  I_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix},\quad
I_2=
\begin{pmatrix}
0&-1\\
1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsl(2,\eC)_2$:
\[
  D_1=\begin{pmatrix}
&&1&0\\
&&0&1\\
0&0\\
0&0
\end{pmatrix},\quad
D_2=
\begin{pmatrix}
&&0&-1\\
&&1&0\\
0&0&\\
0&0&
\end{pmatrix}
\]
\item $\gsl(2,\eC)_{-2}$
\[
  C_1=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&0\\
0&1
\end{pmatrix},\quad
C_2=\begin{pmatrix}
&&0&0\\
&&0&0\\
0&-1\\
1&0
\end{pmatrix}.
\]
\end{itemize}
It is natural to choose $\gsl(2,\eC)_2$ as positive root space system. In this case, $\iN_{\gsl(2,\eC)}=\{ D_1,D_2 \}$, $\iA_{\gsl(2,\eC)}=\{ I_1 \}$ and the table of $\iA\oplus\iN$ is
\begin{align}
[I_1,D_1]&=2D_1&		[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&
\end{align}

    The full table is
\begin{align}
[I_1,D_1]&=2D_1&	[I_2,D_1]&=2D_2&	[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&	[I_2,D_2]&=-2D_1&	[D_1,C_1]&=I_1\\
[I_1,C_1]&=-2C_1&	[I_2,C_1]&=-2C_2&	[D_1,C_2]&=I_2\\
[I_1,C_2]&=-2C_2&	[I_2,C_2]&=2C_1&	[D_2,C_1]&=I_2\\
	&	&		&     &		[D_2,C_2]&=-I_1.
\end{align}
\section{Symplectic group}		\label{SecSympleGp}
%+++++++++++++++++++++++++

\subsection{Iwasawa decomposition}
%-----------------------------
\index{Iwasawa!decomposition!of $\SP(2,\eR)$}

A simple computation shows that $4\times 4$ matrices subject to $A^t\Omega+\Omega A=0$ are given by
\[
  \begin{pmatrix}
A&B\\
C&-A^t
\end{pmatrix}
\]
where $A$ is any $2\times 2$ matrix while $B$ and $C$ are symmetric matrices. Looking at general form \eqref{EqGenslMatr}, we see that the operation to invert the two last column and then to invert the two last lines provides a homomorphism $\phi\colon \gsl(2,\eC)\to \gsp(2,\eR)$. The aim is now to build an Iwasawa decomposition of $\gsp(2,\eR)$ which ``contains'' the one of $\gsl(2,\eC)$.

Using the Cartan involution $\theta(X)=-X^t$, we find the Cartan decomposition
\begin{align}
\iK_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
A&S\\-S&A
\end{pmatrix},
&\iP_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
S&S'\\S'&-S
\end{pmatrix}
\end{align}
where $S$ and $S'$ are any symmetric matrices while $A$ is a skew-symmetric one. We have $\dim\iK_{\gsp(2,\eR)}=4$ and $\dim\iP_{\gsp(2,\eR)}=6$. It turns out that $\phi(\iK_{\gsl(2,\eC)})\subset\iK_{\gsp(2,\eR)}$ and $\phi(\iP_{\gsl(2,\eC)})\subset \iP_{\gsp(2,\eR)}$. A maximal abelian subalgebra of $\iP_{\gsp(2,\eR)}$ is spanned by the matrices $A'_1$ and $A'_2$ listed below and the corresponding root spaces are:
\begin{itemize}
\item $\gsp(2,\eR)_{(0,0)}$:
\[
  A'_1=
\begin{pmatrix}
1&0\\
0&1\\
&&-1&0\\
&&0&-1
\end{pmatrix},
\quad
A'_2=
\begin{pmatrix}
0&1\\
1&0\\
&&0&-1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,2)}$:
\[
 X'= \begin{pmatrix}
1&-1&\\
1&-1&\\
&&-1&-1\\
&&1&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,-2)}$:
\[
 V'= \begin{pmatrix}
1&1\\
-1&-1\\
&&-1&1\\
&&-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,0)}$:
\[
 W'= \begin{pmatrix}
&&1&0\\
&&0&-1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,2)}$:
\[
  L'=
\begin{pmatrix}
&&1&1\\
&&1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,-2)}$:
\[
  M'=
\begin{pmatrix}
&&1&-1\\
&&-1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,0)}$
\[
Y'=
\begin{pmatrix}
&&0&0\\&&0&0\\
1&0\\0&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,2)}$:
\[
  N'=\begin{pmatrix}
&&0&0\\&&0&0\\
1&-1\\
-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,-2)}$:
\[
  F'=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&1\\
1&1
\end{pmatrix}
\]
\end{itemize}
It is important to notice how do the root spaces of $\gsl(2,\eC)$ embed:
\begin{align}
\phi(I_1)&=A'_1	&\phi(I_2)&=\frac{ V'-X' }{ 2 }\\
\phi(D_1)&=\frac{ L'-M' }{2}	&\phi(D_2)&=-W'\\
\phi(C_1)&=\frac{ F'-N' }{2}	&\phi(C_2)&=Y'.
\end{align}
So $\iN_{\gsp(2,\eR)}$ must at least contain the elements $L'$, $M'$ and $W'$. We complete the notion of positivity by $V'$. The Iwasawa algebra reads
\[
\begin{split}
\iA_{\gsp(2,\eR)}&=\{ B_1,B_2 \}\\
\iN_{\gsp(2,\eR)}&=\{ L',M',W',V' \}
\end{split}
\]
with
\begin{align*}
[L',V']&=-4W'	&[W',V']&=-2M'\\
[B_1',L']&=2L'	&[B'_2,M']&=2M'\\
[B_1',W']&=W'	&[B_2',W']&=W'\\
[B_1',V']&=-V'	&[B_2',V']&=V'
\end{align*}
where $B'_1=\frac{ 1 }{2}(A'_1+A'_2)$ and $B_2=\frac{ 1 }{2}(A_1'-A_2')$. The generators of $\iK_{\gsp(2,\eR)}$ are
\begin{align*}
K'_t&=
\begin{pmatrix}
&&1&0\\
&&0&1\\
-1&0\\
0&-1
\end{pmatrix}
	&K'_1&=
\begin{pmatrix}
0&1\\-1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}\\
K'_2&=
\begin{pmatrix}
&&0&1\\&&1&0\\0&-1\\-1&0
\end{pmatrix}
	&K'_3&=
\begin{pmatrix}
&&1&0\\
&&0&-1\\
-1&0\\
0&1
\end{pmatrix}.
\end{align*}
Notice that $[K'_t,K'_i]=0$ for $i=1$, $2$, $3$.


\subsection{Isomorphism}		\label{SubSecIsosp}
%-----------------------

The following provides an isomorphism $\psi\colon \so(2,3)\to \gsp(2,\eR)$:
\begin{align*}
\psi(H_i)&=B'_i		&\psi(u)&=K'_t\\
\psi(W)&=W'		&\psi(R_1)&=\frac{ 1 }{2}K'_1\\
\psi(M)&=M'		&\psi(R_2)&=\frac{ 1 }{2}K'_2\\
\psi(L)&=L'		&\psi(R_3)&=\frac{ 1 }{2}K'_3\\
\psi(V)&=\frac{ 1 }{2}V'
\end{align*}
where the $R_i$'s are the generators of the $\so(3)$ part of $\sK_{\so(2,3)}$ satisfying the relations $[R_i,R_j]=\epsilon_{ijk}R_k$. It is now easy to check that the image of the embedding $\phi\colon \gsl(2,\eC) \to \gsp(2,\eR)$ is exactly $\so(1,3)$, so that
\begin{equation}
\psi^{-1}\circ\phi\colon \gsl(2,\eC)\to \sH
\end{equation}
is an isomorphism which realises $\sH$ as subalgebra of $\gsp(2,\eR)$. This circumstance will be useful in defining a spin structure on $AdS_4$.

One can prove that the kernel of the adjoint representation of $\SP(2,\eR)$ on its Lie algebra is $\pm\mtu$, in other words, $\Ad(a)=\id$ if and only if $a=\pm\mtu$. We define a bijective map $h\colon \SO(2,3)\to \SP(2,\eR)/\eZ_2$ by the requirement that
\begin{equation}		\label{Eqdefhspsl}
  \psi\big( \Ad(g)X \big)=\Ad\big( h(g) \big)\psi(X)
\end{equation}
for every $X\in\so(2,3)$. The following is true for all $\psi(X)$:
\[
\begin{split}
\Ad\big(h(gg'\big)) \psi(X)&=\psi\Big( \Ad(g)\big( \Ad(g')X \big) \Big)\\
			&=\Ad\big( h(g) \big)\psi\big( \Ad(g')X \big)\\
			&=\Ad\big( h(g)h(g') \big)\psi(X),
\end{split}
\]
 the map $h$ is therefore a homomorphism. If an element $a\in \SP(2,\eR)$ reads $a= e^{X_A} e^{X_N} e^{X_K}$ in the Iwasawa decomposition, the property $\Ad(a)\psi(X)=\psi\big( \Ad(g)X \big)$ holds for the element\label{PgSolhpsiSP} $g= e^{\psi^{-1}X_A} e^{\psi^{-1}X_N} e^{\psi^{-1}X_K}$ of $\SO(2,3)$. This shows that $h$ is surjective.

\subsection{Reductive structure on the symplectic group}		\label{SubSecRedspT}
%-------------------------------------------------------

A lot of structure of $\so(2,3)$, such as the reductive homogeneous space decomposition as $\sQ\oplus\sH$, can be immediately transported from $\so(2,3)$ to $\gsp(2,\eR)$. Indeed, let $\mT=\psi(\sQ)$ and $\mI=\phi\big( \gsl(2,\eC) \big)$. We have the direct sum decomposition
\[
\gsp(2,\eR)=\mT\oplus\mI.
\]
 Let $X\in\mT\cap\mI$, then $\psi^{-1}X$ belongs to $\sQ\cap\sH$ which only contains $0$. The fact that $\psi$ is an isomorphism yields that $X=0$. Since $\psi$ preserves linear independence, a simple dimension counting shows that the sum actually spans the whole space.

Putting $g=h^{-1}(a)$ in the definition \eqref{Eqdefhspsl} of $h$, we find
\[
  \psi\left( \Ad\big( h^{-1}(a) \big)X \right)=\Ad(a)\psi(X).
\]
Considering a path $a(t)$ with $a(0)=e$, we differentiate this expression with respect to $t$ at $t=0$ we find
\[
  \ad(dh^{-1}\dot a)X=d\psi^{-1}\big( \ad(\dot a)\psi(X) \big)=\ad(d\psi^{-1}\dot a)(d\psi^{-1}\psi X),
\]
but $d\psi=\psi$ because $\psi$ is linear, hence $[dh^{-1}\dot a,X]=[\psi^{-1}\dot a,X]$ for all $X\in \so(2,3)$ and $\dot a\in \gsp(2,\eR)$. We deduce that $(dh^{-1})_e=\psi^{-1}$. We define
\begin{align*}
	\theta_{\gsp}&=\id|_{\iK_{\gsp}}\oplus(-\id)|_{\iP_{\gsp}}\\
	\sigma_{\gsp}&=\id|_{\mT}\oplus(-\id)|_{\mI}.
\end{align*}
We can check that $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$ and $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$. Then it is clear that
\[
  [\sigma_{\gsp},\theta_{\gsp}]=0
\]
using the corresponding vanishing commutator in $\so(2,3)$. We denote $\mT_a=dL_a\mT$ and the fact that $dp= d\pi\circ dh^{-1}= d\pi\circ \psi^{-1}$ shows that $dp(\mT_a)$ is a basis of $T_{p(a)}(G/H)$. So we consider the basis $t_i=\psi(q_i)$ of $\mT$ and the corresponding left invariant vector fields $\tilde t_i(a)=dL_at_i$.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Heisenberg group and algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Let $V$ be a symplectic vector space with the symplectic form $\Omega$. The \hypertarget{HyperHeisenberg}{Heisenberg algebra} build on $V$ is the vector space
\begin{equation}
	\pH(V,\Omega)=V\oplus \eR E
\end{equation}
endowed with the bracket defined by
\begin{enumerate}

	\item
		$[\pH(V,\Omega),E]=0$,
	\item
		$[v,w]=\Omega(v,w)E$ for every $v,w\in V$.

\end{enumerate}
The first conditions makes $E$ central in $\pH$.

The Heisenberg group is, as set, the same as the algebra: $H=V\oplus\eR E$ with the product
\begin{equation}		\label{EqProduitHeisenbergGp}
	g_1\cdot g_2=g_1+g_2+\frac{ 1 }{2}[g_1,g_2]
\end{equation}
where the bracket is the one in the Lie algebra. Direct computations show that this product is associative, the neutral is $(0,0)$ and that the inverse is given by
\begin{equation}
	g^{-1}=-g.
\end{equation}
We are now going to prove that the Lie algebra of that group actually is $\pH(V,\Omega)$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exponential mapping}
%---------------------------------------------------------------------------------------------------------------------------

Let us build the exponential map between the Heisenberg algebra and its group. Let $(x,\tau)\in T_eH$ and consider $g(s)= e^{s(x,\tau)}=\big( v(s),h(s) \big)$.  This map is subject to the following three relations:
\begin{enumerate}

	\item
		$g(s)g(t)=g(s+t)$,
	\item
		$g(0)=0$,
	\item
		$g'(01)=(x,t)$.

\end{enumerate}
Taking the derivative of the first one with respect to $s$ and taking into account $v'(0)=x$ and $h'(0)=\tau$, we find
\begin{subequations}
	\begin{align}
		\Dsdd{g(s)+g(t)+\frac{ 1 }{2}\big[ g(s),g(t) \big]  }{s}{0}&=\Dsdd{ \big( v(s+t),h(s+t) \big) }{s}{0}\\
		\Dsdd{ v(s)+v(t),h(s)+h(t)+\frac{ 1 }{2}\Omega\big( v(s),v(t) \big) }{s}{0}	&=\big( v'(t),h'(t) \big)\\
		\Big( x,\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big) \Big)&=\big( v'(t),h'(t) \big)
	\end{align}
\end{subequations}
We deduce that $v'(t)=x$ and $h'(t)=\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big)$, so that $v(t)=tx$ and $h(t)=t\tau$. The exponential mapping is thus given by the identity:
\begin{equation}
	\exp(x,\tau)=(x,\tau).
\end{equation}

In order to prove that the law \eqref{EqProduitHeisenbergGp} accepts the Heisenberg algebra as Lie algebra, we need to compute the adjoint action.
\begin{equation}
	\begin{aligned}[]
		\Ad( e^{t(x,\tau)})(x',\tau')&=\Dsdd{ \AD( e^{t(x,\tau)}) e^{s(x',\tau')} }{s}{0}\\
		&=\Dsdd{ (tx,t\tau)(sx',s\tau')(-tx,-t\tau) }{s}{0}\\
		&=\Dsdd{ \big(tx+sx',t\tau+s\tau'+\frac{ ts }{2}\Omega(x,x')\big)(-tx,t\tau) }{s}{0}\\
		&=\big( x',\tau'+t\Omega(x,x') \big).
	\end{aligned}
\end{equation}
Now, the Lie algebra bracket is given by
\begin{equation}
	\begin{aligned}[]
		\big[ (x,\tau),(x',\tau') \big]&=\Dsdd{ \Ad( e^{t(x,\tau)})(x',\tau') }{t}{0}\\
			&=\big( 0,\Omega(x,x') \big)\\
			&=\Omega(x,x')E,
	\end{aligned}
\end{equation}
which is the bracket of $\pH(V,\Omega)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Hermitian conjugate, unitary operators}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}
    Let \( \hH\) be an Hilbert space. Let \( A\) be a linear continuous operator on \( \hH\). The map
    \begin{equation}
        \begin{aligned}
            S_A\colon \hH'&\to \hH' \\
            \alpha&\mapsto \alpha\circ A
        \end{aligned}
    \end{equation}
    is well defined and continuous.
\end{lemma}

\begin{proof}
    We have to prove two elements: firstly \( \alpha\circ A\) must be continuous, so that \( S_A\) takes its values in \( \hH'\), and secondly we want \( S_A\) itself to be continuous.

    The fact that \( \alpha\circ A\) is continuous is simply the fact that \( \alpha\) and \( A\) are continuous.

    Now we compute the norm of \( S_A\); first we have
    \begin{equation}
        \| S_A \|=\sup_{\alpha\in \hH'}\frac{ \| S_A(\alpha) \| }{ \| \alpha \| }.
    \end{equation}
    Then we compute
    \begin{equation}
        \| S_A(\alpha) \|=\sup_{\| u \|=1}\| S_A(\alpha)u \|=\sup_{\| u \|=1}\| \alpha(Au) \|\leq \| \alpha \|\| A \|\| u \|=\| \alpha \|\| A \|.
    \end{equation}
    Then we have
    \begin{equation}
        \| S_A \|=\sup_{\alpha\in \hH'}\frac{ \| S_A(\alpha) \| }{ \| \alpha \| }\leq \sup_{\alpha\in \hH'}\frac{ \| \alpha \|\| A \| }{ \| \alpha \| }=\| A \|<\infty.
    \end{equation}
    The proposition \ref{PROPooQZYVooYJVlBd} shows that \( S_A \) is continuous because it is bounded.
\end{proof}

We will use the map 
\begin{equation}
    \begin{aligned}
        \Phi\colon \hH&\to \hH' \\
        y&\mapsto \Phi_y 
    \end{aligned}
\end{equation}
where \( \Phi_y\) is defined by \( \Phi_y(x)=\langle x, y\rangle \). From the Riesz representation theorem \ref{ThoQgTovL} we know that \( \Phi\) is a bijective isometry. For the sake of notational convenience we will write \( \Phi(u)\) for \( \Phi_u\).

Notice the following formula:
\begin{equation}        \label{EQooHWQPooNeYokT}
    \langle u, \Phi^{-1}(\alpha)\rangle =\alpha(u)
\end{equation}
for every \( u\in \hH\) and \( \alpha\in\hH'\).

\begin{propositionDef}[\cite{MonCerveau}]
    Let \( \hH\) be an Hilbert space over \( \eC\). We consider a continuous \( A\colon \hH\to \hH\). There exists an unique linear operator \( B\colon \hH\to \hH\) such that
    \begin{equation}
        \langle Au, v\rangle =\langle u, Bv\rangle 
    \end{equation}
    for every \( u,v\in \hH\).

    The so-defined operator \( B\) is the \defe{hermitian conjugate of \( A\)}{hermitian conjugate} and is denoted \( A^{\dag}\). In other words, \( A^{\dag}\) is defined by the equality
    \begin{equation}        \label{EQooPTUWooPCbNxA}
        \langle Au, v\rangle =\langle u, A^{\dag}v\rangle 
    \end{equation}
    for every \( u,v\in\hH\).
\end{propositionDef}

\begin{proof}
    In two parts.
    \begin{subproof}
        \item[Unicity]
            We have to prove that, \( v\) being given, there exists an unique \( w\) such that
            \begin{equation}        \label{EQooVBJXooOtdmlQ}
                \langle Au, v\rangle =\langle u,w, \rangle 
            \end{equation}
            for every \( u\). Be clear: the same \( w\) must works for every \( u\). The condition \eqref{EQooVBJXooOtdmlQ} can be written as
            \begin{subequations}
                \begin{align}
                    \Phi(v)Au&=\Phi(w)u\\
                    S_A\big( \Phi(v) \big)u&=\Phi(w)u\\
                    S_A\big( \Phi(v) \big)&=\Phi(w),
                \end{align}
            \end{subequations}
            and finally
            \begin{equation}
                    w=\Phi^{-1}\Big( S_A\big( \Phi(v) \big) \Big).      \label{EQooPRVGooUfIELg}
            \end{equation}
            This proves the unicity: \( Bv\) must be given by the expression \eqref{EQooPRVGooUfIELg}.
        \item[Existence]
            We check that the formula
            \begin{equation}        \label{EQooOIROooXUjCWL}
                A^{\dag}=\Phi^{-1}\circ S_A\circ \Phi.
            \end{equation}
            satisfy the properties. Using the formula \eqref{EQooHWQPooNeYokT} we have:
            \begin{subequations}
                \begin{align}
                    \langle u, (\Phi^{-1}\circ S_A\circ \Phi)(v)\rangle &=(S_A\circ\Phi)(v)u\\
                    &=\Phi(v)Au\\
                    &=\langle Au, v\rangle ,
                \end{align}
            \end{subequations}
            so that we see that the operator given by \eqref{EQooOIROooXUjCWL} makes the work.
    \end{subproof}
\end{proof}

\begin{definition}[Unitary, hermitian]      \label{DEFooOKGXooFCzCHu}
    An operator \( A\colon \hH\to \hH\) is \defe{unitary}{unitary operator} if it satisfies
    \begin{equation}
        A^{\dag}A=AA^{\dag}=\id.
    \end{equation}
    An operator \( A\colon \hH\to \hH\) is \defe{hermitian}{hermitian operator} if it satisfies
    \begin{equation}
        A^{\dag}=A.
    \end{equation}
    This was already the definition \ref{DEFooKEBHooWwCKRK}.
\end{definition}

\begin{lemma}
    An unitary operator is an isometry: it preserves the hermitian product on an Hilbert space.
\end{lemma}

\begin{proof}
    Let \( u,v\in \hH\) and \( A\) be an unitary operator on the Hilbert space \( \hH\). We have
    \begin{equation}
        \langle Au, Au\rangle =\langle A^{\dag}Au, v\rangle =\langle u,v, \rangle .
    \end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooJYGRooPTMZwY}
    The hermitian conjugation satisfies:
    \begin{enumerate}
        \item
            \( \langle A^{\dag}u, v\rangle =\langle u, Av\rangle \) for every \( u,v\in \hH\)
        \item
            \( (A^{\dag})^{\dag}=A\).
        \item
            \( (AB)^{\dag}=B^{\dag}A^{\dag}\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    We have
    \begin{equation}
        \langle A^{\dag}u, v\rangle =\overline{ \langle v, A^{\dag}u\rangle  }=\overline{ \langle Av, u\rangle  }=\langle u, Av\rangle .
    \end{equation}
    This proves the first point.

    For the second point, \( (A^{\dag})^{\dag}\) is the unique operator satisfying
    \begin{equation}
        \langle A^{\dag}u, v\rangle =\langle u, (A^{\dag})^{\dag}v\rangle 
    \end{equation}
    for every \( u,v\in \hH\). Using the first point,
    \begin{equation}
        \langle u, (A^{\dag})^{\dag} v\rangle =\langle u, Av\rangle .
    \end{equation}
    This shows that \( \Phi\big( (A^{\dag})^{\dag}v \big)=\Phi(Av)\). Since \( \Phi\) is bijective, \( (A^{\dag})^{\dag}v=Av\) for every \( v\in \hH\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{The group \texorpdfstring{$ \SU(n)$}{SUn} of special unitary operators}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Some settings}
%---------------------------------------------------------------------------------------------------------------------------

We particularize ourself to the finite dimensional Hilbert space \( \hH=\eC^n\). The vector space \( \eC^n\) has a canonical basis; so we use it, and the linear maps \( \eC^n\to \eC^n\) are identified with their matrices in that very basis. Using the conventions described around \eqref{EQooOMSCooGsSBIA} we have 
\begin{equation}
    A(e_j)=\sum_iA(e_j)_ie_i=\sum_iA_{ij}e_i
\end{equation}
where \( \{ e_i \}_{i=1,\ldots, n}\) is the canonical basis of \( \eC^n\).

When \( u\in \eC^n\), we denote by \( u_k\in \eC\) the \( k\)\ieme\ component of \( u\) with respect to the canonical basis. The vector space \( \eC^n\) has an hermitian product\footnote{Definition \ref{DefMZQxmQ}.} given by
\begin{equation}
    \langle u, v\rangle =\sum_{k=1}^nu_k\bar v_k.
\end{equation}

\begin{lemma}       \label{LEMooKEUZooUjQVmp}
    Let \( A\) be a linear continuous operator on the Hilbert space \( \hH\). We have
    \begin{equation}
        \det(A^{\dag})=\overline{ \det(A) }
    \end{equation}
    where the bar stands for the complex conjugate.

    If \( A\) is unitary, then \( \det(A)\in S^1\) where \( S^1\) is the set of elements of norm \( 1\) in \( \eC\).
\end{lemma}

\begin{proof}
    We use for the determinant the formula given by lemma \ref{LEMooEZFIooXyYybe}:
    \begin{equation}
        \det(A^{\dag})=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\langle e_{\sigma(i)}, A^{\dag}e_i\rangle =  \sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n \langle Ae_{\sigma(i)}, e_i\rangle = \sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\overline{ e_i,Ae_{\sigma(i)} }.
    \end{equation}
    At this point we re-index the product and we make a change of variable $\sigma\to \sigma^{-1}$ for the sum (as in the proof for the transposed matrix, lemma \ref{LEMooCEQYooYAbctZ}).

    If the operator is unitary,
    \begin{equation}
        1=\det(AA^{\dag})=\det(A)\det(A^{\dag})=| \det(A) |^2.
    \end{equation}
    Thus \( | \det(A) |=1\).
\end{proof}


\begin{lemma}
    As for the matrices,
    \begin{equation}
        (U^{\dag})_{ij}=\overline{ U_{ji} }.
    \end{equation}
\end{lemma}

\begin{proof}
    The matrix convention are summarized around the equation \eqref{EQooDSKBooQkgtWv}. We write the definition \eqref{EQooPTUWooPCbNxA} for the basis vectors:
    \begin{equation}
        \langle Ue_i, e_j\rangle =\langle e_i, U^{\dag}e_j\rangle .
    \end{equation}
    This means \( U_{ji}=\overline{ \langle U^{\dag}e_j, e_i\rangle  }=\overline{ U^{\dag}_{ij} }\).
\end{proof}

\begin{definition}[\cite{BIBooUXTFooXTeMOn}]        \label{DEFooVIQUooQbnYMu}
    The group of the unitary operators with determinant \( 1\) is \( \SU(n)\). More explicitly,
    \begin{equation}
        \SU(n)=\{U\in \GL(n,\eC)\tq U^{\dag}U=\id,\det(U)=1\}.
    \end{equation}
\end{definition}

\begin{proposition}     \label{PROPooAKZEooEfpxPp}
    The Lie algebra \( \su(n)\) is given by
    \begin{equation}
        \su(n)=\{X\in\gl(2,\eC)\tq X^{\dag}=-X,\tr(X)=0\}.
    \end{equation}
\end{proposition}

\begin{proposition}     \label{PROPooYXPRooBgikdE}
    A unitary endomorphism of \( \eC^n\) is diagonalizable by a unitary operator.
\end{proposition}

\begin{proof}
    A unitary operator \( U\) is normal (definition \ref{DefWQNooKEeJzv}), so that the spectral theorem \ref{ThogammwA} provides an unitary matrice \( V\) such that \( V^{\dag}UV\) is diagonal.
\end{proof}

\begin{proposition}     \label{PROPooZBJSooEIguXR}
    A special unitary matrix is the exponential of a skew-hermitian matrix with vanishing trace. 
\end{proposition}

\begin{proof}
    Let \( U\in \SU(n)\). We prove that here exists an hermitian operator \( H\) with \( U= e^{iH}\) and \( \tr(H)=0\). Then \( iH\) is the requested skew-hermitian operator.

    By mean of the proposition \ref{PROPooYXPRooBgikdE}, we diagonalize it with the unitary operator \( S\). We have 
    \begin{equation}
        U=SDS^{\dag},
    \end{equation}
    and
    \begin{equation}
        \det(U)=1,
    \end{equation}
    and
    \begin{equation}
        D=\begin{pmatrix}
            e^{i\varphi_1}    &       &       \\
                &   \ddots    &       \\
                &       &    e^{i\varphi_n}
        \end{pmatrix}.
    \end{equation}
    Since \( U\in\SU(n)\) we have
    \begin{equation}
        1=\det(U)=\det(S)\det(D)\det(S^{\dag})=| \det(S) |^2\det(D)=\det(D).
    \end{equation}
    Thus \( \det(D)=1\) and we deduce
    \begin{equation}
        \sum_{i=1}^n\varphi_i=2k\pi
    \end{equation}
    for some \( k\in \eZ\). We consider the two following  operators:
    \begin{equation}
        \Delta=\begin{pmatrix}
            \varphi_1    &       &       \\
                &   \ddots    &       \\
                &       &   \varphi_n
        \end{pmatrix},
    \end{equation}
    and \( H=S\Delta S^{\dag}\). Since \( \Delta^{\dag}=\Delta\), we have \( H^{\dag}=H\), so that \( H\) is hermitian. We also have \( U= e^{iH}\). Indeed, since \( (S\Delta S^{\dag})^k=S\Delta^kS^{\dag}\) we have
    \begin{equation}
        e^{iH}=\sum_{k=0}^{\infty}\frac{ (iH)^k }{ k! }=\sum_k\frac{ i^k }{ k! }(S\Delta S^{\dag})^k=\sum_k\frac{ i^k }{ k! }S\Delta^kS^{\dag}=S e^{i\Delta}S^{\dag}=SDS^{\dag}=U.
    \end{equation}
    We also have \( \tr(H)=2k\pi\).
    
    This means that \( H\) is almost the operator we are searching for. It is easy to modify \( H\) in order to get our answer. We set
    \begin{equation}
        \Delta'=\begin{pmatrix}
            \varphi_1    &       &       \\
                &   \ddots   &       \\
                &       &   \varphi_n-2k\pi
        \end{pmatrix}.
    \end{equation}
    This matrix satisfies \(   e^{i\Delta'}= e^{i\Delta}\) and \( \tr(\Delta')=0\). If we set \( H'=S\Delta'S^{\dag}\) we still have \( \tr(H')=\tr(\Delta')=0\) because of the cyclic invariance of the trace. And finally the operator \( H'\) satisfies
    \begin{equation}
        e^{iH'}=S e^{i\Delta'}S^{\dag}= e^{iH}=U.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{The center of \texorpdfstring{$ \SU(n)$}{SUn} }
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooLMGHooKrKpsa}
    The center\footnote{The set of elements which commute with all the elements, see definition \ref{defGroupeCentre}.} of the group \( \SU(n)\), \( \mZ\big( \SU(n) \big)\), is the subgroup of the element of the form
    \begin{equation}
        e^{i\varphi}\id.
    \end{equation}
    In particular, the center of \( \SU(2)\) is \( \{ \id,-\id \}\).
\end{proposition}

\begin{proof}
    Let \( Z\in\mZ\big( \SU(n) \big)\). It can be diagonalized by an unitary matrix \( S\):
    \begin{equation}
        SZS^{\dag}=D.
    \end{equation}
    The operator \( S\) belongs to \( \gU(n)\) while \( Z\) only commutes with the elements of \( \SU(n)\). Thus we cannot immediately deduce \( Z=D\).  However, the operator \( S_0 = S/\det(S)\) diagonalizes \( Z\) as well as \( S\): \( S_0ZS_0^{\dag}=D\). But since \( S_0\) is in \( \SU(n)\) we can deduce \( Z=D\).

    Long story short, the elements of \( \mZ\big( \SU(n) \big)\) are diagonal. Let \( Ze_i=\lambda_ie_i\). We consider the operator \( A\) of \( \SU(n)\) given by the formula
    \begin{subequations}
        \begin{align}
            Ae_1&=-e_2\\
            Ae_2&=e_1\\
            Ae_k&=e_k\qquad\text{otherwise.}
        \end{align}
    \end{subequations}
    This is the element of \( \SU(n)\) which permutes \( e_1\) with \( e_2\) (with a sign for the sake of the determinant). Since \( ZA=AZ\), we have
    \begin{equation}
        AZe_1=\lambda_1Ae_1=-\lambda_1e_1
    \end{equation}
    and 
    \begin{equation}
        ZAe_1=Z(-)e_2=-\lambda_2e_2.
    \end{equation}
    So we have \( \lambda_1=\lambda_2\). The same being true not only for \( 1\) and \( 2\) but for the other ones, we know that the numbers \( \lambda_i\) are all equal.

    So far, \( Z=\lambda\id\). The value of \( \lambda\) is not arbitrary: we must impose \( Z\in\SU(n)\). The determinant condition provides
    \begin{equation}        \label{EQooZZPJooCeKPDD}
        1=\det(Z)=\lambda^n
    \end{equation}
    and the unitary condition imposes 
    \begin{equation}
        1=\langle e_i, e_i\rangle =\langle Ze_i, Ze_i\rangle =| \lambda |^2,
    \end{equation}
    so that \( | \lambda |^2=1\). This conditions imposes \( \lambda= e^{i\varphi}\) for some \( \varphi\in \eR\) while the condition \eqref{EQooZZPJooCeKPDD} furnishes
    \begin{equation}
        e^{in\varphi}=1.
    \end{equation}
    This shows the existence of \( k\in \eZ\) such that \( in\varphi=2ki\pi\) and finally
    \begin{equation}
        \varphi=\frac{ 2k\pi }{ n }.
    \end{equation}
    
    In the case \( n=2\) we have \( \varphi=k\pi\) and \(  e^{i\varphi}=\pm 1\).
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representations of \texorpdfstring{$ U(1)$}{U(1)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}
    Let \( V\) be a complex vector space of dimension \( 1\). Let \( \xi\neq 0\in V\). If \( \langle ., .\rangle \) is an hermitian product on \( V\), there exists \( m\in \eR\) such that
    \begin{equation}
        \langle z_1\xi, z_2\xi\rangle =mz_1\bar z_2.
    \end{equation}
\end{proposition}

\begin{proof}
    Every element of \( V\) can be written under the form \( z\xi\) for some \( z\in \eC\). The properties of an hermitian product say
    \begin{equation}
        \langle z_1\xi, z_2\xi\rangle =z_1\bar z_2\langle \xi, \xi\rangle 
    \end{equation}
    and \( \langle \xi, \xi\rangle \in \eR\).
\end{proof}

\begin{normaltext}
    What we write \( S^1\) and \( \gU(1)\) are the same thing. When we write \( S^1\) we have in mind the geometrical object (with a measure) made of the complex numbers of norm \( 1\); when we write \( \gU(1)\) we have in mind the group structure. But it's the same.

    However the generalizations \( S^2\) and \( \gU(2)\) are not the same.
\end{normaltext}

\begin{normaltext}[\cite{MonCerveau}]
    We are searching now for the irreducible representations of \( U(1)\). More precisely we will determine the irreducible continuous representations of \( U(1)\). Here the fact to be «continuous» means that \( \rho\colon U(1)\to \GL(V)\) is continuous; in particular, \( V\) has to be a topological vector space.

    This is not a restriction because the theorem \ref{THOooFFJGooCekFQc} shows that every irreducible representation of \( U(1)\) has dimension \( 1\).
\end{normaltext}

\begin{proposition}[Irreducible representations of \( U(1)\)]       \label{PROPooLWWEooUmqbRA}
    Let \( m\in \eZ\). We consider
    \begin{equation}        \label{EQooXPXKooJasMyY}
        \begin{aligned}
            T_m\colon U(1)&\to \GL(\eC) \\
            T_m(g)z&=g^mz.
        \end{aligned}
    \end{equation}
    \begin{enumerate}
        \item
            The formula \eqref{EQooXPXKooJasMyY} defines a representation of \( U(1)\).
        \item
            The representation \( T_m\) is irreducible.
        \item
            The representation \( T_m\) is continuous.
        \item
            If \( m\neq l\), then the representations \( T_m\) and \( T_l\) are not equivalent.
        \item       \label{ITEMooUPVQooQddQOJ}
            Every continuous\footnote{For the norm of proposition \ref{PROPooNTCFooEcwZwt} on \( V\) and the corresponding operator norm one on \( \GL(V)\), definition \ref{DefNFYUooBZCPTr}.} irreducible representation of \( U(1)\) is equivalent to one of them.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Several points.
    \begin{subproof}
        \item[It is a representation]
            Since \( U(1)\) is abelian, \( (g_1g_2)^m=g_1^mg_2^m\).
        \item[Irreducible]
            The representation \( T_m\) is irreducible because the vector space is \( \eC\) which has dimension \( 1\).
        \item[Continuous]
            Let \( g_k\stackrel{U(1)}{\longrightarrow}g\). We have
            \begin{equation}
                \| T_m(g_k)-T_m(g) \|=\sup_{| z |1=1}| g_k^m-g^mz |=| g_k^m-g^m |\to 0.
            \end{equation}
            This shows that \( T_m\) is continuous.
        \item[Non equivalence]
            Let \( \psi\colon \eC\to \eC\) be a linear map such that \( T_m(g)\circ\psi=\psi\circ T_l(g)\) for every \( g\in U(1)\). This implies
            \begin{equation}
                g^m\psi(z)=\psi(g^lz),
            \end{equation}
            hence
            \begin{equation}
                g^m\psi(z)=g^l\psi(z).
            \end{equation}
            Taking \( z\) such that \( \psi(z)\neq 0\) we have \( g^m=g^l\) for every \( g\in U(1)\), or \( g^{l-m}=1\). Writing \( g= e^{ix}\) we have
            \begin{equation}
                e^{i(l-m)x=1}
            \end{equation}
            for every \( x\in \eR\). We conclude \( l-m=2k \pi\). Since \( l-m\in \eZ\) the only solution is \( l-m=0\).
            
    \end{subproof}
    At this point, it ``remains'' to prove the point \ref{ITEMooUPVQooQddQOJ}. Let \( (\rho, V)\) be a continuous irreducible representation of \( U(1)\). 
    
    \begin{subproof}
        \item[The function \( \lambda\)]
            Since \( U(1)\) is abelian, \( \dim(V)=1\) (theorem \ref{THOooFFJGooCekFQc}). So there exist a function \( \lambda\colon U(1)\to \eC\) such that 
            \begin{equation}
                \rho(g)=\lambda(g)\id.
            \end{equation}

        \item[\( \lambda\) is continuous]

            The spaces \( V\), \( \GL(V)\) and \( \eC\) are metric, thanks to the restriction we imposed on the topology of \( V\). Let \( g_k\to g\) in \( U(1)\). Since \( \rho\) is continuous we have \( \rho(g_k)\stackrel{\GL(V)}{\longrightarrow}\rho(g)\). From the definition of the operator norm, that implies, for each \( v\in V\) that
            \begin{equation}
                \rho(g_k)\stackrel{V}{\longrightarrow}\rho(g)v,
            \end{equation}
            which means
            \begin{equation}        \label{EQooTPAXooAPSgxP}
                \lambda(g_k)v\stackrel{V}{\longrightarrow}\lambda(g)v.
            \end{equation}
            Using the definition of the topology on \( V\),
            \begin{equation}
                \| \lambda(g_k)v-\lambda(g)v \|=\|\big( \lambda(g_k)-\lambda(g) \big)v \|=\| \lambda(g_k)-\lambda(g) \|\| v \|.
            \end{equation}
            The convergence \eqref{EQooTPAXooAPSgxP} means
            \begin{equation}
                | \lambda(g_k)-\lambda(g) |\| v \|\stackrel{\eR}{\longrightarrow}0,
            \end{equation}
            which implies the convergence \( \lambda(g_k)\to \lambda(g)\), hence the continuity of \( \lambda\).

        \item[\( \lambda\) takes values in \( S^1\)]

            We show that \( |\lambda(g)|=1\) pour tout \( g\in U(1)\). An element of \( U(1)\) reads \( g= e^{2\pi i x}\) with \( x\in \eR\)\footnote{Proposition \ref{PROPooZEFEooEKMOPT}.}.

            If \( x\in \eZ\) we have \( g=1\), so that \( \lambda(g)=1\). If \( x=1/n\) (\( n\in \eZ\)) we have \( g^n=1\), but
            \begin{equation}
                \lambda(g^n)=\lambda(g)^n,
            \end{equation}
            so that \( | \lambda(g) |^n=1\) which proves that \( | \lambda(g) |=1\).

            From here we know that \( |\lambda( e^{2\pi i q})|=1\) for every \( q\in \eQ\).

            Since \( \lambda\) is continuous, the function \( x\mapsto | \lambda( e^{2\pi i x}) |\) is continuous. A continuous function whose value is \( 1\) over \( \eQ\) is constant.

        \item[Functional equation]
            For \( x,y\in \eR\) we have \( \rho\big(  e^{ix} e^{iy} \big)=\rho( e^{ix})\rho( e^{iy})\), but also \( \rho( e^{ix} e^{iy})=\rho( e^{i(x+y)})\). We introduce the notation
            \begin{equation}
                \alpha=\lambda\circ\varphi
            \end{equation}
            where \( \varphi\colon \eR\to U(1)\) is \( \varphi(x)= e^{ix}\). The function
            \begin{equation}
                \begin{aligned}
                    \alpha\colon \eR&\to S^1 \\
                    x&\mapsto \lambda( e^{ix}) 
                \end{aligned}
            \end{equation}
            satisfies
            \begin{subequations}
                \begin{numcases}{}
                    \alpha(0)=1\\
                    \alpha(x+y)=\alpha(x)\alpha(y).
                \end{numcases}
            \end{subequations}
            The proposition \ref{PROPooVJLYooOzfWCd} shows that there exists \( m\in \eR\) such that
            \begin{equation}
                \alpha(x)= e^{imx}.
            \end{equation}
            
            Since \( \alpha(2\pi)=\alpha(0)=1\) we have \(  e^{2\pi im}=1\) and we conclude that \( m\in \eZ\).

        \item[The value of \( \lambda\)]

            We have defined \( \alpha=\lambda\circ \varphi\). Since \( \varphi\) is not a bijection, we cannot write \( \lambda=\alpha\circ \varphi^{-1}\). However for every \( x\in \eR\) we have
            \begin{equation}
                \lambda( e^{ix})=\alpha(x)= e^{imx}=( e^{ix})^m.
            \end{equation}
            So
            \begin{equation}
                \lambda(g)=g^m.
            \end{equation}
            
        \item[Equivalence]

            We prove that \( \rho\) is equivalent to the representations \( T_m\). Let \( \{ v \}\) be a basis of \( V\); we consider the linear map
            \begin{equation}
                \begin{aligned}
                    \psi\colon V&\to \eC \\
                    sv&\mapsto s. 
                \end{aligned}
            \end{equation}
            We have
            \begin{equation}
                \big( T_m(g)\circ \psi \big)(sv)=T_m(g)s=g^ms
            \end{equation}
            while
            \begin{equation}
                \big( \psi\circ\rho(g) \big)(sv)=\psi\big( g^msv \big)=g^ms\psi(v)=g^ms.
            \end{equation}
            So we have
            \begin{equation}
                T_m(g)\circ \psi=\psi\circ\rho(g)
            \end{equation}
            which proves that \( T_m\) and \( \rho\) are equivalent.
    \end{subproof}
\end{proof}

\section{The group \texorpdfstring{$SU(2)$}{SU2}}
%--------------------------------------------------

The group \( \SU(2)\) is already defined in \ref{DEFooVIQUooQbnYMu}.

\begin{proposition}[\cite{BIBooUXTFooXTeMOn,BIBooMXBZooFCLGYe}]       \label{PROPooZMPLooUFyAPW}
    The matrices of \( \SU(2)\) are
    \begin{equation}
        \SU(2)=\{ \begin{pmatrix}
        \alpha    &   -\bar \beta    \\ 
    \beta    &   \bar \alpha    
\end{pmatrix}\tq \alpha,\beta\in \eC,| \alpha |^2+| \beta |^2=1\}.
    \end{equation}
\end{proposition}

\begin{proof}
    We initiate with a matrix \( U=\begin{pmatrix}
        \alpha    &   \beta    \\ 
        \gamma    &   \delta    
    \end{pmatrix}\in \eM(2,\eC)\). Then we impose the conditions. The unitary property gives:
    \begin{equation}
    UU^{\dag}=
    \begin{pmatrix}
    \alpha & \beta \\
    \gamma & \delta
    \end{pmatrix}
    \begin{pmatrix}
    \oalpha & \ogamma \\
    \obeta & \odelta
    \end{pmatrix}
    =
    \begin{pmatrix}
    \alpha\oalpha+\beta\obeta & \alpha\bar \gamma+\beta\bar\delta \\
    \gamma\bar \alpha+\delta\bar\beta & \gamma\ogamma+\delta\odelta
    \end{pmatrix}
    \stackrel{!}{=}
    \begin{pmatrix}
    1  & 0 \\
    0 & 1
    \end{pmatrix}.
    \end{equation}
    Among with the determinant conditions, we have the system
    \begin{subequations}        \label{SUBEQSooGUDNooOoxdSO}
        \begin{numcases}{}
            \alpha\delta-\gamma\beta=1\\
            | \alpha |^2+| \beta |^1=1\\
            | \gamma |^2+| \delta |^2=1\\
            \alpha\bar \gamma+\beta\bar\delta=0.        \label{SUBEQooSPRRooWjAUNi}
        \end{numcases}
    \end{subequations}
    We multiply \eqref{SUBEQooSPRRooWjAUNi} by \( \gamma\), and we substitute \( \gamma\bar \gamma=1-| \delta |^2\)  and \( \gamma\beta=\alpha\delta-1\). What we get is
    \begin{equation}
        \alpha(1-| \delta |^2)+(\alpha\delta-1)\bar \delta=0.
    \end{equation}
    If you develop the products, you see some simplifications and you remain with \( \delta=\bar \alpha\).

    Now we substitute \( \delta=\bar \alpha\) in \eqref{SUBEQooSPRRooWjAUNi} again. We obtain
    \begin{equation}        \label{EQooNOVWooYSTXqJ}
        \alpha(\bar \gamma+\beta)=0.
    \end{equation}
    There are two possibilities: \( \alpha=0\) or \( \alpha\neq 0\).
    \begin{subproof}
    \item[If \( \alpha\neq 0\)]
        In that case the equality \eqref{EQooNOVWooYSTXqJ} produces \( \gamma=-\bar\beta\) and the result is proved.
    \item[If \( \alpha=0\)]
        The system \eqref{SUBEQSooGUDNooOoxdSO} reduces to
        \begin{subequations}
            \begin{numcases}{}
                \gamma\beta=-1 \label{SUBEQooVCDOooTGejzi}\\
                | \beta |^2=1\\
                | \gamma |^2=1.     \label{SUBEQooWJJMooItqDsi}
            \end{numcases}
        \end{subequations}
        There exist \( \theta\in \eR\) such that \( \beta= e^{i\theta}\). The equation \eqref{SUBEQooVCDOooTGejzi} then shows that \( \gamma=- e^{-i\theta}\). The condition \eqref{SUBEQooWJJMooItqDsi} is automatically satisfied. At the end we have the matrix
        \begin{equation}
            U=\begin{pmatrix}
                0    &   \beta    \\ 
                -\bar\beta    &   0    
            \end{pmatrix}.
        \end{equation}
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{\texorpdfstring{$ \SU(2)$}{SU(2)} as compact group}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooGLPQooKOfrjl}
    The group \( \SU(2)\) is compact.
\end{proposition}

\begin{proof}
    The proposition \ref{PROPooZMPLooUFyAPW} show that $\SU(2)$ is contained in a bounded subset of $\eR^8$, and it is clear that $\SU(2)$ is closed in $\eR^8$ because it is defined by equalities.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pauli matrices}
%---------------------------------------------------------------------------------------------------------------------------

We denote by \( V\) the set of the hermitian traceless matrices. These are the elements \( u\in \eM(2,\eC)\) such that
\begin{subequations}
    \begin{align}
        u^{\dag}&=u\\
        \trace(u)&=0.
    \end{align}
\end{subequations}
This is not the Lie algebra \( \su(2)\) because the elements of \( \su(2)\) are anti-hermitian\footnote{Proposition \ref{PROPooSERWooFtxBgV}.}. This set \( V\) is a vector space over \( \eR\), but not over the field \( \eC\) because if \( u\in V\), then \( (iu)^{\dag}=-iu^{\dag}=-iu\neq iu\).

\begin{definition}      \label{DEFooRNTDooTVkPtB}
    The \defe{Pauli matrices}{Pauli matrices} are the following three:
    \begin{equation}
        \begin{aligned}[]
            \sigma_1=\begin{pmatrix}
                0    &   1    \\ 
                1    &   0    
            \end{pmatrix},&&
            \sigma_2=\begin{pmatrix}
                0    &   -i    \\ 
                i    &   0    
            \end{pmatrix},&&
            \sigma_3=\begin{pmatrix}
                1    &   0    \\ 
                0    &   -1    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
\end{definition}

For the sake of notations, we write \( \sigma\) the vector of \( V^3\) given by \( \sigma=(\sigma_1, \sigma_2, \sigma_3)\). This allows us to write combinations like
\begin{equation}        \label{EQooXNRGooZaRQoZ}
    a\cdot \sigma=a_1\sigma_1+a_2\sigma_2+a_3\sigma_3
\end{equation}
when \( a\in \eR^3\). It must be noticed however that the notation «\( a\cdot \sigma\)» is not a scalar product. In particular, the formula \eqref{EQooXNRGooZaRQoZ} depends on the chosen basis \( \{ \sigma_i \}\) of \( V\) and \( \{ e_i \}\) on \( \eR^3\).

\begin{lemma}       \label{LEMooZNCQooLgoReX}
    The Pauli matrice form a basis\footnote{Definition \ref{DEFooNGDSooEDAwTh}.} of the real vector space \( V\) of hermitian traceless matrices.
\end{lemma}

\begin{proof}
    An element of \( V\) is a matrice of the form
    \begin{equation}
        u=\begin{pmatrix}
            a    &   b    \\ 
            c    &   d    
        \end{pmatrix}
    \end{equation}
    with \( a,b,c,d\in \eC\). The condition \( u=u^{\dag}\) imposes the relations \( a=\bar a\), \( d=\bar d\) and \( c=\bar b\), so that
    \begin{equation}
        u=\begin{pmatrix}
            x    &   z    \\ 
            \bar z    &   y    
        \end{pmatrix}.
    \end{equation}
    The trace condition imposes \( x=-y\). Finally a general element of \( V\) has the form
    \begin{equation}
        u=\begin{pmatrix}
            x    &   z    \\ 
            \bar z    &   -x    
        \end{pmatrix}
    \end{equation}
    with \( x\in \eR\) and \( z\in \eC\). We have:
    \begin{equation}
        u=-\imag(z)\sigma_1+\real(z)\sigma_2+x\sigma_3.
    \end{equation}
    This proves that \( \{ \sigma_i \}_{i=1,2,3}\) spans \( V\). 
    
    We still have to prove that \( \{ \sigma_i \}_{i=1,2,3}\) is free. For that, consider \( a,b,c\in \eR\) such that \( a\sigma_1+b\sigma_2+c\sigma_3=0\):
    \begin{equation}
        \begin{pmatrix}
            c    &   a-bi    \\ 
            a+bi    &   -c    
        \end{pmatrix}=\begin{pmatrix}
            0    &   0    \\ 
            0    &   0    
        \end{pmatrix}.
    \end{equation}
    We immediately deduce \( c=0\), \( a+bi=0\) and \( a-bi=0\). Thus \( a=b=c=0\).
\end{proof}

Notice that the hermitian matrices do not form a vector space over \( \eC\) because, if \( X\) is hermitian,
\begin{equation}
    (\lambda X)^{\dag}=\bar \lambda X^{\dag}=\bar \lambda X\neq \lambda X.
\end{equation}

\begin{lemma}
    The matrices \( i\sigma_k\) are unitary.
\end{lemma}

\begin{proof}
    A simple computation show that \( \sigma_k^2=\mtu\) and \( \sigma_k^{\dag}=\sigma_k\), so that 
    \begin{equation}
        (i\sigma_k)(i\sigma_k)^{\dag}=(i\sigma_k)(-i\sigma_k)=\sigma_k^2=\mtu.
    \end{equation}
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Some relations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{lemma}       \label{LEMooIBJMooTYnooZ}
    For every \( i,j=1,2,3\) we have the formula
    \begin{equation}
        \sigma_i\sigma_j=\delta_{ij}\mtu+i\sum_m\epsilon_{ijm}\sigma_m.
    \end{equation}
    In particular \( \sigma_i^2=\mtu\).
\end{lemma}

\begin{proof}
    Explicit matricial computation.
\end{proof}

\begin{lemma}       \label{LEMooJRWXooMkzRnk}
    We have the commutator\footnote{We are adult here; I believe you will not confuse the \( i\) of the index and the \( i\) of the imaginary numbers.}
    \begin{equation}
        [\sigma_i,\sigma_j]=2i\sum_k\epsilon_{ijk}\sigma_k.
    \end{equation}
\end{lemma}

\begin{proof}
    This is a computation using the lemma \ref{LEMooIBJMooTYnooZ}:
    \begin{equation}
        [\sigma_i,\sigma_j]=\delta_{ij}\mtu+\sum_ki\epsilon_{ijk}\sigma_k-\delta_{ji}\mtu-\sum_{k}i\epsilon_{jik}\sigma_k=2i\sum_k\epsilon_{ijk}\sigma_k
    \end{equation}
    where we used the fact that \( \epsilon_{jik}=-\epsilon_{ijk}\).
\end{proof}

\begin{lemma}       \label{LEMooLNCSooPHsVut}
    If \( a,b\in \eR^3\) we have
    \begin{equation}
        (a\cdot \sigma)(b\cdot \sigma)=(a\cdot b)\mtu+i(a\times b)\cdot \sigma.
    \end{equation}
\end{lemma}

\begin{proof}
    We use lemme \ref{LEMooIBJMooTYnooZ}:
    \begin{subequations}
        \begin{align}
            (a\cdot b)(b\cdot \sigma)&=\big( \sum_i a_i\sigma_i \big)(\sum_jb_j\sigma_j)\\
            &=\sum_{ij}a_ib_j\sigma\sigma_j\\
            &=\sum_{ij}a_ib_j(\delta_{ij}\mtu+\sum_mi\epsilon_{ijm}\sigma_m)
        \end{align}
    \end{subequations}
    Then we use the property \( \sum_{kl}a_kb_l\epsilon_{klm}=(a\times b)_m\).
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Isomorphism with \( \eR^3\)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The following lemme does not aims to provide a norm on \( V\). The norm on \( V\) is already the operator norm:
\begin{equation}
    \|X  \|=\sup_{| z |=1}| Xz |
\end{equation}
where \( z\in \eC\) and \( X\in V\). This is not something new.

In the same perspective, the elements of \( \SU(2)\) are normed to \( 1\) because, if \( U\in \SU(2)\),
\begin{equation}
    \| U \|_{\SU(2)}=\sup_{| z |=1}| Uz |=\sup\langle Uz, Uz\rangle =\sup_{| z=1 |}\langle U^{\dag}Uz, z\rangle =\sup_{| z |=1} \langle z, z\rangle=1.
\end{equation}

\begin{lemma}       \label{LEMooRFBTooIRDbEq}
    The map
    \begin{equation}
        \begin{aligned}
            \phi\colon \eR^3&\to V \\
            a&\mapsto a\cdot \sigma 
        \end{aligned}
    \end{equation}
    is a vector space isomorphism and satisfies
    \begin{equation}
        \det\big( \phi(x) \big)=-\| x \|^2.
    \end{equation}
\end{lemma}
 
\begin{proof}
    Some immediate facts:
    \begin{itemize}
        \item \( \phi\) is linear,
        \item \( \phi\) is bijective because \( \{ \sigma_i \}_{i=1,2,3}\) is a basis (lemme \ref{LEMooZNCQooLgoReX}).
        \item Thus \( \phi\) is a vector space isomorphism.
    \end{itemize}
    The formula \( \det\big( \phi(x) \big)=-\| x \|^2\) is a computation:
    \begin{subequations}
        \begin{align}
        \det\big( \phi(x) \big)&=\det\begin{pmatrix}
            x_3    &   x_1-ix_2    \\ 
            x_1+ix_2    &   -x_3
        \end{pmatrix}\\
        &=-x_3^2-(x_1-ix_2)(x_1+ix_2)\\
        &=-(x_1^2+x_2^2+x_3^2)\\
        &=-\| x \|^2.
        \end{align}
    \end{subequations}
\end{proof}

Some more properties about the Pauli matrices and the map \( \phi\).
\begin{proposition}[\cite{ooJQZGooElGniq}]
    Let \( x,y\in \eR^3\). We have
    \begin{enumerate}
        \item       \label{ITEMooDDRNooGZASBN}
            $[\sigma_i,\sigma_j]=2i\sum_k\epsilon_{ijk}\sigma_k$.
        \item       \label{ITEMooXORKooXFwQhR}
            $\phi(x\times y)=\frac{1}{ 2i }[\phi(x),\phi(y)]$.
        \item       \label{ITEMooREMBooLPVnxz}
            \( \tr\big( \phi(x)\phi(y) \big)=2x\cdot y\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Several points.

    \begin{subproof}
        \item[Formula \ref{ITEMooDDRNooGZASBN}]
            We use the product formula of lemma \ref{LEMooIBJMooTYnooZ}:
            \begin{equation}
                [\sigma_i,\sigma_j]=\sigma_i\sigma_j-\sigma_j\sigma_i=\delta_{ij}\id+i\sum_k\epsilon_{ijk}\sigma_k-\delta_{ij}\id-i\sum_k\epsilon_{jik}\sigma_k.
            \end{equation}
            Using the fact that \( \epsilon_{ijk}=-\epsilon_{jik}\) we get the result.
        \item[Formula \ref{ITEMooXORKooXFwQhR}]
            We have
            \begin{equation}
                [\phi(x),\phi(y)]=[x\cdot \sigma,y\cdot \sigma]=\sum_{ij}x_iy_j[\sigma_i,\sigma_j].
            \end{equation}
            Substituting the first result and using the formula \( \sum_{ij}x_iy_j\epsilon_{ijk}=(x\times y)_k\)\footnote{Definition \ref{DEFooTNTNooRjhuJZ}.} we get
            \begin{equation}
                [\phi(x),\phi(y)] = \sum_{ijk}x_iy_j2i\epsilon_{ijk}\sigma_k=2i\sum_k(x\times y)_k\sigma_k=2i(x\times y)\cdot \sigma=2i\phi(x\times y).
            \end{equation}
        \item[Formula \ref{ITEMooREMBooLPVnxz}]
            Using formula of lemma \ref{LEMooLNCSooPHsVut},
            \begin{subequations}
                \begin{align}
                    \tr\big( \phi(x)\phi(y) \big)&=\tr\big( (x\cdot \sigma)(y\cdot \sigma) \big)\\
                    &=\sum_i\big[ (x\cdot \sigma)(y\cdot \sigma) \big]_{ii}\\
                    &=\tr\big( (x\cdot y)\mtu_2+i(x\times y)\cdot \sigma \big)      \label{SUBEQooRJKRooNrLhhV}\\
                    &=(x\cdot y)\tr(\mtu_2)\\
                    &=2(x\cdot y).
                \end{align}
            \end{subequations}
            We used the fact that the Pauli matrice have vanishing trace, so that the second term in \eqref{SUBEQooRJKRooNrLhhV} is zero.
    \end{subproof}
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Path connection}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{proposition}     \label{PROPooLEKXooSXPhRX}
    The Lie groups \( \SO(3)\) and \( \SU(2)\) are path connected.
\end{proposition}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{One representation}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We still consider \( V\), the real vector space of hermitian matrices with vanishing trace. Thanks to the lemma \ref{LEMooRFBTooIRDbEq} we define the following norm on \( V\):
\begin{equation}
    \| X \|=\| \phi^{-1}(X) \|_{\eR^3}=-\det(X).
\end{equation}

For the sake of notational convenience in the proof of the next proposition, we introduce the maps \( L\) and \( R\).
\begin{lemma}       \label{LEMooQVYXooQFNaGc}
    The maps
    \begin{equation}
        \begin{aligned}
            L\colon \GL(V)&\to \End\big( \End(V) \big) \\
            L(A)X&=AX
        \end{aligned}
    \end{equation}
    and
    \begin{equation}
        \begin{aligned}
            R\colon \GL(V)&\to \End\big( \End(V) \big) \\
            R(A)X&=XA
        \end{aligned}
    \end{equation}
    are continuous.
\end{lemma}

\begin{proof}
    We prove our statement for \( L\). Let \( A_k\stackrel{\End(V)}{\longrightarrow}A\). We want to prove that
    \begin{equation}
        \| L(A_k)-L(A) \|_{\End\big( \End(V) \big)}\to 0.
    \end{equation}
    Using the definition of the operator norm\footnote{Definition \ref{DefNFYUooBZCPTr}.}, and the fact that it is an algebra norm (lemme \ref{LEMooFITMooBBBWGI}),
    \begin{subequations}
        \begin{align}
            \| L(A_k)-L(A) \|&=\sup_{X\in \End(V)}\frac{ \| L(A_k)X-L(A)X \| }{ \| X \| }\\
            &=\sup_{X\in \End(V)}\frac{ \| (A_k-A)X \|_{\End(V)} }{ \| X \| }\\
            &\leq \sup_{X\in\End(V)}\| A_k-A \|\\
            &\to 0.
        \end{align}
    \end{subequations}
    Thus \( L\) is continuous by proposition \ref{PROPooJYOOooZWocoq}.
\end{proof}

\begin{proposition}     \label{PROPooRQUZooAoZzwx}
    We still consider \( V\), the real vector space of hermitian matrices\footnote{Definition \ref{DEFooOKGXooFCzCHu}.} with vanishing trace. Let
    \begin{equation}
        \begin{aligned}
            \rho\colon \SU(2)&\to \End(V) \\
            \rho(U)X&=UXU^{\dag}.
        \end{aligned}
    \end{equation}
    Then
    \begin{enumerate}
        \item
            The map \( \rho\) is well defined: \( \rho(U)X\in V\) for every \( U\in \SU(2)\) and \( X\in V\).
        \item
            The map \( \rho\) is a representation of \( \SU(2)\) on \( V\) by isometries,
        \item       \label{ITEMooBZUQooNXNVfs}
            for each \( U\) the map \( \rho(U)\colon V\to V\) is continuous,
        \item       \label{ITEMooGHZYooQuabWb}
            the map \( \rho\colon \SU(2)\to \End\big( \End(V)\big) \) is continuous.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Let us prove that \( \rho(U)X\in V\). First, using the properties of lemma \ref{LEMooJYGRooPTMZwY},
    \begin{equation}
        (UXU^{\dag})^{\dag}=(U^{\dag})^{\dag}X^{\dag}U^{\dag}=UXU^{\dag}.
    \end{equation}
    Then, with the cyclic invariance of the trace (lemma \ref{LEMooUXDRooWZbMVN}),
    \begin{equation}
        \tr(UXU^{\dag})=\tr(U^{\dag}UX)=\tr(X)=0.
    \end{equation}
    So \( UXU^{-1}=UXU^{\dag}\in V\).

    The fact that \( \rho(U)\) is linear is a small computation. It is a representation because
    \begin{equation}
        \rho(U_1)\rho(U_2)X=\rho(U_1)U_2XU_2^{\dag}=U_1U_2XU_2^{\dag}U_1^{\dag}=\rho(U_1U_2)X.
    \end{equation}
    
    For the isometry part, the determinant being multiplicative (proposition \ref{PROPooHQNPooIfPEDH}),
    \begin{equation}
        \| UXU^{\dag} \|=-\det(UXU^{\dag})=-\det(U)\det(X)\det(U^{\dag})=-| \det(U) |\det(X).
    \end{equation}
    Since \( U\in\SU(2)\) we have \( | \det(U) |=1\) and then \( \| \rho(U)X \|=\| X \|\).

    The last point to check is the continuity of \( \rho\colon \SU(2)\to \End(V)\). With the notations of lemma \ref{LEMooQVYXooQFNaGc} we have \( \rho(U)=L(U)\circ R(U^{\dag})\) while \( L(U)\) and \( R(U)\) are continuous\footnote{They are matrix multiplication.}. This is point \ref{ITEMooBZUQooNXNVfs} of continuity.

    The point \ref{ITEMooGHZYooQuabWb} of the continuity statement is more subtle. It is done in proposition \ref{PROPooJGNFooEwtNmJ}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Link with \texorpdfstring{$ \SO(3)$}{SO(3)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooYTTJooYpPYLT}]     \label{PROPooGEHAooPCReoU}
    The map
    \begin{equation}        \label{EQooSOZTooTIkONx}
        \begin{aligned}
            f\colon \SU(2)&\to \SO(3) \\
            U&\mapsto \phi^{-1}\circ \rho(U)\circ\phi 
        \end{aligned}
    \end{equation}
    where
    \begin{equation}
        \begin{aligned}
            \phi\colon \eR^3&\to V \\
            x&\mapsto x\cdot \sigma 
        \end{aligned}
    \end{equation}
    is
    \begin{enumerate}
        \item
            continuous,
        \item
            a group homomorphism,
        \item
            surjective,
        \item
            \( \ker(f)=\{ \pm \mtu \}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Several points.
    \begin{subproof}
        \item[Continuous]
            We know from proposition \ref{PROPooRQUZooAoZzwx} that \( U\mapsto \rho(U)\) is continuous. The inequality
            \begin{equation}
                \| \phi^{-1}\circ\rho(U_k)\circ \phi-\phi^{-1}\circ\rho(U)\circ\phi \|\leq \| \phi^{-1} \|\| \phi(U_k)-\rho(U) \|\| \phi \|
            \end{equation}
            shows that \( f\) is continuous too.
        \item[Group homomorphism]
            Let \( U_1,U_2\in \SU(2)\) and \( x\in \eR^3\). We have
            \begin{subequations}
                \begin{align}
                    f(U_1U_2)x&=\phi^{-1}\big( U_1U_2\phi(x)U_2^{-1}U_1^{-1} \big)\\
                    &=\phi^{-1}\Big( \rho(U_1)\big( U_2\phi(x)U_2^{-1} \big) \Big)\\
                    &=\phi^{-1}\big( \rho(U_1)\circ\rho(U_2)\phi(x) \big)\\
                    &=\big( \phi^{-1}\circ\rho(U_1)\circ\rho(U_2)\circ\phi \big)(x).
                \end{align}
            \end{subequations}
            We can suppress the dependency on \( x\) and continue:
            \begin{subequations}
                \begin{align}
                    f(U_1U_2)&=\phi^{-1}\circ\rho(U_1)\circ\rho(U_2)\circ\phi\\
                    &=\phi^{-1}\circ\rho(U_1)\circ\phi\circ\phi^{-1}\circ\rho(U_2)\circ\phi\\
                    &=f(U_1)\circ f(U_2).
                \end{align}
            \end{subequations}
            Since \( \rho(\id)=\id\) we also have \( f(\id)=\id\). Thus \( f\) is a group homomorphism.
        \item[Surjective]
            Elements of \( \SO(3)\) are compositions of two reflexions by corollary \ref{CORooJCURooSRzSFb}. A generic element of \( \SO(3)\) has the form \(ST \) where \( S\) and \( T\) are reflexions. They have the form
            \begin{equation}
                \begin{aligned}
                    S\colon \eR^3&\to \eR^2 \\
                    x&\mapsto x-2(x\cdot n_1)n_1 
                \end{aligned}
            \end{equation}
            and
            \begin{equation}
                \begin{aligned}
                    T\colon \eR^3&\to \eR^3 \\
                    x&\mapsto x-2(x\cdot n_2)n_2 
                \end{aligned}
            \end{equation}
            with \( \| n_1 \|=\| n_2 \|=1\).
            
            Let \( M= \phi(n_1) =n_1\cdot \sigma\) and \( Q=\phi(n_2)=n_2\cdot \sigma\). We will prove that \( MQ\in \SU(2)\) and \( f(MQ)=S\circ T\).

            \begin{subproof}
                \item[\( M^2=\mtu\)]
                    We have
                    \begin{equation}
                        M^2=(n_1\cdot n_1)\mtu_2+i(n_1\times n_1)\cdot \sigma=\mtu_2.
                    \end{equation}
                \item[\( \det(M)=-1\)]
                    We know from lemma \ref{LEMooRFBTooIRDbEq} that \( \det(M)=\det\big( \phi(n_1) \big)=-\| n_1 \|^2=-1\).
                \item[\( MQ\in \SU(2)\)]
                    First, \( \det(MQ)=\det(M)\det(Q)=-1\). Second, since \( M\) and \( Q\) are hermitian, \( (MQ)^{\dag}=Q^{\dag}M^{\dag}=QM\) and then
                    \begin{equation}
                        (MQ)^{\dag}(MQ)=QMMQ=\mtu_2
                    \end{equation}
                    because \( M^2=Q^2=1\).
                \item[\( \phi(Sx)=-M\phi(x)M\)]
                    Let \( x\in \eR^3\). We have
                    \begin{subequations}        \label{EQooSHKEooAhOxfH}
                        \begin{align}
                        \phi(Sx)&=\phi\big( x-2(x\cdot n_1)n_1 \big)\\
                        &=\phi(x)-2(x\cdot n_1)\phi(n_1)\\
                        &=\phi(x)-2(x\cdot n_1)M.
                        \end{align}
                    \end{subequations}
                    Using the formula \( (a\cdot \sigma)(b\cdot \sigma)=(a\cdot b)\mtu+i(a\times b)\cdot \sigma\) we have
                    \begin{equation}
                        \phi(x)M=\phi(x)\phi(n_1)=(x\cdot n_1)\mtu+i(x\times n_1)\cdot \sigma
                    \end{equation}
                    and
                    \begin{equation}
                        M\phi(x)=(n_1\cdot \sigma)(x\cdot \sigma)=(n_1\cdot x)\mtu+i(n_1\times x)\cdot \sigma,
                    \end{equation}
                    si that
                    \begin{equation}
                        \phi(x)M+M\phi(x)=2(x\cdot n_1)\mtu.
                    \end{equation}
                    Multiplying that by \( M\) and using \( M^2=\mtu\) we deduce
                    \begin{equation}
                        \phi(x)=2(x\cdot n_1)M-M\phi(x)M.
                    \end{equation}
                    Now we substitute that into \eqref{EQooSHKEooAhOxfH} in order to see that
                    \begin{equation}
                        \phi(Sx)=-M\phi(x)M.
                    \end{equation}
                \item[Conclusion (surjective)]
                    We can now compute the action of \( f(MQ)\) on \( x\in \eR^3\):
                    \begin{subequations}
                        \begin{align}
                            f(MQ)x&=\big( \phi^{-1}\circ\rho(MQ)\circ\phi \big)x\\
                            &=\phi^{-1}\big( MQ\phi(x)QM \big)\\
                            &=\phi^{-1}\big( M\phi(Tx)M \big)\\
                            &=\phi^{-1}\big( \phi(STx) \big)\\
                            &=STx.
                        \end{align}
                    \end{subequations}
                    So we have \( f(MQ)=ST\) and \( f\) is surjective.
            \end{subproof}
        \item[Kernel]
            Let \( U\in \SU(2)\) be such that \( f(U)=\mtu_3\in \SO(3)\). For every \( x\in \eR^3\) we have \( x=f(U)x\) while
            \begin{equation}
                f(U)x=\phi^{-1}\big( U\phi(x)U^{-1} \big).
            \end{equation}
            We conclude that \( U\phi(x)U^{-1}=\phi(x)\) for every \( x\in \eR^3\). Since \( f\) is surjective on the vector space \( V\) of hermitian matrices with vanishing trace, we have
            \begin{equation}
                UXU^{\dag}=X
            \end{equation}
            for every \( X\in V\). In particular \( UX=XU\). Since the matricial product is continuous, we can commute \( U\) and the infinite sum and get
            \begin{equation}
                U\sum_{k=0}^{\infty}\frac{ (iX)^k }{ k! }=\lim_{N\to \infty} \sum_{k=0}^N\frac{ U(iX)^k }{ k! }=\lim_{N\to \infty} \sum_{k=0}^N\frac{ (iX)^kU }{ k! }= e^{iX}U.
            \end{equation}
            So we have \( [U, e^{iX}]=0\) for every \( X\in V\). Since proposition \ref{PROPooZBJSooEIguXR} says that every element of \( \SU(2)\) is the exponential of an element in \( V\) the element \( U\) is in the center of \( \SU(2)\). The center of \( \SU(2)\) is \( \{ \pm\id \}\) by the proposition \ref{PROPooLMGHooKrKpsa}.

            Until now we have \( \ker(f)\subset \{ \id,-\id \}\). It is a simple verification to check that \( \{ \id,-\id \}\) are in the kernel of \( f\). We conclude that \( \ker(f)=\{ \pm\mtu_2 \}\).
    \end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooDKPTooBnLflt}
    We have the group isomorphism
    \begin{equation}
        \SO(3)=\frac{ \SU(2) }{ \eZ_2 }.
    \end{equation}
\end{proposition}

\begin{proof}
    We use the first isomorphism theorem \ref{ThoPremierthoisomo} with \( \theta\) being the map \( f\colon \SU(2)\to \SO(3)\) defined the proposition \ref{PROPooGEHAooPCReoU}. It says that
    \begin{equation}
        \frac{ \SU(2) }{ \ker(f) }=\Image(f).
    \end{equation}
    The known properties of \( f\) are that \( \ker(f)=\eZ_2\) and \( \Image(f)=\SO(3)\). This is the expected result.
\end{proof}

\begin{lemma}       \label{LEMooSYGUooVWxGYX}
    The images of the unitary matrices \( i\sigma_k\) by \( f\) are
    \begin{equation}
        \begin{aligned}[]
            f(i\sigma_1)=\begin{pmatrix}
                1    &       &       \\
                    &   -1    &       \\
                    &       &   -1
            \end{pmatrix},
            f(i\sigma_2)=\begin{pmatrix}
                -1    &       &       \\
                    &   1    &       \\
                    &       &   -1
            \end{pmatrix},
            f(i\sigma_3)=\begin{pmatrix}
               -1     &       &       \\
                    &   -1    &       \\
                    &       &   1
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
\end{lemma}

\begin{proof}
    We know that 
    \begin{equation}
        \phi(x)=\begin{pmatrix}
            x_3    &   x_1-ix_2    \\ 
            x_1+ix_2    &   -x_3    
        \end{pmatrix}.
    \end{equation}
    We have
    \begin{equation}
        (i\sigma_1)\phi(x)(i\sigma_1)^{\dag}=\sigma_1\phi(x)\sigma_1=\begin{pmatrix}
            -x_3    &   x_1+ix_2    \\ 
            x_1-ix_2    &   x_3    
        \end{pmatrix}=\phi\begin{pmatrix}
            x_1    \\ 
            -x_2    \\ 
            -x_3    
        \end{pmatrix}.
    \end{equation}
    This shows that
    \begin{equation}
        f(i\sigma_1)=\begin{pmatrix}
            x_1    \\ 
            -x_2    \\ 
            -x_3    
        \end{pmatrix},
    \end{equation}
    so that the matrix of \( i\sigma_1\) is
    \begin{equation}
        f(i\sigma_1)=\begin{pmatrix}
            1    &       &       \\
                &   -1    &       \\
                &       &   -1
        \end{pmatrix}.
    \end{equation}
    The same kind of computations provide the result.
\end{proof}

\begin{proposition}
    Let \( f\colon \SU(2)\to \SO(3)\) be the map of the proposition \ref{PROPooGEHAooPCReoU}:
    \begin{equation}
        \begin{aligned}
            f\colon \SU(2)&\to \SO(3) \\
            f(U)&=\phi^{-1}\circ\rho(U)\circ \phi.
        \end{aligned}
    \end{equation}
    There exist no group homomorphism \( g\colon \SO(3)\to \SU(2)\) such that \( f\circ g=\id\).
\end{proposition}

\begin{proof}
    Let \( g\) be such an homomorphism and let's derive a contradiction. Since \( g\) is an homomorphism it satisfies \( g(\mtu_3)=\mtu_2\). Let 
    \begin{equation}
        T_x=\begin{pmatrix}
            1    &   0    &   0    \\
            0    &   -1    &   0    \\
            0    &   0    &   -1
        \end{pmatrix}\in \SO(3).
    \end{equation}
    The map \( f\) is surjective, so there exist \( U\in \SU(2)\) such that \( f(U)=T_x\). From lemma \ref{LEMooSYGUooVWxGYX} we have
    \begin{equation}
        f(i\sigma_1)=f(-i\sigma_1)=T_x.
    \end{equation}
    Thus \( g(T_x)=i\sigma_1\) or \( g(T_x)=-i\sigma_1\). In both cases we have a contradiction. Indeed, since \( T_x^2=\mtu\) and \( g(T_x^2)=g(T_x)^2\) we must have \( g(T_x)^2=\mtu\) while
    \begin{equation}
        (i\sigma_1)^2=(-i\sigma_1)^2=\mtu.
    \end{equation}
\end{proof}

\begin{lemma}       \label{LEMooRCSSooTvAaJY}
    Let \( \alpha_0\in \SO(3)\) and \( U\in \SU(2)\) such that \( f(U)=\alpha_0\). There exists a neighborhood \(\mO\) of \( \alpha_0\) in \( \SU(2)\) such that
    \begin{equation}
        f^{-1}(\mO)=V_1\cup V_2
    \end{equation}
    where \( V_1\) is a neighborhood of \( U\), \( V_2\) is a neighborhood of \( -U_1\) and \( V_1\cap V_2=\emptyset\).
\end{lemma}

\begin{proof}
    We know that \( f(U)=f(-U)=\alpha_0\). Let \( W_1\) be a neighborhood of \( U\) and \( W_2\) be a neighborhood of \( -U\) such that \( W_1\cap W_2=\emptyset\).

    The part \( -W_2\) is a neighborhood of \( U\). we consider \( V_1\), a neighborhood of \( U\) contained in \( W_1\cap -W_2\). Then we set \( V_2=-V_1\). This is a neighborhood of \(-U\) contained in \( W_2\).

    Thus we have \( V_1\cap V_2=\emptyset\) and \( f(V_1)=f(V_2)\) is a neighborhood of \( \alpha_0\). It remains to define \( \mO=f(V_1)\).
\end{proof}

\begin{proposition}       \label{PROPooHQENooUsQeiZ}
    Let \( U\in \SU(2)\) be such that 
    \begin{equation}
        UX=XU
    \end{equation}
    for every \( X\in V\). Then \( U\in\{\mtu,-\mtu\}\).
\end{proposition}

\begin{proof}
    The proof is a simple computation. Let \( a,b,c,d\in \eC\) such that \( U=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\). We have
    \begin{equation}
        U\sigma_1=\begin{pmatrix}
            a    &   b    \\ 
            c    &   d    
        \end{pmatrix}\begin{pmatrix}
            0    &   1    \\ 
            1    &   0    
        \end{pmatrix}=\begin{pmatrix}
            b    &   a    \\ 
            d    &   c    
        \end{pmatrix}
    \end{equation}
    while
    \begin{equation}
        \sigma_1U=\begin{pmatrix}
            c    &   d    \\ 
            a    &   b    
        \end{pmatrix}.
    \end{equation}
    We deduce \( b=c\) and \( a=d\) and \( U=\begin{pmatrix}
        a    &   b    \\ 
        b    &   a    
    \end{pmatrix}\). Taking that into account, the same work with \( \sigma_2\) provides
    \begin{equation}
        U\sigma_2=\begin{pmatrix}
            bi    &   -ia    \\ 
            ia    &   -ib    
        \end{pmatrix}
    \end{equation}
    and 
    \begin{equation}
        \sigma_2U=\begin{pmatrix}
            -bi    &   -ia    \\ 
            ia    &   bi    
        \end{pmatrix},
    \end{equation}
    so that \( b=0\). Now \( U=\begin{pmatrix}
        a    &   0    \\ 
        0    &   a    
    \end{pmatrix}\) for some \( a\in \eC\).

    The constrain \( U\sigma_3=\sigma_3U\) does not provide new informations.

    Since \( U\in \SU(2)\) we have \( \det(U)=1\) which implies \( a^2=1\), which in turn means \( a=\pm1\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooBHVBooEPbWwZ}
    Let \( U_k\in \SU(2)\) such that 
    \begin{enumerate}
        \item
            \( \rho_{U_k}\stackrel{\End(V)}{\longrightarrow}\id_V\)
        \item
            there exists a neighborhood of \( -\mtu\) in \( \SU(2)\) which contains no element \( U_k\).
    \end{enumerate}
    Then we have \( U_k\stackrel{\SU(2)}{\longrightarrow}\mtu\).
\end{lemma}

\begin{proof}
    Let \( A_k\) be a converging subsequence of \( U_k\) (we will see later that it exists) with \( A_k\stackrel{\SU(2)}{\longrightarrow}A\). For each \( X\in V\) we have \( A_kXA_k^{-1}\stackrel{\SU(2)}{\longrightarrow}X\), so that
    \begin{subequations}
        \begin{align}
            \| A_kX-XA_k \|&=\| A_kXA_k^{-1}A_k-XA_k \|\\
            &\leq\| A_kXA_k^{-1}-X \|\| A_k \|\\
            &\leq \| A_kXA_k^{-1}-X \|M\to 0
        \end{align}
    \end{subequations}
    where \( M\) is some constant majoration of \( \| A_k \|\). Thus we have \( A_kX-XA_k\to 0\) which means 
    \begin{equation}
        XA=AX.
    \end{equation}
    If it is true for every \( X\), we conclude that \( A=\pm\mtu\) (proposition \ref{PROPooHQENooUsQeiZ}). Since there is a neighborhood of \( -\mtu\) in which there are no elements \( U_k\), we cannot have \( A_k\to -\mtu\), so we have \( A=\mtu\).

    Now \( U_k\) is a sequence in the compact \( \SU(2)\) (proposition \ref{PROPooGLPQooKOfrjl}), so that every subsequence has a converging subsequence\footnote{Bolzano-Weierstrass \ref{THOooZJWLooAtGMxD}.}. We are in the case of the lemma \ref{LEMooSJKMooKSiEGq} and we conclude \( U_k\stackrel{\SU(2)}{\longrightarrow}\mtu\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooMNWSooAjmBQK}
    Let \( U\in \SU(2)\). We consider the linear map\footnote{See proposition \ref{PROPooRQUZooAoZzwx}.}
    \begin{equation}
        \begin{aligned}
            \rho_U\colon V&\to V \\
            v&\mapsto UvU^{-1}. 
        \end{aligned}
    \end{equation}
    Then \( \| \rho_U \|\leq 1\).
\end{lemma}

\begin{proof}
    By definition, the norme of \( \rho_U\colon V\to V\) is
    \begin{equation}
        \| \rho_U \|=\sup_{\| v \|=1}\| \rho_Uv \|=\sup_{\| v \|=1}\| UvU^{-1} \|.
    \end{equation}
    In the last expression, the norms are in \( \End(\eC^2)\) because \( U\) and \( v\) are both \( 2\times 2\) complex matrices. The operator norm is an algebra norm\footnote{Lemma \ref{LEMooFITMooBBBWGI}.}, so that
    \begin{equation}
        \| UvU^{-1} \|\leq \| U \|\| U^{-1} \|\| v \|=\| v \|
    \end{equation}
    because the éléments of \( \SU(2)\) are normed to \( 1\). Thus
    \begin{equation}
        \| \rho_1 \|\leq \sup_{\| v \|=1}\| v \|=1.
    \end{equation}
\end{proof}

\begin{lemma}       \label{LEMooHPQQooIGwljm}
    Let \( U_k\in \SU(2)\) and \( U\in \SU(2)\) such that 
    \begin{enumerate}
        \item
            \( \rho_{U_k}\stackrel{\End(V)}{\longrightarrow}\rho_U\)
        \item
            there exists a neighborhood of \( -A\) in \( \SU(2)\) which contains no element \( U_k\).
    \end{enumerate}
    Then we have \( U_k\stackrel{\SU(2)}{\longrightarrow}U\).
\end{lemma}

\begin{proof}
    Several steps.
        \begin{subproof}
        \item[\( \rho_{U_k^{-1}U}\to\id\)]
                We start by proving that \( \rho_{U_k^{-1} U}\to \id\). For each \( v\in V\) we have
                \begin{equation}
                    \| \rho_{U_k^{-1} U}v-v \|=\| \rho_{U_k^{-1}}\big( \rho_Uv-\rho_{U_k}v \big) \|\leq \| \rho_{U_k^{-1}}\| \rho_Uv-\rho_{U_k}v \| \|
                \end{equation}
                Thus we have
                \begin{subequations}
                    \begin{align}
                        \| \rho_{U_k^{-1}U}-\id \|&=\sup_{\| v \|=1}\| \rho_{U_k^{-1}U}v-v \|\\
                        &\leq \| \rho_{U_k^{-1}} \|\sup_{\| v \|=1}\| \rho_Uv-\rho_{U_k}v \|\\
                        &=\| \rho_{U_k} \|\| \rho_u-\rho_{U_k} \|\\
                        &\leq \| \rho_U-\rho_{U_k} \|.
                    \end{align}
                \end{subequations}
                We used lemme \ref{LEMooMNWSooAjmBQK}. In conclusion,
                \begin{equation}
                    \| \rho_{U_k^{-1}U}-\id \|\leq \| \rho_U-\rho_{U_k} \|\to 0.
                \end{equation}
            \item[No neighborhood of \( -\mtu\)]   
                We prove that there exists a neighborhood of \( -\mtu\) which contains no elements of the sequence \( U_k^{-1}U\). Suppose that each neighborhood of \( -\mtu\) contains one of the \( U_k^{-1}U\). At this point we have a subsequence \( (B_k)\) of \( (U_k)\) such that 
                \begin{equation}
                    B_k^{-1}U\to-\mtu. 
                \end{equation}
                Since the multiplication and the inverse are continuous operations\footnote{The group \( \SU(2)\) is a Lie group, proposition \ref{PROPooWMKGooKftzGv}.} we also have 
                \begin{equation}
                    B_k^{-1}\to-U^{-1}
                \end{equation}
                and
                \begin{equation}
                    B_k\to -U.
                \end{equation}
                This prove that for every neighborhood of \( -U\) we have a \( B_k\) and then a \( U_k\), which is a contradiction with the hypothesis.
            \item[Conclusion] 
                The sequence \( U_k^{-1}U\) satisfies the lemma \ref{LEMooBHVBooEPbWwZ} and we conclude \( U_k^{-1}U\to \mtu\). Thus \( U_k\to U\).
        \end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooYTTJooYpPYLT,MonCerveau}]        \label{PROPooHCVZooMOSzTm}
    Let \( \alpha_0\in \SO(3)\) and \( \mO\) be a neighborhood of \( \alpha_0\) such that \( f^{-1}(\mO)=V_1\cup V_2\) with \( V_1\cap V_2=\emptyset\)\footnote{Such choice is possible by the lemma \ref{LEMooRCSSooTvAaJY}.}.

    The map
    \begin{equation}
        \begin{aligned}
            \varphi\colon \mO&\to\SU(2) \\
            \alpha&\mapsto f^{-1}(\alpha)\cap V_1 
        \end{aligned}
    \end{equation}
    is continuous.
\end{proposition}

\begin{proof}
    Let \( \sigma_k\stackrel{\SO(3)}{\longrightarrow} \alpha\) with \( \alpha_k\in \mO\). We have to prove that \( \varphi(\alpha_k)\stackrel{\SU(2)}{\longrightarrow}\varphi(\alpha)\).
    \begin{subproof}
        \item[General setting]
            First we suppose that \( \alpha_k\) converges to the identity. For each \( k\) we have
            \begin{equation}
                f\big( \varphi(\alpha_k) \big)=\alpha_k,
            \end{equation}
            with the map \eqref{EQooSOZTooTIkONx}. That means, for each \( k\):
            \begin{equation}
                \phi^{-1}\circ\rho_{\varphi(\alpha_k)}\circ \phi=\alpha_k,
            \end{equation}
            or
            \begin{equation}
                \rho_{\varphi(\alpha_k)}=\varphi\circ \alpha_k\circ\phi^{-1}
            \end{equation}
            as operator on \( V\).
        \item[Norm convergence]
            We have the following computation:
            \begin{subequations}
                \begin{align}
                    \| \rho_{\varphi(\alpha_k)}-\rho_{\varphi(\alpha)} \|_{\End(V)}&=\| \phi\circ\alpha_k\circ\phi^{-1}-\phi\circ\alpha\circ\phi^{-1} \|\\
                    &=\| \phi\circ(\alpha_k-\alpha)\circ\phi^{-1} \|\\
                    &\leq \| \phi \|\| \alpha_k-\alpha \|\| \phi^{-1} \|\to0.
                \end{align}
            \end{subequations}
            This shows that
            \begin{equation}        \label{EQooCEFUooTCoczi}
                \rho_{\varphi(\alpha_k)}\to \rho_{\varphi(\alpha)}.
            \end{equation}

        \item[Conclusion]

            The sequence \( U_k=\varphi(\alpha_k)\) and the element \( U=\varphi(\alpha)\) satisfy the lemma \ref{LEMooHPQQooIGwljm}, so that \( \varphi(\alpha_k)\to \varphi(\alpha)\).
    \end{subproof}
\end{proof}

\begin{proposition}
    The map \( f\colon \SU(2)\to \SO(3)\) is a representation of \( \SU(2)\) on \( \eR^3\), but is not faithful\footnote{Définition \ref{DEFooAFSAooGDSDBb}.}.
\end{proposition}

\begin{proof}
    The function \( f\) is written as
    \begin{equation}
        f(U)=\phi^{-1}\circ\rho_U\circ\phi.
    \end{equation}
    On the other hand, we have
    \begin{equation}
        \rho_{U_1U_2}v=U_1U_2v(U_1U_2)^{-1}=U_1U_2vU_2^{-1}U_1^{-1}=(\rho_{U_1}\circ\rho_{U_2})v.
    \end{equation}
    Thus
    \begin{equation}
        f(U_1U_2)=\phi^{-1}\circ\rho_{U_1}\circ\rho_{U_2}\circ\phi=\underbrace{\phi^{-1}\rho_{U_1}\phi}_{f(U_1)}\underbrace{phi^{-1}\rho_{U_2}\phi}_{f(U_2)}=f(U_1)\circ f(U_2).
    \end{equation}
    Thus \( f\) is a representation.

    It is not faithful because \( f(\mtu)=f(-\mtu)=\id\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{The Lie algebras \texorpdfstring{\( \su(2)\)}{su(2)} and \texorpdfstring{$ \so(3)$}{so(n)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}         \label{PROPooSERWooFtxBgV}
    The Lie algebra \( \su(n)\) of \( \SU(n)\) is the space of traceless anti-hermitian matrices\footnote{See the definitions \ref{DEFooKDCPooZOJsMD} and \ref{DEFooJJVIooDUBwDJ}.}.
\end{proposition}

\begin{proof}
    Let consider $G=\SU(n)$; the elements are complexes $n\times n$ matrices $U$ such that $UU^{\dag}=\mtu$ and $\det U=1$. An element of the Lie algebra is given by a path $\dpt{u}{\eR}{G}$ in the group with $u(0)=\mtu$. Since \( \SU(n)\) is a Lie subgroup of \( \GL(n,\eC)\)\footnote{Proposition \ref{PROPooWMKGooKftzGv}.}, by the proposition \ref{PROPooSQHLooGQAykc}, it is sufficient to compute the usual derivative of such a path.
    Since for all $t$, $u(t)u(t)^{\dag}=\mtu$,
    \begin{subequations}
        \begin{align}
      0&=\Dsdd{u(t)u(t)^{\dag}}{t}{0}\\
       &=u(0)\Dsdd{u(t)^{\dag}}{t}{0}+\Dsdd{u(t)}{t}{0}u(0)^{\dag}\\
       &=[d_tu(t)]^{\dag}+[d_tu(t)].
        \end{align}
    \end{subequations}
    So a general element of the Lie algebra $\su(n)$ is an anti-hermitian matrix.

    An element of \( \SU(n)\) has also a determinant equal to \( 1\). What condition does it implies on the elements of the Lie algebra? 
    
    Let \( X\) be an element of \( \su(2)\). For each \( t\), the element \(  e^{tX}\) is part of the Lie group and satisfy \( \det( e^{tX})=1\). Using the formula\footnote{Corollary \ref{CORooOKKSooHrsYOs}.}
    \begin{equation}
        \Dsdd{ \det( e^{tX}) }{t}{0}=\tr(X)
    \end{equation}
    we deduce \( \tr(X)=0\).
    
    An other way to prove the same result is to consider a path in \( \SU(n)\) and derive; let's do it. If \( g(t)\) is a path in \( \SU(n)\) with \( g(0)=\mtu\). For each \( t\) we have \( \det\big( g(t) \big)=1\).
    
    Using the formula expression the determinant with the minors,
    \begin{equation}
        \det\begin{pmatrix}
            g_{11}(t)    &   g_{12}(t)    &   \ldots    \\
            f_{21}(t)    &   g_{22}(t)    &   \ldots    \\
            \vdots    &   \vdots    &   \ddots
        \end{pmatrix}=g_{11}(t)M_{11}(t)+g_{12}(t)M_{12}(t)+\ldots=1
    \end{equation}
    where \( M_{ij}\) is the minor of \( g\). If we derive the left hand side we get
    \begin{equation}
        g'_{11}(0)M_{11}(0)+g_{11}(0)M'_{11}(0)+g'_{12}(0)M_{12}(0)+g_{12}(0)M'_{12}(0)+\ldots
    \end{equation}
    where the numbers \( g'_{ij}(0)\) are the matrix entries of the tangent matrix, that is the matrix elements of a general element in \( \gsu(n)\). Since \( g(0)=\mtu\) we have \( M_{11}(0)=1\), \( g_{11}(0)=1\), \( M_{12}(0)=0\) and \( g_{12}(0)=0\). Thus we have
    \begin{equation}
        (\det g)'(0)=X_{11}+M'_{11}(0)
    \end{equation}
    where \( X=g'(0)\). By induction we found that the trace of \( X\) appears. Thus the elements of \( \gsu(n)\) have vanishing trace.
\end{proof}

\begin{normaltext}
    The space \( V\) spanned by the matrices \( \sigma_i\) is not \( \su(2)\).
\end{normaltext}

\begin{proposition}     \label{PROPooDNNEooMOdrkq}
    The Lie algebra \( \so(n)\) is the vector space of antisymmetric matrices.
\end{proposition}

\begin{proof}
    As said in the proposition \ref{PROPooSQHLooGQAykc}, the Lie algebra \( \so(n)\) can be seen as \( \SO(n)'\), the Lie algebra of the matrices obtained by componentwise derivate paths in \( \SO(n)\). 
    
    \begin{subproof}
        \item[Inclusion in one sense]
            So let be a path \( g\colon \eR\to \SO(n)\). For each \( t\) we have the equality
            \begin{equation}
                g(t)g(t)^t=\mtu.
            \end{equation}
            We differentiate that equation with respect to \( t\) at \( t=0\) taking into account \( g(0)=\mtu\):
            \begin{equation}
                g'(0)+g'(0)^t=0.
            \end{equation}
            This shows that the matrices of the Lie algebra \( \so(n)\) are skew-symmetric.
        \item[Inclusion in the other sense]
            Now we prove that every skew-symmetric matrix is of the form \( \gamma'(0)\) for some path \( \gamma\colon \eR\to \SO(n) \). Let \( X\) be a skew-symmetric matrix. We consider the path
            \begin{equation}
                \gamma(t)= e^{tX}
            \end{equation}
            defined in the proposition \ref{PropPEDSooAvSXmY}. We have to prove that \( \gamma(t)\in \SO(3)\) for every \( t\) (at least in a neighborhood of \( t=0\)) and that \( \gamma(0)=\mtu\).
            \begin{subproof}
                \item[\( \gamma(0)=\mtu\)]
                    This is the proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooCVALooEfLQCyI}.
                \item[\( \gamma(t)\) is orhogonal]
                    By proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooEOSMooQWjcjA} we know that 
                    \begin{equation}
                        ( e^{tX})^t= e^{tX^t}.
                    \end{equation}
                    Since \( X\) is skew-symmetric we also have \( [X,X^t]=0\), because
                    \begin{equation}
                        (XX^t)_{ij}=\sum_kX_{ik}X^t_{kj}=\sum_kX_{ik}X_{jk}
                    \end{equation}
                    while
                    \begin{equation}
                        (X^tX)_{ij}=\sum_kX^t_{ik}X_{kj}=\sum_kX_{ki}X_{kj}=\sum_kX_{ik}X_{jk}.
                    \end{equation}
                    The last equality accounts the fact that \( X\) is skew-symmetric. Since \( X\) and \( X^t\) commute we can use the theorem \ref{THOooXCPEooYGyLOp}:
                    \begin{equation}
                        e^{tX}( e^{tX})^t= e^{tX} e^{tX^t}= e^{t(X+X^t)}= e^{0}=\mtu.
                    \end{equation}
                \item[\( \gamma(t)\) is special] We prove that \( \det\big( \gamma(t) \big)=1\). The proposition \ref{PROPooZUHOooQBwfZq} provides
                    \begin{equation}
                        \det( e^{tX})= e^{\tr(tX)}= e^{0}=1.
                    \end{equation}
                    By the way, these equalities are equalities in \( \eR\), not equalities on \( \GL(n,\eR)\).
                \item[Pause]
                    We finished to prove that \( \gamma(t)\in \SO(n)\) for every \( t\in \eR\). We still have to prove that \( \gamma'(0)=X\).
                \item[\( \gamma'(0)=X\)]
                    This is from proposition \ref{PROPooSDNNooQtHkhA} :
                    \begin{equation}
                        \Dsdd{  e^{tX} }{t}{0}=X.
                    \end{equation}
            \end{subproof}
    \end{subproof}
\end{proof}

\begin{normaltext}
    Notice that antisymmetric matrices are automatically with vanishing trace.    
\end{normaltext}

\begin{proposition}     \label{PROPooHOOLooOrcquD}
    Two Lie algebras.
    \begin{enumerate}
        \item       \label{ITEMooFSTMooGSjovL}
    The Lie algebra of \( \gU(n)\) is the set \( \gu(n)\) of anti-hermitian matrices.
\item       \label{ITEMooYEFMooRmGmlF}
    The Lie algebra of \( \SU(n)\) is the set \( \su(n)\) of anti-hermitian matrices with vanishing trace\footnote{Just to be clear: as set this is the skew-hermitian matrices. As vector space, this is a real vector space. The fact to be skew-hermitian is not preserved by a multiplication by \( i\).}.
\item           \label{ITEMooXXTRooQZzCfs}
    A basis of \( \su(2)\) is given by the matrices \( t_k=-i\sigma_k\) where \( \sigma_k\) are the Pauli matrices\footnote{Definition \ref{DEFooRNTDooTVkPtB}.}, that is
    \begin{equation}
        \begin{aligned}[]
            t_1=i\sigma_1=\begin{pmatrix}
                0    &   i    \\ 
                i    &   0    
            \end{pmatrix}&&t_2=i\sigma_2=\begin{pmatrix}
                0    &   1    \\ 
                -1    &   0    
            \end{pmatrix}&&t_3=i\sigma_3=\begin{pmatrix}
                i    &   0    \\ 
                0    &   -i    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
\item
    The commutation relations in \( \su(2)\) are
    \begin{equation}        \label{EQooFJIDooRtQGjA}
        [t_i,t_j]=2\sum_k\epsilon_{ijk}t_k.
    \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    The point \ref{ITEMooFSTMooGSjovL} is the same kind of proof that the one of proposition \ref{PROPooDNNEooMOdrkq}; the only difference is that one starts with \( g(t)g(t)^{\dag}=\mtu\). Then one use \(  e^{X^{\dag}}=( e^{X})^{\dag}\).

    The point \ref{ITEMooYEFMooRmGmlF}, is already proved in the proposition \ref{PROPooSERWooFtxBgV}.

    For the point \ref{ITEMooXXTRooQZzCfs}, an explicit computation shows that the matrices \( t_k\) belong to \( \su(2)\) and are linearly independent. Now there are two ways to proceed.
    
    One way is to prove that \( \su(2)\) has dimension \( 3\). For that, you can write down an explicit manifold structure on \( \SU(2)\) and show that it is a manifold of dimension \( 3\). Then the Lie algebra has the same dimension.

    An other way is to make it by hand. We consider a matrix \( \begin{pmatrix}
        a    &   c    \\ 
        d    &   b    
    \end{pmatrix}\in \eM(\eC,2)\). The fact to be traceless imposes \( a=-b\). Then we have
    \begin{equation}
        \begin{pmatrix}
            a    &   c    \\ 
            d    &   -a    
        \end{pmatrix}^{\dag}=\begin{pmatrix}
            \bar a    &   \bar d    \\ 
            \bar c    &   -\bar a    
        \end{pmatrix}.
    \end{equation}
    The condition to be skew-hermitian is
    \begin{equation}
        \begin{pmatrix}
            a    &   c    \\ 
            d    &   -a    
        \end{pmatrix}=-\begin{pmatrix}
            \bar a    &   \bar d    \\ 
            \bar c    &   -\bar a    
        \end{pmatrix}.
    \end{equation}
    That provide the constrains that \( a\) is purely imaginary and that, if \( c=x+iy\), then \( d=-x+iy\). Thus a generic matrix in \( \su(2)\) is given by
    \begin{equation}
        \begin{pmatrix}
            \lambda i    &   x+iy    \\ 
            -x+iy    &   -\lambda i    
        \end{pmatrix}=\lambda\begin{pmatrix}
            i    &   0    \\ 
            0    &   -i    
        \end{pmatrix}+x\begin{pmatrix}
            0    &   1    \\ 
            -1    &   0    
        \end{pmatrix}+y\begin{pmatrix}
            0    &   i    \\ 
            i    &   0    
        \end{pmatrix}=x(i\sigma_2)+y(i\sigma_1)+\lambda(i\sigma_3).
    \end{equation}
    with \( x,y,\lambda\in \eR\). That shows that \(  \{ i\sigma_k \} \) is a basis of \( \su(2)\).  In the same time this is a proof a proof that \( \su(2)\) has dimension \( 3\).

    The lemma \ref{LEMooJRWXooMkzRnk} provides the commutators for the Pauli matrices. We can adapt them for our basis of \( \su(2)\):
    \begin{equation}
        [t_i,t_j]=[i\sigma_i , i\sigma_j ]=-[\sigma_i,\sigma_j]=-2i\sum_k\epsilon_{ijk}\sigma_k=2\sum_k\epsilon_{ijk}t_k.
    \end{equation}
\end{proof}

\begin{proposition}
    The Lie algebras \( \su(2)\) and \( \so(3)\) are isomorphic.
\end{proposition}

\begin{proof}
    The propositions \ref{PROPooDNNEooMOdrkq} and \ref{PROPooHOOLooOrcquD} provide a description of \( \su(2)\) and \( \so(3)\). The easiest way to prove the isomorphism is to show an explicit isomorphism. A basis of \( \so(3)\) is
    \begin{equation}
        \begin{aligned}[]
            O_1&=\begin{pmatrix}
                0    &   0    &   0    \\
                0    &   0    &   1    \\
                0    &   -1    &   0
            \end{pmatrix},&O_2&=\begin{pmatrix}
                0    &   0    &   1    \\
                0    &   0    &   0    \\
                -1    &   0    &   0
            \end{pmatrix},&O_3&=\begin{pmatrix}
                0    &   1    &   0    \\
                -1    &   0    &   0    \\
                0    &   0    &   0
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    These matrices satisfy
    \begin{equation}        \label{EQooWJMUooOtFAkW}
        [O_i,O_j]=\sum_k\epsilon_{ijk}O_k.
    \end{equation}
    A basis of \( \su(2)\) is given by \( t_k=i\sigma_k\). Our isomorphism is
    \begin{equation}
        \begin{aligned}
            \varphi\colon \so(3)&\to \su(2) \\
            O_i&\mapsto -\frac{ i\sigma_k }{ 2 }. 
        \end{aligned}
    \end{equation}
    The fact that \( \varphi\) is a bijection derives from the fact that it maps a basis on a basis. We have to check that \( \varphi\) is a morphism, that is
    \begin{equation}
        \big[ \varphi(O_i),\varphi(O_j) \big]=\varphi\big( [O_i,O_j] \big).
    \end{equation}
    This is done by virtue of the commutators \eqref{EQooWJMUooOtFAkW} and of lemma \ref{LEMooJRWXooMkzRnk}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Irreducible representations of \texorpdfstring{$\gsl(2,\eC)$}{sl(2,C)}}
%---------------------------------------------------------------------------------------------------------------------------

We are not here to joke or to be funny. We are here to make quantum fields theory. So we need (among maaaaaany other things) the irreducible representations of the groups \( \SL(2,\eC)\) and \( \SU(2)\). Here is a good news: at the Lie algebra level, these two are more or less related.

\begin{lemma}[\cite{BIBooUXTFooXTeMOn}]     \label{LEMooVEJZooUVNdmE}
    As sets,
    \begin{equation}
        \su(2)_{\eC}=\su(2)\otimes_{\eR}\eC=\gsl(2,\eC)=\{ \begin{pmatrix}
        \alpha    &   \beta    \\ 
    \gamma    &   -\alpha    
\end{pmatrix}\tq \alpha,\beta,\gamma\in \eC\}.
    \end{equation}
    The first equality is a definition for the notation \( \su(2)_{\eC}\).
\end{lemma}

\begin{proof}
    A basis of \( \su(2)\) is \( \{ t_1,t_2,t_3 \}\); so \( \su(2)_{\eC}=\eC t_1\oplus \eC t_2\oplus \eC t_3\), that is
    \begin{subequations}
        \begin{align}
            \su(2)_\eC&=\{ \begin{pmatrix}
            0    &   ix    \\ 
        ix    &   0    
    \end{pmatrix}+\begin{pmatrix}
    0    &   y    \\ 
-y    &   0    
\end{pmatrix}+\begin{pmatrix}
iz    &   0    \\ 
0    &   -iz    
\end{pmatrix}\tq x,y,z\in \eC\}\\
&=\{\begin{pmatrix}
iz    &   ix+y    \\ 
ix-y    &   -iz    
\end{pmatrix}\rq x,y,z\in \eC\}\\
&=\{\begin{pmatrix}
\alpha    &   \beta    \\ 
\gamma    &   -\alpha    
\end{pmatrix}\tq \alpha,\beta,\gamma\in \eC\}.
        \end{align}
    \end{subequations}
    In order to determine the Lie algebra \( \gsl(2,\eC)\) of \( \SL(2,\eC)\) we use the proposition \ref{PROPooSQHLooGQAykc} to allow ourself to work at the matrix level. Let \( g\) be a smooth path in \( \SL(2,\eC)\) such that \( g(0)=\mtu\). A generic element of \( \gsl(2,\eC)\) has the form \( g'(0)\). We have
    \begin{equation}
        g(t)=\begin{pmatrix}
            \alpha(t)    &   \beta(t)    \\ 
            \gamma(t)    &   \delta(t)    
        \end{pmatrix}
    \end{equation}
    with 
    \begin{equation}        \label{EQooMNXMooVkbfDg}
        \alpha(t)\delta(t)-\gamma(t)\beta(t)=1
    \end{equation}
    for every \( t\).  Moreover \( g(0)=\mtu\) implies \( \alpha(0)=1\), \( \beta(0)=0\), \( \gamma(0)=0\) and \( \delta(0)=1\). Now we differentiate \eqref{EQooMNXMooVkbfDg} with respect to \( t\) at \( t=0\) :
    \begin{equation}
        \alpha'(0)\delta(0)+\alpha(0)\delta'(0)-\gamma'(0)\beta(0)-\gamma(0)\beta'(0)=0
    \end{equation}
    which reduces to \( \alpha'(0)+\delta'(0)=0\). An element of \( \gsl(2,\eC)\) is thus of the form \( \begin{pmatrix}
        a    &   b    \\ 
        c    &   -a    
    \end{pmatrix}\) with \( a,b,c\in \eC\).
\end{proof}

\begin{normaltext}
    Lemma \ref{LEMooVEJZooUVNdmE} speaks about the \emph{sets} of \( \su(2)_{\eC}\) and \( \gsl(2,\eC)\). The Lie bracket, in both cases, is the matrix commutator. As a vector space, one can consider on \( \gsl(2,\eC)\) a vector space structure on \( \eC\) or on \( \eR\). If you want \( \{ t_1, t_2, t_3 \}\) to be a basis, you have to consider the complex linear combinations. If you really want real linear combinations, you need a larger basis.
\end{normaltext}

We consider the following basis for \( \gsl(2,\eC)\):
\begin{subequations}        \label{EQSooORIBooAsgdDp}
    \begin{align}
        h_3&=\begin{pmatrix}
            1/2    &   0    \\ 
            0    &   -1/2    
        \end{pmatrix}\\
        h_+&=\begin{pmatrix}
            0    &   \sqrt{ 2 }/2    \\ 
            0    &   0    
        \end{pmatrix}\\
        h_-&=\begin{pmatrix}
            0    &   0    \\ 
            \sqrt{ 2 }/2    &   0    
        \end{pmatrix}
    \end{align}
\end{subequations}
They satisfy the commutation relations
\begin{subequations}        \label{SUBEQSooXMMVooKtnRXW}
    \begin{align}
        [h_3,h_+]&=h_+\\
        [h_3,h_-]&=-h_-\\
        [h_+,h_-]&=h_3.
    \end{align}
\end{subequations}

\begin{lemma}[\cite{BIBooUXTFooXTeMOn}]     \label{LEMooDGUYooPUkDNr}
    Let \( (\rho, V)\) be a representation of \( \gsl(2,\eC)\). If \( V_{\lambda}\) is the eigenspace of the eigenvalue \( \lambda\in \eC\) for \( \rho(h_3)\), then
    \begin{subequations}
        \begin{align}
            \rho(h_+)V_{\lambda}&\subset V_{\lambda+1}\\
            \rho(h_-)V_{\lambda}&\subset V_{\lambda-1}
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Let \( w\in V_{\lambda}\). We test the eigenvalue of \( \rho(h_3)\) on \( \rho(H_+)w\):
    \begin{subequations}
        \begin{align}
            \phi(h_3)\rho(h_+)w&=\big( [\rho(h_3),\rho(h_+)]+\rho(h_+)\rho(h_3) \big)\\
            &=\rho(h_+)w+\lambda\rho(h_+)w\\
            &=(\lambda+1)\rho(h_+)w,
        \end{align}
    \end{subequations}
    so that \( \phi(h_+)w\in V_{\lambda+1}\).

    The computation is the same for the other one.
\end{proof}

\begin{lemma}           \label{LEMooWXDYooUyijnm}
    Let \( (V,\rho)\) be a finite dimensional representation of \( \gsl(2,\eC)\) over the complex vector space \( V\). There exists \( \lambda_0\in \eC\) such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}=\{ 0 \}\).
\end{lemma}

\begin{proof}
    A vector \( w\in V\) belong to \( V_{\lambda}\) if \( \rho(h_3)w=\lambda w\), which meas that \( \big( \rho(h_3)-\lambda\id \big)w=0\). The equation \( \det\big( \rho(h_3)-\lambda\id \big)\) has (at least) one solution \( \lambda\in \eC\). So there exists \( \lambda\in \eC\) such that \( V_{\lambda}\neq \{ 0 \}\). 
    
    Let \( \lambda\) be such a number and a non vanishing vector \( w\in V_{\lambda}\).

    Now the sequence of elements \( w_k= \rho(h_+)w   \) satisfy \( w_k\in V_{\lambda+k}\). Since \( V\) is finite dimensional only a finite number of the \( V_{\lambda+k}\) are different to \( \{ 0 \}\). The space \( V_{\lambda}\) on the other hand contains \( w\neq 0\). Let \( k_0\) be the lowest natural such that \( V_{\lambda+k_0}=\{ 0 \}\). What we have is \( V_{\lambda+k_0-1}\neq \{ 0 \}\) and \( V_{\lambda_0+k_0}=\{ 0 \}\).

    The proposition is done with \( \lambda_0=\lambda+k_0\).
\end{proof}

\begin{proposition}[\cite{BIBooUXTFooXTeMOn}]      \label{PROPooZCAOooHHGxQk}
    Let \( (\rho, V)\) be a finite dimensional complex representation of \( \gsl(2,\eC)\). Let \( \lambda_0\) be such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}=\{ 0 \}\). Let \( w_0\in V_{\lambda_0}\) and 
    \begin{equation}
        w_k=\rho(h_-)^kw_0.
    \end{equation}
    Then
    \begin{enumerate}
        \item       \label{ITEMooBPPFooKdGyqO}
            \( w_k\in V_{\lambda_0-k}\)
        \item       \label{ITEMooHNULooHoTgEa}
                    \( \rho(h_+)w_k=\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1}\)
                \item       \label{ITEMooHDAPooClASpy}
                    There exists \( n\in \eN\) such that \( w_n\neq 0\) and \( w_{n+1}=0\).
                \item       \label{ITEMooJBZFooGqallS}
                    \( \lambda_0=n/2\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Notice that the existence of \( \lambda_0\) such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}\neq\{ 0 \}\) is provided by lemma \ref{LEMooWXDYooUyijnm}.
    \begin{subproof}
    \item[For \ref{ITEMooBPPFooKdGyqO}]
        By recursion, using lemma \ref{LEMooDGUYooPUkDNr}.
    \item[For \ref{ITEMooHNULooHoTgEa}]
        We'll have a recursion. Just to be clear here are two facts that are not yet proved:
        \begin{itemize}
            \item The spaces \( V_{\lambda_0+k}\) are one-dimensional.
            \item \( \rho(h_+)w_{k+1}\) is a multiple of \( w_k\).
        \end{itemize}
        We will now prove by recursion that the first fact is true. The second one is, in general, false. We will see later that it is true when the representation is irreducible.

        Ok. So let's begin our work. For \( k=0\) we already have
        \begin{equation}
            \rho(h_+)w_0=0.
        \end{equation}
        Let work out the case of \( k=1\).
        \begin{subequations}
            \begin{align}
                \rho(h_+)w_1&=\rho(h_+)\rho(h_-)w_0\\
                &=\big( \underbrace{[\rho(h_+),\rho(h_-)]}_{=\rho(h_3)}+\rho(h_-)\rho(h_+) \big)w_0\\
                &=\rho(h_3)w_0\\
                &=\lambda_0w_0.
            \end{align}
        \end{subequations}
        
        For the recursion, suppose that \( \rho(h_+)w_k=f(k)w_{k-1}\) for some function \( f\colon \eN\to \eC\). Then we compute \( \rho(h_+)w_{k+1}\):
        \begin{subequations}
            \begin{align}
                \rho(h_+)w_{k+1}=\rho(h_+)\rho(h_-)w_k&=\big( \underbrace{[\rho(h_+),\rho(h_-)]}_{=\rho(h_3)}+\rho(h_-)\rho(h_+) \big)w_k\\
                &=\lambda_0w_k+f(k)\rho(h_-)w_{k-1}\\
                &=\big( \lambda_0+f(k) \big)w_k.
            \end{align}
        \end{subequations}
        This shows that \( \rho(h_+)w_{k+1}\) is a multiple of \( w_k\) and that the proportionality factor \( f(k)\) satisfy
        \begin{subequations}        \label{SUBEQSooHGQNooRjMCap}
            \begin{numcases}{}
                f(k+1)=f(k)+\lambda_0-k\\
                f(1)=\lambda_0.
            \end{numcases}
        \end{subequations}
        The function \( f\) is defined by recursion and you see that at each step \( k\), we substrat \( k\) and add \( \lambda_0\). The guess is
        \begin{equation}
            f(k)=k\lambda_0-\frac{ k(k-1) }{ 2 }.
        \end{equation}
        Check that this satisfy \eqref{SUBEQSooHGQNooRjMCap}.
        
    \item[For \ref{ITEMooHDAPooClASpy}]
        The sequence of elements \( w_k\in V_{\lambda_0-k}\) has to finish on \( 0\) because the space \( V\) is finite dimensional.
    \item[For \ref{ITEMooJBZFooGqallS}]
        Let \( n\in \eN\) such that \( w_n\neq 0\) and \( w_{n+1}=0\). This means \( f(k+1)=0\). Solving
        \begin{equation}
            (n+1)\big( \lambda_0+\frac{ 1-(n+1) }{ 2 } \big)=0
        \end{equation}
        we get \( \lambda_0=n/2\).
    \end{subproof}
\end{proof}

This is quite an achievement because we proved not only that \( \rho(h_3)\) has a real eigenvalue, but that it has an eigenvalue in \( \eN/2\).

\begin{proposition}     \label{PROPooDAIQooPZVjju}
    Let \( \lambda_0\) and \( w_0\) be as before. We suppose that the representation is irreducible. Then
    \begin{equation}
        V=\Span\{  \rho(h_-)^kw_0 \}_{k=0,\ldots, 2\lambda_0}
    \end{equation}

    The eigenspaces of \( \rho(h_3)\) are one-dimensional.
\end{proposition}

\begin{proof}
    Let \(  W=\Span\{  \rho(h_-)^kw_0 \}_{k=0,\ldots, 2\lambda_0}\). This space is invariant under \( \rho\) because of the definitions and the proposition \ref{PROPooZCAOooHHGxQk}:
    \begin{equation}
        \begin{aligned}[]
            \rho(h_3)w_k&=(\lambda_0-k)w_k\\
            \rho(h_+)w_k&=\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1}\\
            \rho(h_-)w_k&=w_{k+1}
        \end{aligned}
    \end{equation}
    with the convention that \( w_{k+1}\) could be \( 0\).   

    Since \( W\) is a non trivial invariant subspace, it has to be \( V\). So \( W=V\).
\end{proof}

\begin{normaltext}
In the proposition \ref{PROPooZCAOooHHGxQk} and \ref{PROPooDAIQooPZVjju}, the vectors \( w_k\) are more or less enumerated in the reverse order: the larger \( k\) is, the lower is the eigenvalue. That leads to missleading formula like \( \rho(h_-)v_k=v_{K+1}\). In the following theorem, we make it in the correct order and one has to think \( v_m\) as being \( w_{\lambda_0-m}\).

Notice that up to now, the results we have collected are «if a representation of \( \gsl(2,\eC)\) exists». The next theorem \ref{THOooSRQYooXQDZpT} will show that a representation exists.
\end{normaltext}

Here is the theorem which provides every irredicible finite-dimensional representations of the Lie algebra \( \gsl(2,\eC)=\su(2)_{\eC}\).
\begin{theorem}     \label{THOooSRQYooXQDZpT}
    Let \( j\in \eN/2\). Let \( V_j\) be a complex vector space of dimension \( 2j+1\); we label a basis of \( V_j\) in the following way: \( \{ v_m \}_{m=j,j-1,\ldots, -j}\).

    We define the map \( \rho_j\colon \gsl(2,\eC)\to \End(V_j)\) by
    \begin{subequations}
        \begin{align}
            \rho_j(h_3)v_m&=mv_m,\\
            \rho_j(h_+)v_m&=\begin{cases}
                0    &   \text{if } m=j\\
                \frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}    &    \text{otherwise },
            \end{cases}\\
            \rho_j(h_-)v_m&=\begin{cases}
                v_{m-1}    &   \text{if } m\neq -j\\
                0    &    \text{if } m=-j.
            \end{cases}
        \end{align}
    \end{subequations}
    Two statements.
    \begin{enumerate}
        \item
            The map \( \rho_j\) is a representation of \( \gsl(2,\eC)\).
        \item
            Every finite dimensional complex irreducible representation is isomorphic to \( \rho_j\) for some \( j\in \eN/2\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    For the first item we have to check the algebra of \( \gsu(2)\) given by \eqref{SUBEQSooXMMVooKtnRXW}. There are three computations.

    We begin to check \( [\rho_j(h_3), \rho_j(h_+)]=\rho_j(h_+)\). The bracket in the left-hand side is the commutator of operators in \( \End(V)\). We have:
    \begin{subequations}
        \begin{align}
            \big( \rho_j(h_3)\rho_j(h_+)-\rho(h_+)\rho(h_3) \big)v_m&=\rho_j(h_3)\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}-m\rho_j(h_+)v_m\\
            &=(m+1)\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}\\
                &\quad-m\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}\\
            &=\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}\\
            &=\rho_j(h_+)v_m.
        \end{align}
    \end{subequations}
    The two other ones are checks with the same kind of computations.

    For the second item, we consider an irreducible finite-dimensional representations \( (\rho,V)\) of \( \gsl(2,\eC)\). Combining the propositions  \ref{PROPooZCAOooHHGxQk} and \ref{PROPooDAIQooPZVjju} we have:
    \begin{itemize}
        \item A vector \( w_0\in V\) such that \( \rho(h_+)w_0=0\).
        \item Letting \( n=2\lambda_0\) we have \( n\in \eN\),
        \item we let \( j=\lambda_0\) (pure notational purpose),
        \item \( V=\Span\{ w_k=\rho(h_-)^kw_0 \}\),
        \item  \( w_k\in V_{\lambda_0-k}\) where \( V_{\lambda}\) is the eigenspace of \( \rho(h_3)\) for the eigenvalue \( \lambda\),
        \item \( w_k\neq 0\) if and only if \( k=0,\ldots, n\), so \( \dim(V)=2n+1\)
        \item \( \rho(h_3)w_k=(\lambda_0-k)w_k\)
        \item \( \rho(h_+)w_k=\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1}\)
    \end{itemize}
    We choose \( j=\lambda_0\in \eN/2\). Do you believe that the map
    \begin{equation}
        \begin{aligned}
            \phi\colon V &\to V_j \\
            w_k&\mapsto v_{j-k} 
        \end{aligned}
    \end{equation}
    provides an equivalence of representations between \( \rho_j\) and \( \rho\) ? No ? Ok. We check that for every \( X\in \gsl(2,\eC)\) we have
    \begin{equation}
        \phi\circ\rho(X)=\rho_j(X)\circ \phi.
    \end{equation}
    For \( X=h_3\) we have
    \begin{equation}
        \phi\circ\rho(h_3)w_k=\phi\big( (\lambda_0-k)w_k \big)=(\lambda_0-k)v_{j-k}=(j-k)v_{j-k}
    \end{equation}
    while
    \begin{equation}
        \rho_j(h_3)\phi(w_k)=\rho_j(h_3)v_{j-k}=(j-k)v_{j-k}.
    \end{equation}
    Ok for the first one. Next: \( X=h_+\). We have
    \begin{equation}
        \phi\circ\rho(h_+)w_k=\phi\big( \frac{ 1 }{2}k(2j+1-k)w_{k-1} \big)=\frac{ 1 }{2}k(2j+1-k)v_{j-k+1}
    \end{equation}
    while
    \begin{equation}
        \rho_j\phi(w_k)=\rho_j(h_+)v_{j-k}=\frac{ 1 }{2}\big( j-(j-k) \big)\big( j+(j-k)+1 \big)v_{j-k+1}=\frac{ 1 }{2}k(2j-k+1)v_{j-k+1}.
    \end{equation}
    Ok again. And last one: \( X=h_-\); we have
    \begin{equation}
        \phi\circ\rho(h_-)w_k=\phi(w_{k+1})=v_{j-k-1}
    \end{equation}
    while
    \begin{equation}
        \rho_j(h_-)\phi(w_k)=\rho_j(h_-)v_{j-k}=v_{j-k-1}.
    \end{equation}
    Done\footnote{Now that we reached the end, I recognize that I did not belive neither until the last check.}.
\end{proof}

\begin{normaltext}
    The representations \( \rho_j\) of the theorem \ref{THOooSRQYooXQDZpT} are not yet hermitian for two reasons.
    \begin{itemize}
        \item The representations we expect to be hermitian are the ones of \( \su(2)\). The basis \( \{ h_2,h_{+}, h_{-} \}\) of \( \su(2)_{\eC}\) defined by \eqref{EQSooORIBooAsgdDp} is made of elements which do not belong to \( \su(2)\).
        \item We did not defined a scalar product on \( V_j\); thus the notion of «hermitian» makes no sene.
    \end{itemize}
\end{normaltext}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Haar measure on \texorpdfstring{$\SU(2)$}{SU2}}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The quaternion\index{quaternion} field $\eH$ can be embed in $\eM_2(\eC)$ as a genera element reads
\begin{equation}
	q=
\begin{pmatrix}
  \alpha	&	\beta	\\
  -\bar\beta	&	\bar\alpha
\end{pmatrix}
\end{equation}
with $\alpha$, $\beta\in\eC$. Under that isomorphism, we have
\[
	| q |^2=| \alpha |^2+| \beta |^2=\det q.
\]
Thus we have the identification
\begin{equation}
	\SU(2)=\{ q\in\eH\tq | q |=1 \}.
\end{equation}
We can act on $\eH$ by $\SU(2)\times \SU(2)$ by
\begin{equation}
	(u,v)\cdot q=uqv^{-1}
\end{equation}
for every $(u,v)\in \SU(2)\times\SU(2)$ and $q\in\eH$. That action defines an homomorphism from $\SU(2)\times\SU(2)$ onto $O(4)$.

\begin{proposition}
The previously defined homomorphism
\[
	\phi\colon \SU(2)\times\SU(2)\to O(4).
\]
is surjective over $\SO(4)$ (which is the identity component of $O(4)$) and, moreover, the kernel is $\big\{  (e,e),(-e,-e) \big\}$.
\end{proposition}

\begin{proof}
The group $\SU(2)\times \SU(2)$ being connected, its image can only be included in $\SO(4)$. Let us first determine the kernel of $\phi$. If $(u,v)\in\ker\phi$, we have $uqv^{-1}=q$ for every $q\in\eH$. In particular, with $q=1$, we find $u=v$. Then the relation $uqu^{-1}=q$ means that $u$ belongs to the center of $\eH$, which is $\eR$. We conclude that $u=\pm 1$. That proves that $\ker\phi=\big\{  (e,e),(-e,-e) \big\}$.

The differential $(d\phi)_{(e,e)}$ is an homomorphism
\[
	d\phi\colon \gsu(2)\oplus\gsu(2)\to \so(4).
\]
Let $(S,T)\in\gsu(2)\oplus\gsu(2)$, we have
\[
	d\phi(S,T)q=\Dsdd{ \phi( e^{t(S,T)})q }{t}{0}=\Dsdd{ \phi( e^{tS}, e^{tT})q }{t}{0}=\Dsdd{  e^{tS}q e^{-tT} }{t}{0}=Sq-qT,
\]
on which one sees that $d\phi$ is injective. Moreover we have $\dim\big( \gsu(2)\oplus\gsu(2) \big)=6=\dim\so(4)$. An injective map between vector space of same dimension being an isomorphism, the image of $\phi$ contains a neighborhood of identity in $\SO(4)$. From connectedness of $\SO(4)$, that neighborhood generates the whole group (see proposition~\ref{PropUssGpGenere}), so that $\phi$ is in fact surjective.
\end{proof}

Since the map $\phi\colon \SU(2)\times \SU(2)\to \SO(4)$ is a surjective homomorphism with a discrete kernel, we have an isomorphism at the algebra level:
\[
	\so(4)\simeq \gsu(2)\oplus\gsu(2).
\]

\subsection{Building some representations for \texorpdfstring{$\SU(2)$}{SU2}}
%/////////////////////////////////////////////////////////////////////////////////

Since $\SU(2)$ acts on $\eC^2$, we can build a representation of $\SU(2)$ on functions on $\eC^2$. We define $\dpt{T}{SU(2)}{\End\big(\Cinf(\eC^2)\big)}$ by
\[
  (T(U)f)(\xi)=f(U^{-1}\xi),
\]
if $f\in\Cinf\big(\eC^2\big)$, $\xi\in\eC^2$ and $U\in SU(2)$.

Let $V_j$ be the space of the homogeneous polynomials of degree $j$ on $\eC^2$; a basis of this space is given by the $\phi_{pq}$, $p+q=2j$ defined by
\begin{equation}
   \phi_{pq}(\xi)=\xi_1^p\xi_2^q
\end{equation}
($\xi=\xi_1+i\xi_2$). If $j$ is fixed, we will often write $\phi_m$ instead of $\phi_{pq}$. The signification is $p=j+m$, $q=j-m$, and $m$ takes its values in $-j,\ldots,j$. Note that $p-q=2m$. It is clear that if $A$ is any invertible $2\times 2$ matrix , and $f\in V_j$, then
\[
   \rho(A)f:=f(A^{-1} \cdot)
\]
 is still an element of $V_j$. This representation $\rho$ is defined on the whole $\Cinf(\eC^2)$. We will descent it to $V_j$ later.
Now, we fix $j$ and a $m$ between $-j$ and $j$.

Consider the diagonal matrix
\[   U_{-\theta}=\begin{pmatrix}
e^{-i\theta} & 0 \\
0 & e^{i\theta}
\end{pmatrix} \in\SU(2).
\]
One has
\begin{equation}
  \left(\rho(U_{-\theta})\phi_{pq}\right)(\xi)=\phi_{pq}
\begin{pmatrix}
   e^{i\theta}\xi_1 \\
    e^{-i\theta}\xi_2
\end{pmatrix}
                                               = e^{pi\theta} e^{-qi\theta}\xi_1^p\xi_2^q
					       =e^{2mi\theta}\phi_{pq}(\xi).
\end{equation}
First conclusion: the $\phi$'s are eigenvectors of $\rho(U_{-\theta})$ because
\[
   \rho(U_{-\theta})\phi_m=e^{2mi\theta}\phi_m.
\]
Second, the trace of $\rho(U_{-\theta})$ is
\begin{equation}
   \chi_j(\theta)=\sum_{m=-s}^{s}e^{2mi\theta}.
\end{equation}
By the way, the $\chi_j$ are the characters of the representation $\rho$.

From considerations about the Haar\quextproj{} invariant measure on $\SU(2)$, one knows that the good notion product between functions is:
\begin{equation}
(f_1,f_2)_{\SU(2)}=\frac{2}{\pi}\int_0^{\pi}f_1(\theta)\overline{ f_2(\theta) }\sin^2\theta\,d\theta,
\end{equation}
so that $(\chi_j,\chi_j)=1$. This and the fact that $\SU(2)$ is compact make the theorem of Peter-Weyl (cf. \cite{Sternberg}) applicable, thus the restrictions of $\rho$ to the $V_j$'s are irreducible and moreover, these provide \emph{all} the irreducible representations.

\subsection{Special case: \texorpdfstring{$j=\frac{1}{2}$}{j=1/2}}
%//////////////////////////////////////////////////////////////////////

Consider a matrix $A\in\SU(2)$:
\begin{equation}
A=\begin{pmatrix}
\oalpha & -\beta \\
\obeta & \alpha
\end{pmatrix},\qquad
A^{-1}=\begin{pmatrix}
\alpha & \beta \\
-\obeta & \oalpha
\end{pmatrix}.
\end{equation}
A basis of $V_{\frac{1}{2}}$ is given by $\phi_{10}$ and $\phi_{01}$. Let us see how $\rho(A)$ acts on. Since
\[
A^{-1}\begin{pmatrix}
\xi_1 \\
\xi_2
\end{pmatrix}=
\begin{pmatrix}
\alpha\xi_1+\beta\xi_2 \\
-\obeta\xi_1+\oalpha\xi_2
\end{pmatrix},
\]
we find
\begin{equation}
\begin{split}
  (\rho(A)\phi_{10})(\xi)&=\alpha\xi_1+\beta\xi_2=(\alpha\phi_{10}+\beta\phi_{01})(\xi)\\
  (\rho(A)\phi_{01})(\xi)&=-\obeta\xi_1+\oalpha\xi_2=(-\obeta\phi_{10}+\oalpha\phi_{01})(\xi).
\end{split}
\end{equation}
Thus in the basis $\{\phi_{10},\phi_{01}\}$, the matrix of $\rho(A)$ is given by
\begin{equation}
\rho(A)=\begin{pmatrix}
\alpha & -\obeta \\
\beta & \oalpha
\end{pmatrix}=\overline{A}.
\end{equation}

Up to here, we were looking at the representation $\rho$ of $\SU(2)$ on the whole set of functions on $\eC^2$, and more precisely, its restriction to $V_j$. We could define the representation $\rho_{\frac{1}{2}}$ as $\rho_{\frac{1}{2}}=\rho|_{V_{\frac{1}{2}}}$, but we will not do it. Our definition is
\begin{equation}
  \rho_{\frac{1}{2}}(A)=\rho(\overline{A})|_{V_{\frac{1}{2}}}.
\end{equation}
Note that
\[
\begin{pmatrix}
0 & -1 \\
1 & 0
\end{pmatrix}
A
\begin{pmatrix}
0 & 1 \\
-1 & 0
\end{pmatrix}=\overline{A},
\]
thus the representation $A\to\rho(\overline{A})|_{V_{\frac{1}{2}}}$ is equivalent to $A\to\rho(A)|_{V_{\frac{1}{2}}}$. This equivalence can also be seen because these two representations have the same characters\quextproj.

The basis $\phi_{pq}$ is orthogonal; we will build an orthonormal one: $e_m(\xi)$ is the vector whose coordinates are
\begin{equation}
e_m^j(\xi)=\frac{ \xi_1^{j+m}\xi_2^{j-m} }{\sqrt{ (j+m)!(j-m)! }}
\end{equation}
for $m=-j,-j+1,\ldots,j$. The metric to take in order to define $(e_m,e_n)$ is the unique one on $V_j$ which is $\SU(2)$-invariant.

The Newton's formula for the binomial yields:
\begin{equation}
\begin{split}
  \sum_{m=-s}^s e_m(\xi)\overline{e_m(\eta)}
        &=\sum\frac{ \xi_1^{j+m}\xi_2^{j-m}\oeta_1^{j+m}\oeta_2^{j-m} }{(j+m)!(j-m)!}\\
	&=\us{(2j)!}(\xi_1\oeta_1+\xi_2\oeta_2)^2\\
	&=\us{(2j)!}\scal{\xi}{\eta}^{2j}.
\end{split}
\end{equation}
But we know that $A\in\SU(2)$ preserves the scalar product: $\scal{A\xi}{A\eta}=\scal{\xi}{\eta}$. Therefore:
\begin{equation}\label{eq:produit_e_m}
\sum (\rho_j(A)e_m)(\xi)\overline{ (\rho_j(A)e_m)(\eta) }=\sum e_m(\xi)\overline{e_m(\eta)}.
\end{equation}
Now, instead of considering the matrices $\rho_j(A)$ on $V_j$ for the basis $\phi_m$, we looks at the ones with respect to the basis $e_m$:
\begin{equation}
\rho_j(A)e_m=r(A)^k_me_k;
\end{equation}
in others words, we looks at the representation $A\to r(A)$. The equations \eqref{eq:produit_e_m} makes
\[
  \sum_{m=-j}^j\left(
                      r(A)^l_me_l(\xi)\overline{ r(A)^k_me_k(\eta)   }
		        -\delta^l_me_l(\xi)\delta^k_me_k(\eta)
                \right)=0.
\]
Since the functions
\begin{equation}
\begin{aligned}
 e_k\otimes\overline{e_l}\colon \eC^2\times\eC^2 &\to \eC \\
(\xi,\eta) &\mapsto  e_k(\xi)\overline{e_l(\eta)}
\end{aligned}
\end{equation}
 are linearly independent, one gets $\sum_m r(A)^k_l\overline{r(A)^l_m}=\delta^{kl}$, or
\begin{equation}
r(A)r(A)^*=\mtu,
\end{equation}
the conclusion is that in this basis, the matrices $\rho_j(A)$ are unitary.

\subsection{Clebsch-Gordan}
%//////////////////////////////////////////////////////////////////////

From the knowledge of the characters of $\rho_j$, one can decompose the product $\rho_s\otimes\rho_r$ into irreducible representations. For example,
\[
   V_{\frac{1}{2}}\otimes V_{\frac{1}{2}}=V_0\oplus V_1.
\]
More generally,
\begin{equation}
  V_s\otimes V_r=V_{|r-s|} \oplus V_{|r-s|+1}\oplus\ldots\oplus V_{r+s}.
\end{equation}
For this reason, the representation $\rho_j$ is sometimes called the \defe{spin $j$}{spin!representation!of $\SU(2)$} representation of $\SU(2)$.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Representations of \texorpdfstring{$\SO(3)$}{SO3}}
%---------------------------------------------------------------------------------------------------------------------------
\label{pg:reprez_SOt}

The group $\SO(3)$ is strongly linked with $SU(2)$ by the following property:
\begin{equation}
   \SO(3)=\frac{SU(2)}{\eZ_2}.
\end{equation}
proved in proposition \ref{PROPooDKPTooBnLflt}.

\begin{lemma}\label{lem:SO_3}
    A representation $\rho_j$ of $SU(2)$ is a representation of $\SO(3)$ if and only if $\rho_j(X)=\id$ for any $X$ in the kernel of the homomorphism $SU(2)\to \SO(3)$, namely: $\rho_j(\pm\mtu)=\id$.
\end{lemma}

\begin{proof}
    We consider $\dpt{\rho_j}{SU(2)}{\End{V_j}}$. By proposition \ref{PROPooDKPTooBnLflt} we have \( SO(3)=\SU(2)/\eZ_2\) and there exists a group homomorphism\footnote{Defined and studied in proposition \ref{PROPooGEHAooPCReoU}.} $\dpt{\psi}{SU(2)}{\SO(3)}$ such that $\psi(\mtu)=\psi(-\mtu)=\mtu$, which is an important equation because it ensures us that the rest of the expressions are well defined with respect to the class representative.

    If $\rho_j(-\mtu)=\mtu$, we define $\dpt{d_j}{\SO(3)}{\End{V}}$ by $d_j([x])=\rho_j(x)$ (check that this is well defined). With this,
    \[
      d_j([x])d_j([y])=\rho_j(x)\rho_j(y)=\rho_j(xy)=d_j([xy]).
    \]

    Now let us suppose that $d_j([x])=\rho_j(x)$ is a representation. Thus
    \[
      \rho_j(x)=d_j([x])=d_j([-x])=\rho_j(-x)=\rho_j(-\mtu)\rho_j(x),
    \]
    so $\rho_j(-\mtu)=\id_{V_j}$.

\end{proof}

Moreover, any representation of $\SO(3)$ comes from a representation $\tilde\rho$ of $SU(2)$ by setting $\tilde\rho(-\mtu)=\id$ and $\tilde\rho(x)=\rho([x])$.

Now, we research the representations of $SU(2)$ for which the matrix $-\mtu$ is represented by the identity operator. These will be representations of $\SO(3)$. The spin $j$ representations of $SU(2)$ is given by
\[
   \rho_j(X)\phi_{pq}(\xi)=\phi_{pq}(X^{-1}\xi).
\]
With $X=-\mtu$, this gives: $\phi_{pq}(-\xi)=(-1)^{p+q}\phi_{pq}(\xi)$. If we want it to be equal to $\phi_{pq}(\xi)$, we need $p+q=2j$ even. This is true if and only if $j\in\eN$.

\label{pg:reprez_SO3}The conclusion is that the irreducible representations of $\SO(3)$ are the integer spin irreducible representations of $\SU(2)$. Note that the non relativistic mechanics has $\SO(3)$ as group of space symmetry. Thus there are no hope to find any half integer spin in a non relativistic theory.

%---------------------------------------------------------------------------------------------------------------------------
					\section{Representations of \texorpdfstring{$\SL(2,\eR)$}{SL2R} and \texorpdfstring{$\SU(2)$}{SU2}}
%---------------------------------------------------------------------------------------------------------------------------

The representation $\pi_m$ of $\SL(2,\eC)$ restricts to $\SL(2,\eR)$.

\begin{lemma}
The representation $\pi_m$ of $\SL(2,\eR)$ is irreducible.
\end{lemma}

\begin{proof}
If $W$ is an invariant space under $\pi_m\big( \SL(2,\eR) \big)$, then is is invariant under the derived representation $\rho_m\big( \gsl(e,\eR) \big)$. The proof of proposition~\ref{ProprhomirredsldeuxC} still holds here, so that $W=\mP_m$.
\end{proof}

\begin{theorem}
Let $\pi$ be an irreducible representation of $G=\SL(2,\eR)$ or $\SU(2)$ in a complex finite dimensional vector space $V$. Then $\pi$ is equivalent to one of the $\pi_m$.
\end{theorem}

\begin{proof}
Let $\lG$ be the Lie algebra of $G$. One important property shared by $\SL(2,\eR)$ and $\SU(2)$ is that $G=\exp(\lG)$. It is clear that the representation $d\pi$ on $\gsl(2,\eR)$ extends $\eC$-linearly to a representation $\rho$ of $\gsl(2,\eC)$. Looking on the basis \eqref{EqGenssudeux}, one sees that in fact the same is true for $\gsu(2)$ which $\eC$-linearly extends to $\gsl(2,\eC)$.

Let us prove that $\rho$ is irreducible. Let $W\neq\{ 0 \}$ be a subspace of $V$ invariant under $\rho(\lG)$. Then $W$ is invariant under $ e^{\rho(X)}=\pi( e^{X})$ for every $X\in\lG$. Since $\exp(\lG)=G$, the space $W$ is in fact invariant under $\pi(G)$, and is therefore equal to $V$.

Since $\rho$ is irreducible, we have $\rho=\rho_m$ for a certain $m$. Thus there exists an intertwining operator $A\colon V\to \mP_m$ such that
\[
	A\rho(X)=\rho_m(X)A
\]
for every $X\in\lG$. By linearity, for every $N\in\eN$, we have $A\rho\big( \sum_{k=1}^n X^k/k! \big)=\rho_m\big( \sum_{k=1}^n X^k/k! \big)A$, and at the limit, we have
\begin{equation}
	A e^{\rho(X)}= e^{\rho_m(X)A}.
\end{equation}
From that we deduce that $A\pi( e^{X})=\pi_m( e^{X})A$ which means that
\[
	A\pi(g)=\pi_m(g)A.
\]
That shows that $A$ intertwines $\pi$ and $\pi_m$, so that $\pi$ is equivalent to $\pi_m$.
\end{proof}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Representations of \texorpdfstring{$\so(2,d-1)$}{so2d}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here we deal with the representations of \( \so(2,d-1)\). For singleton theory as field theory, see the section~\ref{SecUKPhZVd} (you are welcome if you can fill that section which is for the moment almost empty).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Verma module}
%---------------------------------------------------------------------------------------------------------------------------

One can find text about these representations in \cite{Ferrata,Dolan_son,SingString}, while we will mainly follow the developments of \cite{SingletonCompposites,HowMassless,Teschner}. We are going to study representations of the algebra $\so(2,d-1)$ which fulfills the commutation relations described in lemma~\ref{LemCommsopqAlg}:
\begin{equation}		\label{EqCommsodeuxdmoinsun}
	[M_{ab},M_{cd}]=-i\eta_{ac}M_{bd}+i\eta_{ad}M_{bc}+i\eta_{bc}M_{ad}-i\eta_{bd}M_{ac}.
\end{equation}
Notice that $M_{ab}=-M_{ba}$. As convention, the indices $a$, $b$,\ldots run over $\{ 0,0',1,2,\ldots d-1 \}$ while $r$, $s$, \ldots run over $\{ 1,2,\ldots,d-1 \}$. As on page \pageref{PgDefsGenre}, we choose the convention
\begin{equation}
	\eta =
	\begin{pmatrix}
		\mtu_{2\times 2}\\
		&-\mtu_{(d-1)\times (d-1)}
	\end{pmatrix}.
\end{equation}
Notice that this convention numerically holds for the matrix $\eta_{st}$ as well as for its inverse $\eta^{st}$.

The algebra separates into two parts: the compact and the non compact part. The maximal compact subalgebra is $\so(2)\oplus\so(d-1)$ which is generated by $E=M_{00'}$ and $J_{rs}=M_{rs}$. The non compact generators are $M_{0'r}$ and $M_{0r}$ that we rearrange into ladder operators
\begin{equation}
	L^{\pm}_r=M_{0r}\mp iM_{0'r}.
\end{equation}
Using commutation relations \eqref{EqCommsodeuxdmoinsun}, one computes the commutators in the new basis. For example
\[
	[E,L^{\pm}_r]=[M_{0'0},M_{0r}]\mp i[E_{0'0},M_{0'r}]=\pm M_{0r}-iM_{0'r}=\pm L^{\pm}_r.
\]
The table of $\so(2,d-1)$ in this basis is
\begin{subequations}		\label{SubEqsCommssodeuxd}
	\begin{align}
		[E,L^{\pm}_r]&=\pm L^{\pm}_r\\
		[J_{rs},L_t^{\pm}]&=-i(\delta_{rt}L_s^{\pm}-\delta_{st}L_r^{\pm})\\
		[L_r^{-},L_s^+]&=2(iJ_{rs}+\delta_{rs}E)\\
		[J_{rs},J_{tu}]&=-i\delta_{ac}M_{bd}+i\delta_{ad}M_{bc}+i\delta_{bc}M_{ad}-i\delta_{bd}M_{ac}.
	\end{align}
\end{subequations}
The unitary properties are $(M_{rs})^{\dag}=M_{rs}$, $E^{\dag}=E$ and $(L^{\pm}_r)^{\dag}=L_r^{\mp}$. From these commutators, we deduce the following rules that will be always used
\begin{subequations}
	\begin{align}
		L^-_rL^+_s&=L^+_sL^-_r+2(iJ_{rs}+\delta_{rs}E)\\
		J_{rs}L^+_t&=L^+_tJ_{rs}-i(\delta_{rt}L^+_s-\delta_{st}L^+_r)\\
		EL^+_r&=L^+_rE+L^+_r.
	\end{align}
\end{subequations}

The Cartan algebra of $\so(d)$ is given by the elements $A_p=M_{2p-1,2p}$ with $p=1,\ldots, r$ for $\so(2r)$ and $\so(2r+1)$.

The unitary irreducible representations of $\so(2,n)$ have the form $\mD(e_0,\bar\jmath)$. It is given by a basis vector $| e_0,\bar\jmath \rangle$ on which $E$ and $J_{rs}$ act by their respective representations (of $\so(n)$ and $\so(2)$). The \defe{energy}{energy!in the representations of $\so(2,d-1)$} of the vector $\ket{e,\overline{ m }}$ is its eigenvalue for the operator $E$, namely $e$:
\begin{equation}
	E\ket{e,\overline{ m }}=e\ket{e,\overline{ m }}.
\end{equation}
Using the commutators \eqref{SubEqsCommssodeuxd}, we find $L_r^{\pm}=(E\pm 1)L_r^{\pm}$, so that
\begin{equation}
	E L^{\pm}_r\ket{e,\overline{ m }} =(e\pm 1)\ket{e,\overline{ m }}.
\end{equation}
We see that the ladder operator $L_r^+$ raises the value of the energy of one unit, while the operator $L_r^-$ lower the energy of one unit. The vector $\ket{e_0,\bar\jmath}$ is the \defe{vacuum vector}{vacuum!vector}, it has the lowest energy in the sense that $L^{-}_r| e_0,\jmath \rangle=0$. A \defe{scalar representation}{scalar!representation} is a representation with $\bar\jmath=0$. They are, logically, denoted by $\mD(e_0)$ and its vacuum is $| e_0 \rangle$ which satisfies
\begin{align}		\label{Eqaldefketezerovac}
	J_{rs}| e_0 \rangle & = 0	& (E-e_0)| e_0 \rangle&=0	&L_r^{-}| e_0 \rangle&=0.
\end{align}
Then one build the generalised Verma module
\begin{equation}	\label{EqmVVermaldots}
	\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}.
\end{equation}
Notice that the Verma module is not automatically irreducible. We will soon build irreducible representations by taking quotient of the Verma module by its singular module.

In order to compute the norm of $L_s^+L_r^+\ket{e_0}$, we compute $L^-_rL^-_sL^+_sL^+_r\ket{e_0} =4\big(E+E^2+\delta_{rs}E^2+(J_{rs})^2\big)\ket{e_0}$. In order to get that result, we moved all the $L^-$ on the right using the commutation relation, and we taken into account the simplifications induced by the definition relations \eqref{Eqaldefketezerovac}. Now, using the relation $J_{rs}\ket{e_0}=0$, we have
\begin{equation}
	4\big(E+E^2+\delta_{rs}E^2\big)\ket{e_0}.
\end{equation} We also have
\begin{equation}
	(E-e_0)L^+_sL^+_s\ket{e_0}=0.
\end{equation}

\begin{proposition}
The vectors $L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}$ and $L^+_{t_1}\ldots L^+_{t_l}\ket{e_0}$ are orthogonal if $k\neq l$.
\end{proposition}
That proposition says that different layers are orthogonal\quext{À justifier en analysant qui est exactement $\lH$ et les racines simples, mais ça me semble ok.}

\begin{proof}
We proceed by induction. We suppose that the result is proved for $k,l\geq n$, and we prove that
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}=0.
\end{equation}
First, remark that, using the commutation relations and the fact that $J_{rs}\ket{e_0}=0$ and $E\ket{e_0}=e_0\ket{e_0}$, the vectors
\begin{subequations}		\label{SubEqsJELLket}
	\begin{align}
		J_{st}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}\\
		EL^+_{r_1}\ldots L^+_{r_k}\ket{e_0}
	\end{align}
\end{subequations}
are combinations of vectors of the form $L^+_{a_1}\ldots L^+_{a_k}\ket{e_0}$. Now, we have
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}= L^-_{t_1}\ldots L^-_{t_n}\big( L^+_{r_1}L^-_{t_{n+1}}+2i(J_{t_{n+1},r_1} + \delta_{t_{n+1},r_1}E ) \big)L^+_{r_2}\ldots L^+_{r_n}\ket{e_0}
\end{equation}
which decomposes in three terms. The first one is
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_n}L^+_{r_1}L^-_{t_{n+1}}L^+_{r_2}\ldots L^+_{r_n}\ket{e_0},
\end{equation}
and according to equations \eqref{SubEqsJELLket}, the two other terms reduce to zero. Continuing that way, the operator $L^-_{t_{n+1}}$ advance of one position at each step and finishes to kill himself on $\ket{e_0}$.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Singular module}
%---------------------------------------------------------------------------------------------------------------------------

Let us compute the norm of the general vector $L^+_{r_1}\ldots L^+_{r_k}\ket{s_0,s}$. We have
\begin{equation}
	\begin{aligned}[]
		L^-_{r_k}\ldots L^-_{r_1}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0,s}
				&= L^-_{r_k}\ldots L^-_{r_2} (L^+_{r_1}L^-_{r_2}+2E) L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&= L^-_{r_k}\ldots L^-_{r_2}L^+_{r_1}L^-_{r_2}L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&\quad +2(e_0+k-1)L^-_{r_k}\ldots L^-_{r_2} L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}
	\end{aligned}
\end{equation}
Using again and again the commutation relations, we eliminate all the operators $L^+_r$ and we obtain a sum of terms of the form $(e_0+k-l)$. It will, obviously be positive for large enough $e_0$. Thus, unitarity of the representation is enforced for large values of $e_0$, and there exists a lower bound $E_0(s)$ such that negative norm states appears when $e_0<E_0(s)$. If $e_0=E_0(s)$, then these vectors have a vanishing norm.

Let us consider a limit representation: $e_0=E_0$; there are vectors of vanishing norm, but no vectors with negative norm. In that case, if $v\cdot v=0$, then $v\cdot w=0$ for every other vector $w$. Indeed, if $v\cdot w\neq 0$, we have
\begin{equation}
	(v-w)\cdot(v-w)=w\cdot w - v\cdot w-w\cdot v,
\end{equation}
which holds for every positive multiple $\lambda v$ and $\mu w$. Choosing a big $\lambda$ and a small $\mu$, the norm of $\lambda v-\mu w$ becomes negative. What we proved is
\begin{lemma}
	If the energy $e_0$ of a representation saturates the unitary condition, then a vector with vanishing norm is orthogonal to every other vectors. Moreover, the vectors with vanishing norm form an invariant subspace.
\end{lemma}
The second part is the fact that, if $A\in\lG$, and $\| \ket{\psi} \|=0$  then $\| A\ket{\psi} \|=0$, because it is the scalar product of $\ket{\psi}$ with the vector $A^{\dag}A\ket{\psi}$. The submodule made of vectors of zero norm is the \defe{singular submodule}{singular!submodule}, and is denoted by $\mS(e_0,s)$.

\begin{proposition}		\label{PropSinModRedSSIADesNuls}
A module is reducible if and only if it possesses a vector $\ket{v}$ (different from $\ket{e_0,s}$) such that $L^-_r\ket{v}=0$ for every $r$. Such a vector is said to be \defe{null}{null vector}.
\end{proposition}

\begin{proof}
Since the energy is bounded from bellow, applying several times the lowering operators $L^-_r$ on any vector ends up on zero. Thus, any submodule contains a vector $\ket{v}$ such that $L^-\ket{v}=0$ for every $r$. If that vector is not $\ket{e_0,s}$, then the submodule is a proper submodule.

If $\ket{v}\neq\ket{e_0,s}$, then it is of the form $L^+_{\bar r}\ket{e_0,s}$ and its norm is given by
\begin{equation}
	\| L^+_{\bar r}\ket{e_0,s} \|=\bra{e_0,s} L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0
\end{equation}
because $L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0$ by assumption.
\end{proof}
From the Verma module \eqref{EqmVVermaldots}, we thus extract the irreducible representation taking the quotient by the singular module:
\begin{equation}
	\mH(e_0) = \mV(e_0)/\mS(e_0).
\end{equation}

Most of time, we have only one extra vacuum, let $\ket{e'_0,s'}$, and in this case, the whole singular module is generated by vectors of the form
\begin{equation}
	L^+_{r_1}\ldots L^+_{r_k}\ket{e'_0,s'}.
\end{equation}
Let $\ket{e_0,s}$ be the vacuum with $s=(s_1,s_2,0,\ldots,0)$, corresponding to the Young diagram
\begin{equation}
   \input{auto/pictures_tex/Fig_AIFsOQO.pstricks}
%	\input{image_Young_sssSing.pstricks}
\end{equation}
where the first line has $s_1$ boxes and the second one has $s_2$ boxes. Thanks to theorem~\ref{ThoOpqrepreTens}, it can be realized with the tensor
\begin{equation}
	v_{a_1,\ldots a_{s_2}b_1\ldots s_1}(e_0)
\end{equation}
which is separately symmetric in the indices $a$ and $b$, in the same time as being antisymmetric in the couples $a_i$, $b_i$ when $i\leq s_2$, for example,
\begin{equation}
	v_{a_1\ldots a_{s_2},b_1\ldots b_{s_1}} = -v_{b_1 a_1\ldots a_{s_2},b_2\ldots b_{s_1}}.
\end{equation}
In particular, if we symmetrise $v$ on $s_1+1$ indices, we always found zero. Moreover, all the traces vanishes. If $\eta$ is the metric of $O(D-1)$, we have for example
\begin{equation}
	\eta^{b_1b_2}v_{a_1\ldots a_{s_2},b_1,\ldots b_{s_2}}=0.
\end{equation}

The vectors of the first level are the ones of the form	$L^+_r\ket{e_0,s}$. As far as notations are concerned, we have
\begin{equation}
	L^+_rv_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}=(L^+_rv)_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}.
\end{equation}
Remark that the operators $\{ L_r^+ \}_{r=1,\ldots,D-1}$ carry a representation of $o(D-1)$, namely the vector representation. Thus, the states of the first level form the representation given by the tensor product of $(s_1,s_2,0,\ldots)$ and the vector representation. In order to see the irreducible components of that representation, we have to know what are the symmetry properties that we can give to the indices
\begin{equation}
	r,a_1,\ldots,a_{s_2},b_1,\ldots,b_{s_1}.
\end{equation}
There are three possibilities: we can contract the $r$ with one of the $a_i$ (by symmetry, all of these contractions are equivalent), or with one of the $b_i$, or add one box in the Young diagram. The latter possibility splits into three cases: the diagram $(s_1,s_2,0,\ldots)$ can be transformed in $(s_1+1,s_2,0,\ldots)$, $(s_1,s_2+1,0,\ldots)$ or $(s_1,s_2,1,0,\ldots)$. So we have $5$ irreducible component in the $o(D-1)$ representation carried by the level one.

The question that naturally arises is to know if one of these have a singular vacuum. In other words, if $\Pi_{\beta}$ are the projections to the irreducible components, do we have
\begin{equation}
	L^-_{t}\Pi_{\alpha}\big( L^+_rv_{\bar a,\bar b}(e_0) \big)=0
\end{equation}
for a certain $e_0$?

Notice that the contraction with the last $b_i$'s is not the same as the one with the firsts ones because of the symmetry properties with respect to the $a_i$'s. The first representation with cell cut is given by
\begin{equation}
	v^1_{\bar a,b_1\ldots b_{s_1-1}}	=\eta^{rt}L^+_r\big\{ v_{\bar a,b_1 \ldots b_{s_1-1}t}(e_0)
							+\frac{ s_2 }{ s_1-s_2+1 } v_{ca_1\ldots a_{s_2-1},b_1\ldots b_{s_1-1}a_{s_2}(e_0)}\big\},
\end{equation}
while the second representation with cell cut is easier:
\begin{equation}
	v^2_{a_1\ldots a_{s_{2}-1},\bar b}=\eta^{rt}L^+_rv_{ta_1\ldots a_{s_2-1},\bar b}(e_0).
\end{equation}
Now, the sport is to compute $L^-_qv^1_{\bar a,b_1\ldots b_{s_1-1}}$ and $L^-_q v^2_{a_1\ldots a_{s_{2}-1},\bar b}$.

\begin{probleme}
Il y a du calcul non terminé, ici.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{The quotient for the scalar singleton}
%---------------------------------------------------------------------------------------------------------------------------

The value of the energy which saturates the unitary condition is $1$ when $s=\frac{1}{ 2 }$ and $\frac{ 1 }{2}$ when $s=0$. That is the reason why we consider the two special representations
\begin{equation}
	\begin{aligned}[]
		\rDi&=\mD(1,\frac{ 1 }{2})		&& \rRac&=\mD(\frac{ 1 }{2},0).
	\end{aligned}
\end{equation}
We are now interested in the scalar case, the $Rac$.

We know that, when $\epsilon_0$ is the value of the energy which saturates the unitary condition $e_0\geq \frac{ d-3 }{ 2 }$ (in the scalar case, then
\begin{enumerate}
\item the vectors $L^+_sL^+_s\ket{\epsilon_0}$ are singular vectors,
\item the vectors $L^+_{r_1}\cdots L^+_{r_n}L^+_sL^+_s\ket{\epsilon_0}$ is orthogonal to all other states, it is a null vector.
\end{enumerate}
On the other hand, we know from proposition~\ref{PropSinModRedSSIADesNuls} that a module is reducible if and only if it has a vector $\ket{v}\neq\ket{e_0,s}$ such that $L^-_r\ket{v}=0$ for every $r$. Thus one constructs irreducible representations by taking the quotient of the Verma module by the singular module.

What is the dimension of the scalar singleton? We have to count how many different vectors we have in the Verma module $\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}$, and which \emph{are not} build over $L^+_sL^+_s\ket{e_0}$. In the case of $\SO(2,3)$, we have the generators $L^+_1$, $L^+_2$ and $L^+_3$ (which are commuting), so the only vectors that are left after removing the singular modules are the seven following ones: $L^+_1\ket{e_0}$, $L^+_2\ket{e_0}$,$L^+_3\ket{e_0}$, $L^+_1L^+_2\ket{e_0}$, $L^+_1L^+_3\ket{e_0}$,$L^+_2L^+_3\ket{e_0}$, and $L^+_1L^+_2L^+_3\ket{e_0}$. The scalar singleton representation is thus $7$ dimensional.
