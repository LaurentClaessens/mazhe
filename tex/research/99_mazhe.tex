% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014, 2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\begin{definition}      \label{DEFooVBPKooGxlDBn}
    A \defe{Lie algebra}{Lie algebra} is a vector space \( \lG\) on \( \eK(=\eR,\eC)\) endowed with a bilinear operation \( (x,y)\mapsto [x,y]\) from \( \lG\times\lG\) with the properties
    \begin{enumerate}
        \item
            \( [x,y]=-[y,x]\)
        \item
            \( \big[ x,[y,z] \big]+\big[ y,[z,x] \big]+\big[ z,[x,y] \big]=0\).
    \end{enumerate}
    The second condition is the \defe{Jacobi identity}{Jacobi!identity}.
\end{definition}

\begin{definition}      \label{DEFooDUEUooZLhKdv}
    \defe{derivation}{derivation!of a Lie algebra} of $\lA$:
    \begin{equation}
      D[X,Y]=[DX,Y]+[X,DY]
    \end{equation}
    for every $X$, $Y\in\lA$. 
\end{definition}

\begin{lemma}       \label{LemadhomomadXadYadXY}
    The adjoint map is an homomorphism \( \ad\colon \lG\to \GL(\lG)\). In other terms for every \( X,Y\in\lG\) we have
    \begin{equation}
        \big[ \ad(X),\ad(Y) \big]=\ad\big( [X,Y] \big)
    \end{equation}
    as operators on \( \lG\). In particular the algebra acts on itself and \( \lG\) carries a representation of each of its subalgebra.
\end{lemma}

\begin{proof}
    Using the fact that \( \ad(X)\) is a derivation and Jacobi, for \( Z\in\lG\) we have
    \begin{subequations}
        \begin{align}
            \big[ \ad(X),\ad(Y) \big]Z&=\ad(X)\ad(Y)Z-\ad(Y)\ad(X)Z\\
            &=\big[ [X,Y],Z \big]+\big[ Y,[X,Z] \big]-\big[ [Y,X],Z \big]-\big[ X,[Y,Z] \big]\\
            &=\ad\big( [X,Y] \big)Z.
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}[\cite{BIBooWSHFooKoDjAs}]       \label{PROPooSWQSooSEfTuX}
    The set of all the smooth\footnote{Smooth means \( C^{\infty}\).} vector fields on a manifold is a Lie algebra\footnote{Definition \ref{DEFooVBPKooGxlDBn}.}.
\end{proposition}

\begin{proof}
    If \( X\) and \( Y\) are smooth vector fields, the commutator \( [X,Y]\) is still smooth by proposition \ref{DEFooHOTOooRaPwyo}. From the definition \eqref{EQooDSKWooXdjPPP}, it is apparent that \( [X,Y]=-[Y,X]\). The Jacobi identity remains to be proven.

    Let \( X,Y,Z\) be smooth vector fields. If \( f\colon M\to \eR\) is a smooth function, we write \( XYf\) the function \( x\mapsto X_x(Yf)\) and similarly \( XYZf\) and other combinations. In particular we have
    \begin{equation}
        [X,Y]f=X(Yf)-Y(Xf)=XYf-YXf
    \end{equation}
    which is shorthanded into \( [X,Y]=XY-YX\). We also have
    \begin{equation}
        \big[ X, [Y,Z] \big]=[X,YZ-ZY]=XYZ-XZY-YZX+ZYX.
    \end{equation}
    Making the same with the two other terms we see that
    \begin{equation}
        \big[ X,[Y,Z] \big]+\big[ Y,[Z,X] \big]+\big[ Z,[X,Y] \big]=0.
    \end{equation}
\end{proof}

\begin{normaltext}
    When the manifold is a Lie group, the Lie algebra of the smooth vector fields on a smooth manifold is not \emph{the} Lie algebra of the Lie group. The latter is the subalgebra of left invariant vector fields \ldots well in fact it is not even that : the Lie algebra of a Lie group is the tangent space to the identity and the Lie algebra structure is build \emph{via} the left invariant vector fields.

    We'll discuss that later.
\end{normaltext}

\section{Adjoint representation}
%////////////////////////////////////

Let $G$ be a Lie group and $g\in G$ we consider the map $\AD(g)\colon G\to G$ given by $\AD(g)h=g hg^{-1}$. This is an analytic automorphism of $G$. We define:\nomenclature[D]{$\Ad$}{Adjoint representation}
\[
    \Ad(g)=d\AD(g)_e.
\]
Using equation $\varphi(\exp X)=\exp d\varphi_e(X)$ with $\varphi=I(g)$,
\begin{equation}\label{eq:sigma_X_sigma}
  g e^{X}g^{-1}=\exp[ \Ad(g)X ]
\end{equation}
for every $g\in G$ and $X\in\lG$. The map $g\to\Ad(g)$ is a homomorphism from $G$ to $\GL(\lG)$. This homomorphism is called the \defe{adjoint representation}{adjoint!representation!Lie group on its Lie algebra}\index{representation!adjoint} of $G$.

\begin{proposition}
The adjoint representation is analytic.
\end{proposition}

\begin{proof}
We have to prove that for any $X\in\lG$ and for any linear map $\dpt{\omega}{\lG}{\eR}$, the function $\omega(\Ad(g)X)$ is analytic at $g=e$. Indeed if we take as $\omega$ , the projection to the $i$th component and $X$ as the $j$th basis vector ($\lG$ seen as a vector space), and if we see the product $\Ad(g)X$ as a product matrix times vector, $(\Ad(g)X)_i$ is just $\Ad(g)_{ij}$. Then our supposition is the analyticity of $g\to\Ad(g)_{ij}$ at $g=e$. \quext{L'analicit√© de $\Ad$, elle vient par prolongement analytique depuis juste un point ?}

Now we prove it. Consider $f\in\Cinf(G)$, analytic at $g=e$ and such that $Yf=\omega(Y)$ for any $Y\in\lG$. Using equation \eqref{eq:sigma_X_sigma},
\begin{equation}
  \omega(\Ad(g)X)=(\Ad(g)X)f
                      =\Dsdd{ f(e^{t\Ad(g)X}) }{t}{0}
                      =\Dsdd{ f(g e^{tX}g^{-1}) }{t}{0},
\end{equation}
which is well analytic at $g=e$.
\end{proof}


\begin{proposition}
Let $G$ be a connected Lie group and $H$, an analytic subgroup of $G$. Then $H$ is a normal subgroup\index{normal!subgroup} of $G$ if and only if $\lH$ is an ideal in $\lG$.
\end{proposition}

\begin{proof}
We consider $X$, $Y\in\lG$. Formula $\exp tX\exp tY\exp-tY=\exp( tY+t^2[X,Y]+o(t^3) )$ and equation \eqref{eq:sigma_X_sigma} give
\[
   \exp\Big( \Ad(e^{tX})tY \Big)=\exp\Big(  tY+t^2[X,Y]+o(t^3)  \Big).
\]
Since it is true for any $X$, $Y\in\lG$, $\Ad(e^{tX})tY=tY+t^2[X,Y]$; thus
\begin{equation}
  \Ad(e^{tX})=\mtu+t[X,Y]+o(t^2).
\end{equation}
Since we know that $\dpt{d\Ad_e}{\lG}{\gl(\lG)}$ is a homomorphism ($\Ad$ is seen as a map $\dpt{\Ad}{G}{\GL(\lG)}$), taking the derivative of the last equation with respect to $t$ gives
\begin{equation}
  d\Ad_e(X)=\ad X.
\end{equation}
Then $\Ad(e^X)=e^{\ad X}$. Since is connected, an element of $G$ can be written as $\exp X$ for a certain $X\in\lG$\footnote{Because $G$ is generated by any neighbourhood of $e$ and there exists such a neighbourhood of $e$ which is diffeomorphic to a subset of $\lG$ by $\exp$.}. The purpose is to prove that $g\exp Xg^{-1}=\exp(\Ad(g)X)$ remains in $H$ for any $g\in G$ if and only if $\lH$ is an ideal in $\lG$. In other words, we want $\Ad(g)X\in\lH$ if and only if $\lH$ is an ideal. We can write $g=e^Y$ for a certain $Y\in\lG$. Thus
\[
  \Ad(g)X=\Ad(e^Y)X=e^{\ad Y}X.
\]
Using the expansion
\begin{equation}
e^{\ad Y}=\sum_k\us{k!}(\ad Y)^k,
\end{equation}
we have the thesis.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representation of the complex algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

When \(\lG \) is a real Lie algebra, the corresponding complex Lie algebra is defined as
\begin{equation}
    \lG_{\eC}=\lG\otimes_{\eR}\eC.
\end{equation}
We have, as an example, in the lemma \ref{LEMooVEJZooUVNdmE} the equality $\su(2)_{\eC}=\gsl(2,\eC)$.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooIGAFooTSUsJR}
    Let \( \lG\) be a real Lie algebra and \( \rho\colon \lG\to \GL(V)\) be an irreducible representation. There exists an irreducible representation \( \rho'\colon \lG_{\eC}\to \GL(V)\) such that \( \rho=\rho'|_{\lG}\).
\end{lemma}

\begin{proof}
    We prove that the map
    \begin{equation}
        \begin{aligned}
            \rho'\colon \lG_{\eC}&\to \GL(V) \\
            X+iY&\mapsto \rho(X)+i\rho(Y) 
        \end{aligned}
    \end{equation}
    is the representation we are searching for.

    \begin{subproof}
        \item[This is a representation]
            Using the linearity, \( [X+iY,Z+iT]=[X,Z]-[Y,T]+i[X,T]+i[Y,Z]\). On the one hand we have
            \begin{equation}
                \rho'\big( [X+iY,Z+iT] \big)=\rho\big( [X,Z]-[Y,T] \big)+i\rho\big( [X,T]+[Y,Z] \big);
            \end{equation}
            while on the other hand,
            \begin{subequations}
                \begin{align}
                    \big[ \rho'(X+iY),\rho'(Z+iT) \big]&=\big[ \rho(X)+i\rho(Y),\rho(Z)+i\rho(T) \big]\\
                    &=\big[ \rho(X),\rho(Z) \big]+i\big[ \rho(X),\rho(T) \big]\\
                    &\qquad+i\big[ \rho(Y),\rho(Z) \big]-\big[ \rho(Y),\rho(T) \big]\\
                    &=\rho\big( [X,Z] \big)-\rho\big( [Y,T] \big)+i\rho\big( [X,T]+[Y,Z] \big).
                \end{align}
            \end{subequations}
            This proves that the map \( \rho'\) commutes with the Lie bracket, so that \( \rho'\) is a representation.
        \item[Irreducible]
            Let \( W\) be a subspace of \( V\) and suppose that \( W\) is invariant under \( \rho'\). This means that, for every \( X,Y\in \lG_{\eC}\), we have \( \rho'(X+iY)W\subset W\). In particular, with \( Y=0\) we have
            \begin{equation}
                \rho(X)W=\rho'(X)W\subset W.
            \end{equation}
            This shows that \( W\) is invariant under \( \rho\). Since \( \rho\) is irreducible, the subspace \( W\) must be \( \{ 0 \}\) or \( V\). Thus \( \rho'\) is irreducible.
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Jordan decomposition}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{Jordan decomposition}
\index{decomposition!Jordan}

If $V$ is a finite dimensional space, a subspace $W$ in $V$ is \defe{invariant}{invariant!vector subspace} under a subset $G\subset\Hom(V,V)$ if $sW\subset W$ for any $s\in G$. The space $V$ is \defe{irreducible}{irreducible!vector space} when $V$ and $\{0\}$ are the only two invariant subspaces. The set $G$ is \defe{semisimple}{semisimple} if any invariant subspace has an invariant complement. In this case, the vector space split into $V=\sum_iV_i$ with $V_i$ invariant and irreducible.

% Reformulate these two ``Jordan'' theorems.
\begin{theorem}[Jordan decomposition]\index{Jordan decomposition}\index{decomposition!Jordan}
Any element $A\in\Hom(V,V)$ is decomposable in one and only one way as $A=S+N$ with $S$ semisimple and $N$ nilpotent and $NS=SN$. Furthermore, $S$ and $N$ are polynomials in $A$. More precisely:

If $V$ is a complex vector space and $A\in\Hom(V,V)$ with $\lambda_1,\ldots,\lambda_r$ his eigenvalues, we pose
\[
V_i=\{ v\in V\tq (A-\lambda_i\mtu)^kv=0 \textrm{ for large enough $k$}\}.
\]
Then

\begin{enumerate}\label{tho:jordan}
\item $V=\sum_{i=1}^rV_i$,
\item each $V_i$ is invariant under $A$,
\item the semisimple part of $A$ is given by
\[
   S(\sum_{i=1}^rv_i)=\sum_{i=1}^r\lambda_iv_i,
\]
for $v_i\in V_i$,

\item the characteristic polynomial of $A$ is
\[
  \det(\lambda\mtu-A)=(\lambda-\lambda_1)^{d_1}\ldots(\lambda-\lambda_r)^{d_r}
\]
where $d_i=\dim V_i$ ($1\leq i\leq r$).
\end{enumerate}
\end{theorem}

Now we give a great theorem without proof.
\begin{theorem}[Jordan decomposition]
Let $V$ be a finite dimensional vector space and $x\in\End{V}$.

\begin{enumerate}
\item There exists one and only one choice of $x_s,x_n\in\End(V)$ such that $x=x_s+x_n$, $x_s$ is semisimple, $n_n$ is nilpotent and $[x_s,x_n]=0$.

\item There exists polynomials $p$ and $q$ without independent term such that $x_s=p(x)$, $x_n=q(x)$; in particular if $y\in\End{V}$ commutes with $x$, then it commutes with $x_s$ and $x_n$.

\item If $A\subset B\subset V$ are subspaces of $V$ and if $x(B)\subset A$, then $x_s(B)\subset A$ and $x_n(B)\subset A$.
\end{enumerate}
\label{prop:Jordan_decomp}
\end{theorem}

\begin{lemma}\label{lem:Jordan_ad}
    Let $x\in\End{V}$ with his Jordan decomposition $x=x_s+x_n$. Then the Jordan decomposition of $\ad x$ is
    \begin{equation}\label{eq:ad_x_xs_xn}
       \ad x=\ad x_s+\ad x_n.
    \end{equation}
\end{lemma}

\begin{proof}
We already know that $\ad x_s$ is semisimple and $\ad x_n$ is nilpotent. They commute because $[\ad x_s,\ad x_n]=\ad[x_s,x_n]=0$. Then the unicity part of Jordan theorem~\ref{prop:Jordan_decomp} makes \eqref{eq:ad_x_xs_xn} the Jordan decomposition of $\ad x$.
\end{proof}

\begin{lemma}\label{lem:M_nil}
Let $A\subset B$ be two subspace of $\gl(V)$ with $\dim V<\infty$. We pose
\[
   M=\{x\in\gl(V)\tq [x,B]\subset A\},
\]
and we suppose that $x\in M$ verify $\tr(x\circ y)=0$ for all $y\in M$. Then $x$ is nilpotent.
\end{lemma}

\begin{proof}
We use the Jordan decomposition $x=x_s+x_n$ and a basis in which $x_s$ takes the form $diag(a_1,\ldots,a_m)$; let $\{v_1,\ldots,v_m\}$ be this basis. We denotes by $E$ the vector space on $\eQ$ spanned by $\{a_1,\ldots,a_m\}$. We want to prove that $x_s=0$, i.e. $E=0$. Since $E$ has finite dimension, it is equivalent to prove that its dual is zero. In other words, we have to see that any linear map $\dpt{f}{E}{\eQ}$ is zero.

We consider $y\in\gl(V)$, an element whose matrix is $diag(f(a_1),\ldots,f(a_m))$ and $(E_{ij})$, the usual basis of $\gl(V)$. We know that
\begin{subequations}
\begin{align}
  (\ad x_s)E_{ij}&=(a_i-a_j)E_{ij},\\
  (\ad y)E_{ij}&=(f(a_i)-f(a_j))E_{ij}.
\end{align}
\end{subequations}
It is always possible to find a polynomial $r$ on $\eR$ without constant term such that $r(a_i-a_j)=f(a_i)-f(a_j)$. Note that this is well defined because of the linearity of $f$: if $a_i-a_j=a_k-a_l$, then $f(a_i)-f(a_j)=f(a_k)-f(a_l)$. Since $\ad x_s$ is diagonal, $r(\ad x_s)$ is the matrix with $r(\ad x_s)_{ii}$ on the diagonal and zero anywhere else. Then $r(\ad x_s)=\ad y$. By lemma~\ref{lem:Jordan_ad}, $\ad x_s$ is the semisimple part of $\ad x$, then $\ad y$ is  a polynomial without constant term with respect to $\ad x$ (second point of theorem~\ref{prop:Jordan_decomp}).

Since $(\ad y)B\subset A$, $y\in M$ and $\tr(xy)=0$. It is easy to convince ourself that the $s_n$ part of $x$ will not contribute to the trace because $x_n$ is strictly upper triangular and $y$ is diagonal. From the explicit forms of $x_s$ and $y$,
\[
  \tr(xy)=\sum_ia_if(a_i)=0.
\]
This is a $\eQ$-linear combination of element of $E$: we have to see it as $a_i$ being a basis vector and $f(a_i)$ a coefficient, so that we can apply $f$ on both sides to find $0=\sum_if(a_i)^2$. Then for all $i$, $f(a_i)=0$, so that $f=0$ because  the $a_i$ spans $E$.
\end{proof}

\begin{definition}[semisimple endomorphism]\index{semisimple!endomorphism}
    If $V$ is a finite dimensional vector space, we say that an element $u\in\End{V}$ is \defe{semisimple}{semisimple!endomorphism}\label{pg:def_semisimple} if every \( u\)-invariant subspace of \( V\) has a complementary \( u\)-invariant.
\end{definition}

\begin{proposition}[\cite{SIUaYwD}]
    If \( V\) a vector space over an algebraically closed field, an endomorphism is semisimple if and only if it is diagonalizable.
\end{proposition}
% TODO: give the proof; it is easy.

\label{pg:E_ij}Let $E_{kl}$ be the $(n+2)\times(n+2)$ matrix with a $1$ at position $(k,l)$ and $0$ anywhere else: $(E_{kl})_{ij}=\delta_{ki}\delta_{lj}$. An easy computation show that \nomenclature{$E_{ij}$}{Matrix full of zero's and $1$ at position $ij$}
\begin{equation}        \label{EqFormMulEmtr}       %\label{EqJsqnmunmtu}       Ce second label est certainement une erreur.
    E_{kl}E_{ab}=\delta_{la}E_{kb},
\end{equation}
and
\begin{equation}\label{comm_de_E}
    [E_{kl},E_{rs}]=\delta_{lr}E_{ks}-\delta_{sk}E_{rl}.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Killing form}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    The \defe{Killing form}{Killing!form} of $\mG$ is the symmetric bilinear form:
    \begin{equation}
                 B(X,Y)=Tr(\ad X\circ \ad Y).
    \end{equation}
\end{definition}

\begin{proposition}
    It is \defe{invariant}{invariant!form} in the sense of
    \begin{equation}                        \label{eq:Killing_invariant}
         B\big((\ad S)X,Y\big)=-B\big(X,(\ad S)Y\big),
    \end{equation}
    $\forall X$, $Y$, $S\in\mG$.
\end{proposition}

\begin{proposition} \label{PropAutomInvarB}
If $\dpt{\varphi}{\mG}{\mG}$ is an automorphism of $\mG$, then
\[
   B(\varphi(X),\varphi(Y))=B(X,Y).
\]
\label{prop:auto_2}
\end{proposition}

\begin{proof}
The fact that $\varphi$ is an automorphism of $\mG$ is written as $\varphi\circ\ad X=\ad(\varphi(X))\circ\varphi$, or
\[
  \ad(\varphi(X))=\varphi\circ\ad X\circ\varphi^{-1}.
\]
Then
\begin{equation}
\begin{split}
\tr(\ad(\varphi(X))\circ\ad(\varphi(Y)))&=\tr(\varphi\circ\ad X\circ\varphi^{-1}\circ\varphi\ad Y\circ\varphi^{-1})\\
                                &=\tr(\ad X\circ\ad Y).
\end{split}
\end{equation}
\end{proof}


\begin{remark}
The Killing $2$-form is a map $\dpt{B}{\mG\times\mG}{\eR}$. When we say that it is preserved by a map $\dpt{f}{G}{G}$, we mean that it is preserved by $df$: $B(df\cdot,df\cdot)=B(\cdot,\cdot)$.
\end{remark}

\begin{remark}
The Killing form is \emph{a priori} only defined on $\mG=T_eG$. For $A$, $B\in T_gG$, one naturally defines
\begin{equation}
  B_g(A,B)=B(dL_{g^{-1}}A,dL_{g^{-1}}B).
\end{equation}
This assures the left invariance of $B$. Now we prove the right invariance.
\end{remark}

An other important property of the Killing form is its bi-invariance.

\begin{lemma}
Let $\lG$ be a Lie algebra and $\lI$ an ideal in $\lG$. Let $\dpt{B}{\lG\times\lG}{\eR}$ be the Killing form on $\lG$ and $\dpt{B'}{\lI\times\lI}{\eR}$, the one of $\lI$. Then $B'=B|_{\lI\times\lI}$, i.e. the Killing form on $\lG$ descent to the ideal $\lI$.
\label{lem:Killing_descent_ideal}
\end{lemma}

\begin{proof}
If $W$ is a subspace of a (finite dimensional) vector space $V$ and $\dpt{\phi}{V}{W}$ and endomorphism, then $\tr\phi=\tr(\phi|_W$). Indeed, if $\{X_1,\ldots,X_n\}$ is a basis of $V$ such that $\{X_1,\ldots,X_r\}$ is a basis of $W$, the matrix element $\phi_{kk}$ is zero for $k>r$. Then
\[
  \tr\phi=\sum_{i=1}^{n}\phi_{ii}=\sum_{i=1}^r\phi_{ii}=\tr(\phi|_W).
\]

Now consider $X$, $Y\in\lI$; $(\ad X\circ\ad Y)$ is an endomorphism of $\lG$ which sends $\lG$ to $\lI$ (because $\lI$ is an ideal). Then
\[
B'(X,Y)=\tr\big( (\ad X\circ\ad Y)|_{\lI} \big)=\tr(\ad X\circ\ad Y)=B(X,Y).
\]
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Solvable and nilpotent algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

If $\lG$ is a Lie algebra, the \defe{derived Lie algebra}{derived!Lie algebra}\index{Lie!algebra!derived} is
\[
   \dD\lG=\Span\{[X,Y]\tq X,Y\in\lG\}.
\]
We naturally define $\dD^0\lG=\lG$ and $\dD^n\lG=\dD(\dD^{n-1}\lG)$ this is the \defe{derived series}{derived!series}. Each $\dD^n\lG$ is an ideal in~ $\lG$. We also define the \defe{central decreasing sequence}{central!decreasing sequence} by $\lA^0=\lA$, $\lA^{p+1}=[\lA,\lA^p]$.

\begin{definition}
The Lie algebra $\lG$ is \defe{solvable}{solvable!Lie algebra}\index{Lie!algebra!solvable} if there exists a $n\geq 0$ such that $\dD^n\lG=\{0\}$. A Lie group is solvable when its Lie algebra is\index{Lie!group!solvable}\index{solvable!Lie group}.

    The Lie algebra  \( \lG\) is \defe{nilpotent}{nilpotent!Lie algebra} if \( \lG^n=0\) for some~\( n\). We say that \( \lG\) is \( \ad\)-nilpotent if \( \ad(X)\) is a nilpotent endomorphism of \( \lG\) for each \( X\in\lG\).
\end{definition}

Do not confuse \emph{nilpotent} and \emph{solvable} algebras. A nilpotent algebra is always solvable, while the algebra spanned by $\{ A,B \}$ with the relation $[A,B]=B$ is solvable but not nilpotent.

If $\lG\neq\{0\}$ is a solvable Lie algebra and if $n$ is the smallest natural such that $\dD^n\lG=\{0\}$, then $\dD^{n-1}\lG$ is a non zero abelian ideal in $\lG$. We conclude that a solvable Lie algebra is never semisimple (because the center of a semisimple Lie algebra is zero).

A Lie algebra is said to fulfil the \defe{chain condition}{chain!condition} if for every ideal $\lH\neq\{0\}$ in $\lG$, there exists an ideal $\lH_1$ in $\lH$ with codimension $1$.

\begin{lemma}
A Lie algebra is solvable if and only if it fulfils the chain condition.
\end{lemma}

\begin{proof}
\subdem{Necessary condition}
The Lie algebra $\lG$ is solvable (then $\dD\lG\neq\lG$) and $\lH$ is an ideal in $\lG$. We consider $\lH_1$, a subspace of codimension $1$ in $\lH$ which contains $\dD\lH$. It is clear that $\lH_1$ is an ideal in $\lH$ because $[H_1,H]\in\dD\lH\subset\lH_1$.
\subdem{Sufficient condition}
We have a sequence
\begin{equation}\label{eq:solvable_chaine}
   \{0\}=\lG_n\subset\lG_{n-1}\subset\ldots\subset\lG_0=\lG
\end{equation}
where $\lG_r$ is an ideal of codimension $1$ in $\lG_{r-1}$. Let $A$ be the unique vector in $\lG_{r-1}$ which don't belong to $\lG_r$.  When we write $[X,Y]$ with $X$, $Y\in\lG_{r-1}$, at least one of $X$ or $Y$ is not $A$ (else, it is zero) then at least one of the two is in $\lG_r$. But $\lG_r$ is an ideal; then $[X,Y]\in\lG_r$. Thus $\dD(\lG_{r-1})\subset\lG_r$ and
\[
\dD^n\lG=\dD^{n-1}\dD\lG\subset\dD^{n-1}\lG_1\subset\ldots\subset\lG_n=0.
\]
\end{proof}

\begin{theorem}[Lie theorem]\label{tho:Lie_Vu}\index{Lie!theorem}
    Consider $\lG$, a real (resp. complex) solvable Lie algebra and a real (resp. complex) vector space $V\neq\{0\}$. If $\dpt{\pi}{\lG}{\gl(V)}$ is a homomorphism, then there exists a non zero vector in $V$ which is eigenvector of all the elements of $\pi(\lG)$.
\end{theorem}

\begin{probleme}
    It is strange to be stated for real and complex Lie algebras. Following \cite{SamelsonNotesLieAlg}, this is only true for complex Lie algebras while there exists other versions for reals ones.
\end{probleme}

\begin{proof}
Let us do it by induction on the dimension of $\lG$. We begin with $\dim\lG=1$. In this case, $\pi$ is just a map $\dpt{\pi}{\lG}{\gl(V)}$ such that $\pi(aX)=a\pi(X)$. We have to find an eigenvector for the homomorphism $\dpt{\pi(X)}{V}{V}$. Such a vector exists  from the Jordan decomposition~\ref{tho:jordan}. Indeed, if there are no eigenvectors, there are no spaces $V_i$ and the decomposition $V=\sum V_i$ can't be true.

Now we consider a general solvable Lie algebra $\lG$ and we suppose that the theorem is true for any solvable Lie algebra with dimension less that $\dim\lG$. Since $\lG$ is solvable, there exists an ideal $\lH$ of codimension $1$ in $\lG$; then there exists a $e_0\neq 0\in V$ which is eigenvector of all the $\pi(H)$ with $H\in\lH$. So we have $\dpt{\lambda}{\lH}{\eR}$ naturally defined by
\[
  \pi(H)e_0=\lambda(H)e_0.
\]
Now we consider $X\in\lG\setminus\lH$ and $e_{-1}=0$, $e_p=\pi(X)^pe_0$ for $p=1,2,\ldots$ We will show that $\pi(H)e_p=\lambda(H)e_p\mod(e_0,\ldots,e_{p-1})$ for all $H\in\lH$ and $p\geq 0$. It is clear for $p=0$. Let us suppose that it is true for $p$. Then
\begin{equation}
\begin{split}
  \pi(H)e_{p+1}&=\pi(H)\pi(X)e_p\\
               &=\pi([H,X])e_p+\pi(X)\pi(H)e_p\\
           &=\lambda([H,X])e_p+\pi(X)\lambda(H)e_p\\
                      &\quad\mod(e_0,\ldots,e_{p-1},\pi(X)e_0,\ldots,\pi(X)e_{p-1}).
\end{split}
\end{equation}
But we can put $\pi([H,X])$ and $\pi(X)e_i$ into the modulus. Thus we have
\[
  \pi(H)e_{p+1}=\lambda(H)e_{p+1}\mod(e_0,\ldots,e_p).
\]

Now we consider the subspace of $V$ given by $W=\Span\{e_p\}_{p=1,\ldots}$. The algebra $\pi(\lH)$ leaves $W$ invariant and our induction hypothesis works on $(\pi(\lH),W)$; then one can find in $W$ a common eigenvector for all the $\pi(H)$. This vector is the one we were looking for.
\end{proof}

\begin{corollary}
Let $\lG$ be a solvable Lie group and $\pi$ a representation of $\lG$ on a finite dimensional vector space $V$. Then there exists a basis $\{e_1,\ldots,e_n\}$ of $V$ in which all the endomorphism $\pi(X)$, $X\in\lG$ are upper triangular matrices.
\label{cor:de_Lie_Vu}
\end{corollary}

\begin{proof}
Consider $e_1\neq 0\in V$, a common eigenvector of all the $\pi(X)$, $X\in\lG$. We consider $E_1=\Span\{e_1\}$. The representation $\pi$ induces a representation $\pi_1$ of $\lG$ on the space $V/E_1$. If $V/E_1\neq\{0\}$, we have a $e_2\in V$ such that $(e_2+E_1)\in V/E_1$ is an eigenvector of all the $\pi_(X)$.

In this manner, we build a basis $\{e_1,\ldots,e_n\}$ of $V$ such that $\pi(X)e_i=0\mod(e_1,\ldots,e_i)$ for all $X\in\lG$. In this basis, $\pi(X)$ has zeros under the diagonal.
\end{proof}

\begin{theorem}
Let $V$ be a real or complex vector space and $\lG$, a subalgebra of $\gl(V)$ made up with nilpotent elements. Then

\begin{enumerate}
\item $\lG$ is nilpotent;
\item $\exists v\neq 0$ in $V$ such that $\forall Z\in\lG$, $Zv=0$;
\item There exists a basis of $V$ in which the elements of $\lG$ are matrices with only zeros under the diagonal.
\end{enumerate}
\label{tho:trois_nil}
\end{theorem}

\begin{proof}
\subdem{First item} We consider a $Z\in\lG$ and we have to see that $\ad_{\lG}Z$ is a nilpotent endomorphism of $\lG$. Be careful on a point: an element $X$ of $\lG$ is nilpotent as endomorphism of $V$ while we want to prove that $\ad X$ is nilpotent as endomorphism of $\lG$. We denote by $L_Z$ and $R_Z$, the left and right multiplication; since we are in a matrix algebra, the bracket is given by the commutator: $\ad Z=L_Z-R_Z$. We have
\begin{equation}
(\ad Z)^p(X)=\sum_{i=0}^p(-1)^p \binom{p}{i}  Z^{p-i}XZ^i
\end{equation}
There exists a $k\in\eN$ such that $Z^k=0$. For this $k$, $(\ad Z)^{2k+1}$ is a sum of terms of the form $Z^{p-i}XZ^i$: either $p-i$ either $i$ is always bigger than $k$. But $\ad_{\lG}Z$ is the restriction of $\ad Z$ (which is defined on $\gl(V)$) to $\lG$. Then $\lG$ is nilpotent.

\subdem{Second item} Let $r=\dim\lG$. If $r=1$, we have only one $Z\in\lG$ and $Z^k=0$ for a certain (minimal) $k\in\eN$. We take $v$ such that $w=Z^{k-1}v\neq 0$ (this exists because $k$ is the minimal natural with $Z^k=0$). Then $Zw=0$.

Now we suppose that the claim is valid for any algebra with dimension less than $r$. Let $\lH$ be a strict subalgebra of $\lG$ with maximal dimension. If $H\in\lH$, $\ad_{\lG}H$ is a nilpotent endomorphism of $\lG$ which sends $\lH$ onto itself. Thus $\ad_{\lG}H$ induces a nilpotent endomorphism $H^*$ on the vector space $\lG/\lH$. We consider the set $\mA=\{H^*\tq H\in\lH\}$; this is a subalgebra of $\gl(\lG/\lH)$ made up with nilpotent elements which has dimension strictly less than $r$.

The induction assumption gives us a non zero $u\in \lG/\lH$ which is sent to $0$ by all $\mA$, i.e. $(\ad_{\lG}H)u=0$ in $\lG/\lH$. In other words, $u\in\lG\setminus\lH$ is such that $(\ad_{\lG}H)u\in\lH$.

The space $\lH+\eK X$ (here, $\eK$ denotes $\eR$ or $\eC$) of $\lG$ is a subalgebra of $\lG$. Indeed, with obvious notations,
\begin{equation}\label{eq:H_k_X}
[H+kX,H'+k'X]=[H,H']+\ad H(k'X)-\ad H'(kX)+kk'[X,X].
\end{equation}
The first term lies in $\lH$ because it is a subalgebra; the second and third therms belongs to $\lH$ by definition of $X$. The last term is zero. Since $\lH$ is maximal, $\lH+\eK X=\lG$. Then \eqref{eq:H_k_X} shows that $\lH$ is also an ideal. Now we consider
\[
  W=\{e\in V\tq\forall H\in\lH, He=0\}.
\]
Since $\dim\lH< r$, $W\neq\{0\}$ from our induction assumption. Furthermore, for $e\in W$, $HXe=[H,X]e+XHe=0$. Then $X\cdot W\subset W$. The restriction of $X$ to $W$ is nilpotent. Then there exists a $v\in W$ such that $Xv=0$. For him $Hv=0$ because $v\in W$ and $Xv=0$ by definition of $X$. Then $Gv=0$ for any $G\in\lH+\eK X=\lG$.

\subdem{Third item} Let $e_1$ be a non zero vector in $V$ such that $Ze_1=0$ for any $Z\in\lG$ (the existence comes from the second item). We consider $E_1=\Span e_1$. Any $Z\in\lG$ induces a nilpotent endomorphism $Z^*$ on the vector space $V/E_1$. If $V/E_1\neq\{0\}$, we take a $e_2\in V\setminus E_1$ such that $e_2+E_1\in V/E_1$ fulfils $Z^*(e_2+E_1)=0$ for all $Z\in\lG$. By going on so, we have $Ze_1=0$, $Ze_i=0\mod(e_1,\ldots,e_{i-1})$. In this basis, the matrix of $Z$ has zeros on and under the diagonal.
\end{proof}

\begin{corollary}
Let us consider $V$, a finite dimensional vector space on $\eK$ and $\lG$, a subalgebra of $\gl(V)$ made up with nilpotent elements. Then if $s\geq\dim V$ and $X_i\in\lG$, we have $X_1X_2\ldots X_s=0$.
\label{cor:nil_XXX}
\end{corollary}

\begin{proof}
We write the $X_i$'s in a basis where they have zeros on and under the diagonal. It is rather easy to see that each product push the non zero elements into the upper right corner.
\end{proof}

\begin{corollary}
A nilpotent algebra is solvable.
\end{corollary}

\begin{proof}
The algebra $\ad_{\lG}(\lG)$ is a subalgebra of $\gl(\lG)$ made up with nilpotent endomorphisms of $\lG$. The product of $s$ (see notations of previous corollary) such endomorphism is zero. In particular $\lG$ is solvable.
\end{proof}

We recall the definition of the central decreasing sequence: $\lA^0=\lA$, $\lA^{p+1}=[\lA,\lA^p]$.

\begin{corollary}
A Lie algebra $\lA$ is nilpotent if and only if $\lA^m=\{0\}$ for $m\geq\dim\lA$.
\label{cor:nil_Gn}
\end{corollary}

\begin{proof}
The direct sense is easy: we use corollary~\ref{cor:nil_XXX} with $\lG=\ad(\lA)$ ($\dim\lG=\dim\lA$). Since $\lG$ is nilpotent, for any $X_i\in\lG$ we have $X_1\ldots X_s$, so that $\lA^m=0$. The inverse sense is trivial.

\end{proof}

\begin{corollary}
A nilpotent Lie algebra $\lA\neq\{0\}$ has a non zero center
\end{corollary}

\begin{proof}
If $m$ is the smallest natural such that $\lA^m=0$, $\lA^{m-1}$ is in the center.
\end{proof}


\begin{lemma}
If $\lI$ and $\lJ$ are ideals in $\lG$, then we have a canonical isomorphism $\dpt{\psi}{(\lI+\lJ)/\lJ}{\lI/(\lI\cap\lJ)}$ given by
\[
  \psi([x])=\cloi
\]
if $x=i+j$ with $i\in\lI$ and $j\in\lJ$. Here classes with respect to $\lJ$ are denoted by $[.]$ and the one with respect to $(\lI\cap\lJ)$ by a bar.
\label{lem:pre_trois_resoluble}
\end{lemma}

\begin{proof}
We first have to see that $\psi$ is well defined. If $x'=i+j+j'$, $\psi([x])=\cloi$ because $j+j'\in\lJ$. If $x=i'+j'$ (an other decomposition for $x=i+j$), $\cloi=\cloj$, $j'-j=i-i'\in\lJ\cap\lI$. Then $\cloi=\overline{i'+j'-j}=\cloip$.

Now it is easy to see that $\psi$ is a homomorphism.
\end{proof}

\begin{proposition}
Let $\lG$ and $\lG'$ be Lie algebras.

\begin{enumerate}
\item If $\lG$ is solvable then any subalgebra is solvable and if $\dpt{\phi}{\lG}{\lG'}$ is a Lie algebra homomorphism, then $\phi(\lG)$ is solvable in $\lG'$.

\item  If $\lI$ is a solvable ideal in $\lG$ such that $\lG/\lI$ is solvable, then $\lG$ is solvable.
\item If $\lI$ and $\lJ$ are solvable ideals in $\lG$, then $\lI+\lJ$ is also a solvable ideal in $\lG$.
\end{enumerate}
\label{prop:trois_resoluble}
\end{proposition}

\begin{proof}
\subdem{First item}
If $\lH$ is a subalgebra of $\lG$, then $\dD^k\lH\subset\dD^k\lG$, so that $\lH$ is solvable. Now consider $\lH=\phi(\lG)\subset\lG'$. This is a subalgebra of $\lG'$ because $[h,h']=[\phi(g),\phi(g')]=\phi([g,g'])\in\lH$. It is clear that $\dD(\phi(\lG))\subset\phi(\dD(\lG))$ and
\begin{equation}
\dD^2(\phi(\lG))=\dD\big( \dD\phi(\lG) \big)
                \subset\dD(\phi\dD(\lG))
        \subset\phi\dD\dD(\lG)
        =\phi(\dD^2(\lG)).
\end{equation}
Repeating this argument, $\dD^k(\lH)\subset\phi(\dD^k\lG)$. So $\lH$ is also solvable. Note that
\begin{equation}
    \phi([g,g'])=[\phi(g),\phi(g')]\subset\dD(\pi(\lG)). 
\end{equation}
Then
\begin{equation}
  \dD^k\pi(\lG)=\pi(\dD^k\lG).
\end{equation}

\subdem{Second item}
Let $n$ be the smallest integer such that $\dD^n(\lG/\lI)=0$; we look at the canonical homomorphism $\dpt{\pi}{\lG}{\lG/\lI}$. This satisfies $\dD^n(\pi(\lG))=\pi(\dD^n\lG)=0$. Then $\dD^n(\lG)\subset\lI$. If $\dD^m\lI=0$, then $\dD^{m+n}\lG=0$.

\subdem{Third item}
The space $\lI/(\lI\cap\lJ)$ is the image of $\lI$ by a homomorphism, then it is solvable and $(\lI+\lJ)/\lJ$ is also solvable. The second item makes $\lI+\lJ$ solvable.
\end{proof}

Now we consider $\lG$, any Lie algebra and $\lS$ a maximum solvable ideal i.e. it is included in none other solvable ideal. Let us consider $\lI$, an other solvable ideal in $\lG$. Then $\lI+\lS$ is a solvable ideal; since $\lS$ is maximal, $\lI+\lS=\lS$. Thus there exists an unique maximal solvable ideal which we call the \defe{radical}{radical!of a Lie algebra} of $\lG$. It will be often denoted by $\Rad\lG$. If $\beta$ is a symmetric bilinear form, his \defe{radical}{radical!of a quadratic form} is the set
\begin{equation}
  S=\{x\in\lG\tq\beta(x,y)=0\;\forall y\in\lG\}.
\end{equation}
The form $\beta$ is nondegenerate if and only if $S=\{0\}$.

\begin{proposition}
Let $\lG$ and $\lG'$ be Lie algebras.

\begin{enumerate}
\item If $\lG$ is nilpotent, then his subalgebras are nilpotent and if $\dpt{\phi}{\lG}{\lG'}$ is a Lie algebra homomorphism, then $\phi(\lG)$ is nilpotent.

\item If $\lG/\mZ(\lG)$ is nilpotent, then $\lG$ is nilpotent. For recall,
\[
   \mZ(\lG)=\{z\in\lG\tq [x,z]=0\;\forall x\in\lG\}.
\]

\item If $\lG$ is nilpotent, then $\mZ(\lG)\neq 0$.
\end{enumerate}
\label{prop:nil_homom_nil}
\end{proposition}

\begin{proof}
The proof of the first item is the same as the one of~\ref{prop:trois_resoluble}. Now if $(\lG/\mZ(\lG))^n=0$, then $\lG^n/\mZ(\lG)=0$; thus $\lG^n\subset\mZ(\lG)$, so that $\lG^{n+1}=[\lG,\mZ(\lG)]=0$. Finally, if $n$ is the smallest natural such that $\lG^n=0$, then $[\lG^{n-1},\lG]=0$ and $\lG^{n-1}\subset\mZ(\lG)$.
\end{proof}

The condition to be nilpotent can be reformulated by $\exists n\in\eN$ such that $\forall X_i$, $Y\in\lG$,
\[
   (\ad X_1\circ\ldots\circ\ad X_n)Y=0,
\]
in particular for any $X\in\lG$, there exists a $n\in\eN$ such that $(\ad X)^n=0$. An element for which such a $n$ exists is \defe{ad-nilpotent}{ad-nilpotent@$\ad$-nilpotent}. If $\lG$ is nilpotent, then all his elements are ad-nilpotent.

Some results without proof:

\begin{lemma}\label{lem:pre_Engel}
If $X\in\gl(V)$ is a nilpotent endomorphism, then $\ad X$ is nilpotent.
\end{lemma}

\begin{lemma}
If $x\in\gl(V)$ is semisimple, then $\ad(x)$ is also semisimple.
\end{lemma}

\begin{proof}
We choose a basis $\{v_1,\cdots,v_n\}$ of $V$ in which $x$ is diagonal with eigenvalues $a_1,\ldots,a_n$. For $\gl(V)$, we consider the basis $\{E_{ij}\}$ in which $E_{ij}$ is the matrix with a $1$ at position $(i,j)$ and zero anywhere else. This satisfies $[E_{kl},E_{rs}]=\delta_{lr}E_{ks}-\delta_{sk}E_{rl}$. We easily check that $E_{kl}(v_i)=\delta_{li}v_k$. Since we are in a matrix algebra, the adjoint action is the commutator: $(\ad x)E_{ij}=[x,E_{ij}]$; as we know that $x=a_kE_{kk}$,
\begin{equation}
 (\ad x)E_{ij}=a_k[E_{kk},E_{ij}]=(a_i-a_j)E_{ij}
\end{equation}
which proves that $\ad x$ has a diagonal matrix in the basis $\{E_{ij}\}$ of $\gl(V)$. Furthermore, we have an explicit expression for his matrix: the eigenvalues are $(a_i-a_j)$.
\end{proof}

\begin{remark}
The inverse implication is not true, as the unit matrix shows.
\end{remark}

\begin{theorem}[Engel,\cite{SamelsonNotesLieAlg}]\label{tho:Engel}
    A Lie algebra is nilpotent if and only if all his elements are ad-nilpotent.
\end{theorem}
\index{theorem!Engel}\index{Engel theorem}



\begin{proposition}\label{PropBDrongP}
    If an algebra $\lG\subset\gl(V)$ is made up with nilpotent endomorphisms of $V$, then $\lG$ is nilpotent as Lie algebra.
\end{proposition}
%TODO: a proof

\begin{proposition}[\cite{Sagle,SamelsonNotesLieAlg}] \label{PropKillingTraceDeuxn}
    On the Lie algebra \( \gl(\eR^n)\), the following formula holds:
    \begin{equation}
       B(X,Y)=2n\tr(XY).
    \end{equation}
\end{proposition}
%TODO: pr√©ciser que gl est l'alg√®bre de Lie des $n\times n$ matrices with vanishing trace.

\begin{proof}
    We consider a simple subalgebra $\lG$ of $\gl(V)$ for a certain vector space $V$ and a nondegenerate $\ad$-invariant symmetric $2$-form $f$. Then there exists a $S\in\GL(\lG)$ such that
    \begin{subequations}
    \begin{align}
      f(X,Y)&=B(SX,Y) \label{eq:S_un}  \\
      B(SX,Y)&=B(X,SY).  \label{eq:S_deux}
    \end{align}
    \end{subequations}
    If we consider a basis of $\lG$, we can write $f(X,Y)$ (and the Killing) in a matricial form\footnote{We systematically use the sum convention on the repeated subscript.} as
    \[
      f(X,Y)=f_{ij}X^iY^j,\qquad B(X,Y)=B_{ij}X^iY^j.
    \]
    Since $B$ is nondegenerate, we can define the matrix $(B^{ij})$ by $B^{ij}B_{jk}=\delta^i_k$. It is easy to see that the searched endomorphism of $\lG$ is given by $S^k_l=f_{kj}B^{jl}$.

    Using the invariance \eqref{eq:Killing_invariant} of the Killing form and \eqref{eq:S_deux}, we find
    \[
       B\big( (\ad X\circ S)Y,Z  \big)=-B\big( (S\circ\ad X)Z,Y  \big)
    \]
    for any $X$, $Y$, $Z\in\lG$. Now using \eqref{eq:S_un},
    \begin{equation}
     f\big(  (S^{-1}\circ\ad X\circ S)Y,Z  \big)=-f\big((\ad X) Z,Y\big)
                                               =f\big( (\ad Z) X,Y \big)
                           =f\big( Z, (\ad X)Y \big).
    \end{equation}
    Since $f$ is nondegenerate, we find $\ad X\circ S=S\circ\ad X$. It follows from Schurs'lemma that $S=\lambda I$. Note that $f(X,Y)=\lambda B(X,Y)$; this proves a certain unicity of the Killing form relatively to his invariance properties.

    Now we consider $f(X,Y)=\tr(XY)$. This is symmetric because of the cyclic invariance of the trace and this is $ad$-invariant because of the formula $\tr([a,b]c)=\tr(a[b,c])$ which holds for any matrices $a,b,c$.

    The next step to show that $f$ is nondegenerate; we define
    \[
      \lG\hperp=\{X\in\lG\tq f(X,Y)=0\,\forall Y\in\lG   \}.
    \]
    The simplicity of $\lG$ ($\lG$ has no proper ideals) makes $\lG$ equal to $0$ or $\lG$. Indeed consider $Z\in\lG\hperp$. For any $X$, $Y\in\lG$, we have
    \[
    0=f(Z,[X,Y])=f([Z,X],Y).
    \]
    Then $[Z,X]\in\lG\hperp$ and $\lG\hperp$ is an ideal. We will see that the reality is $\lG\hperp=0$ (cf. error~\ref{err:f_dege}). Let us suppose $\lG\hperp=\lG$ and consider the lemma~\ref{lem:M_nil} with $A=B=\lG$. We define
    \[
       M=\{ X\in\lG\tq [X,\lG]\subset\lG \}=\lG.
    \]
    If $X\in M$ satisfies $\tr(XY)=0$ for any $Y\in M$, then $X$ is nilpotent. Here, $X\in M$ is not a true condition because $M=\lG$. Since $\lG\hperp=\lG$, the trace condition is also trivial. Then $\lG$ is made up with nilpotent endomorphisms of $V$. Then lemma~\ref{lem:pre_Engel} makes all the $X\in\lG$ ad-nilpotent, so that $\lG$ is nilpotent by proposition~\ref{PropBDrongP}.

    By the third item of proposition~\ref{prop:nil_homom_nil}, $\mZ(\lG)\neq 0$ which contradicts the simplicity of $\lG$. Then $\lG\hperp=0$ and $f$ is nondegenerate. Finally,
    \begin{equation}
      B(X,Y)=\lambda\tr(X,Y)
    \end{equation}
    for a certain real number $\lambda$. With a certain amount of work, one can determine the exact value of $\lambda$ when $\lG$ is the Lie algebra of $n\times n$ matrices with vanishing trace.
\end{proof}
%TODO: finish the proof

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Flags and nilpotent Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here we give a ``flag description'' of some previous results. In particular the chain \eqref{eq:solvable_chaine}. If $V$ is a vector space of dimension $n<\infty$, a \defe{flag}{flag} in $V$ is a chain of subspaces $0=V_0\subset V_1\subset\ldots\subset V_{n-1}\subset V_n=\lG$ with $\dim V_k=k$. If $x\in\End{V}$ fulfils $x(V_i)\subset V_i$, then we say that $x$ \defe{stabilise}{stabiliser of a flag} the flag.

\begin{theorem}
If $\lG$ is a subalgebra of $\gl(V)$ in which the elements are nilpotent endomorphisms and if $V\neq 0$, then there exists a $v\in V$, $v\neq 0$ such that $\lG v=0$.
\end{theorem}

\begin{proof}
This is the second item of theorem~\ref{tho:trois_nil}.
\end{proof}

\begin{corollary}
Under the same assumptions, there exists a flag $(V_i)$ stable under $\lG$ such that $\lG V_i\subset V_{i-1}$. In other words, there exists a basis of $V$ in which the matrices of $\lG$ are nilpotent; this basis is the one given by the flag.
\end{corollary}

\begin{proof}
Let $v\neq 0$ such that $\lG v=0$ which exists by the theorem and $V_1=\Span v$. We consider $W=V/V_1$; the action of $\lG$ on $W$ is also made up with nilpotent endomorphisms. Then we go on with $V_1$ and $W_1=W/V_2$,\dots
\end{proof}


\begin{lemma}
If $\lG$ is nilpotent and if $\lI$ is an non trivial ideal in $\lG$, then $\lI\cap\mZ(\lG)\neq 0$.
\end{lemma}

\begin{proof}
Since $\lI$ is an ideal, $\lG$ acts on $\lI$ with the adjoint representation. The restriction of an element $\ad X$ for $X\in\lG$ to $\lI$ is in fact a nilpotent element in $\gl(\lI)$. Then we have a $I\in\lI$ such that $\lG I=0$. Thus $I\in\lI\cap\mZ(\lG)$.
\end{proof}

\begin{theorem}
Let $\lG$ be a solvable Lie subalgebra of $\gl(V)$. If $V\neq 0$, then $V$ posses a common eigenvector for all the endomorphisms of $\lG$.
\label{tho:sol_ss_dem}
\end{theorem}

\begin{proof}
This is exactly the Lie theorem~\ref{tho:Lie_Vu}
\end{proof}

\begin{corollary}[Lie theorem]\index{Lie!theorem}\index{theorem!Lie}
Let $\lG$ be a solvable subalgebra of $\gl(V)$. Then $\lG$ stabilize a flag of $V$.
\label{tho:Lie_Vd}
\end{corollary}

\begin{proof}

This corollary is the corollary given in~\ref{cor:de_Lie_Vu}.

We consider $v_1$ the vector given by theorem~\ref{tho:sol_ss_dem}. Since it is eigenvector of all $\lG$, $\Span v_1$ is stabilised by $\lG$. Next we consider $v_2$ in the complementary which is also a common eigenvector,\ldots
\end{proof}

\begin{corollary}
If $\lG$ is a solvable Lie algebra, then there exists a chain of ideals in $\lG$
\[
  0=\lG_0\subset\lG_1\subset\ldots\subset\lG_n=\lG
\]
with $\dim\lG_k=k$.
\end{corollary}

\begin{proof}
If $\dpt{\phi}{\lG}{\gl(V)}$ is a finite-dimensional representation of $\lG$, then $\phi(\lG)$ is solvable by proposition~\ref{prop:nil_homom_nil}. Then $\phi(\lG)$ stabilises a flag of $V$. Now we take as $\phi$ the adjoint representation of $\lG$. A stable flag is the chain of ideals; indeed if $\lG_i$ is a part of the flag, then $\forall H\in\lG$ $\ad H\lG_i\subset\lG_i$ because the flag is invariant.
\end{proof}


\begin{corollary}
If $\lG$ is solvable then $X\in\dD\lG$ implies that $\ad_{\lG}X$ is nilpotent. In particular $\dD\lG$ is nilpotent.
\end{corollary}

\begin{proof}
We consider the ideals chain of previous corollary and an adapted basis: $\{X_1,\ldots,X_n\}$ is such that $\{X_1,\ldots,X_i\}$ spans $\lG_i$. In such a basis the matrices of $\ad(\lG)$ are upper triangular and it is easy to see that in this case, the matrices of $[\ad\lG,\ad\lG]$ are \emph{strictly} upper triangular: they have zeros on the diagonal. But $[\ad\lG,\ad\lG]=\ad_{\lG}[\lG,\lG]$. Then for $X\in\ad_{\lG}\dD\lG$, $\ad_{\lG}X$ is nilpotent. \emph{A fortiori}, $\ad_{\dD\lG}X$ is nilpotent and by the Engels'theorem~\ref{tho:Engel}, $\dD\lG$ is nilpotent.
\end{proof}

The following lemma is computationally useful because it says that if $X$ is a nilpotent element of a Lie algebra, then $g\cdot X$ is also nilpotent with (at most) the same order.

\begin{lemma}
  The following formula
\begin{equation}
\ad(g\cdot X)^nY=g\cdot \ad(X)^n(g^{-1}\cdot Y)
\end{equation}
holds for all $g\in G$ and $X$,$Y\in\lG$,
\label{lem:nil_Ad}
\end{lemma}

The proof is a simple induction on $n$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Semisimple Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

A useful reference to go trough semisimple Lie algebras is \cite{Wisser}. Very few proofs, but the statements of all the useful results with explanations.

\begin{definition}
    A Lie algebra is \defe{semisimple}{semisimple!Lie algebra} if it has no proper abelian invariant Lie subalgebra. A Lie algebra is \defe{simple}{simple!Lie algebra} if it is not abelian and has no proper Lie subalgebra.
\end{definition}

In that definition, we say that a Lie subalgebra \( \lH\) is \defe{invariant}{invariant!Lie subalgebra} if \( \ad(\lG)\lH\subset\lH\).

There are a lot of equivalent characterisations. Here are some that are going to be proved (or not) in the next few pages. A Lie algebra is semisimple if an only if one of the following conditions is respected.
\begin{enumerate}
    \item
        The Killing form is nondegenerate.
    \item
        The radical of \( \lG\) is zero (theorem~\ref{ThoRadicalEquivSS}).
    \item
        There are no abelian proper invariant subalgebra.
\end{enumerate}

\begin{probleme}
    I think that in the following I took the degenerateness of Killing as definition.
\end{probleme}

The Killing form is a convenient way to define a Riemannian metric on a semisimple\footnote{In this case, $B$ is nondegenerate.} Lie group.

\begin{proposition}
Let $\lG$ be a semisimple Lie algebra, $\lA$ an ideal in $\lG$, and $\lA^{\perp}=\{X\in\lG\tq B(X,A)=0\forall A\in\lA\}$.
Then
\begin{enumerate}
\item $\lA^{\perp}$ is an ideal,
\item $\lG=\lA\oplus\lA^{\perp}$,
\item $\lA$ is semisimple,
\end{enumerate}
\label{prop:a_aperp}
\end{proposition}

\begin{proof}
\subdem{First item}
We have to show that for any $X\in\lG$ and $P\in\lA^{\perp}$, $[X,P]\in\lA^{\perp}$, or $\forall\, Y\in\lA$, $B(Y,[X,P])=0$. From invariance of $B$,
\[
  B(Y,[X,P])=B(P,[Y,X])=0.
\]
\subdem{Second item}
Since $B$ is nondegenerate, $\dim\lA+\dim\lA^{\perp}=\dim\lG$. Let us consider $Z\in\lG$ and $X$, $Y\in\lA\cap\lA^{\perp}$. We have $B(Z,[X,Y])=B([Z,X],Y)=0$. Then $[X,Y]=0$ because $B(Z,[X,Y])=0$ for any $Z$ and $B$ is nondegenerate. Thus $\lA\cap\lA^{\perp}$ is abelian. It is also an ideal because $\lA$ and $\lA^{\perp}$ are.

Now we consider $\lB$, a complementary of $\lA\cap\lA^{\perp}$ in $\lG$, $Z\in\lG$ and $T\in\lA\cap\lA^{\perp}$. The endomorphism $E=\ad T\circ\ad Z$ sends $\lA\cap\lA^{\perp}$ to $\{0\}$. Indeed consider $A\in\lA\cap\lA^{\perp}$; $(\ad Z) A\in\lA\cap\lA^{\perp}$ because it is an ideal, and then $(\ad T\circ\ad Z)A=0$ because it is abelian.

The endomorphism $E$ also sends $\lB$ to $\lA\cap\lA^{\perp}$ (it may not be surjective); then $\tr(\ad T\circ\ad Z)=0$ and $\lA\cap\lA^{\perp}=\{0\}$. Since $B$ is nondegenerate, $\dim\lA+\dim\lA^{\perp}=\dim\lG$. Then $\lA\oplus\lA^{\perp}=\lG$ is well a direct sum.
\subdem{Third item}
From lemma~\ref{lem:Killing_descent_ideal}, the Killing form of $\lG$ descent to the ideal $\lA$; then it is also nondegenerate and $\lA$ is also semisimple.
 \end{proof}

\begin{corollary}
A semisimple Lie algebra has center $\{0\}$.
\label{cor:ss_no_centre}
\end{corollary}

\begin{proof}
If $Z\in\ker\lG$, $\ad Z=0$. So $B(Z,X)=0$ for any $X\in\lG$. Since $B$ is nondegenerate, it implies $Z=0$.
\end{proof}


\begin{corollary}
If $\lG$ is a semisimple Lie algebra, it can be written as a direct sum
\[
   \lG=\lG_1\oplus\ldots\oplus\lG_r
\]
where the $\lG_i$ are simples ideals in $\lG$. Moreover each simple ideal in $\lG$ is a direct sum of some of them.
\label{cor:decomp_ideal}
\end{corollary}

\begin{proof}
If $\lG$ is simple, the statement is trivial. If it is not, we consider $\lA$, an ideal in $\lG$.
Proposition~\ref{prop:a_aperp} makes $\lG=\lA\oplus\lA^{\perp}$. Since $\lA$ and $\lA^{\perp}$ are semisimple, we can once again brake them in the same way. We do it until we are left with simple algebras.

For the second part, consider $\lB$ a simple ideal in $\lG$ which is not a sum of $\lG_i$. Then $[\lG_i,\lB]\subset\lG_i\cap\lB=\{0\}$. Then $\lB$ is in the center of $\lG$. This contradict corollary~\ref{cor:ss_no_centre}.
\end{proof}

\begin{proposition}
If $\lG$ is semisimple then
\[
   \ad(\lG)=\partial(\lG),
\]
i.e. any derivation is an inner automorphism:
\label{prop:ss_derr_int}
\end{proposition}

\begin{proof}
We saw at page \pageref{pg:ad_subset_der} that $\ad(\lG)\subset\partial(\lG)$ holds without assumptions of (semi)simplicity. Now we consider $D$, a derivation: $\forall X\in\lG$,
\[
   \ad(DX)=[D,\ad X].
\]
Then $\ad(\lG)$ is an ideal in $\partial(\lG)$ because the commutator of $\ad X$ with any element of $\partial(\lG)$ still belongs to $\ad(\lG)$. Let us denote by $\lA$ the orthogonal complement of $\ad(\lG)$ in $\partial(\lG)$ (for the Killing metric). The algebra $\ad(\lG)$ is semisimple because of it isomorphic to $\lG$. Since the Killing form on $\ad(\lG)$ is nondegenerate, $\lA\cap\ad(\lG)=\{0\}$. Finally $D\in\lA$ implies $[D,\ad X]\in\lA\cap\ad(\lG)=\{0\}$. Then $\ad(DX)=0$ for any $X\in\lG$, so that $D=0$. This shows that $\lA=\{0\}$, so that $\ad(\lG)=\partial(\lG)$.
\end{proof}
