% This is part of Giulietta
% Copyright (c) 2013-2015, 2019-2021
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Here are the results which relate Lie groups and Lie algebras.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Lie algebra of a Lie group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemmaDef}        \label{DEFooSSDYooOwjHso}
    Let \( G\) be a smooth Lie group and \( X\in T_eG\). The vector field \( X^L\) defined by
    \begin{equation}
        X^L_g=dL_Xv
    \end{equation}
    is smooth. This is the \defe{left invariant}{left invariant vector field} vector field associated with the vector \( X\in T_eG\).
\end{lemmaDef}

\begin{propositionDef}      \label{DEFooKDCPooZOJsMD}
    Let \( G\) be a smooth Lie group. If \( X,Y\in T_eG\) we define the bracket\footnote{On the right, this is the bracket of vector fields defined in \ref{DEFooHOTOooRaPwyo}.}
    \begin{equation}
        [X,Y] = [X^L,Y^L]_e.
    \end{equation}
    The set \( T_eG\) with this bracket is a Lie algebra. This is the \defe{Lie algebra of the Lie group}{Lie algebra of a Lie group} \( G\). It will usually be denoted by \( \lG\).

    The topology on \( \lG=T_eG\) is the usual one of the tangent spaces, definition \ref{}.
\end{propositionDef}

\begin{proof}
    We know from proposition \ref{PROPooEJBWooSbvypo} that \( T_eG\) is a vector space. We have to define a Lie bracket on it. For that we use the left-invariant vector field. Let \( X\in T_eG\) and \( g\in M\) we define
    \begin{equation}
        X^L_g=dL_gX
    \end{equation}
    where \( L_g\colon G\to G\) is the left translation: \( L_g(h)=gh\). If \( X,Y\in T_eG\) we define
    \begin{equation}
        [X,Y]=[X^L,Y^L]_e
    \end{equation}
    where the bracket on the right hand side is the commutator of vector field defined in \ref{DEFooHOTOooRaPwyo}. It defines a Lie algebra structure by the proposition \ref{PROPooSWQSooSEfTuX}.
\end{proof}

In order to make the notations clear, let us write the formula explicitly. If \( X,Y\in T_eG\) are given by \( X=\alpha'(0)\) and \( Y=\beta'(0)\) we have
\begin{subequations}        \label{SUBEQSooHKWMooQbeStl}
    \begin{align}
        (XY)f&=X(Y(f))\\
        &=\Dsdd{ (Yf)\big( \alpha(t) \big) }{t}{0}\\
        &=\Dsdd{ Y_{\alpha(t)}(f) }{t}{0}\\
        &=\Dsdd{ Y^L_{\alpha(t)}(f) }{t}{0}\\
        &=\DDsdd{ f\big( \alpha(t)\beta(u) \big) }{t}{0}{s}{0}.
    \end{align}
\end{subequations}

Now a great theorem without proof:
\begin{theorem} \label{tho:loc_isom}
Two Lie groups are locally isomorphic if and only if their Lie algebras are isomorphic.
\end{theorem}

\begin{theorem}		\label{ThoSubGpSubAlg}		\label{tho:gp_alg}
If $G$ is a Lie group, then
\begin{enumerate}
\item\label{ThoSubGpSubAlgi} if $\lH$ is the Lie algebra of a Lie subgroup $H$ of $G$, then it is a subalgebra of $\lG$,
\item Any subalgebra of $\lG$ is the Lie algebra of one and only one connected Lie subgroup of $G$.
\end{enumerate}

\begin{probleme}
À mon avis, il faut dire ``connexe et simplement connexe'', et non juste ``connexe''.
\end{probleme}

\end{theorem}
\begin{proof}

\subdem{First item}
Let $\dpt{i}{H}{G}$ be the identity map; it is a homomorphism from $H$ to $G$, thus $di_e$ is a homomorphism from $\lH$ to $\lG$. Conclusion: $\lH$ is a subalgebra of $\lG$.

\subdem{Characterization for $\lH$}
Before to go on with the second point, we derive an important characterization of $\lH$:
\begin{equation}\label{eq:path_alg}
\lH=\{X\in\lG:\text{the map } t\to\exp tX\text{ is a path in $H$}\}.
\end{equation}
For that, consider $\dpt{\exp_H}{\lH}{H}$ and $\dpt{\exp_G}{\lG}{G}$; from unicity of the exponential, for any $X\in\lH$, $\exp_HX=\exp_GX$, so that one can simply write ``$\exp$''\ instead of ``$\exp_h$''\ or ``$\exp_G$''.

Now, if $X\in\lH$, the map $t\to\exp tX$ is a curve in $H$. But it is not immediately clear that such a curve in $H$ is automatically build from a vector in $\lH$ rather than in $\lG$.  More precisely, consider a $X\in\lG$ such that $t\to\exp tX$ is a path (continuous curve) in H. By lemma~\ref{lem:var_cont_diff}, the map $t\to\exp tX$ is differentiable and thus by derivation, $X\in\lH$.
The characterisation \eqref{eq:path_alg} is proved.

Thus $\lH$ is a Lie subalgebra of $\lG$.

\subdem{Second item}
For the second part, we consider $\lH$ any subalgebra of $\lG$ and $H$, the smallest subgroup of $G$ which contains $\exp\lH$. We also consider a basis $\{X_1,\ldots,X_n\}$ of $\lG$ such that $\{X_{r+1},\ldots,X_n\}$ is a basis of $\lH$.

By corollary~\ref{cor:/24}, the set of linear combinations of elements of the form $X(M)$ with $M=(0,\ldots,0,m_{r+1},\ldots,m_r)$ form a subalgebra of $U(\lG)$. If $X=x_1X_1+\cdots+x_nX_n$, we define $|X|=(x_1^2+\cdots+x_n^2)^{1/2}$ ($x_i\in\eR$).

Let us consider a $\delta>0$ such that $\exp$ is a diffeomorphism (normal neighbourhood) from $B_{\delta}=\{X\in\lG:|X|<\delta\}$ to a neighbourhood $N_e$ of $e\in G$ and such that $\forall x,y,xy\in N_e$,
\begin{equation}\label{eq:coord_xy}
   (xy)_k=\sum_{M,N}C^{[k]}_{MN}x^My^N
\end{equation}
holds\footnote{The validity of this second condition is assured during the proof of theorem~\ref{tho:loc_isom} which is not given here.}. We note $V=\exp(\lH\cap B_{\delta})\subset N_e$. The map
\[
   \exp(x_{r+1}X_{r+1}+\cdots+x_nX_n)\to(x_{r+1},\ldots,x_n)
\]
is a coordinate system on $V$ for which $V$ is a connected manifold. But $\lH\cap B_{\delta}$ is a submanifold of $B_{\delta}$, then $V$ is a submanifold of $N_e$ and consequently of~$G$.

Let $x$, $y\in V$ such that $xy\in N_e$ (this exist: $x=y=e$); the canonical coordinates of $xy$ are given by \eqref{eq:coord_xy}. Since $x_k=y_k=0$ for $1\leq k\leq r$, $(xy)_k=0$ for the same $k$ because for $(xy)_k$ to be non zero, one need $m_1=\ldots=m_r=n_1=\ldots=n_r=0$ -- otherwise, $x^M$ or $y^N$ is zero. Now we looks at $C^{[k]}_{MN}$ for such a $k$ (say $k=1$ to fix ideas): $[k]=(\delta_{11},\ldots,\delta_{1k})=(1,0,\ldots,0)$ and by definition of the $C$'s,
\[
   X(M)X(N)=\sum_PC_{MN}^PX(P).
\]
But we had seen that the set of the $X(A)$ with $A=(0,\ldots,0,a_{r+1},\ldots,a_n)$ form a subalgebra of $U(\lG)$. Then, only terms with $P=(0,\ldots,0,p_{r+1},\ldots,p_n)$ are present in the sum; in particular, $C_{MN}^{[k]}=0$ for $k=1,\ldots,r$. Thus $VV\cap N_e\subset V$.

The next step is to consider $\mV$, the set of all the subset of $H$ whose contains a neighbourhood of $e$ in $V$. We can check that this fulfils the six axioms of a topological group\index{topological!group}:

\begin{enumerate}
\item The intersection of two elements of $\mV$ is in $\mV$;
\item the intersection of all the elements of $\mV$ is $\{e\}$;
\item any subset of $H$ which contains a set of $\mV$ is in $\mV$;
\item If $\mU\in\mV$, there exists a $\mU_1\in\mV$ such that $\mU_1\mU_1\subset\mU$ because $VV\cap N_e\subset V$;
\item if $\mU\in\mV$, then $\mU^{-1}\in\mV$ because the inverse map is differentiable and transforms a neighbourhood of $e$ into a neighbourhood of $e$;
\item if $\mU\in\mV$ and $h\in H$, then $h\mU h^{-1}\in\mV$.
\end{enumerate}

To see this last item, we denote by $\log$ the inverse map of $\dpt{\exp}{B_{\delta}}{N_e}$. By definition of $V$, it sends $V$ on $\lH\cap B_{\delta}$. If $X\in\lG$, there exists one and only one $X'\in\lG$ such that $he^{tX}h^{-1}=e^{tX'}$ for any $t\in\eR$. Indeed we know that $he^{X}h^{-1}=e^{\Ad_hX}$, then $X'$ must satisfy $e^{tX'}=e^{\Ad_htX}$. If it is true for any $t$, then, by derivation, $X'=\Ad_hX$.

The map $X\to X'$ is an automorphism of $\lG$ which sent $\lH$ on itself. So one can find a $\delta_1$ with $0<\delta_1<\delta$ such that
\[
   h\exp({B_{\delta_1}\cap\lH})h^{-1}\subset V.
\]
Indeed, $he^{\lH} h^{-1}\subset\lH$, so that taking $\delta_1<\delta$, we get the strict inclusion. We can choose $\delta_1$ even smaller to satisfy $he^{B_{\delta_1}}h^{-1}\subset N_e$. Since the map $X\to\log(he^{X}h^{-1})$ from $B_{\delta_1\cap\lH}$ to $B_{\delta}\cap\lH$ is regular, the image of $B_{\delta_1}\cap\lH$ is a neighbourhood of $0$ in $\lH$. Thus $he^{B_{\delta_1}\cap\lH}h^{-1}$ is a neighbourhood of $e$ in $V$. Finally, $h\mU h^{-1}\in\mV$ and the last axiom of a topological group is checked.

This is important because there exists a topology on $H$ such that $H$ becomes a topological group and $\mV$ is a family of neighbourhood of $e$ in $H$. In particular, $V$ is a neighbourhood of $e$ in $H$.

For any $z\in G$, we define the map $\dpt{\phi_z}{zN_e}{B_{\delta}}$ by
\begin{equation}
  \phi_z(ze^{x_1X_1+\cdots+x_nX_n})=(x_1,\ldots,x_n),
\end{equation}
and we denote by $\varphi_z$ the restriction of $\phi_z$ to $zV$. If $z\in H$, then $\varphi_z$ sends the neighbourhood $zV$ of $z$ in $H$ to the open set $B_{\delta}\cap\lH$ in $\eR^{n-r}$. Indeed, an element of $zV$ is a $ze^Z$ with $Z\in\lH\cap B_{\delta}$ which is sent by $\varphi_z$ to an element of $\lH\cap B_{\delta}$. (we just have to identify $x_1X_1+\cdots+x_nX_n$ with $(x_1,\ldots,x_n)$).

Moreover, if $z_1,z_2\in H$, the map $\varphi_{z_1}\circ\varphi_{z_2}^{-1}$ is the restriction to an open subset of $\lH$ of $\phi_{z_1}\circ\phi_{z_2}$. Then $\varphi_{z_1}\circ\varphi_{z_2}^{-1}$ is differentiable. Conclusion: $(H,\varphi_z: z\in H)$ is a differentiable manifold.

Recall that the definition of $\lH$ was to be a subalgebra of $\lG$; therefore $V=e^{\lH\cap B_{\delta}}$ is a submanifold of $G$. But the left translations are diffeomorphism of $H$ and $H$ is the smallest subgroup of $G$ containing $e^{\lH}$. Thus $H$ is a manifold on which the multiplication is diffeomorphic and consequently, $H$ is a Lie subgroup of $G$.

Rest to prove that the Lie algebra of $H$ is $\lH$ and the unicity part of the theorem.

We know that $\dim H=\dim\lH$ and moreover for $i>r$, the map $t\to\exp tX_i$ is a curve in $H$. Now, the fact that $\lH$ is the set of $X\in\lG$ such that $t\to\exp tX$ is a path in $H$ show that $X_i\in\lH$. Then the Lie algebra of $H$ is $\lH$ and $H$ is a connected group because it is generated by $\exp\lH$ which is a connected neighbourhood of $e$ in $H$.

We turn our attention to the unicity part. Let $H_1$ be a connected Lie subgroup of $G$ such that $T_eH_1=\lH$. Since $\exp_{\lH}X=\exp_{\lH_1}X$, $H=H_1$ as set. But $\exp$ is a differentiable diffeomorphism from a neighbourhood of $0$ in $\lH$ to a neighbourhood of $e$ in $H$ and $H_1$, so as Lie groups, $H$ and $H_1$ are the same.

Let us consider an element $X\in\lG$ such that $\exp tX\in H$ for every $t\in\eR$, and the map $\dpt{\varphi}{\eR}{G}$, $\varphi(t)=\exp tX$. This is continuous, then there exists a connected neighbourhood $\mU$ of $0$ in $\eR$ such that $\varphi(\mU)\subset V$. Then $\varphi(\mU)\subset H\cap V$ and the connectedness of $\varphi(\mU)$ makes $\varphi(\mU)\subset\exp\mU_h$. But $\exp\mU_h$ is an arbitrary small neighbourhood of $e$ in $H$; the conclusion is that $\varphi$ is a continuous map from $\eR$ into $H$. Indeed, we had chosen $X$ such that $\exp tX\in H$.

Moreover, we know that
\[
  e^{(t_0+\epsilon)X}=e^{t_0X}e^{\epsilon X},
\]
but $\exp \epsilon X$ can be as close to $e$ as we want (this proves the continuity at $t_0$). Then $\varphi$ is a path in $H$.

In definitive, we had shown that $\exp tX\in H$ implies that $t\to\exp tX$ is a path. Now equation \eqref{eq:path_alg} gives the result.

\end{proof}

\begin{corollary}
Let $G$ be a Lie group and $H_1$, $H_2$, two subgroups both having a finite number of connected components (each for his own topology). If $H_1=H_2$ as sets, then $H_1=H_2$ as Lie groups.
\end{corollary}

\begin{proof}
The proposition shows that $H_1$ and $H_2$ have same Lie algebra. But any Lie subalgebra of $\lG$ is the Lie algebra of exactly one connected subgroup of $G$ (theorem~\ref{tho:gp_alg}). Then as Lie groups, ${H_1}_0={H_2}_0$. Since $H_1$ and $H_2$ are topological groups, the equality of they topology on one connected component gives the equality everywhere (because translations are differentiable).
\end{proof}

\begin{lemma}
Let $\lG$ admit a direct sum decomposition (as vector space) $\lG=\lM\oplus\lN$. Then there exists open and bounded neighbourhoods $\mU_m$ and $\mU_n$ of $0$ in $\lM$ and $\lN$ such that the map
		\begin{equation}
		\begin{aligned}
			\phi \colon \mU_m\times\mU_n &\to G\
			(A,B)&\mapsto e^Ae^B
		\end{aligned}
	\end{equation}
is a diffeomorphism between $\mU_m\times\mU_n$ and an open neighbourhood of $e$ in $G$.
 \label{lem:decomp}
\end{lemma}


\begin{proof}
Let $\{X_1,\ldots,X_n\}$ be a basis of $\lG$ such that $X_i\in\lM$ for $1\leq i\leq r$ and $X_j\in\lN$ for $r<j\leq n$. We consider $\{t_1,\ldots,t_n\}$, the canonical coordinates of $\exp(x_1X_1+\cdots+x_rX_r)\exp(x_{r+1}X_{r+1}+\cdots+x_nX_n)$ in this coordinate system. By properties of the exponential, the function $\varphi_j$ defined by $t_j=\varphi_j(x_1,\ldots,x_n)$ is differentiable at $(0,\ldots,0)$. If $x_i=\delta_{ij}s$, then $t_i=\delta_{ij}s$ and the Jacobian of
\[
   \dsd{(\varphi_1,\ldots,\varphi_n)}{(x_1,\ldots,x_n)}
\]
is $1$ for $x_1=\ldots=x_n=0$. Thus $d\varphi_e$ is a diffeomorphism and so $\varphi$ is a locally diffeomorphic.
\end{proof}

With the notations and the structure of theorem~\ref{tho:diff_sur_ferme}, the subgroup $H$ is discrete if and only if $\lH=\{0\}$. Indeed, recall the definition \eqref{eq:lH_de_G}:
\[
  \lH=\{X\in\lG: \forall t\in\eR, e^{tX}\in H\},
\]
and the fact that there exists a neighbourhood of $e$ in $H$ on which the exponential map is a diffeomorphism.

\begin{remark}
This fact should not be placed after the following lemma. In fact, we use here just the existence of normal neighbourhood (which is a common result) while the following lemma gives much more than normal neighbourhood.
\end{remark}

\begin{lemma}       \label{LEMooOBIMooVvIDnb}
 Let $G$ be a Lie group and $H$, a Lie subgroup of $G$ ($\lG$ and $\lH$ are the corresponding Lie algebras). If $H$ is a topological subspace of $G$, then there exists an open neighbourhood $V$ of $0$ in $\mG$ such that
 \begin{enumerate}
 \item $\exp$ is a diffeomorphism between $V$ and an open neighbourhood of $e$ in~$G$,
 \item $\exp(V\cap\lH)=(\exp V)\cap H$.
 \end{enumerate}
\label{lem:sugroup_normal}
\end{lemma}

\begin{definition}
A \defe{differentiable subgroup}{differentiable!subgroup} is a connected Lie subgroup.
\end{definition}

\begin{corollary}
Let $G$ be a Lie group, and $K$, $H$ two differentiable subgroups of $G$. We suppose $K\subset H$. Then $K$ is a differentiable subgroup of the Lie group $H$.
\end{corollary}

\begin{proof}
The Lie algebras of $K$ and $H$ are respectively denoted by $\lK$ and $\lH$. We denote by $K^*$ the differentiable subgroup of $H$ which has $\lK$ as Lie algebra. The differentiable subgroups $K$ and $K^*$ have same Lie algebra, and then coincide as Lie groups.
\end{proof}

\label{pg:ex_topo_Lie}
Consider the group $T=S^1\times S^1$ and the continuous map $\dpt{\gamma}{\eR}{T}$ given by
\[
  \gamma(t)=(e^{it},e^{i\alpha t})
\]
with a certain irrational $\alpha$ in such a manner that $\gamma$ is injective and $\Gamma=\gamma(\eR)$ is dense in $T$.

The subset $\Gamma$ is not closed because his complementary in $T$ is not open: any neighbourhood of element $p\in T$ which don't lie in $\Gamma$ contains some elements of $\Gamma$. We will show that the inclusion map $\dpt{\iota}{\Gamma}{T}$ is continuous. An open subset of $T$ is somethings like
\[
  \mO=(e^{iU},e^{iV})
\]
where $U,V$ are open subsets of $\eR$. It is clear that
\[
   \iota^{-1}(\mO)=\{ \gamma(t)\tq t\in U+2k\pi,\alpha t\in V+2m\pi \},
\]
but the set of elements $t$ of $\eR$ which satisfies it is clearly open. Then $\Gamma$ has at least the induced topology from $T$ (as shown in proposition~\ref{prop:topo_sub_manif}). In fact, the own topology of $\Gamma$ is \emph{more} than the induced: the open subsets of $\Gamma$ whose are just some small segments clearly doesn't appear in the induced topology. Thus the present case is an example (and not a counter-example) of theorem~\ref{tho:H_ferme}.

This example show the importance of the condition for a topological subspace to have \emph{exactly} the induced topology. If not, any Lie subgroup were a topological Lie subgroup because a submanifold has at least the induced topology. We will go further with this example after the proof.

\section{Cosets}
%------------------

We consider $G$, a Lie group and $H$, a closed subgroup. Then from theorem~\ref{tho:diff_sur_ferme},  there exists an unique analytic structure on $H$ for which $H$ is a topological Lie subgroup of $G$. We naturally consider this structure on $H$. We also consider $\lG$ and $\lH$, the Lie algebras of $G$ and $H$, and $\lM$ be a subspace of $\lG$ such that $\lG=\lM\oplus\lH$.

Now we will study the structure of the coset space $G/H$ on which we put the topology such that $\pi$ is continuous and open; this is the \defe{natural topology}{natural topology}\index{topology!natural on $G/H$}\label{pg:natur_topo}. As notations, we define $p_0=\pi(e)$ and $\dpt{\psi}{\lM}{G}$, the restriction to $\lM$ of the exponential.

\begin{lemma}
The dimension of $G/H$ is $\dim (G/H)=\dim G-\dim H$.
 \label{lem:dim_G_H}
\end{lemma}

\begin{proof}
We decompose the Lie algebra $\lG$ as $\lG=\lH\oplus\lM$, and we will see that there exists a real vector space isomorphism $\dpt{\psi}{T_{[e]}(G/H)}{\lM}$ given by
\begin{equation}
   \psi(X)=\Dsdd{ e^{m(t)} }{t}{0}
\end{equation}
if $X(t)=[g(t)]$ with $g(t)=e^{m(t)}e^{h(t)}$ where $m(t)\in\lM$ and $h(t)\in\lH$ (the existence of such a decomposition in reasonably small neighbourhood of $e$ is given by lemma~\ref{lem:decomp}). The fact that $\psi$ is surjective is clear. The injectivity is also easy: $\psi(X)=0$ implies that $\exp m(t)$ is a constant. Thus
\[
X=\Dsdd{ [cst\, e^{h(t)}] }{t}{0}=\Dsdd{[cst]}{t}{0}=0.
\]

\end{proof}

\begin{lemma} \label{lem:vois_U}
There exists a neighbourhood $U$ of $0$ in $\lM$ such that
 \begin{enumerate}
 \item $\psi$ is homeomorphic on $U$,
 \item $\pi$ sends homeomorphically $\psi(U)$ on a neighbourhood of $p_0$ in $G/H$.
 \end{enumerate}
\end{lemma}

\begin{proof}
By lemma~\ref{lem:decomp}, we consider bounded, open and connected neighbourhoods $\mU_m$ and $\mU_h$ of $0$ in $\lM$ and $\lH$ such that
\[
  \dpt{\phi}{(A,B)}{e^Ae^B}
\]
is a diffeomorphism from $\mU_m\times\mU_h$ to an open neighbourhood of $e$ in $G$. Since $H$ has the induced topology from $G$, we can find a neighbourhood $V$ of $e$ in $G$ such that $V\cap H=\exp\mU_h$.

Now we take $U$, a compact neighbourhood of $0$ in $\mU_m$ such that
\begin{equation}\label{eq:UUV}
  e^{-U}e^{U}\subset V.
\end{equation}
So, $\psi$ is an homeomorphism from $U$ to $\psi(U)$. Indeed for $X\in U$, $\psi(X)=e^X=\phi(X,0)$ and $\phi$ is diffeomorphic.

On the other hand, $\pi$ is bijective on $\psi(U)$. In order to see that it is injective, let us consider $X$, $Y\in U$ such that $\pi(e^{X})=\pi(e^{Y})$. Then $\exp X$ and $\exp Y$ are in the same class with respect to $H$: $\exp X\in[\exp Y]$. Then $\exp(-X)\exp Y\in H$, and reversing the role\angl of $X$ and $Y$, $\exp(-Y)\exp X\in H$. Since $X',X''\in U$ and \eqref{eq:UUV},
\[
  e^{-Y}e^{X}\in V\cap H.
\]
Then there exists a $Z$ in $\mU_h$ such that $\exp X=\exp Y\exp Z$, but $U$ is a subset of $\mU_m$ (so that $(A,B)\to e^Ae^B$ is diffeomorphic), then $X=Y$ and $Z=0$.

Since $\pi$ is bijective on $\psi(U)$, it is homeomorphic because the topology is build in order for $\pi$ to be open and continuous.

On a third hand, $U\times\mU_h$ is a neighbourhood of $(0,0)$ in $\mU_m\times\mU_h$, so that $e^Ue^{\mU_h}$ is a neighbourhood of $e$ in $G$. Since $\pi$ is open, $\pi(\exp U\exp \mU_h)=\pi(\psi(U))$ is a neighbourhood of $p_0$ in $G/H$.
\end{proof}


Let $N_0$ be the interior of $\pi(\psi(U))$ and $\{X_1,\ldots, X_r\}$ a basis of $\lM$. If $g\in G$, we looks at the map
\[
  \pi(g\cdot e^{x_1X_1+\cdots+x_rX_r})\to(x_1,\ldots,x_r).
\]
This is an homeomorphism from $g\cdot N_0$ to an open subset of $\eR^r$ because $\pi$ is homeomorphic from $U$. With this chart, $G/H$ is an analytic manifold \nomenclature{$G/H$}{as analytic manifold} and moreover if $x\in G$, the map
\begin{equation}\label{eq:tau_x_y}
  \dpt{\tau(x)}{[y]}{[xy]}
\end{equation}
is an analytic diffeomorphism of $G/H$. Let us prove it. If we consider $[x]\in G/H$, we can write $x=gm$ for a certain $m\in\psi(U)$. Hence the chart around $[x]$ will be around $[gm]=[ge^{x_1X_1+\cdots+x_rX_r}]$ (in other word, we can find an open set around $[x]$ on which can be parametrised so). We can forget the $g$ because the action is a diffeomorphism. Then we looks at the chart $\dpt{\varphi}{G/H}{\eR^r}$, $\varphi[e^{x_1X_1+\cdots+x_rX_r}]=(x_1,\ldots,x_r)$. The map \eqref{eq:tau_x_y} makes 
\begin{equation}
    (y_1,\ldots,y_r)\to( CBH_1(x_1,\ldots,x_r,y_1,\ldots y_r),\ldots, CBH_r(x_1,\ldots,x_r,y_1,\ldots y_r)).
\end{equation}
But $CBH$ is a diffeomorphism.

\begin{theorem}[\cite{Helgason}]\label{Helgason4.2}\label{tho:struc_anal}
Let $G$ be a Lie group, $H$ a closed subgroup of $G$ and $G/H$ with the natural topology. Then $G/H$ has an unique analytic structure with the property that $G$ is a Lie transformation group of $G/H$.
\end{theorem}

\begin{proof}
We denote by $\UU$ the interior of the $U$ given by the lemma~\ref{lem:vois_U}, and $B=\psi(\UU)\subset G$. Since $\dpt{\phi}{(A,B)}{\exp A\exp B}$ is a diffeomorphism, $\psi(\UU)=\phi(U,0)$ is a submanifold of $G$. We consider the following diagram:
\[
\xymatrix{
    G\times B  \ar[d]_{\displaystyle I\times\pi}\ar[r]^{\displaystyle\Phi}    &
                                                                     G\ar[d]^{\displaystyle\pi}\\
    G\times N_0 &                                                             G/H
  }\]
with, for $g\in G$ and $x\in B$,
\[
I\times\pi\colon (g,x)\mapsto (g,[x])
\]
and
\[
\Phi\colon (g,x)\mapsto gx.
\]

\noindent The classes $[x]$ are taken with respect to $H$. The map $\dpt{\mu}{G\times N_0}{G/H}$, $\mu(g,[x])=[gx]$ can be written under the form
\[
   \mu=\pi\circ\Phi\circ(I\times\pi)^{-1}
\]
which is analytic\footnote{Notice that the inverse of $I\times\pi$ exists because $\pi$ is homeomorphic on the spaces considered here.}. So $G$ is a Lie transformation group on $G/H$.

% Faut encore taper l'unicité
\end{proof}


\begin{lemma}[Category theorem] \label{lem:categ}
If a locally compact space $M$ can be written as a countable union
\begin{equation}\label{eq:M_union}
   M=\bigcup_{n=1}^{\infty}M_n
\end{equation}
where each $M_i$ is closed in $M$, then at least one of them contains an open subset of $M$.
\end{lemma}

\begin{proof}
We suppose that none of the $M_i$ contains an open subset of $M$. Let $U_1$ be an open whose closure is compact, $a_1\in U_1\setminus M_1$ and a neighbourhood $U_2$ of $a_1$ such that $\overline{U}_2\subset U_1$ and $\overline{U_2}\cap M_1=\varnothing$. Let $a_2\in U_2\setminus M_2$ and a neighbourhood $U_3$ of $a_2$ such that $\overline{U_3}\subset U_2$ and $\overline{U_3}\cap M_2=\varnothing$\ldots and so on. The existence of the $a_i$ comes from the fact that $U_j$ is open, so that it is contained in no one of the $M_k$.

The decreasing sequence $\overline{U}_1,\overline{U}_2 ,\ldots$ is made up from non empty compacts sets. Then $\bigcap_{i=1}^{\infty}U_i\neq\emptyset$ and the elements of this intersection are in none of the $M_i$; this contradict \eqref{eq:M_union}.
\end{proof}


\begin{theorem} \label{tho:homeo_action}
Let $G$ be a locally compact group with a countable basis. Suppose that it is a transitive, locally compact and Hausdorff topological group of transformation on $M$. Consider $p\in M$ and $H=\{g\in G\tq g\cdot p=p\}$. Then
\begin{enumerate}
\item $H$ is closed,
\item the map $[g]\to g\cdot p$ is homeomorphic between  $G/H$ and $M$.
\end{enumerate}
\end{theorem}

\begin{proof}
By definition of a group action, the map $\dpt{\varphi}{G}{M}$, $\varphi(g)=g\cdot p$ is continuous. Then $H=\varphi^{-1}(p)$ is closed in $G$.

As usual, the topology considered on $G/H$ is a topology which makes the canonical projection $\dpt{\pi}{G}{G/H}$ continuous and open. Now we study the map $\dpt{\psi}{G/H}{M}$, $\psi([g])=g\cdot p$ which is well defined because $H$ fixes $p$ by definition. It is clearly injective, and it is surjective because the action is transitive.

Now remark that $\psi=\varphi\circ\pi^{-1}$. Since $\pi$ is continuous and open, and $\varphi$ is continuous, it just remains to be proved that $\varphi$ is open in order for $\psi$ to be continuous and open. In order to do it, consider $V$, an open subset of $G$, $g\in V$ and a compact neighbourhood $U$ of $e$ in $G$ such that $U=U^{-1}$ and $gU^2\subset V$. If $U$ is small and $u$, $v\in U$ close to $e$, then $guv$ can keep in $V$, so that such a $U$ exists.

We can find a sequence $(g_n)$ in $G$ such that $G=\bigcup_ng_nU$; the transitivity of $G$ on $M$ implies that
\[
  M=\bigcup_ng_nU\cdot p.
\]
Each term in this union is compact, and therefore closed in $M$. By lemma~\ref{lem:categ}, one of the $g_nU\cdot p$ contains an open subset of $M$. Since the action ``$g\cdot$''\ is continuous, $U\cdot p$ also contains an open subset in $M$. The conclusion is that one can find a $u\cdot p$ in the interior of $M$, and $p$ is then an interior point of $u^{-1} U\cdot p\subset U^2\cdot p$. Then $g\cdot p$ in in the interior of $V\cdot p$ and $\varphi$ is therefore open.
\end{proof}

\begin{proposition}
Let $G$ be a transitive transformation Lie group on a $\Cinf$ manifold $M$. Consider $p_0\in M$ and $H$, the stabilizer of $p_{0}$:
\[
  H=\{ g\in G\tq g\cdot p_0=p_0 \}.
\]
Let
\begin{equation}
\begin{aligned}
 \alpha\colon G/H&\to M \\
[g]&\mapsto g\cdot p_{0}.
\end{aligned}
\end{equation}
We have:
\begin{enumerate}
\item The stabilizer $H$ is closed in $G$.
\item If $\alpha$ is homeomorphic, then it is diffeomorphic (if $G/H$ has the analytic structure of theorem~\ref{tho:struc_anal}).
\item If $\alpha$ is homeomorphic and if $M$ is connected, then $G_0$, the identity component of $G$, is transitive on $M$.
\end{enumerate}
\label{propHelgason4.3}
\end{proposition}

This comes from \cite{Helgason}, chapter 2, proposition 4.3. The interest of this theorem is the fact that one only has to check the continuity of $\alpha$ and $\alpha^{-1}$ in order to have a diffeomorphism $M\simeq G/H$.

\begin{proof}
\subdem{The group $H$ is closed in $G$}
We consider the map $\dpt{\varphi}{G}{M}$, $\varphi(g)=g\cdot p_0$. This is continuous; therefore $\varphi^{-1}(p_0)$ is closed. Remark that we are in the situation of theorem~\ref{tho:homeo_action}

\subdem{First item}
We will use lemma~\ref{lem:vois_U}. Se denotes by $\lH$, the Lie algebra of $H$ and we consider a $\lM$ such that $\lG=\lM\oplus\lH$; the lemma~\ref{lem:vois_U} assure us that we have a neighbourhood $U$ of $0$ in $\lM$ on which $\psi$ is homeomorphic and such that $\pi$ sends homeomorphically $\psi(U)$ to a neighbourhood of $p_0$ in $G/H$. We define $\UU$, the interior of $U$, $B=\psi(\UU)$ and $N_0$, the interior of $\pi(\psi(U))$.

The set $B$ is a submanifold of $G$, diffeomorphic to $N_0$ by $\pi$ because everything is continuous and then everything respect the interiors.

\begin{probleme}
C'est n'importe quoi comme justification. C'est lié au problème~\ref{prob:diffeo_2}.
\label{prob:diffeo_1}
\end{probleme}

Consider $\dpt{\iota}{B}{G}$, the identity and $\dpt{\beta}{G}{M}$, $\beta(g)=g\cdot p_0$. The restriction $\alpha_{N_0}$ of $\alpha$ to $N_0$ is an homeomorphism (this is a part of the assumptions) from $N_0$ to an open subset of $M$: $N_0$ is open (this is an interior), then its image by an homeomorphism is open.

Now we can see that $\alpha_{N_0}$ is differentiable. The reason is that it can be written as $\alpha_{N_0}=\dpt{\beta\circ\iota\circ\pi^{-1}}{N_0}{M}$. The construction makes $\pi$ a diffeomorphism and $\beta$ a diffeomorphism when $G$ is a Lie group of transformations (as it is the case here); $\iota$ is clear. Now we have to see that the whole $\alpha$ is also differentiable, and then we will have to prove the same for $\alpha^{-1}$.

By definition, $\alpha([g])=g\cdot p_0$ (the classes $[g]$ is taken with respect to $H$). Consider $[n]\in H$; for any $g\in G$, one can write $[g]=[gn^{-1} n]$. Then
\begin{equation}
  \alpha([g])=\alpha([gn^{-1} n])
             =gn^{-1} n\cdot p_0
	     =gn^{-1}\alpha([n])
	     =gn^{-1}\cdot\alpha_{N_0}([n]),
\end{equation}
but the last dot denotes a differentiable action, and $\alpha_{N_0}$ is differentiable. Thus $\alpha$ is differentiable.

In order for $\alpha$ to be a diffeomorphism, we still have to prove that $\alpha^{-1}$ is differentiable., we begin to show that the Jacobian of $\beta$ at $g=e$ has rank $r_{\beta}=\dim M$. We looks at $\dpt{d\beta_e}{\lG}{T_{p_0}M}$, and consider $X\in\ker (d\beta_e)$. For $f\in\Cinf(M)$, we compute
\begin{equation}
  0=(d\beta_e X)f=X(f\circ\beta)=\Dsdd{ f(e^{tX}\cdot p_0) }{t}{0}.
\end{equation}
Let $s\in\eR$, and we write this equation for $f^*$ instead of $f$, which $f^*$ defined by $f^*(q)=f(e^{sX}\cdot q)$ for each $q\in M$:
\begin{equation}
  0=\Dsdd{f^*(e^{tX}\cdot p_0)}{t}{0}
      =\Dsdd{ f(e^{(s+t)X}\cdot p_0) }{t}{0}
      =\Dsdd{ f(e^{tX}\cdot p_0) }{t}{s}.
\end{equation}
Thus $f(e^{sX}\cdot p_0)$ is a constant with respect to $s$. Since $f$ is arbitrary, $e^{sX}\cdot p_0=p_0$ for any $s$. So $X\in\lH$ because $\exp sX\in H$ for any $s$. Then $\ker d\beta_e\subset \lH$.

On the other hand, $\lH\subset\ker d\beta_e$ is clear, then
\[
   \ker d\beta_e=\lH
\]
and $r_{\beta}=\dim\lG-\dim\lH$.

Since $\alpha$ is an homeomorphism, the dimension of the origin and the target space are the same: $\dim G/H=\dim M$. On the other hand, lemma~\ref{lem:dim_G_H} gives $\dim G/H=\dim\lG-\dim\lH$, so that $r_{\beta}=\dim M$.

Now we prove that $\alpha^{-1}$ is differentiable. Remark that $\beta(g)=g\cdot p_0$ and $\alpha([g])=g\cdot p_0=\beta(g)$ is a good definition for $\alpha$ because the class are taken with respect to the stabilizer of $p_0$. Since $r_{\beta}=\dim M$, the map $\beta$ is locally a diffeomorphism from a neighbourhood of $e$ to a neighbourhood of $p_0$.

If $p=g\cdot p_0$, $\alpha^{-1}(p)=[g]$ because $[k]\in\alpha^{-1}(o)$ if $\alpha([k])=p$, i.e. $k\cdot p_0=p$. But $k=gr$ for a certain $r\in G$. It is clear that $p=k\cdot p_0=gr\cdot p_0$. In particular, $g\cdot(r\cdot p_0)$. We know that in general $g\cdot p=g\cdot q$ implies $p=q$; here it gives us $r\in H$, so that $k\in [g]$.

We consider a $n\in G$ such that $n\cdot$ and $n^{-1}\cdot$ are diffeomorphic. We can make the following manipulation:
\begin{equation}
   \alpha^{-1}(p)=[g]
                =[gnn^{-1}]
		=\pi(gn)\alpha^{-1}(n^{-1}\cdot p_0).
\end{equation}
Under this form, $\alpha^{-1}$ is diffeomorphic.


\subdem{Second item}
If $\alpha$ is an homeomorphism, then $\beta$ is open. Let us denote by $G_0$ the identity component of $G$. There exists a subset $\{x\bgamma\tq\gamma\in I\}$ of $G$ such that
\[
    G=\bigcup_{\gamma\in I}G_0x\bgamma.
\]
This comes from the fact that the components are all some left translations of the identity component (this is true for any Lie group). Each orbit $G_0x\bgamma\cdot p_0$ is open in $M$ and two orbits are either disjoint either equals. Since $M$ is connected, all these orbits must coincide; thus each orbit contains the whole $M$. In particular, the orbit $G_0\cdot p_0=M$: $G_0$ is transitive on $M$.

\end{proof}

\begin{probleme}
Il parra\^it que \c ca donne l'unicit\'e pour~\ref{tho:struc_anal}.
\end{probleme}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Matrix Lie group and its algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooTSAJooNtjgMD}

In this section we deal with Lie groups made from matrices, that is subgroups of \( \GL(n, \eC)\) (typically \( \SO(n)\) or \( \SU(n)\)) and their Lie algebra. We will denote the identity either by \( e\) or by \( \mtu\).

\begin{normaltext}      \label{NORMooHZGKooJEiamo}
    It is time to reread the remark \ref{REMooJQFHooQuoZxt}. In this section, when \( \gamma\) is a path in the matrix group \( G\), we denote by \( \gamma'(0)\) the ``usual'' derivative of \( \gamma\): that is the component-wise derivative; not the differential operator.

    We denote by \( D_{\gamma}\) the differential operator
    \begin{equation}
        \begin{aligned}
            D_{\gamma}\colon  C^{\infty}(G)&\to \eR \\
            f&\mapsto \Dsdd{ f\big( \gamma(t) \big) }{t}{0}. 
        \end{aligned}
    \end{equation}

    We aim to study the link between \( D_{\gamma}\) and \( \gamma'(0)\).

    From the Lie group of matrix \( G\) we can build (at least) two Lie algebras\footnote{Definition \ref{DEFooVBPKooGxlDBn}.}:
    \begin{itemize}
        \item The usual Lie algebra of the group: \( T_eG\) with the definition \ref{DEFooKDCPooZOJsMD}. As set, this is
            \begin{equation}
                T_eG=\{ D_{\gamma}\st \gamma(0)=e \}
            \end{equation}
            with the implicit that \( \gamma\) is a smooth path in \( G\).
        \item 
            The set of ``usual'' derivatives of the paths in \( G\):
            \begin{equation}
                G'=\{ \gamma'(0)\tq \gamma(0)=e \}.
            \end{equation}
            This is a set of matrices on which we can use the bracket \( [X,Y]=XY-YX\) (matrix product). We will see the following facts.
            \begin{itemize}
                \item 
                    The set \( G'\) is a Lie algebra in proposition \ref{PROPooUKITooLnEKZW},
                \item
                    The Lie algebras \( G'\) and \( T_eG\) are isomorphic as Lie algebras in theorem \ref{THOooWQGMooHyjRtx} for the case \( G=\GL(n,\eC)\)
                \item
                    When \( H\) is a Lie subgroup of \( \GL(n,\eC)\), the Lie algebras \( H'\) and \( T_eH\) are isomorphic as Lie algebras in proposition \ref{PROPooSQHLooGQAykc} for the Lie subgroups of \( \GL(n,\eC)\).
            \end{itemize}
    \end{itemize}
\end{normaltext}

\begin{lemma}[\cite{MonCerveau}]
    Let \( G\) be a matrix Lie group, et \( g\in G\) and \( X\in G'\). Then \( gXg^{-1}\in G'\).
\end{lemma}

\begin{proof}
    Let \( x\colon \eR\to G\) be a smooth path such that \( X=x'(0)\). Then we the derivative of the path given by the matrix product
    \begin{equation}
        t\mapsto gx(t)g^{-1}
    \end{equation}
    is \( gXg^{-1}\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooHQUYooSoiKbI}
    Let \( G\) be a matrix Lie group. Then \( G'\) is a vector space on \( \eR\).
\end{lemma}

\begin{proof}
    Let \( X,Y\in G'\) be the derivatives of the paths \( x\) and \( y\). If we set \( \varphi_1(t)=x(t)y(t)\) we have
    \begin{equation}
        \varphi_1'(0)=x'(0)y(0)+x(0)y'(0).
    \end{equation}
    Since \( x(0)=y(0)=e\) we have \( \varphi'(0)=X+Y\), so that \( X+Y\in G'\).

    For the product by a scalar, let the path \( \varphi_2(t)=x(\lambda t)\). The component-wise derivative
    \begin{equation}
        \varphi_2'(0)=\lambda x'(0)=\lambda X,
    \end{equation}
    so that \( \lambda X\in G'\).
\end{proof}

\begin{proposition}     \label{PROPooUKITooLnEKZW}
    Let \( G\) be a matrix Lie group. The vector space \( G'\) is a Lie algebra for the matrix commutator.
\end{proposition}

\begin{proof}
    We already know that \( G'\) is a real vector space by lemma \ref{LEMooHQUYooSoiKbI}. The fact that \( (X,Y)\mapsto XY-YX\) satisfies the axioms of a Lie algebra is easy to check. The only point is to show that if \( X,Y\in G'\), then \( [X,Y]=XY-YX\in G'\).

    Let
    \begin{equation}        \label{EQooJDTLooGWsDiq}
        \varphi(t)=x(t)Yx(-t).
    \end{equation}
    This is for sure a path in the full matrix vector space, and this is derivable because \( x\) is derivable while the matrix product is linear. So the derivative \( \varphi'(0)\) is still a matrix. The question is: why \( \varphi'(0)\in G'\) ?

    By lemma \ref{LEMooHQUYooSoiKbI}, for each \( t\) we have
    \begin{equation}
        \frac{ \varphi(t)-\varphi(0) }{ t }\in G'.
    \end{equation}
    Now, \( G'\) is a vector subspace of \( \eM(n,\eC)\) which is finite dimensional; is is thus closed and the limit belongs to \( G'\).

    Is is now a simple computation to show that \( \varphi'(0)=[X,Y]\).
\end{proof}

\begin{normaltext}
The following theorem is a Giulietta's masterpiece in the following sense:
\begin{itemize}
    \item It is fundamental because the Lie algebra isomorphism between \( T_eGL(n,\eR)\) and the matrices is used everywhere one says «The Lie algebra of $\SO(3)$ is the set of skew-symmetric traceless matrices».
    \item
        Either I'm idiot, either I never seen that theorem even stated (let alone being proved)\footnote{There is in fact a third possibility:  this theorem is a classic one but cannot be found \emph{on internet}.}.
    \item
        I think that the fundamental misunderstanding\footnote{Once again, either I'm idiot either everybody is wrong but me\ldots well \ldots} is that in the context of Lie groups, people \emph{define} \( [X,Y]\) as being \( \ad(X)Y\) while \( \ad\) is defined as the ``second differential'' of \( \AD(g)h=ghg^{-1}\). In that case, obviously we get \( [X,Y]=XY-YX\) with the matrix product. This way fails to make the link with the commutator of vector fields as defined by \ref{DEFooHOTOooRaPwyo}.
    \item
        So you must read this proof with much care and write me if you see any mistake or unclear point.
\end{itemize}
\end{normaltext}
So here it is with the notations explained in \ref{NORMooHZGKooJEiamo}.
    

\begin{theorem}     \label{THOooWQGMooHyjRtx}
    Let \( G=\GL(n,\eC)\) be the group of invertible matrices. The map
    \begin{equation}
        \begin{aligned}
            \phi\colon G'&\to T_eG \\
            \gamma'(0)&\mapsto D_{\gamma} 
        \end{aligned}
    \end{equation}
    is 
    \begin{enumerate}
        \item
            well defined,
        \item
            bijective,
        \item
            linear,
        \item
            a Lie algebra isomorphism.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Several points to be proved.
    \begin{subproof}
        \item[\( \phi\) is well defined]
            Let \( \alpha\) and \( \beta\) be paths in \( G\) such that \( \alpha'(0)=\beta'(0)\) and let \( f\colon G\to \eR\) be a smooth function. We have to prove that \( D_{\alpha}(f)=D_{\beta}(f)\).

            We consider a chart \( \varphi\colon \mU\to \mO\) where \( \mU\) is a neighbourhood of \( 0\) in \( \eR^m\) and \( \mO\) is a neighbourhood of \( e\) in \( \GL(n,\eC)\). We suppose that \( \varphi(0)=e\). We set \( \tilde f=f\circ \varphi\), \( \tilde \alpha=\varphi^{-1}\circ \alpha\) and \( \tilde \beta=\varphi^{-1}\circ\beta\). We have
            \begin{subequations}
                \begin{align}
                    D_{\alpha}(f)&=\Dsdd{ f\big( \alpha(t) \big) }{t}{0}\\
                    &=\Dsdd{ \tilde f\big( \tilde \alpha(t) \big) }{t}{0}\\
                    &=\sum_{i=1}^m\frac{ \partial \tilde f }{ \partial x_i }\big( \tilde \alpha(0) \big)\tilde \alpha_i(0).
                \end{align}
            \end{subequations}
            Since \( \tilde \alpha(0)=\tilde \beta(0)\) we still have to prove that \( \tilde \alpha_i'(0)=\tilde \beta_i'(0)\). As you remember, \( \tilde \alpha\) is a map from \( \eR\) to \( \eR^m\), so that the following derivative is quite usual:
            \begin{subequations}
                \begin{align}
                    \tilde \alpha'(0)&=\Dsdd{ (\varphi^{-1}\circ \alpha)(t) }{t}{0}\\
                    &=d\varphi^{-1}_{\alpha(0)}\big( \alpha'(0) \big)\\
                    &=d\varphi^{-1}_{\beta(0)}\big( \beta'(0) \big).
                \end{align}
            \end{subequations}
            Thus the map \( \phi\) is well defined.
        \item[\( \phi\) is linear]
            This is from the linearity of the derivation.
        \item[\( \phi\) is injective]
            If \( \phi(\alpha')=\phi(\beta')\), then \( D_{\alpha}(f)=D_{\beta}(f)\) for every function \( f\). In that case,
            \begin{equation}
                \sum_{i=1}^m\frac{ \partial \tilde f }{ \partial x_i }(e)\tilde \alpha_i'(0)=\sum_{i=1}^m\frac{ \partial \tilde f }{ \partial x_i }(e)\tilde \beta_i'(0).
            \end{equation}
            That equation must be satisfied for every function. Taking the projection on the components, we get \( \tilde \alpha_i'(0)=\tilde b_i'(0)\), which means \( \alpha'(0)=\beta'(0)\) because \( \varphi^{-1}\) is bijective.
        \item[\( \phi\) is surjective]
            Every element of \( T_eG\) is of the form \( D_{\alpha}\) for some path \( \alpha\), so \( \phi\) is surjective.
        \item[\( \phi\) is a Lie algebra isomorphism]
            Let \( X,Y\in G'\) being the derivative of the paths \( \alpha\) and \( \beta\). We have to prove that
            \begin{equation}
                [\phi(X),\phi(Y)]=\phi[X,Y].
            \end{equation}
            If \( t\) is small enough, the paths
            \begin{subequations}
                \begin{align}
                    \alpha(t)=\mtu+tX\\
                    \beta(t)=\mtu+tY\\
                \end{align}
            \end{subequations}
            are good ones because \( \det(\mtu)\neq 0\), so that the determinant of \( \mtu+tX\) remains different from zero when \( t\) is small, whatever \( X\) is. So \( \alpha\) and \( \beta\) are paths in \( \GL(n,\eC)\). Using the general definition in differential geometry,
            \begin{subequations}        \label{SUBEQSooCYRDooFOdLrn}
                \begin{align}
                    [\phi(X),\phi(Y)]f&=[\phi(X)^L,\phi(Y)^L]_ef\\
                    &=\phi(X)^L_e\big( \phi(Y)^L(f) \big)-\phi(Y)^L_e\big( \phi(X)^L(f) \big) \label{SUBEQooOPUAooZYsZlX}.
                \end{align}
            \end{subequations}
            We focus on the first term:
            \begin{subequations}        \label{SUBEQooTUNFooFkDmuP}
                \begin{align}
                    \phi(X)^L\big( \phi(Y)^L(f) \big)&=\Dsdd{ \phi(Y)^L_{\phi(X)^L_e(t)}(f) }{t}{0}\\
                    &=\DDsdd{ f\big( (\mtu+tX)(\mtu+sY) \big) }{t}{0}{s}{0}\\
                    &=\DDsdd{ f(\mtu+tX+sY+tsXY) }{t}{0}{s}{0}\\
                    &=\Dsdd{ df_{\mtu+tX}\big( (\mtu+tX)Y \big) }{t}{0} \label{SUBEQooLHPBooTnXiZd}\\
                    &=\Dsdd{ df_{\mtu+tX}(Y) }{t}{0}+\Dsdd{ df_{\mtu+tX}(tXY) }{t}{0}   \label{SUBEQooMXJJooBFTLsM}
                \end{align}
            \end{subequations}
            where we have used the linearity of \( df_{\mtu+tX}\) and where \( XY\) stands for the matrix product. In the expression \eqref{SUBEQooLHPBooTnXiZd}, the symbol \( df\) stands for the differential of \( f\) as function from \( \eM(n,\eC)\) (as vector space), not for the differential of \( f\) on \( G\) as manifold. This is why we are allowed to put an expression as the matrix \( Y\) as argument of \( df_{\mtu+tX}\) while \( Y\) is not an element of \( T_{\mtu+tX}G\).

            The expression \eqref{SUBEQooMXJJooBFTLsM} is still made of two terms. The second one is
            \begin{equation}
                \Dsdd{ df_{\mtu+tX}(tXY) }{t}{0}=\Dsdd{ tdf_{\mtu+tX}(XY) }{t}{0}=df_{\mtu}(XY)
            \end{equation}
            where we used the Leibnitz rule\footnote{In general, notice that \( \Dsdd{ tf(t) }{t}{0}=f(0)\)}.

            The first term in \eqref{SUBEQooMXJJooBFTLsM} is computed as
            \begin{equation}
                    \Dsdd{ df_{\mtu+tX}(Y) }{t}{0}=\DDsdd{ f(\mtu+tX+sY) }{t}{0}{s}{0}.
            \end{equation}
            We set 
            \begin{equation}
                \begin{aligned}
                    \gamma\colon \eR^2&\to G \\
                    (t,s)&\mapsto \mtu+tX+sY, 
                \end{aligned}
            \end{equation}
            so that
            \begin{subequations}
                \begin{align}
                    \Dsdd{ df_{\mtu+tX}(Y) }{t}{0}&=\DDsdd{ f(\mtu+tX+sY) }{t}{0}{s}{0}\\
                    &=\DDsdd{ (\tilde f\circ\varphi^{-1}\circ\gamma)(t,s) }{t}{0}{s}{0}\\
                    &=\DDsdd{ g(t,s) }{t}{0}{s}{0}
                \end{align}
            \end{subequations}
            where the function \( g=\tilde f\circ\varphi^{-1}\circ \gamma\) is a smooth function from \( \eR^2\) to \( \eR\).        

            The expression \eqref{SUBEQooTUNFooFkDmuP} is now
            \begin{equation}
                \phi(X)^L\big( \phi(Y)^L(f) \big)=\DDsdd{ g(t,s) }{t}{0}{s}{0}+df_{\mtu}(XY).
            \end{equation}
            The commutator we have to compute, with the same computations is
            \begin{equation}
                [\phi(X),\phi(Y)]f=\DDsdd{ g(t,s) }{t}{0}{s}{0}+df_{\mtu}(XY)-\DDsdd{ g(s,t) }{t}{0}{s}{0}-df_{\mtu}(YX).
            \end{equation}
            The function \( g\) being \(  C^{\infty}\), the derivative commute and the corresponding termes annihilate each other and we are left with
            \begin{equation}
                [\phi(X),\phi(Y)]f=df_{\mtu}(XY)-df_{\mtu}(YX)=df_{\mtu}(XY-YX)
            \end{equation}
            where we used the linearity of the differential.

            In the other sense,
            \begin{equation}
                \phi[X,Y]f=\Dsdd{ f(\mtu+tXY-tYX) }{t}{0}=df_{\mtu}\big( [X,Y] \big)
            \end{equation}
            where, once again, \( df\) stands for the ``usual'' differential.
    \end{subproof}
\end{proof}

Ok. This is proved for \( G=\GL(n,\eC)\), the full matrix group. What about subgroups ? Here is the result.

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooSQHLooGQAykc}
    Let \( H\) be a Lie subgroup\footnote{Thanks to the Cartan theorem \ref{THOooDEJHooVKJYBL}, there are plenty of them.} of \( \GL(n,\eC)\). With the same notations as above, the map
    \begin{equation}
        \begin{aligned}
            \phi\colon H'&\to T_eH \\
            \gamma'(0)&\mapsto D_{\gamma} 
        \end{aligned}
    \end{equation}
    is a Lie algebra isomorphism.
\end{proposition}

\begin{proof}
    We have to prove that
    \begin{equation}        \label{EQooRLBBooYgHhtH}
        \phi[X,Y]f=[\phi(X),\phi(Y)]f
    \end{equation}
    for every \( X,Y\in H'\) and \( f\in  C^{\infty}(H)\). For that, we will see the left and right hand sides of \eqref{EQooRLBBooYgHhtH} in \( G=\GL(n,\eC)\), and use the already proved result, theorem \ref{THOooWQGMooHyjRtx}.

    If \( X,Y\in H'\) we know from proposition \ref{PROPooUKITooLnEKZW} that \( [X,Y]\in H'\). Thus there exists a path \( \gamma\colon \eR\to H\) such that \( [X,Y]=\gamma'(0)\). We consider the extension\footnote{The proposition \ref{PROPooOTZQooIfboXV} can be used since \( H\) is a submanifold of \( G\) by \ref{PROPooFXZJooCOFXZX}.} \( \tilde f\colon W\to \eR\) of \( f\) such that \( \tilde f=f\) on \( H\) and \( W\) is an open set around \( e\) in \( \GL(n,\eC)\). For the sake of making things complicated we also define \( \tilde \gamma=\iota\circ \gamma\) where \( \iota\colon H\to \GL(n,\eC)\) is the inclusion. With all that we have
    \begin{equation}
        \phi[X,Y]f=\Dsdd{ f\big( \gamma(t) \big) }{t}{0}=\Dsdd{ \tilde f\big( \tilde \gamma(t) \big) }{t}{0}=\clubsuit.
    \end{equation}
    At this point, notice that \( [X,Y]\in \GL(n,\eC)'\) and \( [X,Y]=\tilde \gamma'(0)\), so that if we consider the map \( \tilde \phi\colon \GL(n,\eC)\to T_e\GL(n,\eC)\) we also have
    \begin{equation}
        \clubsuit=\Dsdd{ \tilde f\big( \tilde \gamma(t) \big) }{t}{0}=\tilde \phi[X,Y]\tilde f=\big[ \tilde \phi(X),\tilde \phi(Y) \big]\tilde f
    \end{equation}
    where we used the result \ref{THOooWQGMooHyjRtx} on \( \GL(n,\eC)\).

    We still have to prove that \( \tilde \phi(X)\tilde \phi(Y)\tilde f=\phi(X)\phi(Y)f\). Using, among others the formula \ref{SUBEQSooHKWMooQbeStl} adapted to \( \tilde \phi(X)\) instead of \( X\):
    \begin{subequations}
        \begin{align}
            \tilde \phi(X)\tilde \phi(Y)\tilde f&=\Dsdd{ \big( \tilde \phi(Y)^L\tilde f \big)\big( \alpha(t) \big) }{t}{0}\\
            &=\Dsdd{ \tilde \phi(Y)^L_{\alpha(t)}\tilde f }{t}{0}\\
            &=\DDsdd{ \tilde f\big( \alpha(t)\beta(u) \big) }{t}{0}{s}{0}.
        \end{align}
    \end{subequations}
    At this point, notice that \( \alpha(t)\) and \( \beta(u)\) are elements in \( H\) which is a group, so \( \tilde f\big( \alpha(t)\beta(u) \big)=f\big( \alpha(t)\beta(u) \big)\). Thus
    \begin{subequations}
        \begin{align}
            \tilde \phi(X)\tilde \phi(Y)\tilde f&=\DDsdd{ \tilde f\big( \alpha(t)\beta(u) \big) }{t}{0}{s}{0}\\
            &=\DDsdd{ f\big( \alpha(t)\beta(u) \big) }{t}{0}{s}{0}\\
            &=\phi(X)\phi(y)f.
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]
    Let \( G\) be a Lie group of matrices and \( X\in T_eG\) such that 
    \begin{equation}
        df_e(X)=0
    \end{equation}
    for every smooth function \( f\colon G\to \eR\). Then \( X=0\).
\end{lemma}

\begin{proof}
    We consider the functions \( \pr_{ij}\colon G\to \eR\) defined by \( \pr_{ij}(A)=A_{ij}\). If \( g\colon \eR\to G\) is a path, for every \( t\) we have \( \pr_{ij}g(t)=g(t)_{ij}\) and then
    \begin{equation}
        \Dsdd{ \pr_{ij}g(t) }{t}{0}=g'(0)_{ij}.
    \end{equation}
    Then we build
    \begin{equation}
        \begin{aligned}
            f\colon G&\to \eR \\
            A&\mapsto \pr_{11}(A)\pr_{ij}(A). 
        \end{aligned}
    \end{equation}
    If \( g\colon \eR\to G\) is a path such that \( g(0)=e\) and \( g'(0)=X\), then we have
    \begin{subequations}
        \begin{align}
            \Dsdd{ f\big( g(t) \big) }{t}{0}&=\Dsdd{ \pr_{11}\big( g(t) \big)\pr_{ij}\big( g(t) \big) }{t}{0}\\
            &=\pr_{11}g(0)\Dsdd{ \pr_{ij}g(t) }{t}{0}+\Dsdd{ \pr_{11}g(t) }{t}{0}\pr_{ij}g(0)\\
            &=X_{ij}+\delta_{ij}X_{11}\\
            &=X_{ij}+\delta_{ij}X_{11}.
        \end{align}
    \end{subequations}
    We know that this is zero for every choice of \( ij\):
    \begin{equation}
        X_{ij}+\delta_{ij}X_{11}=0
    \end{equation}
    In particular with \( i=j=1\) we have \( 2X_{11}=0\), so that \( X_{11}=0\). Then we are left with \( X_{ij}=0\) for every \( ij\).
\end{proof}

\section{Adjoint group, inner automorphisms}\label{sec:adj_gp}
%--------------------------

Let $\lA$ be a \emph{real} Lie algebra. We denote by $GL(\lA)$\nomenclature[G]{$GL(\lA)$}{The group of nonsingular endomorphisms of $\lA$} the group of all the nonsingular endomorphisms of $\lA$: the linear and nondegenerate operators on $\lA$ as vector space. An element $\sigma\in\GL(\lA)$ does not specially fulfils somethings like $\sigma[X,Y]=[\sigma X,\sigma Y]$. The Lie algebra $\gl(\lA)$\nomenclature[G]{$\protect\gl(\lA)$}{space of endomorphisms with usual bracket} is the vector space of the endomorphisms (without non degeneracy condition) endowed with the usual bracket $(\ad A)B=[A,B]=A\circ B-B\circ A$. The map $X\to\ad X$ is a homomorphism from $\lA$ to the subalgebra $\ad(\lA)$ of $\gl(\lA)$.

The group $\Int(\lA)$\nomenclature[G]{$\Int(\lA)$}{Adjoint group of $\lA$} is the analytic Lie subgroup of $\GL(\lA)$ whose Lie algebra is $\ad(\lA)$ by theorem~\ref{tho:gp_alg}. This is the \defe{adjoint group}{adjoint!group}\index{group!adjoint} of $\lA$.

\begin{proposition}
The group $\Aut(\lA)$\nomenclature[G]{$\Aut\lA$}{Group of automorphisms of $\lA$} of all the automorphisms of $\lA$ is a closed subgroup of $\GL(\lA)$.
\end{proposition}

\begin{proof}
The property which distinguish the elements in $\Aut(\lA)$ from the ``commons'' elements of $\GL(\lA)$ is the preserving of structure: $\varphi[A,B]=[\varphi A,\varphi B]$. These are equalities, and we know that a subset of a manifold which is given by some equalities is closed.
\end{proof}

Now, theorem~\ref{tho:diff_sur_ferme} provides us an unique analytic structure on $\Aut(\lA)$ in which it is a topological Lie subgroup of $\GL(\lA)$. From now we only consider this structure. We denote by $\partial(\lA)$\nomenclature[G]{$\partial\lA$}{The Lie algebra of $\Aut(\lA)$} the Lie algebra of $\Aut(\lA)$: this is the set of the endomorphisms $D$ of $\lA$ such that $\forall t\in\eR$, $e^{tD}\in\Aut(\lA)$. By differencing the equality
\begin{equation}\label{eq:exp_der}
  e^{tD}[X,Y]=[e^{tD}X,e^{tD}Y]
\end{equation}
with respect to $t$, we see\footnote{As usual, if we consider a basis of $\lA$ as vector space, the expression in the right hand side of \[[e^{tD}X,e^{tD}Y]=\ad(e^{tD}X)e^{tD}X\] can be seen as a product matrix times vector, so that Leibnitz works.} that $D$ is a derivation\footnote{Definition \ref{DEFooDUEUooZLhKdv}.} of \( \lA\)

Conversely, consider $D$, any derivation of $\lA$; by induction,
\begin{equation}
   D^k[X,Y]=\sum_{i+j=k}\frac{k!}{i!j!}[D^iX,D^jY]
\end{equation}
where by convention, $D^0$ is the identity in $\lA$. This relation shows that $D$ fulfils condition \eqref{eq:exp_der}, so that any derivation of $\lA$ lies in $\partial(\lA)$. Then
\[
  \partial(\lA)=\{\text{derivations of }\lA\}.
\]
The Jacobi identities show that
\[
\ad(\lA)\subset\partial(\lA).    \label{pg:ad_subset_der}
\]
From this, we deduce\footnote{See error~\ref{err:Intt_Aut}}: 
\begin{equation}\label{eq:int_sub_aut}
  \Int(\lA)\subset\Aut(\lA).
\end{equation}
Indeed the group $\Int(\lA)$ being connected, it is generated\footnote{See proposition~\ref{PropUssGpGenere}} by any neighbourhood of $e$; note that $\Aut(\lA)$ has not specially this property. We take a neighbourhood of $e$ in $\Int(\lA)$ under the form  $\exp V$  where $V$ is a sufficiently small neighbourhood of $0$ in $\ad(\lA)$ to be a neighbourhood of $0$ in $\partial(\lA)$ on which $\exp$ is a diffeomorphism. In this case, $\exp V\subset\Aut(\lA)$ and then $\Int(\lA)\subset\Aut(\lA)$.

Elements of $\ad(\lA)$ are the \defe{inner derivations}{derivation!inner} while the ones of $\Int(\lA)$ are the \defe{inner automorphisms.}{inner!automorphism}

Let $\mO$ be an open subset of $\Aut(\lA)$; for a certain open subset $U$ of $\GL(\lA)$, $\mO=U\cap\Aut(\lA)$. Then
\begin{equation}
  \iota^{-1}(\mO)=\mO\cap\Int(\lA)
           =U\cap\Aut(\lA)\cap\Int(\lA)
       =U\cap\Int(\lA).
\end{equation}

The subset $U\cap\Int(\lA)$ is open in $\Int(\lA)$ for the topology because $\Int(\lA)$ is a Lie\quext{Is it true??} subgroup of $\GL(\lA)$ and thus has at least the induced topology. This proves that the inclusion map $\dpt{\iota}{\Int(\lA)}{\Aut(\lA)}$ is continuous.

The lemma \ref{lem:var_cont_diff} and the consequence below makes $\Int(\lA)$ a Lie subgroup of $\Aut(\lA)$. Indeed $\Int(\lA)$ and $\Aut(\lA)$ are both submanifolds of $\GL(\lA)$ which satisfy \eqref{eq:int_sub_aut}. 


By definition, $\Aut(\lA)$ has the induced topology from $\GL(\lA)$. Then $\Int(\lA)$ is a submanifold of $\Aut(\lA)$. 
This is also a subgroup and a topological group : $\Int(\lA)$ is not a topological subgroup of $\Aut(\lA)$. Then $\Int(\lA)$ is a Lie subgroup of $\Aut(\lA)$. Schematically, links between $\Int\lG$, $\ad\lG$, $\Aut\lG$ and $\partial\lG$ are
\begin{subequations}\label{eq:schem_ad_int}
\begin{align}
  \Int\lG&\longleftarrow\ad\lG\\
  \Aut\lG&\longrightarrow\partial\lG.
\end{align}
\end{subequations}
Remark that the sense of the arrows is important. By definition $\partial\lG$ is the Lie algebra of $\Aut\lG$, then there exist some algebras $\lG$ and $\lG'$ with $\Aut\lG\neq\Aut\lG'$ but with $\partial\lG=\partial\lG'$, because the equality of two Lie algebras doesn't implies the equality of the groups. The case of $\Int\lG$ and $\ad\lG$ is very different: the group is defined from the algebra, so that $\ad\lG=\ad\lG'$ implies $\Int\lG=\Int\lG'$ and $\Int\lG=\Int\lG'$ if and only if $\ad\lG=\ad\lG'$.

A result about the group of inner automorphism which will be useful later:

\begin{lemma}\label{lem:Int_g_gR}
If $\lG$ is a complex semisimple Lie algebra, then $\Int\lG=\Int\lG\heR$.
\end{lemma}

\begin{proof}
If $\{X_i\}$ is a basis of $\lG$, then $\{X_j,iX_j\}$ is a basis of $\lG\heR$. We define $\dpt{\psi}{\ad\lG}{\ad\lG\heR}$ by
\[
   \psi(\ad(a^jX_j))=\ad(a^jX_j).
\]
It is clearly surjective. On the other hand, if $\ad(a^jX_j)\ad(b^kX_k)$ as elements of $\ad\lG\heR$, then they are equals as elements of $\ad\lG$. The discussion following equations \eqref{eq:schem_ad_int} finishes the proof.
\end{proof}

\begin{corollary}
Any two real compact form of a complex semisimple Lie algebra are conjugate by an inner automorphism.
\end{corollary}

\begin{proof}
    We know that any real form of $\lG$ induces an involution (the conjugation) and that if the real form is compact, the involution is Cartan on $\lG\heR$. Let $\lU_0$ and $\lU_1$ be two compact real forms of $\lG$ and $\tau_0$, $\tau_1$ the associated involutions of $\lG$ (which are Cartan involutions of $\lG\heR$). For a suitable $\varphi\in\Int\lG\heR$,
    \[
       \tau_0=\varphi\tau_1\varphi^{-1}.
    \]
    The fact that $\Int\lG=\Int\lG\heR$ (lemma~\ref{lem:Int_g_gR}) finishes the proof.
\end{proof}

\begin{proposition}
 The group $\Int(\lA)$ is a normal subgroup of $\Aut(\lA)$.
\end{proposition}

\begin{proof}
Let us consider a $s\in\Aut(\lA)$. The map $\dpt{\sigma_s}{\Aut(\lA)}{\Aut(\lA)}$, $\sigma_s(g)=sgs^{-1}$ is an automorphism of $\Aut(\lA)$. Indeed, consider $g$, $h\in\AutA$; direct computations show that $\sigma_s(gh)=\sigma_s(g)\sigma_s(h)$ and $[\sigma_s(g),\sigma_s(h)]=\sigma_s([g,h])$. From this, $(d\sigma_s)_e$ is an automorphism of $\partial(\lA)$, the Lie algebra of $\AutA$. For any $D\in\partial(\lA)$ we have
\begin{equation}\label{eq:ad_s_2}
 (d\sigma_s)_eD=\Dsdd{ sD(t)s^{-1} }{t}{0}
             =sDs^{-1}.
\end{equation}
Since $s$ is an automorphism of $\lA$ and $\ad(\lA)$, a subalgebra of $\gl(\lA)$,
\begin{equation}\label{eq:ad_s_1}
  s\ad Xs^{-1}=\ad(sX)
\end{equation}
for any $X\in\lA$, $s\in\Aut(\lA)$. Since $\ad(\lA)\subset\partial(\lA)$, we can write \eqref{eq:ad_s_2} with $D=\ad X$ and put it in \eqref{eq:ad_s_1}:
\[
   (d\sigma)_e\ad X=s\ad Xs^{-1}=\ad(s\cdot X).
\]
We know from general theory of linear operators on vector spaces that if $A,B$ are endomorphism of a vector space and if $A^{-1}$ exists, then $Ae^BA^{-1}=e^{ABA^{-1}}$. We write it with $A=s$ and $B=\ad X$:
\[
  \sigma_s\cdot e^{\ad X}=se^{\ad X}s^{-1}=e^{s\ad Xs^{-1}}=e^{\ad(s\cdot X)},
\]
sot that
\begin{equation}\label{eq:sigma_aut_s}
  \sigma_s\cdot e^{\ad X}=e^{\ad(s X)}.
\end{equation}

Ont the other hand, we know that $\IntA$ is connected, so it is generated by elements of the form $e^{\ad X}$ for $X\in\lA$. Then $\IntA$ is a normal subgroup of $\AutA$; the automorphism $s$ of $\lA$ induces the isomorphism $g\to sgs^{-1}$ in $\IntA$ because of equation \eqref{eq:sigma_aut_s}.
\end{proof}

More generally, if $s$ is an isomorphism from a Lie algebra $\lA$ to a Lie algebra $\lB$, then the map $g\to sgs^{-1}$ is an isomorphism between $\AutA$ and $\AutB$ which sends $\IntA$ to $\IntB$. Indeed, consider an isomorphism $\dpt{s}{\lA}{\lB}$ and $g\in\AutA$. If $g\in\IntA$, we have to see that $sgs^{-1}\in\IntB$. By definition, $\IntA$ is the analytic subgroup of $\GL(\lA)$ which has $\ad(\lA)$ as Lie algebra. We have $g=e^{\ad A}$, then $sgs^{-1}=e^{\ad(sA)}$ which lies well in $\IntB$.

\section{Fundamental vector field}\label{sec:fond_vec}
%++++++++++++++++++++++++++++++++++++

\begin{definition}
    If $\yG$ is the Lie algebra\footnote{Lie algebra of a Lie group, definition \ref{DEFooKDCPooZOJsMD}.} of a Lie group $G$ acting on a manifold $M$ (the action of $g$ on $x$ being denoted by $x\cdot g$), the \defe{fundamental vector field}{fundamental!vector field} associated with $A\in\yG$ is given by
    \begin{equation}			\label{EqDefChmpFond}
       A^*_x=\Dsdd{ x\cdot e^{-tA} }{t}{0}.
    \end{equation}
\end{definition}

If the action of $G$ is transitive, the fundamental vectors at point $x\in M$ form a basis of $T_xM$. More precisely, we have the

\begin{lemma}
For any $v\in T_xM$, there exists a $A\in\yG$ such that $v=A^*_x$, in other terms
\[
  \Span\{ A^*_{x}\tq A\in\yG \}=T_{x}M.
\]
\label{LemFundSpansTan}
\end{lemma}

\begin{proof}
The vector $v$ is given by a path $v(t)$ in $M$. Since the action is transitive, one can write $v(t)=x\cdot c(t)$ for a certain path $c$ in $G$ which fulfills $c(0)=e$. We have to show that $v$ depends only on $c'(0)\in\yG$. We consider
\begin{equation}  \label{eq_def_RGM}
\begin{aligned}
 R\colon G\times M&\to M \\
R(g,x)&= x\cdot g,
\end{aligned}
\end{equation}
so
\begin{equation}\label{eq:v_Rc}
   v=\Dsdd{ R(c(t),x) }{t}{0}=dR_{(e,x)}\big[  (d_tc(t),x)+(c(0),x)   \big].
\end{equation}

\end{proof}

\begin{lemma}\label{lem:As_Bs_A_B}
If $A$, $B\in\yG$ are such that $A^*=B^*$, and if the action is effective, then $A=B$.
\end{lemma}

\begin{proof}
 We consider once again the map \eqref{eq_def_RGM} and we look at
\[
  v=\Dsdd{ R(c(t),x) }{t}{0}
   =(dR)_{(e,x)}\Dsdd{ (c(t),x) }{t}{0},
\]
keeping in mind that $c(t)=e^{-tA}$. In order to treat this expression, we define
\begin{subequations}
\begin{align}
  \dpt{R_1}{G}{M},\quad  R_1(h)&=R(h,x),\\
  \dpt{R_2}{M}{M},\quad  R_2(y)&=R(g,y).
\end{align}
\end{subequations}
So
\[
  v=dR_1(X)+dR_2(0)=dR_1c'(0)
\]
and the assumption $A^*_x=B^*_x$ becomes $dR_1 A=dR_1 B$. This makes, for small enough $t$, 
\begin{equation}
    R_1(e^{tA}e^{-tB})=x\cdot e^{tA}e^{-tB}=x; 
\end{equation}
if the action is effective, it imposes $A=B$.
\end{proof}

\begin{lemma}
If we consider the action of a matrix group, $R_g$ acts on the fundamental field by
\[
  dR_g(A^*_{\xi})=\big( \Ad(g^{-1})A \big)^*_{\xi\cdot g}.
\]
\label{lem:dRgAstar}
\end{lemma}

\begin{proof}
Just notice that $e^{-t\Ad(g^{-1})A}=\AD_{g^{-1}}(e^{-tA})=g^{-1} e^{-tA}g$, thus
\begin{equation}
  \big( \Ad(g^{-1})A \big)^*_{\xi\cdot g}=\Dsdd{ \xi\cdot ge^{-t\Ad(g^{-1})A} }{t}{0}=dR_g(A^*_{\xi}).
\end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exponential map}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    A \defe{topological group}{topological!group} is a group $G$ equipped with a topological structure such that the maps $(x,y)\in G^2\to xy\in G$ and $x\in G\to x^{-1}\in G$ are continuous.
\end{definition}

\begin{remark}\label{rem:ouvert}
From the existence of an unique inverse for any element of $G$, the multiplication and the inversion are also open maps.
\end{remark}

\begin{definition}
    A \defe{Lie group}{Lie!group} is a group $G$ which is in the same times an manifold such that the group operations (multiplication and inverse) are smooth.

    A Lie group is \defe{analytic}{analytic Lie group} if the manifold is analytic and the group operations are analytic.
\end{definition}

\begin{probleme}
    It seems to me that every smooth Lie group is in fact an analytic Lie group \cite{BIBooIMFVooPoybEp,BIBooTKQTooGjFxwB}. I'm not sure of that, I have not a precise statement. So from now on, we suppose that every Lie group is analytic.

    The reason is that we need power expansions of the exponential map.
\end{probleme}

\subsection{Invariant vector fields}\index{invariant!vector field}
%-----------------------------------

\begin{definition}[\cite{BIBooUGWHooPbodCu}]
    If $G$ is a Lie group, a vector field $X\in\Gamma^{\infty}(TG)$ is \defe{left invariant}{left invariant!vector field} if
    \begin{equation}
        (dL_g) X= X,
    \end{equation}
    which means that for every \( g,h\in G\),
    \begin{equation}
        (dL_h)_gX_g=X_{hg}.
    \end{equation}
    In the same way, the vector field \( Y\) is \defe{right invariant}{right!invariant!vector field} if
    \begin{equation}
        (dR_g)Y=Y.
    \end{equation}
\end{definition}

When \( X\in T_eG\), we define the associated left-invariant vector field \( X^L\) by
\begin{equation}        \label{DEFooYPUIooAzcdjP}
    X^L_g=(dL_g)_eX.
\end{equation}

\begin{theorem}[\cite{BIBooUGWHooPbodCu}]
	The map \( \varphi\colon X\mapsto X^L\) where \( X^L_g=(dL_g)_eX\) is a bijection from \( T_eG\) to the set of left-invariant vector fields.
\end{theorem}

\begin{proof}
    Two parts.
    \begin{subproof}
        \item[Surjective]
            Let \( X\) be a left-invariant vector field. We have \( X=(X_e)^L\) because
            \begin{equation}
                (X_e)^L_g=(dL_g)X_e=X_g.
            \end{equation}
            The first equality is the definition of the left-invariant associated vector field (equation \eqref{DEFooYPUIooAzcdjP} applied to \( X_e\)) and the second equality is the fact that \( X\) is left-invariant. Thus \( X\) is the left-invariant vector field associated with \( X_e\).
        \item[Injective]
            Let \( X,Y\in T_eG\) be such that \( X^L=Y^L\). In particular \( X^L_e=Y^L_e\), which means \( X=Y\).
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooUGWHooPbodCu, MonCerveau}]
    Let \( G\) be a Lie group. The map
    \begin{equation}
        \begin{aligned}
            \varphi\colon G\times \lG&\to TG \\
            (g,X)&\mapsto X^L_g 
        \end{aligned}
    \end{equation}
    is a bijection.

    Moreover for each \( g\in G\), the map
    \begin{equation}
        \begin{aligned}
            \varphi_g\colon \lG&\to T_gG \\
           X&\mapsto X^L_g 
        \end{aligned}
    \end{equation}
    is a vector space isomorphism.
\end{proposition}

\begin{proof}
    Several points.
    \begin{subproof}
        \item[\( \varphi\) is surjective]
            Let \( X\in TG\); there is some \( g\in G\) such that \( X\in T_gG\). Since \( X=(dL_g)_e(dL_{g^{-1}})_gX\) we have
            \begin{equation}
                X=(dL_{g^{-1}}X)^L_g=\varphi(g,dL_{g^{-1}}X).
            \end{equation}
        \item[\( \varphi\) is injective]
            If \( \varphi(g,X)=\varphi(h,Y)\), we have \( X_g^L=Y^L_h\), so that \( g=h\). The equality  \( X_g^L=Y_g^L\) means \( (dL_g)_eX=(dL_g)_eY\). Applying \( (dL_{g^{-1}})_g\) on both sides we get \( X=Y\).
        \item[\( \varphi_g\) is bijective]
            These are the same verifications.
        \item[\( \varphi_g\) is linear]
            The map \( \varphi_g\) is nothing else than \( (dL_g)_e\), so it is linear.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Flow and exponential}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PROPooUXFQooIwimav}
    Let \( \Phi\) be the flow of the left-invariant vector field \( X\). We have
    \begin{equation}
        \Phi(t,g)=g\Phi(t,e).
    \end{equation}
\end{proposition}

\begin{proposition}     \label{PROPooZHBOooGTLXsi}
    Let \( G\) be a Lie group, \( \lG\) its Lie algebra\footnote{Definition \ref{DEFooKDCPooZOJsMD}.} and \( X\in\lG\). 
    \begin{enumerate}
        \item
            There exists a unique \(  C^{\infty}\) group homomorphism \( h_X\colon (\eR,+)\to G\) such that \( \Dsdd{ h_x(t) }{t}{0}=X\).
        \item
            The path \( h_X\) is the maximal integral curve of \( X^L\) and \( X^R\) for the initial condition \( h_X(0)=e\).
        \item
            The flows of \( X^L\) and \( X^R\) are defined on \( \eR\).
    \end{enumerate}
\end{proposition}

\begin{definition}
    If \( G\) is a Lie group with algebra \( \lG\), we define the \defe{exponential}{exponential from a Lie algebra} is the map
    \begin{equation}
        \begin{aligned}
            \exp\colon \lG&\to G \\
            X&\mapsto h_X(1) 
        \end{aligned}
    \end{equation}
    where \( h_X\) is the homomorphism defined by the proposition \ref{PROPooZHBOooGTLXsi}. We often write \(  e^{X} \) for \( \exp(X)\).
\end{definition}


The following proposition is a generalization of \ref{PROPooKDKDooCUpGzE}.
\begin{proposition}     \label{PROPooNRVJooEDCpOI}
    If \( X\in \lG\) and \( s,t\in \eR\) we have
    \begin{equation}
        e^{sX} e^{tX}= e^{(s+t)X}.
    \end{equation}
\end{proposition}

\begin{lemma}       \label{LEMooRPHVooAtZJnz}
    Let \( G\) be a Lie group with algebra \( \lG\). If \( n\in \eN\) we have
    \begin{equation}
        \exp(X)^n=\exp(nX).
    \end{equation}
\end{lemma}

\begin{proof}
    Apply \( n\) times the proposition \ref{PROPooNRVJooEDCpOI}.
\end{proof}

\begin{lemma}       \label{LEMooLMTZooCvunSl}
    Let \( G\) be a Lie group and \( X\in G\). We have
    \begin{equation}        \label{EQooNBENooPXLENs}
        X^R_g=\Dsdd{  e^{tX}g }{t}{0}
    \end{equation}
    and
    \begin{equation}
        X^L_g=\Dsdd{  ge^{tX} }{t}{0}
    \end{equation}
\end{lemma}

\begin{normaltext}      \label{NORMooSATDooIhwXXr}
    We will often write the relation \eqref{EQooNBENooPXLENs} under the form
    \begin{equation}
        X^R_g(t)= e^{tX}g.
    \end{equation}
    This is a way to implies that \( t\mapsto  e^{tX}g\) is a path for the vector \( X^R_g\). It is a common abuse of notation to write the vector and a path representing the vector with the same symbol.
\end{normaltext}

\begin{proposition}     \label{PROPooYFZZooLUOuOj}
    Let \( G\) be a Lie group. There exists a neighbourhood \( U\) of \( 0\) in \( \lG\) and a neighbourhood \( V\) of \( e\) in \( G\) such that
    \begin{equation}
        \exp\colon U\to V
    \end{equation}
    is a \(  C^{\infty}\) diffeomorphism\footnote{\( \exp\) is \(  C^{\infty}\), invertible and he inverse is \(  C^{\infty}\) as well.}.
\end{proposition}

\begin{proposition}     \label{PROPooAICDooQcmPZB}
    Let \( G\) be an analytic Lie group. There exists a neighbourhood \( U\) of \( 0\) in \( \lG\) and a neighbourhood \( V\) of \( e\) in \( G\) such that
    \begin{equation}
        \exp\colon U\to V
    \end{equation}
    is an analytic diffeomorphism\footnote{\( \exp\) is analytic, invertible and he inverse is analytic too.}.
\end{proposition}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Invariant vector and derivation}
%---------------------------------------------------------------------------------------------------------------------------

You may want to know how the exponential can be used to write some formulas linking left-invariant vector field and derivation of functions. Here you are.

\begin{normaltext}
    Let \( X\in \lG\), \( g\in G\) and \( u\in \eR\). Let \( f\colon G\to \eR\) be a smooth function. Using the abuse of notation described in \ref{NORMooSATDooIhwXXr} and the proposition \ref{PROPooNRVJooEDCpOI},
    \begin{subequations}
        \begin{align}
            (X^Lf)(g e^{uX})&=\Dsdd{ f\big( X^L_{g e^{uX}}(t) \big) }{t}{0}\\
            &=\Dsdd{ f\big( g e^{uX} e^{tX} \big) }{t}{0}\\
            &=\Dsdd{ f\big( g e^{(t+u)X)} \big)}{t}{0}\\
            &=\Dsdd{ f(g e^{tX}) }{t}{u}.
        \end{align}
    \end{subequations}
    The formula
    \begin{equation}
        (X^Lf)(g e^{uX})=\Dsdd{ f(g e^{tX}) }{t}{u}
    \end{equation}
    means that \( X^L\) derives \( f\) in the direction of the path \(  e^{tX}\) at right.
\end{normaltext}

\begin{normaltext}
    By the way, we recall that, if \( f\) is a function and \( X\) a vector field, \( (Xf)\) is a new function, given by
    \begin{equation}
        (Xf)(a)=X_a(f).
    \end{equation}
    In that sense we can write combinations like \( XYf\) or \( (X^2+X)f\) where \( X\) and \( Y\) are vector fields.
\end{normaltext}

\begin{proposition}[\cite{BIBooPBAMooNcYhCM}]       \label{PROPooKSIDooVIFkiM}
    Let \( G\) be a Lie group with Lie algebra \( \lG\). We consider \( X,Y\in \lG\) and a smooth function \( f\colon G\to \eR\). We have\quext{My source \cite{BIBooPBAMooNcYhCM} seems to write \( (X^R)^n(Y^R)^m\) instead of \( (X^R)^n(Y^L)^m\). Let me know where I'm wrong.}
    \begin{equation}
        \big( (X^R)^n(Y^L)^mf \big)( e^{sX} e^{tY})=\frac{ d^n }{ du^n }\frac{ d^m }{ dv^m }\Big( f( e^{uX} e^{vY}) \Big)_{\substack{u=s\\v=t}}.
    \end{equation}
\end{proposition}

\begin{proof}
    We have to do a proof by induction on \( (n,m)\). We start with \( (n,m)=(0,0)\) and we prove the steps \( (n,m)\to (n+1,m)\) and \( (n,m)\to (n,m+1)\).

    \begin{subproof}
        \item[\( (0,0)\)]
            With \( (n,m)=(0,0)\) we are okay.
        \item[\( (n+1,m)\)]
            We have
            \begin{equation}
                \Big( (X^R)^{n+1}(Y^L)^mf \Big)( e^{sX} e^{tY})=\big( (X^R)(X^R)^n(Y^L)^mf \big)( e^{sX} e^{tY}).
            \end{equation}
            We will apply the induction hypothesis on the function \( (X^R)^n(Y^L)^mf\), but in a first time we just apply the vector field \( X^R\) to the function \( (X^R)^n(Y^L)^m\) and we evaluate at \(  e^{sX} e^{tY}\). Here is a couple of computations:
            \begin{subequations}
                \begin{align}
                    \Big( (X^R)(X^R)^n(Y^L)^mf \Big)( e^{sX} e^{tY})&=\Dsdd{  \Big( (X^R)^n(Y^L)^mf \Big)\big( X^R_{ e^{sX} e^{tY}}(u) \big)  }{u}{0}\\
                    &=\Dsdd{  \Big( (X^R)^n(Y^L)^mf \Big)(  e^{uX} e^{sX} e^{tY} )  }{u}{0}\\
                    &=\Dsdd{  \Big( (X^R)^n(Y^L)^mf \Big)( e^{uX} e^{tY})  }{u}{s}.
                \end{align}
            \end{subequations}
            At this point we use the induction hypothesis:
            \begin{subequations}
                \begin{align}
                    \Dsdd{  \Big( (X^R)^n(Y^L)^mf \Big)( e^{uX} e^{tY})  }{u}{s}&=\frac{ d }{ du }\left( \frac{ d^n }{ dw^n }\frac{ d^m }{ dv^m }\big( f( e^{wX} e^{vY}) \big)_{\substack{w=u\\v=t}}  \right)_{u=s}\\
                    &=\frac{ d^{n+1} }{ dw^{n+1} }\frac{ d^m }{ dv^m }\left( f( e^{wX} e^{vY}) \right)_{\substack{w=s\\v=t}}.
                \end{align}
            \end{subequations}
        \item[\( (n,m+1)\)]
            Same kind of computations.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Analytic Lie group, Taylor formula}
%---------------------------------------------------------------------------------------------------------------------------

In this subsection we study the analytic functions over an analytic Lie group.

\begin{lemma}[\cite{BIBooPBAMooNcYhCM}]     \label{LEMooPILVooHQbtAH}
    Let \( G\) be an analytic Lie group. We consider an analytic function \( f\colon G\to \eR\), an element \( X\in \lG\), a basis \( \{ X_i \}\) of \( \lG\) and \( g\in G\). There exists an absolutely converging power series \( P\) such that
    \begin{equation}
        f(g e^{x_1X_1+\ldots +x_nX_n})=P(x_1,\ldots, x_n).
    \end{equation}
\end{lemma}

\begin{proof}
    First we make the proof for \( g=e\).

    We consider a basis \( \{ e_i \}\) of \( \lG\). Let \( U\) be a neighbourhood of \( 0\) in \( \lG\) and \( V\) a neighbourhood of \( e\) in \( G\) such that \( \exp\colon U\to V\) is an analytic diffeomorphism\footnote{By proposition \ref{PROPooAICDooQcmPZB}.}.

    We consider \( U'\), the open set in \( \eR^n\) which correspond to \( U\) via the basis \( \{ e_i \}\). The map
    \begin{equation}
        \begin{aligned}
            \varphi\colon U'&\to V \\
            (x_1,\ldots, x_n)&\mapsto \exp(x_1e_1+\ldots+x_ne_n)
        \end{aligned}
    \end{equation}
    is analytic chart of \( V\).

    The fact that \( f\) is analytic means that the composition of \( f\) with the charts are analytic. In our case, the map \( \tilde f =f\circ\varphi\) is analytic from \( U'\subset \eR^n\) to \( \eR\). Thus there exists an absolutely converging power series \( P\) such that
    \begin{equation}
        \tilde f(x_1,\ldots, x_n)=P(x_1,\ldots, x_n).
    \end{equation}
    We conclude:
    \begin{equation}
        f\big( \exp(x_1e_1+\ldots +x_ne_n) \big)=f\big( \varphi(x_1,\ldots, x_n) \big)=P(x_1,\ldots, x_n).
    \end{equation}
    
    If \( g\) is not \( e\), we consider the neighbourhood \( gV\) and the map
    \begin{equation}
        \begin{aligned}
            \varphi\colon U&\to gV \\
            (x_1,\ldots, x_n)&\mapsto g\exp(x_1e_1+\ldots +x_ne_n)
        \end{aligned}
    \end{equation}
    is a chart, so that
    \begin{equation}
        f(g e^{x_1e_1+\ldots +x_ne_n})=\tilde f(x_1,\ldots, x_n)
    \end{equation}
    which is a power series.
\end{proof}

\begin{proposition}[Taylor formula\cite{BIBooPBAMooNcYhCM}]     \label{PROPooIYWQooZJtKiu}
    Let \( G\) be an analytic Lie group. We suppose that \( f\colon G\to \eR\) is an analytic functions. For \( g\in G\) and \( X\in \lG\) we have
    \begin{equation}
        f(g e^{X})=\sum_{k=0}^{\infty}\frac{1}{ n! }\big( (X^R)^nf \big)(g).
    \end{equation}
\end{proposition}

\begin{proof}
    We know from proposition \ref{LEMooPILVooHQbtAH} that \( f(g e^{X})=P(x_1,\ldots, x_n)\) for some power series \( P\). We consider a neighbourhood \( U\) of \( 0\) in \( \lG\) and \( V\) of \( g\) in \( G\) such that
    \begin{equation}
        \begin{aligned}
            \varphi\colon U&\to V \\
            X&\mapsto  ge^{X} 
        \end{aligned}
    \end{equation}
is an analytic diffeomorphism (i.e. an analytic chart for \( G\) around \( g\)). Let \( X\in U\) and \( \delta\) such that \( tX\in U\) for all \( t\in \mathopen] -\delta , \delta \mathclose[\). Notice that \( \delta>1\). Now, \( X\) being fixed, the value of \( P(tx_1,\ldots, tx_n)\) is an absolutely convergent power series of \( t\). We have
    \begin{equation}
        f(g e^{tX})=P(tx_1,\ldots, tx_n)=\sum_{k=0}^{\infty}\frac{ a_m }{ m! }t^m
    \end{equation}
    for some constants \( a_m\in \eR\).

    But considering the function
    \begin{equation}
        \begin{aligned}
            r\colon \eR&\to \eR \\
            t&\mapsto f(g e^{tX}), 
        \end{aligned}
    \end{equation}
    there is an unicity of its power series expansion; thus \( a_m\) is the \( m\)-th derivative of \( r\) at \( t=0\).

    But we also know from proposition \ref{PROPooKSIDooVIFkiM} that
    \begin{equation}
        \big( (X^L)^mf \big)(g e^{tX})=\frac{ d^m }{ du^m }\big( f(g e^{uX}) \big)_{u=t};
    \end{equation}
    taking that at \( t=0\) we have
    \begin{equation}
        a_m=\big( (X^L)^mf \big)(g)
    \end{equation}
    and the Taylor formula
    \begin{equation}
        f(g e^{tX})=\sum_{k=0}^{\infty}\frac{1}{ k! }\frac{ d^k }{ du^k }\big( f(g e^{uX}) \big)_{u=0}t^m.
    \end{equation}
    Finally taking \( t=1\) (recall that \( \delta>1\), so it is valid):
    \begin{equation}
        f(g e^{X})=\sum_{k=0}^{\infty}\frac{1}{ k! }\frac{1}{ k! }\big( (X^L)^kf \big)(g).
    \end{equation}
\end{proof}

\begin{lemma}       \label{LEMooWKFIooRHsrFX}
    Let \( G\) be an analytic Lie group with algebra \( \lG\). We consider a basis \( \{ e_i \}_{i=1,\ldots, n}\) of \( \lG\) and the functions
    \begin{equation}
        \begin{aligned}
            f_i\colon U&\to \eR \\
            \exp(x_1e_1+\ldots+x_ne_n)&\mapsto x_i 
        \end{aligned}
    \end{equation}
    defined on a normal neighbourhood \( U\) of \( e\).
    
    If \( X,Y\in \lG\) satisfy
    \begin{equation}
        Xf_i=Yf_i
    \end{equation}
    for every \( i\), then \( X=Y\).
\end{lemma}

\begin{proof}
    If \( X=\sum_kX_ke_k\) we have
    \begin{equation}
        X(f_i)=\Dsdd{ f_i( e^{tX}) }{t}{0}=\Dsdd{ f_i\big(  e^{t\sum_kX_ke_k} \big) }{t}{0}=\Dsdd{ tX_i }{t}{0}=X_i.
    \end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooPBAMooNcYhCM}]     \label{LEMooMJBRooMOuJpa}
    Let \( G\) be an analytic Lie group with Lie algebra \( \lG\). For \( X,Y\in \lG\) we have:
    \begin{enumerate}
        \item       \label{ITEMooHVOIooKDrUSw}
            \( \exp(tX)\exp(tY)=\exp\big( t(X+Y)+\frac{ t^2 }{2}[X,Y]+t^2\alpha(t) \big)\),
        \item       \label{ITEMooWIQIooHphJcP}
            \( \exp\big( t(X+Y) \big)=\exp(tX)\exp(tY)\exp(t\alpha(t))\)
        \item       \label{ITEMooVMDCooExpIrp}
            \( \exp(-tX)\exp(-tY)\exp(tX)\exp(tY)=\exp\big( t^2[X,Y]+t^3\alpha(t) \big)\).
    \end{enumerate}
    In both formulas, \( \alpha\) is a function \( \alpha\colon \eR\to \lG\) satisfying \( \lim_{t\to 0} \alpha(t)=0\).
\end{lemma}

\begin{proof}
    Several steps.
    \begin{subproof}
        \item[A good function]
        
            Let \( \{ e_i \}_{i=1,\ldots, n}\) be a basis of \( \lG\). We consider a neighbourhood \( U\) of \( 0\) in \( \lG\) and \( V\) of \( e\) in \( G\) such that \( \exp\colon U\to V\) is an analytic diffeomorphism. On that \( U\) we consider the function
            \begin{equation}
                \begin{aligned}
                    f\colon U&\to \eR \\
                    \exp(x_1e_1+\ldots +x_ne_n)&\mapsto x_i 
                \end{aligned}
            \end{equation}
            for some fixed \( i\). This function is analytic and satisfies \( f(e)=0\). 
        \item[Some Taylor expansions] 
            Using proposition \ref{PROPooKSIDooVIFkiM} we have
            \begin{equation}
                \big( (X^R)^n(X^L)^mf \big)( e^{sX} e^{tY})=\frac{ d^n }{ du^n }\frac{ d^m }{ dv^m }\big( f( e^{uX} e^{vY}) \big)_{\substack{u=s\\v=t}}.
            \end{equation}
            Considering the function \( q(s,t)=f( e^{sX} e^{tY})\), we have the Taylor expansion
            \begin{equation}        \label{EQooNBOIooRxlZmP}
                f( e^{sX} e^{tY})=q(s,t)=\sum_{m,n\geq 0}\frac{ s^n }{ n! }\frac{ t^m }{ m! }\big( (X^R)^n(Y^L)^mf \big)(e)=\sum_{m,n\geq 0}\frac{ s^n }{ n! }\frac{ t^m }{ m! }\big( X^nY^mf \big)(e).
            \end{equation}
            Here the second equality is due to the fact that \( (X^Lf)(e)=(X^Rf)(e)=X(f)\).

        \item[The function \( Z\)]

            On the other hand, when \( t\) is small enough, the element \(  e^{tX} e^{tY}\) belongs to a normal neighbourhood of \( e\), so that there exists an element \( Z(t)\in \lG\) satisfying
            \begin{equation}
                e^{tX} e^{tY}= e^{Z(t)}.
            \end{equation}
            The element \( Z(t)\) is given by
            \begin{equation}
                Z(t)=\exp^{-1}\big(  e^{tX} e^{tY} \big).
            \end{equation}
            Since the exponential is an analytic diffeomorphism\footnote{Proposition \ref{PROPooAICDooQcmPZB}.} (the inverse is analytic), \( Z\) is an analytic function around \( t=0\). Thus there exists a function \( \alpha\colon \eR\to \lG\) such that
            \begin{equation}        \label{EQooRPGGooXtZzFy}
                Z(t)=tZ_1+t^2Z_2+t^2\alpha(t)
            \end{equation}
            and \( \lim_{t\to 0} \alpha(t)=0\). Notice that \( Z(0)=0\), which explain the absence of constant term in \eqref{EQooRPGGooXtZzFy}.

        \item[A formula for \( f\big(  e^{Z(t)} \big)\)]

            We pose \( Z_1=\sum_ka_{1k}e_k\), \( Z_2=\sum_ka_{2k}e_k\) and \( \alpha(t)=\sum_k\sigma_k(t)e_k\), so that
            \begin{equation}
                Z(t)=\sum_k\big( ta_{1k}+t^2a_{2k}+t^2\alpha_k(t) \big)e_k.
            \end{equation}
            Applying \( f\) we have
            \begin{equation}
                f\big(  e^{Z(t)} \big)=ta_{1i}+t^2a_{2i}+t^2\alpha_i(t)=f\big(  e^{tZ_1+t^2Z_2} \big)+t^2\alpha_i(t).
            \end{equation}
            
        \item[Some more Taylor expansions]

            We use the Taylor expansion of proposition \ref{PROPooIYWQooZJtKiu} with \( g=e\) and \( X=Z(t)\):
            \begin{equation}        \label{EQooSFKOooDAavVy}
                f( e^{Z(t)})=\sum_k\frac{1}{ k! }\big( [tZ^L_1+t^2Z_2^L]^kf \big)(e)+t^2\alpha_i(t).
            \end{equation}
            Once again we can drop the \( L\) exponent since \( (X^Lf)(e)=X(f)\). We collect out of \eqref{EQooSFKOooDAavVy} the terms with \( t\) and \( t^2\):
            \begin{equation}        \label{EQooEYUSooTDntym}
                f( e^{tX} e^{tY})=f( e^{Z(t)})=tZ_1(f)+t^2 Z_2 +\frac{ t^2 }{2}Z_1^2 +t^2\beta(t)
            \end{equation}
            with \( \lim_{t\to 0} \beta(t)=0\).

        \item[Comparison]

            The formulas \eqref{EQooNBOIooRxlZmP} with \( s=t\) and \eqref{EQooEYUSooTDntym} are Taylor expansions of the same quantity. They are equal; we copy them here:
            \begin{equation}
                \sum_{m,n}\frac{ t^{m+n} }{ m!n! }\big( (X^R)^n(Y^L)^mf \big)(e)=tZ_1(f)+t^2 Z_2 +\frac{ t^2 }{2}Z_1^2 +t^2\beta(t)
            \end{equation}
            On the left hand side, the terms with \( t\) and \( t^2\) are obtained when \( (n,m)\) is among the possibilities $(0,1)$, $(1,0)$, $(2,0)$, $(0,2)$, and $(1,1)$. Collecting we have on the left
            \begin{equation}
                (X+Y)f+XYf+\frac{ 1 }{2}X^2f+\frac{ 1 }{2}Y^2f
            \end{equation}
            where we used the fact that \( \big( (X^R)^2f \big)(e)=X(Xf)=X^2f\).

            Using lemma \ref{LEMooWKFIooRHsrFX} we have \( Z_1f=(X+Y)f\), so that \( Z_1=X+Y\) and then
            \begin{equation}
                \frac{ 1 }{2}[X,Y]=Z_2.
            \end{equation}
    \end{subproof}
    At this point we proved that
    \begin{equation}
        e^{tX} e^{tY}= e^{t(X+Y)+\frac{ t^2 }{2}[X,Y]+t^2\alpha(t)}.
    \end{equation}
    This is \ref{ITEMooHVOIooKDrUSw}.

    For point \ref{ITEMooWIQIooHphJcP}, we are searching for a function \( \beta\) such that 
    \begin{equation}
        e^{tX} e^{tY} e^{t\beta(t)}= e^{t(X+Y)}.
    \end{equation}
    We replace in the left-hand side the value of \(  e^{tX} e^{tY}\) given by the point \ref{ITEMooHVOIooKDrUSw} (this is the reason why we write \( \beta\) instead of \( \alpha\)) and we isolate \(  e^{t\beta(t)}\):
    \begin{equation}        \label{EQooLTMBooVIChyC}
        e^{t\beta(t)}= e^{t(X+Y)} e^{-t(X+Y)-t^2[X,Y]/2-t^2\alpha(t)}.
    \end{equation}
    So now our aim is to show that the right-hand side of \eqref{EQooLTMBooVIChyC} can be written as only one exponential with an argument of the form \( t\beta(t)\) satisfying \( \beta(t)\to 0\). For that, we use \ref{ITEMooHVOIooKDrUSw} once again with \( X+Y\) instead of \( X\) and \( -(X+Y)-t[X,Y]/2-t\alpha(t)\) instead of \( Y\). What we get is
    \begin{subequations}
        \begin{align}
            e^{t\beta(t)}&=\exp\big( t(-t[X,Y]/3-t\alpha(t))+\frac{ t^2 }{2}\big[ X+Y,-(X+Y)-t[X,Y]/2-t\alpha(t) \big] \big)\\
            &=\exp\big( -\frac{ t^2 }{2}[X,Y]  -t^2\alpha(t)-\frac{ t^3 }{ 4 }\big[ X+Y,[X,Y] \big]-\frac{ t^3 }{ 2 }\alpha(t)  \big).
        \end{align}
    \end{subequations}
    We are done with \ref{ITEMooWIQIooHphJcP}.

et

    \ref{ITEMooVMDCooExpIrp}

\end{proof}


\begin{lemma}[\cite{BIBooTKQTooGjFxwB}]
    Let \( G\) be an analytic Lie group and \( H\) be a closed subgroup of \( G\). The set
    \begin{equation}
        \lH=\{ X\in \lG\tq  e^{tX}\in H\forall t\in \eR \}
    \end{equation}
    is a vector subspace of \( \lG\).
\end{lemma}

\begin{proof}
    Let \( X,Y\in \lH\), \( \lambda\in \eR\) and \( t\in \eR\). The element \( \lambda X\) belongs to \( \lH\). We have to prove that \( X+Y\in \lH\).

    For every \( n\in \eN\), we have \(  e^{tX/n} e^{tY/n}\in H\) because \( H\) is a subgroup. For every \( n\) we also have
    \begin{equation}
        \big(  e^{tX/n} e^{tY/n} \big)^n\in H.
    \end{equation}
    we have to following computation :
    \begin{subequations}        \label{SUBEQSooMDRVooQXBwiS}
        \begin{align}
            \big(  e^{tX/n} e^{tY/n} \big)^n&=\left[ \exp\left( \frac{ t(X+Y) }{ n }+\frac{ t^2 }{ n^2 }\alpha(t/n) \right)  \right]^n \label{SUBEQooUYNCooJVIWMi}\\
            &=\exp\left( t(X+Y)+\frac{ t^2 }{n  }\alpha(t/n) \right)    \label{SUBEQooUYKKooXtGaxL}
        \end{align}
    \end{subequations}
    Justifications.
    \begin{itemize}
        \item For \eqref{SUBEQooUYNCooJVIWMi}. This is lemma \ref{LEMooMJBRooMOuJpa}\ref{ITEMooWIQIooHphJcP}.
        \item For \eqref{SUBEQooUYKKooXtGaxL}. The \( n\) enters the exponential from lemma \ref{LEMooRPHVooAtZJnz}.
    \end{itemize}
    We have the limit
    \begin{equation}        \label{EQooTJWDooVJsDJt}
        \lim_{n\to \infty} \exp\left( t(X+Y)+\frac{ t^2 }{n  }\alpha(t/n) \right) =  e^{t(X+Y)}
    \end{equation}
    because the exponential is continuous\footnote{See \ref{}.} and the properties of \( \alpha\). Since the limit exists on the right hand side of \eqref{SUBEQSooMDRVooQXBwiS}, the limit exists on the left hand side too.

    The limit
    \begin{equation}
        \lim_{n\to \infty} \big(  e^{tX/n} e^{tY/n} \big)^n
    \end{equation}
    is a limit of elements in \( H\). Since \( H\) is closed, this is an element of \( H\). We deduce that \eqref{EQooTJWDooVJsDJt} is an element of \( H\).
\end{proof}


\begin{lemma}[\cite{BIBooTKQTooGjFxwB}]     \label{LEMooFDIIooCkSJpY}
    Let \( G\) be an analytic Lie group and \( H\) be a closed subgroup of \( G\). We consider the set
    \begin{equation}
        \lH=\{ X\in \lG\tq  e^{tX}\in H\forall t\in \eR \}.
    \end{equation}

    Let \( (X_i)\) be a sequence in \( \lG\) such that\footnote{The topology and the norm on \( \lH\) are given in definition \ref{PROPooHJOXooMGANfd}.}
    \begin{enumerate}
        \item
            \( X_i\to 0\)
        \item
            \(  e^{X_i}\in H\) for every \( i\).
        \item
            The limit \( \lim_{i\to \infty} \frac{ X_i }{ \| X_i \| }\) exists. We name it \( X\).
    \end{enumerate}
    Then \( X\in \lH\).
\end{lemma}
<++>

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Other stuff}
%---------------------------------------------------------------------------------------------------------------------------

The concept of normal neighbourhood will be widely used for the study of the relations between a Lie group and its algebra. Let $M$ be a differentiable manifold. If $V$ is a neighbourhood of zero in $T_pM$ on which the exponential $\dpt{\exp_p}{T_pM}{M}$ is a diffeomorphism, then $\exp_pV$ is  \defe{normal neighbourhood}{normal!neighbourhood} of $p$.

\begin{lemma}
Let $\lG$ be a Lie algebra and $A$, a linear operator on $\lG$ (see as a common vector space) such that $\forall t\in\eR$, the map $e^{tA}$ is an automorphism of $\lG$. Then $A$ is a derivation of $\lG$.
\label{lem:autom_derr}
\end{lemma}

\begin{proof}
Let us consider $X$, $Y\in\lG$;  the assumption is
\[
  e^{tA}[X,Y]=[e^{tA}X,e^{tA}Y].
\]
Since $e^{tA}$ is a linear map, it has a ``good behavior''\ with the derivations:
\[
\Dsddc{e^{tA}[X,Y]}{t}{0}=\Dsddc{e^{tA}}{t}{0}[X,Y]=A[X,Y].
\]
Using on the other hand the linearity of $\ad$, we can see
\[
  (\ad(e^{tA}X))(e^{tA}Y)
\]
as a product ``matrix times vector''. Then
\begin{equation}
\begin{split}
  \Dsddc{[e^{tA}X,e^{tA}Y]}{t}{0}&=\Dsddc{(\ad e^{tA}X)Y}{t}{0}+\Dsddc{(\ad X)(e^{tA}Y)}{t}{0}\\
                                 &=(\ad AX)Y+(\ad X)(AY).
\end{split}
\end{equation}
Finally, $A[X,Y]=[AX,Y]+[X,AY]$.

\end{proof}

As notational convention, if $G$ and $H$ are Lie groups, their Lie algebra are denoted by $\lG$ and $\lH$.

\begin{lemma}		\label{LemAlgEtGroupesGenere}
	Let $\lG$ be a Lie algebra ans $\lS$ be a subset of $\lG$. The algebra of the group generated by $ e^{\lS}$ is the algebra generated by $\lS$.
\end{lemma}

Invariant vector fields are also often used in order to transport a structure from the identity of a Lie group to the whole group by $A_g(X_g)=A_e(dL_{g^{-1}}X_g)$ where $A_e$ is some structure and $X_g$, a vector at $g$.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exponential from the Lie algebra to the Lie group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{Lie}]		\label{lemsur5d}
    Let $G$, $H$ be two Lie groups with algebras\footnote{Lie algebra of a Lie group, definition \ref{DEFooKDCPooZOJsMD}.} $\mG$ and $\mH$. Let $\dpt{\phi}{G}{H}$ be a homomorphism differentiable at $e$, the unit in $G$. Then for all $X\in\mG$, the following formula holds:
	\[
		\phi(\exp X)=\exp(d\phi_eX).
	\]
\end{lemma}

\begin{corollary}\label{Ad_e}
An useful formula:
\[
   \Ad(e^X)=e^{\ad X}.
\]
\end{corollary}

\begin{corollary}
Another useful corollary of lemma~\ref{lemsur5d} is the particular case $\phi=\AD(e^X)$:
\[
   e^Xe^Ye^{-X}=e^{Ad(e^Y)X}.
\]
\label{cor:eXeYe-X}
\end{corollary}

\begin{proposition}
	Let $G$ be a connected Lie group.
	\begin{enumerate}

		\item
			All the left invariant vector fields are complete. That means that the map $X\mapsto  e^{X}$ is defined for every $X\in \mG$.
		\item
			The map $\exp\colon \mG\to G$ is a local diffeomorphism in a neighbourhood of $0$ in $\mG$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	\begin{enumerate}

		\item
			The flow is a one parameter subgroup. Thus if $ e^{tX}$ is defined for $t\in[0,a]$, by composition, $ e^{2a}$ is defined. So $ e^{tX}$ is defined for every value of $t$ in $\eR$.
		\item
			Let us consider the manifold $G\times \mG$ and the vector field $\Xi$ defined by
			\begin{equation}
				\Xi_{(g,X)}=\tilde X_g\oplus 0\in T_g(G)\oplus T_X\mG\simeq T_{(g,X)}(G\times \mG).
			\end{equation}
			The flow of that vector field is given by
			\begin{equation}
				\Phi_t(g,X)=\big( g\exp(tX),X \big).
			\end{equation}
			In particular, $\Xi$ is a complete vector field, and we consider the global diffeomorphism
			\begin{equation}
				\begin{aligned}
					\Phi_1\colon G\times \mG&\to G\times \mG \\
					(g,X)&\mapsto \big( g\exp(X),X \big).
				\end{aligned}
			\end{equation}
			On the point $(e,X)$ we have $\Phi_1(e,X)=(\exp(X),X)$. Thus the exponential is the projection on the first component of $\Phi_1(e,X)$ and we can write
			\begin{equation}
				\exp(X)=\pr_1\circ\Phi_1(e,X).
			\end{equation}
			It is a smooth function since both the projection and $\Phi_1$ are smooth.

			Now, the differential $(d\exp)_0$ is the identity on $\mG$, so that the theorem of inverse function makes $\exp$ a local diffeomorphism.
	\end{enumerate}
\end{proof}


\begin{theorem}
For any $p\in M$, there exist a $\delta>0$ and a neighbourhood $W$ of $p$ in $M$ such that for every $q\in W$, we have

\begin{itemize}
\item $\exp_q$ is a diffeomorphism on $B\bdelta(0)\subset T_qM$,
\item $\exp_q B\bdelta(0)$ contains $W$
\end{itemize}
\end{theorem}
This theorem says that everywhere on a differentiable manifold, one can find a neighbourhood which is a normal neighbourhood of each of its points. Such a neighbourhood is said a \emph{totally} normal neighbourhood.

\begin{lemma}
In a Lie group, $e$ is an isolated fixed point for the inversion.
\end{lemma}

\begin{proof}
One can use an exponential map in a neighbourhood of $e$. In this neighbourhood, an element $g$ can be written as $g=e^X$ for a certain $X\in\lG$. The equality $g=g^{-1}$ gives (because the exponential is a diffeomorphism) $X=-X$, so that $X=0$ and $g=e$.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Properties using the exponential}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}
Let $G$ and $H$ be two Lie groups and $\dpt{\varphi}{G}{H}$ a continuous homomorphism. Then $\varphi$ is analytic.
\end{theorem}

\begin{proof}
The Lie algebra of the product manifold $G\times H$ as $\lG\times\lH$ is given in~\ref{lemLeibnitz}. We define
\begin{equation}
  K=\{(g,\varphi(g)):g\in G\}\subset G\times H.
\end{equation}
It is clear that $K$ is closed in $G\times H$ because $G$ is closed and $\varphi$ is continuous.
By theorem~\ref{tho:diff_sur_ferme}, there exists an unique differentiable structure on $G\times H$ such that $K$ is a topological Lie subgroup of $G\times H$ (i.e.: Lie subgroup + induced topology). The Lie algebra of $K$ is
\begin{equation}
  \lK=\{(X,Y)\in\lG\times\lH:\forall t\in\eR, (e^{tX},e^{tY})\in K\}.
\end{equation}
Let $N_0$ be an open neighbourhood of $0$ in $\lH$ such that $\exp$ is diffeomorphic between $N_0$ and an open neighbourhood $N_e$ of $e$ in $H$. We define $M_0$ and $M_e$ in the same way, for $G$ instead of $H$. We can suppose $\varphi(M_e)\subset N_e$: if it is not, we consider a smaller $M_e$: the openness of $N_e$ and the continuity of $\varphi$ make it coherent.

The lemma~\ref{lem:sugroup_normal} allow us to consider $M_0$ and $N_0$ small enough to say that
\[
   \dpt{\exp}{(M_0\times N_0)\cap\lK}{(M_e\times N_e)\cap K}
\]
is diffeomorphic. Now, we are going to show that for any $X\in\lG$, there exists an unique $Y\in\lH$ such that $(X,Y)\in\lK$. The unicity is easy: consider $(X,Y_1),(X,Y_2)\in\lK$; then $(0,Y_1-Y_2)\in\lK$ (because a Lie algebra is a vector space). Then the definition of $\lK$ makes for any $t\in\eR$, $(e,\exp{t(Y_1-Y_2)})\in K$. Consequently, $\exp t(Y_1-Y_2)=\varphi(e)=e$ and then $Y_1-Y_2=0$.

In order to show the existence, let us consider a $r>0$ such that $X_r=(1/r)X$ keeps in $M_0$. This exists because the sequence $X_r\to 0$ (then it comes $M_0$ from a certain $r$). From the definitions, $\exp$ is diffeomorphic between $M_0$ and $M_e$, then $\exp X_r\in M_e$ and $\varphi(\exp X_r)\in N_e$ because $\varphi(M_e)\subset N_e$.

From this, there exists an unique $Y_r\in N_0$ such that $\exp Y_r=\varphi(\exp X_r)$ and an unique $Z_r\in(M_0\times N_0)\cap\lK$ satisfying  $\exp Z_r=(\exp X_r,\exp Y_r)$. But $\exp$ is bijective from $M_0\times N_0$, so that $Z_r=(X_r,Y_r)$ and we can choose $Y=rY_r$ as a $Y\in\lH$ such that $(X,Y)\in\lK$ (it is not really a choice: the unicity was previously shown). We denotes by $\dpt{\psi}{\lG}{\lH}$ the map which gives the unique $Y\in\lH$ associated with $X\in\lG$ such that $(X,Y)\in\lK$. This is a homomorphism between $\lG$ and $\lH$.

By definition, $(X,\psi(X))\in\lK$, i.e. $(\exp tX,\exp t\psi(X))\in K$ or
\begin{equation}
  \varphi(\exp tX)=\exp t\psi(X).
\end{equation}
Let us now consider a basis $\{X_1,\ldots,X_n\}$ of $\lG$. Since $\varphi$ is a homomorphism,
\begin{equation}\label{eq:coord_vp_exp}
   \varphi\big((\exp t_1X_1)(\exp t_2X_2)\ldots(\exp t_nX_n)\big)
     =\big(\exp t_1\psi(X_1)\big)\ldots\big( \exp t_n\psi(X_n) \big)
\end{equation}
Now, we apply lemma~\ref{lem:decomp} on the decomposition of $\lG$ into the $n$ subspace spanned by the $n$ vector basis (this is $n$ applications of the lemma), the map
\[
  (\exp t_1X_1)\ldots(\exp t_nX_n)\to (t_1,\ldots,t_n)
\]
is a coordinate system around $e$ in $G$. In this case, the relation \eqref{eq:coord_vp_exp} shows that $\varphi$ is differentiable at $e$. Then it is differentiable anywhere in $G$.
\end{proof}


\begin{proposition}
Let $G$ be a Lie group and $H$, a Lie subgroup of $G$ ($\lG$ and $\lH$ are the corresponding Lie algebras). We suppose that $H$ has at most a countable number of connected components. Then
\begin{equation}
  \lH=\{ X\in\lG:\forall t\in\eR,e^{tX}\in H \}
\end{equation}
\end{proposition}

\begin{proof}
We will once again use the lemma ~\ref{lem:decomp} with $\lN=\lH$ and $\lM$, a complementary vector space of $\lH$ in $\lG$. We define
\[
   V=\exp\mU_m\exp\mU_h
\]
where $\mU_m$ and $\mU_h$ are the sets given by the lemma. We consider on $V$ the induced topology from $G$. If we define
\[
   \mA=\{A\in\mU_m:e^{A}\in H\},
\]
we have
\begin{equation}\label{eq:union_A}
   H\cap V=\bigcup_{A\in\mA}e^{A}e^{\mU_h}.
\end{equation}
First, the definition of $V$ makes clear that the elements of the form $\exp A\exp\mU_h$ are in $V$. They are also in $H$ because $\exp A\in H$ (definition of $\mA$) and $\exp\mU_h$ still by definition. In order to see the inverse inclusion, let us consider a $h\in H\cap V$. We know that
\begin{equation}\label{eq:AB_to_exp}
(A,B)\to\exp A\exp B
\end{equation}
is a diffeomorphism between $\mU_m\times\mU_h$ and a neighbourhood of $e$ in $G$ which we called $V$. Thus any element of $V$ (\emph{a fortiori} in $V\cap H$) can be written as $\exp A\exp B$ with $A\in\mU_m$ and $B\in\mU_h$. Then $h=e^Ae^B$ for some $A\in\mU_m$, $B\in\mU_h$. Since $H$ is a group and $e^B\in H$, in order the product to belongs to $H$, $e^A$ must lies in $H$: $A\in\mA$.

\begin{remark}\label{rem:union_disj}
Note that since \eqref{eq:AB_to_exp} is diffeomorphic, the union in right hand side of \eqref{eq:union_A} is disjoint. Each member of this union is a neighbourhood in $H$ because it is a set $h\exp\mU_h$ where $\exp\mU_h$ is a neighbourhood of $e$ in $H$.
\end{remark}

Now we consider the map $\dpt{\pi}{V}{\mU_m}$,
\[
  \pi(e^{X}e^Y)=X
\]
if $X\in\mU_m$ and $Y\in\mU_h$. This is a continuous map which sends $H\cap V$ into $\mA$. The identity component of $H\cap V$ (in the sense of topology of $V$) is sent to a countable subset of $\mU_m$. Indeed by remark~\ref{rem:union_disj}, identity component of $H\cap V$ is only one of the terms in the union \eqref{eq:union_A}, namely $A=0$. But we know that $\pi^{-1}(o)=\exp\mU_h$, thus $\exp\mU_h$ is the identity component of $H\cap V$ for the topology of $V$.
\end{proof}


\begin{theorem}
Let $G$ be a Lie group and $H$, a Lie subgroup of $G$.
\begin{enumerate}
\item If $H$ is a topological Lie subgroup of $G$, then it is closed in $G$,
\item If $H$ has at most a countable number of connected components and is closed in $G$, then $H$ is a topological subgroup of $G$.
\end{enumerate}
\label{tho:H_ferme}
\end{theorem}

\begin{proof}
\subdem{First point} It is sufficient to prove that if a sequence $h_n\in H$ converges (in $G$) to $g\in G$, then $g\in H$ (this is almost the definition of a closed subset). We consider $V$, a neighbourhood of $0$ in $\lG$ such that

\begin{itemize}
\item $\exp$ is diffeomorphic between $V$ and an open neighbourhood  of $e$ in $G$,
\item $\exp(V\cap \lH)=(\exp V)\cap H$.
\end{itemize}
This exists by the lemma~\ref{lem:sugroup_normal}; we can suppose that $V$ is bounded. Consider $\mU$, an open neighbourhood of $0$ in $\lG$ contained in $V$ such that $\exp-\mU\exp\mU\subset\exp V$.

Since $h_n\to g$, there exists a $N\in\eN$ such that $n\geq N$ implies $h_n\in g\exp\mU$ (i.e $h_n$ is the product of $g$ by an element rather close to $e$; since the multiplication is differentiable, the notion of ``not so far''\ is good to express the convergence notion). From now we only consider such elements in the sequence. So, $h_N^{-1} h_n\in(\exp V)\cap H$ ($n\geq N$) because
\[
   h_N^{-1} h_n\in\exp-\mU g^{-1} g\exp\mU\subset\exp V.
\]
(note that $H$ is a group, then $h_i^{-1}\in H$) From the second point of the definition of $V$, there exists a $X_n\in V\cap\lH$ such that $h^{-1}_N h_n=\exp X_n$ for any $n\geq N$.

Since $V$ in bounded, there exists a subsequence out of $(X_i)$ (which is also called $X_i$) converging to a certain $Z\in\lG$. But $\lH$ is closed in $\lG$ because it is a vector subspace (we are in a finite dimensional case), then $Z\in\lH$ and thus the sequence $(h_i)$ converges to $h_N\exp Z$; therefore $g\in H$.

\subdem{Second point} The subgroup $H$ is closed in $G$ and has a countable number of connected component. Since $H$ is closed, theorem~\ref{tho:diff_sur_ferme} it has an analytics structure for which it is a topological Lie subgroup of $G$. We denotes by $H'$ this Lie group.

The identity map $\dpt{I}{H}{H'}$ is continuous\quext{pourquoi ?} (see error~\ref{err:gross}). Thus any connected component of $H$ is contained in a connected component of $H'$, the it has only a countable number of connected components. By corollary~\ref{cor:top_subgroup}, $H=H'$ as Lie group.

\end{proof}

Now we take back our example with $G=S^1\times S^1$, $H=\gamma(\eR)$. In this case, the theorem doesn't works. Let us see why as deep as possible. We have $\lG=\eR\oplus\eR=\eR^2$ and $\lH=\eR$, a one-dimensional vector subspace of $\lG$. ($\lH$ is a ``direction ''\ in $\lG$) First, we build the neighbourhood $V$ of $0$ in $\lG$. It is standard to require that $\exp$ is diffeomorphic between $V$ and an open around $(1,1)\in S^1\times S^1$. It also must satisfy $e^{V\cap\lH}=e^V\cap H$. This second requirement is impossible.

Intuitively. We can see $V\subset\lG$ as a little disk tangent to  the torus. The exponential map deposits it on the torus, as well that $e^V$ covers a little area on $G$. Then $e^V\cap H$ is one of these amazing open subset of $\Gamma$ which are dense in a certain domain of $G$.

On the other hand, $V\cap\lH$ is just a little vector in $\lH$; the exponential deposits it on a small line in $G$. This is not the same at all. Then lemma~\ref{lem:sugroup_normal} fails in our case. Let us review the proof of this lemma until we find a problem.

Let $W_0\subset\lG$  be a neighbourhood of $0$ which is in bijection with an open around $e$ in $G$. We consider $N_0$, an open subset of $H$ such that $N_0\subset W_0$ and $N_0$ is in bijection with $N_e$, a neighbourhood of $e$ in $G$. Until here, no problems. But now the proof says that there exists an open $U_e$ in $G$ such that $N_e=U_e\cap H$. This is false in our case. Indeed, $N_e=e^{N_0}$ is just a segment in $G$ while any subset of $G$ of the form $U_e\cap H$ is an ``amazing''\ open.

So we see that deeply, the obstruction for a Lie subgroup to be a topological Lie subgroup resides in the fact that the topology of a submanifold is \emph{more} than the induced topology, so that we can't automatically find the open $U_e$ in $G$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Connected components}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}\label{lem:vp_G_X}
    Let $G$ be a connected Lie group with Lie algebra $\lG$. If $\dpt{\varphi}{G}{X}$ is an analytic homomorphism ($X$ is a Lie group with Lie algebra $\lX$), then

    \begin{enumerate}
    \item The kernel $\varphi^{-1}(e)$ is a topological Lie subgroup of $G$; his algebra is the kernel of $d\varphi_e$.
    \item The image $\varphi(G)$ is a Lie subgroup of $X$ whose Lie algebra is $d\varphi(\lG)\subset\lX$.
    \item The quotient group $G/\varphi^{-1}(e)$ with his canonical analytic structure is a Lie group. The map $g\varphi^{-1}(e)\mapsto\varphi(g)$ is an analytic isomorphism $G/\varphi^{-1}(e)\to\varphi(G)$. In particular the map $\dpt{\varphi}{G}{\varphi(G)}$ is analytic.
    \end{enumerate}
\end{lemma}

\begin{proof}
\subdem{First item} We know that a subgroup $H$ closed in $G$ admits an unique analytic structure such that $H$ becomes a topological Lie subgroup of $G$. This is the case of $\varphi^{-1}(e)$. We know that $Z\in\lG$ belongs to the Lie algebra of $\varphi^{-1}(e)$ if and only if $\varphi(\exp tZ)=e$ for any $t\in\eR$. But $\varphi(\exp tZ)=\exp(td\varphi(Z))=e$ if and only if $d\varphi(Z)=0$.

\subdem{Second item}
Consider $X_1$, the analytic subgroup of $X$ whose Lie algebra is $d\varphi(\lG)$. The group $\varphi(G)$ is generated by the elements of the form $\varphi(\exp Z)$ for $Z\in\lG$. The group $X_1$ is generated by the $\exp(d\varphi Z)$. Because of lemma~\ref{lemsur5d}, these two are the same. Then $\varphi(G)=X_1$ and their Lie algebras are the same.

\subdem{Third item}
We consider $H$, a closed normal subgroup of $G$; this is a topological subgroup and the quotient $G/H$ has an unique analytic structure such that the map $G\times G/H\to G/H$, $(g,[x])\to [gx]$ is analytic. We consider a decomposition $\lG=\lH\oplus\lM$ and we looks at the restriction $\dpt{\psi}{\lM}{G}$ of the exponential. Then there exists a neighbourhood $U$ of $0$ in $\lM$ which is homomorphically send by  $\psi$ into an open neighbourhood of $e$ in $G$ and such that $\dpt{\pi}{G}{G/H}$ sends homomorphically $\psi(U)$ to a neighbourhood  of $p_0\in G/H$ (cf. lemma~\ref{lem:vois_U}).

We consider $\UU$, the interior of $U$ and $B=\psi(\UU)$. The following diagram is commutative:
\begin{equation}
 \xymatrix{
    G\times G/H  \ar[rr]^{\displaystyle\Phi}\ar[dr]_{\displaystyle \pi\times I} &&  G/H\\
     &     G/H\times G/H\ar[ur] _{\displaystyle\alpha}
  }
\end{equation}
with $\Phi(g,[x])=[g^{-1} x]$, $(\pi\times I)(g,[x])=([g],[x])$ and $\alpha([g],[x])=[g^{-1} x]$. Indeed,
\[
   \alpha\circ(\pi\times I)(g,[x])=\alpha([g],[x])=[g^{-1} x].
\]
In order to see that $\alpha$ is well defined, remark that if $[h]=[g]$ and $[y]=[x]$ $[g^{-1} x]=[h^{-1} y]$ because $H$ is a normal subgroup of $G$.

Now, we consider $g_0,x_0\in G$ and the restriction of $(\pi\times I)$ to $(g_0B)\times(G/H)$. Since $\pi$ is homeomorphic on $\psi(U)$ and $B=\psi(\UU)$, on $g_0B$, $\pi$ is a diffeomorphism (because the multiplication is diffeomorphic as well)

\begin{probleme}\label{prob:diffeo_2}
    Why is the \( \pi\) a diffeomorphism? I understand why it is qn homeomorphism, but no more.
\end{probleme}

This diffeomorphism maps to a neighbourhood $N$ of $([g_0],[x_0])$ in $G/H\times G/H$. From the commutativity, we know that $\alpha=\Phi\circ(\pi\times I)^{-1}$, so that $\alpha$ is analytic. Consequently, $G/H$ is a Lie group. On $N$, $\alpha$ is analytic, then $\alpha(N)$ is analytic.

All this is for a closed normal subgroup $H$ of $G$. Now we consider $H=\varphi^{-1}(e)$ and $\lH$, the Lie algebra of $H$. From the first item, we know that the Lie algebra of $H$ is the kernel of $d\varphi$: $\lH=d\varphi^{-1}(0)$ which is an ideal in $\lG$.

From the second point, the Lie algebra of $G/H$ is $d\pi(\lG)$ which is isomorphic to $\lG/\lH$; the bijection is $\gamma(d\pi(X))=[X]\in\lG/\lH$. In order to prove the injectivity, let us consider $\gamma(A)=\gamma(B)$; $A=d\pi(X)$, $B=d\pi(Y)$. The condition is $[X]=[Y]$; thus it is clear that $d\pi(X)=d\pi(Y)$

Let us consider on the other hand the map $Z+\lH\to d\varphi(Z)$ for $Z\in\lG$\footnote{Note that $\lG$ and $\lH$ are not groups; by $[X]$, we mean $[X]=\{ X+h\tq h\in\lH \}$.}. In other words, the map is $[Z]\to d\varphi(Z)$. This is an isomorphism $\lG/\lH\to d\varphi(\lG)$, which gives a local isomorphism between $G/H$ and $\varphi(G)$. This local isomorphism is $[g]\to\varphi(g)$ for $g$ in a certain neighbourhood of $e$ in $G$.

Since $[g]\to\varphi(g)$ has a differential which is an isomorphism, this is analytic at $e$. Then it is analytic everywhere.

\end{proof}


\begin{corollary}
If $G$ is a connected Lie group and if $Z$ is the center of $G$, then
\begin{enumerate}
\item $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(G)$, with kernel $Z$,
\item the map $[g]\to\Ad_G(g)$ is an analytic isomorphism from $G/Z$ to $\Int(\lG)$ (the class $[g]$ is taken with respect to $Z$).
\end{enumerate}
\label{cor:Ad_homom}
\end{corollary}


\begin{proof}
\subdem{First item}
A connected Lie group is generated by a neighbourhood of identity, and any element of a suitable such neighbourhood can be written as the exponential of an element in the Lie algebra. So $\Int(\lG)$ is generated by elements of the form $\exp(\ad X)=\Ad(\exp X)$; this shows that $\Int(\lG)\subset\Ad(G)$. In order to find the kernel, we have to  see $\Ad_G^{-1}(e)$ by the formula
\[
   e^{\Ad(g)X}=g e^Xg^{-1}.
\]
We have to find the $g\in G$ such that $\forall X\in\lG$, $\Ad_G(g)X=X$. We taking the exponential of the two sides and using \eqref{eq:sigma_X_sigma},
\begin{equation}
  g e^Xg^{-1}=e^X.
\end{equation}
Then $g$ must commute with any $e^X\in G$: in other words, $g$ is in the kernel of $G$.

\subdem{Second item}
This is contained in lemma~\ref{lem:vp_G_X}. Indeed $G$ is connected and we had just proved that $\dpt{\Ad_G}{G}{\Int(\lG)}$ with kernel $Z$; the third item of lemma~\ref{lem:vp_G_X} makes $G/Z$ a Lie group and the map $[g]\to\Ad_G(g)$ an analytic isomorphism from $G/Z$ to $\Ad_G(G)=\Int(\lG)$.
\end{proof}


\begin{lemma}
Let $G_1$ and $G_2$ be two locally isomorphic connected Lie groups with trivial center (i.e. $\lG_1=\lG_2=\lG$ and $Z(G_i)=\{ e \}$). In this case, we have $G_1=G_2=\Int(\lG)$ where $\Int\lG$ stands for the group of internal automorphism of $\lG$.
\end{lemma}

\begin{proof}
We denote by $G_0$ the group $\Int\lG$. The adjoint actions $\Ad_i\colon G_i\to G_0$ are both surjective because of corollary~\ref{cor:Ad_homom}. Let us give an alternative proof for injectivity. Let $Z_i=\ker(\Ad_i)=\{ g\in G_i\tq\Ad(g)X=X,\,\forall X\in\lG \}$. Since $G_i$ is connected, it is generated by any neighbourhood of the identity in the sense of proposition~\ref{PropUssGpGenere}; let $V_0$ be such a neighbourhood. Taking eventually a subset we can suppose that $V_0$ is a normal coordinate system. So we have
\[
  g\exp_{G_i}(X)g^{-1}=\exp_{g_i}(X)
\]
for every $X\in V_0$. Using proposition~\ref{PropUssGpGenere} we deduce that $gxg^{-1}=x$ for every $x\in G_i$, thus $g\in Z(G_i)$. That proves that $\ker(\Ad_i)\subset Z(G_i)$. The assumption of triviality of $Z(G_i)$ concludes injectivity of $\Ad_i$.
\end{proof}

\begin{corollary}
Let $\lG$ be a real Lie algebra with center $\{0\}$. Then the center of $\Int(\lG)$ is only composed of the identity.
\end{corollary}

\begin{proof}
We note $G'=\Int(\lG)$ and $Z$ his center; $\ad$ is the adjoint representation of $\lG$ and $\Ad'$, $\ad'$, the ones of $G'$ and $\ad(\lG)$ respectively. We consider the map $\dpt{\theta}{G'/Z}{\Int(\ad(\lG))}$, $\theta([g])=\Ad'(g)$. By the second item of the corollary~\ref{cor:Ad_homom}, $[g]\to\Ad_{G'}(g)$ is an analytic homomorphism from $G'$ to $\Int(\lG')$ where $\lG'$ is the Lie algebra of $G'$; this is $\ad(\lG)$. So $\dpt{\theta}{G'/Z}{\Int(\lG')}$ is isomorphic.

Now we consider the map $\dpt{s}{\lG}{\ad(\lG)}$, $s(X)=\ad(X)$; this is an isomorphism. We also consider $\dpt{S}{G'}{ \GL(\ad(\lG))}$, $S(g)=s\circ g\circ s^{-1}$. The Lie algebra of $S(G')$ is $\ad(\lG')=\ad\big(\ad(\lG)\big)$. Then $S(G')$ is the subset of $\GL(\ad\lG)$ whose Lie algebra is $\ad\big(\ad\lG\big)$, i.e. exactly $\Int(\ad\lG)$. So $S$ is an isomorphism $\dpt{S}{G'}{\Int(\ad\lG)}$. From all this,
\begin{equation}
   S(e^{\ad X})=s\circ e^{\ad X}\circ s^{-1}
               =e^{\ad'(\ad X)}
           =\Ad'(e^{\ad X}).
\end{equation}
With this equality, $\dpt{S^{-1}\circ\theta}{G'/Z}{G'}$ is an isomorphism which sends $[g]$ on $g$ for any $g\in Z$. Then $Z$ can't contains anything else than the identity.
\end{proof}

If we relax the assumptions of the trivial center, we have a counter-example with $\lG=\eR^3$ and the commutations relation
\[
   [X_1,X_2]=X_3,\quad [X_1,X_3]=[X_2,X_3]=0.
\]
The group $\Int(\lG)$ is abelian; then his center is the whole group, although $\lG$ is not abelian.

Note that two groups which have the same Lie algebra are not necessarily isomorphic. For example the sphere $S^2$ and $\eR^2$ both have $\eR^2$ as Lie algebra. But two groups with same Lie algebra are locally the same. More precisely, we have the following lemma.

\begin{lemma}
If $G$ is a Lie group and $H$, a topological subgroup of $G$ with the same Lie algebra ($\lH=\lG$), then there exists a common neighbourhood $A$ of $e$ of $G$ and $G$ on which the products in $G$ and $H$ are the same.
\end{lemma}

\begin{proof}
The exponential is a diffeomorphism between $U\subset\lG$ and $V\subset G$ and between $U'\subset\lH$ and $W\subset H$ (obvious notations). We consider an open $\mO\subset\lH$ such that $\mO\subset U\subset U'$. The exponential is diffeomorphic from $\mO$ to a certain open $A$ in $G$ and $H$. Since $H$ is a subgroup of $G$, the product $e^Xe^Y$ of elements in $A$ is the same for $H$ and $G$. (cf error~\ref{err:gp_meme_alg})
\end{proof}

Under the same assumptions, we can say that $H$ contains at least the whole $G_0$ because it is generated by any neighbourhood of the identity. Since $H$ is a subgroup, the products keep in $H$.

For a semisimple Lie group, the Lie algebras $\partial(\lG)$ and $\ad(\lG)$ are the same. Then $\Int(\lG)$ contains at least the identity component of $\Aut(\lG)$. Since $\Int(\lG)$ is connected, for a semisimple group, it is the identity component of $\Aut(\lG)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Compact Lie algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{pg:compact_Lie}

We consider $\lG$, a real Lie algebra and $\lH$, a subalgebra of $\lG$. Let $K^*$ be the analytic subgroup of $\Int(\lG)$ which corresponds to the subalgebra $\ad_{\lG}(\lH)$ of $\ad_{\lG}(\lG)$.

\begin{definition}      \label{DEFooROMGooTLicyL}
We say that $\lH$ is \defe{compactly embedded}{compactly embedded} in $\lG$ if $K^*$ is compact. A Lie algebra is \defe{compact}{compact!Lie algebra}\index{Lie!algebra!compact} when it is compactly embedded in itself.
\end{definition}

The analytic subgroup of $\Int(\lG)$ which corresponds to $\ad_{\lG}(\lG)$, by definition, is $\Int(\lG)$. Then the compactness of $\lG$ is the one of $\Int(\lG)$.

\begin{remark}
The compactness notion on a Lie group is defined from the topological structure of the Lie group seen as a manifold. It is all but trivial that the compactness on a Lie group is related to the compactness on its Lie algebra; the proposition~\ref{prop:alg_grp_compact} will however make the two notions related in the natural way.
\end{remark}

\begin{remark}
The topology on $K^*$ is not necessary the same as the induced one from $\Int(\lG)$ and $\Int(\lG)$ has also not necessary the induced topology from $\GL(\lG)$. However the next proposition will show that the compactness notion is well the one induced from $\GL(\lG)$.
\end{remark}

\begin{proposition}
We consider $\tK$, the same set and group as $K^*$, but with the induced topology from $\GL(\lG)$. Then $\tK$ is compact if and only if $K^*$ is compact.
\end{proposition}

Note however that $K^*$ and $\tK$ are not automatically the same as manifold.

\begin{proof}
\subdem{$K^*$ compact implies $\tK$ compact}
The identity map $\dpt{\iota}{K^*}{\GL(\lG)}$ is analytic, and then is continuous because $\Int(\lG)$ is by definition an analytic subgroup of $\GL(\lG)$ and $K^*$ an analytic subgroup of $\Int(\lG)$. If we have a covering of $\tK$ with open set $\mO_i\cap\tK$ of $\tK$ ($\mO_i$ is open in $\GL(\lG)$), the continuity of $\iota$ make the finite subcovering of $K^*$ good for $\tK$.
\subdem{$\tK$ compact implies $K^*$ compact}
If $\tK$ is compact, then it is closed in $\GL(\lG)$. As set, $K^*$ is closed in $\GL(\lG)$ and by definition it is connected. Then by the theorem~\ref{tho:H_ferme}, $K^*$ is a topological subgroup of $\GL(\lG)$. Consequently, $K^*$ and $\tK$ are homeomorphic and they have same topology.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]
    If $G$ is a compact group in $\GL(n,\eR)$, then there exists a $G$-invariant quadratic form on $\eR^n$\quext{Is it true ? I've not even found a precise statement of this claim.}.
\end{lemma}

\begin{proposition}     \label{ProplGcompactKillNeg}
Let $\lG$ be a real Lie algebra.

\begin{enumerate}
\item If $\lG$ is semisimple, then $\lG$ is compact if and only if  the Killing form is strictly negative definite.
\item If it is compact then it is a direct sum
\begin{equation}
   \lG=\mZ\oplus [\lG,\lG]
\end{equation}
where $\mZ$ is the center of $\lG$ and the ideal $[\lG,\lG]$ is compact and semisimple.
\end{enumerate}
\label{prop:compact_Killing}
\end{proposition}

\begin{proof}
\subdem{If the Killing form is nondegenerate}
We consider $\lG$, a Lie algebra whose Killing form is strictly negative definite. Up to some dilatations (and a sign), this is the euclidian metric. Then $O(B)$, the group of linear transformations which leave $B$ unchanged is compact in the topology of $\GL(\lG)$: this is almost the rotations. From equation \eqref{eq:Aut_Iso}, $\Aut(\lG)\subset O(B)$. With this, $\Aut(\lG)$ is closed in a compact, then it is compact. Then $\Int(\lG)$ is closed in $\Aut(\lG)$ --here is the assumption of semi-simplicity-- and $\Int(\lG)$ is compact.
\subdem{If $\lG$ is compact}
Since $\lG$ is compact, $\Int(\lG)$ is compact in the topology of $\Aut(\lG)$; then there exists an $\Int(\lG)$-invariant quadratic form $Q$. In a suitable basis $\{X_1,\ldots,X_n\}$ of $\lG$, we can write this form as
\[
   Q(X)=\sum x_i^2
\]
for $X=\sum x_iX_i$. In this basis the elements of $\Int(\lG)$ are orthogonal matrices and the matrices of $\ad(\lG)$ are skew-symmetric matrices (the Lie algebra of orthogonal matrices). Let us consider a $X\in\lG$ and denote by $a_{ij}(X)$ the matrix of $\ad(X)$. We have
\begin{equation}
\begin{split}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_i\sum_ja_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2\leq 0.
\end{split}
\end{equation}
Then the Killing form is negative definite\footnote{Here we use ``negative definite''\ and ``\emph{strictly} negative definite''; in some literature, the terminology is slightly different and one says ``\emph{semi} negative definite''\ and ``negative definite''.}. On the other hand, $B(X,X)=0$ implies $\ad(X)=0$ and $X\in\mZ(\lG)$. Thus $\lG^{\perp}\subset\mZ$. If $\lG$ is semisimple, this center is zero; this conclude the first item of the proposition.

Now $\mZ$ is an ideal and corollary~\ref{cor:decomp_ideal} decomposes $\lG$ as
\begin{equation}
  \lG=\mZ\oplus\lG'.
\end{equation}
Let us suppose that the restriction of $B$ to $\lG'\times\lG'$ is actually the Killing form on $\lG'$ (we will prove it below). Then the Killing form on $\lG'$ is strictly negative definite; then $\lG'$ is compact.

Now we prove that the Killing form on $\lG$ descent to the Killing form on~$\lG'$. Remark that $\mZ$ is invariant under all the automorphism. Indeed consider $Z\in\mZ$, i.e.  $[X,Z]=0$. If $\sigma$ is an automorphism,
\[
   [X,\sigma Z]=\sigma[\sigma^{-1} X,Z]=0.
\]
Here the difference between $\Int(\lG)$ and $\Aut(\lG)$ is the fact that $\Int(\lG)$ is compact; then we can construct a $\Int(\lG)$-invariant quadratic form $Q$, but not a $\Aut(\lG)$-invariant one. We consider an orthogonal complement (with respect to $Q$) $\lG'$ of $\mZ$:
\begin{equation}
   \lG=\lG'\oplus_{\perp}\mZ.
\end{equation}
The algebra $\lG'$ is also invariant because for any $Z\in\mZ$,
\[
Q(Z,\sigma X)=Q(\sigma^{-1}(Z),X)=0.
\]
It is also clear that $\mZ$ is invariant under $\ad\lG$ because $(\ad X)Z=0$. Finally $\lG'$ is invariant as well under $\ad(\lG)$. Indeed $a\in\ad(\lG)$ can be written as $a=a'(0)$ for a path $a(t)\in\Int(\lG)$. We identify $\lG$ and his tangent space (as vector spaces),
\[
  aX=\Dsdd{ a(t)X }{t}{0}.
\]
If $X\in\lG'$, $a(t)X\in\lG'$ for any $t$ because $\lG'$ is invariant under $\Int(\lG)$\footnote{As physical interpretation, if something is invariant under a group of transformations, it is invariant under the infinitesimal transformations as well.}. Thus $a(t)X$ is a path in $\lG'$ and his derivative is a vector in $\lG'$.

All this make $\lG'$ an ideal in $\lG$; then the Killing form descent by lemma~\ref{lem:Killing_descent_ideal}. Now if $X\in\lG$, we have
\begin{equation}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_{ij}a_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2;
\end{equation}
then $B(X,X)\leq 0$ and the equality holds if and only if $\ad X=0$ i.e. if and only if $X\in\mZ$. Thus $B$ is strictly negative definite on $\lG'$.

Up to now we have proved that $\lG'$ is semisimple (because $B$ is nondegenerate) and compact (because $B$ is strictly negative definite).

It remains to be proved that $\lG'=[\lG,\lG]=\dD(\lG)$. From corollary~\ref{cor:decomp_ideal}, $\dD\lG$ has a complementary $\lA$ which is also an ideal: $\lG=\dD\lG+\lA$. Then $[\lG,\lA]\subset\dD\lG$ and $[\lG,\lA]\subset\lA\cap\dD\lG:\{0\}$. Then $\lA\subset\mZ$, so that
\begin{align}\label{eq:G_Z_B}
   \lG=\mZ+\dD\lG&&\text{(non direct sum)}.
\end{align}
Now we have to prove that the sum is actually direct. The ideal $\mZ$ has a complementary ideal $\lB$: $\lG=\mZ\oplus\lB$ and
\[
   \dD\lG=[\lG,\lG]\subset\underbrace{[\lG,\mZ]}_{=0}+[\lG,\lB]\subset\lB.
\]
Then $\dD\lG\subset\lB$ which implies that $\dD\lG\cap\mZ=\{0\}$ because the sum $\lG=\mZ\oplus\lB$ is direct. Then the sum \eqref{eq:G_Z_B} is direct.

\end{proof}

\begin{proposition}
A real Lie algebra $\lG$ is compact if and only if one can find a compact Lie group $G$ which Lie algebra is isomorphic to $\lG$.
\label{prop:alg_grp_compact}
\end{proposition}

\begin{proof}
\subdem{Direct sense} Since $\lG$ is compact, $\lG=\mZ\oplus\dD\lG$ with $\dD\lG=\lG'$ compact and semisimple; in particular, the center of $\lG'$ is $\{0\}$. Since $\mZ$ is compact and abelian, it is isomorphic to the torus $S^1\times\ldots\times S^1$. Since $\lG'$ is compact, $\Int(\lG')$ is compact, but the Lie algebra if $\Int(\lG')$ is --by definition--  $\ad(\lG')$. The center of a semisimple Lie algebra is zero; then $\ad X'=0$ implies $X=0$ (for $X\in\lG'$). Then $\ad$ is an isomorphism between $\lG'$ and $\ad\lG'$.

All this shows that --up to isomorphism-- $\mZ$ and $[\lG,\lG]$ are Lie algebras of compact groups. We know from lemma~\ref{lemLeibnitz} that the Lie algebra of $G\times H$ is $\lG\oplus\lH$. Thus, here, $\lG$ is the Lie algebra of the compact group $S^1\times\ldots\times S^1\times\Int(\lG)$.
\subdem{Reverse sense}
We consider a compact group $G$ and we have to see the its Lie algebra $\lG$ is compact. If $G$ is connected, $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(\lG)$. If $G$ is not connected, the Lie algebra of $G$ is $T_eG_0$ ($G_0$ is the identity component of $G$) where $G_0$ is connected and compact because closed in a compact.
\end{proof}

\begin{proposition}
Let $\lG$ be a real Lie algebra and $\mZ$, the center of $\lG$. We consider $\lK$, a compactly embedded in $\lG$. If $\lK\cap\mZ=\{0\}$ then the Killing form of $\lG$ is strictly negative definite on $\lK$.
\label{prop:K_Z_Killing}
\end{proposition}

\begin{proof}
Let $B$ be the Killing form on $\lG$ and $K$ the analytic subgroup of $\Int(\lG)$ whose Lie algebra is $\ad_{\lG}(\lK)$. By assumption, $K$ is a compact Lie subgroup of $\GL(\lG)$. Then there exists a quadratic form on $\lG$ invariant under $K$, and a basis in which the endomorphisms $\ad_{\lG}(T)$ for $T\in\lK$ are skew-symmetric because the matrices of $K$ are orthogonal. If the matrix of $\ad T$ is $(a_{ij})$, then
\begin{equation}
   B(T,T)=\sum_{ij}a_{ij}(T)a_{ji}(T)
         =-\sum_{ij}a_{ij}^2(T)\leq 0,
\end{equation}
and the equality hold only if $\ad T=0$ i.e. if $T\in\mZ$. From the assumptions, $\lK\cap\mZ=\{0\}$; then $B(T,T)=0$ if and only if $T=0$.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representations from the Lie algebra to the Lie group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}[\cite{BIBooYTTJooYpPYLT}]       \label{PROPooXCGMooKlJlwp}
    Let \( G\) be a Lie group and \( \lG\) be its Lie algebra. Let \( (\rho, V)\) be a smooth representation of \( G\). We consider the map
    \begin{equation}
        \begin{aligned}
            s\colon \lG&\to \End(V) \\
            s(X)v&=\Dsdd{ \rho( e^{tX})v }{t}{0}.
        \end{aligned}
    \end{equation}
    We have the equality
    \begin{equation}
        \rho( e^{tX})= e^{ts(X)}
    \end{equation}
    as operators on \( V\).
\end{proposition}

\begin{proof}
    Let \( X\in \lG\). We define \( M(t)=\rho( e^{tX})\) and \( N(t)= e^{ts(X)}\). These are maps from \( \eR\) to \( \End(V)\); the proposition \ref{PROPooSDNNooQtHkhA} helps to derive them.

    For \( N\) we immediately have
    \begin{equation}
        N'(t)=s(X) e^{ts(X)}.
    \end{equation}
    For \( M\), we have few more prudence. We fix \( \epsilon>0\) and we write (thanks to proposition \ref{PROPooKDKDooCUpGzE})
    \begin{equation}
        M(t+\epsilon)=\rho( e^{(t+\epsilon)X})=\rho( e^{tX} e^{\epsilon X})= \rho( e^{tX}) e^{\epsilon X}.
    \end{equation}
    Now for the differential quotient,
    \begin{equation}
        \frac{ M(t+\epsilon)-M(t) }{ \epsilon }=\rho( e^{tX})\frac{ \rho( e^{\epsilon X})-\id }{ \epsilon }.
    \end{equation}
    If we compute the limit \( \epsilon\to 0\), the second factor goes, by definition to \( s(X)\). So
    \begin{equation}
        M'(t)=\rho( e^{tX})s(X)=s(X)M(t).
    \end{equation}
    So \( M\) and \( N\) satisfy the same differential equation
    \begin{subequations}
        \begin{numcases}{}
            y'=s(X)y(t)\\
            y(0)=\mtu
        \end{numcases}
    \end{subequations}
    for the function \( y\colon \eR\to \End(V)\). This is a work for Cauchy-Lipschitz, theorem \ref{THOooZIVRooPSWMxg}. So we define \( f(t,m)=s(X)m\) and we show that this is Lipschitz with respect to \( m\) :
    \begin{subequations}
        \begin{align}
            \| f(t_0,m_0)-f(t,m) \|=\| s(X)(m_0-m) \|\leq \| s(X) \|\| m_0-m \|.
        \end{align}
    \end{subequations}
    So the function \( f\) is Lipschitz with respect to \( m\) with a Lipschitz constant bounded by \( \| s(X) \|\).

    The unicity part of Cauchy-Lipschitz shows that \( M(t)=N(t)\) for every \( t\) for which the expressions make sense.
\end{proof}

\begin{theorem}[\cite{BIBooYTTJooYpPYLT}]       \label{THOooLVSNooOpzYgO}
    Let \( G\) be a Lie group and \( \lG\) be its Lie algebra. If \( (\rho, V)\) is a representation of \( G\), the map
    \begin{equation}
        \begin{aligned}
            s\colon \lG&\to \End(V) \\
            s(X)v&=\Dsdd{ \rho( e^{tX})v }{t}{0} 
        \end{aligned}
    \end{equation}
    is a representation of \( \lG\) on \( V\).
\end{theorem}

\section{Real Lie algebras}
%++++++++++++++++++++++++++

\subsection{Real and complex vector spaces}
%//////////////////////////////////////////////

If $V$ is a real vector space, the \defe{complexification}{complexification!of a vector space} of $V$ is the vector space\nomenclature{$V\heC$}{Complexification of $V$}
\[
  V\heC:=V\otimes\beR\eC.
\]
If $\{v_i\}$ is a basis of $V$ on $\eR$, then $\{v_i\otimes 1\}$ is a basis of $V\heC$ on $C$. Then
\[
   \dim_{\eR}V=\dim_{\eC}V\heC.
\]

Let $W$ be a complex vector space. If one restrains the scalars to $\eR$, we find a real vector space denoted by $W\heR$\nomenclature{$W\heR$}{Restriction of a complex vector spaces to $\eR$}. If $\{w_j\}$ is a basis of $W$, then $\{w_j,iw_j\}$ is a basis of $W\heR$ and
\[
  \dim\beC W=\frac{1}{2}\dim\beR W\heR.
\]
Note that $(V\heC)\heR=V\oplus iV$.

A real vector space $V$ is a \defe{real form}{real!form!of complex vector space} of a complex vector space $W$ if $W\heR=V\oplus iV$. If $V$ is a real form of $W$, the map $\dpt{\varphi}{V\heC}{V\heC}$ given by the identity on $V$ and the multiplication by $-1$ on $iV$ is the \defe{conjugation}{conjugation} of $V\heC$ with respect of the real form $V$.

\subsection{Real and complex Lie algebras}
%/////////////////////////////////////////

For notational convenience, if not otherwise mentioned, $\lG$ will denote a complex Lie algebra and $\lF$ a real one. If $\lF$ is a real Lie algebra and $\lF\heC=\lF\otimes\eC$, its complexification (as vector space), we endow $\lF\heC$ with a Lie algebra structure by defining
\[
  [ (X\otimes a),(Y\otimes b)  ]=[X,Y]\otimes ab.
\]
This is a bilinear extension of the Lie algebra bracket of $\lF$. It is rather easy to see that $[\lF,\lF]\heC=[\lF\heC,\lF\heC]$.

Now we turn our attention to the Killing form. Let $\lF$ be a real Lie algebra with a Killing form $B\blF$. A basis of $\lF$ is also a basis of $\lF\heC$. Then the matrix $B_{ij}=\tr( \ad X_i\circ\ad X_j )$ of the Killing form is the same for $\lF\heC$ than for $\lF$. In conclusion:
\[
   B_{\lF\heC}|_{\lF\times\lF}=B\blF.
\]

Let us study the inverse process: $\lG$ is a complex Lie algebra and $\lG\heR$ is the real Lie algebra obtained from $\lG$ by restriction of the scalars. If $\mB=\{v_j\}$ is a basis of $\lG$, $\mB'=\{v_j,iv_j\}$ is a one of $\lG\heR$. For a certain $X\in\lG$ we denote by $(c_{kl})$ the matrix of $\ad_{\lG}X$. Now we study the matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ by computing
\begin{equation}
(\ad_{\lG}X)v_i=c_{ik}v_k
               =\big[ \real(c_{ik})+i\imag(c_{ik}) \big]v_k
           =a_{ik}v_k+b_{ik}(iv_k)
\end{equation}
if $a=\real c$ and $b=\imag c$. Then the columns of $\ad_{\lG\heR}$ which correspond to the $v_i\in\mB'$'s are given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&\cdot\\
         b&\cdot
               \end{pmatrix}
\]
where the dots denote some entries to be find now:
\begin{equation}
(\ad_{\lG}X)(iv_i)=i\big(  a_{ik}v_k+b_{ik}(iv_k)  \big)\\
                  =a_{ik}(iv_k)-b_{ik}v_k,
\end{equation}
so that the complete matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ is given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&-b\\
         b&a
               \end{pmatrix}.
\]
So,
\[
\ad_{\lG\heR}X\circ\ad_{\lG\heR}X'=\begin{pmatrix}
                 aa'-bb'&\cdot\\
         \cdot&aa'-bb'
               \end{pmatrix}.
\]
Then $B(X,X')=2\tr(aa'-bb')$ while
\begin{equation}
  B(X,Y)=\tr\big(  (a+ib)(a'+ib')  \big)\\
        =\tr(aa'-bb')+i\tr(ab'+ba').
\end{equation}
Thus we have
\begin{equation}
     B_{\lG\heR}=2\real B_{\lG},
\end{equation}
so that $\lG\heR$ is semisimple if and only if $\lG$ is semisimple.

\subsection{Split real form}
%//////////////////////////////

Let $\lG$ be a complex semisimple Lie algebra, $\lH$ a Cartan subalgebra, $\Phi$ the set roots, $\Delta$ the set of non zero roots and $B$, the Killing form. From property \eqref{eq:enuaiv} and the fact that $c(-\alpha,-\beta)=c(\alpha,\beta)$, we find $c(\alpha,\beta)^2=\frac{1}{2}\lbha(1+\lbba)|\alpha|^2$,
 so that $c(\alpha,\beta)^2\geq 0$ which gives $c(\alpha,\beta)\in\eR$. We can define

\[
   \lGeR=\lH_0\bigoplus_{\alpha\in\Phi}\eR x_{\alpha}.
\]
Remark that $\lG_{\alpha}$ has dimension one with respect to $\eC$, not $\eR$; then $\eR x_{\alpha}\neq\lG_{\alpha}$, but $\eC x_{\alpha}=\lG_{\alpha}$ and $\lG_{\alpha}=\eR x_{\alpha}\oplus i\eR x_{\alpha}$. Since it is clear that $\bigoplus_{\alpha\in\Delta}( \eR x_{\alpha}\oplus i\eR x_{\alpha} )=\bigoplus_{\alpha\in\Delta}\lG_{\alpha}$, the proposition~\ref{prop:lHeR} gives
\begin{equation}
  \lG=\lGeR\oplus i\lGeR.
\end{equation}
Any real form of $\lG$ which contains the $\lHeR$ of a certain Cartan subalgebra $\lH$ of $\lG$ is said a \defe{split real form}{split!real form}. The construction shows that any complex semisimple Lie algebra admits a split real form.

\subsection{Compact real form}
%///////////////////////////////

\begin{definition}
    A \defe{compact real form}{compact!real form}\index{compact!Lie algebra} of a complex Lie algebra is a real form which is compact as Lie algebra\footnote{Definition \ref{DEFooROMGooTLicyL}.}. 
\end{definition}

\begin{theorem}
Any complex semisimple Lie algebra contains a compact real form.
\end{theorem}

\begin{proof}
Let $\lH$ be a Cartan algebra of the complex semisimple Lie algebra $\lG$ and $ x_{\alpha}$, some root vectors. We consider the space
\begin{equation}
 \lU_0=\underbrace{\sum_{\alpha\in\Phi}\eR ih_{\alpha}}_A+\underbrace{\sum_{\alpha\in\Phi}\eR( x_{\alpha}-\xbma)}_B+\underbrace{\sum_{\alpha\in\Phi}\eR i( x_{\alpha}+\xbma)}_C.
\end{equation}
Since $\lU_0\oplus i\lU_0$ contains all the $\eC h_{\alpha}$, $\lH\subset\lU_0\oplus i\lU_0$; it is also rather clear that $\lU_0$ is a real form of $\lG$ (as vector space), for example, $i\eR( x_{\alpha}-\xbma)+\eR i( x_{\alpha}+\xbma)=\eR i x_{\alpha}$. Now we have to check that $\lU_0$ is a real form of $\lG$ as Lie algebra, i.e. that $\lU_0$ is closed for the Lie bracket. This is a lot of computations:
\[
\begin{split}
[i h_{\alpha},i\hbb]               &=0,\\
[i h_{\alpha},( x_{\alpha}-\xbma)]        &=i(\alpha( h_{\alpha}) x_{\alpha}-(-\alpha)( h_{\alpha})\xbma)\\
                            &=i\alpha( h_{\alpha})( x_{\alpha}+\xbma)\in C,\\
[i h_{\alpha},i( x_{\alpha}+\xbma)]       &=-\alpha( h_{\alpha})( x_{\alpha}-\xbma)\in B,\\
[( x_{\alpha}-\xbma),(\xbb-\xbmb)] &=c(\alpha,\beta)( x_{\alpha+\beta}-x_{-(\alpha+\beta)} )\in B\\
                            &\quad -c(\alpha,\beta)(x_{\alpha-\beta}-x_{\beta-\alpha})\in B,\\
[( x_{\alpha}-\xbma),i(\xbb+\xbmb)]&=ic(\alpha,\beta)(x_{\alpha+\beta}+x_{-(\alpha+\beta)})\in C\\
                            &\quad +ic(\alpha,-\beta)(x_{\alpha-\beta)}+x_{-\alpha+\beta})\in C\\
[i h_{\alpha},(\xbb-\xbmb)]     &=i\beta( h_{\alpha})(\xbb-\xbmb)\in C\\
[i h_{\alpha},i(\xbb+\xbmb)]       &=-\beta( h_{\alpha})(\xbb-\xbmb)\in B\\
[i( x_{\alpha}+\xbma),i(\xbb+\xbmb)]&=-c(\alpha,\beta)(x_{\alpha+\beta}-x_{-(\alpha+\beta)})\\
                             &\quad -c'(\alpha,-\beta)(x_{\alpha-\beta}-x_{-\alpha+\beta}).
\end{split}
\]

From proposition~\ref{prop:compact_Killing}, it just remains to prove that the Killing form of $\lU_0$ is strictly negative definite. We know that $B_{\lG}(\lG_{\alpha},\lG_{\beta})=0$ if $\alpha,\beta\in\Phi$ and $\alpha+\beta\neq 0$; then $A\perp B$ and $A\perp C$. It is a lot of computation to compute the Killing form; we know that $B$ is strictly positive definite on $\sum_{\alpha\in\Delta}\eR h_{\alpha}$ (and then strictly negative definite on $A$) a part this, the non zero elements are (recall that if $\alpha\neq 0$, $B( x_{\alpha}, x_{\alpha})=0$ from corollary~\ref{cor:Bxy_zero})
\[
\begin{split}
  B( ( x_{\alpha}-\xbma),( x_{\alpha}-\xbma) )&=-2B( x_{\alpha},\xbma)=-2\\
  B(i( x_{\alpha}+\xbma),i( x_{\alpha},\xbma))&=-2.
\end{split}
\]

What we have in the matrix of $B_{\lG}|_{\lU_0\times\lU_0}$ is a negative definite block (corresponding to $A$), $-2$ on the rest of the diagonal and zero anywhere else. Then it is well negative definite and $\lU_0$ is a compact real from of $\lG$.
\end{proof}

\subsection{Involutions}
%//////////////////////////

Let $\lG$ be a (real or complex) Lie algebra. An automorphism $\dpt{\sigma}{\lG}{\lG}$ which is not the identity such that $\sigma^2$ is the identity is a \defe{involution}{involutive!automorphism}. An involution $\dpt{\theta}{\lF}{\lF}$ of a \emph{real} semisimple Lie algebra $\lF$ such that the quadratic form $B_{\theta}$ defined by
\[
   B_{\theta}(X,Y):=-B(X,\theta Y)
\]
is positive definite is a \defe{Cartan involution}{Cartan!involution}.

\begin{proposition}
Let $\lG$ be a complex semisimple Lie algebra, $\lU_0$ a compact real form and $\tau$, the conjugation of $\lG$ with respect to $\lU_0$. Then $\tau$ is a Cartan involution of $\lG\heR$.
\label{prop:conj_invol}
\end{proposition}

\begin{proof}
From the assumptions, $\lG=\lU_0\oplus i\lU_0$, $\tau_{\lU_0}=id$ and $\tau_{i\lU_0}=-id$; then it is clear that $\tau_{\lG\heR}^2=id|_{\lG\heR}$. If $Z\in\lG$, we can decompose into $Z=X+iY$ with $X$, $Y\in\lU_0$. For $Z\neq 0$, we have
\begin{equation}
    B_{\lG}(Z,\tau Z)=B_{\lG}(X+iY,X-iY)
                     =B_{\lG}(X,X)+B_{\lG}(Y,Y)
             =B_{\lU_0}(X,X)+B_{\lU_0}(Y,Y)<0
\end{equation}
because $B$ restricts itself to $\lU_0$ which is compact. Then
\begin{equation}
  (B_{\lG\heR})_{\tau}(Z,Z')=B_{\lG\heR}(Z,\tau Z)
                            =-2\real B_{\lG}(Z,\tau Z')
\end{equation}
is positive definite because $(B_{\lG})_{\tau}$ is negative definite. Thus $\tau$ is a Cartan involution of $\lG\heR$.
\end{proof}

\begin{lemma}
If $\varphi$ and $\psi$ are involutions of a vector space $V$ (we denote by $V_{\psi^+}$ and $V_{\psi^-}$ the subspaces of $V$ for the eigenvalues $1$ and $-1$ of $\psi$ and similarly for $\varphi$), then
\[
[\varphi,\psi]=0\quad\text{iff}\quad \left\{   \begin{aligned}
                                                   V_{\varphi^+}&=(V_{\varphi^+}\cap V_{\psi^+})\oplus(V_{\varphi^+}\cap V_{\psi^-})\\
                           V_{\varphi^-}&=(V_{\varphi^-}\cap V_{\psi^+})\oplus(V_{\varphi^-}\cap V_{\psi^-}),
                                           \end{aligned}
                      \right.
\]
i.e. if and only if the decomposition of $V$ with respect to $\varphi$ is ``compatible''{} with the one with respect to $\psi$.
\label{lem:invol_compat}
\end{lemma}

\begin{proof}
\subdem{Direct sense}
Let us first see that $\varphi$ leaves the decomposition $V=V_{\psi^+}\oplus V_{\psi^-}$ invariant. If $x=x_{\psi^+}+x_{\psi^-}$,
\[
   \varphi(x_{\psi^+})=(\varphi\circ\psi)(x_{\psi^+})=(\psi\circ\varphi)(x_{\psi^+}).
\]
Then $\varphi(x_{\psi^+})\in V_{\psi^+}$, and the matrix of $\varphi$ is block-diagonal with respect to the decomposition given by $\psi$. Thus $V_{\psi^+}$ and $V_{\psi^-}$ split separately into two parts with respect to $\varphi$.

\subdem{Inverse sense}
If $x\in V$, we can write $x=x_{++}+x_{+-}+x_{-+}+x_{--}$ where the first index refers to $\psi$ while the second one refers to $\psi$; for example, $x_{+-}\in V_{\psi^+}\cap V_{\varphi^-}$. The following computation is easy:
\begin{equation}
\begin{split}
(\varphi\circ\psi)(x)&=\varphi(x_{++}+x_{+-}-x_{-+}-x_{--})\\
                 &=x_{++}-x_{+-}-x_{-+}+x_{--}\\
         &=\psi(x_{++}-x_{+-}-x_{-+}-x_{--})\\
         &=(\psi\circ\varphi)(x).
\end{split}
\end{equation}
\end{proof}

\begin{corollary}\label{cor:Cartan_conj_inner}
    Any two Cartan involutions of a real semisimple Lie algebra are conjugate by an inner automorphism. \index{inner!automorphism}
\end{corollary}

\begin{proof}
Let $\sigma$ and $\sigma'$ be two Cartan involutions of $\lF$. We can find a $\varphi\in\inf\lF$ such that $[\varphi\sigma\varphi^{-1},\sigma']=0$. Thus it is sufficient to prove that any two Cartan involutions which commute are equals. So let us consider $\theta$ and $\theta'$, two Cartan involutions such that $[\theta,\theta']=0$. By lemma~\ref{lem:invol_compat}, we know that the decompositions into $+1$ and $-1$  eigenspaces with respect to $\theta$ and $\theta'$ are compatibles. If we consider $X\in\lF$ such that $\theta X=X$ and $\theta' X=-1$ (it is always possible if $\theta\neq\theta'$), we have
\[
\begin{split}
  0<B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X)\\
  0<B_{\theta'}(X,X)=-B(X,\theta' X)=B(X,X)
\end{split}
\]
which is impossible.
\end{proof}

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra, $\theta$ a Cartan involution on $\lF$ and $\sigma$, another involution (not specially Cartan). Then there exists a $\varphi\in\Int\lF$ such that $[\varphi\theta\varphi^{-1},\sigma]=0$
\label{tho:sigma_theta_un}
\end{theorem}

\begin{proof}
If $\theta$ is a Cartan involution, then $B_{\theta}$ is a scalar product on $\lF$. Let $\omega=\sigma\theta$. By using $\sigma^2=\theta^2=1$, $\theta=\theta^{-1}$ and the invariance property~\ref{prop:auto_2} of the Killing form,
\begin{equation}
B(\omega X,\theta Y)=B(X,\omega^{-1}\theta Y)
                    =B(X,\theta\sigma\theta Y)
            =B(X,\theta\omega Y).
\end{equation}
Then $B_{\theta}(\omega X,Y)=B_{\theta}(X,\omega Y)$. This is a general property of scalar product that in this case, the matrix of $\omega$ is symmetric while the one of $\omega^2$ is positive definite. If we consider the classical scalar product whose matrix is $(\delta_{ij})$, the property is written as $A_{ij}v_jw_j=v_iA_{ij}w_j$ (with sum over $i$ and $j$); this implies the symmetry of $A$. To see that $A^2$ is positive definite, we compute (using the symmetry):
\[
   A_{ij}A_{jk}v_iv_k=v_iA_{ij}v_kA_{kj}=\sum_j(v_iA_{ij})^2>0.
\]
The next step is to see that there is an unique linear transformation $\dpt{A}{\lF}{\lF}$ such that $\omega^2=e^A$, and that for any $t\in\eR$, the transformation $e^{tA}$ is an automorphism of $\lF$.

We choose an orthonormal (with respect to the inner product $B_{\theta}$) basis $\{X_1,\ldots,X_n\}$  of $\lF$ in which $\omega$ is diagonal. In this basis, $\omega^2$ is also diagonal and has positive real numbers on the diagonal; then the existence and unicity of $A$ is clear. Now we take some notations:
\begin{subequations}
\begin{align}
  \omega(X_i)&=\lambda_iX_i\\
  \omega^2(X_i)&=e^{a_i}X_i,
\end{align}
\end{subequations}
(no sum at all) where the $a_i$ are the diagonals elements of $A$. The structure constants are as usual defined by
\begin{equation}
   [X_i,X_j]=c_{ij}^kX_k.
\end{equation}
Since $\sigma$ and $\theta$ are automorphisms, $\omega^2$ is also one. Then
\[
\omega^2[X_i,X_j]=c_{ij}^k\omega^2(X_k)=c_{ij}^ke^{a_k}X_k
\]
can also be computed as
\[
   \omega^2[X_i,X_j]=[\omega^2X_i,\omega^2X_j]=e^{a_i}e^{a_j}c_{ij}^kX_k,
\]
so that $c_{ij}^ke^{a_k}=c_{ij}^ke^{a_i}e^{a_j}$, and then $\forall t\in\eR$,
\[
   c_{ij}^ke^{ta_k}=c_{ij}^ke^{ta_i}e^{ta_j},
\]
which proves that $e^{tA}$ is an automorphism of $\lF$. By lemma~\ref{lem:autom_derr}, $A$ is thus a derivation of $\lF$. The semi-simplicity makes $\partial\lF=\ad\lF$, then $A\in\ad\lF$ and $e^{tA}\in\Int\lF$ because it clearly belongs to the identity component of $\Aut\lF$.

Now we can finish de proof by some computations. Remark that $\omega=e^{A/2}$ and $[e^{tA},\omega]=0$ because it can be seen as a common matrix commutator. Since $\omega^{-1}=\theta\sigma$, we have $\theta\omega^{-1}\theta=\sigma\theta$, or $\theta\omega^2\theta=\omega^2$ and
\begin{equation}\label{eq:eAth}
   e^{A}\theta=\theta e^{-A}.
\end{equation}
From this, one can deduce that $e^{tA}\theta=\theta e^{-tA}$. Indeed, as matricial identity, equation \eqref{eq:eAth} reads
\[
    (e^{A}\theta)_{ik}=(e^{A})_{ij}\theta_{jk}
                      =e^{a_i}\theta_{ik}
              =e^{-a_k}\theta_{ik}.
\]
Then for any $ik$ such that $\theta_{ik}\neq 0$, we find $e^{a_i}=e^{-a_k}$ and then also $e^{ta_i}=e^{-ta_k}$. Thus $(e^{tA}\theta)_{ik}=(e^{tA})_ij\theta_{jk}=e^{ta_i}\theta_{ik}=\theta_{ik}e^{-ta_k}=(\theta e^{-tA})_{ik}$. So we have
\begin{equation}
  e^{tA}\theta=\theta e^{-tA}.
\end{equation}
Now we consider $\varphi=e^{A/4}\in\Int\lF$ and $\theta_1=\varphi\theta\varphi^{-1}$. We find $\theta_1\sigma=e^{A/2}\omega^{-1}$ and $\sigma\theta^{-1}=e^{-A/2}\omega$. Since $\omega^2=A$, we have $e^{A/2}=e^{-A/2}\omega^2$ and thus $\theta_1\sigma=\sigma\theta_1$.

\end{proof}

\begin{corollary}
    Every real Lie algebra has a Cartan involution.
\end{corollary}

\begin{proof}
Let $\lF$ be a real Lie algebra and $\lG$ be his complexification: $\lG=\lF\heC$. Let $\lU_0$ be a compact real form of $\lG$ and $\tau$ the induced involution (the conjugation) on $\lG$. By the proposition~\ref{prop:conj_invol}, we know that $\tau$ is  a Cartan involution of $\lG\heR$. We also consider $\sigma$, the involution of $\lG$ with respect to the real form $\lF$. It is in particular an involution on the real Lie algebra $\lF$. Then one can find a $\varphi\in\Int\lG\heR$ such that $[\varphi\tau\varphi^{-1},\sigma]=0$ on $\lG\heR$. Let $\lU_1=\varphi\lU_0$ and $X\in\lU_1$. We can write $X=\varphi Y$ for a certain $Y\in\lU_0$. Then
\[
   \varphi\tau\varphi^{-1} X=\varphi\tau Y=\varphi Y=X,
\]
so that $\varphi\tau\varphi^{-1}=id|_{\lU_1}$. Note that $\lU_1$ is also a real compact form of $\lG$ because the Killing form is not affected by $\varphi$. Let $\tau_1$ be the involution of $\lG$ induced by $\lU_1$. We have
\[
   \tau_1|_{\lU_1}=\varphi\tau\varphi^{-1}_{\lU_1}=\id|_{\lU_1}.
\]
Since $\varphi$ is $\eC$-linear, we have in fact $\tau_1=\varphi\tau\varphi^{-1}$. Now we forget $\lU_0$ and we consider the compact real form $\lU_1$ with his involution $\tau_1$ of $\lG$ which satisfy $[\tau_1,\sigma]=0$ on $\lG\heR$ This relation holds also on $i\lG\heR$, then
\[
   [\tau_1,\sigma]=0
\]
on $\lG=\lF\heC$. Let $X\in\lF$, i.e. $\sigma X=X$; it automatically fulfils
\[
  \sigma\tau_1 X=\tau_1\sigma X=\tau_1 X,
\]
so that $\tau_1$ restrains to an involution on $\lF$ (because $\tau_1\lF\subset\lF$). Let $\theta=\tau_1|_{\lF}$. For $X$, $Y\in\lF$, we have
\begin{equation}
B_{\theta}(X,Y)=-B_{\lF}(X, \theta Y)
             =-B_{\lF}(X,\tau Y)
         =\frac{1}{2}(B_{\lG\heR})_{\tau_1}(X,Y),
\end{equation}
which shows that $\theta$ is a Cartan involution. The half factor on the last line comes from the fact that $\lG\heR=(\lF\heC)\heR=\lF\oplus i\lF$.

\end{proof}

\subsection{Cartan decomposition}
%-------------------------------

Examples of Cartan and Iwasawa decomposition are given in sections~\ref{SecToolSL},~\ref{SubSecIwaSOunn},\ref{subsecIwasawa_un},~\ref{SecSympleGp} and~\ref{SecIwasldeuxC}. An example of how it works to prove isomorphism of Lie algebras is provided in subsection~\ref{sssIsomsoslplussl}.

Let $\lF$ be a real semisimple Lie algebra. A vector space decomposition $\lF=\lK\oplus\lP$ is a \defe{Cartan decomposition}{Cartan!decomposition} if the Killing form is negative definite on $\lK$ and positive definite on $\lP$ and the following commutators hold:
\begin{equation}\label{eq:comm_Cartan}
   [\lK,\lK]\subseteq\lK,\quad[\lK,\lP]\subseteq\lP,\quad[\lP,\lP]\subseteq\lK.
\end{equation}
If $X\in\lK$ and $Y\in\lP$, we have $(\ad X\circ\ad Y)\lK\subseteq\lP$ and $(\ad X\circ\ad Y)\lP\subseteq\lK$, therefore $B_{\lF}(X,Y)=0$.

Let $\dpt{\theta}{\lF}{\lF}$ be a Cartan involution, $\lK$ its $+1$ eigenspace and $\lP$ his $-1$ one. It is easy to see that the relations \eqref{eq:comm_Cartan} are satisfied for the decomposition  $\lF=\lK\oplus\lP$. For example, for $X,X'\in\lK$, using the fact that $\theta$ is an automorphism,
\[
   [X,X']=[\theta X,\theta X']=\theta[X,X'],
\]
which proves that $[\lK,\lK]\subseteq\lK$. Since $\theta$ is a Cartan involution, $B_{\theta}$ is positive definite. For $X\in\lK$,
\[
  B(X,X)=B(X,\theta X)=-B_{\theta}(X,X)
\]
proves that $B$ is negative definite on $\lK$; in the same way we find that $B$ is also positive definite on $\lP$. Then the Cartan involution gives rise to a Cartan decomposition. We are going to prove that any Cartan decomposition defines a Cartan involution.

Let us now do the converse. Let $\lF=\lK\oplus\lP$ be a Cartan decomposition of the real semisimple Lie algebra $\lF$. We define $\theta=\id|_{\lK}\oplus(-\id)|_{\lP}$. If $X,X'\in\lK$, the definition of a Cartan algebra makes $[X,X']\in\lK$ and so
\[
  \theta[X,X']=[X,X']=[\theta X,\theta X'],
\]
and so on, we prove that $\theta$ is an automorphism of $\mF$. It remains to prove that $B_{\theta}$ is positive definite. If $X\in\lK$,
\[
   B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X).
\]
Then $B_{\theta}$ is positive definite on $\lK$ because on this space, $B$ is negative definite by definition of a Cartan involution. The same trick shows that $B_{\theta}$ is also positive definite on $\lP$. We had seen that $\lP$ and $\lK$ where $B_{\theta}$-orthogonal spaces. Thus $B_{\theta}$ is positive definite and $\theta$ is a Cartan involution.

Let $\lF=\lK\oplus\lP$ be a Cartan decomposition. Then it is quite easy to see that $\lK\oplus i\lP$ is a compact real form of $\lG=(\lFeC)$.

\begin{proposition}
Let $\lL$ and $\lQ$ be the $+1$ and $-1$ eigenspaces of an involution $\sigma$. Then $\sigma$ is a Cartan involution if and only if $\lL\oplus i\lQ$ is  a compact real form of $\lFeC$.
\end{proposition}

\begin{proof}
First remark that $\lL\oplus i\lQ$ is always a real form of $\lFeC$. The direct sense is yet done. Then we suppose that $B_{\lFeC}$ is negative definite on $\lL\oplus i\lQ$ and we have to show that $\lL\oplus\lQ$ is a Cartan decomposition of $\lF$. The condition about the brackets on $\lL$ and $\lQ$ is clear from their definitions. If $X\in\lL$, $B(X,X)<0$ because $B$ is negative definite on $\lL$. If $Y\in\lQ$, $B(Y,Y)=-B(iY,iY)>0$ because $B$ is negative definite on $i\lQ$.
\end{proof}

\section{Root spaces in the real case}
%----------------------------------------

Let $\lF$ be a real semisimple Lie algebra with a Cartan involution $\theta$ and the corresponding Cartan decomposition $\lF=\lK\oplus\lP$. We consider $B$, a ``Killing like''{} form, i.e. $B$ is a symmetric nondegenerate invariant bilinear form on $\lF$ such that $B(X,Y)=B(\theta X,\theta Y)$ and $B_{\theta}:=-B(X,\theta X)$ is positive definite. Then $B$ is negative definite on the compact real form $\lK\oplus i\lP$. Indeed if $Y\in\lP$,
\begin{equation}
  B(iY,iY)=-B(\theta Y,\theta Y)
          =B(Y,\theta Y)
      =-B_{\theta}(Y,Y)<0.
\end{equation}
The case with $X\in\lK$ is similar. It is easy to see that $B_{\theta}$ is in fact a scalar product on $\lF$, so that we can define the orthogonality and the adjoint from $B_{\theta}$. If $\dpt{A}{\lF}{\lF}$ is an operator on $\lF$, his adjoint is the operator $A^*$ given by the formula
\[
   B_{\theta}(A X,Y)=B_{\theta}(X,A^*Y)
\]
for all $X$, $Y\in\lF$.

\begin{proposition}
With this definition, when $X\in\lF$, the adjoint operator of $\ad X$ is given by means of the Cartan involution:
\[
(\ad X)^*= \ad(\theta X).
\]
\end{proposition}

\begin{proof}
This is a simple computation
\begin{equation}
B_{\theta}\big(  (\ad\theta X)Y,Z \big)=-B\big(  Y,[\theta X,\theta Y]  \big)
                                     =-B_{\theta}(Y,[X,Z])
                     =-B_{\theta}\big( (\ad X)^*Y,Z \big).
\end{equation}
\end{proof}

Let $\lA$ be a maximal abelian subalgebra of $\lP$ (the existence comes from the finiteness of the dimensions). If $H\in\lA$, the operator $\ad H$ is self-adjoint because
\begin{equation}
(\ad H)^*X=(-\ad\theta H)X
          =[H,X]
      =(\ad H)X,
\end{equation}
where we used the fact that $H\in\lP$.  For $\lambda\in\lA^*$, we define the space
\begin{equation}
  \lF_{\lambda}=\{ X\in\lF\tq\forall H\in\lA,\, (\ad H)X=\lambda(H)X\}.
\end{equation}
If $\lF_{\lambda}\neq 0$ and $\lambda\neq 0$, we say that $\lambda$ is a \defe{restricted root}{restricted root (real case)}\index{root!restricted (real case)} of $\lF$. We denote by $\Sigma$ the set of restricted roots of $\lF$. We may sometimes write $\Sigma_{\lF}$ if the Lie algebra is ambiguous.

The main properties of the real root spaces are given in the following proposition.

\begin{proposition}     \label{PropPropRacincesReelles}
The set $\Sigma$ of the restricted roots of a real semisimple Lie algebra $\lF$ has the following properties:
\label{prop:enuc}
\begin{enumerate}
\item\label{enuci} $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$,
\item\label{enucii} $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$,
\item\label{enuciii} $\theta\lF_{\lambda}=\lF_{-\lambda}$,
\item\label{enuciv} $\lambda\in\Sigma$ implies $-\lambda\in\Sigma$,
\item\label{enucv} $\lF_0=\lA\oplus\lM$ where $\lM=\mZ_{\lK}(\lA)$ and $\lA\perp\lM$.
\end{enumerate}
\end{proposition}

\begin{proof}
Proof of~\ref{enuci}. The operators $\ad H$ with $H\in\lA$ form an abelian algebra of self-adjoint operators, then they are simultaneously diagonalisable. Let $\{X_i\}$ be a basis which realize this diagonalisation, and $\lF_i=\Span X_i$, so that $\lF=\oplus_i\lF_i$. We have $(\ad H)\lF_i=\lF_i$ and then $(\ad H)X_i=\lambda_i(H)X_i$ for a certain $\lambda_i\in\lA^*$. This shows that $\lF_i\subseteq\lF_{\lambda_i}$.\quext{pourquoi ça n'implique pas que $\dim\lF_{\lambda_i}=1$? Réponse par Philippe: tu as oublié les valeurs propres nulles  dans ta base ce qui doit entrainer quelques modifs dans ton texte(par  ex.  $adH f_i = f_i$ pas toujours ) }

Proof of~\ref{enucii}. Let $H\in\lA$, $X\in\lF_{\lambda}$ and $Y\in\lF_{\mu}$. We have
\begin{equation}
   (\ad H)[X,Y]=[[H,X],Y]+[X,[H,Y]]
               =\big(  \lambda(H)+\mu(H) \big) [X,Y].
\end{equation}

Proof of~\ref{enuciii}. Using the fact that $\theta H=-H$ because $H\in\lP$,
\begin{equation}
  (\ad H)\theta X=\theta[\theta H,X]
                 =-\theta\lambda(H)X
         =-\lambda(H)(\theta X).
\end{equation}

Proof of~\ref{enuciv}. It is a consequence of~\ref{enuciii} because if $\lF_{\lambda}\neq 0$, then $\theta\lF_{_{\lambda}}\neq 0$.

Proof of~\ref{enucv}. By~\ref{enuciii}, $\theta\lF_0=\lF_0$, then $\lF_0=(\lK\cap\lF_0)\oplus(\lP\cap\lF_0)$. If $X\in\lF_0$, then it commutes with all the elements of $\lA$ and by the maximality property of $\lA$, provided that $X\in\lP$, it also must belongs to $\lA$. This fact makes $\lA=\lP\cap\lF_0$. Now,
\[
  \lM=\mZ_{\lK}(\lA)=\{X\in\lK\tq [X,\lA]=0\}=\lK\cap\lF_0.
\]
All this gives $\lF_0=\mZ_{\lK}(\lA)\oplus\lA$.
\end{proof}

We choose a positivity notion on $\lA^*$, we consider $\Sigma^+$, the set of restricted positive roots and we define\nomenclature{$\lN$}{Restricted roots}
\[
  \lN=\bigoplus_{\lambda\in\Sigma^+}\lF_{\lambda}.
\]

From finiteness of the dimension, there are only a finitely many forms $\lambda\in\lA^*$ such that $\lF_{\lambda}\neq 0$. Then, taking, more and more commutators in $\lN$, the formula $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$ shows that the result finish to fall into a $\lF_{\mu}=0$. On the other hand, since $\lA\subset\lF_0$, we have $[\lA,\lN]=\lN$. If $a_1,a_2\in\lA$ and $n_1,n_2\in\lN$,
\begin{equation}
   [a_1+n_1,a_2+n_2]=\underbrace{[a_1,a_2]}_{=0}+\underbrace{[a_1,n_2]}_{\in\lN}
                      \quad+\underbrace{[n_1,a_2]}_{\in\lN}+\underbrace{[n_1,n_2]}_{\in\lN},
\end{equation}
then $[\lA\oplus\lN,\lA\oplus\lN]=\lN$. This proves the three following important properties:

\begin{enumerate}
\item $\lN$ is nilpotent.
\item $\lA$ is abelian.
\item $\lA\oplus\lN$ is a solvable Lie subalgebra of $\lF$.
\end{enumerate}

\subsection{Iwasawa decomposition}
%----------------------------------

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra and $\lK$, $\lA$, $\lN$ as before. Then we have the following direct sum:
\begin{equation}
   \lF=\lK\oplus\lA\oplus\lN.
\end{equation}
\end{theorem}

This is the \defe{Iwasawa decomposition}{Iwasawa!decomposition}\index{decomposition!Iwasawa} for the real semisimple Lie algebra $\lF$.

\begin{proof}
We yet know the direct sum $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$. Roughly speaking, in $\lN$ we have only vectors of $\Sigma^+$, in $\theta\lN$, only of $\Sigma^-$ and in $\lA$, only in ``zero''. Then the sum $\lA\oplus\lN\oplus\theta\lN$ is direct.

Now we prove that the sum $\lK+\lA+\lN$ is also direct. It is clear that $\lA\cap\lN=0$ because $\lA\subseteq\lF_0$. Let $X\in\lK\cap(\lA\oplus\lN)$. Then $\theta X=X$. But $\theta X\in\lA\oplus\theta\lN$. Thus $X\in\lA\oplus\lN\cap\lA\oplus\lN$ which implies $X\in\lA$. All this makes $X\in\lP\oplus\lK$ and $X=0$.

Now we prove that $\lK\oplus\lA\oplus\lN=\lF$. An arbitrary $X\in\lF$ can be written as
\[
   X=H+X_0+\sum_{\lambda\in\Sigma}X_{\lambda}
\]
where $H\in\lA$, $X_0\in\lM$ and $X_{\lambda}\in\lF_{\lambda}$. Now there are just some manipulations\ldots
\begin{equation}
  \sum_{\lambda\in\Sigma}X_{\lambda}=\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+X_{\lambda})
                                  =\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})
                  +\sum_{\lambda\in\Sigma^+}(X_{\lambda}+\theta X_{-\lambda}),
\end{equation}
but $\theta(X_{-\lambda}+\theta X_{-\lambda})=X_{-\lambda}+\theta X_{-\lambda}$, then $X_{-\lambda}+X_{-\lambda}\in\lK$. Moreover, $X_{\lambda}, \theta X_{-\lambda}\in\lF_{\lambda}$, then $X_{\lambda}-\theta X_{-\lambda}\in\lF_{\lambda}\subseteq\lN$. Then
\begin{equation}
  X=X_0+\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})+H+\sum_{\lambda\in\Sigma^+}(X_{\lambda}-\theta X_{-\lambda})
\end{equation}
where the two first term belong to $\lK$, $H\in\lA$ and the last term belongs to $\lN$.
\end{proof}

\begin{lemma}
There exists a basis $\{X_i\}$ of $\lF$ in which

\begin{enumerate}
\item\label{enudi} The matrices of $\ad\lK$ are symmetric,
\item\label{enudii} The matrices of $\ad\lA$ are diagonal and real,
\item\label{enudiii} The matrices of $\ad\lN$ are upper triangular with zeros on the diagonal.
\end{enumerate}
\end{lemma}

\begin{proof}
We have the orthogonal decomposition $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$ given by proposition~\ref{prop:enuc}. Let $\{X_i\}$ be an orthogonal basis of $\lF$ compatible with this decomposition and in such an order that $i<j$ implies $\lambda_i\geq\lambda_j$. From the orthogonality of the basis it follows that the matrix of $B_{\theta}$ is diagonal. Thus the adjoint is the transposition.

\ref{enudi} If $X\in\lK$, $(\ad X)^t=(\ad X)^*=-\ad\theta X=-\ad X$.

\ref{enudii} Each $X_i$ is a restricted root; then $(\ad H)X_i=\lambda_i(H)X_i$, then the diagonal of $\ad H$ is made of $\lambda_i(H)$ whose are real.

\ref{enudiii} If $Y_i\in\lF_{\lambda_i}$ with $\lambda_i\in\Sigma^+$, $(\ad Y_i)X_j$ has only components in $\lF_{\lambda_i+\lambda_j}$ with $\lambda_i+\lambda_j>\lambda_j$ because $\lambda_i\in\Sigma^+$.
\end{proof}


\begin{lemma}
Let $\lH$ be a subalgebra of the real semisimple Lie algebra $\lF$. Then $\lH$ is a Cartan subalgebra if and only if $\lHeC$ is Cartan in $\lFeC$.
\end{lemma}

\begin{proof}
\subdem{Direct sense} If $\lH$ is nilpotent in $\lF$, it is cleat that $\lHeC$ is nilpotent in $\lFeC$. We have to prove that $[x,\lHeC]\subseteq\lHeC$ implies $x\in\lHeC$. As set, $\lFeC=\mF\oplus i\lF$  (but not as vector space!), then we can write $x=a+ib$ with $a$, $b\in\lF$. The assumption makes that for any $h\in\lH$, there exists $h',h''\in\lH$ such that
\[
   [a+ib,h]=h+ih''.
\]
This equation can be decomposed in $\lF$-part and $i\lF$-part: for any $h\in\lH$, there exists a $h'\in\lH$ such that $[a,h]=h'$,  and for any $h\in\lH$, there exists a $h''\in\lH$ such that $[b,h]=h''$. Thus $a$, $b\in\lH$ because $\lH$ is Cartan in $\lF$.

\subdem{Inverse sense} The assumption is that $[x,\lHeC]\subset\lHeC$ implies $x\in\lHeC$. In particular consider a $x\in\lH$ such that $[x,\lH]\subset\lH$. Then $x\in\lHeC$ because $[x,\lHeC]\subset\lHeC$. But $\lHeC\cap\lF=\lH$.
\end{proof}

In the complex case, the Cartan subalgebras all have same dimensions because they are maximal abelian.

\section{The group \texorpdfstring{$SL(2,\eR)$}{SL2R} and its algebra}  \label{SecToolSL}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The study of \( \SL(2,\eR)\) and \( \gsl(2,\eC)\) is required before to go further in the general study because of proposition~\ref{PropScalrooTsQ} that will reduce the study of genera Lie algebras into combinations of \( \gsl(2,\eC)\) algebras.

\subsection{Iwasawa decomposition}
%----------------------------------
\index{Iwasawa!decomposition!of $SL(2,\eR)$}

Let $G=\SL(2,\eR)$ the group of $2\times 2$ matrices with unit determinant. The Lie algebra $\lG=\gsl(2,\eR)$ is the algebra of matrices with vanishing trace:
\begin{equation}
 \lG =  \{ X\in\End(\eR^2)\tq \tr(X) = 0\}
=\left\{ \begin{pmatrix}
x & y \\
z & -x
\end{pmatrix}\textrm{ with }x,y,z\in\eR  \right\}.
\end{equation}
The following elements will be intensively used:
\begin{equation}    \label{EqsXPdnZlG}
H=\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
,\quad
  E=\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
,\quad
 F=\begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix},
\quad
T=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}
\end{equation}
where $T=E-F$ has been introduced for later convenience. The commutators are
\begin{subequations}\label{EqTableSLdR}
\begin{align}
  [H,E]&=2E &[T,H]&=-2T  \\
  [H,F]&=-2F    &[T,E]&=H   \\
  [E,F]&=H  &[T,F]&=H.
\end{align}
\end{subequations}
The exponentials can be easily computed and the result is
\begin{align}               \label{EqExpMatrsSLdeuxR}
 e^{tH}=
\begin{pmatrix}
   e^{t}    &   0   \\
  0 &    e^{-t}
\end{pmatrix},
&&
 e^{tE}=
\begin{pmatrix}
  1 &   t   \\
  0 &   1
\end{pmatrix},
&&
 e^{tF}=
\begin{pmatrix}
  1 &   0   \\
  t &   1
\end{pmatrix}.
\end{align}
Notice that the sets $\{ H,E,F \}$, $\{ H,E,F \}$ and $\{ H,E+F,T \}$ are basis. A Cartan involution is given by $\theta(X)=-X^t$, and the corresponding Cartan decomposition is
\begin{align}
   \lK&=\Span\{ T \},
&\lP&=\Span\{ H,E+F \}.
\end{align}
Indeed, we are in a matrix algebra, then $\tr(XY)$ is proportional to $\tr(\ad X\circ \ad Y)$.
 In order to see that $\theta$ is a Cartan involution, we have to prove that $B|_{\lK\times\lK}$ is negative definite and $B|_{\lP\times\lP}$ positive. It is true because for $X\in\lK$,
\[
    \tr(\ad X\circ \ad X)=\tr(XX)=\tr\begin{pmatrix}
-x^2 & 0 \\
0 & -x^2
\end{pmatrix}<0,
\]
and for $Y\in\lP$,
\[
    \tr(YY)=\tr\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix}\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix} =\tr\begin{pmatrix}
x^2+y^2 & 0 \\
0 & x^2+y^2
\end{pmatrix} >0.
\]

Up to some choices, the Iwasawa decomposition\label{pg_iwasldr} of the group $\SL(2,\eR)$ is given by the exponentiation of $\lA$, $\lN$ and~$\lK$
\begin{equation}
\begin{aligned}
  \lA&=\Span\{ H \}
&\lN&=\Span\{ E \}
&\lK&=\Span\{T\},
\end{aligned}
\end{equation}
so that
\begin{equation}\label{eq:expo_ANK}
A=\begin{pmatrix}
e^a & 0 \\
0 & e^{-a}
\end{pmatrix}\quad
N=\begin{pmatrix}
1 & l \\
0 & 1
\end{pmatrix}\quad
K=\begin{pmatrix}
\cos k & \sin k \\
-\sin k & \cos k
\end{pmatrix}.
\end{equation}

A common parametrization of $AN$ by $\eR^2$ is provided by
\begin{equation}   \label{EqParmalSL}
(a,l)=
\begin{pmatrix}
  e^a&le^a\\
  0  &e^{-a}
\end{pmatrix}.
\end{equation}
One immediately has the following formula for the left action of $AN$ on itself:
\[
  L_{(a,l)}(a',l')=\begin{pmatrix}
e^{a+a'} & e^{a+a'}l'+e^{a-a'}l \\
0 & e^{-a-a'}
\end{pmatrix}=(a+a',l'+e^{-2a'}l).
\]
In this setting, the inverse is given by $(a,l)^{-1}=(-a,-l e^{2a})$.

Let's give some formulas in $\SL(2,\eR)$. Using corollary~\ref{Ad_e} and exponentiating commutation relations,
\begin{subequations}  \label{eq_eaHsldr}
\begin{align}
\Ad(e^{aH})E&=e^{2a}E,\\
\Ad(e^{aH})F&=e^{-2a}F,\\
\Ad( e^{aH})T&= e^{2a}E- e^{-2u}E+ e^{-2u}T\\
\Ad(e^{tE})H&=H-2tE,                            \label{eq_AdetE}\\
\Ad( e^{tE})T&=-tH+(t^2+1)E-E+T\\
\Ad( e^{tT})H&=\cos(2t)H+\sin(2t)(2E-T)\\
\Ad( e^{tT})E&=\frac{ 1 }{2}\Big( \sin(2t)H+\cos(2t)(2E-T)+T \Big)
\end{align}
\end{subequations}
where $z$ belongs to the center: $z=\pm\mtu$.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{A companion: \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

We can consider the Iwasawa decomposition which is $\theta$-conjugated to the $AN$ that we just saw. That decomposition is generated by
\begin{equation}
    \bar\lN=\begin{pmatrix}
    0   &   0   \\
    1   &   0
\end{pmatrix}.
\end{equation}
The exponentiation produces
\begin{equation}
    \bar N=\begin{pmatrix}
    1   &   0   \\
    t   &   1
\end{pmatrix},
\end{equation}
and the Iwasawa group is given by
\begin{equation}        \label{EqGeneANbarSLdeuxR}
    A\bar N=\begin{pmatrix}
    e^a &   0   \\
    l e^{-a}    &    e^{-a}
\end{pmatrix}.
\end{equation}

\subsection{Killing form}
%------------------------

In the basis $\{ H,E,T \}$, the adjoint operators are given by
\[
\ad H=\begin{pmatrix}
 0 & 0 &-2 \\
 0 & 0 &0 \\
 0 & 2 &2
\end{pmatrix},
\ad E=\begin{pmatrix}
 0 & 0 &0 \\
 0 & 0 &-1\\
-2 & 0&0
\end{pmatrix},\textrm{ and }
  \ad T=\begin{pmatrix}
 2 & 0 &0 \\
0 & 1 &0\\
-2 & 0&0
\end{pmatrix}.
\]
so that the Killing form can be computed directly from definition $B(X,Y)=\tr(\ad X,\ad Y)$. The result is
\begin{subequations}
\begin{align}
B(T,H)&=0  & B(H,H)&=8\\
B(T,E)&=-4 & B(E,E)&=0\\
B(H,E)&=0  &  B(T,T)&=-4.
\end{align}
\end{subequations}
Expressed in the basis $\{H,E,F\}$, the matrix of the Killing form reads
\begin{equation}
B=
\begin{pmatrix}
8&&\\
&&4\\
&4&
\end{pmatrix}
\end{equation}
while, in the basis  $\{H,E+F,T\}$, we find
\begin{equation}   \label{EqBHEFTsldR}
B=
\begin{pmatrix}
8\\
&8\\
&&-8
\end{pmatrix}.
\end{equation}
The latter is the reason of the name of the vector $T$: the sign of its norm is different, so that $T$ is candidate to be a time-like direction.

\subsection{Abstract root space setting}
%---------------------------------------

Looking on the table \eqref{EqTableSLdR} from an abstract point of view, we see that $E$ and $F$ are eigenvectors of $\ad(H)$ with eigenvalues $2$ and $-2$. So $\lA=\lG_0=\eR H$; $\lG_2=\eR E$; and $\lG_{-2}=\eR F$. Using a more abstract notation, the table of $\SL(2,\eR)$ becomes
\begin{subequations}  \label{subeq_rootSLR}
\begin{align}
  [A_{0},A_{2}]&=2A_{2}\\
    [A_{0},A_{-2}]&=-2A_{-2}\\
    [A_{2},A_{-2}]&=A_{0}.
\end{align}
\end{subequations}

\subsection{Isomorphism}
%-----------------------

As pointed out in the chapter II, \S6 of \cite{Knapp_reprez}, the map (seen as a conjugation in $\SL(2,\eC)$)
\begin{equation}
    \begin{aligned}
        \psi\colon \SU(1,1)&\to \SL(2,\eR) \\
        U&\mapsto AUA^{-1}
    \end{aligned}
\end{equation}
with $A=\begin{pmatrix}
1&i\\i&1
\end{pmatrix}$ is an isomorphism between $\SL(2,\eR)$ and $\SU(1,1)$.

%---------------------------------------------------------------------------------------------------------------------------
                    \section{The complex algebra \texorpdfstring{$\protect\gsl(2,\eC)$}{sl2C} and its representations}
%---------------------------------------------------------------------------------------------------------------------------
\label{SecsldeuxCandrepres}

The book \cite{Kassel} contains the representations of \( \gsl(2,\eC)\).

The algebra $\gsl(2,\eC)$ is the complex algebra of complex $2\times 2$ matrices with vanishing trace. As generating matrices, one can take the elements $u_i$ of \eqref{EqGenssudeux} and complete them by
\begin{align*}
v_1&=\frac{ 1 }{2}
\begin{pmatrix}
  -1    &   0   \\
  0 &   1
\end{pmatrix},
&v_2&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   i   \\
  -i    &   0
\end{pmatrix},
&v_3&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   -1  \\
  -1    &   0
\end{pmatrix}
\end{align*}
which satisfy the commutation relations
\begin{subequations}
\begin{align}
    [v_i,v_j]&=-\epsilon_{ikj}u_k\\
    [v_i,u_j]&=\epsilon_{ikj}v_k.
\end{align}
\end{subequations}

\begin{remark}
    This is not the algebra \( \gsl(2,\eC)\) used in physics. The latter is the \emph{four}-dimensional \emph{real} algebra of trace vanishing \( 2\times 2\) complex matrices. There is one more generator and the representation theory is different. Moreover the physics works with the \emph{group} instead of the \emph{algebra}.
\end{remark}

The change of basis
\begin{align}
    x_j&=\frac{ 1 }{2}(u_j+iv_j),   &y_j&=\frac{ 1 }{2}(u_j-iv_j)
\end{align}
provides the simplification
\begin{align}
[x_i,x_j]&=\epsilon_{ijk}x_k    &[y_i,y_j]&=\epsilon_{ijk}y_k   &[x_i,y_j]&=0,
\end{align}
so that, as algebras, we have the isomorphism
\begin{equation}
    \gsl(2,\eC)=\gsu(2)\oplus\gsu(2).
\end{equation}
Thus the representation theory of $\gsl(2,\eC)$ is determined by the one of $\gsu(2)$.


\index{representation!of $\gsl(2,\eC))$}
Consider the space $\mP_m$ of homogeneous polynomials of degree $m$ in two variables with complex coefficients[\cite{GpAlgLie_Faraut}]. The dimension of $\mP_m$ is $m+1$ and we have the following representation of $\SL(2,\eC)$ thereon:
\begin{equation}
    \big( \pi_m(g)f \big)(u,v)=f\big(
g
\begin{pmatrix}
u\\v
\end{pmatrix}
 \big)
=
f(au+bv,cu+dv)
\end{equation}
if $g=\begin{pmatrix}
  a &   b   \\
  c &   d
\end{pmatrix}$. We are going to determine the corresponding representation $\rho_m$ of the Lie algebra $\gsl(2,\eC)$ as algebra over complex numbers.

A basis of $\gsl(2,\eC)$ over $\eC$ is given by the matrices $\{ H,E,F \}$ given in equation \eqref{EqsXPdnZlG} and are subject to the commutation relations \eqref{subEqsSBhuAWx}.




Using the exponentiation \eqref{EqExpMatrsSLdeuxR}, we find
\[
    \big( \pi_m( e^{tH})f \big)(u,v)=f( e^{t}u, e^{-t}v),
\]
so that
\begin{equation}
    \big( \rho_m(H)f \big)(u,v)=u\frac{ \partial f }{ \partial u }-v\frac{ \partial f }{ \partial v }.
\end{equation}
In the same way, we find
\begin{equation}
    \big( \rho_m(E)f \big)(u,v)=\Dsdd{ \big( \pi_m( e^{tE})f \big)(u,v) }{t}{0}=v\frac{ \partial f }{ \partial u },
\end{equation}
and
\begin{equation}
     \big( \rho_m(F)f \big)(u,v)=u \frac{ \partial f }{ \partial v }.
\end{equation}
A natural basis of $\mP_m$ is given by the monomials $f_j(u,v)=u^jv^{m-j}$ with $j=0,\ldots,m$. The representation $\rho_m$ on this basis reads
\begin{equation}        \label{EqReprezgsldeuxC}
\begin{split}
    \rho_m(H)f_j&=(2j-m)f_j\\
    \rho_m(E)f_j&=(m-j)f_{j+1}\\
    \rho_m(F)f_j&=jf_{j-1}.
\end{split}
\end{equation}

\begin{proposition}     \label{ProprhomirredsldeuxC}
The representation $\rho_m$ is irreducible.
\end{proposition}

\begin{proof}
Let $W\neq\{ 0 \}$ be an invariant subspace of $\mP_m$. If $p\in W$, from invariance, $\rho_m(H)(p)\in W$. If $p$ is a linear combination of $\{ f_j \}_{j\in I}$ ($I\subseteq \{ 0,\ldots m \}$), then $\rho_m(H)p$ is still a linear combination $q$ of elements in the same set. Thus there exists a linear combination of $p$ and $\rho_m(H)p$ which is a linear combination of $\{ f_j \}_{j\in J}$ with $J\subset I$ (strict inclusion). Using the same trick with $q$ and $\rho_m(H)q$, we still reduce the number of basis elements. Proceeding in the same way at most $m$ times, we find that one of the $f_j$ belongs to $W$. From there, acting with $\rho_m(E)$ and $\rho_m(F)$, one generates the whole $\mP_m$. That proves that $W=\mP_m$ and thus that $\rho_m$ is irreducible.
\end{proof}

\begin{theorem}[\cite{GpAlgLie_Faraut}]
Every $\eC$-linear irreducible finite dimensional representation of $\gsl(2,\eC)$ is equivalent to one of the $\rho_m$.
\end{theorem}

These results will be proved also in the quantum case in theorems~\ref{ThoVfintemofdsldcun} and~\ref{ThoVfintemofdslddeux}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{The group \texorpdfstring{$\SO(3)$}{SO3} and its Lie algebra}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SubSecTheGroupSotrois}

\begin{proposition}[\cite{WormerAngular}]
An element of $\SO(3)$ has exactly one eigenvector with eigenvalue $1$. That vector is the \defe{rotation axis}{axis!of rotation in $\SO(3)$}.
\end{proposition}

The generator of rotation around the axis $n$ (unit vector) is given by the matrix
\begin{equation}
\begin{pmatrix}
  0 &   -n_3    &   n_2\\
  n_3   &   0   &   -n_1\\
 -n_2   &   n_1 & 0
\end{pmatrix}.
\end{equation}
That form results form the requirement that $Nr=n\times r$. If we denote by $R(n,\theta)$ the operator of rotation in $\eR^3$ by an angle $\theta$ around the axis $n$, one shows that
\begin{equation}
    R(b,\theta)=\mtu+\sin(\theta) N+\big(1-\cos(\theta)\big)N^2.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsection{Rotations of functions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Consider any function $f\colon \eR^3\to \eC$; we define the \defe{rotation operator}{rotation!on functions} $U(n,\theta)$\nomenclature{$U(n,\theta)$}{Rotation operator on functions} by
\begin{equation}        \label{EqRotFunSOtrois}
    \big( U(n\theta)f \big)(r)=f\big( R(n,\theta)^{-1}r \big).
\end{equation}
These operators form a group, and we have in particular that
\[
    U(n,\theta_1)U(n,\theta_2)=U(n,\theta_1+\theta_2).
\]
We are interested in \emph{infinitesimal} rotations, that is rotations of angle $d\theta$ for which $(d\theta)^2\ll d\theta$, or in other words, we are interested in a development of equation \eqref{EqRotFunSOtrois} restricted to linear terms in $\theta$. What one obtains is
\begin{equation}
    \big( U(n,d\theta)f \big)(r)=\big( (1-i d\theta\, n\cdot l)f\big)(r)
\end{equation}
where the operator $l$ is defined by
\begin{equation}
    l=-ir\times\nabla.
\end{equation}
Its components $l_i=-i\epsilon_{ijk}r_j\partial_k $ satisfy commutation relations
\begin{equation}    \label{EqAldllepsl}
    [l_i,l_j]=i\epsilon_{ijk}l_k.
\end{equation}
The operator $n\cdot l$ is refereed as the \defe{generator of infinitesimal rotations}{generator!of infinitesimal rotations}. One can derive an expression of $U(n,\theta)$ in terms of $n\cdot l$ by the following:
\[
    U(n,\theta+d\theta)f=U(n,\theta)U(n,d\theta)f=U(n,\theta)(1-id\theta\, n\cdot l)f,
\]
so that we have the differential equation
\begin{equation}
    \frac{ dU }{ d\theta }(n,\theta)=-iU(n,\theta)n\cdot l
\end{equation}
with the initial condition $U(n,0)=1$. The solution is
\begin{equation}
    U(n,\theta)= e^{-i\theta\, n\cdot l}.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Verma module}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

When $\lG$ is a semisimple Lie algebra, we have the usual decomposition\cite{VermaPiercey}
\begin{equation}
    \lG=\lN^-\oplus\lH\oplus\lN^+,
\end{equation}
where each of the three components are Lie algebras. In particular, the universal enveloping algebra $\mU(\lN^-)$ makes sense. Let $\mu\in\lH^*$. We build a representation $\pi_{\mu}$ of $\lG$ on $V_{\mu}=\mU(\lN^-)$ in the following way
\begin{itemize}
\item If $Y_{\alpha}\in\lN^-$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(Y_{\alpha})1  &=Y_{\alpha}\\
        \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_n})&=Y_{\alpha}Y_{\alpha_1}\ldots Y_{\alpha_n},
    \end{align}
\end{subequations}
\item if $H\in\lH$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(H)1   &=\mu(H)\\
        \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_k})  &= \big( \mu(H)-\sum_{j=1}^k\alpha_j(H) \big)Y_{\alpha_1}\ldots Y_{\alpha_k},
    \end{align}
\end{subequations}
\item and if $X_{\alpha}\in\lN^+$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(X_{\alpha})1  &=0\\
        \pi_{\mu}(X_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k}    &=Y_{\alpha_1}\big( \pi_{\mu}(X_{\alpha})Y_{\alpha_2}\ldots Y_{\alpha_k} \big)\\
                                    &\quad  -\delta_{\alpha,\alpha_1}\sum_{j=1}^k\alpha_j(H_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k}.
    \end{align}
\end{subequations}
\end{itemize}
In the last one, we do an inductive definition.
\begin{lemma}
The couple $(\pi_{\mu},V_{\mu})$ is a representation of $\lG$ on $V_{\mu}$.
\end{lemma}
\begin{proof}
    No proof.
\end{proof}
That representation is one \defe{Verma module}{Verma module} for $\lG$. If the algebra $\lG$ is an algebra over the field $\eK$, the field $\eK$ itself is part of $\mU(\lN)^-$, so that the scalars are vectors of the representation. In that context, the multiplicative unit $1\in \eK$ is denoted by $v_0$.

\begin{theorem}
The representation $(\pi_{\mu},V_{\mu})$ of the semisimple Lie algebra $\lG$ is a cyclic module of highest weight, with highest weight $\mu$ and where $v_0$ is a vector of weight $\mu$.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
The Verma module is, \emph{a priori}, infinite dimensional and non irreducible, thus one has to perform quotients of the Verma module in order to build finite dimensional irreducible representations.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{Cyclic modules and representations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

An example over $\so(3)$ is given in subsection~\ref{subSubSecweightsotrois}. The case of $\so(5)$ is treated in subsection~\ref{SubSecsocinq}. Let $\lG$ be a semisimple Lie algebra with a Cartan subalgebra $\lH$ and a basis $\Delta$ for its roots $\Phi=\Phi^+\cup\Phi^-$. Let $W$ be a finite dimensional $\lG$-module.

\begin{lemma}
If $\lG$ is a nilpotent complex algebra and if $\gamma$ is a weight, then there exists a $v$ in $V_{\gamma}$ such that $c\cdot v=\gamma(x)v$ for every $x\in\lG$.
\end{lemma}
This is the proposition~\ref{prop:trois_poids}. Notice that a Cartan algebra is nilpotent, thus one has at least one vector of $W$ which is a common eigenvector of every elements of $\lH$, in other words, $\exists\mu\in\lH^*$ and $\exists w\in W$ such that
\begin{equation}
    hw=\mu(h)w
\end{equation}
for every $h\in\lH$, and $w\neq 0$. If $w$ is such and if $x\in\lG_{\alpha}$, we have
\begin{equation}
    (hx)\cdot w=[h,x]\cdot w+(xh)\cdot w=\alpha(h)x\cdot w+x\mu(h)w=(\alpha+\mu)(h)x\cdot w.
\end{equation}
If we define
\begin{equation}
    S=\{ w\in W\tq\exists\mu\in\lH^*\tq hw=\mu(h)w \},
\end{equation}
this is not a vector space, but the vector space $\Span S$ generated by $S$ is invariant under $\lG$ because $S$ itself is invariant under all the $\lG_{\alpha}$ with $\alpha\in\lG^*$.

On the other hand, we suppose that $\lG$ and $W$ are finite dimensional, so that their dual are isomorphic. Since a Cartan subalgebra is chosen, we have the decomposition
\begin{equation}
    \lG=\lH\oplus_{\alpha\in\lH^*}\lG_{\alpha}
\end{equation}
where $\lG_{\alpha}=\{ x\in\lG\tq [h,x]=\alpha(h)x\,\forall g\in\lH \}$. When $\alpha\in\lH^*$, the two following spaces are independent of the choice of the Cartan subalgebra $\lH$:
\begin{equation}
    \begin{aligned}
        W_{\alpha}  &=\{ v\in W \tq hv=\alpha(h)v\,\forall h\in\lH \}\\
        \lG_{\alpha}    &=\{ x\in\lG    \tq [h,x]=\alpha(h)x\,\forall h\in\lH \}.
    \end{aligned}
\end{equation}
If $v_{\alpha}\in W_{\alpha}$ and $x_{\beta}\in\lG_{\beta}$, we have
\begin{equation}
    h(x_{\beta}v_{\alpha})=\big( [h,x_{\beta}]+x_{\beta}h \big)v_{\alpha}=\big( \beta(h)+\alpha(h) \big)x_{\beta} v_{\alpha},
\end{equation}
so $x_{\beta}v_{\alpha}\in W_{\alpha+\beta}$. Thus $x_{\beta}$ is a map
\begin{equation}
    x_{\alpha}\colon W_{\alpha}\to W_{\alpha+\beta}.
\end{equation}
Since $W$ is finite dimensional, there exists a maximal $\alpha$ such that $W_{\alpha}\neq0$. We name it $\lambda$. For every $\beta\in\Phi^+$, we have $W_{\lambda+\beta}=\{ 0 \}$. In particular, if $v_{\lambda}\in W_{\lambda}$,
\begin{equation}
    x_{\alpha}x_{\lambda}=0
\end{equation}
for every $\alpha\in\Phi^+$, and, of course,
\begin{equation}
    hv_{\lambda}=\lambda(h)v_{\lambda}.
\end{equation}
On the other hand, for every vector $v\in W$, and for $v_{\lambda}$ in particular, the space $\mU(\lG)v$ is invariant, so
\begin{equation}
    W=\mU(\lG)v_{\lambda}
\end{equation}
by irreducibility. One say that $W$ is the \defe{cyclic module}{cyclic!module} generated by $v_{\lambda}$.


%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Choice of basis}
%---------------------------------------------------------------------------------------------------------------------------



\begin{theorem}     \label{ThoBaseUGxxmono}
    Let $\lG$ be a Lia algebra on a field of characteristic zero. If $\{ x_i \}$ is an ordered basis of $\lG$, then
    \begin{equation}
        \{ x_{i_1}\cdots x_{i_n}\tq i_1\leq\ldots\leq i_n \}
    \end{equation}
    is a basis for the universal enveloping algebra $\mU(\lG)$ of $\lG$.
\end{theorem}
One can find a proof in \cite{DirkEnvFiniteDimNilLieAlg}.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Roots and highest weight vectors}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropoIrrrgenffflamble}
An irreducible cyclic module is generated by the elements of the form $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$.
\end{proposition}

\begin{proof}
    From theorem~\ref{ThoBaseUGxxmono}, the monomials of the form
    \begin{equation}
        (f_1^{i_1}\cdots f_m^{i_m})\cdot (h_1^{j_1}\cdots h_l^{j_l})\cdot (e_1^{k_1}\cdots e_m^{k_m})
    \end{equation}
    form a basis of $\mU(\lG)$. When one act with such an element on $v_{\lambda}$, the $e_i$ kill it, while the $h_i$ do not act (a part of changing the norm). Thus, in fact, the module $W$ is generated by the only elements $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$
\end{proof}
In very short, one can write
\begin{equation}        \label{EqWnmoinvlambldarootmodul}
    W=(\lN^-)^nv_{\lambda}.
\end{equation}
Since $f_kv_{\alpha}\in\lG_{\alpha-\alpha_k}$, we have
\begin{equation}        \label{Eqfmlaphamoinsmouns}
    f_1^{i_1}\cdot f_m^{i_m}v_{\lambda}\in\lG_{\lambda-(i_m\alpha_m-\ldots i_1\alpha_1)}.
\end{equation}
The set of roots is ordered by
\begin{equation}
    \begin{aligned}
        \mu_1&\prec\mu_2    &   \text{iff}  &&  \mu_2-\mu_1&=\sum_i k_i\alpha_i
    \end{aligned}
\end{equation}
with $\alpha_i>0$ and with $k_i\in\eN$. Equation \eqref{Eqfmlaphamoinsmouns} means that
\begin{equation}
    \mu\prec\lambda
\end{equation}
for every weight $\mu$ of $W$.

\begin{definition}
Let $\lG$ be a finite dimensional Lia algebra. A \defe{cyclic module of highest weight}{module!highest weight} for $\lG$ is a module (not specially of finite dimension) in which there exists a vector $v_+$ such that $x_+v_+=0$ for every $x_+\in\lN^+$ and $hv_+=\lambda(h)v_+$ for every $h\in\lH$.
\end{definition}

\begin{proposition}
Every submodule of a cyclic highest weight module is a direct sum of weight spaces.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

From the relation $x_+v_+=0$, we know that all the weight spaces satisfy $V_{\mu}$ satisfy $\mu\prec\lambda$, and, since a module is the sum of all its submodules,
\begin{equation}        \label{EqVsumValpha}
    V=\bigoplus V_{\mu}.
\end{equation}
Notice that if $v_+$ is in a submodule, then that submodule is the whole $V$, thus the sum of two proper submodules is a proper submodule. We conclude that $V$ has an unique maximal submodule, and has thus an unique irreducible quotient.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dominant weight}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDomiunSei}

We know that every representation is defined by a highest weight. The following proposition shows that every root cannot be a highest weight of an irreducible representation.

\begin{proposition}[\cite{Anupam}]
    The highest weight of an irreducible representation of a simple complex Lie algebra is an integral dominant weight.
\end{proposition}

\begin{proof}
    Let \( \alpha_i\) be a simple root and consider the corresponding copy of \( \gsl(2,\eC)\) generated by \( \{ e_i,f_i,h_i \}\) (see proposition~\ref{PropWEzZYzC}). The following part of \( L(\Lambda)\) is a \( \gsl(2,\eC)_i\)-module:
    \begin{equation}
        V(\alpha_i)=\bigoplus_{n\in\eZ}V_{\Lambda+n\alpha_i}=V_{\Lambda}\oplus V_{\Lambda-\alpha_i}\oplus V_{\Lambda-2\alpha_i}\oplus\ldots\oplus V_{\Lambda-r\alpha_i}
    \end{equation}
    for some positive integer \( r\). Notice that the sum over \( n\in\eZ\) does not contain terms with \( n<0\) because \( \Lambda\) being an highest weight, \( V_{\Lambda+k\alpha_i}=\emptyset\) when \( k>0\). We know that in a \( \gsl(2,\eC)\)-module the eigenvalues of \( h\) run from \( -m\) to \( m\) (see equations \eqref{EqReprezgsldeuxC} for example). Thus here
    \begin{equation}
        \Lambda(h_i)=-(\Lambda-r\alpha_i)(h_i).
    \end{equation}
    By construction \( \alpha_i(h_i)=2\), so \( \Lambda(h_i)=r\) and the proof is finished.
\end{proof}

\begin{proposition}
    If \( \Lambda\) is the highest weight of the representation \( L(\Lambda)\) of the complex simple Lie algebra \( \lG\) and if \( w_0\) is the longest elements of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{proposition}

\begin{proof}
    First remember that whenever \( \lambda\) is a weight of a representation and \( w\) is an element of the Weyl group, the root \( w\lambda\) is a weight\quext{To be proved.}; in particular \( w_0\Lambda\) is a weight of \( L(\Lambda)\).   Let \( v\in L(\Lambda)_{w_0\Lambda}\); we want to show that \( X_i^-v=0\).

    If \( X_i^-v\neq 0\), then \( w_0\Lambda-\alpha_i\) is a weight and \( w_0\big( w_0\Lambda-\alpha_i \big)=\Lambda-w_0\alpha_i\) is a weight too. Here we used the fact that \( w_0^2=\id\).
\end{proof}

\begin{probleme}
    Still to be shown:
    \begin{enumerate}
        \item
            \( w\lambda\) is a weight
        \item
            \( w_0^2=\id\)
    \end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Verma modules}
%---------------------------------------------------------------------------------------------------------------------------

Let us consider
\begin{equation}
    \lB=\lH\oplus\lN^+,
\end{equation}
and take $\alpha\in\lH^*$. Now, we define $\eC_{\alpha}$ as the vector space $\eC$ (one dimensional, generated by $z_+\in\eC$) equipped with the following action of $\lB$:
\begin{equation}
    \big( h+\sum_{\mu\prec 0}x_{\mu} \big)z_+=\alpha(h)z_+.
\end{equation}
The vector space $\eC_{\alpha}$ becomes a left $\mU(\lB)$-module. On the other hand, $\mU(\lG)$ is a free right $\mU(\lB)$-module because $\mU(\lB)\cup\mU(\lG)\subseteq\mU(\lG)$. As $\mU(\lB)$-module, a basis of $\mU(\lG)$ is given by $\lN^-$, i.e. by $\{ f_1^{i_1}\cdots f_m^{i_l} \}$. The \defe{Verma module}{Verma module} is the cyclic module
\begin{equation}
    \Verm(\alpha)=\mU(\lG)\otimes_{\mU(\lB)}\eC_{\alpha}
\end{equation}
which has a highest weight vector $v_{\lambda}=1\otimes z_+$. The tensor product over $\mU(\lB)$ beans that, when $X\in\mU(\lG)$, then
\begin{equation}
    \big( h+\sum_{\mu}x_{\mu} \big)X\otimes_{\mU(\lB)}zz_+=X\otimes\big( h+\sum_{\mu}x_{\mu} \big)zz_+=X\otimes_{\mU(\lG)}z\alpha(h)z_+=\alpha(h)X\otimes_{\mU(\lB)}zz_+.
\end{equation}
The Verma module is generated by $1\otimes z_+$ and the fact that
\begin{equation}
    zX(1\otimes z_+)=X\otimes zz_+.
\end{equation}

\begin{proposition}
Two irreducible cyclic modules with same highest weight are isomorphic.
\end{proposition}

\begin{proof}
Let $V$ and $W$ be two highest weight cyclic modules with highest weight $\lambda$ and highest weight vectors $v_{\lambda}$ and $w_{\lambda}$. In the module $V\oplus W$, the vector $v_{\lambda}\oplus w_{\lambda}$ is a highest weight vector of weight $\lambda$. Let us consider the module
\begin{equation}
    Z=\mU(\lG)(v_{\lambda}\oplus w_{\lambda}).
\end{equation}
That module is a highest weight cyclic module. The projections onto $V=Z/W$ and $W=Z/V$ are non vanishing surjective homomorphisms, so $V$ and $W$ are irreducible quotients of $Z$. But we saw bellow equation \eqref{EqVsumValpha} that $Z$ can only accept one irreducible quotient. Thus $V$ and $W$ are isomorphic.
\end{proof}
We denote by $\Irr_{\lG}(\lambda)$\nomenclature{$\Irr_{\lG}(\lG)$}{the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.} the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Old stuff}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

I move here the theorems which are not really well proven and which are replaced by better ones.

\begin{theorem}\label{tho:diff_sur_ferme}
    Let $G$ be a Lie group whose Lie algebra is $\lG$ and $H$, a closed subgroup (not specially a \emph{Lie} subgroup) of $G$. Then there exists one and only one analytic structure on $H$ for which $H$ is a topological Lie subgroup of $G$.
\end{theorem}

\begin{remark}
A \textit{topological} Lie subgroup\index{topological!Lie subgroup} is stronger that a common Lie subgroup because it needs to be a topological subgroup: it must carry \emph{exactly} the induced topology. In our definition of a Lie group, this feature doesn't appears.
\end{remark}

\begin{proof}
   Let $\lH$ be the subspace of $\lG$ defined by
\begin{equation}\label{eq:lH_de_G}
  \lH=\{X\in\lG\tq \forall t\in\eR,\, e^{tX}\in H\}.
\end{equation}
We begin to show that $\lH$ is a subalgebra of $\lG$; i.e. to show that $t(X+Y)\in\lH$ and $t^2[X,Y]\in\lH$ if $X$, $Y\in\lH$. Remark that $X\in\lH$ and $s\in\eR$ implies $sX\in\lH$. Consider now $X$, $Y\in\lH$ and the classical formula:
\begin{subequations}        \label{SUBEQSooASPNooZOpKRt}
\begin{align}
\left(  \exp(\frac{t}{n}X)\exp(\frac{t}{n}Y)  \right )^n
                       =\exp( t(X+Y)+\frac{t^2}{2n}[X,Y]+o(\frac{1}{n^2}) ),\\
\left(  \exp(-\frac{t}{n}X)\exp(-\frac{t}{n}Y)\exp(\frac{t}{n}X)\exp(\frac{t}{n}Y)   \right)^{n^2}
                       =\exp\left( t^2[X,Y]+o(\frac{1}{n})\right).
\end{align}
\end{subequations}
The left hand side of these equations are in $H$ for any $n$; but, since $H$ is closed, it keeps in $H$ when $n\to\infty$. The right hand side, at the limit, is just $\exp(t(X+Y))$ and $\exp(t^2[X,Y])$, which keeps in $H$ for any $t$. Thus $X+Y$ and $[X,Y]$ belong to $\lH$. The space $\lH$ is thus a Lie subalgebra of $\lG$.

Let $H^*$ be the connected Lie subgroup of $G$ whose Lie algebra is $\lH$ (existence and unicity from~\ref{tho:gp_alg}). From the proof of theorem~\ref{tho:gp_alg}, we know that $H^*$ is the smallest subgroup of $G$ containing $\exp\lH$, then it is made up from products and inverses of elements of the type $e^X$ with $X\in\lH$, and thus is is included in $H$ by definition of $\lH$. So, $H^*\subset H$.

We will show that if we put on $H^*$ the induced topology from $G$ and if $H_0$ denotes the identity component of $H$, then $H^*=H_0$ as topological groups. For this, we first have to show the equality as set and then prove that if $N$ is a neighbourhood of $e$ in $H^*$, then it is a neighbourhood of $e$ in $H_0$. In facts, the equality as set can be derives from this second fact. Indeed, since $H_0$ is a connected topological group, it is generated by any neighbourhood of $e$, so if one can show that any neighbourhood $N$ of $e$ in $H^*$ is a neighbourhood of $e$ in $H$, then $H^*$ is a neighbourhood of $e$ in $H_0$ and then $H_0$ should be generated by $H^*$, so that $H_0\subset H^*$ (as set). Moreover, the most general element of $H^*$ is product and inverse of $e^X$ with $X\in\lH$ and $e^X$ is connected to $e$ by the path $e^{tX}$ ($\dpt{t}{1}{0}$). Then $H^*\subset H_0$, and $H^*=H_0$ as set. Immediately, $H^*=H_0$ as topological groups from our assertion about neighbourhoods of $e$. Let us now prove it.

We consider a neighbourhood $N$ of $e$ in $H^*$ and suppose that this is not a neighbourhood of $e$ in $H$. Thus there exists a sequence $c_k\in H\setminus N$ such that $c_k\to e$ in the sense of the topology on $G$. Indeed, a neighbourhood of $e$ in the sense of $H$ must contains at least a point which is not in $N$ because if we have an open set of $H$ around $e$ included in $N$, then $N$ is a neighbourhood of $e$ for $H$. So we consider a suitable sequence of such open sets around $e$ and one element not in $N$ in each of them. There is the $c_k$'s\quext{Je crois qu'on utilise l'axiome du choix.}.

Using lemma~\ref{lem:decomp} with a decomposition $\lG=\lH\oplus\lM$ (i.e. $\lM$: a complementary for $\lH$ for $\lG$), one can find sequences $A_k\in\mU_m$ and $B_k\in\mU_n$ such that
\[
   c_k=e^{A_k}e^{B_k}.
\]
Here, $\mU_m$ is an open neighbourhood of $0$ in $\lM$ and $\mU_h$, an open neighbourhood of $0$ in $\lH$.

As $e^{B_k}\in N$ and $c_k\in H\setminus N$, $A_k\neq 0$ and $\lim A_k=\lim B_k=0$ (because $(A,B)\to e^Ae^B$ is a diffeomorphism and $e^0e^0=e$ -- and also because all is continuous and thus has a good behaviour with respect to the limit). The set $\mU_m$ is open and bounded --this is a part of the lemma. Then there exist a sequence of positive reals numbers $r_k\in$ such that $r_kA_k\in\mU_m$ and $(r_k+1)A_k\notin\mU_m$. We know that $\mU_m$ is a bounded open subset of the vector space $\lM$, then the whole sequences $r_kA_k$ and $(r_k+1)A_k$ are in a compact domain of $\lM$. Then --by eventually considering subsequences-- there are no problems to consider limits of these sequences in $\lM$: $r_kA_k\to A\in\lM$ (not necessary in $\mU_m$). Since $A_k\to 0$, the point $A$ is the common limit of $r_kA_k\in\mU_m$ and of $(r_k+1)A_k\notin\mU_m$. Thus $A$ is in the boundary of $\mU_m$; in particular, $A\neq 0$.

On the other hand, consider two integers $p,q$ with $q>0$. One can find sequences $s_k,t_k\in\eN$ and $0\leq t_k<q$ such that $pr_k=qs_k+t_k$. It is clear that
\begin{equation}
  \lim_{k\to\infty}\frac{t_k}{q}A_k=0,
\end{equation}
thus
\[
   \exp \frac{p}{q}A=\lim \exp\frac{pr_k}{a}A_k=\lim (\exp A_k)^{s_k},
\]
which belongs to $H$. By continuity, $\exp tA\in H$ for any $t\in\eR$ and finally $A\in\lH$; this contradict $A\neq 0$ so that $A\in\lM$ (because by definition, $A\in\lM$ and the sum $\lG=\lH\oplus\lM$ is direct).

By its definition, $H^*$ has an analytic structure of Lie subgroup of $G$; but we had just proved that the induced topology from $G$ is the one of $H_0$ which by definition is a submanifold of $G$. So the set $H_0=H^*$ becomes a submanifold of $G$ whose topology is compatible with the analytic structure: thus it is a Lie subgroup of $G$. From analyticity, this structure is extended to the whole $H$.

\begin{probleme}
Est-ce bien vrai, tout \c ca ? En particulier, je n'utilise pas que $H_0$ est ouvert dans $H$ (ce qui est un tho de topo classique : je ne vois pas pourquoi Helgason fait tout un cin\'ema --que je ne comprends pas-- dessus). En prenant $N=H^*$, on a juste d\'emontr\'e que $H_0$ est un voisinage de $e$ dans $H$, mais ça, on le savait bien avant.
\end{probleme}

The unicity part comes from the corollary~\ref{cor:top_subgroup}.
\end{proof}

