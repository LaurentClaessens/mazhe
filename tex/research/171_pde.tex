% This is part of (almost) Everything I know in mathematics
% Copyright (c) 2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Basic definitions}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We consider PDE's for functions \( u\colon \eR^d\to \eR\). We denote by \( D^lu\) the set of partial derivatives of order \( l\):
\begin{equation}
    (D^lu)(x)=\{ (D^{\alpha}u)(x) \}_{| \alpha |=l}.
\end{equation}

Some definitions.
\begin{definition}
    A PDE of order \( k\) is \defe{linear}{linear!PDE} if it is under the form
    \begin{equation}
        \sum_{| \alpha |\leq k}a_{\alpha}(x)(\partial^{\alpha}u)(x)+f(x)=0.
    \end{equation}
    That equation is \defe{homogeneous}{homogeneous!linear PDE} if \( f=0\).
\end{definition}

\begin{definition}
    A PDE is \defe{semi-linear}{semi-linear!PDE} is it is linear only with respect to the highest derivatives:
    \begin{equation}
        \sum_{| \alpha |=k}a_{\alpha}(\partial^{\alpha}u)(x)+F\big( x, (Du)(x),\ldots, D^{k-1}u(x) \big)=0
    \end{equation}
    where \( F\colon \Omega\times \eR^{\ldots}\to \eR\) is some function. The dots here represent the number of different partial derivative of order \( 0\) to \( k-1\) one has to consider.
\end{definition}

\begin{definition}
    A PDE of order \( k\) is \defe{quasi-linear}{quasi!linear PDE} if it is linear with respect to the higher order derivatives, with a coefficient that can depend to the lower order derivatives of the unknown:
    \begin{equation}
        \sum_{| \alpha |=k}a_{\alpha}\big( x,u(x),(Du)(x),\ldots, (D^{k-1})u(x) \big)(\partial^{\alpha}u)(x)+F\big( x,u(x),\ldots, (D^{k-1}u)(x) \big)=0.
    \end{equation}
\end{definition}

\begin{example}
    The equation
    \begin{equation}
        x(\partial_xu)(x,y)+y(partial_yu)(x,y)+u\sin(xy)=1
    \end{equation}
    is linear of order \( 1\). Indeed the coefficient of the partial derivatives of all orders  (\( 0\) and \( 1\)) depend on \( x\) and \( y\), but not of \( u\) or the derivatives of \( u\).
\end{example}

\begin{example}
    The equation
    \begin{equation}
        x(\partial_xu)(x,y)+y(\partial_yu)(x,y)+u^2\sin(xy)=1
    \end{equation}
    is semi-linear of order \( 1\). Indeed the coefficient of the partial derivatives of order \( 1\) (the higher order) depend on \( x\) and \( y\) but not of \( u\) or the derivatives of \( u\). However, the derivatives of lower order (here: \( 0\)) are not linear.
\end{example}

\begin{example}
    The heat equation
    \begin{equation}
        \frac{ \partial u }{ \partial t }-\frac{ \partial^2u }{ \partial x^2 }=1
    \end{equation}
    is linear ans non homogeneous.
\end{example}

\begin{example}
    The equation
    \begin{equation}
        \frac{ \partial u }{ \partial t }-\frac{ \partial  }{ \partial x }\left( c(u)\frac{ \partial u }{ \partial x } \right)=0
    \end{equation}
    is an abuse of notations for asking
    \begin{equation}
        \frac{ \partial u }{ \partial t }(x,t)-\frac{ \partial x }{ \partial  }\left( (c\circ u)\times \frac{ \partial u }{ \partial x } \right)(x,t).
    \end{equation}
    for every \( t\) and \( x\). This is a second order equation whose highest order term is
    \begin{equation}
        (c\circ u)\frac{ \partial^2u }{ \partial x^2 }
    \end{equation}
    The coefficient of \( \partial^2_xu\) depend on \( u\), but not on the second order derivatives. So this is a quasi-linear equation.
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Principal symbol}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let the semi-linear equation
\begin{equation}
    \sum_{| \alpha |=k}a_{\alpha}(x)(\partial^{\alpha}u)(x)+F\big( x,u(x),(Du)(x),\ldots,(D^{k-1}u)(x) \big).
\end{equation}
The differential operator associated with that equation is
\begin{equation}       \label{EQooNSOMooCfAdvt}
    \sum_{| \alpha |=k}a_{\alpha}\partial^k +F
\end{equation}
where we admit quite a notational shortcut to say that \( F\) applies on \( u\) by
\begin{equation}
    (Fu)(x)=F  \big( x,u(x),(Du)(x),\ldots,(D^{k-1}u)(x) \big).
\end{equation}

\begin{definition}
    The \defe{principal symbol}{symbol!principal} of the operator \eqref{EQooNSOMooCfAdvt} is the function
    \begin{equation}
        \begin{aligned}
            \sigma\colon \eR^d\times \eR^d&\to \eR \\
            (x,\xi)&\mapsto \sum_{| \alpha |=k}a_{\alpha}(x)\xi^{\alpha}
        \end{aligned}
    \end{equation}
    where
    \begin{equation}
        \xi^{\alpha}=\xi_1^{\alpha_1}\ldots \xi_d^{\alpha_d}.
    \end{equation}
\end{definition}

\begin{definition}
    The characteristic associated with the symbol \( \sigma\) is the set of surfaces \( S\subset \eR^d\) of the form
    \begin{equation}
        S=\{ x\in \eR^d\st \phi(x)=0 \}
    \end{equation}
    where \( \phi\colon \eR^d\to \eR\) satisfies
    \begin{enumerate}
        \item
            \( \sigma\big( x,(\nabla \phi)(x) \big)=0\) for every \( x\in\eR^d\),
        \item
            \( (\nabla\phi)(x)\neq 0\) for every \( x\in S\).
    \end{enumerate}
\end{definition}

\begin{example}
    Let the differential equation
    \begin{equation}
        a\frac{ \partial^2u }{ \partial x^2 }+b\frac{ \partial^2u }{ \partial x\partial y }+x\frac{ \partial^2u }{ \partial y^2 }=f
    \end{equation}
    for the function \( u\colon \eR^2\to \eR\) and some given functions \( a\), \( b\) and \( c\).

    The principal symbol of that operator is
    \begin{equation}
        \sigma(x,y,\xi_1,\xi_2)=a(x,y)\xi_1^2+b(x,y)\xi_1\xi_2+c(x,y)\xi_2^2
    \end{equation}
    and the characteristic equation reads\footnote{We are not going to explicit all dependencies in \( x\) and \( y\).}
    \begin{equation}        \label{EQooTMQAooZPdcWT}
        \sigma(x,y,  (\nabla\phi)(x,y)   )=a(x,y)\left( \frac{ \partial^2\phi }{ \partial x }(x,y) \right)^2+b(x,y)\frac{ \partial \phi }{ \partial x }(x,y)\frac{ \partial \phi }{ \partial y }(x,y)+c(x,y)\left( \frac{ \partial \phi }{ \partial y }(x,y) \right)^2=0.
    \end{equation}
    This is a differential equation for \( \phi\colon \eR^2\to \eR\).

    The condition \( \nabla\phi\neq 0\) seems now natural: if \( \partial_i\phi=0\) for every \( i\), the equation trivializes. We suppose that \( (\partial_y\phi)(x_0,y_0)\neq 0\) and that this condition holds on a neighborhood of \( (x_0,y_0)\). By the implicit function theorem~\ref{ThoAcaWho} there exists a function \( x\mapsto y(x)\) such that
    \begin{equation}
        \phi\big( x,y(x) \big)=0
    \end{equation}
    for every \( x\) in a neighborhood of \( x_0\). Thus the function
    \begin{equation}
        \varphi\colon x\mapsto \phi\big( x,y(x) \big)
    \end{equation}
    is identically zero. Its derivative is
    \begin{equation}
        \varphi'(x)=\frac{ \partial \phi }{ \partial x }\big( x,y(x) \big)+\frac{ \partial \phi }{ \partial y }\big( x,y(x) \big)y'(x)=0.
    \end{equation}
    We substitute \( \frac{ \partial \phi }{ \partial x }\) by \( -\frac{ \partial \phi }{ \partial y }y'\) in the characteristic equation \eqref{EQooTMQAooZPdcWT} written on the point \( (x,y(x))\).  We simplify the whole by \( (\partial_y\phi)(x,y(x))^2\) (which is non vanishing) and obtain
    \begin{equation}
        ay'(x)^2-by'(x)+c=0.
    \end{equation}
    where \( a\), \( b\) and \( c\) are taken at \( \big( x,y(x) \big)\).

    The existence of \( y'(x)\) depend on the sign of \( b^2-4ac\) which is a function of \( x\).

\end{example}

\begin{definition}
    Let a partial differential equation.
    \begin{enumerate}
        \item
            If the characteristic equation has no real solutions, the equation is \defe{elliptic}{elliptic!pde}
        \item
            If the characteristic equation has \( k\) distinct real solutions, the equation is \defe{hyperbolic}{hyperbolic!pde}
        \item
            If some solutions to the characteristic equation have a multiplicity, the equation is \defe{parabolic}{parabolic!pde}
    \end{enumerate}
\end{definition}

Since the number of solutions of the characteristic equation depend on the coefficients that are themselves functions, an equation can have different type at different places.

\begin{example}
    Let the equation
    \begin{equation}
        \frac{ \partial^2u }{ \partial x^2 }-(x^2-y^2)\frac{ \partial^2u }{ \partial y^2 }=0.
    \end{equation}
    The type of this equation is governed by \( \delta=4(x^2-y^2)\); this can have any sign depending on the point.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Classification with respect to the boundary conditions}
%---------------------------------------------------------------------------------------------------------------------------

For the next few definitions we suppose that \( \Omega\) is a sufficiently smooth domain in \( \eR^d\); in particular we suppose that the boundary \( \partial\Omega\) admits a normal outward vector \( n(x)\) for every \( x\in \partial\Omega\).

\begin{definition}
    A problem with \defe{Dirichlet}{problem!Dirichlet boundary condition} boundary condition is a partial derivative equation with the condition
    \begin{equation}
        u(x)=g(x)
    \end{equation}
    for \( x\in\Gamma\subset\partial\Omega\). In other words, we impose the value of \( u\) on a part of the boundary.
\end{definition}

\begin{definition}
    A problem with \defe{Neumann}{problem!Neumann boundary condition} boundary condition is a partial derivative equation with the condition
    \begin{equation}
        \frac{ \partial u }{ \partial n }\cdot x=g(x)
    \end{equation}
    for \( x\in\Gamma\subset\partial\Omega\). In other words, we impose the normal derivative of \( u\) on a part of the boundary.
\end{definition}

\begin{definition}
    Let \( \Omega\) be a domain of \( \eR^d\) and a differential operator \( L\) on a part of \( \Fun(\Omega)\). Let \( g\) be a function on \( \partial \Omega\). A problem with \defe{stationary boundary condition}{problem!stationary boundary condition} is a problem of the type: find a function \( u\) defined on \( \Omega\) such that
    \begin{equation}
        \begin{cases}
            L(u)=f    &   \text{on } \Omega\\
            u =   & g   \text{on }\partial\Omega.
        \end{cases}
    \end{equation}
\end{definition}

\begin{definition}
Let \( \Omega\) be a domain of \( \eR^d\) and a differential operator \( L\) on a part of \( \Fun(\Omega)\). Let \( g\) be a function on \( \partial \Omega\). A problem with \defe{evolution boundary condition}{problem!stationary boundary condition} is a problem of the type: find a function \( u\) defined on \( \mathopen] 0 , \infty \mathclose[\times \Omega\) such that
    \begin{equation}
        \begin{cases}
            \frac{ \partial^mu }{ \partial t^m }+l(u)=f
            u(t,.)=g(t,.)&\text{ on }\partial\Omega\\
            u(0,.)=u_0&\text{ on }\Omega
        \end{cases}
    \end{equation}
    where \( L\) does not operate on the ``\( t\)'' variable.
\end{definition}

\begin{definition}
    A boundary problem is \defe{well posed}{problem!well posed} in the sense of Hadamard if
    \begin{enumerate}
        \item
            it accepts an unique solution
        \item
            this solution is continuous with respect to the parameters.
    \end{enumerate}
    Here the ``parameters'' are the constants as well as the functions given in the problem. So one needs to precise the topology on the functional spaces that are implied in the problem.
\end{definition}

\begin{example}[\cite{ooXNFJooFENjjP}]          \label{EXooLTODooOwJtGC}
    Let the problem: find a function \( u\) defined on \( \Omega=\mathopen[ 0 , 1 \mathclose]^2\) such that
    \begin{equation}
         \begin{cases}
             \Delta u=0    &   \text{on } \Omega\\
             u=g    &    \text{on } \partial\Omega.
         \end{cases}
     \end{equation}
     Let us prove the unicity of the solution (we do not prove existence). Let \( u_1\) and \( u_2\) be two solutions and define \( v=u_1-u_2\). This function satisfies
     \begin{equation}
          \begin{cases}
              \Delta v=0    &   \text{on } \Omega\\
              v=0    &    \text{on }\partial\Omega.
          \end{cases}
      \end{equation}
      Using the by part formula of example~\ref{EXooWLUVooNamnKG}, with \( u=v\),
      \begin{equation}
          \int_{\Omega}v\Delta v=-\int_{\Omega}\| \nabla v \|^2+\int_{\partial\Omega}v\frac{ \partial v }{ \partial n }.
      \end{equation}
      Since \( \Delta v=0\) and \( v=0\) on \( \partial\Omega\), we are left with
      \begin{equation}
          \int_{\Omega}\| \nabla v \|^2=0.
      \end{equation}
      If \( v\) is smooth enough, that implies \( \nabla v=0\) on \( \Omega\). Since the function \( v\) is constant on \( \Omega\) and is zero on \( \partial\Omega\) we deduce that \( v=0\) on \( \Omega\). This proves the unicity, up to numerous regularity and topological considerations on the choice of the functional spaces.

     Remark that we did not prove existence and even less proved the continuity of the solution with respect to \( g\). For this we should precise a functional space \( S\) in which one chooses \( g\) and a functional space \( V\) in which one chooses \( u\) (for sure not the same because \( g\) is defined on \( \partial \Omega\) while \( u\) is defined on \( \Omega\)) and prove that \( g\mapsto u\) is a continuous map \( S\to V\).
\end{example}

From this example one can believe that under some good choices of functional spaces, the Dirichlet problem for the Laplace equation is well posed. We give now different conditions for the same equation, and show that the resulting problem is for sure not well posed.

\begin{example}[\cite{MonCerveau}]
Let \( \epsilon>0\) and the problem for \( u_{\epsilon}\) on \( \Omega=\eR\times \mathopen] 0 , \infty \mathclose[\),
    \begin{equation}        \label{EQooUBUEooMCDIlZ}
         \begin{cases}
             \Delta u_{\epsilon}=0    &   \text{on } \Omega\\
             u_{\epsilon}(x,0)=0    &    \text{for } x\in \eR \\
             \frac{ \partial u_{\epsilon} }{ \partial y }(x,0)=\epsilon\sin(x/\epsilon)    &    \text{for }x\in \eR.
         \end{cases}
     \end{equation}
     We are searching the solutions in
     \begin{equation}
         \big(  C^{\infty}(\Omega),p_{K,m} \big)
     \end{equation}
     where \( p_{K,m}\) are the semi-norms defined in \eqref{EQooZSQUooAJRIFe}. The solution is unique from the same reason as in example~\ref{EXooLTODooOwJtGC}\quext{Is that true?}.

     One checks that, when \( \epsilon\neq 0\), the unique solution is
     \begin{equation}
         u_{\epsilon}(x,y)=\epsilon^2\sin(\frac{1}{ \epsilon }x)\sinh(\frac{1}{ \epsilon }y).
     \end{equation}
     The parameters in the equation is \( \epsilon\in \eR\), and in order to check the well-posedness we consider the map \( \psi\) that maps \( \epsilon\) to the unique corresponding solution:
     \begin{equation}
         \begin{aligned}
             \psi\colon \eR&\to  C^{\infty}(\Omega) \\
             \epsilon&\mapsto \begin{cases}
                 (x,y)\mapsto \epsilon^2\sin(\frac{ x }{ \epsilon })\sinh(\frac{ y }{ \epsilon })   &   \text{if } \epsilon\neq 0\\
                 (x,y)\mapsto 0 &    \text{if } \epsilon=0.
             \end{cases}
         \end{aligned}
     \end{equation}
     Our point is that \( \psi\) is not continuous at \( \epsilon=0\). Due to proposition~\ref{PropQPzGKVk}, in order \( \psi\) to be continuous we need \( p_{K,m}(u_{\epsilon})\to 0\) when \( \epsilon\to 0\) for every \( m\) and every compact \( K\subset \Omega\).

     Let \( m=0\) and \( K=\mathopen[ 0 , 2\pi \mathclose]\times\mathopen[ 1 , 2 \mathclose]\). For every \( \epsilon<1\) we can find \( x\in\mathopen[ 0 , 2\pi \mathclose]\) such that \( \sin(x/\epsilon)=1\), so that
     \begin{equation}
         \| u_{\epsilon} \|_{K,0}\geq \epsilon^2\sinh\left( \frac{ 2 }{ \epsilon } \right)=\frac{ 1 }{2}\epsilon^2|  e^{2/\epsilon}- e^{-2/\epsilon} |.
     \end{equation}
     Now one has
     \begin{equation}
         \lim_{\epsilon\to 0}\| u_{\epsilon} \|_{K,0}\geq \lim_{\epsilon\to 0}\frac{ 1 }{2}\epsilon^2|  e^{2/\epsilon}- e^{-2/\epsilon} |=\infty.
     \end{equation}
     Thus one has not \( \lim_{\epsilon\to 0}\psi(\epsilon)=\psi(0)\) and \( \psi\) is not continuous.

     We conclude that the problem \eqref{EQooUBUEooMCDIlZ} is not well posed in the sense of Hadamard.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Characteristic speed}
%---------------------------------------------------------------------------------------------------------------------------

Let the PDE for \( u\colon \eR^2\to \eR\)
\begin{equation}        \label{EQooEVOBooTabJIg}
    \frac{ \partial u }{ \partial t }+\lambda\frac{ \partial u }{ \partial x }=0.
\end{equation}
The principal symbol is
\begin{equation}
    \sigma(t,x,\xi_1,\xi_2)=\xi_1+\lambda \xi_2,
\end{equation}
so that the characteristic equation is the same as the original equation. Thus in the case of a first order equation, the characteristic equation has to be something else.

Suppose \( u\) to be a solution of \eqref{EQooEVOBooTabJIg} and pose \( \varphi(t)=u\big( x(t),t \big)\) for some function \( x\). We have
\begin{equation}
    \varphi'(t)=\frac{ \partial u }{ \partial x }x'+\frac{ \partial u }{ \partial t }.
\end{equation}
If \( x'(t)=\lambda\) then the initial equation says that \( \varphi'(t)=0\) for every \( t\). That means that \( u\) is constant on the characteristic curves
\begin{equation}
    t\mapsto \big( x(t),t \big).
\end{equation}
The data of an initial condition \( u(x,0)=u_0(x)\) fix the value of \( u\) on each characteristic curve.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Decoupling a system}
%---------------------------------------------------------------------------------------------------------------------------

Let the system
\begin{subequations}
    \begin{numcases}{}
        \rho\frac{ \partial u }{ \partial t }+\frac{ \partial p }{ \partial x }=0\\
        \frac{ \partial p }{ \partial t }+\rho c^2\frac{ \partial u }{ \partial x }=0
    \end{numcases}
\end{subequations}
where \( \rho\) and \( c\) are non vanishing constants. This system can be written under the form
\begin{equation}
    \frac{ \partial  }{ \partial t }\begin{pmatrix}
        u    \\
        p
    \end{pmatrix}+
    \begin{pmatrix}
        0    &   \rho^{-1}    \\
        \rho c^2    &   0
    \end{pmatrix}\frac{ \partial  }{ \partial x }\begin{pmatrix}
        u    \\
        p
    \end{pmatrix}=0.
\end{equation}
We set
\begin{equation}
    \begin{aligned}[]
        \begin{pmatrix}
            u_1    \\
            u_2
        \end{pmatrix}=\begin{pmatrix}
            u    \\
            p
        \end{pmatrix}&&\text{ and }&&A=\begin{pmatrix}
            0    &   \rho^{-1}    \\
            \rho c^2    &   0
        \end{pmatrix},
    \end{aligned}
\end{equation}
so that the equation reads
\begin{equation}
    \partial_t\bar u+A\partial_x\bar u=0.
\end{equation}

Now we diagonalize the matrix \( A\). Some computations show that the eigenvalues of \( A\) are \( \pm c\) and that the corresponding eigenvectors are
\begin{subequations}
    \begin{align}
        f_1&=\begin{pmatrix}
            -(\rho c)^{-1}    \\
            1
        \end{pmatrix};\\
        f_2&=\begin{pmatrix}
            (\rho c)^{-1}    \\
            1
        \end{pmatrix}.
    \end{align}
\end{subequations}
If one consider
\begin{equation}
    K=\begin{pmatrix}
        -(\rho c)^{-1}    &   (\rho c)^{-1}    \\
        1    &   1
    \end{pmatrix}
\end{equation}
we have
\begin{equation}
    K^{-1}=\frac{ 1 }{2}\begin{pmatrix}
        -\rho c    &   1    \\
        \rho c    &   1
    \end{pmatrix}
\end{equation}
and
\begin{equation}
    D=K^{-1} AK=\begin{pmatrix}
        -c    &   0    \\
        0    &   c
    \end{pmatrix}
\end{equation}
Now we consider the new variables \( \bar v=K^{-1}\bar u\), so that we have
\begin{subequations}
    \begin{align}
        \partial_t\bar u+A\partial_x\bar u=0\\
        \partial_tK\bar u+A\partial_xK\bar u=0\\
        \partial_t\bar v+K^{-1}AK\partial_x\bar v=0\\
        \partial_t\bar v+D\partial_x\bar v=0.
    \end{align}
\end{subequations}
The system is now decoupled:
\begin{subequations}
    \begin{numcases}{}
        \frac{ \partial v_1 }{ \partial t} -c\frac{ \partial v_1 }{ \partial x }=0\\
        \frac{ \partial v_2 }{ \partial t} +c\frac{ \partial v_2 }{ \partial x }=0
    \end{numcases}
\end{subequations}
and we have two equations of the form \eqref{EQooEVOBooTabJIg} already treated.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Some examples}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let the equation
\begin{equation}
    \frac{ \partial^2u }{ \partial x\partial y }+\frac{ \partial u }{ \partial x }=0.
\end{equation}
If we let \( v=\frac{ \partial u }{ \partial x }\) we can write the equation as
\begin{equation}
    \frac{ \partial v }{ \partial y }+v=0.
\end{equation}
Let \( x_0\) be fixed and consider the function \( v_0(y)=v(x_0,y)\). It satisfies
\begin{equation}
    v_0'(y)=\frac{ \partial v }{ \partial y }(x_0,y)=-v(x_0,y)=-v_0(y),
\end{equation}
so that
\begin{equation}
    v_0(y)=A_0 e^{-y}
\end{equation}
for a constant \( A_0\). The latter can be a function of \( x\). We have:
\begin{equation}
    v(x,y)=A(x) e^{-y}
\end{equation}
and we we get back to the original equation \( v=\partial_cu\):
\begin{equation}
    \frac{ \partial u }{ \partial x }=A(x) e^{-y}.
\end{equation}
Integrating,
\begin{equation}
    u(x,y)=B(x) e^{-y}+C(x)
\end{equation}
where \( B\) is a primitive of \( A\).

Notice that \( B\) is almost completely arbitrary. The fact that it is a primitive only says that it has to be derivable.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Principle of superposition}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let \( L\) be a linear differential operator and consider the equation
\begin{equation}        \label{EQooLAZSooGGfMMT}
    Lu=f.
\end{equation}
By linearity of \( L\), if \( u_1\) and \( u_2\) are solution, the difference \( v=u_1-u_2\) satisfies \( Lv=0\). Thus if one has a particular solution \( u_P\) of \( Lu=f\) and the general solution \( u_0\) of \( Lu=0\), the general solution of \eqref{EQooLAZSooGGfMMT} is
\begin{equation}
    u=u_P+u_0.
\end{equation}

Now suppose that we are working on \( \eR^2\) and that \( f\) can be decomposed as \( f(x,y)=f_1(x)+f_2(x)\). We should solve separately the equations
\begin{subequations}        \label{SUBEQSooNAWRooPayDFv}
    \begin{align}
        (Lu_1)(x,y)=f_1(x)\\
        (Lu_2)(x,y)=f_2(y)\\
        (Lu_0)(x,y)=0.
    \end{align}
\end{subequations}
and then write the solution \( u=u_1+u_2+u_0\).

This decomposition invites us to solve \( (Lu_1)(x,y)=f_1(x)\) for a function \( u_1\) that only depends on \( x\) because (if one suppose that \( L\) commutes with the derivative with respect to \( y\)),
\begin{equation}
    L\left( \frac{ \partial u_1 }{ \partial y } \right)=\frac{ \partial  }{ \partial y }\big( L(u_1) \big)=\frac{ \partial  }{ \partial y }\big( f_1(x) \big)=0.
\end{equation}
This shows that \( \partial_yu_1\) is a solution of \( Lu=0\), and thus will be included in \( u_0\).

This is not a proof that searching for \( u_1\) that only depends on \( x\) is a good idea. But it is a strong incitation. Anyway, if we solve the equations \eqref{SUBEQSooNAWRooPayDFv} in any way, we are done by the superposition principle.

\begin{example}
    Let the equation
    \begin{equation}
        \frac{ \partial^2u }{ \partial t^2 }-4\frac{ \partial^2u }{ \partial x^2 }=\sin(t)+x^{2017}.
    \end{equation}
    The differential operator \( L=\partial_t^2-4\partial_x^2\) is linear and we can use the principle of superposition. So we solve the equations
    \begin{subequations}
        \begin{align}
            Lu_1&=\sin(t)\\
            Lu_2&=x^{2017}\\
            Lu_0&=0
        \end{align}
    \end{subequations}
    and we are searching \( u_1(t)\), \( u_2(x)\) and \( u_0(t,x)\).

    Since \( u_1\) only depends on \( t\), its equation reduces to
    \begin{equation}
        \frac{ \partial^2u_1 }{ \partial t^2 }=\sin(t),
    \end{equation}
    so that
    \begin{equation}
        u_1(t)=-\sin(t)+Ct+D
    \end{equation}
    where \( C\) and \( D\) are constants with respect to \( t\). One could write
    \begin{equation}
        u_1(t,x)=-\sin(t)+C(x)t+D(x),
    \end{equation}
    but we are in no way interested in the general solution for \( u_1\). Any solution is sufficient, so that we write the easiest one:
    \begin{equation}
        u(t,x)=-\sin(t).
    \end{equation}
    We already know that the ``missing'' part will be included in \( u_0\).

    For \( u_2\) we have
    \begin{equation}
        u_2(t,x)=\frac{ -x^{2019} }{ 4\cdot 2018\cdot 2019 }.
    \end{equation}

    We are left to solve
    \begin{equation}
        \frac{ \partial^2u_0 }{ \partial t^2 }-4\frac{ \partial^2u_0 }{ \partial x^2 }=0,
    \end{equation}
    which is the wave equation. This is already solved in the subsection~\ref{SUBSECooYBBKooUOIlCS}. The general solution of the problem is
    \begin{equation}
        u(t,x)=-\sin(t)-\frac{ x^{2019} }{ 4\cdot 2018\cdot 2019 }+g_1(x+2t)+g_2(x-2t)
    \end{equation}
    where \( g_1\) and \( g_2\) are any functions suitable for the functional space in which we are searching for \( u\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Separation of variables}
%---------------------------------------------------------------------------------------------------------------------------

Let the equation for \( \Omega=\mathopen] 0 , l \mathclose[\)
\begin{equation}
     \begin{cases}
         \frac{ \partial u }{ \partial t }-k\frac{ \partial^2u }{ \partial x^2 }=0    &   \text{on } \mathopen] 0 , \infty \mathclose[\times \Omega\\
         u(t,0)=u(t,l)=0    &    \text{for }t\in\mathopen] 0 , \infty \mathclose[ \\
         u(0,x)=u_0(x)    &    \text{on }\Omega.
     \end{cases}
\end{equation}
This is the heat equation in which one has fixed the initial distribution and one maintains the extremities at zero.

We are searching for a solution under the form
\begin{equation}
    u(t,x)=\sum_{n\in \eZ}f_n(t)g_n(x).
\end{equation}
For this we hope that the superposition principle works for infinite sums. So we have to solve
\begin{equation}
    \frac{ \partial u }{ \partial t }-k\frac{ \partial^2u }{ \partial x^2 }=0
\end{equation}
with \( u(t,x)=f(t)g(x)\). We have
\begin{equation}
    g(x)f'(t)-kf(t)g''(x)=0,
\end{equation}
so that
\begin{equation}
    \frac{ f'(t) }{ f(t) }=\frac{ g''(x) }{ g(x) }.
\end{equation}
The left hand side only depends on \( t\) while the right hand one only depend on \( x\). So they are both constant: there exists \( \lambda\in \eR\) such that
\begin{equation}
    \frac{ f'(t) }{ f(t) }=\lambda=\frac{ g''(x) }{ g(x) }.
\end{equation}
We have the independent equations
\begin{subequations}
    \begin{align}
        f'(t)&=\lambda f(t)\\
        g''(x)&=\lambda g(x).
    \end{align}
\end{subequations}
The first one produces
\begin{equation}
    f(t)=C e^{\lambda t}
\end{equation}
where \( C\) is a constant that has to be fixed by the initial and boundary conditions.

For \( g\) we have the technique described in subsection~\ref{SUBSECooMXLVooALNtge}. We solve the characteristic equation $r^2+\lambda=0$ and we get the general solution
\begin{equation}
    g(x)=A e^{\sqrt{ \lambda }x}+B e^{-\sqrt{ \lambda }x}.
\end{equation}

From the condition
\begin{equation}
    u(t,0)=u(t,l)=0
\end{equation}
we want to deduce that \( f(t)g(0)=f(t)g(l)=0\) so that \( g(0)=g(l)=0\) because \( f(t)=0\) for every \( t\) leads to a trivial solution. This is however not true because the product \( f(t)g(x)\) does not have to satisfy the boundary conditions; only the final sum \( \sum_{n\in \eZ}f_n(t)g_n(x)\) has to.

For each \( n\in \eZ\) we have a free parameter \( \lambda_n\) and the functions
\begin{subequations}
    \begin{align}
        f_n(t)&=C_n e^{\lambda_n t}\\
        g_n(x)&=A_n e^{\sqrt{ \lambda_n }x}+B_n e^{-\sqrt{ \lambda_n }x}.
    \end{align}
\end{subequations}
where \( A_n\), \( B_n\) and \( C_n\) are still free parameters. We can suppose that \( \lambda_n\neq \lambda_m\) when \( n\neq m\); if not we only have to redefine \( C_n\), \( A_n\) and \( B_n\).

The boundary condition reads
\begin{equation}
    \sum_{n\in \eZ}C_n e^{\lambda_n t}g_n(0)=0.
\end{equation}
Having that for every \( t\) implies \( g_n(0)=0\). The same holds for \( g_n(l)\), so that one indeed has
\begin{equation}
    g_n(0)=g_n(l)=0.
\end{equation}
The condition \( g_n(0)=0\) implies
\begin{equation}
    A_n+B_n=0,
\end{equation}
and the condition \( g_n(l)=0\) then produces
\begin{equation}
    A_n\big(  e^{\sqrt{ \lambda_n }l}- e^{-\sqrt{ \lambda_n }l} \big)=0.
\end{equation}
If \( A_n=0\), we have \( g_n=0\) which is uninteresting. The possibilities for the parenthesis to be vanishing are not numerous: it needs
\begin{equation}
    \sqrt{ \lambda_n }l=ki\pi,
\end{equation}
that gives $\lambda_n=-\frac{ k^2\pi^2 }{ l^2 }$. Since \( \lambda_n\neq \lambda_m\) we can number the \( \lambda_n\) in order to have
\begin{equation}
    \lambda_n=-\frac{ n^2\pi^2 }{ l^2 }.
\end{equation}
In this case,
\begin{equation}
    g_n(x)=A_n\big( e^{in\pi x/l}-  e^{-in\pi x/l} \big),
\end{equation}
that is
\begin{equation}
    g_n(x)=A_ni\sin\left( \frac{ n\pi x }{ l } \right).
\end{equation}
If we want a real solution we have to choice \( A_n\) to be imaginary and up to redefinition we write
\begin{equation}
    g_n(x)=A_n\sin\left( \frac{ n\pi x }{ l } \right)
\end{equation}
with real \( A_n\). Now we have
\begin{equation}
    u_n(t,x)=C_n e^{-n^2\pi^2 t/l}\sin\left( \frac{ n\pi x }{ l } \right)
\end{equation}
and \( n\in \eZ\).

As far as negative \( n\) are concerned,
\begin{equation}
    u_{-n}(t,x)=-C_{-n} e^{-n^2\pi^2 t/l^2}\sin\left( \frac{ n\pi x }{ l } \right)=-\frac{ C_{-n} }{C_n  }u_n(t,x).
\end{equation}
Thus one can skip the negative ones by redefining the coefficients. Obviously \( n=0\) produces a vanishing term (because of the sine). At the end of the day we have the following solution for our problem:
\begin{equation}
    u(t,x)=\sum_{n\in \eN^*}C_n e^{-n^2\pi^2 t/l^2}\sin\left( \frac{ n\pi x }{ l } \right).
\end{equation}
At this point we have to check that
\begin{itemize}
    \item this is actually a solution,
    \item the solution is unique up to choice of \( C_n\).
\end{itemize}
The first point is an exercise of permutation of sum and differentiation. For the second point, we still have to impose \( u(0,x)=u_0(x)\), that is
\begin{equation}
    \sum_{n\in \eN^*}C_n\sin\left( \frac{ n\pi x }{ l } \right)=u_0(x).
\end{equation}
So we have to identify \( u_0\colon  \mathopen[ 0 , l \mathclose]\to \eR \) with its Fourier development. Thanks to the division by \( l\), this problem is equivalent to the Fourier development of a function on \( \mathopen[ 0 , 2\pi \mathclose]\).

From the example~\ref{EXooQDWUooLtuIOm}, the function \( u_0\) being real, it can be written as a series of sine \ldots at least as far as \( u_0\in L^2\big( \mathopen[ 0 , l \mathclose] \big)\).
