% This is part of (almost) Everything I know in mathematics
% Copyright (c) 2010-2017, 2019, 2021-2022, 2024-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Flow and integral curve}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{propositionDef}[Integral curve, flow of a vector field\cite{MonCerveau,BIBooDPYDooWgxxQH}]       \label{PROPooJACTooXBSxfE}
	Let \( X\) be a \( C^k\) vector field on \( M\) and let \( p\in M\). There exists a unique maximal interval \( I\subset \eR\) and \( C^k\) map\footnote{The map \( \Phi_p\) is \( C^k\). We are not speaking of the regularity with respect to \( p\). % Do not join the lines, for the sake of the accepted future reference.
		For more regularity, see the proposition \ref{PROPooQVQAooJVdwOa}.} \( \Phi_p\colon I\to M\) such that
	\begin{enumerate}
		\item
		      \( \Phi_p(0)=p\),
		\item
		      \( \Dsdd{ \Phi_p(t) }{t}{t_0}=X_{\Phi_p(t_0)}\).
	\end{enumerate}
	The map \( \Phi\) satisfy
	\begin{equation}        \label{EQooCHYXooCbECRN}
		\Phi_{t+u}=\Phi_t\circ \Phi_u.
	\end{equation}
	We say that \( \Phi_p\) is the \defe{integral curve}{integral curve} of \( X\) at \( p\), and that \( \Phi\) is the \defe{flow}{flow} of \( X\). We will often write \( \Phi(p,t)\) instead of \( \Phi_p(t)\). When we are interested in only one point \( p\), we can write \( \gamma_X\) or simply \( \gamma\).

	When we want to specify the vector field, we write \( \Phi^X_g(t)\).
	%TODOooPEABooAVCYdS. Il manque certainement des morceaux à la démonstration.
\end{propositionDef}

\begin{proof}
	In several parts.
	\begin{subproof}
		\spitem[Local unicity]        \label{ITEMooZPSEooPivDiw}
		%TODOooPEABooAVCYdS. Il y a plus bas de références à ITEMooZPSEooPivDiw qui parlent d'existence
		% -------------------------------------------------------------------------------------------- 

		Let \( p\in M\), we show that there exists an interval \( I\subset \eR\) such that a \( C^k\) integral curve \( \gamma\colon \eR\to M\) satisfying \( \gamma(0)=p\) is unique. Here the existence is not yet discussed.

		Let \( J\) be an interval around \( 0\) in \( \eR\) and \( \gamma\colon J \to M\), an integral curve of \( X\) such that \( \gamma(0)=p\).

		Let \( \varphi\colon U\to M\) be a local chart around \( p\). We know from lemma \ref{LEMooZWFAooDlYaJm} that there exist functions \( v_k\in C^k(M,\eR)\) such that
		\begin{equation}
			X_x(f)=\sum_{k=1}^nv_k(x)\partial_k(f\circ \varphi)\big( \varphi^{-1}(x) \big).
		\end{equation}
		We consider the map \( \tilde \gamma\colon I\to U\) defined by
		\begin{equation}
			\tilde \gamma=\varphi^{-1}\circ \gamma,
		\end{equation}
		and, if \( f\) is a function on \( M\) we define \( \tilde f\colon U\to \eR\) by \( \tilde f=f\circ\varphi\).

		On the one hand,
		\begin{equation}
			\gamma'(a)f=\Dsdd{ (f\circ \varphi\circ\varphi^{-1}\circ\gamma)(t) }{t}{a}=\Dsdd{ (\tilde f\circ\tilde \gamma) }{t}{a}.
		\end{equation}
		On the other hand,
		\begin{equation}
			X_{\gamma(a)}(f)=\sum_k(\partial_k\tilde f)\big( \tilde \gamma(a) \big)\Dsdd{ \tilde \gamma_k(t) }{t}{a}=\sum_k(\partial_k\tilde f)\big( \varphi^{-1}(p) \big)\tilde \gamma'_k(a)=\sum_k(\partial_k\tilde f)\big(a)\tilde \gamma'_k(a)
		\end{equation}
		where \( \tilde \gamma_k'\) is an usual derivative.

		So, in order to be an integral curve, we need the equality
		\begin{subequations}        \label{SUBEQSooXSIYooPlVcEI}
			\begin{numcases}{}
				v_k\big( \varphi(a) \big)=\tilde \gamma_k'(a)\\
				\tilde \gamma(0)=\varphi^{-1}(p).
			\end{numcases}
		\end{subequations}
		for every \( a\in I\). Each map \( \tilde \gamma_k\) satisfy the equations
		\begin{subequations}        \label{SUBEQSooBLWCooATQGEn}
			\begin{numcases}{}
				\tilde\gamma_k'=v_k\circ \varphi\\
				\tilde\gamma_k(0)=\varphi^{-1}(p)_k.
			\end{numcases}
		\end{subequations}
		The function \( v_k\circ\varphi\colon U\to \eR\) is a \( C^k\) function. We know from the Cauchy-Lipschitz theorem \ref{ThokUUlgU} that the system \eqref{SUBEQSooBLWCooATQGEn} has a unique solution on an interval \( I\) around \( 0\) in \( \eR\).
		\spitem[Local existence]
		% -------------------------------------------------------------------------------------------- 

		%TODOooPEABooAVCYdS. À faire plus sérieusement.

		Consider a solution of \eqref{SUBEQSooBLWCooATQGEn}.

		\spitem[Maximum interval]
		% -------------------------------------------------------------------------------------------- 

		%TODOooPEABooAVCYdS. Encore à faire

	\end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooQVQAooJVdwOa}
	Let \( X\) be a \( C^k\) vector field on \( M\).

	Let \( \Phi\colon M\times \eR\to M\) be a map satisfying
	\begin{enumerate}
		\item
		      \( \Phi_p(0)=p\),
		\item
		      \( \Dsdd{ \Phi_p(t) }{t}{t_0}=X_{\Phi(t_0)}\).
	\end{enumerate}
	Then \( \Phi\) is \( C^k\).
\end{proposition}

\begin{definition}  \label{DEFooMZSBooFkfKkS}
	A vector field \( X\) is \defe{complete}{complete vector field} if for every \( p\in M\), there exists an integral curve \( \gamma\colon \eR\to M\) such that \( \gamma(0)=p\).
\end{definition}

The point of a complete vector field is that the integral curves are defined on \( \eR\), not just a neighbourhood of \( 0\).

The integral curve given by point \ref{ITEMooZPSEooPivDiw} is only valid on a neighbourhood given by a local chart.


\begin{proposition}[\cite{MonCerveau}]      \label{PROPooQNWOooBOxYtu}
	Let \( X\) be a complete \( C^k\) vector field on \( M\). For every \( p\in M\), there is an unique integral curve \( \gamma_p\colon \eR\to M\) such that \( \gamma_p(0)=p\).
\end{proposition}

\begin{proof}
	The existence is not an issue because it is part of the definition of a complete vector field. The trick is the unicity. We already know from point \ref{ITEMooZPSEooPivDiw} that we have the unicity on each chart. The difficulty is to glue them together.

	Let \( \gamma\colon \eR\to M\) and \( \sigma\colon \eR\to M\) be integral curves satisfying \( \gamma(0)=\sigma(0)=p\).

	Let \( K\) be a compact interval containing \( 0\) in \( \eR\). For each \( a\in K\), the point \ref{ITEMooZPSEooPivDiw} provides us an open interval \( I_a\) containing \( a\) and such that there exists a unique \( \alpha\colon I_a\to M\) satisfying
	\begin{subequations}        \label{EQSooCEPBooJCuFJL}
		\begin{numcases}{}
			\alpha\text{ is an integral curve of} X\\
			\alpha(a)=\gamma(a).
		\end{numcases}
	\end{subequations}
	Since \( \gamma\) satisfy these requirements, we know that every map satisfying the conditions \eqref{EQSooCEPBooJCuFJL} is equal to \( \gamma\) on \( I_a\).

	The intervals \( I_a\) make a cover of \( K\) which is compact; we extract a finite subcover \( \{ I_i \}_{i=1,\ldots, n}\). Lemma \ref{LEMooNMGWooTfQDeO} allows us to sort these intervals in such a way that \( 0\in I_1\) and
	\begin{equation}
		I_m\cap\bigcup_{i=1}^{m-1}I_i\neq \emptyset
	\end{equation}
	for every \( m=1,\ldots, n\).

	From unicity on \( I_1\) we know that \( \gamma=\sigma\) on \( I_1\) because \( \gamma(0)=\sigma(0)\) and \( 0\in I_1\). We make a induction. We suppose that, for some \( m\) we have \( \gamma=\sigma\) on \( \bigcup_{i=1}^mI_i\). Let
	\begin{equation}
		t_0\in I_{m+1}\cap\bigcup_{i=1}^mI_i.
	\end{equation}
	From the induction hypothesis, we have \( \gamma(t_0)=\sigma(t_0)\) because \( t_0\in \bigcup_{i=1}^mI_i\).

	The curve \( \sigma\) is, on \( I_{m+1}\) an integral curve of \( X\) satisfying \( \sigma(t_0)=\gamma(t_0)\) with \( t_0\in I_{m+1}\). Thus \( \sigma=\gamma\) on \( I_{m+1}\). We deduce that \( \sigma=\gamma\) on \( \bigcup_{i=1}^nI_i\) and in particular \( \sigma=\gamma\) on \( K\).

	Since the whole is valid for every compact \( K\subset \eR\) we conclude that \( \sigma=\gamma\) on \( \eR\).
\end{proof}

Let \( X\) be a vector field on \( M\) and \( p\in M\). We denote by \( \Phi(p,t)=\Phi_t(p)\in M\) the point \( \gamma_p(t)\) where \( \gamma_p\) is the integral curve of proposition \ref{PROPooQNWOooBOxYtu}. In other terms, for every \( p\in M\), there exist an interval \( I_p\) such that
\begin{equation}
	\Dsdd{ \Phi(p,t) }{t}{t_0}=X_{\Phi(p,t_0)}.
\end{equation}
and for every \( p\in M\), we have \( \Phi(p,0)=p\).


\begin{proposition}[\cite{BIBooDPYDooWgxxQH}]       \label{PROPooDPXIooTvXOIP}
	Let \( X,Y\) be smooth vectors fields on the manifold \( M\). Let \( \Phi^X\) and \( \Phi^Y\) be their flows\footnote{Proposition \ref{PROPooJACTooXBSxfE}.}. The following are equivalent:
	\begin{enumerate}
		\item
		      \( [X,Y]=0\),
		\item
		      \( (d\Phi_t^X)Y=Y\)
		\item
		      For every \( t,u\) in the domains, \( \Phi_t^X\circ\Phi_u^Y=\Phi^Y_u\circ\Phi^X_t\).
	\end{enumerate}
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Next stuff}
%---------------------------------------------------------------------------------------------------------------------------

The manifold could, of course have some additional structure which allows to write the differential quotient \eqref{EQooVMGFooFUCNEY}. This is the case when \( M=\eR^n\) or when \( M\) is a matrix group. In these cases, the question of the link between \( \gamma'(0)\) and the ``true'' derivative of \( \gamma\) has to be studied.

In that case we have the same notational problem with ``$df$''. Let \( f\colon M\to \eR\) where \( M\) is a manifold like \( \eR^n\). The symbol \( df_a\) with \( a\in M\) can be the differential of \( f\) as function \( M\to \eR\), so that \( df_a\) is a linear map from \( \eR^n\) to \( \eR\). But \( df_a\) can also be the linear map \( df_a\colon T_aM\to T_{f(a)}\eR\) where the spaces \( T_aM\) and \( T_{f(a)}\eR\) are made of differential operators.

This is the point of the section \ref{SECooTSAJooNtjgMD}.

Using the chain rule $d(g\circ f)(a)=dg(f(a))\circ df(a)$ for the differentiation in $\eR^n$, one sees that this equivalence notion doesn't depend on the choice of $\varphi$. In other words, if $\varphi$ and $\tilde{\varphi}$ are two charts for a neighbourhood of $x$, then $(\varphi^{-1} \circ\gamma)'(0)=(\varphi^{-1} \circ\sigma)'(0)$ if and only if $(\tilde{\varphi}^{-1} \circ\gamma)'(0)=(\tilde{\varphi}^{-1} \circ\sigma)'(0)$. The space of all tangent vectors at $x$ is denoted by $T_xM$. There exists a bijection $[\gamma]\leftrightarrow (\varphi^{-1}\circ\gamma)'(0)$ between $T_xM$ and $\eR^n$, so $T_xM$ is endowed with a vector space structure.

If $(\mU,\varphi)$ is a chart around $X(0)$, we can express $Xf$ using only well know objects by defining the function $\tilde f =f\circ\varphi$ and $\tX=\varphi^{-1}\circ X$
\[
	Xf=\Dsdd{ (\tilde f \circ\tX)(t) }{t}{0}=\left.\dsd{\tilde f }{x^{\alpha}}\right|_{x=\tX(0)}\left.\frac{d\tX^{\alpha}}{dt}\right|_{t=0}.
\]
In this sense, we write
\begin{equation}
	X=\frac{d\tX^{\alpha}}{dt} \dsd{}{x^{\alpha}}
\end{equation}
and we say that $\{\partial_1,\ldots,\partial_n\}$ is a basis of $T_xM$. As far as notations are concerned, from now a tangent vector is written as $X=X^{\alpha}\partial_{\alpha}$ where $X^{\alpha}$ is related to the path $\dpt{X}{\eR}{M}$ by $X^{\alpha}=d\tX^{\alpha}/dt$. We will no more mention the chart $\varphi$ and write
\[
	Xf=\Dsdd{f(X(t))}{t}{0}.
\]
Correctness of this short notation is because the equivalence relation is independent of the choice of chart. When we speak about a tangent vector to a given path $X(t)$ without specification, we think about $X'(0)$.

All this construction gives back the notion of tangent vector when $M\subset \eR^m$. In order to see it, think to a surface in $\eR^3$. A tangent vector is precisely given by a derivative of a path: if $\dpt{c}{\eR}{\eR^n}$ is a path in the surface, a tangent vector to this curve is given by
\[
	\lim_{t\to 0}\frac{c(t_0)-c(t_0+t)}{t}
\]
which is a well know limit of a difference in $\eR^n$.

\label{pg:vecto_vecto}Let us precise how does a tangent vector acts on maps others than $\eR$-valued functions. If $V$ is a vector space and $\dpt{f}{M}{V}$, we define
\[
	Xf=(Xf^i)e_i
\]
where $\{e_i\}$ is a basis of $V$ and the functions $\dpt{f^i}{M}{\eR}$, the decomposition of $f$ with respect to this basis. If we consider a map $\dpt{\varphi}{M}{N}$ between two manifolds, the natural definition is $Xf:=dfX$. More precisely, if we consider local coordinates $x^{\alpha}$ and a function $\dpt{f}{M}{\eR}$,
\begin{equation}\label{eq:dvp_phi}
	(d\varphi X)f=\Dsdd{  (f\circ\varphi\circ X)(t) }{t}{0}=\dsd{f}{x^{\alpha}}\dsd{\varphi^{\alpha}}{x\hbeta}\frac{dX\hbeta}{dt}.
\end{equation}
Now we are in a notational trouble: when we write $X=X^{\alpha}\partial_{\alpha}$, the ``$X^{\alpha}$``{} is the derivative of the ``$X^{\alpha}$``{} which appears in the path $X(t)=(X^1(t),\ldots,X^n(t))$ which gives $X$ by $X=X'(0)$. So equation \eqref{eq:dvp_phi} gives
\begin{equation}
	X(\varphi):=d\varphi X=X\hbeta(\partial_{\beta}\varphi^{\alpha})\partial_{\alpha}.
\end{equation}

\subsection{Differential of a map}
%------------------------------------------

Let $\dpt{f}{M_1}{M_2}$ be a differentiable map, $x\in M_1$ and $X\in T_xM_1$, i.e. $\dpt{X}{\eR}{M_1}$ with $X(0)=x$ and $X'(0)=X$. We can consider the path $Y=f\circ X$ in $M_2$. The tangent vector to this path is written $df_x X$.

\begin{proposition}
	If $\dpt{f}{M_1}{M_2}$ is a differentiable map between two differentiable manifolds, the map
	\begin{equation}
		\begin{aligned}
			df_x \colon T_xM_1 & \to T_{f(x)}M_2        \\
			X'(0)              & \mapsto (f\circ X)'(0)
		\end{aligned}
	\end{equation}
	is linear.
\end{proposition}

\begin{proof}
	We consider local coordinates $\dpt{x}{\eR^n}{M_1}$ and $\dpt{y}{\eR^m}{M_2}$. The maps $\dpt{f}{M_1}{M_2}$ and $\dpt{y^{-1}\circ f\circ x}{\eR^n}{\eR^m}$ will sometimes be denoted by the same symbol $f$. We have $(x^{-1}\circ X)(t)=(x_1(t),\ldots,x_n(t))$ and $(y^{-1}\circ Y)(t)=\big( y_1(x_1(t),\ldots,x_n(t),\ldots, y_m(x_1(t),\ldots,x_n(t)  \big)$, so that
	\[
		Y'(0)=\left(   \sum_{i=1}^n \dsd{y_1}{x_i}x_i'(0),\ldots,\sum_{i=1}^n \dsd{y_m}{x_i}x_i'(0)   \right)\in\eR^m
	\]
	which can be written in a more matricial way under the form
	\[
		Y'(0)=\left( \dsd{y_i}{x_j}x'_j(0) \right).
	\]
	So in the parametrisations $x$ and $y$, the map $df_x$ is given by the matrix $\partial y^i/\partial x_j$ which is well defined from the only given of $f$.
\end{proof}


Let $\dpt{x}{\mU}{M}$ and $\dpt{y}{\mV}{M}$ be two charts systems around $p\in M$. Consider the path $c(t)=x(0,\ldots,t,\ldots 0)$ where the $t$ is at the position $k$. Then, with respect to these coordinates,
\[
	c'(0)f=\Dsdd{ f(c(t))  }{t}{0}=\dsd{f}{x^i}\frac{dc^i}{dt}=\dsd{f}{x^k},
\]
so $c'(0)=\partial/\partial x^k$. Here, implicitly, we wrote $c^i=(x^i)^{-1}\circ c$ where $(x^i)^{-1}$ is the $i$th component of $x^{-1}$ seen as element of $\eR^n$. We can make the same computation with the system $y$. With these abuse of notation,
\begin{equation}
	\dsd{}{x^i}=\sum_j\dsd{y^j}{x^i}\dsd{}{y^j}
\end{equation}
as it can be seen by applying it on any function $\dpt{f}{M}{\eR}$. More precisely if $\dpt{x}{\mU}{M}$ and $\dpt{y}{\mU}{M}$ are two charts (let $\mU$ be the intersection of the domains of $x$ and $y$), let $\dpt{f}{M}{\eR}$ and $\ovf=f\circ x$, $\tilde f =f\circ y$. The action of the vector $\partial_{x^i}$ of the function $f$ is given by
\[
	\partial_{x^i}f=\dsd{\ovf}{x^i}
\]
where the right hand side is a real number that can be computed with usual analysis on $\eR^n$. This real \emph{defines} the left hand side. Now, $\ovf=\tilde f \circ y^{-1}\circ x$, so that
\[
	\dsd{\ovf}{x^i}=\dsd{ (\tilde f \circ y^{-1}\circ x) }{x^i}=\dsd{\tilde f }{y^j}\dsd{y^j}{x^i}
\]
where $\dsd{\tilde f }{y^j}$ is precisely what we write now by $\partial_{y^j}f$ and $\dsd{y^j}{x^i}$ must be understood as the derivative with respect to $x^i$ of the function $\dpt{(y^{-1}\circ x)}{\eR^n}{\eR^n}$.

Let $\dpt{f}{M}{N}$ and $\dpt{g}{N}{\eR}$; the definitions gives
\[
	(df_xX)g=\Dsdd{(g\circ f)(X(t))}{t}{0}
	=\dsd{g}{y^i}\dsd{f^i}{x^{\alpha}}\frac{dX^{\alpha}}{dt}.
\]
This shows that $\dsd{f^i}{x^{\alpha}}\frac{dX^{\alpha}}{dt}$ is $(df_xX)^i$.  But $dX^{\alpha}/dt$ is what we should call $X^{\alpha}$ in the decomposition $X=X^{\alpha}\partial_{\alpha}$ then the matrix of $df$ is given by $\dsd{f^i}{x^{\alpha}}$. So we find back the old notion of differential.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Examples}
%---------------------------------------------------------------------------------------------------------------------------

\subsubsection{Example: the sphere}\index{sphere}

The sphere $S^n$ is the set
\[
	S^n=\{  (x_1,\ldots, x_{n+1})\in\eR^{n+1}\tq \|x\|=1  \}
\]
for which we consider the following open set in $\eR^n$:
\[
	\mU=\{  (u_1,\ldots,u_n)\in\eR^n\tq\|u\|<1  \}
\]
and the charts $\dpt{\varphi_i}{\mU}{S}$, and $\dpt{\tilde{\varphi}_i}{\mU}{S}$
\begin{subequations}
	\begin{align}
		\varphi_i(u_1,\ldots,u_n)         & =(u_1,\ldots,u_{i-1}, \sqrt{  1-\|u\|^2  },u_i,\ldots,u_n )   \\
		\tilde{\varphi}_i(u_1,\ldots,u_n) & =(u_1,\ldots,u_{i-1}, -\sqrt{  1-\|u\|^2  },u_i,\ldots,u_n ).
	\end{align}
\end{subequations}
These map are clearly injective. To see that $\varphi(\mU)\cup\tilde{\varphi}(\mU)=S$, consider $(x_1,\ldots,x_{n+1})\in S$. Then at least one of the $x_i$ is non zero. Let us suppose $x_1\neq 0$, thus $x_1^2=1-(x_2^2+\cdots+x_{n+1}^2)$ and
\begin{equation}\label{eq:xupm}
	x_1=\pm\sqrt{1-(\ldots)}.
\end{equation}
If we put $u_i=x_{i+1}$, we have $x=\varphi(u)$ or $x=\tilde{\varphi}(u)$ following the sign in relation \eqref{eq:xupm}. The fact that $\varphi^{-1}\circ\tilde{\varphi}$ and $\tilde{\varphi}^{-1}\circ\varphi$ are differentiable is a ``first year in analysis exercise``.

\subsubsection{Example: projective space}

On $\eR^{n+1}\setminus\{o\}$, we consider the equivalence relation $v\sim\lambda w$ for all non zero $\lambda\in\eR$, and we put
\[
	\eR P^n=\left(\eR^{n+1}\setminus\{o\}\right)/\sim.
\]
This is the set of all the one dimensional subspaces of $\eR^{n+1}$. This is the real \defe{projective space}{projective!real space} of dimension $n$. We set $\mU=\eR^n$ and
\[
	\varphi_i(u_1,\ldots,u_n)=\Span\{ (u_1,\ldots,u_{i-1},1,u_i,\ldots,u_n) \}.
\]
One can see that this gives a manifold structure to $\eR P^n$. Moreover, the map
\begin{equation}
	\begin{aligned}
		A \colon S^n & \to \eR P^n\
		v            & \mapsto \Span v
	\end{aligned}
\end{equation}
is differentiable.

Let us show how to identify $\eR\cup\{ \infty \}$ to $\eR P^1$, the set of directions in the plane $\eR^2$. Indeed consider any vertical line $l$ (which does contain the origin). A non vertical vector subspace of $\eR^2$ intersects $l$ in one and only one point, while the vertical vector subspace is associated with the infinite point.

\subsection{Some Leibniz formulas}

\begin{lemma}[\cite{kobayashi}]
	If $M$ and $N$ are two manifolds, we have a canonical isomorphism
	\[
		T_{(p,q)}(M\times N)\simeq T_pM+T_qN.
	\]
	\label{lemLeibniz}
\end{lemma}

\begin{proof}
	A $Z\in T_{(p,q)}(M\times N)$ is the tangent vector to a curve $(x(t),y(y))$ in $M\times N$. We can consider $X\in T_pM$ given by $X=x'(0)$ and $Y\in T_qN$ given by $Y=y'(0)$. The isomorphism is the identification $(X,Y)\simeq Z$. Indeed, let us define $\oX\in T_{(p,q)}(M\times N)$, the tangent vector to the curve $(x(t),q)$, and $\oY\in T_{(p,q)}(M\times N)$, the tangent vector to the curve $(p,y(t))$. Then $Z=\oX+\oY$ because for any $\dpt{f}{M\times N}{\eR}$,
	\begin{equation}
		Zf=\dsdd{f(x(t),y(t))}{t}{0}
		=\dsdd{f(x(t),y(0))}{t}{0}+\dsdd{f(x(0),y(t))}{t}{0}
		=\oX f+\oY f.
	\end{equation}
\end{proof}

\begin{proposition}[Leibniz formula] \label{Leibniz}
	Let us consider $M,N,V$, three manifold; a map $\dpt{\varphi}{M\times N}{V}$ and a vector $Z\in T_{(p,q)}(M\times N)$ which corresponds (lemma~\ref{lemLeibniz}) to $(X,Y)\in T_pM+T_qN$.

	If we define $\dpt{\varphi_1}{M}{V}$ and  $\dpt{\varphi_2}{N}{V}$ by $\varphi_1(p')=\varphi(p',q)$ and $\varphi_2(q')=\varphi(p,q')$, we have the \defe{Leibniz formula}{Leibniz formula}:
	\begin{equation}
		d\varphi(Z)=d\varphi_1(X)+d\varphi_2(Y).
	\end{equation}
\end{proposition}
\begin{proof}
	Since $Z=\oX+\oY$, we just have to remark that
	\[
		d\varphi(\oX)=\dsdd{\varphi(x(t),q)}{t}{0}=d\varphi_1(X),
	\]
	so $d\varphi(Z)=d\varphi(\oX+\oY)=d\varphi_1(X)+d\varphi_2(Y)$.
\end{proof}
One of the most important application of the Leibniz rule is the corollary~\ref{cor_PrincLeib} on principal bundles.

\subsection{Cotangent bundle}

A form on a vector space $V$ is a linear map $\dpt{\alpha}{V}{\eR}$. The set of all forms on $V$ is denoted by $V^*$ and is called the \defe{dual space}{dual!of a vector space} of $V$. On each point of a manifold, one can consider the tangent bundle which is a vector space. Then one can consider, for each $x\in M$ the dual space $T^*_xM:=(T_xM)^*$ which is called the \defe{cotangent bundle}{cotangent bundle}. A $1$-\defe{differential form}{differential!form} on $M$ is a smooth map $\dpt{\omega}{M}{T^*M}$ such that $\omega_x:=\omega(x)\in T^*_xM$. So, for each $x\in M$, we have a $1$-form $\dpt{\omega_x}{T_xM}{\eR}$.

Here, the smoothness is the fact that for any smooth vector field $X\in\cvec(M)$, the map $x\to\omega_x(X_x)$ is smooth as function on $M$. One often considers vector-valued forms. This is exactly the same, but $\omega_xX_x$ belongs to a certain vector space instead of $\eR$. The set of $V$-valued $1$-forms on $M$ is denoted by $\Omega(M,V)$ \nomenclature{$\Omega(M,V)$}{$V$ valued $1$-forms} and simply $\Omega(M)$ if $V=\eR$
The cotangent space $T^*_pM$ of $M$ at $p$ is the dual space of $T_pM$, i.e. the vector space of all the (real valued) linear\footnote{When we say \emph{a form}, we will always mean \emph{a linear form}.} $1$-forms on $T_pM$. In the coordinate system $\dpt{x}{\mU}{M}$, we naturally use, on $T^*_pM$, the dual basis of the basis $\{\partial/\partial_{x^i},\ldots\partial/\partial_{x^i}\}$ of $T_pM$. This dual basis is denoted by $\{dx_1,\ldots,dx_n\}$, the definition being as usual:
\begin{equation}\label{eq:dx_v}
	dx_i(\partial^j)=\delta^j_i.
\end{equation}
The notation comes from the fact that equation \eqref{eq:dx_v} describes the action of the differential of the projection $\dpt{x_i}{\mU}{\eR}$ on the vector $\partial^j$.

If $(\mU_{\alpha},\varphi_{\alpha})$ is a chart of $M$, then the maps
\begin{equation}
	\begin{aligned}
		\phi_{\alpha} \colon \mU_{\alpha}\times\eR^n & \to T^*M\
		(x,a)                                        & \mapsto a^idx_i|_x
	\end{aligned}
\end{equation}
give to $T^*M$ a $2n$ dimensional manifold structure such that the canonical projection $\dpt{\pi}{T^*M}{M}$ is an immersion.

When $V$ is a finite-dimensional vector space, we denote by $V^*$ its dual\footnote{The vector space of all the linear map $V\to \eR$.} and we often use the identifications $V\simeq V^*\simeq T_vV\simeq T_wV\simeq T^*_vV$ where $v$ and $w$ are any elements of $V$. Note however that there are no \emph{canonical} isomorphism between these spaces, unless we consider some basis.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Immersion, embedding}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++


\begin{definition}[\cite{BIBooECJTooEfmLsr}]    \label{DEFooZEWNooMVOzWI}
	A smooth map \( f\colon M\to N\) between the manifolds \( M\) and \( N\) is an \defe{immersion}{immersion} if its differential \( df_p\colon T_pM\to T_{f(p)N}\) is injective for every \( p\in M\).
\end{definition}

\begin{definition}[Topological embedding\cite{BIBooTUOOooJZFtGe}]		\label{DEFooGXJNooUlWYuI}
	Let \( X\), \( Y\) be topological spaces. A map \( f\colon X\to Y\) is a \defe{topological embedding}{topological embedding} if
	\begin{enumerate}
		\item
		      \( f\) is continuous
		\item
		      \( f\) is injective
		\item
		      \( f\colon X\to f(X)\) is an homeomorphism when \( f(X)\) is equipped with the induced topology from \( Y\).
	\end{enumerate}
\end{definition}

\begin{definition}[Embedding\cite{BIBooTUOOooJZFtGe}]       \label{DEFooQLGLooNyXaOV}
	Let \( M\) and \( N\) be smooth manifolds. A smooth function \( f\colon M\to N\) is an \defe{embedding}{embedding} if
	\begin{enumerate}
		\item
		      \( f\) is an immersion\footnote{Definition \ref{DEFooZEWNooMVOzWI}},
		\item
		      \( f\) is a topological embedding\footnote{Definition \ref{DEFooGXJNooUlWYuI}.}.
	\end{enumerate}
\end{definition}

The following theorem says that immersions are local embeddings.
\begin{theorem}[\cite{BIBooCPDQooFrLOzh}]       \label{THOooXAOUooRKHMBm}
	Let \( M\) be a \( n\)-dimensional manifold and \( S\) be a \( m\)-dimensional manifold. Let \( f\colon M\to S\) be an immersion. For every \( a\in M\), there exists an open set \( \mO\) containing \( a\) in \( M\) such that \( f\colon \mO\to f(\mO)\) is an embedding.
\end{theorem}

\begin{proof}
	We consider maps \( \varphi\colon U\to M\) and \( \psi\colon V\to S\) around \( a\) and \( f(a)\) respectively. Then we consider :
	\begin{enumerate}
		\item
		      \( \tilde f=\psi^{-1}\circ f\circ\varphi\)
		\item
		      We rename \( U  \) in such a way that \( f\circ\varphi(U)\subset \psi(V)\) and we restrict \( \varphi\) to that subset of \( U\).
		\item
		      With that restriction of \( U\), the map \( \tilde f\colon U\to V\) is defined on \( U\).
		\item
		      \( q=\varphi^{-1}(a)\).
	\end{enumerate}

	By hypothesis, \( f\) is an immersion. It means that the map \( df_a\colon T_aM\to T_{f(a)}S\) is injective. The differential \( d\tilde f_q\colon \eR^n\to \eR^m\) of \( \tilde f\colon U\to V\) is injective too. This has two consequences :
	\begin{equation}
		\det(d\tilde f_q)\neq 0
	\end{equation}
	and its image has dimension \( n\). We suppose that
	\begin{equation}
		\Image(d\tilde f_q)=\Span\{ e_1,\ldots, e_n \}.
	\end{equation}
	If not, we consider a linear bijection map \( h\colon \eR^m\to \eR^m\) which maps \( \Image(d\tilde f_q)\) to \( \Span\{ e_1,\ldots, e_n \}\) and we rewrite the whole proof with \( \tilde f\circ h\) instead of \( \tilde f\).

	Let \( k=m-k\). We consider the map
	\begin{equation}
		\begin{aligned}
			\phi\colon U\times \eR^k          & \to \eR^m                                                                                                \\
			(x_1,\ldots, x_n,t_1,\ldots, t_k) & \mapsto \big( \tilde f(x)_1,\ldots, \tilde f(x)_n,\tilde f(x)_{n+1}+t_1,\ldots, \tilde f(x)_m+t_k \big).
		\end{aligned}
	\end{equation}
	It satisfies \( \phi(x,0)=\tilde f(x)\) and the matrix of its differential is
	\begin{equation}
		d\phi_{(q,0)}=\begin{pmatrix}
			d\tilde f_q & 0      \\
			M           & \mtu_k
		\end{pmatrix}
	\end{equation}
	where \( M\) a is some \( k\times n\) matrix. Since it is block-diagonal, its determinant is given by
	\begin{equation}
		\det\big( d\phi_{(q,0)} \big)=\det(d\tilde f_q)\det(\mtu_k)=\det(d\tilde f_q)\neq 0.
	\end{equation}
	Since \( \det(d\phi_{(q,0)})\neq 0\), we can apply the local inversion theorem \ref{THOooDWEXooMClWVi} for \( \phi\) at \( (a,0)\). We have a neighbourhood \( W\) of \( (q,0)\) in \( U\times \eR^k\) such that \( \phi\colon W\to \phi(W)\) is a diffeomorphism.

	Let \( U'=\proj_{\eR^n}(W)\). We will prove that \( \tilde f\colon U'\to \tilde f(U')\) is a diffeomorphism.
	\begin{subproof}
		\spitem[Injective]
		Let \( x,y\in U'\) such that \( \tilde f(x)=\tilde f(y)\). It means \( \phi(x,0)=\phi(y,0)\). The points \( (x,0)\) and \( (y,0)\) belong to \( W\) while \( \phi\) is injective on \( W\), thus \( (x,0)=(y,0)\) and \( x=y\).
		\spitem[Surjective]
		This is not an issue since \( \tilde f\) is obviously surjective on \( \tilde f(U')\).
		\spitem[The inverse]
		The prove that the inverse of \( \tilde f\) is \( \proj_{\eR^n}\circ \phi^{-1}\). Let \( w\in U'=\proj_{\eR^n}(W)\) and compute
		\begin{equation}
			(\proj_{\eR^n}\circ \phi^{-1}\circ\tilde f)(w)=(\proj_{\eR^n}\circ\phi^{-1})\big( \omega(w,0) \big)=\proj_{\eR^n}(w,0)=w.
		\end{equation}
		\spitem[Diffeomorphism]
		Since \( \proj_{\eR^n}\) and \( \phi^{-1}\) are differentiable, the inverse of \( \tilde f\) is differentiable and \( \tilde f\) is a diffeomorphism.
	\end{subproof}
	The fact for \( \tilde f\) to be a diffeomorphisms implies that \( f\) itself (restricted to \( \varphi(U')\)) is bijective, differentiable with the inverse being differentiable.
\end{proof}

\begin{proposition}[\cite{BIBooRNOWooEcnlIA,MonCerveau}]	\label{PROPooMNSUooSsnRkM}
	Let \( M\) be a \( n\)-dimensional \( C^k\) manifold. Let \( S\subset M\). We suppose that for each \( s\in S\), there is an open part \( U\subset\eR^n\) and a chart \(\varphi_s \colon U\to M  \) around \( s\) such that
	\begin{equation}
		S\cap\varphi_s(U)=\varphi_s\big( \{ x_1,\ldots,x_l,0,\ldots,0 \} \big).
	\end{equation}
	We define
	\begin{equation}
		\begin{aligned}
			\phi_s\colon \eR^l & \to S                                                   \\
			(x_1,\ldots,x_k)   & \mapsto \varphi_s\big( x_1,\ldots,x_l,0,\ldots,0 \big).
		\end{aligned}
	\end{equation}
	Then :
	\begin{enumerate}
		\item
		      The couple	\( (S,\{ \phi_s \}_{s\in S})\) is a \( C^k\) manifold of dimension \( l\).
		\item
		      This is a submanifold\footnote{Definition \ref{DEFooLQHWooMOTgzq}.} of \( M\).
		\item
		      The inclusion \(\tau \colon S\to M  \) is a \( C^k\) embedding.
		\item
		      This is the unique \( C^k\) manifold structure on \( S\) such that \( \tau\) is a \( C^k\) embedding.
	\end{enumerate}
	%TODOooKMNPooBQubfe. Prouver ça.
\end{proposition}


\begin{proposition}     \label{PROPooYHOKooYASzRL}
	If \( f\colon M\to N\) is an embedding, the image \( \Image(f)\) is a smooth \( \dim(M)\)-dimensional submanifold of \( N\).
\end{proposition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Distribution and Frobenius theorem}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooGTUCooYXyGvv}
	Let \( M\) be a \( C^2\) manifold. Let \(\varphi_{\alpha} \colon U_{\alpha}\to M  \) be a chart. We consider the associated tangent vectors \( \partial_i\).

	Let \( f\in C^2(M,\eR)\). We have
	\begin{equation}
		\partial_{\alpha,i}(\partial_{\alpha,j}f)=\partial_{\alpha,j}(\partial_{\alpha,i}f).
	\end{equation}
	In other words : \( [\partial_{\alpha,i},\partial_{\alpha,j}]=0\).
\end{lemma}

\begin{proof}
	Let \( q\in M\) and \( a=\varphi_{\alpha}^{-1}(q)\). By definition we have
	\begin{equation}
		(\partial_{\alpha,i}f)(q)=\frac{d}{dt} \left[ f\big( \varphi(a+te_i) \big)  \right]_{t=0}=\partial(f\circ\varphi)(a).
	\end{equation}
	So \( \partial_{\alpha,i}f=\partial_i(f\circ\varphi_{\alpha})\circ\varphi_{\alpha}^{-1}\). Now we compute
	\begin{subequations}
		\begin{align}
			\partial_{\alpha,j}(\partial_{\alpha,i}f)(q) & =\frac{d}{dt} \left[ \big( (\partial_{\alpha,i}f)\circ \varphi_{\alpha} \big)(a+te_j)  \right]_{t=0} \\
			                                             & =\frac{d}{dt} \left[ \partial_i(f\circ\varphi_{\alpha})(a+te_j)  \right]_{t=0}                       \\
			                                             & =\partial_j\big( \partial_i(f\circ\varphi_{\alpha}) \big)(a)                                         \\
			                                             & =\partial_{ji}^2(f\circ\varphi)(a).
		\end{align}
	\end{subequations}
	Since \( f\circ\varphi_{\alpha}\) is \( C^2\), theorem \ref{Schwarz} says \( \partial^2_{ij}(f\circ\varphi_{\alpha})=\partial^2_{ji}(f\circ\varphi_{\alpha})\).
\end{proof}

\begin{lemma}       \label{LEMooQXRSooHDRequ}
	Let \( M\) be a \( C^2\) manifold and \( f\in C^2(M)\). Let \( a,b\in  C^{2}(M)\). We have
	\begin{equation}
		[a\partial_i,b\partial_j]=a(\partial_ib)\partial_j-b(\partial_ja)\partial_i.
	\end{equation}
\end{lemma}

\begin{proof}
	Let \( f\in C^2(M)\). We compute using lemma \ref{LEMooGTUCooYXyGvv} :
	\begin{subequations}
		\begin{align}
			[g\partial_{\alpha,i},h\partial_{\alpha,j}]f & =g\partial_{\alpha,i}\big( h\partial_{\alpha,j}(f) \big)-h\partial_{\alpha,j}(g\partial_{\alpha,i}f)                                \\
			                                             & =g(\partial_{\alpha,i}h)(\partial_{\alpha,j}f)+gh\partial^2_{ij}f-h(\partial_{\alpha,j}g)(\partial_{\alpha,i}f)-hg\partial^2_{ji}f.
		\end{align}
	\end{subequations}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooGHQZooMIWAfn}
	Let \( M\) be a \( n\)-dimensional manifold. Let \( \varphi\colon U\to M\) be a chart, and \( X_1,\ldots, X_r\) (\( r<n\)) be vectors fields such that for each \( p\in \varphi(U)\), the set \( \{ X_i(p) \}_{i=1,\ldots, r}\) is linearly independent in \( T_pM\).

	There exist vector fields \( X_{r+1},\ldots,  X_n\) such that \( \{  X_1(p),\ldots, X_n(p) \}\) is a basis of \( T_pM\) for every \( p\in \varphi(U)\).
	%TODOooRDWOooFEVbJO. Prouver ça.
\end{lemma}

\begin{definition}      \label{DEFooYOMHooZJvsSt}
	Let \( M\) be a manifold. A \( k\) dimensional \defe{distribution}{distribution on a manifold} is the data, for each \( p\in M\) of a \( k\) dimensional vector subspace \( \mD_p\) of \( T_pM\).

	The distribution \( \mD\) is said to be \defe{smooth}{smooth distribution} if for each \( p\in M\), there exists a neighbourhood \( S\) of \( p\) and smooth vector fields\footnote{Definition \ref{DEFooAATTooLhNqDb}.} \( \{ X_i \}_{i=1,\ldots, k}\) in \( S\) such that
	\begin{equation}
		\mD_p=\Span\{ X_i(p) \}.
	\end{equation}

	We denote by \( \Gamma^{\infty}(\mD) \) the set of smooth vector fields with values in \( \mD\). These are the smooth vector fields \( X\) such that \( X_p\in \mD_p\) for every \( p\). More precisely,
	\begin{equation}
		\Gamma^{\infty}(\mD)=\{ X\in \Gamma(TM)\tq X_p\in\mD_p\,\forall p\in M \}.
	\end{equation}
\end{definition}

\begin{definition}
	Let \( \mD\) be a distribution on \( M\). A smoothly immersed\footnote{Definition \ref{DEFooZKUIooXWVGvh}.} submanifold \( N\subset M\) is an \defe{integral manifold}{integral manifold} of \( \mD\) if for every \( p\in N\) we have
	\begin{equation}
		di_p(T_pN)=\mD_p
	\end{equation}
	where \( i\colon N\to M\) is the immersion.

	When an integral submanifold exists, we say that the distribution is \defe{integrable}{integrable distribution}.
\end{definition}

\begin{definition}
	A distribution \( \mD\) is \defe{involutive}{involutive distribution} if we have
	\begin{equation}
		[X,Y]\in \Gamma^{\infty}(\mD)
	\end{equation}
	for every \( X,Y\in \Gamma^{\infty}(TM)\). In other words, for every \( p\in M\) we have \( [X,Y]_p\in \mD_p\).
\end{definition}

\begin{proposition}     \label{PROPooDYJNooAwnaFK}
	An smooth integrable distribution is involutive.
\end{proposition}

\begin{proof}
	Let \( N\) be a smooth integral manifold for the distribution \( \mD\).  Let \( X,Y\in\Gamma^{\infty}(\mD)\). Since \( T_pN=\mD_p\), for each \( p\in M\), we have \( X,Y\in\Gamma^{\infty}(TN)\). So
	\begin{equation}
		[X,Y]\in\Gamma^{\infty}(TN)=\Gamma^{\infty}(\mD)
	\end{equation}
	and \( \mD\) is involutive.
\end{proof}

\begin{theorem}[Frobenius\cite{BIBooJMRFooTAhhcg,BIBooDPYDooWgxxQH}]        \label{THOooDVBHooGRhuGl}
	An involutive distribution on a smooth manifold is integrable.
\end{theorem}

\begin{proof}
	Let \( \mD\) be an involutive \( r\)-dimensional distribution of the smooth \( n\)-dimensional manifold \( M\). Let \( p\in M\). We will build a submanifold \( N\) of \( M\) such that \( T_qN=\mD_q\) for every \( q\) in a neighbourhood of \( p\). By definition of a smooth distribution, there exists a neighbourhood \( S\) of \( p\) and vector fields \( \{ Y_i \}_{i=1,\ldots, r}\) such that
	\begin{equation}
		D_q=\Span\{ Y_i(q) \}_{i=1,\ldots, r}
	\end{equation}
	for every \( q\in S\). The vectors \( Y_i(q)\) are linearly independent since there are \( r\) of them and they span the \( r\)-dimensional vector subspace \( \mD_q\) of \( T_qM\).

	We may reduce \( S\) to fit in a local chart \( \varphi\colon U\to M\) around \( p\). We denote by \( \partial_i\) the basis vector associated with the chart \( \varphi\). For some \( a_{ij}(q)\) we have
	\begin{equation}
		Y_i=\sum_{j=1}^na_{ij}\partial_j.
	\end{equation}
	We use the lemma \ref{LEMooGHQZooMIWAfn} to consider \( Y_{r+1},\ldots,  Y_n\) such that, for every \( q\in \varphi(U)\), \( \Span\{ Y_1(q),\ldots, Y_n(q) \}=T_qM\). Each of these \( Y_i\) can be decomposed in the basis \( \{ \partial_i \}\):
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}(q)\partial_j(q).
	\end{equation}
	The matrix \( a_{ij}(p)\) is invertible. By lemma \ref{LEMooMCIDooYBHrbq}, let \( \sigma\in S_n\) be such that the matrix \( A\in \eM(r,\eR)\) given by
	\begin{equation}
		A_{ij}=a_{i,\sigma^{-1}(j)}(p)
	\end{equation}
	is invertible. We also consider the associated linear bijection
	\begin{equation}
		\begin{aligned}
			\sigma\colon \eR^n & \to \eR^n              \\
			e_i                & \mapsto e_{\sigma(i)}.
		\end{aligned}
	\end{equation}
	Now we consider a new chart \( U'=\sigma^{-1}(U)\) with
	\begin{equation}
		\begin{aligned}
			\varphi'\colon U' & \to M                             \\
			x                 & \mapsto (\varphi\circ \sigma)(x).
		\end{aligned}
	\end{equation}
	By continuity of the determinant, there exists an open neighbourhood \( S\) of \( p\) such that \( \det\big( a_{i\sigma^{-1}(j)}(q) \big)\neq 0\) for every \( q\in S\). We restrict \( U'\) in such a way for \( \varphi'(U')\) to fit in \( S\).

	We denote by \( \partial'_i\) the corresponding vectors in \( TM\). We have \( \partial'_i(q)=\partial_{\sigma(i)}(q)\); indeed let \( q=\varphi'(v')\) and compute:
	\begin{subequations}
		\begin{align}
			\partial'_i(q)f & =\Dsdd{ (f\circ\varphi')(v'+te_i) }{t}{0}                            \\
			                & =\Dsdd{ (f\circ\varphi)\big( \sigma(v')+te_{\sigma(i)} \big) }{t}{0} \\
			                & =\partial_{\sigma(i)}(q)f
		\end{align}
	\end{subequations}
	because \( \varphi\big( \sigma(v') \big)=q\). We can express \( Y_i\) in terms of \( \partial_j'\) instead of \( \partial_j\):
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}(q)\partial_j(q)=\sum_ja_{ij}(q)\partial'_{\sigma(j)}(q)=\sum_ja_{i\sigma^{-1}(j)}(q)\partial'_j(q).
	\end{equation}
	For the sake of simplifying the notations, we redefine \( U\) for being \( U'\), \( \varphi\) for \( \varphi'\) and so on. Thus we have
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}\partial_j(q)
	\end{equation}
	with \( (a_{ij})_{i,j=1,\ldots, r} \) being invertible.

	Let \( A\in \GL(r,\eR)\) be the matrix given by \( A_{ij}=a_{ij}\). We consider \( B=A^{-1}\) and, for \( i=1,\ldots, r\), we define
	\begin{equation}
		X_i=\sum_{k=1}^rB_{ik}Y_k.
	\end{equation}
	We can express them in terms of the \( \partial_i\)'s:
	\begin{subequations}
		\begin{align}
			X_i & =\sum_{k=1}^rB_{ik}Y_k                                                                     \\
			    & =\sum_{k=1}^rB_{ik}\sum_ja_{kj}\partial_j                                                  \\
			    & =\sum_{k=1}^rB_{ik}\big( \sum_{j=1}^ra_{kj}\partial_j+\sum_{j=r+1}^na_{kj}\partial_j \big) \\
			    & =\sum_{k,j=1}^rB_{ik}a_{kj}\partial_j+\sum_{k=1}^r\sum_{j=r+1}^nB_{ik}a_{kj}\partial_j     \\
			    & =\sum_{j=1}^r\delta_{ij}\partial_j+\sum_{j=r+1}^nc_{ij}\partial_j                          \\
			    & =\partial_j+\sum_{j=r+1}^nc_{ij}\partial_j
		\end{align}
	\end{subequations}
	where \( c_{ij}\) is matrix whose value has no importance here.

	We know that \( \mD_q=\Span\{ Y_1(q),\ldots, Y_r(q) \}\) for every \( q\). Since the \( Y_i\)'s are linearly independent and since the matrix \( B\) is invertible, the vectors \( \{ X_i(q) \}_{i=1,\ldots, r}\) are linearly independent and belong to \( \mD_q\). Thus we have
	\begin{equation}
		\mD_q=\Span\{ X_1(q),\ldots, X_r(q)  \}.
	\end{equation}

	We compute the commutators of the \( X_i\)'s using the lemma \ref{LEMooQXRSooHDRequ}:
	\begin{subequations}
		\begin{align}
			[X_i,X_j] & =[\partial_i,\partial_j]+\sum_{k=r+1}^n[\partial_i,c_{ik}\partial_k]+\sum_{k=r+1}^n[c_{ik}\partial_k,\partial_j]+\sum_{k=r+1}^n\sum_{l=r+1}^n[c_{ik}\partial_k,c_{jl}\partial_l] \\
			          & =\sum_{k=r+1}^nd_k\partial_k
		\end{align}
	\end{subequations}
	for some functions \( d_k\). Now the distribution \( \mD\) is involutive, so there exists functions \( f_k\) such that
	\begin{equation}
		[X_i,X_j]=\sum_{k=1}^rf_kX_k=\sum_{k=1}^rf_k\big( \partial_k+\sum_{l=r+1}^nc_{kl}\partial_l \big).
	\end{equation}
	Thus we have
	\begin{equation}
		\sum_{k=r+1}^nd_k\partial_k=\sum_{k=1}^rf_k\big( \partial_k+\sum_{l=r+1}^nc_{kl}\partial_l \big).
	\end{equation}
	On the left hand side there are no components in \( \{ \partial_k \}_{k=1,\ldots, r}\); then on the right hand side we deduce \( \sum_{k=1}^rf_k\partial_k=0\) and then \( f_k=0\) for every \( k=1,\ldots, r\). We proved that
	\begin{equation}
		[X_i,X_j]=0.
	\end{equation}
	We consider the flows \( \Phi^i\) of each \( X_i\) (proposition \ref{PROPooJACTooXBSxfE}), and the map
	\begin{equation}
		\begin{aligned}
			s\colon I         & \to M                                                   \\
			(t_1,\ldots, t_r) & \mapsto (\Phi_{t_1}^1\circ \cdots\circ \Phi_{t_2}^t)(p)
		\end{aligned}
	\end{equation}
	where \( I\) is an open neighbourhood of \( (0,\ldots, 0)\) in \( \eR^r\). Since \( s\) is continuous, the part \( s^{-1}\big( \varphi_{\alpha}(U_{\alpha}) \big)\) is open. We redefine \( I\) as being \( I=\varphi_{\alpha}(U_{\alpha})\).

	The differential at \( 0\in \eR^r\) is a map \( ds_0\colon T_0\eR^r\to T_pM\) and satisfy
	\begin{equation}
		ds_0(\partial_i)\Dsdd{ \Phi_t^i(p) }{t}{0}=X_i(p).
	\end{equation}
	Since the vectors \( \{ X_i(p) \}\) are linearly independent, the map \( ds_0\) is injective. This means that \( s\colon I\to M\) is an immersion\footnote{Definition \ref{DEFooZEWNooMVOzWI}.}. Theorem \ref{THOooXAOUooRKHMBm} says that, reducing the size of \( I\), the map \( s\) becomes an embedding.

	We define \( N=\Image(s)\). This is a \( r\)-dimensional submanifold of \( M\) by proposition \ref{PROPooYHOKooYASzRL}. Since \( s(0)=p\) and \( ds_0(\partial_i)=X_i\) we have \( T_pN=\mD_p\). Our task is to prove that \( \mD_q=T_qN\) for every \( q\in N\).

	Let \( q\in N\). There exists \( (t_1,\ldots, t_r)\in \eR^r\) such that
	\begin{equation}
		q=s(t_1,\ldots, t_r)=(\Phi_{t_1}^1\circ \ldots\circ\Phi_{t_r}^r)(p).
	\end{equation}
	We fix \( i\). As a recall, the vector \( \partial_i\in T_{(t_1,\ldots, t_r)}\eR^n\) is the derivative of the path \( (t_1,\ldots, t_r)+te_i\). We have
	\begin{subequations}
		\begin{align}
			ds_{(t_1,\ldots, t_r)}(\partial_i) & =\Dsdd{ s\big( (t_1,\ldots, t_r)+te_i \big) }{t}{0}                                                           \\
			                                   & =\Dsdd{ (\Phi_t^i\circ\Phi_{t_1}^1\circ\ldots\circ\Phi_{t_r}^r)(p)    }{t}{0}     \label{SUBEQooDNCWooXPRFNz} \\
			                                   & =\Dsdd{ \Phi_t^i(q) }{t}{0}        \label{SUBEQooYCSQooFdYfPb}                                                \\
			                                   & =X_i(q).
		\end{align}
	\end{subequations}
	Justifications.
	\begin{itemize}
		\item For \eqref{SUBEQooDNCWooXPRFNz}. We used the property \eqref{EQooCHYXooCbECRN} of proposition \ref{PROPooJACTooXBSxfE} and the commutation of proposition \ref{PROPooDPXIooTvXOIP}.
		\item For \eqref{SUBEQooYCSQooFdYfPb}. By definition \( q=(\Phi_{t_1}\circ\ldots\circ\Phi_{t_r}^r)(p)\).
	\end{itemize}
	So \( \Span\{ X_1(q),\ldots, X_r(q) \}\) is a \( r\)-dimensional subspace of \( T_qM\) which is included in the \( r\)-dimensional subspace \( T_qN\). We conclude that
	\begin{equation}
		T_qN=\Span\{ X_1(q),\ldots, X_r(q) \}=\mD_qN.
	\end{equation}
\end{proof}


\begin{theorem}[Frobenius\cite{BIBooJMRFooTAhhcg,BIBooDPYDooWgxxQH}]      \label{THOooVRDYooIusxwW}
	About distributions.
	\begin{enumerate}
		\item       \label{ITEMooLDUZooJCwZek}
		      A distribution is integrable if and only if it is involutive.
		\item       \label{ITEMooCQEAooRsLSOV}
		      If \( \mD\) is involutive, for every \( p\in M\), there exists a unique maximal\quext{As far as understand, here «maximal» means that every integral manifold containing \( p\) is contained in that one. It is used in that sense during the proof the theorem \ref{THOooXALIooGiPVdD}.} connected integral submanifold containing \( p\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	Point \ref{ITEMooLDUZooJCwZek} is given by the proposition \ref{PROPooDYJNooAwnaFK} and the theorem \ref{THOooDVBHooGRhuGl}.

	Proof of point \ref{ITEMooCQEAooRsLSOV}. We consider \( \{ N_i \}_{i\in I}\) the set of all the connected integral submanifolds of \( M\) containing \( p\). By proposition \ref{PROPooBCFXooRlMvch}, this is connected and by \ref{PROPooFFDYooLmwISw}, this is a submanifold of \( M\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Analytic}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooPTEZooMZrAGg}]       \label{PROPooKAXOooZCwcwi}
	Let \( M\), \( N\) and \( S\) be analytic manifolds and \( f\colon M\to N\) be analytic.
	\begin{enumerate}
		\item
		      We suppose that \( f\) is an immersion and that \( g\colon S\to M\) is continuous. The map \( g\) is analytic if and only if \( f\circ g\) is analytic.
		\item
		      We suppose that \( f\) is a submersion and that \( g\colon N\to S\) is continuous. The map \( g\) is analytic if and only if \( g\circ f\) is analytic.
	\end{enumerate}
\end{proposition}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Tensor algebra}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Tensor algebra]      \label{DEFooHPQXooETvEyn}
	Let \( V\) be a vector space over \( \eC\). The \defe{tensor algebra}{tensor algebra} of \( V\) is the vector space
	\begin{equation}
		T(V)=\bigoplus_{n\geq 0}\left(\otimes^nV\right)=\eC\oplus V\oplus(V\otimes V)\oplus\ldots
	\end{equation}
\end{definition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exterior calculus}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exterior algebra}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Exterior product]
	If $V$ is a vector space, we denote by $\Lambda^kV^*$ the space of all the $k$-form on $V$. We define the \defe{exterior product}{product!exterior} $\dpt{\wedge}{\Lambda^kV^*\times\Lambda^lV^*}{\Lambda^{k+l}V^*}$ by
	\begin{equation}
		(\omega^k\wedge\eta^l)(v_1,\ldots,v_{k+l})
		=\us{k!l!}\sum_{\sigma\in S_{k+l}} (-1)^{\sigma}   \omega(v_{\sigma(1)},\ldots,v_{\sigma(k)})\eta(v_{\sigma(k+1)},v_{\sigma(k+1)})
	\end{equation}
\end{definition}
If $\{e_1,\ldots,e_n\}$ is a basis of $V$, the dual basis $\{\sigma^1,\ldots,\sigma^n\}$ of $V^*$ is defined by $\sigma^i(e_j)=\delta^i_j$.

If $I=\{1\leq i_1\leq\ldots i_k\leq n\}$, we write $\sigma^I=\sigma^{i_1}\wedge\ldots\sigma^{i_k}$ any $k$-form can be decomposed as
\[
	\omega=\sum_{I}\omega_I\sigma^I.
\]
The exterior algebra is provided with the \defe{interior product}{interior!product} denoted by $\iota$. It is defined by\label{pg_DefProdExt}
\begin{equation}
	\begin{aligned}
		\iota(v_0)\colon\Lambda^kW             & \to \Lambda^{k-1}W               \\
		(\iota(v_0)\omega)(v_1,\ldots,v_{k-1}) & =\omega(v_0,v_1,\ldots,v_{k-1}).
	\end{aligned}
\end{equation}

\begin{lemma}
	Let \( \sigma\) be an element of the symmetric group\footnote{Definition~\ref{DEFooJNPIooMuzIXd}.} of the set \( \{ a_1,\ldots, a_n \}\) where the \( a_i\) are integers. Then
	\begin{equation}
		(dx_{a_1}\wedge\ldots \wedge dx_{a_n})(e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})=(-1)^{\sigma}.
	\end{equation}
\end{lemma}

\begin{proof}
	We make it by induction over \( n\). With \( n=1\) the only permutation is the identity; the claim reduces to \( dx_{a_1}(e_{a_1})=1\). Let us try with \( n=2\). Up to renumbering we have
	\begin{equation}
		(dx_1\wedge dx_2)(e_1,e_2)=1
	\end{equation}
	and
	\begin{equation}
		(dx_1\wedge dx_2)(e_2,e_1)=-1.
	\end{equation}
	We pass to the induction. Let \( \sigma\in S_n\). We have
	\begin{subequations}
		\begin{align}
			(dx_{a_1}\wedge dx_{a_n})( & e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})=dx_{a_1}\wedge (dx_{a_n})(e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})                                                                   \\
			                           & =\sum_{\phi\in S_n}(-1)^{\phi}\frac{1}{ (n-1)! }dx_{a_1}\big( e_{\phi\sigma(a_1)} \big)(dx_{a_2}\wedge\ldots\wedge dx_{a_n})(e_{\phi\sigma(a_2)},\ldots, e_{\phi\sigma(a_n)}) \\
			                           & =\sum_{\phi\in S_n}(-1)^{\phi}\frac{1}{ (n-1)! }\delta_{a_1,\phi\sigma(a_1)}(-1)^{\phi\sigma}                                                                                 \\
			                           & =\sum_{\phi\in S_n}\delta_{a_1,\phi\sigma(a_1)}(-1)^{\sigma}\frac{1}{ (n-1)! }
		\end{align}
	\end{subequations}
	where we used the fact that the sign of a permutation provides a morphism between \( S_n\) and \( \{ -1,1 \}\) (proposition~\ref{ProphIuJrC}\ref{ITEMooBQKUooFTkvSu}). In the sum over \( S_n\), only the \( \phi\) that make \( \sigma(a_1)\to a_1\) remain; there are \( | S_{n-1} |=(n-1)!\) such elements. Thus the whole evaluates to \( (-1)^{\sigma}\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]    \label{LEMooICRXooFKPCRd}
	Let \( \tau_i\colon \eR^n\to \eR^{n-1}\) defined by
	\begin{equation}
		\tau_i(v)_k=\begin{cases}
			v_k     & \text{if } k<i             \\
			v_{k+1} & \text{if } k\geq i\text{.}
		\end{cases}
	\end{equation}
	Then we have
	\begin{equation}
		(dx_1\wedge\ldots\wedge\widehat{dx_i}\wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)=
		\det\Big(  \tau_i(v_1),\ldots, \widehat{\tau_i(v_i)},\ldots, \tau_i(v_n)  \Big)
	\end{equation}
	where the hat denotes a non present term.
\end{lemma}

\begin{proof}
	We extend \( \tau_i\) to the dual : \( \tau_i\colon(\eR^n)^*  \to (\eR^{n-1})^*\) is defined by
	\begin{equation}
		\tau_i(dx_k)=\begin{cases}
			dy_k     & \text{if } k<i \\
			dy_{k-1} & \text{if } k>i
		\end{cases}
	\end{equation}
	(not defined on \( dx_i\)). It is easy to check that, if \( k\neq i\),
	\begin{equation}
		\tau_i(dx_k)\tau_i(v)=dx_k(v).
	\end{equation}
	The value of  $(dx_1\wedge\ldots\wedge\widehat{dx_i}\wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)$ is a polynomial in the variables \( dx_k(v_l)\) (with \( k\neq l\)). Since \( dx_k(v_l)=\tau\i(dx_k)\big( \tau_iv_l \big)\), the same polynomial will give the value of
	\begin{equation}
		(\tau_idx_1\wedge\ldots\wedge \widehat{\tau_idx_i}\wedge\ldots\wedge \tau_idx_n  )(\tau_i v_1,\ldots, \widehat{\tau_iv_i},\ldots, \tau_iv_n).
	\end{equation}
	Thus we have
	\begin{subequations}
		\begin{align}
			(dx_1\wedge\ldots\wedge\widehat{dx_i} & \wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)                                                                               \\
			                                      & =(\tau_idx_1\wedge\ldots\wedge \widehat{\tau_idx_i}\wedge\ldots\wedge \tau_idx_n  )(\tau_i v_1,\ldots, \widehat{\tau_iv_i},\ldots, \tau_iv_n) \\
			                                      & =(dy_1\wedge\ldots\wedge dy_{n-1})(\tau_iv_1,\ldots,\widehat{\tau_iv_i},\ldots, \tau_iv_n) \label{SUBEQooQGSKooSgfxJh}                        \\
			                                      & =\det\big( \tau_iv_1,\ldots, \widehat{\tau_i v_i},\ldots, \tau_iv_n \big)
		\end{align}
	\end{subequations}
	The last equality is because \eqref{SUBEQooQGSKooSgfxJh} is is a \( (n-1)\)-form applied to \( n-1\) vectors in \( \eR^{n-1}\) and so is the determinant.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Differential of \texorpdfstring{$k$}{k}-forms}
%---------------------------------------------------------------------------------------------------------------------------

The differential of a $k$-form is defined by the following theorem.

\begin{theorem}
	Let $M$ be a differentiable manifold. Then for each $k\in \eN$, there exists an unique map
	\[
		\dpt{d}{\Omega^k(M)}{\Omega^{k+1}(M)}
	\]
	such that

	\begin{enumerate}
		\item $d$ is linear,
		\item for $k=0$, we find back the $\dpt{d}{\Cinf(M)}{\Omega^1(M)}$ previously defined,
		\item if $f$ is a function and $\omega^k$ a $k$-form, then
		      \begin{equation}
			      d(f\omega^k)=df\wedge\omega^k+fd\omega^k,
		      \end{equation}


		\item $d(\omega^k\wedge\eta^l)=d\omega^k\wedge\eta^l+(-1)^k\omega^k\wedge d\eta^l$,
		\item $d\circ d=0$.
	\end{enumerate}
\end{theorem}

An explicit expression for $d\omega^k$ is actually given by
\begin{equation}
	d\omega^k=\sum d\omega_I\wedge dx^I
\end{equation}
if $\omega^k=\sum\omega_I dx^I$.
An useful other way to write it is the following. If $\omega$ is a $k$-form and $X_1,\ldots,X_{p+1}$ some vector fields,
\begin{equation}\label{eq:formule_domega}
	\begin{split}
		(k+1)d\omega(X_1,\ldots,X_{p+1})&=\sum_{i=1}^{p+1}(-1)^{i+1}X_i\omega(X_1,\ldots\hat{X}_i,\ldots,X_{p+1})\\
		&\quad+\sum_{i<j}(-1)^{i+j}\omega([X_i,X_j],X_1,\ldots,\hX_i,\hX_j,\ldots,X_{p+1}).
	\end{split}
\end{equation}
Let us show it with $p=1$. Let $\omega=\omega_i dx^i$ and compute $d\omega(X,Y)=\partial_i\omega_j(dx^i\wedge dx^j)(X,Y)$. For this, we have to keep in mind that the $\partial_i$ acts only on $\omega_j$ while, in equation \eqref{eq:formule_domega}, a term $X\omega(Y)$ means --pointwise-- the action of $X$ on the function $\dpt{\omega(Y)}{M}{\eR}$. So we have to use Leibniz formula:
\[
	(\partial_i\omega_j)X^iY^j=(X\omega_j)Y^j
	=X(\omega_j Y^j)-\omega_j XY^j.
\]
On the other hand, we know that $[X,Y]^i=XY^i-YX^i$, so
\begin{equation}
	d\omega(X,Y)=X\omega(Y)-Y\omega(X)-\omega([X,Y]).
\end{equation}

\subsubsection{Hodge dual operator}
%/////////////////////////////
Let us take a manifold $M$ endowed with a metric $g$.  We can define a map $\dpt{r}{T^*_xM}{T_xM}$ by, for $\alpha\in T^*_xM$,
\[
	\scal{r(\alpha)}{v}=\alpha(v).
\]
for all $v\in T_xM$, where $\scal{\cdot}{\cdot}$ stands for the product given by the metric $g$. If we have $\alpha,\beta\in T^*_xM$, we can define
\[
	\scal{\alpha}{\beta}=\scal{r(\alpha)}{r(\beta)}.
\]
With this, we define an inner product on $\Lambda^p(T^*_xM)$:
\[
	\scal{\alpha_1\wedge\ldots\alpha_p}{\beta_1\wedge\ldots\beta_p}=\det_{ij}\scal{\alpha_i}{\beta_j}.
\]

\begin{definition}      \label{DEFooUOJQooSzKjNR}
	The \defe{Hodge operator}{Hodge operator} is $\dpt{\hodge}{\Lambda^p(T^*_xM)}{\Lambda^{n-p}(T^*_xM)}$ such that for any $\phi\in\Lambda^p(T^*_xM)$,
	\begin{equation}
		\phi\wedge(\hodge\psi)=\scal{\phi}{\psi}\Omega=\langle \phi,\psi \rangle\sqrt{|\det(g)|}dx^1\wedge\ldots\wedge dx^n.
	\end{equation}
\end{definition}

\begin{example} \label{EXooCIYIooFPMLMU}
	We consider \( \eR^n\) with the euclidian metric. If \( \sigma=dx_j\), then we expect \( \hodge\sigma\) to be \( sdx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n\) for a certain factor \( s\) to be fixed (something like \( (-1)^j\)).

	For every \( 1\)-form \( \phi\) we need \( \phi\wedge(\hodge \sigma)=\langle \phi, \sigma\rangle dx_1\wedge\ldots\wedge dx_n\). A basis of \( \Wedge^1(TM)\) is \( \{ dx_k \}_{k=1,\ldots, n}\), so we test on \( dx_k\).

	First we have
	\begin{equation}
		\langle dx_k, dx_j\rangle =\langle e_k, e_j\rangle =\delta_{kj}.
	\end{equation}
	Then
	\begin{equation}
		s\,dx_k\wedge dx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n=s\delta_{kj}(-1)^{j+1}dx_1\wedge\ldots\wedge dx_n.
	\end{equation}
	Thus we need \( s=(-1)^{j+1}\) and we have
	\begin{equation}
		\hodge dx_j=(-1)^{j+1}dx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n.
	\end{equation}
\end{example}


\subsection{Musical isomorphism}\label{subsec_musique}\index{musical isomorphism}
%---------------------------------

In some literature, we find the symbols $v^{\flat}$ and $\alpha^{\sharp}$. What does it mean ? For $X\in\cvec(M)$ and $\omega\in\Omega^2(M)$, the \defe{flat}{flat} operation $v^{\flat}\in\Omega^1(M)$ is simply defined by the inner product:
\begin{equation}        \label{EQooBTWXooTqoNxa}
	v^{\flat}=i(v)\omega
\end{equation}
In the same way, we define the \defe{sharp}{sharp} operation by taking a $1$-form $\alpha$ and defining $\alpha^{\sharp}$ by
\begin{equation}
	i(\alpha^{\sharp})\omega=\alpha.
\end{equation}
An immediate property is, for all $v\in\cvec(M)$, $v^{\flat\sharp}=v$, and for all $\alpha\in\Omega^1(M)$, $\alpha^{\sharp\flat}=\omega$.

\subsection{Pull-back and push-forward}

\begin{normaltext}
	Let $\dpt{\varphi}{M}{N}$ be a smooth map, $\alpha$ a $k$-form on $N$, and $Y$ a vector field on $N$. Consider the map $\dpt{d\varphi}{T_xM}{T_{\varphi(x)}M}$. The aim is to extend it to a map from the tensor algebra\footnote{Definition \ref{DEFooHPQXooETvEyn}} of ${T_xM}$ to the one of $T_{\varphi(x)}M$.
\end{normaltext}

The \defe{pull-back}{pull-back!of a $k$-form} of $\varphi$ on a $k$-form $\alpha$ is the map
\[
	\dpt{\varphi^*}{\Omega^k(N)}{\Omega^k(M)}
\]
defined by
\begin{equation}\label{306e1}
	(\varphi^*\alpha)_m(v_1,\ldots,v_k)
	=\alpha_{\varphi(m)}(d\varphi_mv_1,\ldots,d\varphi_mv_k)
\end{equation}
for all $m\in M$ and $v_i\in\cvec(M)$.

Note the particular case $k=0$. In this case, we take --instead of $\alpha$-- a function $\dpt{f}{N}{\eR}$ and the definition \eqref{306e1} gives $\dpt{\varphi^*f}{M}{\eR}$ by
\[
	\varphi^*f=f\circ\varphi.
\]

The \defe{push-forward}{push-forward!of a $k$-form} of $\varphi$ on a $k$-form is the map
\[
	\dpt{\varphi_*}{\Omega^k(M)}{\Omega^k(N)}
\]
defined by $\varphi_*=(\varphi^{-1})^*$. For $v\in T_nN$, we explicitly have:
\[
	(\varphi_*\alpha)_n(v)=\alpha_{\varphi^{-1}(n)}(d\varphi_n^{-1} v).
\]

\begin{definition}	\label{DEFooAHHCooHQMHRk}
	Let now $\dpt{\varphi}{M}{N}$ be a diffeomorphism. The \defe{pull-back}{pull-back!of a vector field} of $\varphi$ on a vector field is the map
	\[
		\dpt{\varphi^*}{\cvec(N)}{\cvec(M)}
	\]
	defined by
	\[
		(\varphi^*Y)(m)=[(d\varphi^{-1})_m\circ Y\circ\varphi](m),
	\]
	or
	\[
		(\varphi^*Y)_{\varphi^{-1}(n)}=(d\varphi^{-1})_nY_n,
	\]
	for all $n\in N$ and $m\in M$. Notice that \[\dpt{(d\varphi^{-1})_n}{T_nN}{T_{\varphi^{-1}(n)}M},\] and that  $\varphi^{-1}(n)$ is well defined because $\varphi$ is an homeomorphism.
\end{definition}

\begin{definition}	\label{DEFooYTPYooLlyuvd}
	The \defe{push-forward}{push-forward!of a vector field} is, as before, defined by $\varphi_*=(\varphi^{-1})^*$. In order to show how to manipulate these notations, let us prove the following equation:
	\[
		f_{*\xi}=(df)_{\xi}.
	\]
	For $\dpt{\varphi}{M}{N}$ and $Y$ in $\cvec(N)$, we just defined $\dpt{\varphi^*}{\cvec(N)}{\cvec(M)}$, by
	\begin{eqnarray}
		\label{2112r1}(\varphi^*Y)_{\varphi^{-1}(n)}=(d\varphi^{-1})_nY_n.
	\end{eqnarray}
	Take $\dpt{f}{M}{N}$; we want to compute $f_*=(f^{-1})^*$ with $\dpt{(f^{-1})^*}{\cvec(M)}{\cvec(N)}$. Replacing the ``$^{-1}$``\ on the right places, the definition \eqref{2112r1} gives us
	\[
		\Big[(f^{-1})^*X\Big]_{f(m)}=(df)_mX_m,
	\]
	if $X\in\cvec(M)$, and $m\in M$.
\end{definition}

We can rewrite it without  any indices: the coherence of the spaces automatically impose the indices: $(f^{-1})^*X=(df)X$. It can also be rewritten as $(f^{-1})^*=df$, and thus $f_*=df$. From there to $f_{* \xi}=(df)_{\xi}$, it is straightforward.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Lie derivative}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Consider $X\in\cvec(M)$ and $\alpha\in\Omega^p(M)$. Let $\dpt{\varphi_t}{M}{M}$ be the flow of $X$. The \defe{Lie derivative}{Lie!derivative!of a $p$-form} of $\alpha$ is
\begin{equation}\label{liesurforme}
	\mL_X\alpha=\lim_{t\to 0}\us{t}[(\varphi^*_t\alpha)-\alpha]=\dsdd{\varphi^*_t\alpha}{t}{0}.
\end{equation}
More explicitly, for $x\in M$ and $v\in T_xM$,
\[
	(\mL_X\alpha)_x(v)=\lim_{t\to 0}\us{t}\left[(\varphi_t^*\alpha)_x(v)-\alpha_x(v)\right]
\]
In the definition of the \defe{Lie derivative}{Lie!derivative!of a vector field} for a vector field, we need an extra minus sign:
\begin{equation}		\label{EqDefLieDerivativeVect}
	(\mL_XY)_x=\dsdd{\varphi_{-t*}Y_{\varphi_t(x)}}{t}{0}.
\end{equation}
Why a minus sign ? Because $Y_{\varphi_t(x)}\in T_{\varphi_t(x)}M$, but $\dpt{(d\varphi_{-t})_a}{T_aM}{T_{\varphi_{-t}(a)}M}$ so that, if we want, $\varphi_{-t*}Y_{\varphi_t(x)}$ to be a vector at $x$, we can't use $\varphi_{t*}$.

These two definitions can be embedded in only one. Let $X\in\cvec(M)$ and $\varphi_t$ its integral curve\footnote{\textit{i.e.} for all $x\in M$, $\varphi_0(x)=x$ and $\dsdd{\varphi_{u+t}(x)}{t}{0}=X_{\varphi_u(x)}$.}\index{integral!curve}. We know that $\varphi_{t*}$ is an isomorphism $\dpt{\varphi_{t*}}{T_{\varphi^{-1}(x)}M}{T_xM}$. It can be extended to an isomorphism of the tensor algebras at $\varphi^{-1}(x)$ and $x$. We note it $\tilde{\varphi}_t$. For all tensor field $K$ on $M$, we define
\[
	(\mL_XK)_x=\lim_{t\to 0}[K_x-(\tilde{\varphi_t}K)_x].
\]

On a Riemannian manifold $(M,g)$, a vector field $X$ is a \defe{\href{http://en.wikipedia.org/wiki/Killing_vector_fields}{Killing vector field}}{killing!vector field} if $\mL_Xg=0$.



\begin{lemma}
	Let $\dpt{f}{(-\epsilon,\epsilon)\times M}{\eR}$ be a differentiable map with $f(0,p)=0$ for all $p\in U$. Then there exists $\dpt{g}{(-\epsilon,\epsilon)\times M}{\eR}$, a differentiable map such that $f(t,p)=tg(t,p)$ and
	\[
		g(0,q)=\left.\dsd{f(t,q)}{t}\right|_{t=0}.
	\]
\end{lemma}
\begin{proof}
	Take
	\[
		g(t,q)=\int_0^1\dsd{f(ts,p)}{(ts)}ds,
	\]
	and use the change of variable $s\to ts$.
\end{proof}

\begin{lemma}
	If $\varphi_t$ is the integral curve of $X$, for all function $\dpt{f}{M}{\eR}$, there exists a map $g$, $g_t(p)=g(t,p)$ such that
	$f\circ\varphi_t=f+tg_t$ and $g_0=Xf$.
\end{lemma}

\begin{proof}
	Consider $\overline{f}(t,p)=f(\varphi_t(p))-f(p)$, and apply the lemma:
	\[
		f\circ\varphi_t=tg_t(p)+f(p).
	\]
	Thus we have
	\[
		Xf=\lim_{t\to 0}\us{t}[f(\varphi_t(p))-f(p)]=\lim_{t\to 0}g_t(p)=g_0(p).
	\]
\end{proof}

One of the main properties of the Lie derivative is the following:
\begin{theorem}		\label{ThoLieDerrComm}
	Let $X$, $Y\in\cvec(M)$ and $\varphi_t$ be the integral curve of $X$. Then
	\[
		[X,Y]_p=\lim_{t\to 0}\us{t}[Y-d\varphi_tY](\varphi_t(p)),
	\]
	or
	\begin{equation}
		\mL_XY=[X,Y].
	\end{equation}
	where the commutator is given by the definition \ref{DEFooHOTOooRaPwyo}.
\end{theorem}
\begin{proof}
	Take $\dpt{f}{M}{\eR}$ and the function given by the lemma: $\dpt{g_t}{M}{\eR}$ such that $f\circ \varphi_t=f+tg_t$ and $g_0=Xf$. Then put $p(t)=\varphi_t^{-1}(p)$. The rest of the proof is a computation:
	\[
		(\varphi_{t*}Y)_pf=Y(f\circ\varphi_t)_{p(t)}=(Yf)_{p(t)}+t(Yg_t)_{p(t)},
	\]
	so
	\begin{equation}
		\begin{split}
			\lim_{t\to 0}\us{t}[Y_p-(\varphi_{t*}Y)_p]f&=\lim_{t\to 0}\us{t}[(Yf)_p-(Yf)_{p(t)}]-\lim_{t\to 0}(Yg_t)_{p(t)}\\
			&=X_p(Yf)-Y_pg_0\\
			&=[X,Y]_pf.
		\end{split}
	\end{equation}

\end{proof}

A second important property is
\begin{theorem}
	For any function $f\colon M\to V$,
	\[
		\mL_Xf=Xf.
	\]
\end{theorem}

\begin{proof}
	If $X(t)$ is the path which defines the vector $X$, it is obvious that at $t=0$, $X(t)$ is an integral curve to $X$, so that we can take $X(t)$ instead of $\varphi_t$ in \eqref{liesurforme}. Therefore we have:
	\begin{equation}
		\mL_Xf=\dsdd{\varphi_t^*f}{t}{0}
		=Xf
	\end{equation}
	by definition of the action of a vector on a function.
\end{proof}
