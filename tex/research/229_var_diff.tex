% This is part of (almost) Everything I know in mathematics
% Copyright (c) 2010-2017, 2019, 2021-2022, 2024
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Flow and integral curve}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{propositionDef}[Integral curve, flow of a vector field\cite{MonCerveau,BIBooDPYDooWgxxQH}]       \label{PROPooJACTooXBSxfE}
	Let \( X\) be a \( C^k\) vector field on \( M\) and let \( p\in M\). There exists a unique maximal interval \( I\subset \eR\) and \( C^k\) map\footnote{The map \( \Phi_p\) is \( C^k\). We are not speaking of the regularity with respect to \( p\). % Do not join the lines, for the sake of the accepted future reference.
		For more regularity, see the proposition \ref{PROPooQVQAooJVdwOa}.} \( \Phi_p\colon I\to M\) such that
	\begin{enumerate}
		\item
		      \( \Phi_p(0)=p\),
		\item
		      \( \Dsdd{ \Phi_p(t) }{t}{t_0}=X_{\Phi_p(t_0)}\).
	\end{enumerate}
	The map \( \Phi\) satisfy
	\begin{equation}        \label{EQooCHYXooCbECRN}
		\Phi_{t+u}=\Phi_t\circ \Phi_u.
	\end{equation}
	We say that \( \Phi_p\) is the \defe{integral curve}{integral curve} of \( X\) at \( p\), and that \( \Phi\) is the \defe{flow}{flow} of \( X\). We will often write \( \Phi(p,t)\) instead of \( \Phi_p(t)\). When we are interested in only one point \( p\), we can write \( \gamma_X\) or simply \( \gamma\).

	When we want to specify the vector field, we write \( \Phi^X_g(t)\).
	%TODOooPEABooAVCYdS. Il manque certainement des morceaux à la démonstration.
\end{propositionDef}

\begin{proof}
	In several parts.
	\begin{subproof}
		\spitem[Local unicity]        \label{ITEMooZPSEooPivDiw}
		%TODOooPEABooAVCYdS. Il y a plus bas de références à ITEMooZPSEooPivDiw qui parlent d'existence
		% -------------------------------------------------------------------------------------------- 

		Let \( p\in M\), we show that there exists an interval \( I\subset \eR\) such that a \( C^k\) integral curve \( \gamma\colon \eR\to M\) satisfying \( \gamma(0)=p\) is unique. Here the existence is not yet discussed.

		Let \( J\) be an interval around \( 0\) in \( \eR\) and \( \gamma\colon J \to M\), an integral curve of \( X\) such that \( \gamma(0)=p\).

		Let \( \varphi\colon U\to M\) be a local chart around \( p\). We know from lemma \ref{LEMooZWFAooDlYaJm} that there exist functions \( v_k\in C^k(M,\eR)\) such that
		\begin{equation}
			X_x(f)=\sum_{k=1}^nv_k(x)\partial_k(f\circ \varphi)\big( \varphi^{-1}(x) \big).
		\end{equation}
		We consider the map \( \tilde \gamma\colon I\to U\) defined by
		\begin{equation}
			\tilde \gamma=\varphi^{-1}\circ \gamma,
		\end{equation}
		and, if \( f\) is a function on \( M\) we define \( \tilde f\colon U\to \eR\) by \( \tilde f=f\circ\varphi\).

		On the one hand,
		\begin{equation}
			\gamma'(a)f=\Dsdd{ (f\circ \varphi\circ\varphi^{-1}\circ\gamma)(t) }{t}{a}=\Dsdd{ (\tilde f\circ\tilde \gamma) }{t}{a}.
		\end{equation}
		On the other hand,
		\begin{equation}
			X_{\gamma(a)}(f)=\sum_k(\partial_k\tilde f)\big( \tilde \gamma(a) \big)\Dsdd{ \tilde \gamma_k(t) }{t}{a}=\sum_k(\partial_k\tilde f)\big( \varphi^{-1}(p) \big)\tilde \gamma'_k(a)=\sum_k(\partial_k\tilde f)\big(a)\tilde \gamma'_k(a)
		\end{equation}
		where \( \tilde \gamma_k'\) is an usual derivative.

		So, in order to be an integral curve, we need the equality
		\begin{subequations}        \label{SUBEQSooXSIYooPlVcEI}
			\begin{numcases}{}
				v_k\big( \varphi(a) \big)=\tilde \gamma_k'(a)\\
				\tilde \gamma(0)=\varphi^{-1}(p).
			\end{numcases}
		\end{subequations}
		for every \( a\in I\). Each map \( \tilde \gamma_k\) satisfy the equations
		\begin{subequations}        \label{SUBEQSooBLWCooATQGEn}
			\begin{numcases}{}
				\tilde\gamma_k'=v_k\circ \varphi\\
				\tilde\gamma_k(0)=\varphi^{-1}(p)_k.
			\end{numcases}
		\end{subequations}
		The function \( v_k\circ\varphi\colon U\to \eR\) is a \( C^k\) function. We know from the Cauchy-Lipschitz theorem \ref{ThokUUlgU} that the system \eqref{SUBEQSooBLWCooATQGEn} has a unique solution on an interval \( I\) around \( 0\) in \( \eR\).
		\spitem[Local existence]
		% -------------------------------------------------------------------------------------------- 

		%TODOooPEABooAVCYdS. À faire plus sérieusement.

		Consider a solution of \eqref{SUBEQSooBLWCooATQGEn}.

		\spitem[Maximum interval]
		% -------------------------------------------------------------------------------------------- 

		%TODOooPEABooAVCYdS. Encore à faire

	\end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooQVQAooJVdwOa}
	Let \( X\) be a \( C^k\) vector field on \( M\).

	Let \( \Phi\colon M\times \eR\to M\) be a map satisfying
	\begin{enumerate}
		\item
		      \( \Phi_p(0)=p\),
		\item
		      \( \Dsdd{ \Phi_p(t) }{t}{t_0}=X_{\Phi(t_0)}\).
	\end{enumerate}
	Then \( \Phi\) is \( C^k\).
\end{proposition}

\begin{definition}  \label{DEFooMZSBooFkfKkS}
	A vector field \( X\) is \defe{complete}{complete vector field} if for every \( p\in M\), there exists an integral curve \( \gamma\colon \eR\to M\) such that \( \gamma(0)=p\).
\end{definition}

The point of a complete vector field is that the integral curves are defined on \( \eR\), not just a neighbourhood of \( 0\).

The integral curve given by point \ref{ITEMooZPSEooPivDiw} is only valid on a neighbourhood given by a local chart.


\begin{proposition}[\cite{MonCerveau}]      \label{PROPooQNWOooBOxYtu}
	Let \( X\) be a complete \( C^k\) vector field on \( M\). For every \( p\in M\), there is an unique integral curve \( \gamma_p\colon \eR\to M\) such that \( \gamma_p(0)=p\).
\end{proposition}

\begin{proof}
	The existence is not an issue because it is part of the definition of a complete vector field. The trick is the unicity. We already know from point \ref{ITEMooZPSEooPivDiw} that we have the unicity on each chart. The difficulty is to glue them together.

	Let \( \gamma\colon \eR\to M\) and \( \sigma\colon \eR\to M\) be integral curves satisfying \( \gamma(0)=\sigma(0)=p\).

	Let \( K\) be a compact interval containing \( 0\) in \( \eR\). For each \( a\in K\), the point \ref{ITEMooZPSEooPivDiw} provides us an open interval \( I_a\) containing \( a\) and such that there exists a unique \( \alpha\colon I_a\to M\) satisfying
	\begin{subequations}        \label{EQSooCEPBooJCuFJL}
		\begin{numcases}{}
			\alpha\text{ is an integral curve of} X\\
			\alpha(a)=\gamma(a).
		\end{numcases}
	\end{subequations}
	Since \( \gamma\) satisfy these requirements, we know that every map satisfying the conditions \eqref{EQSooCEPBooJCuFJL} is equal to \( \gamma\) on \( I_a\).

	The intervals \( I_a\) make a cover of \( K\) which is compact; we extract a finite subcover \( \{ I_i \}_{i=1,\ldots, n}\). Lemma \ref{LEMooNMGWooTfQDeO} allows us to sort these intervals in such a way that \( 0\in I_1\) and
	\begin{equation}
		I_m\cap\bigcup_{i=1}^{m-1}I_i\neq \emptyset
	\end{equation}
	for every \( m=1,\ldots, n\).

	From unicity on \( I_1\) we know that \( \gamma=\sigma\) on \( I_1\) because \( \gamma(0)=\sigma(0)\) and \( 0\in I_1\). We make a induction. We suppose that, for some \( m\) we have \( \gamma=\sigma\) on \( \bigcup_{i=1}^mI_i\). Let
	\begin{equation}
		t_0\in I_{m+1}\cap\bigcup_{i=1}^mI_i.
	\end{equation}
	From the induction hypothesis, we have \( \gamma(t_0)=\sigma(t_0)\) because \( t_0\in \bigcup_{i=1}^mI_i\).

	The curve \( \sigma\) is, on \( I_{m+1}\) an integral curve of \( X\) satisfying \( \sigma(t_0)=\gamma(t_0)\) with \( t_0\in I_{m+1}\). Thus \( \sigma=\gamma\) on \( I_{m+1}\). We deduce that \( \sigma=\gamma\) on \( \bigcup_{i=1}^nI_i\) and in particular \( \sigma=\gamma\) on \( K\).

	Since the whole is valid for every compact \( K\subset \eR\) we conclude that \( \sigma=\gamma\) on \( \eR\).
\end{proof}

Let \( X\) be a vector field on \( M\) and \( p\in M\). We denote by \( \Phi(p,t)=\Phi_t(p)\in M\) the point \( \gamma_p(t)\) where \( \gamma_p\) is the integral curve of proposition \ref{PROPooQNWOooBOxYtu}. In other terms, for every \( p\in M\), there exist an interval \( I_p\) such that
\begin{equation}
	\Dsdd{ \Phi(p,t) }{t}{t_0}=X_{\Phi(p,t_0)}.
\end{equation}
and for every \( p\in M\), we have \( \Phi(p,0)=p\).


\begin{proposition}[\cite{BIBooDPYDooWgxxQH}]       \label{PROPooDPXIooTvXOIP}
	Let \( X,Y\) be smooth vectors fields on the manifold \( M\). Let \( \Phi^X\) and \( \Phi^Y\) be their flows\footnote{Proposition \ref{PROPooJACTooXBSxfE}.}. The following are equivalent:
	\begin{enumerate}
		\item
		      \( [X,Y]=0\),
		\item
		      \( (d\Phi_t^X)Y=Y\)
		\item
		      For every \( t,u\) in the domains, \( \Phi_t^X\circ\Phi_u^Y=\Phi^Y_u\circ\Phi^X_t\).
	\end{enumerate}
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Next stuff}
%---------------------------------------------------------------------------------------------------------------------------

The manifold could, of course have some additional structure which allows to write the differential quotient \eqref{EQooVMGFooFUCNEY}. This is the case when \( M=\eR^n\) or when \( M\) is a matrix group. In these cases, the question of the link between \( \gamma'(0)\) and the ``true'' derivative of \( \gamma\) has to be studied.

In that case we have the same notational problem with ``$df$''. Let \( f\colon M\to \eR\) where \( M\) is a manifold like \( \eR^n\). The symbol \( df_a\) with \( a\in M\) can be the differential of \( f\) as function \( M\to \eR\), so that \( df_a\) is a linear map from \( \eR^n\) to \( \eR\). But \( df_a\) can also be the linear map \( df_a\colon T_aM\to T_{f(a)}\eR\) where the spaces \( T_aM\) and \( T_{f(a)}\eR\) are made of differential operators.

This is the point of the section \ref{SECooTSAJooNtjgMD}.

Using the chain rule $d(g\circ f)(a)=dg(f(a))\circ df(a)$ for the differentiation in $\eR^n$, one sees that this equivalence notion doesn't depend on the choice of $\varphi$. In other words, if $\varphi$ and $\tilde{\varphi}$ are two charts for a neighbourhood of $x$, then $(\varphi^{-1} \circ\gamma)'(0)=(\varphi^{-1} \circ\sigma)'(0)$ if and only if $(\tilde{\varphi}^{-1} \circ\gamma)'(0)=(\tilde{\varphi}^{-1} \circ\sigma)'(0)$. The space of all tangent vectors at $x$ is denoted by $T_xM$. There exists a bijection $[\gamma]\leftrightarrow (\varphi^{-1}\circ\gamma)'(0)$ between $T_xM$ and $\eR^n$, so $T_xM$ is endowed with a vector space structure.

If $(\mU,\varphi)$ is a chart around $X(0)$, we can express $Xf$ using only well know objects by defining the function $\tilde f =f\circ\varphi$ and $\tX=\varphi^{-1}\circ X$
\[
	Xf=\Dsdd{ (\tilde f \circ\tX)(t) }{t}{0}=\left.\dsd{\tilde f }{x^{\alpha}}\right|_{x=\tX(0)}\left.\frac{d\tX^{\alpha}}{dt}\right|_{t=0}.
\]
In this sense, we write
\begin{equation}
	X=\frac{d\tX^{\alpha}}{dt} \dsd{}{x^{\alpha}}
\end{equation}
and we say that $\{\partial_1,\ldots,\partial_n\}$ is a basis of $T_xM$. As far as notations are concerned, from now a tangent vector is written as $X=X^{\alpha}\partial_{\alpha}$ where $X^{\alpha}$ is related to the path $\dpt{X}{\eR}{M}$ by $X^{\alpha}=d\tX^{\alpha}/dt$. We will no more mention the chart $\varphi$ and write
\[
	Xf=\Dsdd{f(X(t))}{t}{0}.
\]
Correctness of this short notation is because the equivalence relation is independent of the choice of chart. When we speak about a tangent vector to a given path $X(t)$ without specification, we think about $X'(0)$.

All this construction gives back the notion of tangent vector when $M\subset \eR^m$. In order to see it, think to a surface in $\eR^3$. A tangent vector is precisely given by a derivative of a path: if $\dpt{c}{\eR}{\eR^n}$ is a path in the surface, a tangent vector to this curve is given by
\[
	\lim_{t\to 0}\frac{c(t_0)-c(t_0+t)}{t}
\]
which is a well know limit of a difference in $\eR^n$.

\label{pg:vecto_vecto}Let us precise how does a tangent vector acts on maps others than $\eR$-valued functions. If $V$ is a vector space and $\dpt{f}{M}{V}$, we define
\[
	Xf=(Xf^i)e_i
\]
where $\{e_i\}$ is a basis of $V$ and the functions $\dpt{f^i}{M}{\eR}$, the decomposition of $f$ with respect to this basis. If we consider a map $\dpt{\varphi}{M}{N}$ between two manifolds, the natural definition is $Xf:=dfX$. More precisely, if we consider local coordinates $x^{\alpha}$ and a function $\dpt{f}{M}{\eR}$,
\begin{equation}\label{eq:dvp_phi}
	(d\varphi X)f=\Dsdd{  (f\circ\varphi\circ X)(t) }{t}{0}=\dsd{f}{x^{\alpha}}\dsd{\varphi^{\alpha}}{x\hbeta}\frac{dX\hbeta}{dt}.
\end{equation}
Now we are in a notational trouble: when we write $X=X^{\alpha}\partial_{\alpha}$, the ``$X^{\alpha}$``{} is the derivative of the ``$X^{\alpha}$``{} which appears in the path $X(t)=(X^1(t),\ldots,X^n(t))$ which gives $X$ by $X=X'(0)$. So equation \eqref{eq:dvp_phi} gives
\begin{equation}
	X(\varphi):=d\varphi X=X\hbeta(\partial_{\beta}\varphi^{\alpha})\partial_{\alpha}.
\end{equation}

\subsection{Differential of a map}
%------------------------------------------

Let $\dpt{f}{M_1}{M_2}$ be a differentiable map, $x\in M_1$ and $X\in T_xM_1$, i.e. $\dpt{X}{\eR}{M_1}$ with $X(0)=x$ and $X'(0)=X$. We can consider the path $Y=f\circ X$ in $M_2$. The tangent vector to this path is written $df_x X$.

\begin{proposition}
	If $\dpt{f}{M_1}{M_2}$ is a differentiable map between two differentiable manifolds, the map
	\begin{equation}
		\begin{aligned}
			df_x \colon T_xM_1 & \to T_{f(x)}M_2        \\
			X'(0)              & \mapsto (f\circ X)'(0)
		\end{aligned}
	\end{equation}
	is linear.
\end{proposition}

\begin{proof}
	We consider local coordinates $\dpt{x}{\eR^n}{M_1}$ and $\dpt{y}{\eR^m}{M_2}$. The maps $\dpt{f}{M_1}{M_2}$ and $\dpt{y^{-1}\circ f\circ x}{\eR^n}{\eR^m}$ will sometimes be denoted by the same symbol $f$. We have $(x^{-1}\circ X)(t)=(x_1(t),\ldots,x_n(t))$ and $(y^{-1}\circ Y)(t)=\big( y_1(x_1(t),\ldots,x_n(t),\ldots, y_m(x_1(t),\ldots,x_n(t)  \big)$, so that
	\[
		Y'(0)=\left(   \sum_{i=1}^n \dsd{y_1}{x_i}x_i'(0),\ldots,\sum_{i=1}^n \dsd{y_m}{x_i}x_i'(0)   \right)\in\eR^m
	\]
	which can be written in a more matricial way under the form
	\[
		Y'(0)=\left( \dsd{y_i}{x_j}x'_j(0) \right).
	\]
	So in the parametrisations $x$ and $y$, the map $df_x$ is given by the matrix $\partial y^i/\partial x_j$ which is well defined from the only given of $f$.
\end{proof}


Let $\dpt{x}{\mU}{M}$ and $\dpt{y}{\mV}{M}$ be two charts systems around $p\in M$. Consider the path $c(t)=x(0,\ldots,t,\ldots 0)$ where the $t$ is at the position $k$. Then, with respect to these coordinates,
\[
	c'(0)f=\Dsdd{ f(c(t))  }{t}{0}=\dsd{f}{x^i}\frac{dc^i}{dt}=\dsd{f}{x^k},
\]
so $c'(0)=\partial/\partial x^k$. Here, implicitly, we wrote $c^i=(x^i)^{-1}\circ c$ where $(x^i)^{-1}$ is the $i$th component of $x^{-1}$ seen as element of $\eR^n$. We can make the same computation with the system $y$. With these abuse of notation,
\begin{equation}
	\dsd{}{x^i}=\sum_j\dsd{y^j}{x^i}\dsd{}{y^j}
\end{equation}
as it can be seen by applying it on any function $\dpt{f}{M}{\eR}$. More precisely if $\dpt{x}{\mU}{M}$ and $\dpt{y}{\mU}{M}$ are two charts (let $\mU$ be the intersection of the domains of $x$ and $y$), let $\dpt{f}{M}{\eR}$ and $\ovf=f\circ x$, $\tilde f =f\circ y$. The action of the vector $\partial_{x^i}$ of the function $f$ is given by
\[
	\partial_{x^i}f=\dsd{\ovf}{x^i}
\]
where the right hand side is a real number that can be computed with usual analysis on $\eR^n$. This real \emph{defines} the left hand side. Now, $\ovf=\tilde f \circ y^{-1}\circ x$, so that
\[
	\dsd{\ovf}{x^i}=\dsd{ (\tilde f \circ y^{-1}\circ x) }{x^i}=\dsd{\tilde f }{y^j}\dsd{y^j}{x^i}
\]
where $\dsd{\tilde f }{y^j}$ is precisely what we write now by $\partial_{y^j}f$ and $\dsd{y^j}{x^i}$ must be understood as the derivative with respect to $x^i$ of the function $\dpt{(y^{-1}\circ x)}{\eR^n}{\eR^n}$.

Let $\dpt{f}{M}{N}$ and $\dpt{g}{N}{\eR}$; the definitions gives
\[
	(df_xX)g=\Dsdd{(g\circ f)(X(t))}{t}{0}
	=\dsd{g}{y^i}\dsd{f^i}{x^{\alpha}}\frac{dX^{\alpha}}{dt}.
\]
This shows that $\dsd{f^i}{x^{\alpha}}\frac{dX^{\alpha}}{dt}$ is $(df_xX)^i$.  But $dX^{\alpha}/dt$ is what we should call $X^{\alpha}$ in the decomposition $X=X^{\alpha}\partial_{\alpha}$ then the matrix of $df$ is given by $\dsd{f^i}{x^{\alpha}}$. So we find back the old notion of differential.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Examples}
%---------------------------------------------------------------------------------------------------------------------------

\subsubsection{Example: the sphere}\index{sphere}

The sphere $S^n$ is the set
\[
	S^n=\{  (x_1,\ldots, x_{n+1})\in\eR^{n+1}\tq \|x\|=1  \}
\]
for which we consider the following open set in $\eR^n$:
\[
	\mU=\{  (u_1,\ldots,u_n)\in\eR^n\tq\|u\|<1  \}
\]
and the charts $\dpt{\varphi_i}{\mU}{S}$, and $\dpt{\tilde{\varphi}_i}{\mU}{S}$
\begin{subequations}
	\begin{align}
		\varphi_i(u_1,\ldots,u_n)         & =(u_1,\ldots,u_{i-1}, \sqrt{  1-\|u\|^2  },u_i,\ldots,u_n )   \\
		\tilde{\varphi}_i(u_1,\ldots,u_n) & =(u_1,\ldots,u_{i-1}, -\sqrt{  1-\|u\|^2  },u_i,\ldots,u_n ).
	\end{align}
\end{subequations}
These map are clearly injective. To see that $\varphi(\mU)\cup\tilde{\varphi}(\mU)=S$, consider $(x_1,\ldots,x_{n+1})\in S$. Then at least one of the $x_i$ is non zero. Let us suppose $x_1\neq 0$, thus $x_1^2=1-(x_2^2+\cdots+x_{n+1}^2)$ and
\begin{equation}\label{eq:xupm}
	x_1=\pm\sqrt{1-(\ldots)}.
\end{equation}
If we put $u_i=x_{i+1}$, we have $x=\varphi(u)$ or $x=\tilde{\varphi}(u)$ following the sign in relation \eqref{eq:xupm}. The fact that $\varphi^{-1}\circ\tilde{\varphi}$ and $\tilde{\varphi}^{-1}\circ\varphi$ are differentiable is a ``first year in analysis exercise``.

\subsubsection{Example: projective space}

On $\eR^{n+1}\setminus\{o\}$, we consider the equivalence relation $v\sim\lambda w$ for all non zero $\lambda\in\eR$, and we put
\[
	\eR P^n=\left(\eR^{n+1}\setminus\{o\}\right)/\sim.
\]
This is the set of all the one dimensional subspaces of $\eR^{n+1}$. This is the real \defe{projective space}{projective!real space} of dimension $n$. We set $\mU=\eR^n$ and
\[
	\varphi_i(u_1,\ldots,u_n)=\Span\{ (u_1,\ldots,u_{i-1},1,u_i,\ldots,u_n) \}.
\]
One can see that this gives a manifold structure to $\eR P^n$. Moreover, the map
\begin{equation}
	\begin{aligned}
		A \colon S^n & \to \eR P^n\
		v            & \mapsto \Span v
	\end{aligned}
\end{equation}
is differentiable.

Let us show how to identify $\eR\cup\{ \infty \}$ to $\eR P^1$, the set of directions in the plane $\eR^2$. Indeed consider any vertical line $l$ (which does contain the origin). A non vertical vector subspace of $\eR^2$ intersects $l$ in one and only one point, while the vertical vector subspace is associated with the infinite point.

\subsection{Some Leibnitz formulas}

\begin{lemma}[\cite{kobayashi}]
	If $M$ and $N$ are two manifolds, we have a canonical isomorphism
	\[
		T_{(p,q)}(M\times N)\simeq T_pM+T_qN.
	\]
	\label{lemLeibnitz}
\end{lemma}

\begin{proof}
	A $Z\in T_{(p,q)}(M\times N)$ is the tangent vector to a curve $(x(t),y(y))$ in $M\times N$. We can consider $X\in T_pM$ given by $X=x'(0)$ and $Y\in T_qN$ given by $Y=y'(0)$. The isomorphism is the identification $(X,Y)\simeq Z$. Indeed, let us define $\oX\in T_{(p,q)}(M\times N)$, the tangent vector to the curve $(x(t),q)$, and $\oY\in T_{(p,q)}(M\times N)$, the tangent vector to the curve $(p,y(t))$. Then $Z=\oX+\oY$ because for any $\dpt{f}{M\times N}{\eR}$,
	\begin{equation}
		Zf=\dsdd{f(x(t),y(t))}{t}{0}
		=\dsdd{f(x(t),y(0))}{t}{0}+\dsdd{f(x(0),y(t))}{t}{0}
		=\oX f+\oY f.
	\end{equation}
\end{proof}

\begin{proposition}[Leibnitz formula] \label{Leibnitz}
	Let us consider $M,N,V$, three manifold; a map $\dpt{\varphi}{M\times N}{V}$ and a vector $Z\in T_{(p,q)}(M\times N)$ which corresponds (lemma~\ref{lemLeibnitz}) to $(X,Y)\in T_pM+T_qN$.

	If we define $\dpt{\varphi_1}{M}{V}$ and  $\dpt{\varphi_2}{N}{V}$ by $\varphi_1(p')=\varphi(p',q)$ and $\varphi_2(q')=\varphi(p,q')$, we have the \defe{Leibnitz formula}{Leibnitz formula}:
	\begin{equation}
		d\varphi(Z)=d\varphi_1(X)+d\varphi_2(Y).
	\end{equation}
\end{proposition}
\begin{proof}
	Since $Z=\oX+\oY$, we just have to remark that
	\[
		d\varphi(\oX)=\dsdd{\varphi(x(t),q)}{t}{0}=d\varphi_1(X),
	\]
	so $d\varphi(Z)=d\varphi(\oX+\oY)=d\varphi_1(X)+d\varphi_2(Y)$.
\end{proof}
One of the most important application of the Leibnitz rule is the corollary~\ref{cor_PrincLeib} on principal bundles.

\subsection{Cotangent bundle}

A form on a vector space $V$ is a linear map $\dpt{\alpha}{V}{\eR}$. The set of all forms on $V$ is denoted by $V^*$ and is called the \defe{dual space}{dual!of a vector space} of $V$. On each point of a manifold, one can consider the tangent bundle which is a vector space. Then one can consider, for each $x\in M$ the dual space $T^*_xM:=(T_xM)^*$ which is called the \defe{cotangent bundle}{cotangent bundle}. A $1$-\defe{differential form}{differential!form} on $M$ is a smooth map $\dpt{\omega}{M}{T^*M}$ such that $\omega_x:=\omega(x)\in T^*_xM$. So, for each $x\in M$, we have a $1$-form $\dpt{\omega_x}{T_xM}{\eR}$.

Here, the smoothness is the fact that for any smooth vector field $X\in\cvec(M)$, the map $x\to\omega_x(X_x)$ is smooth as function on $M$. One often considers vector-valued forms. This is exactly the same, but $\omega_xX_x$ belongs to a certain vector space instead of $\eR$. The set of $V$-valued $1$-forms on $M$ is denoted by $\Omega(M,V)$ \nomenclature{$\Omega(M,V)$}{$V$ valued $1$-forms} and simply $\Omega(M)$ if $V=\eR$
The cotangent space $T^*_pM$ of $M$ at $p$ is the dual space of $T_pM$, i.e. the vector space of all the (real valued) linear\footnote{When we say \emph{a form}, we will always mean \emph{a linear form}.} $1$-forms on $T_pM$. In the coordinate system $\dpt{x}{\mU}{M}$, we naturally use, on $T^*_pM$, the dual basis of the basis $\{\partial/\partial_{x^i},\ldots\partial/\partial_{x^i}\}$ of $T_pM$. This dual basis is denoted by $\{dx_1,\ldots,dx_n\}$, the definition being as usual:
\begin{equation}\label{eq:dx_v}
	dx_i(\partial^j)=\delta^j_i.
\end{equation}
The notation comes from the fact that equation \eqref{eq:dx_v} describes the action of the differential of the projection $\dpt{x_i}{\mU}{\eR}$ on the vector $\partial^j$.

If $(\mU_{\alpha},\varphi_{\alpha})$ is a chart of $M$, then the maps
\begin{equation}
	\begin{aligned}
		\phi_{\alpha} \colon \mU_{\alpha}\times\eR^n & \to T^*M\
		(x,a)                                        & \mapsto a^idx_i|_x
	\end{aligned}
\end{equation}
give to $T^*M$ a $2n$ dimensional manifold structure such that the canonical projection $\dpt{\pi}{T^*M}{M}$ is an immersion.

When $V$ is a finite-dimensional vector space, we denote by $V^*$ its dual\footnote{The vector space of all the linear map $V\to \eR$.} and we often use the identifications $V\simeq V^*\simeq T_vV\simeq T_wV\simeq T^*_vV$ where $v$ and $w$ are any elements of $V$. Note however that there are no \emph{canonical} isomorphism between these spaces, unless we consider some basis.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Immersion, embedding}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{BIBooECJTooEfmLsr}]    \label{DEFooZEWNooMVOzWI}
	A smooth map \( f\colon M\to N\) between the manifolds \( M\) and \( N\) is an \defe{immersion}{immersion} if its differential \( df_p\colon T_pM\to T_{f(p)N}\) is injective for every \( p\in M\).
\end{definition}

\begin{definition}[Topological embedding\cite{BIBooTUOOooJZFtGe}]
	Let \( X\), \( Y\) be topological spaces. A map \( f\colon X\to Y\) is a \defe{topological embedding}{topological embedding} if
	\begin{enumerate}
		\item
		      \( f\) is continuous
		\item
		      \( f\) is injective
		\item
		      \( f\colon X\to f(X)\) is an homeomorphism when \( f(X)\) is equipped with the induced topology from \( Y\).
	\end{enumerate}
\end{definition}

\begin{definition}[Embedding\cite{BIBooTUOOooJZFtGe}]       \label{DEFooQLGLooNyXaOV}
	Let \( M\) and \( N\) be smooth manifolds. A smooth function \( f\colon M\to N\) is an \defe{embedding}{embedding} if
	\begin{enumerate}
		\item
		      \( f\) is an immersion,
		\item
		      \( f\) is a topological embedding.
	\end{enumerate}
\end{definition}

The following theorem says that immersions are local embeddings.
\begin{theorem}[\cite{BIBooCPDQooFrLOzh}]       \label{THOooXAOUooRKHMBm}
	Let \( M\) be a \( n\)-dimensional manifold and \( S\) be a \( m\)-dimensional manifold. Let \( f\colon M\to S\) be an immersion. For every \( a\in M\), there exists an open set \( \mO\) containing \( a\) in \( M\) such that \( f\colon \mO\to f(\mO)\) is an embedding.
\end{theorem}

\begin{proof}
	We consider maps \( \varphi\colon U\to M\) and \( \psi\colon V\to S\) around \( a\) and \( f(a)\) respectively. Then we consider :
	\begin{enumerate}
		\item
		      \( \tilde f=\psi^{-1}\circ f\circ\varphi\)
		\item
		      We rename \( U  \) in such a way that \( f\circ\varphi(U)\subset \psi(V)\) and we restrict \( \varphi\) to that subset of \( U\).
		\item
		      With that restriction of \( U\), the map \( \tilde f\colon U\to V\) is defined on \( U\).
		\item
		      \( q=\varphi^{-1}(a)\).
	\end{enumerate}

	By hypothesis, \( f\) is an immersion. It means that the map \( df_a\colon T_aM\to T_{f(a)}S\) is injective. The differential \( d\tilde f_q\colon \eR^n\to \eR^m\) of \( \tilde f\colon U\to V\) is injective too. This has two consequences :
	\begin{equation}
		\det(d\tilde f_q)\neq 0
	\end{equation}
	and its image has dimension \( n\). We suppose that
	\begin{equation}
		\Image(d\tilde f_q)=\Span\{ e_1,\ldots, e_n \}.
	\end{equation}
	If not, we consider a linear bijection map \( h\colon \eR^m\to \eR^m\) which maps \( \Image(d\tilde f_q)\) to \( \Span\{ e_1,\ldots, e_n \}\) and we rewrite the whole proof with \( \tilde f\circ h\) instead of \( \tilde f\).

	Let \( k=m-k\). We consider the map
	\begin{equation}
		\begin{aligned}
			\phi\colon U\times \eR^k          & \to \eR^m                                                                                                \\
			(x_1,\ldots, x_n,t_1,\ldots, t_k) & \mapsto \big( \tilde f(x)_1,\ldots, \tilde f(x)_n,\tilde f(x)_{n+1}+t_1,\ldots, \tilde f(x)_m+t_k \big).
		\end{aligned}
	\end{equation}
	It satisfies \( \phi(x,0)=\tilde f(x)\) and the matrix of its differential is
	\begin{equation}
		d\phi_{(q,0)}=\begin{pmatrix}
			d\tilde f_q & 0      \\
			M           & \mtu_k
		\end{pmatrix}
	\end{equation}
	where \( M\) a is some \( k\times n\) matrix. Since it is block-diagonal, its determinant is given by
	\begin{equation}
		\det\big( d\phi_{(q,0)} \big)=\det(d\tilde f_q)\det(\mtu_k)=\det(d\tilde f_q)\neq 0.
	\end{equation}
	Since \( \det(d\phi_{(q,0)})\neq 0\), we can apply the local inversion theorem \ref{ThoXWpzqCn} for \( \phi\) at \( (a,0)\). We have a neighbourhood \( W\) of \( (q,0)\) in \( U\times \eR^k\) such that \( \phi\colon W\to \phi(W)\) is a diffeomorphism.

	Let \( U'=\proj_{\eR^n}(W)\). We will prove that \( \tilde f\colon U'\to \tilde f(U')\) is a diffeomorphism.
	\begin{subproof}
		\spitem[Injective]
		Let \( x,y\in U'\) such that \( \tilde f(x)=\tilde f(y)\). It means \( \phi(x,0)=\phi(y,0)\). The points \( (x,0)\) and \( (y,0)\) belong to \( W\) while \( \phi\) is injective on \( W\), thus \( (x,0)=(y,0)\) and \( x=y\).
		\spitem[Surjective]
		This is not an issue since \( \tilde f\) is obviously surjective on \( \tilde f(U')\).
		\spitem[The inverse]
		The prove that the inverse of \( \tilde f\) is \( \proj_{\eR^n}\circ \phi^{-1}\). Let \( w\in U'=\proj_{\eR^n}(W)\) and compute
		\begin{equation}
			(\proj_{\eR^n}\circ \phi^{-1}\circ\tilde f)(w)=(\proj_{\eR^n}\circ\phi^{-1})\big( \omega(w,0) \big)=\proj_{\eR^n}(w,0)=w.
		\end{equation}
		\spitem[Diffeomorphism]
		Since \( \proj_{\eR^n}\) and \( \phi^{-1}\) are differentiable, the inverse of \( \tilde f\) is differentiable and \( \tilde f\) is a diffeomorphism.
	\end{subproof}
	The fact for \( \tilde f\) to be a diffeomorphisms implies that \( f\) itself (restricted to \( \varphi(U')\)) is bijective, differentiable with the inverse being differentiable.
\end{proof}

\begin{proposition}     \label{PROPooYHOKooYASzRL}
	If \( f\colon M\to N\) is an embedding, the image \( \Image(f)\) is a smooth \( \dim(M)\)-dimensional submanifold of \( N\).
\end{proposition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Distribution and Frobenius theorem}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}       \label{LEMooQXRSooHDRequ}
	Let \( a,b\in  C^{\infty}(M)\). We have
	\begin{equation}
		[a\partial_i,b\partial_j]=a(\partial_ib)\partial_j-b(\partial_ja)\partial_i.
	\end{equation}
	In particular, \( [\partial_i,\partial_j]=0\).
\end{lemma}

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooGHQZooMIWAfn}
	Let \( M\) be a \( n\)-dimensional manifold. Let \( \varphi\colon U\to M\) be a chart, and \( X_1,\ldots, X_r\) (\( r<n\)) be vectors fields such that for each \( p\in \varphi(U)\), the set \( \{ X_i(p) \}_{i=1,\ldots, r}\) is linearly independent in \( T_pM\).

	There exist vector fields \( X_{r+1},\ldots,  X_n\) such that \( \{  X_1(p),\ldots, X_n(p) \}\) is a basis of \( T_pM\) for every \( p\in \varphi(U)\).
\end{lemma}

\begin{definition}      \label{DEFooYOMHooZJvsSt}
	Let \( M\) be a manifold. A \( k\) dimensional \defe{distribution}{distribution on a manifold} is the data, for each \( p\in M\) of a \( k\) dimensional vector subspace \( \mD_p\) of \( T_pM\).

	The distribution \( \mD\) is said to be \defe{smooth}{smooth distribution} if for each \( p\in M\), there exists a neighbourhood \( S\) of \( p\) and smooth vector fields\footnote{Definition \ref{DEFooAATTooLhNqDb}.} \( \{ X_i \}_{i=1,\ldots, k}\) in \( S\) such that
	\begin{equation}
		\mD_p=\Span\{ X_i(p) \}.
	\end{equation}

	We denote by \( \Gamma^{\infty}(\mD) \) the set of smooth vector fields with values in \( \mD\). These are the smooth vector fields \( X\) such that \( X_p\in \mD_p\) for every \( p\). More precisely,
	\begin{equation}
		\Gamma^{\infty}(\mD)=\{ X\in \Gamma(TM)\tq X_p\in\mD_p\,\forall p\in M \}.
	\end{equation}
\end{definition}

\begin{definition}
	Let \( \mD\) be a distribution on \( M\). A smoothly immersed\footnote{Definition \ref{DEFooZKUIooXWVGvh}.} submanifold \( N\subset M\) is an \defe{integral manifold}{integral manifold} of \( \mD\) if for every \( p\in N\) we have
	\begin{equation}
		di_p(T_pN)=\mD_p
	\end{equation}
	where \( i\colon N\to M\) is the immersion.

	When an integral submanifold exists, we say that the distribution is \defe{integrable}{integrable distribution}.
\end{definition}

\begin{definition}
	A distribution \( \mD\) is \defe{involutive}{involutive distribution} if we have
	\begin{equation}
		[X,Y]\in \Gamma^{\infty}(\mD)
	\end{equation}
	for every \( X,Y\in \Gamma^{\infty}(TM)\). In other words, for every \( p\in M\) we have \( [X,Y]_p\in \mD_p\).
\end{definition}

\begin{proposition}     \label{PROPooDYJNooAwnaFK}
	An smooth integrable distribution is involutive.
\end{proposition}

\begin{proof}
	Let \( N\) be a smooth integral manifold for the distribution \( \mD\).  Let \( X,Y\in\Gamma^{\infty}(\mD)\). Since \( T_pN=\mD_p\), for each \( p\in M\), we have \( X,Y\in\Gamma^{\infty}(TN)\). So
	\begin{equation}
		[X,Y]\in\Gamma^{\infty}(TN)=\Gamma^{\infty}(\mD)
	\end{equation}
	and \( \mD\) is involutive.
\end{proof}

\begin{theorem}[Frobenius\cite{BIBooJMRFooTAhhcg,BIBooDPYDooWgxxQH}]        \label{THOooDVBHooGRhuGl}
	An involutive distribution on a smooth manifold is integrable.
\end{theorem}

\begin{proof}
	Let \( \mD\) be an involutive \( r\)-dimensional distribution of the smooth \( n\)-dimensional manifold \( M\). Let \( p\in M\). We will build a submanifold \( N\) of \( M\) such that \( T_qN=\mD_q\) for every \( q\) in a neighbourhood of \( p\). By definition of a smooth distribution, there exists a neighbourhood \( S\) of \( p\) and vector fields \( \{ Y_i \}_{i=1,\ldots, r}\) such that
	\begin{equation}
		D_q=\Span\{ Y_i(q) \}_{i=1,\ldots, r}
	\end{equation}
	for every \( q\in S\). The vectors \( Y_i(q)\) are linearly independent since there are \( r\) of them and they span the \( r\)-dimensional vector subspace \( \mD_q\) of \( T_qM\).

	We may reduce \( S\) to fit in a local chart \( \varphi\colon U\to M\) around \( p\). We denote by \( \partial_i\) the basis vector associated with the chart \( \varphi\). For some \( a_{ij}(q)\) we have
	\begin{equation}
		Y_i=\sum_{j=1}^na_{ij}\partial_j.
	\end{equation}
	We use the lemma \ref{LEMooGHQZooMIWAfn} to consider \( Y_{r+1},\ldots,  Y_n\) such that, for every \( q\in \varphi(U)\), \( \Span\{ Y_1(q),\ldots, Y_n(q) \}=T_qM\). Each of these \( Y_i\) can be decomposed in the basis \( \{ \partial_i \}\):
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}(q)\partial_j(q).
	\end{equation}
	The matrix \( a_{ij}(p)\) is invertible. By lemma \ref{LEMooMCIDooYBHrbq}, let \( \sigma\in S_n\) be such that the matrix \( A\in \eM(r,\eR)\) given by
	\begin{equation}
		A_{ij}=a_{i,\sigma^{-1}(j)}(p)
	\end{equation}
	is invertible. We also consider the associated linear bijection
	\begin{equation}
		\begin{aligned}
			\sigma\colon \eR^n & \to \eR^n              \\
			e_i                & \mapsto e_{\sigma(i)}.
		\end{aligned}
	\end{equation}
	Now we consider a new chart \( U'=\sigma^{-1}(U)\) with
	\begin{equation}
		\begin{aligned}
			\varphi'\colon U' & \to M                             \\
			x                 & \mapsto (\varphi\circ \sigma)(x).
		\end{aligned}
	\end{equation}
	By continuity of the determinant, there exists an open neighbourhood \( S\) of \( p\) such that \( \det\big( a_{i\sigma^{-1}(j)}(q) \big)\neq 0\) for every \( q\in S\). We restrict \( U'\) in such a way for \( \varphi'(U')\) to fit in \( S\).

	We denote by \( \partial'_i\) the corresponding vectors in \( TM\). We have \( \partial'_i(q)=\partial_{\sigma(i)}(q)\); indeed let \( q=\varphi'(v')\) and compute:
	\begin{subequations}
		\begin{align}
			\partial'_i(q)f & =\Dsdd{ (f\circ\varphi')(v'+te_i) }{t}{0}                            \\
			                & =\Dsdd{ (f\circ\varphi)\big( \sigma(v')+te_{\sigma(i)} \big) }{t}{0} \\
			                & =\partial_{\sigma(i)}(q)f
		\end{align}
	\end{subequations}
	because \( \varphi\big( \sigma(v') \big)=q\). We can express \( Y_i\) in terms of \( \partial_j'\) instead of \( \partial_j\):
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}(q)\partial_j(q)=\sum_ja_{ij}(q)\partial'_{\sigma(j)}(q)=\sum_ja_{i\sigma^{-1}(j)}(q)\partial'_j(q).
	\end{equation}
	For the sake of simplifying the notations, we redefine \( U\) for being \( U'\), \( \varphi\) for \( \varphi'\) and so on. Thus we have
	\begin{equation}
		Y_i(q)=\sum_{j=1}^na_{ij}\partial_j(q)
	\end{equation}
	with \( (a_{ij})_{i,j=1,\ldots, r} \) being invertible.

	Let \( A\in \GL(r,\eR)\) be the matrix given by \( A_{ij}=a_{ij}\). We consider \( B=A^{-1}\) and, for \( i=1,\ldots, r\), we define
	\begin{equation}
		X_i=\sum_{k=1}^rB_{ik}Y_k.
	\end{equation}
	We can express them in terms of the \( \partial_i\)'s:
	\begin{subequations}
		\begin{align}
			X_i & =\sum_{k=1}^rB_{ik}Y_k                                                                     \\
			    & =\sum_{k=1}^rB_{ik}\sum_ja_{kj}\partial_j                                                  \\
			    & =\sum_{k=1}^rB_{ik}\big( \sum_{j=1}^ra_{kj}\partial_j+\sum_{j=r+1}^na_{kj}\partial_j \big) \\
			    & =\sum_{k,j=1}^rB_{ik}a_{kj}\partial_j+\sum_{k=1}^r\sum_{j=r+1}^nB_{ik}a_{kj}\partial_j     \\
			    & =\sum_{j=1}^r\delta_{ij}\partial_j+\sum_{j=r+1}^nc_{ij}\partial_j                          \\
			    & =\partial_j+\sum_{j=r+1}^nc_{ij}\partial_j
		\end{align}
	\end{subequations}
	where \( c_{ij}\) is matrix whose value has no importance here.

	We know that \( \mD_q=\Span\{ Y_1(q),\ldots, Y_r(q) \}\) for every \( q\). Since the \( Y_i\)'s are linearly independent and since the matrix \( B\) is invertible, the vectors \( \{ X_i(q) \}_{i=1,\ldots, r}\) are linearly independent and belong to \( \mD_q\). Thus we have
	\begin{equation}
		\mD_q=\Span\{ X_1(q),\ldots, X_r(q)  \}.
	\end{equation}

	We compute the commutators of the \( X_i\)'s using the lemma \ref{LEMooQXRSooHDRequ}:
	\begin{subequations}
		\begin{align}
			[X_i,X_j] & =[\partial_i,\partial_j]+\sum_{k=r+1}^n[\partial_i,c_{ik}\partial_k]+\sum_{k=r+1}^n[c_{ik}\partial_k,\partial_j]+\sum_{k=r+1}^n\sum_{l=r+1}^n[c_{ik}\partial_k,c_{jl}\partial_l] \\
			          & =\sum_{k=r+1}^nd_k\partial_k
		\end{align}
	\end{subequations}
	for some functions \( d_k\). Now the distribution \( \mD\) is involutive, so there exists functions \( f_k\) such that
	\begin{equation}
		[X_i,X_j]=\sum_{k=1}^rf_kX_k=\sum_{k=1}^rf_k\big( \partial_k+\sum_{l=r+1}^nc_{kl}\partial_l \big).
	\end{equation}
	Thus we have
	\begin{equation}
		\sum_{k=r+1}^nd_k\partial_k=\sum_{k=1}^rf_k\big( \partial_k+\sum_{l=r+1}^nc_{kl}\partial_l \big).
	\end{equation}
	On the left hand side there are no components in \( \{ \partial_k \}_{k=1,\ldots, r}\); then on the right hand side we deduce \( \sum_{k=1}^rf_k\partial_k=0\) and then \( f_k=0\) for every \( k=1,\ldots, r\). We proved that
	\begin{equation}
		[X_i,X_j]=0.
	\end{equation}
	We consider the flows \( \Phi^i\) of each \( X_i\) (proposition \ref{PROPooJACTooXBSxfE}), and the map
	\begin{equation}
		\begin{aligned}
			s\colon I         & \to M                                                   \\
			(t_1,\ldots, t_r) & \mapsto (\Phi_{t_1}^1\circ \cdots\circ \Phi_{t_2}^t)(p)
		\end{aligned}
	\end{equation}
	where \( I\) is an open neighbourhood of \( (0,\ldots, 0)\) in \( \eR^r\). The differential at \( 0\in \eR^r\) is a map \( ds_0\colon T_0\eR^r\to T_pM\) and satisfy
	\begin{equation}
		ds_0(\partial_i)\Dsdd{ \Phi_t^i(p) }{t}{0}=X_i(p).
	\end{equation}
	Since the vectors \( \{ X_i(p) \}\) are linearly independent, the map \( ds_0\) is injective. This means that \( s\colon I\to M\) is an immersion\footnote{Definition \ref{DEFooZEWNooMVOzWI}.}. Theorem \ref{THOooXAOUooRKHMBm} says that, reducing the size of \( I\), the map \( s\) becomes an embedding.

	We define \( N=\Image(s)\). This is a \( r\)-dimensional submanifold of \( M\) by proposition \ref{PROPooYHOKooYASzRL}. Since \( s(0)=p\) and \( ds_0(\partial_i)=X_i\) we have \( T_pN=\mD_p\). Ou task is to prove that \( \mD_q=T_qN\) for every \( q\in N\).

	Let \( q\in N\). There exists \( (t_1,\ldots, t_r)\in \eR^r\) such that
	\begin{equation}
		q=s(t_1,\ldots, t_r)=(\Phi_{t_1}^1\circ \ldots\circ\Phi_{t_r}^r)(p).
	\end{equation}
	We fix \( i\). As a recall, the vector \( \partial_i\in T_{(t_1,\ldots, t_r)}\eR^n\) is the derivative of the path \( (t_1,\ldots, t_r)+te_i\). We have
	\begin{subequations}
		\begin{align}
			ds_{(t_1,\ldots, t_r)}(\partial_i) & =\Dsdd{ s\big( (t_1,\ldots, t_r)+te_i \big) }{t}{0}                                                           \\
			                                   & =\Dsdd{ (\Phi_t^i\circ\Phi_{t_1}^1\circ\ldots\circ\Phi_{t_r}^r)(p)    }{t}{0}     \label{SUBEQooDNCWooXPRFNz} \\
			                                   & =\Dsdd{ \Phi_t^i(q) }{t}{0}        \label{SUBEQooYCSQooFdYfPb}                                                \\
			                                   & =X_i(q).
		\end{align}
	\end{subequations}
	Justifications.
	\begin{itemize}
		\item For \eqref{SUBEQooDNCWooXPRFNz}. We used the property \eqref{EQooCHYXooCbECRN} of proposition \ref{PROPooJACTooXBSxfE} and the commutation of proposition \ref{PROPooDPXIooTvXOIP}.
		\item For \eqref{SUBEQooYCSQooFdYfPb}. By definition \( q=(\Phi_{t_1}\circ\ldots\circ\Phi_{t_r}^r)(p)\).
	\end{itemize}
	So \( \Span\{ X_1(q),\ldots, X_r(q) \}\) is a \( r\)-dimensional subspace of \( T_qM\) which is included in the \( r\)-dimensional subspace \( T_qN\). We conclude that
	\begin{equation}
		T_qN=\Span\{ X_1(q),\ldots, X_r(q) \}=\mD_qN.
	\end{equation}
\end{proof}


\begin{theorem}[Frobenius\cite{BIBooJMRFooTAhhcg,BIBooDPYDooWgxxQH}]      \label{THOooVRDYooIusxwW}
	About distributions.
	\begin{enumerate}
		\item       \label{ITEMooLDUZooJCwZek}
		      A distribution is integrable if and only if it is involutive.
		\item       \label{ITEMooCQEAooRsLSOV}
		      If \( \mD\) is involutive, for every \( p\in M\), there exists a unique maximal\quext{As far as understand, here «maximal» means that every integral manifold containing \( p\) is contained in that one. It is used in that sense during the proof the theorem \ref{THOooXALIooGiPVdD}.} connected integral submanifold containing \( p\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	Point \ref{ITEMooLDUZooJCwZek} is given by the proposition \ref{PROPooDYJNooAwnaFK} and the theorem \ref{THOooDVBHooGRhuGl}.

	Proof of point \ref{ITEMooCQEAooRsLSOV} still to be done\ldots
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Analytic}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooPTEZooMZrAGg}]       \label{PROPooKAXOooZCwcwi}
	Let \( M\), \( N\) and \( S\) be analytic manifolds and \( f\colon M\to N\) be analytic.
	\begin{enumerate}
		\item
		      We suppose that \( f\) is an immersion and that \( g\colon S\to M\) is continuous. The map \( g\) is analytic if and only if \( f\circ g\) is analytic.
		\item
		      We suppose that \( f\) is a submersion and that \( g\colon N\to S\) is continuous. The map \( g\) is analytic if and only if \( g\circ f\) is analytic.
	\end{enumerate}
\end{proposition}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Tensor algebra}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Tensor algebra]      \label{DEFooHPQXooETvEyn}
	Let \( V\) be a vector space over \( \eC\). The \defe{tensor algebra}{tensor algebra} of \( V\) is the vector space
	\begin{equation}
		T(V)=\bigoplus_{n\geq 0}\left(\otimes^nV\right)=\eC\oplus V\oplus(V\otimes V)\oplus\ldots
	\end{equation}
\end{definition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Rank theorem}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We proof a generalization of the rank theorem \ref{ThoGkkffA}.

\begin{definition}
	Let \( M\) and \( N\) be smooth manifolds of dimension \( m\) and \( n\). Let a smooth map \( f\colon M\to N\). The \defe{rank}{rank} of \( f\) at \( p\in M\) is the rank of the linear map \( df_p\colon T_pM\to T_{f(p)N}\).
\end{definition}

\begin{lemma}
	Let a smooth map \( f\colon M\to N\) and \( p\in M\). Let \( \varphi\colon U\to M\) and \( \psi\colon V\to N\) be chats around \( p\) and \( f(p)\). Then
	\begin{equation}
		\rank(df_p)=\rank\big( f_{\varphi^{-1}(p)}(\psi^{-1}\circ f\circ \varphi) \big)
	\end{equation}
	where the rank on the right han side is the usual rank of a map \( \eR^m\to \eR^n\).
\end{lemma}

\begin{proof}
	By proposition \ref{PROPooEGNBooIffJXc} we can compute the rank of a linear map in whatever base. When a basis is chosen in \( \eR^m\) and \( \eR^n\) we know from lemma \ref{LEMooVCSJooEuDZFz} that the matrix of \( df_p\) is the same as the one of \(  f_{\varphi^{-1}(p)}(\psi^{-1}\circ f\circ \varphi) \). Since these two linear maps have the same matrix, they have the same rank.
\end{proof}

\begin{theorem}[Rank theorem\cite{BIBooVYIRooZyqygg}]       \label{THOooSWKVooTJQsXc}
	Let \( M\) and \( N\) be smooth manifolds of dimension \( m\) and \( n\). Let \( f\colon M\to N\) be a smooth map. Let \( p\in M\). We suppose that the rank of \( f\) is equal to \( k\) at every point \( x\) in a neighbourhood of \( p\).

	There exists charts \( \varphi\colon U\to M\) around \( p\in M\) and \( \psi\colon V\to N\) around \( f(p)\in M\) such that
	\begin{enumerate}
		\item
		      \( \varphi(0)=p\),
		\item
		      \( \psi(0)=f(p)\)
		\item
		      the function \( f\) is more or less trivialized in the sense that
		      \begin{equation}
			      (\psi^{-1}\circ f\circ\varphi)(x_1,\ldots, x_m)=(x_1,\ldots, x_k,0,\ldots, 0)
		      \end{equation}
		      for every \( (x_1,\ldots, x_m)\in U\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	We prove in two parts. Fist we consider the case in which \( M\) and \( N\) are open sets of \( \eR^m\) and \( \eR^n\). Then we will generalize to any smooth manifolds.
	\begin{subproof}
		\spitem[The case of \( \eR^m\) and \( \eR^n\)]
		Let \( W\) be open in \( \eR^m\), \( W'\) be open in \( \eR^n\). We consider a smooth map \( f\colon W\to W'\) such that \( f(0)=0\) and \( \rank(f)=k\) on \( W\).

		By hypothesis, the rank of \( df_0\) is \( k\), so that is one chooses good bases on \( \eR^m\) and \( \eR^n\) we can suppose that the matrix of \( df_0\) has a upper-left square \( k\times k\) with non-zero determinant. We write \( A\) that square matrix:
		\begin{equation}
			A_{ij}=\frac{ \partial f_i }{ \partial x_j }(0)
		\end{equation}
		with \( i,j\leq k\).

		\begin{subproof}
			\spitem[On the \( \eR^m\) side]

			We consider the map
			\begin{equation}
				\begin{aligned}
					\varphi\colon W\subset \eR^m & \to \eR^m                                                                                  \\
					(x_1,\ldots, x_m)            & \mapsto \big( f_1(x_1,\ldots, x_m),\ldots, f_k(x_1,\ldots, x_m),x_{k+1},\ldots, x_m \big).
				\end{aligned}
			\end{equation}
			We have \( \varphi(0)=0\) because \( f_i(0)=f(0)_i=0\). The matrix of the differential is
			\begin{equation}
				d\varphi_0=\begin{pmatrix}
					A & *          \\
					0 & \mtu_{n-k}
				\end{pmatrix}
			\end{equation}
			where \( A\) is \( k\times k\) and \( *\) is some \( k\times (n-k)\) matrix. Thus we have \( \det(d\varphi_0)=\det(A)\neq 0\). From the inverse function theorem \ref{ThoXWpzqCn}, the map \( \varphi\) is a local diffeomorphism, more precisely there exists an open set \( W_1\subset W\subset \eR^m\) such that the restriction
			\begin{equation}
				\varphi\colon W_1\to W_1
			\end{equation}
			is a diffeomorphism. From now on we only consider \( \varphi\) as being that restriction.

			The vector \( (y_1,\ldots, y_m)\) such that \( \varphi^{-1}(x_1,\ldots, x_m)=(y_1,\ldots, y_m)\) has the property that
			\begin{equation}
				\varphi(y_1,\ldots, y_m)=(x_1,\ldots, x_m),
			\end{equation}
			which means that\footnote{At this point, it is really important that \( f\) takes its values in \( \eR^n\), not in a general manifold: if \( (y)\) was in a manifold, the expression \( f_i(y)\) would not make sense.}
			\begin{equation}
				f_i(y_1,\ldots, y_m)=x_i
			\end{equation}
			when \( i=1,\ldots, k\) and
			\begin{equation}
				y_l=x_l
			\end{equation}
			when \( l=k+1,\ldots, m\).

			\spitem[On the middle side]

			Thus we have
			\begin{subequations}
				\begin{align}       \label{EQooAQJGooLqlnXJ}
					(f\circ \varphi^{-1})(x_1,\ldots, x_m) & =f(y_1,\ldots, y_m)                                                                \\
					                                       & =\big( x_1,\ldots, x_k,f_{k+1}(y_1,\ldots, y_m),\ldots, f_n(y_1,\ldots, y_m) \big) \\
					                                       & =\big( x_1,\ldots, x_k,\tilde f_{k+1}(x),\ldots, \tilde f_n(x)\big)
				\end{align}
			\end{subequations}
			where \( \tilde f_i=f_i\circ \varphi^{-1}\colon W_1\to \eR\) are some smooth functions.

			For every \( x\in W_1\) we have
			\begin{equation}        \label{EQooEDJIooLyPslk}
				f(f\circ \varphi^{-1})_x=\begin{pmatrix}
					\mtu_{k\times k} & 0           \\
					*                & d\tilde f_x
				\end{pmatrix}
			\end{equation}
			where \( d\tilde f_x\) is the matrix whose elements are \( \left( \frac{ \partial \tilde f_i }{ \partial x_s } \right)\) with \( i=k+1,\ldots, n\) and \( s=k+1,\ldots, m\). This is not a square matrix by the way. We have, by theorem \ref{THOooIHPIooIUyPaf},
			\begin{equation}
				d(f\circ\varphi^{-1})_x=df_{\varphi^{-1}(x)}\circ(d\varphi^{-1})_x
			\end{equation}
			while \( (d\varphi^{-1})_x\) is invertible. Thus
			\begin{equation}
				\rank\big( d(f\circ\varphi^{-1})_x \big)=\rank\big( df_{\varphi^{-1}(x)} \big)=k.
			\end{equation}
			So the rank of \( f\circ\varphi^{-1}\) is \( k\) all over \( W_1\). But the image of \( d(f\circ\varphi^{-1})_x\) is spanned by the columns of its differential given by \eqref{EQooEDJIooLyPslk}. The \( k \) columns spanned by the identity matrix are obviously linearly independent; these are thus a basis of the image. Since the vectors in the ``\( d\tilde f_x\)'' part are linearly independent of these \( k\) vectors, they must be vanishing:
			\begin{equation}
				\frac{ \partial \tilde f_i }{ \partial x_s }(x)=0
			\end{equation}
			for every \( x\in W_1\), \( i=k+1,\ldots, m\) and \( s=k+1,\ldots, n\).

			\spitem[On the \( \eR^n\) side]

			We do not know if \( n\geq m\) or \( m\geq n\). If \( n\geq n\), we choose \( V_1\) such that the projection of \( V_1\) on its \( m\) first components is included in \( W_1\). If \( n<m\) we choose \( V_1\) such that the projection of \( W_1\) on its \( n\) first components is included in \( V_1\)\quext{This precision about the choice of \( V_1\) is not done in \cite{BIBooVYIRooZyqygg} and seems strange to me. Am I correct ? By the way, there could be a misprint in the definition of \( T\) in \cite{BIBooVYIRooZyqygg}: \( y\) must have \( n\) components, not \( m\).}.

			With that choice of \( V_1\) in mind we can remember the functions \( \tilde f_i\colon W_1\to \eR\). If \( y\in V_1\), we define \( \tilde f(y)\) as \( \tilde f(x)\) with \( x\in W_1\) created from \( y\) either by adding zeroes or by projecting on \( \eR^m\). In both cases, the resulting \( y\) belongs to \( V_1\).

			So now we consider the map
			\begin{equation}        \label{EQooKEZOooSOTBlo}
				\begin{aligned}
					T\colon V_1       & \to \eR^n                                                                                \\
					(y_1,\ldots, y_n) & \mapsto \big( y_1,\ldots, y_k,y_{k+1}+\tilde f_{k+1}(y),\ldots, y_n+\tilde f_n(y) \big).
				\end{aligned}
			\end{equation}
			If \( y\in V_1\), the differential is the matrix given by
			\begin{equation}
				(dT_y)_{ij}=\frac{ \partial T_i }{ \partial y_j }(y)
			\end{equation}
			where
			\begin{itemize}
				\item
				      The upper-left \( k\times k\) corner is \( \mtu_{k\times k}\).
				\item
				      The upper-right \( k\times (n-k)\) corner (non square in general) is given by elements of the form
				      \begin{equation}
					      \frac{ \partial y_i }{ \partial y_{j} }
				      \end{equation}
				      with \( i\leq k\) and \( j>k\). So this is vanishing.
				\item
				      The lower-left (non square in general) corner is made of
				      \begin{equation}
					      \frac{ \partial (y_i+\tilde f_i(y)) }{ \partial y_j }=\frac{ \partial \tilde f_i(y) }{ \partial y_j }
				      \end{equation}
				      with \( i>k\) and \( j\leq k\). The elements in this pars are some numbers.
				\item
				      The lower-right square \( (n-k)\times (n-k)\) corner is made of
				      \begin{equation}
					      \frac{ \partial (y_i+\tilde f_i(y)) }{ \partial y_j }=\delta_{ij}+\frac{ \partial \tilde f_i }{ \partial y_j }
				      \end{equation}
				      with \( i>k\) and \( j\geq k\). For these elements we have \( \frac{ \partial \tilde f_i(y) }{ \partial y_j }=0\) and then the identity matrix.
			\end{itemize}
			With all that,
			\begin{equation}
				dT_y=\begin{pmatrix}
					\mtu_{k\times k} & 0          \\
					*                & \mtu_{n-k}
				\end{pmatrix}.
			\end{equation}
			Moreover \( T(0)=0\) because
			\begin{equation}
				\tilde f_i(0)=f_i\big( \varphi^{-1}(0) \big)=f_i(0)=0.
			\end{equation}
			We deduce that there exist an open set \( V\subset \eR^n\) included in \( V_1\) such that \( T\colon V\to T(V)\) is a diffeomorphism. We restrict \( V\) in such a way that \( T(V)\subset V_1\).

			\spitem[The final map]

			Finally we consider the map
			\begin{equation}
				T^{-1}\circ f\circ \varphi^{-1}\colon W_1 \to V.
			\end{equation}
			If \( (x_1,\ldots, x_m)\in W_1\) from \eqref{EQooAQJGooLqlnXJ} we have
			\begin{equation}
				(f\circ \varphi^{-1})(x_1,\ldots, x_m)=\big( x_1,\ldots, x_k,\tilde f_{k+1}(x),\ldots, \tilde f_n(x) \big).
			\end{equation}
			Using the definition \eqref{EQooKEZOooSOTBlo} we see that
			\begin{equation}
				T(x_1,\ldots, x_k,0,\ldots, 0)=\big( x_1,\ldots, x_k,\tilde f_{k+1}(x),\ldots, \tilde f_n(x) \big)
			\end{equation}
			which proves that
			\begin{equation}
				T^{-1}\big( x_1,\ldots, ,x_k,\tilde f_{k+1}(x),\ldots, \tilde f_n(x) \big)=(x_1,\ldots, x_k,\,\ldots, 0).
			\end{equation}
		\end{subproof}

		\spitem[The general case]

		Now we consider the manifolds \( M\) and \( N\) with the map \( f\colon M\to N\). Let \( p\in M\) and charts \( \varphi_0\colon U_0\to M\), \( \psi_0\colon V_0\to N\) where \( U_0\) is a neighbourhood of \( 0\) in \( \eR^m\) and \( V_0\) a neighbourhood of \( 0\) in \( \eR^n\). We suppose that \( \varphi_0(0)=p\) and \( \psi_0(0)=f(p)\).

		Now we consider the function \( \tilde f=\psi_0^{-1}\circ f\circ \varphi_0\) from \( U_0\) to \( V_0\) and we are left in the previous case.
	\end{subproof}
\end{proof}

If the differential is bijective, then the function is a local diffeomorphism.
\begin{theorem}[local inversion\cite{MonCerveau}]       \label{THOooDWEXooMClWVi}
	Let \( M,N\) be \( C^k\) manifolds. Consider a \( C^k\) map \( f\colon M\to N\). If \( df_p\colon T_pM\to T_{f(p)}N\) is a bijection, the \( f\) is a local \( C^k\)-diffeomorphism.

	More precisely, there exists a neighbourhood \( V_1\) of \( p\) in \( M\) and a neighbourhood \( V_2\) of \( f(p)\) in \( M\) such that \( f\colon V_1\to V_2\) is a \( C^k\)-diffeomorphism.
\end{theorem}

Due to proposition \ref{PropUssGpGenere}, a local diffeomorphism can be often converted into a global diffeomorphism.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exterior calculus}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exterior algebra}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Exterior product]
	If $V$ is a vector space, we denote by $\Lambda^kV^*$ the space of all the $k$-form on $V$. We define the \defe{exterior product}{product!exterior} $\dpt{\wedge}{\Lambda^kV^*\times\Lambda^lV^*}{\Lambda^{k+l}V^*}$ by
	\begin{equation}
		(\omega^k\wedge\eta^l)(v_1,\ldots,v_{k+l})
		=\us{k!l!}\sum_{\sigma\in S_{k+l}} (-1)^{\sigma}   \omega(v_{\sigma(1)},\ldots,v_{\sigma(k)})\eta(v_{\sigma(k+1)},v_{\sigma(k+1)})
	\end{equation}
\end{definition}
If $\{e_1,\ldots,e_n\}$ is a basis of $V$, the dual basis $\{\sigma^1,\ldots,\sigma^n\}$ of $V^*$ is defined by $\sigma^i(e_j)=\delta^i_j$.

If $I=\{1\leq i_1\leq\ldots i_k\leq n\}$, we write $\sigma^I=\sigma^{i_1}\wedge\ldots\sigma^{i_k}$ any $k$-form can be decomposed as
\[
	\omega=\sum_{I}\omega_I\sigma^I.
\]
The exterior algebra is provided with the \defe{interior product}{interior!product} denoted by $\iota$. It is defined by\label{pg_DefProdExt}
\begin{equation}
	\begin{aligned}
		\iota(v_0)\colon\Lambda^kW             & \to \Lambda^{k-1}W               \\
		(\iota(v_0)\omega)(v_1,\ldots,v_{k-1}) & =\omega(v_0,v_1,\ldots,v_{k-1}).
	\end{aligned}
\end{equation}

\begin{lemma}
	Let \( \sigma\) be an element of the symmetric group\footnote{Definition~\ref{DEFooJNPIooMuzIXd}.} of the set \( \{ a_1,\ldots, a_n \}\) where the \( a_i\) are integers. Then
	\begin{equation}
		(dx_{a_1}\wedge\ldots \wedge dx_{a_n})(e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})=(-1)^{\sigma}.
	\end{equation}
\end{lemma}

\begin{proof}
	We make it by induction over \( n\). With \( n=1\) the only permutation is the identity; the claim reduces to \( dx_{a_1}(e_{a_1})=1\). Let us try with \( n=2\). Up to renumbering we have
	\begin{equation}
		(dx_1\wedge dx_2)(e_1,e_2)=1
	\end{equation}
	and
	\begin{equation}
		(dx_1\wedge dx_2)(e_2,e_1)=-1.
	\end{equation}
	We pass to the induction. Let \( \sigma\in S_n\). We have
	\begin{subequations}
		\begin{align}
			(dx_{a_1}\wedge dx_{a_n})( & e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})=dx_{a_1}\wedge (dx_{a_n})(e_{\sigma(a_1)},\ldots, e_{\sigma(a_n)})                                                                   \\
			                           & =\sum_{\phi\in S_n}(-1)^{\phi}\frac{1}{ (n-1)! }dx_{a_1}\big( e_{\phi\sigma(a_1)} \big)(dx_{a_2}\wedge\ldots\wedge dx_{a_n})(e_{\phi\sigma(a_2)},\ldots, e_{\phi\sigma(a_n)}) \\
			                           & =\sum_{\phi\in S_n}(-1)^{\phi}\frac{1}{ (n-1)! }\delta_{a_1,\phi\sigma(a_1)}(-1)^{\phi\sigma}                                                                                 \\
			                           & =\sum_{\phi\in S_n}\delta_{a_1,\phi\sigma(a_1)}(-1)^{\sigma}\frac{1}{ (n-1)! }
		\end{align}
	\end{subequations}
	where we used the fact that the sign of a permutation provides a morphism between \( S_n\) and \( \{ -1,1 \}\) (proposition~\ref{ProphIuJrC}\ref{ITEMooBQKUooFTkvSu}). In the sum over \( S_n\), only the \( \phi\) that make \( \sigma(a_1)\to a_1\) remain; there are \( | S_{n-1} |=(n-1)!\) such elements. Thus the whole evaluates to \( (-1)^{\sigma}\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]    \label{LEMooICRXooFKPCRd}
	Let \( \tau_i\colon \eR^n\to \eR^{n-1}\) defined by
	\begin{equation}
		\tau_i(v)_k=\begin{cases}
			v_k     & \text{if } k<i             \\
			v_{k+1} & \text{if } k\geq i\text{.}
		\end{cases}
	\end{equation}
	Then we have
	\begin{equation}
		(dx_1\wedge\ldots\wedge\widehat{dx_i}\wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)=
		\det\Big(  \tau_i(v_1),\ldots, \widehat{\tau_i(v_i)},\ldots, \tau_i(v_n)  \Big)
	\end{equation}
	where the hat denotes a non present term.
\end{lemma}

\begin{proof}
	We extend \( \tau_i\) to the dual : \( \tau_i\colon(\eR^n)^*  \to (\eR^{n-1})^*\) is defined by
	\begin{equation}
		\tau_i(dx_k)=\begin{cases}
			dy_k     & \text{if } k<i \\
			dy_{k-1} & \text{if } k>i
		\end{cases}
	\end{equation}
	(not defined on \( dx_i\)). It is easy to check that, if \( k\neq i\),
	\begin{equation}
		\tau_i(dx_k)\tau_i(v)=dx_k(v).
	\end{equation}
	The value of  $(dx_1\wedge\ldots\wedge\widehat{dx_i}\wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)$ is a polynomial in the variables \( dx_k(v_l)\) (with \( k\neq l\)). Since \( dx_k(v_l)=\tau\i(dx_k)\big( \tau_iv_l \big)\), the same polynomial will give the value of
	\begin{equation}
		(\tau_idx_1\wedge\ldots\wedge \widehat{\tau_idx_i}\wedge\ldots\wedge \tau_idx_n  )(\tau_i v_1,\ldots, \widehat{\tau_iv_i},\ldots, \tau_iv_n).
	\end{equation}
	Thus we have
	\begin{subequations}
		\begin{align}
			(dx_1\wedge\ldots\wedge\widehat{dx_i} & \wedge\ldots\wedge dx_n)(v_1,\ldots, \widehat{v_i},\ldots, v_n)                                                                               \\
			                                      & =(\tau_idx_1\wedge\ldots\wedge \widehat{\tau_idx_i}\wedge\ldots\wedge \tau_idx_n  )(\tau_i v_1,\ldots, \widehat{\tau_iv_i},\ldots, \tau_iv_n) \\
			                                      & =(dy_1\wedge\ldots\wedge dy_{n-1})(\tau_iv_1,\ldots,\widehat{\tau_iv_i},\ldots, \tau_iv_n) \label{SUBEQooQGSKooSgfxJh}                        \\
			                                      & =\det\big( \tau_iv_1,\ldots, \widehat{\tau_i v_i},\ldots, \tau_iv_n \big)
		\end{align}
	\end{subequations}
	The last equality is because \eqref{SUBEQooQGSKooSgfxJh} is is a \( (n-1)\)-form applied to \( n-1\) vectors in \( \eR^{n-1}\) and so is the determinant.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Differential of \texorpdfstring{$k$}{k}-forms}
%---------------------------------------------------------------------------------------------------------------------------

The differential of a $k$-form is defined by the following theorem.

\begin{theorem}
	Let $M$ be a differentiable manifold. Then for each $k\in \eN$, there exists an unique map
	\[
		\dpt{d}{\Omega^k(M)}{\Omega^{k+1}(M)}
	\]
	such that

	\begin{enumerate}
		\item $d$ is linear,
		\item for $k=0$, we find back the $\dpt{d}{\Cinf(M)}{\Omega^1(M)}$ previously defined,
		\item if $f$ is a function and $\omega^k$ a $k$-form, then
		      \begin{equation}
			      d(f\omega^k)=df\wedge\omega^k+fd\omega^k,
		      \end{equation}


		\item $d(\omega^k\wedge\eta^l)=d\omega^k\wedge\eta^l+(-1)^k\omega^k\wedge d\eta^l$,
		\item $d\circ d=0$.
	\end{enumerate}
\end{theorem}

An explicit expression for $d\omega^k$ is actually given by
\begin{equation}
	d\omega^k=\sum d\omega_I\wedge dx^I
\end{equation}
if $\omega^k=\sum\omega_I dx^I$.
An useful other way to write it is the following. If $\omega$ is a $k$-form and $X_1,\ldots,X_{p+1}$ some vector fields,
\begin{equation}\label{eq:formule_domega}
	\begin{split}
		(k+1)d\omega(X_1,\ldots,X_{p+1})&=\sum_{i=1}^{p+1}(-1)^{i+1}X_i\omega(X_1,\ldots\hat{X}_i,\ldots,X_{p+1})\\
		&\quad+\sum_{i<j}(-1)^{i+j}\omega([X_i,X_j],X_1,\ldots,\hX_i,\hX_j,\ldots,X_{p+1}).
	\end{split}
\end{equation}
Let us show it with $p=1$. Let $\omega=\omega_i dx^i$ and compute $d\omega(X,Y)=\partial_i\omega_j(dx^i\wedge dx^j)(X,Y)$. For this, we have to keep in mind that the $\partial_i$ acts only on $\omega_j$ while, in equation \eqref{eq:formule_domega}, a term $X\omega(Y)$ means --pointwise-- the action of $X$ on the function $\dpt{\omega(Y)}{M}{\eR}$. So we have to use Leibnitz formula:
\[
	(\partial_i\omega_j)X^iY^j=(X\omega_j)Y^j
	=X(\omega_j Y^j)-\omega_j XY^j.
\]
On the other hand, we know that $[X,Y]^i=XY^i-YX^i$, so
\begin{equation}
	d\omega(X,Y)=X\omega(Y)-Y\omega(X)-\omega([X,Y]).
\end{equation}

\subsubsection{Hodge dual operator}
%/////////////////////////////
Let us take a manifold $M$ endowed with a metric $g$.  We can define a map $\dpt{r}{T^*_xM}{T_xM}$ by, for $\alpha\in T^*_xM$,
\[
	\scal{r(\alpha)}{v}=\alpha(v).
\]
for all $v\in T_xM$, where $\scal{\cdot}{\cdot}$ stands for the product given by the metric $g$. If we have $\alpha,\beta\in T^*_xM$, we can define
\[
	\scal{\alpha}{\beta}=\scal{r(\alpha)}{r(\beta)}.
\]
With this, we define an inner product on $\Lambda^p(T^*_xM)$:
\[
	\scal{\alpha_1\wedge\ldots\alpha_p}{\beta_1\wedge\ldots\beta_p}=\det_{ij}\scal{\alpha_i}{\beta_j}.
\]

\begin{definition}      \label{DEFooUOJQooSzKjNR}
	The \defe{Hodge operator}{Hodge operator} is $\dpt{\hodge}{\Lambda^p(T^*_xM)}{\Lambda^{n-p}(T^*_xM)}$ such that for any $\phi\in\Lambda^p(T^*_xM)$,
	\begin{equation}
		\phi\wedge(\hodge\psi)=\scal{\phi}{\psi}\Omega=\langle \phi,\psi \rangle\sqrt{|\det(g)|}dx^1\wedge\ldots\wedge dx^n.
	\end{equation}
\end{definition}

\begin{example} \label{EXooCIYIooFPMLMU}
	We consider \( \eR^n\) with the euclidian metric. If \( \sigma=dx_j\), then we expect \( \hodge\sigma\) to be \( sdx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n\) for a certain factor \( s\) to be fixed (something like \( (-1)^j\)).

	For every \( 1\)-form \( \phi\) we need \( \phi\wedge(\hodge \sigma)=\langle \phi, \sigma\rangle dx_1\wedge\ldots\wedge dx_n\). A basis of \( \Wedge^1(TM)\) is \( \{ dx_k \}_{k=1,\ldots, n}\), so we test on \( dx_k\).

	First we have
	\begin{equation}
		\langle dx_k, dx_j\rangle =\langle e_k, e_j\rangle =\delta_{kj}.
	\end{equation}
	Then
	\begin{equation}
		s\,dx_k\wedge dx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n=s\delta_{kj}(-1)^{j+1}dx_1\wedge\ldots\wedge dx_n.
	\end{equation}
	Thus we need \( s=(-1)^{j+1}\) and we have
	\begin{equation}
		\hodge dx_j=(-1)^{j+1}dx_1\wedge\ldots\wedge \widehat{dx_j}\wedge\ldots\wedge dx_n.
	\end{equation}
\end{example}

\subsubsection{Volume form and orientation}
%//////////////////////////////////////////

\begin{definition}[orientation\cite{BIBooJMRFooTAhhcg}]		\label{DEFooGMOQooVzqFxy}
	Let \( M\) be a \( n\)-dimensional manifold and two charts \(\varphi_{\alpha} \colon U_{\alpha}\to M  \) and \(\varphi_{\beta} \colon U_{\beta}\to M  \). We write \( V_{\alpha=\varphi_{\alpha}(u_{\alpha})}\) and \( V_{\beta}=\varphi_{\beta}(U_{\beta})\) and
	\begin{equation}
		\varphi_{\alpha\beta}=\varphi_{\beta}^{-1}\circ\varphi_{\alpha} \colon \varphi_{\alpha}^{-1}(V_{\alpha}\cap V_{\beta})\to U_{\beta}.
	\end{equation}
	We say that the charts \( \varphi_{\alpha}\) and \( \varphi_{\beta}\) have \defe{the same orientation}{orientation} if
	\begin{equation}
		\det\big( (d\varphi_{\alpha\beta})_{x} \big)>0
	\end{equation}
	for every \( x\in\varphi_{\alpha}^{-1}(V_{\alpha}\cap V_{\beta})\).
\end{definition}

\begin{definition}		\label{DEFooQZJPooOwMbim}
	Let \( M\) be a manifold. An \defe{orientation}{orientation} of \( M\) is an atlas \( \mA=\{ (U_{\alpha},\varphi_{\alpha}) \}_{\alpha\in\Lambda}\) whose charts have the same orientation\footnote{Definition \ref{DEFooGMOQooVzqFxy}}.

	If such an atlas exists, we say that the manifold \( M\) is \defe{orientable}{orientable}.
\end{definition}

\begin{theorem}[\cite{BIBooJMRFooTAhhcg}]		\label{THOooQEFUooQTtPDD}
	A differentiable manifold of dimension \( n\) is orientable\footnote{Definition \ref{DEFooQZJPooOwMbim}.} if and only if it admits a non-zero \( n\)-form.
\end{theorem}

Let $M$ be a $n$ dimensional smooth manifold. A \defe{volume form}{volume!form} on $M$ is a nowhere vanishing $n$-form and the manifold itself is said to be \defe{orientable}{orientable manifold} if such a volume form exists. Two volume forms $\mu_1$ and $\mu_2$ are describe the same orientation if there exists a function $f>0$ such that\footnote{Recall that the space of $n$-forms is one-dimensional.} $\mu_1=f\mu_2$.

One says that the \emph{ordered} basis $(v_1,\cdots,v_n)$ of $T_xM$ is \defe{positively oriented}{positive!orientation} with respect to the volume form $\mu$ is $\mu_x(v_1,\cdots,v_n)>0$.

\subsection{Musical isomorphism}\label{subsec_musique}\index{musical isomorphism}
%---------------------------------

In some literature, we find the symbols $v^{\flat}$ and $\alpha^{\sharp}$. What does it mean ? For $X\in\cvec(M)$ and $\omega\in\Omega^2(M)$, the \defe{flat}{flat} operation $v^{\flat}\in\Omega^1(M)$ is simply defined by the inner product:
\begin{equation}        \label{EQooBTWXooTqoNxa}
	v^{\flat}=i(v)\omega
\end{equation}
In the same way, we define the \defe{sharp}{sharp} operation by taking a $1$-form $\alpha$ and defining $\alpha^{\sharp}$ by
\begin{equation}
	i(\alpha^{\sharp})\omega=\alpha.
\end{equation}
An immediate property is, for all $v\in\cvec(M)$, $v^{\flat\sharp}=v$, and for all $\alpha\in\Omega^1(M)$, $\alpha^{\sharp\flat}=\omega$.

\subsection{Pull-back and push-forward}

\begin{normaltext}
	Let $\dpt{\varphi}{M}{N}$ be a smooth map, $\alpha$ a $k$-form on $N$, and $Y$ a vector field on $N$. Consider the map $\dpt{d\varphi}{T_xM}{T_{\varphi(x)}M}$. The aim is to extend it to a map from the tensor algebra\footnote{Definition \ref{DEFooHPQXooETvEyn}} of ${T_xM}$ to the one of $T_{\varphi(x)}M$.
\end{normaltext}

The \defe{pull-back}{pull-back!of a $k$-form} of $\varphi$ on a $k$-form $\alpha$ is the map
\[
	\dpt{\varphi^*}{\Omega^k(N)}{\Omega^k(M)}
\]
defined by
\begin{equation}\label{306e1}
	(\varphi^*\alpha)_m(v_1,\ldots,v_k)
	=\alpha_{\varphi(m)}(d\varphi_mv_1,\ldots,d\varphi_mv_k)
\end{equation}
for all $m\in M$ and $v_i\in\cvec(M)$.

Note the particular case $k=0$. In this case, we take --instead of $\alpha$-- a function $\dpt{f}{N}{\eR}$ and the definition \eqref{306e1} gives $\dpt{\varphi^*f}{M}{\eR}$ by
\[
	\varphi^*f=f\circ\varphi.
\]

The \defe{push-forward}{push-forward!of a $k$-form} of $\varphi$ on a $k$-form is the map
\[
	\dpt{\varphi_*}{\Omega^k(M)}{\Omega^k(N)}
\]
defined by $\varphi_*=(\varphi^{-1})^*$. For $v\in T_nN$, we explicitly have:
\[
	(\varphi_*\alpha)_n(v)=\alpha_{\varphi^{-1}(n)}(d\varphi_n^{-1} v).
\]

Let now $\dpt{\varphi}{M}{N}$ be a diffeomorphism. The \defe{pull-back}{pull-back!of a vector field} of $\varphi$ on a vector field is the map
\[
	\dpt{\varphi^*}{\cvec(N)}{\cvec(M)}
\]
defined by
\[
	(\varphi^*Y)(m)=[(d\varphi^{-1})_m\circ Y\circ\varphi](m),
\]
or
\[
	(\varphi^*Y)_{\varphi^{-1}(n)}=(d\varphi^{-1})_nY_n,
\]
for all $n\in N$ and $m\in M$. Notice that \[\dpt{(d\varphi^{-1})_n}{T_nN}{T_{\varphi^{-1}(n)}M},\] and that  $\varphi^{-1}(n)$ is well defined because $\varphi$ is an homeomorphism.

The \defe{push-forward}{push-forward!of a vector field} is, as before, defined by $\varphi_*=(\varphi^{-1})^*$. In order to show how to manipulate these notations, let us prove the following equation:
\[
	f_{*\xi}=(df)_{\xi}.
\]
For $\dpt{\varphi}{M}{N}$ and $Y$ in $\cvec(N)$, we just defined $\dpt{\varphi^*}{\cvec(N)}{\cvec(M)}$, by
\begin{eqnarray}
	\label{2112r1}(\varphi^*Y)_{\varphi^{-1}(n)}=(d\varphi^{-1})_nY_n.
\end{eqnarray}
Take $\dpt{f}{M}{N}$; we want to compute $f_*=(f^{-1})^*$ with $\dpt{(f^{-1})^*}{\cvec(M)}{\cvec(N)}$. Replacing the ``$^{-1}$``\ on the right places, the definition \eqref{2112r1} gives us
\[
	\Big[(f^{-1})^*X\Big]_{f(m)}=(df)_mX_m,
\]
if $X\in\cvec(M)$, and $m\in M$.

We can rewrite it without  any indices: the coherence of the spaces automatically impose the indices: $(f^{-1})^*X=(df)X$. It can also be rewritten as $(f^{-1})^*=df$, and thus $f_*=df$. From there to $f_{* \xi}=(df)_{\xi}$, it is straightforward.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Lie derivative}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Consider $X\in\cvec(M)$ and $\alpha\in\Omega^p(M)$. Let $\dpt{\varphi_t}{M}{M}$ be the flow of $X$. The \defe{Lie derivative}{Lie!derivative!of a $p$-form} of $\alpha$ is
\begin{equation}\label{liesurforme}
	\mL_X\alpha=\lim_{t\to 0}\us{t}[(\varphi^*_t\alpha)-\alpha]=\dsdd{\varphi^*_t\alpha}{t}{0}.
\end{equation}
More explicitly, for $x\in M$ and $v\in T_xM$,
\[
	(\mL_X\alpha)_x(v)=\lim_{t\to 0}\us{t}\left[(\varphi_t^*\alpha)_x(v)-\alpha_x(v)\right]
\]
In the definition of the \defe{Lie derivative}{Lie!derivative!of a vector field} for a vector field, we need an extra minus sign:
\begin{equation}		\label{EqDefLieDerivativeVect}
	(\mL_XY)_x=\dsdd{\varphi_{-t*}Y_{\varphi_t(x)}}{t}{0}.
\end{equation}
Why a minus sign ? Because $Y_{\varphi_t(x)}\in T_{\varphi_t(x)}M$, but $\dpt{(d\varphi_{-t})_a}{T_aM}{T_{\varphi_{-t}(a)}M}$ so that, if we want, $\varphi_{-t*}Y_{\varphi_t(x)}$ to be a vector at $x$, we can't use $\varphi_{t*}$.

These two definitions can be embedded in only one. Let $X\in\cvec(M)$ and $\varphi_t$ its integral curve\footnote{\textit{i.e.} for all $x\in M$, $\varphi_0(x)=x$ and $\dsdd{\varphi_{u+t}(x)}{t}{0}=X_{\varphi_u(x)}$.}\index{integral!curve}. We know that $\varphi_{t*}$ is an isomorphism $\dpt{\varphi_{t*}}{T_{\varphi^{-1}(x)}M}{T_xM}$. It can be extended to an isomorphism of the tensor algebras at $\varphi^{-1}(x)$ and $x$. We note it $\tilde{\varphi}_t$. For all tensor field $K$ on $M$, we define
\[
	(\mL_XK)_x=\lim_{t\to 0}[K_x-(\tilde{\varphi_t}K)_x].
\]

On a Riemannian manifold $(M,g)$, a vector field $X$ is a \defe{\href{http://en.wikipedia.org/wiki/Killing_vector_fields}{Killing vector field}}{killing!vector field} if $\mL_Xg=0$.



\begin{lemma}
	Let $\dpt{f}{(-\epsilon,\epsilon)\times M}{\eR}$ be a differentiable map with $f(0,p)=0$ for all $p\in U$. Then there exists $\dpt{g}{(-\epsilon,\epsilon)\times M}{\eR}$, a differentiable map such that $f(t,p)=tg(t,p)$ and
	\[
		g(0,q)=\left.\dsd{f(t,q)}{t}\right|_{t=0}.
	\]
\end{lemma}
\begin{proof}
	Take
	\[
		g(t,q)=\int_0^1\dsd{f(ts,p)}{(ts)}ds,
	\]
	and use the change of variable $s\to ts$.
\end{proof}

\begin{lemma}
	If $\varphi_t$ is the integral curve of $X$, for all function $\dpt{f}{M}{\eR}$, there exists a map $g$, $g_t(p)=g(t,p)$ such that
	$f\circ\varphi_t=f+tg_t$ and $g_0=Xf$.
\end{lemma}

\begin{proof}
	Consider $\overline{f}(t,p)=f(\varphi_t(p))-f(p)$, and apply the lemma:
	\[
		f\circ\varphi_t=tg_t(p)+f(p).
	\]
	Thus we have
	\[
		Xf=\lim_{t\to 0}\us{t}[f(\varphi_t(p))-f(p)]=\lim_{t\to 0}g_t(p)=g_0(p).
	\]
\end{proof}

One of the main properties of the Lie derivative is the following:
\begin{theorem}		\label{ThoLieDerrComm}
	Let $X$, $Y\in\cvec(M)$ and $\varphi_t$ be the integral curve of $X$. Then
	\[
		[X,Y]_p=\lim_{t\to 0}\us{t}[Y-d\varphi_tY](\varphi_t(p)),
	\]
	or
	\begin{equation}
		\mL_XY=[X,Y].
	\end{equation}
	where the commutator is given by the definition \ref{DEFooHOTOooRaPwyo}.
\end{theorem}
\begin{proof}
	Take $\dpt{f}{M}{\eR}$ and the function given by the lemma: $\dpt{g_t}{M}{\eR}$ such that $f\circ \varphi_t=f+tg_t$ and $g_0=Xf$. Then put $p(t)=\varphi_t^{-1}(p)$. The rest of the proof is a computation:
	\[
		(\varphi_{t*}Y)_pf=Y(f\circ\varphi_t)_{p(t)}=(Yf)_{p(t)}+t(Yg_t)_{p(t)},
	\]
	so
	\begin{equation}
		\begin{split}
			\lim_{t\to 0}\us{t}[Y_p-(\varphi_{t*}Y)_p]f&=\lim_{t\to 0}\us{t}[(Yf)_p-(Yf)_{p(t)}]-\lim_{t\to 0}(Yg_t)_{p(t)}\\
			&=X_p(Yf)-Y_pg_0\\
			&=[X,Y]_pf.
		\end{split}
	\end{equation}

\end{proof}

A second important property is
\begin{theorem}
	For any function $f\colon M\to V$,
	\[
		\mL_Xf=Xf.
	\]
\end{theorem}

\begin{proof}
	If $X(t)$ is the path which defines the vector $X$, it is obvious that at $t=0$, $X(t)$ is an integral curve to $X$, so that we can take $X(t)$ instead of $\varphi_t$ in \eqref{liesurforme}. Therefore we have:
	\begin{equation}
		\mL_Xf=\dsdd{\varphi_t^*f}{t}{0}
		=Xf
	\end{equation}
	by definition of the action of a vector on a function.
\end{proof}
