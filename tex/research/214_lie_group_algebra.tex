% This is part of Giulietta
% Copyright (c) 2013-2015, 2019-2022, 2024
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representations from the Lie algebra to the Lie group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}[\cite{BIBooYTTJooYpPYLT}]       \label{PROPooXCGMooKlJlwp}
	Let \( G\) be a Lie group and \( \lG\) be its Lie algebra. Let \( (\rho, V)\) be a smooth representation of \( G\). We consider the map
	\begin{equation}
		\begin{aligned}
			s\colon \lG & \to \End(V)                     \\
			s(X)v       & =\Dsdd{ \rho( e^{tX})v }{t}{0}.
		\end{aligned}
	\end{equation}
	We have the equality
	\begin{equation}
		\rho( e^{tX})= e^{ts(X)}
	\end{equation}
	as operators on \( V\).
\end{proposition}

\begin{proof}
	Let \( X\in \lG\). We define \( M(t)=\rho( e^{tX})\) and \( N(t)= e^{ts(X)}\). These are maps from \( \eR\) to \( \End(V)\); the proposition \ref{PROPooSDNNooQtHkhA} helps to derive them.

	For \( N\) we immediately have
	\begin{equation}
		N'(t)=s(X) e^{ts(X)}.
	\end{equation}
	For \( M\), we have few more prudence. We fix \( \epsilon>0\) and we write (thanks to proposition \ref{PROPooKDKDooCUpGzE})
	\begin{equation}
		M(t+\epsilon)=\rho( e^{(t+\epsilon)X})=\rho( e^{tX} e^{\epsilon X})= \rho( e^{tX}) e^{\epsilon X}.
	\end{equation}
	Now for the differential quotient,
	\begin{equation}
		\frac{ M(t+\epsilon)-M(t) }{ \epsilon }=\rho( e^{tX})\frac{ \rho( e^{\epsilon X})-\id }{ \epsilon }.
	\end{equation}
	If we compute the limit \( \epsilon\to 0\), the second factor goes, by definition to \( s(X)\). So
	\begin{equation}
		M'(t)=\rho( e^{tX})s(X)=s(X)M(t).
	\end{equation}
	So \( M\) and \( N\) satisfy the same differential equation
	\begin{subequations}
		\begin{numcases}{}
			y'=s(X)y(t)\\
			y(0)=\mtu
		\end{numcases}
	\end{subequations}
	for the function \( y\colon \eR\to \End(V)\). This is a work for Cauchy-Lipschitz, theorem \ref{THOooZIVRooPSWMxg}. So we define \( f(t,m)=s(X)m\) and we show that this is Lipschitz with respect to \( m\) :
	\begin{subequations}
		\begin{align}
			\| f(t_0,m_0)-f(t,m) \|=\| s(X)(m_0-m) \|\leq \| s(X) \|\| m_0-m \|.
		\end{align}
	\end{subequations}
	So the function \( f\) is Lipschitz with respect to \( m\) with a Lipschitz constant bounded by \( \| s(X) \|\).

	The unicity part of Cauchy-Lipschitz shows that \( M(t)=N(t)\) for every \( t\) for which the expressions make sense.
\end{proof}

\begin{theorem}[\cite{BIBooYTTJooYpPYLT,MonCerveau}]       \label{THOooLVSNooOpzYgO}
	Let \( G\) be a Lie group and \( \lG\) be its Lie algebra. If \( (\rho, V)\) is a finite dimensional representation of \( G\), the map
	\begin{equation}
		\begin{aligned}
			s\colon \lG & \to \End(V)                    \\
			s(X)v       & =\Dsdd{ \rho( e^{tX})v }{t}{0}
		\end{aligned}
	\end{equation}
	is a representation\footnote{Representation of a Lie algebra, definition \ref{DEFooHINCooYoxPFj}.} of \( \lG\) on \( V\).
\end{theorem}

\begin{proof}
	We choose a basis of \( V\) and we consider the matrix representation associated with \( \rho\), namely\footnote{If you want details, this is \( \tilde \rho=\psi^{-1}\circ\rho\) where \( \psi\colon \eM(n,\eC)\to \GL(V)\) is defined in \ref{PROPooGXDBooHfKRrv}.}
	\begin{equation}
		\tilde \rho\colon G\to \GL(n,\eC).
	\end{equation}
	We have
	\begin{equation}
		\rho\big( \exp(tX) \big)v=\sum_{kl}\tilde \rho\big( \exp(tX) \big)_{kl}v_le_k.
	\end{equation}
	and then
	\begin{subequations}
		\begin{align}
			s(X)v & =\Dsdd{ \sum_{kl}\tilde \rho\big( \exp(tX) \big)_{kl}v_ke_l }{t}{0}                                                                             \\
			      & =\sum_{kl}\left( \Dsdd{ \tilde \rho\big( \exp(tX) \big) }{t}{0}\right)_{kl}v_ke_l      \label{SUBEQooZGVHooStJJQm}                              \\
			      & =\sum_{kl}\left( \Dsdd{  \exp\big( td\tilde \rho_e(X) \big)  }{t}{0}\right)_{kl}v_ke_l                             & \text{lem. \ref{lemsur5d}} \\
			      & =\sum_{kl}d\tilde \rho_e(X)_{kl}v_ke_l                                                                                                          \\
			      & =d\tilde \rho_e(X)v.
		\end{align}
	\end{subequations}
	We keep in mind that, expressed with matrices, we have
	\begin{equation}    \label{EQooAZTTooIFjJmf}
		s(X)v=d\tilde \rho_e(X)v.
	\end{equation}

	\begin{subproof}
		\spitem[\( s\) is linear]
		%------------------------
		This is equation \eqref{EQooAZTTooIFjJmf}.
		\spitem[Lie bracket]
		% -------------------------------------------------------------------------------------------- 
		We have to prove that \( s\big( [X,Y] \big)v=[s(X),s(Y)]v\). On the left-hand side, the bracket is the commutator in \( \GL(V)\). We start from the expression \eqref{SUBEQooZGVHooStJJQm} and we focus on what lies inside the derivative.

		The trick is to use the formulas of lemma \ref{LEMooGMMNooVlDkNm} to enter \( d\tilde \rho_e\) inside the commutator via the lemma \ref{lemsur5d}. Here is the computation:
		\begin{subequations}
			\begin{align}
				\tilde \rho\big( \exp(t[X,Y]) \big) & =\tilde \rho\Big( \alpha(t)\exp(\sqrt{ t }X)\exp(\sqrt{ t }Y)\exp(-\sqrt{ t }X)\exp(-\sqrt{ t }Y) \Big) \\
				                                    & =\tilde \rho\big( \alpha(t) \big) \tilde \rho\big( \exp(\sqrt{ t }X) \big)
				\tilde \rho\big( \exp(\sqrt{ t }Y) \big)                                                                                                      \\\nonumber
				                                    & \qquad\tilde \rho\big( \exp(-\sqrt{ t }X) \big)
				\tilde \rho\big( \exp(-\sqrt{ t }Y) \big)                                                                                                     \\
				                                    & =\tilde \rho\big(\alpha(t)\big)
				\exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)
				\exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)                                                                                                   \\\nonumber
				                                    & \qquad\exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)
				\exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)
			\end{align}
		\end{subequations}
		Thus, using the Leibniz rule, ans the properties \( \alpha(0)=e\), \( \alpha'(0)=0\) we have
		\begin{subequations}
			\begin{align}
				\Dsdd{ \tilde \rho\Big( \exp\big( t[X,Y] \big) \Big) }{t}{0} & =\Dsdd{  \tilde \rho\big( \alpha(t) \big)\exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)\ldots   }{t}{0}                                           \\
				                                                             & =\Dsdd{ \tilde \rho\big( \alpha(t) \big) }{t}{0}                                                                                               \\\nonumber
				                                                             & \qquad+\tilde \rho\big( \alpha(0) \big)\Dsdd{ \exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)\ldots }{t}{0}                                        \\
				                                                             & =\Dsdd{ \exp\big( \sqrt{ t }d\tilde \rho_e(X) \big)
				\exp\big( \sqrt{ t }d\tilde \rho_e(Y) \big)                                                                                                                                                                   \\\nonumber
				                                                             & \qquad \exp\big( -\sqrt{ t }d\tilde \rho_e(X) \big)
				\exp\big( -\sqrt{ t }d\tilde \rho_e(Y) \big) }{t}{0}                                                                                                                                                          \\
				                                                             & =\big[ d\tilde \rho_e(X),d\tilde \rho_e(Y)  \big]                                                       & \text{eq. \eqref{EQooDCUJooYDDZHD}}  \\
				                                                             & =\big[ s(X),s(Y) \big]                                                                                  & \text{eq. \eqref{EQooAZTTooIFjJmf}.}
			\end{align}
		\end{subequations}
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Old stuff}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

I move here the theorems which are not really well proven and which are replaced by better ones.


\section{Cosets}
%------------------

We consider $G$, a Lie group and $H$, a closed subgroup. Then from theorem~\ref{THOooXVXBooZDJzQo},  there exists an unique analytic structure on $H$ for which $H$ is a topological Lie subgroup of $G$. We naturally consider this structure on $H$. We also consider $\lG$ and $\lH$, the Lie algebras of $G$ and $H$, and $\lM$ be a subspace of $\lG$ such that $\lG=\lM\oplus\lH$.

Now we will study the structure of the coset space $G/H$ on which we put the topology such that $\pi$ is continuous and open; this is the \defe{natural topology}{natural topology}\index{topology!natural on $G/H$}\label{pg:natur_topo}. As notations, we define $p_0=\pi(e)$ and $\dpt{\psi}{\lM}{G}$, the restriction to $\lM$ of the exponential.

\begin{lemma}
	The dimension of $G/H$ is $\dim (G/H)=\dim G-\dim H$.
	\label{lem:dim_G_H}
\end{lemma}

\begin{proof}
	We decompose the Lie algebra $\lG$ as $\lG=\lH\oplus\lM$, and we will see that there exists a real vector space isomorphism $\dpt{\psi}{T_{[e]}(G/H)}{\lM}$ given by
	\begin{equation}
		\psi(X)=\Dsdd{ e^{m(t)} }{t}{0}
	\end{equation}
	if $X(t)=[g(t)]$ with $g(t)=e^{m(t)}e^{h(t)}$ where $m(t)\in\lM$ and $h(t)\in\lH$ (the existence of such a decomposition in reasonably small neighbourhood of $e$ is given by lemma~\ref{LEMooEBQUooKXkCda}). The fact that $\psi$ is surjective is clear. The injectivity is also easy: $\psi(X)=0$ implies that $\exp m(t)$ is a constant. Thus
	\[
		X=\Dsdd{ [cst\, e^{h(t)}] }{t}{0}=\Dsdd{[cst]}{t}{0}=0.
	\]

\end{proof}

\begin{lemma} \label{lem:vois_U}
	There exists a neighbourhood $U$ of $0$ in $\lM$ such that
	\begin{enumerate}
		\item $\psi$ is homeomorphic on $U$,
		\item $\pi$ sends homeomorphically $\psi(U)$ on a neighbourhood of $p_0$ in $G/H$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	By lemma~\ref{LEMooEBQUooKXkCda}, we consider bounded, open and connected neighbourhoods $\mU_m$ and $\mU_h$ of $0$ in $\lM$ and $\lH$ such that
	\[
		\dpt{\phi}{(A,B)}{e^Ae^B}
	\]
	is a diffeomorphism from $\mU_m\times\mU_h$ to an open neighbourhood of $e$ in $G$. Since $H$ has the induced topology from $G$, we can find a neighbourhood $V$ of $e$ in $G$ such that $V\cap H=\exp\mU_h$.

	Now we take $U$, a compact neighbourhood of $0$ in $\mU_m$ such that
	\begin{equation}\label{eq:UUV}
		e^{-U}e^{U}\subset V.
	\end{equation}
	So, $\psi$ is an homeomorphism from $U$ to $\psi(U)$. Indeed for $X\in U$, $\psi(X)=e^X=\phi(X,0)$ and $\phi$ is diffeomorphic.

	On the other hand, $\pi$ is bijective on $\psi(U)$. In order to see that it is injective, let us consider $X$, $Y\in U$ such that $\pi(e^{X})=\pi(e^{Y})$. Then $\exp X$ and $\exp Y$ are in the same class with respect to $H$: $\exp X\in[\exp Y]$. Then $\exp(-X)\exp Y\in H$, and reversing the role\angl of $X$ and $Y$, $\exp(-Y)\exp X\in H$. Since $X',X''\in U$ and \eqref{eq:UUV},
	\[
		e^{-Y}e^{X}\in V\cap H.
	\]
	Then there exists a $Z$ in $\mU_h$ such that $\exp X=\exp Y\exp Z$, but $U$ is a subset of $\mU_m$ (so that $(A,B)\to e^Ae^B$ is diffeomorphic), then $X=Y$ and $Z=0$.

	Since $\pi$ is bijective on $\psi(U)$, it is homeomorphic because the topology is build in order for $\pi$ to be open and continuous.

	On a third hand, $U\times\mU_h$ is a neighbourhood of $(0,0)$ in $\mU_m\times\mU_h$, so that $e^Ue^{\mU_h}$ is a neighbourhood of $e$ in $G$. Since $\pi$ is open, $\pi(\exp U\exp \mU_h)=\pi(\psi(U))$ is a neighbourhood of $p_0$ in $G/H$.
\end{proof}


Let $N_0$ be the interior of $\pi(\psi(U))$ and $\{X_1,\ldots, X_r\}$ a basis of $\lM$. If $g\in G$, we looks at the map
\[
	\pi(g\cdot e^{x_1X_1+\cdots+x_rX_r})\to(x_1,\ldots,x_r).
\]
This is an homeomorphism from $g\cdot N_0$ to an open subset of $\eR^r$ because $\pi$ is homeomorphic from $U$. With this chart, $G/H$ is an analytic manifold \nomenclature{$G/H$}{as analytic manifold} and moreover if $x\in G$, the map
\begin{equation}\label{eq:tau_x_y}
	\dpt{\tau(x)}{[y]}{[xy]}
\end{equation}
is an analytic diffeomorphism of $G/H$. Let us prove it. If we consider $[x]\in G/H$, we can write $x=gm$ for a certain $m\in\psi(U)$. Hence the chart around $[x]$ will be around $[gm]=[ge^{x_1X_1+\cdots+x_rX_r}]$ (in other word, we can find an open set around $[x]$ on which can be parametrised so). We can forget the $g$ because the action is a diffeomorphism. Then we looks at the chart $\dpt{\varphi}{G/H}{\eR^r}$, $\varphi[e^{x_1X_1+\cdots+x_rX_r}]=(x_1,\ldots,x_r)$. The map \eqref{eq:tau_x_y} makes
\begin{equation}
	(y_1,\ldots,y_r)\to( CBH_1(x_1,\ldots,x_r,y_1,\ldots y_r),\ldots, CBH_r(x_1,\ldots,x_r,y_1,\ldots y_r)).
\end{equation}
But $CBH$ is a diffeomorphism.

\begin{theorem}[\cite{Helgason}]\label{Helgason4.2}\label{tho:struc_anal}
	Let $G$ be a Lie group, $H$ a closed subgroup of $G$ and $G/H$ with the natural topology. Then $G/H$ has an unique analytic structure with the property that $G$ is a Lie transformation group of $G/H$.
\end{theorem}

\begin{proof}
	We denote by $\UU$ the interior of the $U$ given by the lemma~\ref{lem:vois_U}, and $B=\psi(\UU)\subset G$. Since $\dpt{\phi}{(A,B)}{\exp A\exp B}$ is a diffeomorphism, $\psi(\UU)=\phi(U,0)$ is a submanifold of $G$. We consider the following diagram:
	\[
		\xymatrix{
			G\times B  \ar[d]_{\displaystyle I\times\pi}\ar[r]^{\displaystyle\Phi}    &
			G\ar[d]^{\displaystyle\pi}\\
			G\times N_0 &                                                             G/H
		}\]
	with, for $g\in G$ and $x\in B$,
	\[
		I\times\pi\colon (g,x)\mapsto (g,[x])
	\]
	and
	\[
		\Phi\colon (g,x)\mapsto gx.
	\]

	\noindent The classes $[x]$ are taken with respect to $H$. The map $\dpt{\mu}{G\times N_0}{G/H}$, $\mu(g,[x])=[gx]$ can be written under the form
	\[
		\mu=\pi\circ\Phi\circ(I\times\pi)^{-1}
	\]
	which is analytic\footnote{Notice that the inverse of $I\times\pi$ exists because $\pi$ is homeomorphic on the spaces considered here.}. So $G$ is a Lie transformation group on $G/H$.

	% Faut encore taper l'unicité
\end{proof}


\begin{lemma}[Category theorem] \label{lem:categ}
	If a locally compact space $M$ can be written as a countable union
	\begin{equation}\label{eq:M_union}
		M=\bigcup_{n=1}^{\infty}M_n
	\end{equation}
	where each $M_i$ is closed in $M$, then at least one of them contains an open subset of $M$.
\end{lemma}

\begin{proof}
	We suppose that none of the $M_i$ contains an open subset of $M$. Let $U_1$ be an open whose closure is compact, $a_1\in U_1\setminus M_1$ and a neighbourhood $U_2$ of $a_1$ such that $\overline{U}_2\subset U_1$ and $\overline{U_2}\cap M_1=\varnothing$. Let $a_2\in U_2\setminus M_2$ and a neighbourhood $U_3$ of $a_2$ such that $\overline{U_3}\subset U_2$ and $\overline{U_3}\cap M_2=\varnothing$\ldots and so on. The existence of the $a_i$ comes from the fact that $U_j$ is open, so that it is contained in no one of the $M_k$.

	The decreasing sequence $\overline{U}_1,\overline{U}_2 ,\ldots$ is made up from non empty compacts sets. Then $\bigcap_{i=1}^{\infty}U_i\neq\emptyset$ and the elements of this intersection are in none of the $M_i$; this contradict \eqref{eq:M_union}.
\end{proof}


\begin{theorem} \label{tho:homeo_action}
	Let $G$ be a locally compact group with a countable basis. Suppose that it is a transitive, locally compact and Hausdorff topological group of transformation on $M$. Consider $p\in M$ and $H=\{g\in G\tq g\cdot p=p\}$. Then
	\begin{enumerate}
		\item $H$ is closed,
		\item the map $[g]\to g\cdot p$ is homeomorphic between  $G/H$ and $M$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	By definition of a group action, the map $\dpt{\varphi}{G}{M}$, $\varphi(g)=g\cdot p$ is continuous. Then $H=\varphi^{-1}(p)$ is closed in $G$.

	As usual, the topology considered on $G/H$ is a topology which makes the canonical projection $\dpt{\pi}{G}{G/H}$ continuous and open. Now we study the map $\dpt{\psi}{G/H}{M}$, $\psi([g])=g\cdot p$ which is well defined because $H$ fixes $p$ by definition. It is clearly injective, and it is surjective because the action is transitive.

	Now remark that $\psi=\varphi\circ\pi^{-1}$. Since $\pi$ is continuous and open, and $\varphi$ is continuous, it just remains to be proved that $\varphi$ is open in order for $\psi$ to be continuous and open. In order to do it, consider $V$, an open subset of $G$, $g\in V$ and a compact neighbourhood $U$ of $e$ in $G$ such that $U=U^{-1}$ and $gU^2\subset V$. If $U$ is small and $u$, $v\in U$ close to $e$, then $guv$ can keep in $V$, so that such a $U$ exists.

	We can find a sequence $(g_n)$ in $G$ such that $G=\bigcup_ng_nU$; the transitivity of $G$ on $M$ implies that
	\[
		M=\bigcup_ng_nU\cdot p.
	\]
	Each term in this union is compact, and therefore closed in $M$. By lemma~\ref{lem:categ}, one of the $g_nU\cdot p$ contains an open subset of $M$. Since the action ``$g\cdot$''\ is continuous, $U\cdot p$ also contains an open subset in $M$. The conclusion is that one can find a $u\cdot p$ in the interior of $M$, and $p$ is then an interior point of $u^{-1} U\cdot p\subset U^2\cdot p$. Then $g\cdot p$ in in the interior of $V\cdot p$ and $\varphi$ is therefore open.
\end{proof}

\begin{proposition}
	Let $G$ be a transitive transformation Lie group on a $\Cinf$ manifold $M$. Consider $p_0\in M$ and $H$, the stabilizer of $p_{0}$:
	\[
		H=\{ g\in G\tq g\cdot p_0=p_0 \}.
	\]
	Let
	\begin{equation}
		\begin{aligned}
			\alpha\colon G/H & \to M                 \\
			[g]              & \mapsto g\cdot p_{0}.
		\end{aligned}
	\end{equation}
	We have:
	\begin{enumerate}
		\item The stabilizer $H$ is closed in $G$.
		\item If $\alpha$ is homeomorphic, then it is diffeomorphic (if $G/H$ has the analytic structure of theorem~\ref{tho:struc_anal}).
		\item If $\alpha$ is homeomorphic and if $M$ is connected, then $G_0$, the identity component of $G$, is transitive on $M$.
	\end{enumerate}
	\label{propHelgason4.3}
\end{proposition}

This comes from \cite{Helgason}, chapter 2, proposition 4.3. The interest of this theorem is the fact that one only has to check the continuity of $\alpha$ and $\alpha^{-1}$ in order to have a diffeomorphism $M\simeq G/H$.

\begin{proof}
	\subdem{The group $H$ is closed in $G$}
	We consider the map $\dpt{\varphi}{G}{M}$, $\varphi(g)=g\cdot p_0$. This is continuous; therefore $\varphi^{-1}(p_0)$ is closed. Remark that we are in the situation of theorem~\ref{tho:homeo_action}

	\subdem{First item}
	We will use lemma~\ref{lem:vois_U}. Se denotes by $\lH$, the Lie algebra of $H$ and we consider a $\lM$ such that $\lG=\lM\oplus\lH$; the lemma~\ref{lem:vois_U} assure us that we have a neighbourhood $U$ of $0$ in $\lM$ on which $\psi$ is homeomorphic and such that $\pi$ sends homeomorphically $\psi(U)$ to a neighbourhood of $p_0$ in $G/H$. We define $\UU$, the interior of $U$, $B=\psi(\UU)$ and $N_0$, the interior of $\pi(\psi(U))$.

	The set $B$ is a submanifold of $G$, diffeomorphic to $N_0$ by $\pi$ because everything is continuous and then everything respect the interiors.

	\begin{probleme}
		C'est n'importe quoi comme justification. C'est lié au problème~\ref{prob:diffeo_2}.
		\label{prob:diffeo_1}
	\end{probleme}

	Consider $\dpt{\iota}{B}{G}$, the identity and $\dpt{\beta}{G}{M}$, $\beta(g)=g\cdot p_0$. The restriction $\alpha_{N_0}$ of $\alpha$ to $N_0$ is an homeomorphism (this is a part of the assumptions) from $N_0$ to an open subset of $M$: $N_0$ is open (this is an interior), then its image by an homeomorphism is open.

	Now we can see that $\alpha_{N_0}$ is differentiable. The reason is that it can be written as $\alpha_{N_0}=\dpt{\beta\circ\iota\circ\pi^{-1}}{N_0}{M}$. The construction makes $\pi$ a diffeomorphism and $\beta$ a diffeomorphism when $G$ is a Lie group of transformations (as it is the case here); $\iota$ is clear. Now we have to see that the whole $\alpha$ is also differentiable, and then we will have to prove the same for $\alpha^{-1}$.

	By definition, $\alpha([g])=g\cdot p_0$ (the classes $[g]$ is taken with respect to $H$). Consider $[n]\in H$; for any $g\in G$, one can write $[g]=[gn^{-1} n]$. Then
	\begin{equation}
		\alpha([g])=\alpha([gn^{-1} n])
		=gn^{-1} n\cdot p_0
		=gn^{-1}\alpha([n])
		=gn^{-1}\cdot\alpha_{N_0}([n]),
	\end{equation}
	but the last dot denotes a differentiable action, and $\alpha_{N_0}$ is differentiable. Thus $\alpha$ is differentiable.

	In order for $\alpha$ to be a diffeomorphism, we still have to prove that $\alpha^{-1}$ is differentiable., we begin to show that the Jacobian of $\beta$ at $g=e$ has rank $r_{\beta}=\dim M$. We looks at $\dpt{d\beta_e}{\lG}{T_{p_0}M}$, and consider $X\in\ker (d\beta_e)$. For $f\in\Cinf(M)$, we compute
	\begin{equation}
		0=(d\beta_e X)f=X(f\circ\beta)=\Dsdd{ f(e^{tX}\cdot p_0) }{t}{0}.
	\end{equation}
	Let $s\in\eR$, and we write this equation for $f^*$ instead of $f$, which $f^*$ defined by $f^*(q)=f(e^{sX}\cdot q)$ for each $q\in M$:
	\begin{equation}
		0=\Dsdd{f^*(e^{tX}\cdot p_0)}{t}{0}
		=\Dsdd{ f(e^{(s+t)X}\cdot p_0) }{t}{0}
		=\Dsdd{ f(e^{tX}\cdot p_0) }{t}{s}.
	\end{equation}
	Thus $f(e^{sX}\cdot p_0)$ is a constant with respect to $s$. Since $f$ is arbitrary, $e^{sX}\cdot p_0=p_0$ for any $s$. So $X\in\lH$ because $\exp sX\in H$ for any $s$. Then $\ker d\beta_e\subset \lH$.

	On the other hand, $\lH\subset\ker d\beta_e$ is clear, then
	\[
		\ker d\beta_e=\lH
	\]
	and $r_{\beta}=\dim\lG-\dim\lH$.

	Since $\alpha$ is an homeomorphism, the dimension of the origin and the target space are the same: $\dim G/H=\dim M$. On the other hand, lemma~\ref{lem:dim_G_H} gives $\dim G/H=\dim\lG-\dim\lH$, so that $r_{\beta}=\dim M$.

	Now we prove that $\alpha^{-1}$ is differentiable. Remark that $\beta(g)=g\cdot p_0$ and $\alpha([g])=g\cdot p_0=\beta(g)$ is a good definition for $\alpha$ because the class are taken with respect to the stabilizer of $p_0$. Since $r_{\beta}=\dim M$, the map $\beta$ is locally a diffeomorphism from a neighbourhood of $e$ to a neighbourhood of $p_0$.

	If $p=g\cdot p_0$, $\alpha^{-1}(p)=[g]$ because $[k]\in\alpha^{-1}(o)$ if $\alpha([k])=p$, i.e. $k\cdot p_0=p$. But $k=gr$ for a certain $r\in G$. It is clear that $p=k\cdot p_0=gr\cdot p_0$. In particular, $g\cdot(r\cdot p_0)$. We know that in general $g\cdot p=g\cdot q$ implies $p=q$; here it gives us $r\in H$, so that $k\in [g]$.

	We consider a $n\in G$ such that $n\cdot$ and $n^{-1}\cdot$ are diffeomorphic. We can make the following manipulation:
	\begin{equation}
		\alpha^{-1}(p)=[g]
		=[gnn^{-1}]
		=\pi(gn)\alpha^{-1}(n^{-1}\cdot p_0).
	\end{equation}
	Under this form, $\alpha^{-1}$ is diffeomorphic.


	\subdem{Second item}
	If $\alpha$ is an homeomorphism, then $\beta$ is open. Let us denote by $G_0$ the identity component of $G$. There exists a subset $\{x\bgamma\tq\gamma\in I\}$ of $G$ such that
	\[
		G=\bigcup_{\gamma\in I}G_0x\bgamma.
	\]
	This comes from the fact that the components are all some left translations of the identity component (this is true for any Lie group). Each orbit $G_0x\bgamma\cdot p_0$ is open in $M$ and two orbits are either disjoint either equals. Since $M$ is connected, all these orbits must coincide; thus each orbit contains the whole $M$. In particular, the orbit $G_0\cdot p_0=M$: $G_0$ is transitive on $M$.

\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Connected components}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}\label{lem:vp_G_X}
	Let $G$ be a connected Lie group with Lie algebra $\lG$. If $\dpt{\varphi}{G}{X}$ is an analytic homomorphism ($X$ is a Lie group with Lie algebra $\lX$), then

	\begin{enumerate}
		\item The kernel $\varphi^{-1}(e)$ is a topological Lie subgroup of $G$; his algebra is the kernel of $d\varphi_e$.
		\item The image $\varphi(G)$ is a Lie subgroup of $X$ whose Lie algebra is $d\varphi(\lG)\subset\lX$.
		\item The quotient group $G/\varphi^{-1}(e)$ with his canonical analytic structure is a Lie group. The map $g\varphi^{-1}(e)\mapsto\varphi(g)$ is an analytic isomorphism $G/\varphi^{-1}(e)\to\varphi(G)$. In particular the map $\dpt{\varphi}{G}{\varphi(G)}$ is analytic.
	\end{enumerate}
\end{lemma}

\begin{proof}
	\subdem{First item} We know that a subgroup $H$ closed in $G$ admits an unique analytic structure such that $H$ becomes a topological Lie subgroup of $G$. This is the case of $\varphi^{-1}(e)$. We know that $Z\in\lG$ belongs to the Lie algebra of $\varphi^{-1}(e)$ if and only if $\varphi(\exp tZ)=e$ for any $t\in\eR$. But $\varphi(\exp tZ)=\exp(td\varphi(Z))=e$ if and only if $d\varphi(Z)=0$.

	\subdem{Second item}
	Consider $X_1$, the analytic subgroup of $X$ whose Lie algebra is $d\varphi(\lG)$. The group $\varphi(G)$ is generated by the elements of the form $\varphi(\exp Z)$ for $Z\in\lG$. The group $X_1$ is generated by the $\exp(d\varphi Z)$. Because of lemma~\ref{lemsur5d}, these two are the same. Then $\varphi(G)=X_1$ and their Lie algebras are the same.

	\subdem{Third item}
	We consider $H$, a closed normal subgroup of $G$; this is a topological subgroup and the quotient $G/H$ has an unique analytic structure such that the map $G\times G/H\to G/H$, $(g,[x])\to [gx]$ is analytic. We consider a decomposition $\lG=\lH\oplus\lM$ and we looks at the restriction $\dpt{\psi}{\lM}{G}$ of the exponential. Then there exists a neighbourhood $U$ of $0$ in $\lM$ which is homomorphically send by  $\psi$ into an open neighbourhood of $e$ in $G$ and such that $\dpt{\pi}{G}{G/H}$ sends homomorphically $\psi(U)$ to a neighbourhood  of $p_0\in G/H$ (cf. lemma~\ref{lem:vois_U}).

	We consider $\UU$, the interior of $U$ and $B=\psi(\UU)$. The following diagram is commutative:
	\begin{equation}
		\xymatrix{
			G\times G/H  \ar[rr]^{\displaystyle\Phi}\ar[dr]_{\displaystyle \pi\times I} &&  G/H\\
			&     G/H\times G/H\ar[ur] _{\displaystyle\alpha}
		}
	\end{equation}
	with $\Phi(g,[x])=[g^{-1} x]$, $(\pi\times I)(g,[x])=([g],[x])$ and $\alpha([g],[x])=[g^{-1} x]$. Indeed,
	\[
		\alpha\circ(\pi\times I)(g,[x])=\alpha([g],[x])=[g^{-1} x].
	\]
	In order to see that $\alpha$ is well defined, remark that if $[h]=[g]$ and $[y]=[x]$ $[g^{-1} x]=[h^{-1} y]$ because $H$ is a normal subgroup of $G$.

	Now, we consider $g_0,x_0\in G$ and the restriction of $(\pi\times I)$ to $(g_0B)\times(G/H)$. Since $\pi$ is homeomorphic on $\psi(U)$ and $B=\psi(\UU)$, on $g_0B$, $\pi$ is a diffeomorphism (because the multiplication is diffeomorphic as well)

	\begin{probleme}\label{prob:diffeo_2}
		Why is the \( \pi\) a diffeomorphism? I understand why it is qn homeomorphism, but no more.
	\end{probleme}

	This diffeomorphism maps to a neighbourhood $N$ of $([g_0],[x_0])$ in $G/H\times G/H$. From the commutativity, we know that $\alpha=\Phi\circ(\pi\times I)^{-1}$, so that $\alpha$ is analytic. Consequently, $G/H$ is a Lie group. On $N$, $\alpha$ is analytic, then $\alpha(N)$ is analytic.

	All this is for a closed normal subgroup $H$ of $G$. Now we consider $H=\varphi^{-1}(e)$ and $\lH$, the Lie algebra of $H$. From the first item, we know that the Lie algebra of $H$ is the kernel of $d\varphi$: $\lH=d\varphi^{-1}(0)$ which is an ideal in $\lG$.

	From the second point, the Lie algebra of $G/H$ is $d\pi(\lG)$ which is isomorphic to $\lG/\lH$; the bijection is $\gamma(d\pi(X))=[X]\in\lG/\lH$. In order to prove the injectivity, let us consider $\gamma(A)=\gamma(B)$; $A=d\pi(X)$, $B=d\pi(Y)$. The condition is $[X]=[Y]$; thus it is clear that $d\pi(X)=d\pi(Y)$

	Let us consider on the other hand the map $Z+\lH\to d\varphi(Z)$ for $Z\in\lG$\footnote{Note that $\lG$ and $\lH$ are not groups; by $[X]$, we mean $[X]=\{ X+h\tq h\in\lH \}$.}. In other words, the map is $[Z]\to d\varphi(Z)$. This is an isomorphism $\lG/\lH\to d\varphi(\lG)$, which gives a local isomorphism between $G/H$ and $\varphi(G)$. This local isomorphism is $[g]\to\varphi(g)$ for $g$ in a certain neighbourhood of $e$ in $G$.

	Since $[g]\to\varphi(g)$ has a differential which is an isomorphism, this is analytic at $e$. Then it is analytic everywhere.

\end{proof}


\begin{corollary}
	If $G$ is a connected Lie group and if $Z$ is the center of $G$, then
	\begin{enumerate}
		\item $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(G)$, with kernel $Z$,
		\item the map $[g]\to\Ad_G(g)$ is an analytic isomorphism from $G/Z$ to $\Int(\lG)$ (the class $[g]$ is taken with respect to $Z$).
	\end{enumerate}
	\label{cor:Ad_homom}
\end{corollary}


\begin{proof}
	\subdem{First item}
	A connected Lie group is generated by a neighbourhood of identity, and any element of a suitable such neighbourhood can be written as the exponential of an element in the Lie algebra. So $\Int(\lG)$ is generated by elements of the form $\exp(\ad X)=\Ad(\exp X)$; this shows that $\Int(\lG)\subset\Ad(G)$. In order to find the kernel, we have to  see $\Ad_G^{-1}(e)$ by the formula
	\[
		e^{\Ad(g)X}=g e^Xg^{-1}.
	\]
	We have to find the $g\in G$ such that $\forall X\in\lG$, $\Ad_G(g)X=X$. We taking the exponential of the two sides and using \eqref{eq:sigma_X_sigma},
	\begin{equation}
		g e^Xg^{-1}=e^X.
	\end{equation}
	Then $g$ must commute with any $e^X\in G$: in other words, $g$ is in the kernel of $G$.

	\subdem{Second item}
	This is contained in lemma~\ref{lem:vp_G_X}. Indeed $G$ is connected and we had just proved that $\dpt{\Ad_G}{G}{\Int(\lG)}$ with kernel $Z$; the third item of lemma~\ref{lem:vp_G_X} makes $G/Z$ a Lie group and the map $[g]\to\Ad_G(g)$ an analytic isomorphism from $G/Z$ to $\Ad_G(G)=\Int(\lG)$.
\end{proof}

\begin{lemma}
	Let $G_1$ and $G_2$ be two locally isomorphic connected Lie groups with trivial center (i.e. $\lG_1=\lG_2=\lG$ and $Z(G_i)=\{ e \}$). In this case, we have $G_1=G_2=\Int(\lG)$ where $\Int\lG$ stands for the group of internal automorphism of $\lG$.
\end{lemma}

\begin{proof}
	We denote by $G_0$ the group $\Int\lG$. The adjoint actions $\Ad_i\colon G_i\to G_0$ are both surjective because of corollary~\ref{cor:Ad_homom}. Let us give an alternative proof for injectivity. Let $Z_i=\ker(\Ad_i)=\{ g\in G_i\tq\Ad(g)X=X,\,\forall X\in\lG \}$. Since $G_i$ is connected, it is generated by any neighbourhood of the identity in the sense of proposition~\ref{PropUssGpGenere}; let $V_0$ be such a neighbourhood. Taking eventually a subset we can suppose that $V_0$ is a normal coordinate system. So we have
	\[
		g\exp_{G_i}(X)g^{-1}=\exp_{g_i}(X)
	\]
	for every $X\in V_0$. Using proposition~\ref{PropUssGpGenere} we deduce that $gxg^{-1}=x$ for every $x\in G_i$, thus $g\in Z(G_i)$. That proves that $\ker(\Ad_i)\subset Z(G_i)$. The assumption of triviality of $Z(G_i)$ concludes injectivity of $\Ad_i$.
\end{proof}


\begin{corollary}       \label{CORooDBIGooTUplRL}
	Let $\lG$ be a real Lie algebra with center $\{0\}$. Then the center of $\Int(\lG)$ is only composed of the identity.
\end{corollary}

\begin{proof}
	We note $G'=\Int(\lG)$ and $Z$ his center; $\ad$ is the adjoint representation of $\lG$ and $\Ad'$, $\ad'$, the ones of $G'$ and $\ad(\lG)$ respectively. We consider the map $\dpt{\theta}{G'/Z}{\Int(\ad(\lG))}$, $\theta([g])=\Ad'(g)$. By the second item of the corollary~\ref{cor:Ad_homom}, $[g]\to\Ad_{G'}(g)$ is an analytic homomorphism from $G'$ to $\Int(\lG')$ where $\lG'$ is the Lie algebra of $G'$; this is $\ad(\lG)$. So $\dpt{\theta}{G'/Z}{\Int(\lG')}$ is isomorphic.

	Now we consider the map $\dpt{s}{\lG}{\ad(\lG)}$, $s(X)=\ad(X)$; this is an isomorphism. We also consider $\dpt{S}{G'}{ \GL(\ad(\lG))}$, $S(g)=s\circ g\circ s^{-1}$. The Lie algebra of $S(G')$ is $\ad(\lG')=\ad\big(\ad(\lG)\big)$. Then $S(G')$ is the subset of $\GL(\ad\lG)$ whose Lie algebra is $\ad\big(\ad\lG\big)$, i.e. exactly $\Int(\ad\lG)$. So $S$ is an isomorphism $\dpt{S}{G'}{\Int(\ad\lG)}$. From all this,
	\begin{equation}
		S(e^{\ad X})=s\circ e^{\ad X}\circ s^{-1}
		=e^{\ad'(\ad X)}
		=\Ad'(e^{\ad X}).
	\end{equation}
	With this equality, $\dpt{S^{-1}\circ\theta}{G'/Z}{G'}$ is an isomorphism which sends $[g]$ on $g$ for any $g\in Z$. Then $Z$ can't contains anything else than the identity.
\end{proof}

\begin{normaltext}
	The corollary \ref{CORooDBIGooTUplRL} does not hold i we relax the assumptions of the trivial center. We have a counter-example with $\lG=\eR^3$ and the commutations relation
	\[
		[X_1,X_2]=X_3,\quad [X_1,X_3]=[X_2,X_3]=0.
	\]
	The group $\Int(\lG)$ is abelian; then his center is the whole group, although $\lG$ is not abelian.
\end{normaltext}

\section{Adjoint group, inner automorphisms}\label{sec:adj_gp}
%--------------------------

Let $\lA$ be a \emph{real} Lie algebra. We denote by $GL(\lA)$\nomenclature[G]{$GL(\lA)$}{The group of nonsingular endomorphisms of $\lA$} the group of all the nonsingular endomorphisms of $\lA$: the linear and nondegenerate operators on $\lA$ as vector space. An element $\sigma\in\GL(\lA)$ does not specially fulfils somethings like $\sigma[X,Y]=[\sigma X,\sigma Y]$. The Lie algebra $\gl(\lA)$\nomenclature[G]{$\protect\gl(\lA)$}{space of endomorphisms with usual bracket} is the vector space of the endomorphisms (without non degeneracy condition) endowed with the usual bracket $(\ad A)B=[A,B]=A\circ B-B\circ A$. The map $X\to\ad X$ is a homomorphism from $\lA$ to the subalgebra $\ad(\lA)$ of $\gl(\lA)$.

The group $\Int(\lA)$\nomenclature[G]{$\Int(\lA)$}{Adjoint group of $\lA$} is the analytic Lie subgroup of $\GL(\lA)$ whose Lie algebra is $\ad(\lA)$ by theorem~\ref{tho:gp_alg}. This is the \defe{adjoint group}{adjoint!group}\index{group!adjoint} of $\lA$.

\begin{proposition}
	The group $\Aut(\lA)$\nomenclature[G]{$\Aut\lA$}{Group of automorphisms of $\lA$} of all the automorphisms of $\lA$ is a closed subgroup of $\GL(\lA)$.
\end{proposition}

\begin{proof}
	The property which distinguish the elements in $\Aut(\lA)$ from the ``commons'' elements of $\GL(\lA)$ is the preserving of structure: $\varphi[A,B]=[\varphi A,\varphi B]$. These are equalities, and we know that a subset of a manifold which is given by some equalities is closed.
\end{proof}

Now, theorem~\ref{THOooXVXBooZDJzQo} provides us an unique analytic structure on $\Aut(\lA)$ in which it is a topological Lie subgroup of $\GL(\lA)$. From now we only consider this structure. We denote by $\partial(\lA)$\nomenclature[G]{$\partial\lA$}{The Lie algebra of $\Aut(\lA)$} the Lie algebra of $\Aut(\lA)$: this is the set of the endomorphisms $D$ of $\lA$ such that $\forall t\in\eR$, $e^{tD}\in\Aut(\lA)$. By differencing the equality
\begin{equation}\label{eq:exp_der}
	e^{tD}[X,Y]=[e^{tD}X,e^{tD}Y]
\end{equation}
with respect to $t$, we see\footnote{As usual, if we consider a basis of $\lA$ as vector space, the expression in the right hand side of \[[e^{tD}X,e^{tD}Y]=\ad(e^{tD}X)e^{tD}X\] can be seen as a product matrix times vector, so that Leibniz works.} that $D$ is a derivation\footnote{Definition \ref{DEFooDUEUooZLhKdv}.} of \( \lA\)

Conversely, consider $D$, any derivation of $\lA$; by induction,
\begin{equation}
	D^k[X,Y]=\sum_{i+j=k}\frac{k!}{i!j!}[D^iX,D^jY]
\end{equation}
where by convention, $D^0$ is the identity in $\lA$. This relation shows that $D$ fulfils condition \eqref{eq:exp_der}, so that any derivation of $\lA$ lies in $\partial(\lA)$. Then
\[
	\partial(\lA)=\{\text{derivations of }\lA\}.
\]
The Jacobi identities show that
\[
	\ad(\lA)\subset\partial(\lA).    \label{pg:ad_subset_der}
\]
From this, we deduce\footnote{See error~\ref{err:Intt_Aut}}:
\begin{equation}\label{eq:int_sub_aut}
	\Int(\lA)\subset\Aut(\lA).
\end{equation}
Indeed the group $\Int(\lA)$ being connected, it is generated\footnote{See proposition~\ref{PropUssGpGenere}} by any neighbourhood of $e$; note that $\Aut(\lA)$ has not specially this property. We take a neighbourhood of $e$ in $\Int(\lA)$ under the form  $\exp V$  where $V$ is a sufficiently small neighbourhood of $0$ in $\ad(\lA)$ to be a neighbourhood of $0$ in $\partial(\lA)$ on which $\exp$ is a diffeomorphism. In this case, $\exp V\subset\Aut(\lA)$ and then $\Int(\lA)\subset\Aut(\lA)$.

Elements of $\ad(\lA)$ are the \defe{inner derivations}{derivation!inner} while the ones of $\Int(\lA)$ are the \defe{inner automorphisms.}{inner!automorphism}

Let $\mO$ be an open subset of $\Aut(\lA)$; for a certain open subset $U$ of $\GL(\lA)$, $\mO=U\cap\Aut(\lA)$. Then
\begin{equation}
	\iota^{-1}(\mO)=\mO\cap\Int(\lA)
	=U\cap\Aut(\lA)\cap\Int(\lA)
	=U\cap\Int(\lA).
\end{equation}

The subset $U\cap\Int(\lA)$ is open in $\Int(\lA)$ for the topology because $\Int(\lA)$ is a Lie\quext{Is it true??} subgroup of $\GL(\lA)$ and thus has at least the induced topology. This proves that the inclusion map $\dpt{\iota}{\Int(\lA)}{\Aut(\lA)}$ is continuous.

The lemma \ref{lem:var_cont_diff} and the consequence below makes $\Int(\lA)$ a Lie subgroup of $\Aut(\lA)$. Indeed $\Int(\lA)$ and $\Aut(\lA)$ are both submanifolds of $\GL(\lA)$ which satisfy \eqref{eq:int_sub_aut}.


By definition, $\Aut(\lA)$ has the induced topology from $\GL(\lA)$. Then $\Int(\lA)$ is a submanifold of $\Aut(\lA)$.
This is also a subgroup and a topological group : $\Int(\lA)$ is not a topological subgroup of $\Aut(\lA)$. Then $\Int(\lA)$ is a Lie subgroup of $\Aut(\lA)$. Schematically, links between $\Int\lG$, $\ad\lG$, $\Aut\lG$ and $\partial\lG$ are
\begin{subequations}\label{eq:schem_ad_int}
	\begin{align}
		\Int\lG & \longleftarrow\ad\lG        \\
		\Aut\lG & \longrightarrow\partial\lG.
	\end{align}
\end{subequations}
Remark that the sense of the arrows is important. By definition $\partial\lG$ is the Lie algebra of $\Aut\lG$, then there exist some algebras $\lG$ and $\lG'$ with $\Aut\lG\neq\Aut\lG'$ but with $\partial\lG=\partial\lG'$, because the equality of two Lie algebras doesn't implies the equality of the groups. The case of $\Int\lG$ and $\ad\lG$ is very different: the group is defined from the algebra, so that $\ad\lG=\ad\lG'$ implies $\Int\lG=\Int\lG'$ and $\Int\lG=\Int\lG'$ if and only if $\ad\lG=\ad\lG'$.

A result about the group of inner automorphism which will be useful later:

\begin{lemma}\label{lem:Int_g_gR}
	If $\lG$ is a complex semisimple Lie algebra, then $\Int\lG=\Int\lG\heR$.
\end{lemma}

\begin{proof}
	If $\{X_i\}$ is a basis of $\lG$, then $\{X_j,iX_j\}$ is a basis of $\lG\heR$. We define $\dpt{\psi}{\ad\lG}{\ad\lG\heR}$ by
	\[
		\psi(\ad(a^jX_j))=\ad(a^jX_j).
	\]
	It is clearly surjective. On the other hand, if $\ad(a^jX_j)\ad(b^kX_k)$ as elements of $\ad\lG\heR$, then they are equals as elements of $\ad\lG$. The discussion following equations \eqref{eq:schem_ad_int} finishes the proof.
\end{proof}

\begin{corollary}
	Any two real compact form of a complex semisimple Lie algebra are conjugate by an inner automorphism.
\end{corollary}

\begin{proof}
	We know that any real form of $\lG$ induces an involution (the conjugation) and that if the real form is compact, the involution is Cartan on $\lG\heR$. Let $\lU_0$ and $\lU_1$ be two compact real forms of $\lG$ and $\tau_0$, $\tau_1$ the associated involutions of $\lG$ (which are Cartan involutions of $\lG\heR$). For a suitable $\varphi\in\Int\lG\heR$,
	\[
		\tau_0=\varphi\tau_1\varphi^{-1}.
	\]
	The fact that $\Int\lG=\Int\lG\heR$ (lemma~\ref{lem:Int_g_gR}) finishes the proof.
\end{proof}

\begin{proposition}
	The group $\Int(\lA)$ is a normal subgroup of $\Aut(\lA)$.
\end{proposition}

\begin{proof}
	Let us consider a $s\in\Aut(\lA)$. The map $\dpt{\sigma_s}{\Aut(\lA)}{\Aut(\lA)}$, $\sigma_s(g)=sgs^{-1}$ is an automorphism of $\Aut(\lA)$. Indeed, consider $g$, $h\in\AutA$; direct computations show that $\sigma_s(gh)=\sigma_s(g)\sigma_s(h)$ and $[\sigma_s(g),\sigma_s(h)]=\sigma_s([g,h])$. From this, $(d\sigma_s)_e$ is an automorphism of $\partial(\lA)$, the Lie algebra of $\AutA$. For any $D\in\partial(\lA)$ we have
	\begin{equation}\label{eq:ad_s_2}
		(d\sigma_s)_eD=\Dsdd{ sD(t)s^{-1} }{t}{0}
		=sDs^{-1}.
	\end{equation}
	Since $s$ is an automorphism of $\lA$ and $\ad(\lA)$, a subalgebra of $\gl(\lA)$,
	\begin{equation}\label{eq:ad_s_1}
		s\ad Xs^{-1}=\ad(sX)
	\end{equation}
	for any $X\in\lA$, $s\in\Aut(\lA)$. Since $\ad(\lA)\subset\partial(\lA)$, we can write \eqref{eq:ad_s_2} with $D=\ad X$ and put it in \eqref{eq:ad_s_1}:
	\[
		(d\sigma)_e\ad X=s\ad Xs^{-1}=\ad(s\cdot X).
	\]
	We know from general theory of linear operators on vector spaces that if $A,B$ are endomorphism of a vector space and if $A^{-1}$ exists, then $Ae^BA^{-1}=e^{ABA^{-1}}$. We write it with $A=s$ and $B=\ad X$:
	\[
		\sigma_s\cdot e^{\ad X}=se^{\ad X}s^{-1}=e^{s\ad Xs^{-1}}=e^{\ad(s\cdot X)},
	\]
	sot that
	\begin{equation}\label{eq:sigma_aut_s}
		\sigma_s\cdot e^{\ad X}=e^{\ad(s X)}.
	\end{equation}

	Ont the other hand, we know that $\IntA$ is connected, so it is generated by elements of the form $e^{\ad X}$ for $X\in\lA$. Then $\IntA$ is a normal subgroup of $\AutA$; the automorphism $s$ of $\lA$ induces the isomorphism $g\to sgs^{-1}$ in $\IntA$ because of equation \eqref{eq:sigma_aut_s}.
\end{proof}

More generally, if $s$ is an isomorphism from a Lie algebra $\lA$ to a Lie algebra $\lB$, then the map $g\to sgs^{-1}$ is an isomorphism between $\AutA$ and $\AutB$ which sends $\IntA$ to $\IntB$. Indeed, consider an isomorphism $\dpt{s}{\lA}{\lB}$ and $g\in\AutA$. If $g\in\IntA$, we have to see that $sgs^{-1}\in\IntB$. By definition, $\IntA$ is the analytic subgroup of $\GL(\lA)$ which has $\ad(\lA)$ as Lie algebra. We have $g=e^{\ad A}$, then $sgs^{-1}=e^{\ad(sA)}$ which lies well in $\IntB$.

\begin{theorem}
	Let $G$ be a Lie group and $H$, a Lie subgroup of $G$.
	\begin{enumerate}
		\item If $H$ is a topological Lie subgroup of $G$, then it is closed in $G$,
		\item If $H$ has at most a countable number of connected components and is closed in $G$, then $H$ is a topological subgroup of $G$.
	\end{enumerate}
	\label{tho:H_ferme}
\end{theorem}

\begin{proof}
	\subdem{First point} It is sufficient to prove that if a sequence $h_n\in H$ converges (in $G$) to $g\in G$, then $g\in H$ (this is almost the definition of a closed subset). We consider $V$, a neighbourhood of $0$ in $\lG$ such that

	\begin{itemize}
		\item $\exp$ is diffeomorphic between $V$ and an open neighbourhood  of $e$ in $G$,
		\item $\exp(V\cap \lH)=(\exp V)\cap H$.
	\end{itemize}
	This exists by the lemma~\ref{LEMooOBIMooVvIDnb}; we can suppose that $V$ is bounded. Consider $\mU$, an open neighbourhood of $0$ in $\lG$ contained in $V$ such that $\exp-\mU\exp\mU\subset\exp V$.

	Since $h_n\to g$, there exists a $N\in\eN$ such that $n\geq N$ implies $h_n\in g\exp\mU$ (i.e $h_n$ is the product of $g$ by an element rather close to $e$; since the multiplication is differentiable, the notion of ``not so far''\ is good to express the convergence notion). From now we only consider such elements in the sequence. So, $h_N^{-1} h_n\in(\exp V)\cap H$ ($n\geq N$) because
	\[
		h_N^{-1} h_n\in\exp-\mU g^{-1} g\exp\mU\subset\exp V.
	\]
	(note that $H$ is a group, then $h_i^{-1}\in H$) From the second point of the definition of $V$, there exists a $X_n\in V\cap\lH$ such that $h^{-1}_N h_n=\exp X_n$ for any $n\geq N$.

	Since $V$ in bounded, there exists a subsequence out of $(X_i)$ (which is also called $X_i$) converging to a certain $Z\in\lG$. But $\lH$ is closed in $\lG$ because it is a vector subspace (we are in a finite dimensional case), then $Z\in\lH$ and thus the sequence $(h_i)$ converges to $h_N\exp Z$; therefore $g\in H$.

	\subdem{Second point} The subgroup $H$ is closed in $G$ and has a countable number of connected component. Since $H$ is closed, theorem~\ref{THOooXVXBooZDJzQo} it has an analytics structure for which it is a topological Lie subgroup of $G$. We denotes by $H'$ this Lie group.

	The identity map $\dpt{I}{H}{H'}$ is continuous\quext{pourquoi ?} (see error~\ref{err:gross}). Thus any connected component of $H$ is contained in a connected component of $H'$, the it has only a countable number of connected components. By corollary~\ref{CORooMCWWooXkpkNO}, $H=H'$ as Lie group.
\end{proof}

With the notations and the structure of theorem~\ref{THOooXVXBooZDJzQo}, the subgroup $H$ is discrete if and only if $\lH=\{0\}$. Indeed, recall the definition \eqref{eq:lH_de_G}:
\[
	\lH=\{X\in\lG: \forall t\in\eR, e^{tX}\in H\},
\]
and the fact that there exists a neighbourhood of $e$ in $H$ on which the exponential map is a diffeomorphism.

\begin{theorem}
	Let $G$ and $H$ be two Lie groups and $\dpt{\varphi}{G}{H}$ a continuous homomorphism. Then $\varphi$ is analytic.
\end{theorem}

\begin{proof}
	The Lie algebra of the product manifold $G\times H$ as $\lG\times\lH$ is given in~\ref{lemLeibniz}. We define
	\begin{equation}
		K=\{(g,\varphi(g)):g\in G\}\subset G\times H.
	\end{equation}
	It is clear that $K$ is closed in $G\times H$ because $G$ is closed and $\varphi$ is continuous.
	By theorem~\ref{THOooXVXBooZDJzQo}, there exists an unique differentiable structure on $G\times H$ such that $K$ is a topological Lie subgroup of $G\times H$ (i.e.: Lie subgroup\footnote{Definition \ref{DEFooGCHDooHUMSju}.} + induced topology). The Lie algebra of $K$ is
	\begin{equation}
		\lK=\{(X,Y)\in\lG\times\lH:\forall t\in\eR, (e^{tX},e^{tY})\in K\}.
	\end{equation}
	Let $N_0$ be an open neighbourhood of $0$ in $\lH$ such that $\exp$ is diffeomorphic between $N_0$ and an open neighbourhood $N_e$ of $e$ in $H$. We define $M_0$ and $M_e$ in the same way, for $G$ instead of $H$. We can suppose $\varphi(M_e)\subset N_e$: if it is not, we consider a smaller $M_e$: the openness of $N_e$ and the continuity of $\varphi$ make it coherent.

	The lemma~\ref{LEMooOBIMooVvIDnb} allow us to consider $M_0$ and $N_0$ small enough to say that
	\[
		\dpt{\exp}{(M_0\times N_0)\cap\lK}{(M_e\times N_e)\cap K}
	\]
	is diffeomorphic. Now, we are going to show that for any $X\in\lG$, there exists an unique $Y\in\lH$ such that $(X,Y)\in\lK$. The unicity is easy: consider $(X,Y_1),(X,Y_2)\in\lK$; then $(0,Y_1-Y_2)\in\lK$ (because a Lie algebra is a vector space). Then the definition of $\lK$ makes for any $t\in\eR$, $(e,\exp{t(Y_1-Y_2)})\in K$. Consequently, $\exp t(Y_1-Y_2)=\varphi(e)=e$ and then $Y_1-Y_2=0$.

	In order to show the existence, let us consider a $r>0$ such that $X_r=(1/r)X$ keeps in $M_0$. This exists because the sequence $X_r\to 0$ (then it comes $M_0$ from a certain $r$). From the definitions, $\exp$ is diffeomorphic between $M_0$ and $M_e$, then $\exp X_r\in M_e$ and $\varphi(\exp X_r)\in N_e$ because $\varphi(M_e)\subset N_e$.

	From this, there exists an unique $Y_r\in N_0$ such that $\exp Y_r=\varphi(\exp X_r)$ and an unique $Z_r\in(M_0\times N_0)\cap\lK$ satisfying  $\exp Z_r=(\exp X_r,\exp Y_r)$. But $\exp$ is bijective from $M_0\times N_0$, so that $Z_r=(X_r,Y_r)$ and we can choose $Y=rY_r$ as a $Y\in\lH$ such that $(X,Y)\in\lK$ (it is not really a choice: the unicity was previously shown). We denotes by $\dpt{\psi}{\lG}{\lH}$ the map which gives the unique $Y\in\lH$ associated with $X\in\lG$ such that $(X,Y)\in\lK$. This is a homomorphism between $\lG$ and $\lH$.

	By definition, $(X,\psi(X))\in\lK$, i.e. $(\exp tX,\exp t\psi(X))\in K$ or
	\begin{equation}
		\varphi(\exp tX)=\exp t\psi(X).
	\end{equation}
	Let us now consider a basis $\{X_1,\ldots,X_n\}$ of $\lG$. Since $\varphi$ is a homomorphism,
	\begin{equation}\label{eq:coord_vp_exp}
		\varphi\big((\exp t_1X_1)(\exp t_2X_2)\ldots(\exp t_nX_n)\big)
		=\big(\exp t_1\psi(X_1)\big)\ldots\big( \exp t_n\psi(X_n) \big)
	\end{equation}
	Now, we apply lemma~\ref{LEMooEBQUooKXkCda} on the decomposition of $\lG$ into the $n$ subspace spanned by the $n$ vector basis (this is $n$ applications of the lemma), the map
	\[
		(\exp t_1X_1)\ldots(\exp t_nX_n)\to (t_1,\ldots,t_n)
	\]
	is a coordinate system around $e$ in $G$. In this case, the relation \eqref{eq:coord_vp_exp} shows that $\varphi$ is differentiable at $e$. Then it is differentiable anywhere in $G$.
\end{proof}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Compact Lie algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{pg:compact_Lie}



We consider $\lG$, a real Lie algebra and $\lH$, a subalgebra of $\lG$. Let $K^*$ be the analytic subgroup of $\Int(\lG)$ which corresponds to the subalgebra $\ad_{\lG}(\lH)$ of $\ad_{\lG}(\lG)$.

\begin{definition}      \label{DEFooROMGooTLicyL}
	We say that $\lH$ is \defe{compactly embedded}{compactly embedded} in $\lG$ if $K^*$ is compact. A Lie algebra is \defe{compact}{compact!Lie algebra}\index{Lie!algebra!compact} when it is compactly embedded in itself.
\end{definition}

The analytic subgroup of $\Int(\lG)$ which corresponds to $\ad_{\lG}(\lG)$, by definition, is $\Int(\lG)$. Then the compactness of $\lG$ is the one of $\Int(\lG)$.

\begin{remark}
	The compactness notion on a Lie group is defined from the topological structure of the Lie group seen as a manifold. It is all but trivial that the compactness on a Lie group is related to the compactness on its Lie algebra; the proposition~\ref{prop:alg_grp_compact} will however make the two notions related in the natural way.
\end{remark}

\begin{remark}
	The topology on $K^*$ is not necessary the same as the induced one from $\Int(\lG)$ and $\Int(\lG)$ has also not necessary the induced topology from $\GL(\lG)$. However the next proposition will show that the compactness notion is well the one induced from $\GL(\lG)$.
\end{remark}

\begin{proposition}
	We consider $\tK$, the same set and group as $K^*$, but with the induced topology from $\GL(\lG)$. Then $\tK$ is compact if and only if $K^*$ is compact.
\end{proposition}

Note however that $K^*$ and $\tK$ are not automatically the same as manifold.

\begin{proof}
	\subdem{$K^*$ compact implies $\tK$ compact}
	The identity map $\dpt{\iota}{K^*}{\GL(\lG)}$ is analytic, and then is continuous because $\Int(\lG)$ is by definition an analytic subgroup of $\GL(\lG)$ and $K^*$ an analytic subgroup of $\Int(\lG)$. If we have a covering of $\tK$ with open set $\mO_i\cap\tK$ of $\tK$ ($\mO_i$ is open in $\GL(\lG)$), the continuity of $\iota$ make the finite subcovering of $K^*$ good for $\tK$.
	\subdem{$\tK$ compact implies $K^*$ compact}
	If $\tK$ is compact, then it is closed in $\GL(\lG)$. As set, $K^*$ is closed in $\GL(\lG)$ and by definition it is connected. Then by the theorem~\ref{tho:H_ferme}, $K^*$ is a topological subgroup of $\GL(\lG)$. Consequently, $K^*$ and $\tK$ are homeomorphic and they have same topology.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]
	If $G$ is a compact group in $\GL(n,\eR)$, then there exists a $G$-invariant quadratic form on $\eR^n$\quext{Is it true ? I've not even found a precise statement of this claim.}.
\end{lemma}

\section{Real Lie algebras}
%++++++++++++++++++++++++++

\subsection{Real and complex vector spaces}
%//////////////////////////////////////////////

If $V$ is a real vector space, the \defe{complexification}{complexification!of a vector space} of $V$ is the vector space\nomenclature{$V\heC$}{Complexification of $V$}
\[
	V\heC:=V\otimes\beR\eC.
\]
If $\{v_i\}$ is a basis of $V$ on $\eR$, then $\{v_i\otimes 1\}$ is a basis of $V\heC$ on $C$. Then
\[
	\dim_{\eR}V=\dim_{\eC}V\heC.
\]

Let $W$ be a complex vector space. If one restrains the scalars to $\eR$, we find a real vector space denoted by $W\heR$\nomenclature{$W\heR$}{Restriction of a complex vector spaces to $\eR$}. If $\{w_j\}$ is a basis of $W$, then $\{w_j,iw_j\}$ is a basis of $W\heR$ and
\[
	\dim\beC W=\frac{1}{2}\dim\beR W\heR.
\]
Note that $(V\heC)\heR=V\oplus iV$.

A real vector space $V$ is a \defe{real form}{real!form!of complex vector space} of a complex vector space $W$ if $W\heR=V\oplus iV$. If $V$ is a real form of $W$, the map $\dpt{\varphi}{V\heC}{V\heC}$ given by the identity on $V$ and the multiplication by $-1$ on $iV$ is the \defe{conjugation}{conjugation} of $V\heC$ with respect of the real form $V$.

\subsection{Real and complex Lie algebras}
%/////////////////////////////////////////

For notational convenience, if not otherwise mentioned, $\lG$ will denote a complex Lie algebra and $\lF$ a real one. If $\lF$ is a real Lie algebra and $\lF\heC=\lF\otimes\eC$, its complexification (as vector space), we endow $\lF\heC$ with a Lie algebra structure by defining
\[
	[ (X\otimes a),(Y\otimes b)  ]=[X,Y]\otimes ab.
\]
This is a bilinear extension of the Lie algebra bracket of $\lF$. It is rather easy to see that $[\lF,\lF]\heC=[\lF\heC,\lF\heC]$.

Now we turn our attention to the Killing form. Let $\lF$ be a real Lie algebra with a Killing form $B\blF$. A basis of $\lF$ is also a basis of $\lF\heC$. Then the matrix $B_{ij}=\tr( \ad X_i\circ\ad X_j )$ of the Killing form is the same for $\lF\heC$ than for $\lF$. In conclusion:
\[
	B_{\lF\heC}|_{\lF\times\lF}=B\blF.
\]

Let us study the inverse process: $\lG$ is a complex Lie algebra and $\lG\heR$ is the real Lie algebra obtained from $\lG$ by restriction of the scalars. If $\mB=\{v_j\}$ is a basis of $\lG$, $\mB'=\{v_j,iv_j\}$ is a one of $\lG\heR$. For a certain $X\in\lG$ we denote by $(c_{kl})$ the matrix of $\ad_{\lG}X$. Now we study the matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ by computing
\begin{equation}
	(\ad_{\lG}X)v_i=c_{ik}v_k
	=\big[ \real(c_{ik})+i\imag(c_{ik}) \big]v_k
	=a_{ik}v_k+b_{ik}(iv_k)
\end{equation}
if $a=\real c$ and $b=\imag c$. Then the columns of $\ad_{\lG\heR}$ which correspond to the $v_i\in\mB'$'s are given by
\[
	\ad_{\lG\heR}X=\begin{pmatrix}
		a & \cdot \\
		b & \cdot
	\end{pmatrix}
\]
where the dots denote some entries to be find now:
\begin{equation}
	(\ad_{\lG}X)(iv_i)=i\big(  a_{ik}v_k+b_{ik}(iv_k)  \big)\\
	=a_{ik}(iv_k)-b_{ik}v_k,
\end{equation}
so that the complete matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ is given by
\[
	\ad_{\lG\heR}X=\begin{pmatrix}
		a & -b \\
		b & a
	\end{pmatrix}.
\]
So,
\[
	\ad_{\lG\heR}X\circ\ad_{\lG\heR}X'=\begin{pmatrix}
		aa'-bb' & \cdot   \\
		\cdot   & aa'-bb'
	\end{pmatrix}.
\]
Then $B(X,X')=2\tr(aa'-bb')$ while
\begin{equation}
	B(X,Y)=\tr\big(  (a+ib)(a'+ib')  \big)\\
	=\tr(aa'-bb')+i\tr(ab'+ba').
\end{equation}
Thus we have
\begin{equation}
	B_{\lG\heR}=2\real B_{\lG},
\end{equation}
so that $\lG\heR$ is semisimple if and only if $\lG$ is semisimple.

\subsection{Split real form}
%//////////////////////////////

Let $\lG$ be a complex semisimple Lie algebra, $\lH$ a Cartan subalgebra, $\Phi$ the set roots, $\Delta$ the set of non zero roots and $B$, the Killing form. From property \eqref{eq:enuaiv} and the fact that $c(-\alpha,-\beta)=c(\alpha,\beta)$, we find $c(\alpha,\beta)^2=\frac{1}{2}\lbha(1+\lbba)|\alpha|^2$,
so that $c(\alpha,\beta)^2\geq 0$ which gives $c(\alpha,\beta)\in\eR$. We can define

\[
	\lGeR=\lH_0\bigoplus_{\alpha\in\Phi}\eR x_{\alpha}.
\]
Remark that $\lG_{\alpha}$ has dimension one with respect to $\eC$, not $\eR$; then $\eR x_{\alpha}\neq\lG_{\alpha}$, but $\eC x_{\alpha}=\lG_{\alpha}$ and $\lG_{\alpha}=\eR x_{\alpha}\oplus i\eR x_{\alpha}$. Since it is clear that $\bigoplus_{\alpha\in\Delta}( \eR x_{\alpha}\oplus i\eR x_{\alpha} )=\bigoplus_{\alpha\in\Delta}\lG_{\alpha}$, the proposition~\ref{prop:lHeR} gives
\begin{equation}
	\lG=\lGeR\oplus i\lGeR.
\end{equation}
Any real form of $\lG$ which contains the $\lHeR$ of a certain Cartan subalgebra $\lH$ of $\lG$ is said a \defe{split real form}{split!real form}. The construction shows that any complex semisimple Lie algebra admits a split real form.

\subsection{Compact real form}
%///////////////////////////////

\begin{definition}
	A \defe{compact real form}{compact!real form}\index{compact!Lie algebra} of a complex Lie algebra is a real form which is compact as Lie algebra\footnote{Definition \ref{DEFooROMGooTLicyL}.}.
\end{definition}

\begin{theorem}
	Any complex semisimple Lie algebra contains a compact real form.
\end{theorem}

\begin{proof}
	Let $\lH$ be a Cartan algebra of the complex semisimple Lie algebra $\lG$ and $ x_{\alpha}$, some root vectors. We consider the space
	\begin{equation}
		\lU_0=\underbrace{\sum_{\alpha\in\Phi}\eR ih_{\alpha}}_A+\underbrace{\sum_{\alpha\in\Phi}\eR( x_{\alpha}-\xbma)}_B+\underbrace{\sum_{\alpha\in\Phi}\eR i( x_{\alpha}+\xbma)}_C.
	\end{equation}
	Since $\lU_0\oplus i\lU_0$ contains all the $\eC h_{\alpha}$, $\lH\subset\lU_0\oplus i\lU_0$; it is also rather clear that $\lU_0$ is a real form of $\lG$ (as vector space), for example, $i\eR( x_{\alpha}-\xbma)+\eR i( x_{\alpha}+\xbma)=\eR i x_{\alpha}$. Now we have to check that $\lU_0$ is a real form of $\lG$ as Lie algebra, i.e. that $\lU_0$ is closed for the Lie bracket. This is a lot of computations:
	\[
		\begin{split}
			[i h_{\alpha},i\hbb]               &=0,\\
			[i h_{\alpha},( x_{\alpha}-\xbma)]        &=i(\alpha( h_{\alpha}) x_{\alpha}-(-\alpha)( h_{\alpha})\xbma)\\
			&=i\alpha( h_{\alpha})( x_{\alpha}+\xbma)\in C,\\
			[i h_{\alpha},i( x_{\alpha}+\xbma)]       &=-\alpha( h_{\alpha})( x_{\alpha}-\xbma)\in B,\\
			[( x_{\alpha}-\xbma),(\xbb-\xbmb)] &=c(\alpha,\beta)( x_{\alpha+\beta}-x_{-(\alpha+\beta)} )\in B\\
			&\quad -c(\alpha,\beta)(x_{\alpha-\beta}-x_{\beta-\alpha})\in B,\\
			[( x_{\alpha}-\xbma),i(\xbb+\xbmb)]&=ic(\alpha,\beta)(x_{\alpha+\beta}+x_{-(\alpha+\beta)})\in C\\
			&\quad +ic(\alpha,-\beta)(x_{\alpha-\beta)}+x_{-\alpha+\beta})\in C\\
			[i h_{\alpha},(\xbb-\xbmb)]     &=i\beta( h_{\alpha})(\xbb-\xbmb)\in C\\
			[i h_{\alpha},i(\xbb+\xbmb)]       &=-\beta( h_{\alpha})(\xbb-\xbmb)\in B\\
			[i( x_{\alpha}+\xbma),i(\xbb+\xbmb)]&=-c(\alpha,\beta)(x_{\alpha+\beta}-x_{-(\alpha+\beta)})\\
			&\quad -c'(\alpha,-\beta)(x_{\alpha-\beta}-x_{-\alpha+\beta}).
		\end{split}
	\]

	From proposition~\ref{prop:compact_Killing}, it just remains to prove that the Killing form of $\lU_0$ is strictly negative definite. We know that $B_{\lG}(\lG_{\alpha},\lG_{\beta})=0$ if $\alpha,\beta\in\Phi$ and $\alpha+\beta\neq 0$; then $A\perp B$ and $A\perp C$. It is a lot of computation to compute the Killing form; we know that $B$ is strictly positive definite on $\sum_{\alpha\in\Delta}\eR h_{\alpha}$ (and then strictly negative definite on $A$) a part this, the non zero elements are (recall that if $\alpha\neq 0$, $B( x_{\alpha}, x_{\alpha})=0$ from corollary~\ref{cor:Bxy_zero})
	\[
		\begin{split}
			B( ( x_{\alpha}-\xbma),( x_{\alpha}-\xbma) )&=-2B( x_{\alpha},\xbma)=-2\\
			B(i( x_{\alpha}+\xbma),i( x_{\alpha},\xbma))&=-2.
		\end{split}
	\]

	What we have in the matrix of $B_{\lG}|_{\lU_0\times\lU_0}$ is a negative definite block (corresponding to $A$), $-2$ on the rest of the diagonal and zero anywhere else. Then it is well negative definite and $\lU_0$ is a compact real from of $\lG$.
\end{proof}

\subsection{Involutions}
%//////////////////////////

Let $\lG$ be a (real or complex) Lie algebra. An automorphism $\dpt{\sigma}{\lG}{\lG}$ which is not the identity such that $\sigma^2$ is the identity is a \defe{involution}{involutive!automorphism}. An involution $\dpt{\theta}{\lF}{\lF}$ of a \emph{real} semisimple Lie algebra $\lF$ such that the quadratic form $B_{\theta}$ defined by
\[
	B_{\theta}(X,Y):=-B(X,\theta Y)
\]
is positive definite is a \defe{Cartan involution}{Cartan!involution}.

\begin{proposition}
	Let $\lG$ be a complex semisimple Lie algebra, $\lU_0$ a compact real form and $\tau$, the conjugation of $\lG$ with respect to $\lU_0$. Then $\tau$ is a Cartan involution of $\lG\heR$.
	\label{prop:conj_invol}
\end{proposition}

\begin{proof}
	From the assumptions, $\lG=\lU_0\oplus i\lU_0$, $\tau_{\lU_0}=id$ and $\tau_{i\lU_0}=-id$; then it is clear that $\tau_{\lG\heR}^2=id|_{\lG\heR}$. If $Z\in\lG$, we can decompose into $Z=X+iY$ with $X$, $Y\in\lU_0$. For $Z\neq 0$, we have
	\begin{equation}
		B_{\lG}(Z,\tau Z)=B_{\lG}(X+iY,X-iY)
		=B_{\lG}(X,X)+B_{\lG}(Y,Y)
		=B_{\lU_0}(X,X)+B_{\lU_0}(Y,Y)<0
	\end{equation}
	because $B$ restricts itself to $\lU_0$ which is compact. Then
	\begin{equation}
		(B_{\lG\heR})_{\tau}(Z,Z')=B_{\lG\heR}(Z,\tau Z)
		=-2\real B_{\lG}(Z,\tau Z')
	\end{equation}
	is positive definite because $(B_{\lG})_{\tau}$ is negative definite. Thus $\tau$ is a Cartan involution of $\lG\heR$.
\end{proof}

\begin{lemma}
	If $\varphi$ and $\psi$ are involutions of a vector space $V$ (we denote by $V_{\psi^+}$ and $V_{\psi^-}$ the subspaces of $V$ for the eigenvalues $1$ and $-1$ of $\psi$ and similarly for $\varphi$), then
	\[
		[\varphi,\psi]=0\quad\text{iff}\quad \left\{   \begin{aligned}
			V_{\varphi^+} & =(V_{\varphi^+}\cap V_{\psi^+})\oplus(V_{\varphi^+}\cap V_{\psi^-})  \\
			V_{\varphi^-} & =(V_{\varphi^-}\cap V_{\psi^+})\oplus(V_{\varphi^-}\cap V_{\psi^-}),
		\end{aligned}
		\right.
	\]
	i.e. if and only if the decomposition of $V$ with respect to $\varphi$ is ``compatible''{} with the one with respect to $\psi$.
	\label{lem:invol_compat}
\end{lemma}

\begin{proof}
	\subdem{Direct sense}
	Let us first see that $\varphi$ leaves the decomposition $V=V_{\psi^+}\oplus V_{\psi^-}$ invariant. If $x=x_{\psi^+}+x_{\psi^-}$,
	\[
		\varphi(x_{\psi^+})=(\varphi\circ\psi)(x_{\psi^+})=(\psi\circ\varphi)(x_{\psi^+}).
	\]
	Then $\varphi(x_{\psi^+})\in V_{\psi^+}$, and the matrix of $\varphi$ is block-diagonal with respect to the decomposition given by $\psi$. Thus $V_{\psi^+}$ and $V_{\psi^-}$ split separately into two parts with respect to $\varphi$.

	\subdem{Inverse sense}
	If $x\in V$, we can write $x=x_{++}+x_{+-}+x_{-+}+x_{--}$ where the first index refers to $\psi$ while the second one refers to $\psi$; for example, $x_{+-}\in V_{\psi^+}\cap V_{\varphi^-}$. The following computation is easy:
	\begin{equation}
		\begin{split}
			(\varphi\circ\psi)(x)&=\varphi(x_{++}+x_{+-}-x_{-+}-x_{--})\\
			&=x_{++}-x_{+-}-x_{-+}+x_{--}\\
			&=\psi(x_{++}-x_{+-}-x_{-+}-x_{--})\\
			&=(\psi\circ\varphi)(x).
		\end{split}
	\end{equation}
\end{proof}

\begin{corollary}\label{cor:Cartan_conj_inner}
	Any two Cartan involutions of a real semisimple Lie algebra are conjugate by an inner automorphism. \index{inner!automorphism}
\end{corollary}

\begin{proof}
	Let $\sigma$ and $\sigma'$ be two Cartan involutions of $\lF$. We can find a $\varphi\in\inf\lF$ such that $[\varphi\sigma\varphi^{-1},\sigma']=0$. Thus it is sufficient to prove that any two Cartan involutions which commute are equals. So let us consider $\theta$ and $\theta'$, two Cartan involutions such that $[\theta,\theta']=0$. By lemma~\ref{lem:invol_compat}, we know that the decompositions into $+1$ and $-1$  eigenspaces with respect to $\theta$ and $\theta'$ are compatibles. If we consider $X\in\lF$ such that $\theta X=X$ and $\theta' X=-1$ (it is always possible if $\theta\neq\theta'$), we have
	\[
		\begin{split}
			0<B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X)\\
			0<B_{\theta'}(X,X)=-B(X,\theta' X)=B(X,X)
		\end{split}
	\]
	which is impossible.
\end{proof}

\begin{theorem}
	Let $\lF$ be a real semisimple Lie algebra, $\theta$ a Cartan involution on $\lF$ and $\sigma$, another involution (not specially Cartan). Then there exists a $\varphi\in\Int\lF$ such that $[\varphi\theta\varphi^{-1},\sigma]=0$
	\label{tho:sigma_theta_un}
\end{theorem}

\begin{proof}
	If $\theta$ is a Cartan involution, then $B_{\theta}$ is a scalar product on $\lF$. Let $\omega=\sigma\theta$. By using $\sigma^2=\theta^2=1$, $\theta=\theta^{-1}$ and the invariance property~\ref{prop:auto_2} of the Killing form,
	\begin{equation}
		B(\omega X,\theta Y)=B(X,\omega^{-1}\theta Y)
		=B(X,\theta\sigma\theta Y)
		=B(X,\theta\omega Y).
	\end{equation}
	Then $B_{\theta}(\omega X,Y)=B_{\theta}(X,\omega Y)$. This is a general property of scalar product that in this case, the matrix of $\omega$ is symmetric while the one of $\omega^2$ is positive definite. If we consider the classical scalar product whose matrix is $(\delta_{ij})$, the property is written as $A_{ij}v_jw_j=v_iA_{ij}w_j$ (with sum over $i$ and $j$); this implies the symmetry of $A$. To see that $A^2$ is positive definite, we compute (using the symmetry):
	\[
		A_{ij}A_{jk}v_iv_k=v_iA_{ij}v_kA_{kj}=\sum_j(v_iA_{ij})^2>0.
	\]
	The next step is to see that there is an unique linear transformation $\dpt{A}{\lF}{\lF}$ such that $\omega^2=e^A$, and that for any $t\in\eR$, the transformation $e^{tA}$ is an automorphism of $\lF$.

	We choose an orthonormal (with respect to the inner product $B_{\theta}$) basis $\{X_1,\ldots,X_n\}$  of $\lF$ in which $\omega$ is diagonal. In this basis, $\omega^2$ is also diagonal and has positive real numbers on the diagonal; then the existence and unicity of $A$ is clear. Now we take some notations:
	\begin{subequations}
		\begin{align}
			\omega(X_i)   & =\lambda_iX_i \\
			\omega^2(X_i) & =e^{a_i}X_i,
		\end{align}
	\end{subequations}
	(no sum at all) where the $a_i$ are the diagonals elements of $A$. The structure constants are as usual defined by
	\begin{equation}
		[X_i,X_j]=c_{ij}^kX_k.
	\end{equation}
	Since $\sigma$ and $\theta$ are automorphisms, $\omega^2$ is also one. Then
	\[
		\omega^2[X_i,X_j]=c_{ij}^k\omega^2(X_k)=c_{ij}^ke^{a_k}X_k
	\]
	can also be computed as
	\[
		\omega^2[X_i,X_j]=[\omega^2X_i,\omega^2X_j]=e^{a_i}e^{a_j}c_{ij}^kX_k,
	\]
	so that $c_{ij}^ke^{a_k}=c_{ij}^ke^{a_i}e^{a_j}$, and then $\forall t\in\eR$,
	\[
		c_{ij}^ke^{ta_k}=c_{ij}^ke^{ta_i}e^{ta_j},
	\]
	which proves that $e^{tA}$ is an automorphism of $\lF$. By lemma~\ref{lem:autom_derr}, $A$ is thus a derivation of $\lF$. The semi-simplicity makes $\partial\lF=\ad\lF$, then $A\in\ad\lF$ and $e^{tA}\in\Int\lF$ because it clearly belongs to the identity component of $\Aut\lF$.

	Now we can finish de proof by some computations. Remark that $\omega=e^{A/2}$ and $[e^{tA},\omega]=0$ because it can be seen as a common matrix commutator. Since $\omega^{-1}=\theta\sigma$, we have $\theta\omega^{-1}\theta=\sigma\theta$, or $\theta\omega^2\theta=\omega^2$ and
	\begin{equation}\label{eq:eAth}
		e^{A}\theta=\theta e^{-A}.
	\end{equation}
	From this, one can deduce that $e^{tA}\theta=\theta e^{-tA}$. Indeed, as matricial identity, equation \eqref{eq:eAth} reads
	\[
		(e^{A}\theta)_{ik}=(e^{A})_{ij}\theta_{jk}
		=e^{a_i}\theta_{ik}
		=e^{-a_k}\theta_{ik}.
	\]
	Then for any $ik$ such that $\theta_{ik}\neq 0$, we find $e^{a_i}=e^{-a_k}$ and then also $e^{ta_i}=e^{-ta_k}$. Thus $(e^{tA}\theta)_{ik}=(e^{tA})_ij\theta_{jk}=e^{ta_i}\theta_{ik}=\theta_{ik}e^{-ta_k}=(\theta e^{-tA})_{ik}$. So we have
	\begin{equation}
		e^{tA}\theta=\theta e^{-tA}.
	\end{equation}
	Now we consider $\varphi=e^{A/4}\in\Int\lF$ and $\theta_1=\varphi\theta\varphi^{-1}$. We find $\theta_1\sigma=e^{A/2}\omega^{-1}$ and $\sigma\theta^{-1}=e^{-A/2}\omega$. Since $\omega^2=A$, we have $e^{A/2}=e^{-A/2}\omega^2$ and thus $\theta_1\sigma=\sigma\theta_1$.

\end{proof}

\begin{corollary}
	Every real Lie algebra has a Cartan involution.
\end{corollary}

\begin{proof}
	Let $\lF$ be a real Lie algebra and $\lG$ be his complexification: $\lG=\lF\heC$. Let $\lU_0$ be a compact real form of $\lG$ and $\tau$ the induced involution (the conjugation) on $\lG$. By the proposition~\ref{prop:conj_invol}, we know that $\tau$ is  a Cartan involution of $\lG\heR$. We also consider $\sigma$, the involution of $\lG$ with respect to the real form $\lF$. It is in particular an involution on the real Lie algebra $\lF$. Then one can find a $\varphi\in\Int\lG\heR$ such that $[\varphi\tau\varphi^{-1},\sigma]=0$ on $\lG\heR$. Let $\lU_1=\varphi\lU_0$ and $X\in\lU_1$. We can write $X=\varphi Y$ for a certain $Y\in\lU_0$. Then
	\[
		\varphi\tau\varphi^{-1} X=\varphi\tau Y=\varphi Y=X,
	\]
	so that $\varphi\tau\varphi^{-1}=id|_{\lU_1}$. Note that $\lU_1$ is also a real compact form of $\lG$ because the Killing form is not affected by $\varphi$. Let $\tau_1$ be the involution of $\lG$ induced by $\lU_1$. We have
	\[
		\tau_1|_{\lU_1}=\varphi\tau\varphi^{-1}_{\lU_1}=\id|_{\lU_1}.
	\]
	Since $\varphi$ is $\eC$-linear, we have in fact $\tau_1=\varphi\tau\varphi^{-1}$. Now we forget $\lU_0$ and we consider the compact real form $\lU_1$ with his involution $\tau_1$ of $\lG$ which satisfy $[\tau_1,\sigma]=0$ on $\lG\heR$ This relation holds also on $i\lG\heR$, then
	\[
		[\tau_1,\sigma]=0
	\]
	on $\lG=\lF\heC$. Let $X\in\lF$, i.e. $\sigma X=X$; it automatically fulfils
	\[
		\sigma\tau_1 X=\tau_1\sigma X=\tau_1 X,
	\]
	so that $\tau_1$ restrains to an involution on $\lF$ (because $\tau_1\lF\subset\lF$). Let $\theta=\tau_1|_{\lF}$. For $X$, $Y\in\lF$, we have
	\begin{equation}
		B_{\theta}(X,Y)=-B_{\lF}(X, \theta Y)
		=-B_{\lF}(X,\tau Y)
		=\frac{1}{2}(B_{\lG\heR})_{\tau_1}(X,Y),
	\end{equation}
	which shows that $\theta$ is a Cartan involution. The half factor on the last line comes from the fact that $\lG\heR=(\lF\heC)\heR=\lF\oplus i\lF$.

\end{proof}

\subsection{Cartan decomposition}
%-------------------------------

Examples of Cartan and Iwasawa decomposition are given in sections~\ref{SecToolSL},~\ref{SubSecIwaSOunn},\ref{subsecIwasawa_un},~\ref{SecSympleGp} and~\ref{SecIwasldeuxC}. An example of how it works to prove isomorphism of Lie algebras is provided in subsection~\ref{sssIsomsoslplussl}.

Let $\lF$ be a real semisimple Lie algebra. A vector space decomposition $\lF=\lK\oplus\lP$ is a \defe{Cartan decomposition}{Cartan!decomposition} if the Killing form is negative definite on $\lK$ and positive definite on $\lP$ and the following commutators hold:
\begin{equation}\label{eq:comm_Cartan}
	[\lK,\lK]\subseteq\lK,\quad[\lK,\lP]\subseteq\lP,\quad[\lP,\lP]\subseteq\lK.
\end{equation}
If $X\in\lK$ and $Y\in\lP$, we have $(\ad X\circ\ad Y)\lK\subseteq\lP$ and $(\ad X\circ\ad Y)\lP\subseteq\lK$, therefore $B_{\lF}(X,Y)=0$.

Let $\dpt{\theta}{\lF}{\lF}$ be a Cartan involution, $\lK$ its $+1$ eigenspace and $\lP$ his $-1$ one. It is easy to see that the relations \eqref{eq:comm_Cartan} are satisfied for the decomposition  $\lF=\lK\oplus\lP$. For example, for $X,X'\in\lK$, using the fact that $\theta$ is an automorphism,
\[
	[X,X']=[\theta X,\theta X']=\theta[X,X'],
\]
which proves that $[\lK,\lK]\subseteq\lK$. Since $\theta$ is a Cartan involution, $B_{\theta}$ is positive definite. For $X\in\lK$,
\[
	B(X,X)=B(X,\theta X)=-B_{\theta}(X,X)
\]
proves that $B$ is negative definite on $\lK$; in the same way we find that $B$ is also positive definite on $\lP$. Then the Cartan involution gives rise to a Cartan decomposition. We are going to prove that any Cartan decomposition defines a Cartan involution.

Let us now do the converse. Let $\lF=\lK\oplus\lP$ be a Cartan decomposition of the real semisimple Lie algebra $\lF$. We define $\theta=\id|_{\lK}\oplus(-\id)|_{\lP}$. If $X,X'\in\lK$, the definition of a Cartan algebra makes $[X,X']\in\lK$ and so
\[
	\theta[X,X']=[X,X']=[\theta X,\theta X'],
\]
and so on, we prove that $\theta$ is an automorphism of $\mF$. It remains to prove that $B_{\theta}$ is positive definite. If $X\in\lK$,
\[
	B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X).
\]
Then $B_{\theta}$ is positive definite on $\lK$ because on this space, $B$ is negative definite by definition of a Cartan involution. The same trick shows that $B_{\theta}$ is also positive definite on $\lP$. We had seen that $\lP$ and $\lK$ where $B_{\theta}$-orthogonal spaces. Thus $B_{\theta}$ is positive definite and $\theta$ is a Cartan involution.

Let $\lF=\lK\oplus\lP$ be a Cartan decomposition. Then it is quite easy to see that $\lK\oplus i\lP$ is a compact real form of $\lG=(\lFeC)$.

\begin{proposition}
	Let $\lL$ and $\lQ$ be the $+1$ and $-1$ eigenspaces of an involution $\sigma$. Then $\sigma$ is a Cartan involution if and only if $\lL\oplus i\lQ$ is  a compact real form of $\lFeC$.
\end{proposition}

\begin{proof}
	First remark that $\lL\oplus i\lQ$ is always a real form of $\lFeC$. The direct sense is yet done. Then we suppose that $B_{\lFeC}$ is negative definite on $\lL\oplus i\lQ$ and we have to show that $\lL\oplus\lQ$ is a Cartan decomposition of $\lF$. The condition about the brackets on $\lL$ and $\lQ$ is clear from their definitions. If $X\in\lL$, $B(X,X)<0$ because $B$ is negative definite on $\lL$. If $Y\in\lQ$, $B(Y,Y)=-B(iY,iY)>0$ because $B$ is negative definite on $i\lQ$.
\end{proof}

\section{Root spaces in the real case}
%----------------------------------------

Let $\lF$ be a real semisimple Lie algebra with a Cartan involution $\theta$ and the corresponding Cartan decomposition $\lF=\lK\oplus\lP$. We consider $B$, a ``Killing like''{} form, i.e. $B$ is a symmetric nondegenerate invariant bilinear form on $\lF$ such that $B(X,Y)=B(\theta X,\theta Y)$ and $B_{\theta}:=-B(X,\theta X)$ is positive definite. Then $B$ is negative definite on the compact real form $\lK\oplus i\lP$. Indeed if $Y\in\lP$,
\begin{equation}
	B(iY,iY)=-B(\theta Y,\theta Y)
	=B(Y,\theta Y)
	=-B_{\theta}(Y,Y)<0.
\end{equation}
The case with $X\in\lK$ is similar. It is easy to see that $B_{\theta}$ is in fact a scalar product on $\lF$, so that we can define the orthogonality and the adjoint from $B_{\theta}$. If $\dpt{A}{\lF}{\lF}$ is an operator on $\lF$, his adjoint is the operator $A^*$ given by the formula
\[
	B_{\theta}(A X,Y)=B_{\theta}(X,A^*Y)
\]
for all $X$, $Y\in\lF$.

\begin{proposition}
	With this definition, when $X\in\lF$, the adjoint operator of $\ad X$ is given by means of the Cartan involution:
	\[
		(\ad X)^*= \ad(\theta X).
	\]
\end{proposition}

\begin{proof}
	This is a simple computation
	\begin{equation}
		B_{\theta}\big(  (\ad\theta X)Y,Z \big)=-B\big(  Y,[\theta X,\theta Y]  \big)
		=-B_{\theta}(Y,[X,Z])
		=-B_{\theta}\big( (\ad X)^*Y,Z \big).
	\end{equation}
\end{proof}

Let $\lA$ be a maximal abelian subalgebra of $\lP$ (the existence comes from the finiteness of the dimensions). If $H\in\lA$, the operator $\ad H$ is self-adjoint because
\begin{equation}
	(\ad H)^*X=(-\ad\theta H)X
	=[H,X]
	=(\ad H)X,
\end{equation}
where we used the fact that $H\in\lP$.  For $\lambda\in\lA^*$, we define the space
\begin{equation}
	\lF_{\lambda}=\{ X\in\lF\tq\forall H\in\lA,\, (\ad H)X=\lambda(H)X\}.
\end{equation}
If $\lF_{\lambda}\neq 0$ and $\lambda\neq 0$, we say that $\lambda$ is a \defe{restricted root}{restricted root (real case)}\index{root!restricted (real case)} of $\lF$. We denote by $\Sigma$ the set of restricted roots of $\lF$. We may sometimes write $\Sigma_{\lF}$ if the Lie algebra is ambiguous.

The main properties of the real root spaces are given in the following proposition.

\begin{proposition}     \label{PropPropRacincesReelles}
	The set $\Sigma$ of the restricted roots of a real semisimple Lie algebra $\lF$ has the following properties:
	\label{prop:enuc}
	\begin{enumerate}
		\item\label{enuci} $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$,
		\item\label{enucii} $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$,
		\item\label{enuciii} $\theta\lF_{\lambda}=\lF_{-\lambda}$,
		\item\label{enuciv} $\lambda\in\Sigma$ implies $-\lambda\in\Sigma$,
		\item\label{enucv} $\lF_0=\lA\oplus\lM$ where $\lM=\mZ_{\lK}(\lA)$ and $\lA\perp\lM$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Proof of~\ref{enuci}. The operators $\ad H$ with $H\in\lA$ form an abelian algebra of self-adjoint operators, then they are simultaneously diagonalisable. Let $\{X_i\}$ be a basis which realize this diagonalisation, and $\lF_i=\Span X_i$, so that $\lF=\oplus_i\lF_i$. We have $(\ad H)\lF_i=\lF_i$ and then $(\ad H)X_i=\lambda_i(H)X_i$ for a certain $\lambda_i\in\lA^*$. This shows that $\lF_i\subseteq\lF_{\lambda_i}$.\quext{pourquoi ça n'implique pas que $\dim\lF_{\lambda_i}=1$? Réponse par Philippe: tu as oublié les valeurs propres nulles  dans ta base ce qui doit entrainer quelques modifs dans ton texte(par  ex.  $adH f_i = f_i$ pas toujours ) }

	Proof of~\ref{enucii}. Let $H\in\lA$, $X\in\lF_{\lambda}$ and $Y\in\lF_{\mu}$. We have
	\begin{equation}
		(\ad H)[X,Y]=[[H,X],Y]+[X,[H,Y]]
		=\big(  \lambda(H)+\mu(H) \big) [X,Y].
	\end{equation}

	Proof of~\ref{enuciii}. Using the fact that $\theta H=-H$ because $H\in\lP$,
	\begin{equation}
		(\ad H)\theta X=\theta[\theta H,X]
		=-\theta\lambda(H)X
		=-\lambda(H)(\theta X).
	\end{equation}

	Proof of~\ref{enuciv}. It is a consequence of~\ref{enuciii} because if $\lF_{\lambda}\neq 0$, then $\theta\lF_{_{\lambda}}\neq 0$.

	Proof of~\ref{enucv}. By~\ref{enuciii}, $\theta\lF_0=\lF_0$, then $\lF_0=(\lK\cap\lF_0)\oplus(\lP\cap\lF_0)$. If $X\in\lF_0$, then it commutes with all the elements of $\lA$ and by the maximality property of $\lA$, provided that $X\in\lP$, it also must belongs to $\lA$. This fact makes $\lA=\lP\cap\lF_0$. Now,
	\[
		\lM=\mZ_{\lK}(\lA)=\{X\in\lK\tq [X,\lA]=0\}=\lK\cap\lF_0.
	\]
	All this gives $\lF_0=\mZ_{\lK}(\lA)\oplus\lA$.
\end{proof}

We choose a positivity notion on $\lA^*$, we consider $\Sigma^+$, the set of restricted positive roots and we define\nomenclature{$\lN$}{Restricted roots}
\[
	\lN=\bigoplus_{\lambda\in\Sigma^+}\lF_{\lambda}.
\]

From finiteness of the dimension, there are only a finitely many forms $\lambda\in\lA^*$ such that $\lF_{\lambda}\neq 0$. Then, taking, more and more commutators in $\lN$, the formula $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$ shows that the result finish to fall into a $\lF_{\mu}=0$. On the other hand, since $\lA\subset\lF_0$, we have $[\lA,\lN]=\lN$. If $a_1,a_2\in\lA$ and $n_1,n_2\in\lN$,
\begin{equation}
	[a_1+n_1,a_2+n_2]=\underbrace{[a_1,a_2]}_{=0}+\underbrace{[a_1,n_2]}_{\in\lN}
	\quad+\underbrace{[n_1,a_2]}_{\in\lN}+\underbrace{[n_1,n_2]}_{\in\lN},
\end{equation}
then $[\lA\oplus\lN,\lA\oplus\lN]=\lN$. This proves the three following important properties:

\begin{enumerate}
	\item $\lN$ is nilpotent.
	\item $\lA$ is abelian.
	\item $\lA\oplus\lN$ is a solvable Lie subalgebra of $\lF$.
\end{enumerate}

\subsection{Iwasawa decomposition}
%----------------------------------

\begin{theorem}
	Let $\lF$ be a real semisimple Lie algebra and $\lK$, $\lA$, $\lN$ as before. Then we have the following direct sum:
	\begin{equation}
		\lF=\lK\oplus\lA\oplus\lN.
	\end{equation}
\end{theorem}

This is the \defe{Iwasawa decomposition}{Iwasawa!decomposition}\index{decomposition!Iwasawa} for the real semisimple Lie algebra $\lF$.

\begin{proof}
	We yet know the direct sum $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$. Roughly speaking, in $\lN$ we have only vectors of $\Sigma^+$, in $\theta\lN$, only of $\Sigma^-$ and in $\lA$, only in ``zero''. Then the sum $\lA\oplus\lN\oplus\theta\lN$ is direct.

	Now we prove that the sum $\lK+\lA+\lN$ is also direct. It is clear that $\lA\cap\lN=0$ because $\lA\subseteq\lF_0$. Let $X\in\lK\cap(\lA\oplus\lN)$. Then $\theta X=X$. But $\theta X\in\lA\oplus\theta\lN$. Thus $X\in\lA\oplus\lN\cap\lA\oplus\lN$ which implies $X\in\lA$. All this makes $X\in\lP\oplus\lK$ and $X=0$.

	Now we prove that $\lK\oplus\lA\oplus\lN=\lF$. An arbitrary $X\in\lF$ can be written as
	\[
		X=H+X_0+\sum_{\lambda\in\Sigma}X_{\lambda}
	\]
	where $H\in\lA$, $X_0\in\lM$ and $X_{\lambda}\in\lF_{\lambda}$. Now there are just some manipulations\ldots
	\begin{equation}
		\sum_{\lambda\in\Sigma}X_{\lambda}=\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+X_{\lambda})
		=\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})
		+\sum_{\lambda\in\Sigma^+}(X_{\lambda}+\theta X_{-\lambda}),
	\end{equation}
	but $\theta(X_{-\lambda}+\theta X_{-\lambda})=X_{-\lambda}+\theta X_{-\lambda}$, then $X_{-\lambda}+X_{-\lambda}\in\lK$. Moreover, $X_{\lambda}, \theta X_{-\lambda}\in\lF_{\lambda}$, then $X_{\lambda}-\theta X_{-\lambda}\in\lF_{\lambda}\subseteq\lN$. Then
	\begin{equation}
		X=X_0+\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})+H+\sum_{\lambda\in\Sigma^+}(X_{\lambda}-\theta X_{-\lambda})
	\end{equation}
	where the two first term belong to $\lK$, $H\in\lA$ and the last term belongs to $\lN$.
\end{proof}

\begin{lemma}
	There exists a basis $\{X_i\}$ of $\lF$ in which

	\begin{enumerate}
		\item\label{enudi} The matrices of $\ad\lK$ are symmetric,
		\item\label{enudii} The matrices of $\ad\lA$ are diagonal and real,
		\item\label{enudiii} The matrices of $\ad\lN$ are upper triangular with zeros on the diagonal.
	\end{enumerate}
\end{lemma}

\begin{proof}
	We have the orthogonal decomposition $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$ given by proposition~\ref{prop:enuc}. Let $\{X_i\}$ be an orthogonal basis of $\lF$ compatible with this decomposition and in such an order that $i<j$ implies $\lambda_i\geq\lambda_j$. From the orthogonality of the basis it follows that the matrix of $B_{\theta}$ is diagonal. Thus the adjoint is the transposition.

	\ref{enudi} If $X\in\lK$, $(\ad X)^t=(\ad X)^*=-\ad\theta X=-\ad X$.

	\ref{enudii} Each $X_i$ is a restricted root; then $(\ad H)X_i=\lambda_i(H)X_i$, then the diagonal of $\ad H$ is made of $\lambda_i(H)$ whose are real.

	\ref{enudiii} If $Y_i\in\lF_{\lambda_i}$ with $\lambda_i\in\Sigma^+$, $(\ad Y_i)X_j$ has only components in $\lF_{\lambda_i+\lambda_j}$ with $\lambda_i+\lambda_j>\lambda_j$ because $\lambda_i\in\Sigma^+$.
\end{proof}


\begin{lemma}
	Let $\lH$ be a subalgebra of the real semisimple Lie algebra $\lF$. Then $\lH$ is a Cartan subalgebra if and only if $\lHeC$ is Cartan in $\lFeC$.
\end{lemma}

\begin{proof}
	\subdem{Direct sense} If $\lH$ is nilpotent in $\lF$, it is cleat that $\lHeC$ is nilpotent in $\lFeC$. We have to prove that $[x,\lHeC]\subseteq\lHeC$ implies $x\in\lHeC$. As set, $\lFeC=\mF\oplus i\lF$  (but not as vector space!), then we can write $x=a+ib$ with $a$, $b\in\lF$. The assumption makes that for any $h\in\lH$, there exists $h',h''\in\lH$ such that
	\[
		[a+ib,h]=h+ih''.
	\]
	This equation can be decomposed in $\lF$-part and $i\lF$-part: for any $h\in\lH$, there exists a $h'\in\lH$ such that $[a,h]=h'$,  and for any $h\in\lH$, there exists a $h''\in\lH$ such that $[b,h]=h''$. Thus $a$, $b\in\lH$ because $\lH$ is Cartan in $\lF$.

	\subdem{Inverse sense} The assumption is that $[x,\lHeC]\subset\lHeC$ implies $x\in\lHeC$. In particular consider a $x\in\lH$ such that $[x,\lH]\subset\lH$. Then $x\in\lHeC$ because $[x,\lHeC]\subset\lHeC$. But $\lHeC\cap\lF=\lH$.
\end{proof}

In the complex case, the Cartan subalgebras all have same dimensions because they are maximal abelian.

\section{The group \texorpdfstring{$SL(2,\eR)$}{SL2R} and its algebra}  \label{SecToolSL}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The study of \( \SL(2,\eR)\) and \( \gsl(2,\eC)\) is required before to go further in the general study because of proposition~\ref{PropScalrooTsQ} that will reduce the study of genera Lie algebras into combinations of \( \gsl(2,\eC)\) algebras.

\subsection{Iwasawa decomposition}
%----------------------------------
\index{Iwasawa!decomposition!of $SL(2,\eR)$}

Let $G=\SL(2,\eR)$ the group of $2\times 2$ matrices with unit determinant. The Lie algebra $\lG=\gsl(2,\eR)$ is the algebra of matrices with vanishing trace:
\begin{equation}
	\lG =  \{ X\in\End(\eR^2)\tq \tr(X) = 0\}
	=\left\{ \begin{pmatrix}
		x & y  \\
		z & -x
	\end{pmatrix}\textrm{ with }x,y,z\in\eR  \right\}.
\end{equation}
The following elements will be intensively used:
\begin{equation}    \label{EqsXPdnZlG}
	H=\begin{pmatrix}
		1 & 0  \\
		0 & -1
	\end{pmatrix}
	,\quad
	E=\begin{pmatrix}
		0 & 1 \\
		0 & 0
	\end{pmatrix}
	,\quad
	F=\begin{pmatrix}
		0 & 0 \\
		1 & 0
	\end{pmatrix},
	\quad
	T=\begin{pmatrix}
		0  & 1 \\
		-1 & 0
	\end{pmatrix}
\end{equation}
where $T=E-F$ has been introduced for later convenience. The commutators are
\begin{subequations}\label{EqTableSLdR}
	\begin{align}
		[H,E] & =2E  & [T,H] & =-2T \\
		[H,F] & =-2F & [T,E] & =H   \\
		[E,F] & =H   & [T,F] & =H.
	\end{align}
\end{subequations}
The exponentials can be easily computed and the result is
\begin{align}               \label{EqExpMatrsSLdeuxR}
	e^{tH}=
	\begin{pmatrix}
		e^{t} & 0      \\
		0     & e^{-t}
	\end{pmatrix},
	 &  &
	e^{tE}=
	\begin{pmatrix}
		1 & t \\
		0 & 1
	\end{pmatrix},
	 &  &
	e^{tF}=
	\begin{pmatrix}
		1 & 0 \\
		t & 1
	\end{pmatrix}.
\end{align}
Notice that the sets $\{ H,E,F \}$, $\{ H,E,F \}$ and $\{ H,E+F,T \}$ are basis. A Cartan involution is given by $\theta(X)=-X^t$, and the corresponding Cartan decomposition is
\begin{align}
	\lK & =\Span\{ T \},
	    & \lP            & =\Span\{ H,E+F \}.
\end{align}
Indeed, we are in a matrix algebra, then $\tr(XY)$ is proportional to $\tr(\ad X\circ \ad Y)$.
In order to see that $\theta$ is a Cartan involution, we have to prove that $B|_{\lK\times\lK}$ is negative definite and $B|_{\lP\times\lP}$ positive. It is true because for $X\in\lK$,
\[
	\tr(\ad X\circ \ad X)=\tr(XX)=\tr\begin{pmatrix}
		-x^2 & 0    \\
		0    & -x^2
	\end{pmatrix}<0,
\]
and for $Y\in\lP$,
\[
	\tr(YY)=\tr\begin{pmatrix}
		x & y  \\
		y & -x
	\end{pmatrix}\begin{pmatrix}
		x & y  \\
		y & -x
	\end{pmatrix} =\tr\begin{pmatrix}
		x^2+y^2 & 0       \\
		0       & x^2+y^2
	\end{pmatrix} >0.
\]

Up to some choices, the Iwasawa decomposition\label{pg_iwasldr} of the group $\SL(2,\eR)$ is given by the exponentiation of $\lA$, $\lN$ and~$\lK$
\begin{equation}
	\begin{aligned}
		\lA & =\Span\{ H \}
		    & \lN           & =\Span\{ E \}
		    & \lK           & =\Span\{T\},
	\end{aligned}
\end{equation}
so that
\begin{equation}\label{eq:expo_ANK}
	A=\begin{pmatrix}
		e^a & 0      \\
		0   & e^{-a}
	\end{pmatrix}\quad
	N=\begin{pmatrix}
		1 & l \\
		0 & 1
	\end{pmatrix}\quad
	K=\begin{pmatrix}
		\cos k  & \sin k \\
		-\sin k & \cos k
	\end{pmatrix}.
\end{equation}

A common parametrization of $AN$ by $\eR^2$ is provided by
\begin{equation}   \label{EqParmalSL}
	(a,l)=
	\begin{pmatrix}
		e^a & le^a   \\
		0   & e^{-a}
	\end{pmatrix}.
\end{equation}
One immediately has the following formula for the left action of $AN$ on itself:
\[
	L_{(a,l)}(a',l')=\begin{pmatrix}
		e^{a+a'} & e^{a+a'}l'+e^{a-a'}l \\
		0        & e^{-a-a'}
	\end{pmatrix}=(a+a',l'+e^{-2a'}l).
\]
In this setting, the inverse is given by $(a,l)^{-1}=(-a,-l e^{2a})$.

Let's give some formulas in $\SL(2,\eR)$. Using corollary~\ref{Ad_e} and exponentiating commutation relations,
\begin{subequations}  \label{eq_eaHsldr}
	\begin{align}
		\Ad(e^{aH})E  & =e^{2a}E,                                            \\
		\Ad(e^{aH})F  & =e^{-2a}F,                                           \\
		\Ad( e^{aH})T & = e^{2a}E- e^{-2u}E+ e^{-2u}T                        \\
		\Ad(e^{tE})H  & =H-2tE,                            \label{eq_AdetE}  \\
		\Ad( e^{tE})T & =-tH+(t^2+1)E-E+T                                    \\
		\Ad( e^{tT})H & =\cos(2t)H+\sin(2t)(2E-T)                            \\
		\Ad( e^{tT})E & =\frac{ 1 }{2}\Big( \sin(2t)H+\cos(2t)(2E-T)+T \Big)
	\end{align}
\end{subequations}
where $z$ belongs to the center: $z=\pm\mtu$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{A companion: \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

We can consider the Iwasawa decomposition which is $\theta$-conjugated to the $AN$ that we just saw. That decomposition is generated by
\begin{equation}
	\bar\lN=\begin{pmatrix}
		0 & 0 \\
		1 & 0
	\end{pmatrix}.
\end{equation}
The exponentiation produces
\begin{equation}
	\bar N=\begin{pmatrix}
		1 & 0 \\
		t & 1
	\end{pmatrix},
\end{equation}
and the Iwasawa group is given by
\begin{equation}        \label{EqGeneANbarSLdeuxR}
	A\bar N=\begin{pmatrix}
		e^a      & 0      \\
		l e^{-a} & e^{-a}
	\end{pmatrix}.
\end{equation}

\subsection{Killing form}
%------------------------

In the basis $\{ H,E,T \}$, the adjoint operators are given by
\[
	\ad H=\begin{pmatrix}
		0 & 0 & -2 \\
		0 & 0 & 0  \\
		0 & 2 & 2
	\end{pmatrix},
	\ad E=\begin{pmatrix}
		0  & 0 & 0  \\
		0  & 0 & -1 \\
		-2 & 0 & 0
	\end{pmatrix},\textrm{ and }
	\ad T=\begin{pmatrix}
		2  & 0 & 0 \\
		0  & 1 & 0 \\
		-2 & 0 & 0
	\end{pmatrix}.
\]
so that the Killing form can be computed directly from definition $B(X,Y)=\tr(\ad X,\ad Y)$. The result is
\begin{subequations}
	\begin{align}
		B(T,H) & =0  & B(H,H) & =8   \\
		B(T,E) & =-4 & B(E,E) & =0   \\
		B(H,E) & =0  & B(T,T) & =-4.
	\end{align}
\end{subequations}
Expressed in the basis $\{H,E,F\}$, the matrix of the Killing form reads
\begin{equation}
	B=
	\begin{pmatrix}
		8 &   &   \\
		  &   & 4 \\
		  & 4 &
	\end{pmatrix}
\end{equation}
while, in the basis  $\{H,E+F,T\}$, we find
\begin{equation}   \label{EqBHEFTsldR}
	B=
	\begin{pmatrix}
		8         \\
		 & 8      \\
		 &   & -8
	\end{pmatrix}.
\end{equation}
The latter is the reason of the name of the vector $T$: the sign of its norm is different, so that $T$ is candidate to be a time-like direction.

\subsection{Abstract root space setting}
%---------------------------------------

Looking on the table \eqref{EqTableSLdR} from an abstract point of view, we see that $E$ and $F$ are eigenvectors of $\ad(H)$ with eigenvalues $2$ and $-2$. So $\lA=\lG_0=\eR H$; $\lG_2=\eR E$; and $\lG_{-2}=\eR F$. Using a more abstract notation, the table of $\SL(2,\eR)$ becomes
\begin{subequations}  \label{subeq_rootSLR}
	\begin{align}
		[A_{0},A_{2}]  & =2A_{2}   \\
		[A_{0},A_{-2}] & =-2A_{-2} \\
		[A_{2},A_{-2}] & =A_{0}.
	\end{align}
\end{subequations}

\subsection{Isomorphism}
%-----------------------

As pointed out in the chapter II, \S6 of \cite{Knapp_reprez}, the map (seen as a conjugation in $\SL(2,\eC)$)
\begin{equation}
	\begin{aligned}
		\psi\colon \SU(1,1) & \to \SL(2,\eR)   \\
		U                   & \mapsto AUA^{-1}
	\end{aligned}
\end{equation}
with $A=\begin{pmatrix}
		1 & i \\i&1
	\end{pmatrix}$ is an isomorphism between $\SL(2,\eR)$ and $\SU(1,1)$.

%---------------------------------------------------------------------------------------------------------------------------
\section{The complex algebra \texorpdfstring{$\protect\gsl(2,\eC)$}{sl2C} and its representations}
%---------------------------------------------------------------------------------------------------------------------------
\label{SecsldeuxCandrepres}

The book \cite{Kassel} contains the representations of \( \gsl(2,\eC)\).

The algebra $\gsl(2,\eC)$ is the complex algebra of complex $2\times 2$ matrices with vanishing trace. As generating matrices, one can take the elements $u_i$ of \eqref{EqGenssudeux} and complete them by
\begin{align*}
	v_1 & =\frac{ 1 }{2}
	\begin{pmatrix}
		-1 & 0 \\
		0  & 1
	\end{pmatrix},
	    & v_2            & =\frac{ 1 }{2}
	\begin{pmatrix}
		0  & i \\
		-i & 0
	\end{pmatrix},
	    & v_3            & =\frac{ 1 }{2}
	\begin{pmatrix}
		0  & -1 \\
		-1 & 0
	\end{pmatrix}
\end{align*}
which satisfy the commutation relations
\begin{subequations}
	\begin{align}
		[v_i,v_j] & =-\epsilon_{ikj}u_k \\
		[v_i,u_j] & =\epsilon_{ikj}v_k.
	\end{align}
\end{subequations}

\begin{remark}
	This is not the algebra \( \gsl(2,\eC)\) used in physics. The latter is the \emph{four}-dimensional \emph{real} algebra of trace vanishing \( 2\times 2\) complex matrices. There is one more generator and the representation theory is different. Moreover the physics works with the \emph{group} instead of the \emph{algebra}.
\end{remark}

The change of basis
\begin{align}
	x_j & =\frac{ 1 }{2}(u_j+iv_j), & y_j & =\frac{ 1 }{2}(u_j-iv_j)
\end{align}
provides the simplification
\begin{align}
	[x_i,x_j] & =\epsilon_{ijk}x_k & [y_i,y_j] & =\epsilon_{ijk}y_k & [x_i,y_j] & =0,
\end{align}
so that, as algebras, we have the isomorphism
\begin{equation}
	\gsl(2,\eC)=\gsu(2)\oplus\gsu(2).
\end{equation}
Thus the representation theory of $\gsl(2,\eC)$ is determined by the one of $\gsu(2)$.


\index{representation!of $\gsl(2,\eC))$}
Consider the space $\mP_m$ of homogeneous polynomials of degree $m$ in two variables with complex coefficients[\cite{GpAlgLie_Faraut}]. The dimension of $\mP_m$ is $m+1$ and we have the following representation of $\SL(2,\eC)$ thereon:
\begin{equation}
	\big( \pi_m(g)f \big)(u,v)=f\big(
	g
	\begin{pmatrix}
		u \\v
	\end{pmatrix}
	\big)
	=
	f(au+bv,cu+dv)
\end{equation}
if $g=\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}$. We are going to determine the corresponding representation $\rho_m$ of the Lie algebra $\gsl(2,\eC)$ as algebra over complex numbers.

A basis of $\gsl(2,\eC)$ over $\eC$ is given by the matrices $\{ H,E,F \}$ given in equation \eqref{EqsXPdnZlG} and are subject to the commutation relations \eqref{subEqsSBhuAWx}.




Using the exponentiation \eqref{EqExpMatrsSLdeuxR}, we find
\[
	\big( \pi_m( e^{tH})f \big)(u,v)=f( e^{t}u, e^{-t}v),
\]
so that
\begin{equation}
	\big( \rho_m(H)f \big)(u,v)=u\frac{ \partial f }{ \partial u }-v\frac{ \partial f }{ \partial v }.
\end{equation}
In the same way, we find
\begin{equation}
	\big( \rho_m(E)f \big)(u,v)=\Dsdd{ \big( \pi_m( e^{tE})f \big)(u,v) }{t}{0}=v\frac{ \partial f }{ \partial u },
\end{equation}
and
\begin{equation}
	\big( \rho_m(F)f \big)(u,v)=u \frac{ \partial f }{ \partial v }.
\end{equation}
A natural basis of $\mP_m$ is given by the monomials $f_j(u,v)=u^jv^{m-j}$ with $j=0,\ldots,m$. The representation $\rho_m$ on this basis reads
\begin{equation}        \label{EqReprezgsldeuxC}
	\begin{split}
		\rho_m(H)f_j&=(2j-m)f_j\\
		\rho_m(E)f_j&=(m-j)f_{j+1}\\
		\rho_m(F)f_j&=jf_{j-1}.
	\end{split}
\end{equation}

\begin{proposition}     \label{ProprhomirredsldeuxC}
	The representation $\rho_m$ is irreducible.
\end{proposition}

\begin{proof}
	Let $W\neq\{ 0 \}$ be an invariant subspace of $\mP_m$. If $p\in W$, from invariance, $\rho_m(H)(p)\in W$. If $p$ is a linear combination of $\{ f_j \}_{j\in I}$ ($I\subseteq \{ 0,\ldots m \}$), then $\rho_m(H)p$ is still a linear combination $q$ of elements in the same set. Thus there exists a linear combination of $p$ and $\rho_m(H)p$ which is a linear combination of $\{ f_j \}_{j\in J}$ with $J\subset I$ (strict inclusion). Using the same trick with $q$ and $\rho_m(H)q$, we still reduce the number of basis elements. Proceeding in the same way at most $m$ times, we find that one of the $f_j$ belongs to $W$. From there, acting with $\rho_m(E)$ and $\rho_m(F)$, one generates the whole $\mP_m$. That proves that $W=\mP_m$ and thus that $\rho_m$ is irreducible.
\end{proof}

\begin{theorem}[\cite{GpAlgLie_Faraut}]
	Every $\eC$-linear irreducible finite dimensional representation of $\gsl(2,\eC)$ is equivalent to one of the $\rho_m$.
\end{theorem}

These results will be proved also in the quantum case in theorems~\ref{ThoVfintemofdsldcun} and~\ref{ThoVfintemofdslddeux}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{The group \texorpdfstring{$\SO(3)$}{SO3} and its Lie algebra}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SubSecTheGroupSotrois}

\begin{proposition}[\cite{WormerAngular}]
	An element of $\SO(3)$ has exactly one eigenvector with eigenvalue $1$. That vector is the \defe{rotation axis}{axis!of rotation in $\SO(3)$}.
\end{proposition}

The generator of rotation around the axis $n$ (unit vector) is given by the matrix
\begin{equation}
	\begin{pmatrix}
		0    & -n_3 & n_2  \\
		n_3  & 0    & -n_1 \\
		-n_2 & n_1  & 0
	\end{pmatrix}.
\end{equation}
That form results form the requirement that $Nr=n\times r$. If we denote by $R(n,\theta)$ the operator of rotation in $\eR^3$ by an angle $\theta$ around the axis $n$, one shows that
\begin{equation}
	R(b,\theta)=\mtu+\sin(\theta) N+\big(1-\cos(\theta)\big)N^2.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Rotations of functions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Consider any function $f\colon \eR^3\to \eC$; we define the \defe{rotation operator}{rotation!on functions} $U(n,\theta)$\nomenclature{$U(n,\theta)$}{Rotation operator on functions} by
\begin{equation}        \label{EqRotFunSOtrois}
	\big( U(n\theta)f \big)(r)=f\big( R(n,\theta)^{-1}r \big).
\end{equation}
These operators form a group, and we have in particular that
\[
	U(n,\theta_1)U(n,\theta_2)=U(n,\theta_1+\theta_2).
\]
We are interested in \emph{infinitesimal} rotations, that is rotations of angle $d\theta$ for which $(d\theta)^2\ll d\theta$, or in other words, we are interested in a development of equation \eqref{EqRotFunSOtrois} restricted to linear terms in $\theta$. What one obtains is
\begin{equation}
	\big( U(n,d\theta)f \big)(r)=\big( (1-i d\theta\, n\cdot l)f\big)(r)
\end{equation}
where the operator $l$ is defined by
\begin{equation}
	l=-ir\times\nabla.
\end{equation}
Its components $l_i=-i\epsilon_{ijk}r_j\partial_k $ satisfy commutation relations
\begin{equation}    \label{EqAldllepsl}
	[l_i,l_j]=i\epsilon_{ijk}l_k.
\end{equation}
The operator $n\cdot l$ is refereed as the \defe{generator of infinitesimal rotations}{generator!of infinitesimal rotations}. One can derive an expression of $U(n,\theta)$ in terms of $n\cdot l$ by the following:
\[
	U(n,\theta+d\theta)f=U(n,\theta)U(n,d\theta)f=U(n,\theta)(1-id\theta\, n\cdot l)f,
\]
so that we have the differential equation
\begin{equation}
	\frac{ dU }{ d\theta }(n,\theta)=-iU(n,\theta)n\cdot l
\end{equation}
with the initial condition $U(n,0)=1$. The solution is
\begin{equation}
	U(n,\theta)= e^{-i\theta\, n\cdot l}.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Verma module}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

When $\lG$ is a semisimple Lie algebra, we have the usual decomposition\cite{VermaPiercey}
\begin{equation}
	\lG=\lN^-\oplus\lH\oplus\lN^+,
\end{equation}
where each of the three components are Lie algebras. In particular, the universal enveloping algebra $\mU(\lN^-)$ makes sense. Let $\mu\in\lH^*$. We build a representation $\pi_{\mu}$ of $\lG$ on $V_{\mu}=\mU(\lN^-)$ in the following way
\begin{itemize}
	\item If $Y_{\alpha}\in\lN^-$, we define
	      \begin{subequations}
		      \begin{align}
			      \pi_{\mu}(Y_{\alpha})1                     & =Y_{\alpha}                                 \\
			      \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_n}) & =Y_{\alpha}Y_{\alpha_1}\ldots Y_{\alpha_n},
		      \end{align}
	      \end{subequations}
	\item if $H\in\lH$, we define
	      \begin{subequations}
		      \begin{align}
			      \pi_{\mu}(H)1                              & =\mu(H)                                                                      \\
			      \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_k}) & = \big( \mu(H)-\sum_{j=1}^k\alpha_j(H) \big)Y_{\alpha_1}\ldots Y_{\alpha_k},
		      \end{align}
	      \end{subequations}
	\item and if $X_{\alpha}\in\lN^+$, we define
	      \begin{subequations}
		      \begin{align}
			      \pi_{\mu}(X_{\alpha})1                               & =0                                                                                               \\
			      \pi_{\mu}(X_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k} & =Y_{\alpha_1}\big( \pi_{\mu}(X_{\alpha})Y_{\alpha_2}\ldots Y_{\alpha_k} \big)                    \\
			                                                           & \quad  -\delta_{\alpha,\alpha_1}\sum_{j=1}^k\alpha_j(H_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k}.
		      \end{align}
	      \end{subequations}
\end{itemize}
In the last one, we do an inductive definition.
\begin{lemma}
	The couple $(\pi_{\mu},V_{\mu})$ is a representation of $\lG$ on $V_{\mu}$.
\end{lemma}
\begin{proof}
	No proof.
\end{proof}
That representation is one \defe{Verma module}{Verma module} for $\lG$. If the algebra $\lG$ is an algebra over the field $\eK$, the field $\eK$ itself is part of $\mU(\lN)^-$, so that the scalars are vectors of the representation. In that context, the multiplicative unit $1\in \eK$ is denoted by $v_0$.

\begin{theorem}
	The representation $(\pi_{\mu},V_{\mu})$ of the semisimple Lie algebra $\lG$ is a cyclic module of highest weight, with highest weight $\mu$ and where $v_0$ is a vector of weight $\mu$.
\end{theorem}
\begin{proof}
	No proof.
\end{proof}
The Verma module is, \emph{a priori}, infinite dimensional and non irreducible, thus one has to perform quotients of the Verma module in order to build finite dimensional irreducible representations.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Cyclic modules and representations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

An example over $\so(3)$ is given in subsection~\ref{subSubSecweightsotrois}. The case of $\so(5)$ is treated in subsection~\ref{SubSecsocinq}. Let $\lG$ be a semisimple Lie algebra with a Cartan subalgebra $\lH$ and a basis $\Delta$ for its roots $\Phi=\Phi^+\cup\Phi^-$. Let $W$ be a finite dimensional $\lG$-module.

\begin{lemma}
	If $\lG$ is a nilpotent complex algebra and if $\gamma$ is a weight, then there exists a $v$ in $V_{\gamma}$ such that $c\cdot v=\gamma(x)v$ for every $x\in\lG$.
\end{lemma}
This is the proposition~\ref{prop:trois_poids}. Notice that a Cartan algebra is nilpotent, thus one has at least one vector of $W$ which is a common eigenvector of every elements of $\lH$, in other words, $\exists\mu\in\lH^*$ and $\exists w\in W$ such that
\begin{equation}
	hw=\mu(h)w
\end{equation}
for every $h\in\lH$, and $w\neq 0$. If $w$ is such and if $x\in\lG_{\alpha}$, we have
\begin{equation}
	(hx)\cdot w=[h,x]\cdot w+(xh)\cdot w=\alpha(h)x\cdot w+x\mu(h)w=(\alpha+\mu)(h)x\cdot w.
\end{equation}
If we define
\begin{equation}
	S=\{ w\in W\tq\exists\mu\in\lH^*\tq hw=\mu(h)w \},
\end{equation}
this is not a vector space, but the vector space $\Span S$ generated by $S$ is invariant under $\lG$ because $S$ itself is invariant under all the $\lG_{\alpha}$ with $\alpha\in\lG^*$.

On the other hand, we suppose that $\lG$ and $W$ are finite dimensional, so that their dual are isomorphic. Since a Cartan subalgebra is chosen, we have the decomposition
\begin{equation}
	\lG=\lH\oplus_{\alpha\in\lH^*}\lG_{\alpha}
\end{equation}
where $\lG_{\alpha}=\{ x\in\lG\tq [h,x]=\alpha(h)x\,\forall g\in\lH \}$. When $\alpha\in\lH^*$, the two following spaces are independent of the choice of the Cartan subalgebra $\lH$:
\begin{equation}
	\begin{aligned}
		W_{\alpha}   & =\{ v\in W \tq hv=\alpha(h)v\,\forall h\in\lH \}         \\
		\lG_{\alpha} & =\{ x\in\lG    \tq [h,x]=\alpha(h)x\,\forall h\in\lH \}.
	\end{aligned}
\end{equation}
If $v_{\alpha}\in W_{\alpha}$ and $x_{\beta}\in\lG_{\beta}$, we have
\begin{equation}
	h(x_{\beta}v_{\alpha})=\big( [h,x_{\beta}]+x_{\beta}h \big)v_{\alpha}=\big( \beta(h)+\alpha(h) \big)x_{\beta} v_{\alpha},
\end{equation}
so $x_{\beta}v_{\alpha}\in W_{\alpha+\beta}$. Thus $x_{\beta}$ is a map
\begin{equation}
	x_{\alpha}\colon W_{\alpha}\to W_{\alpha+\beta}.
\end{equation}
Since $W$ is finite dimensional, there exists a maximal $\alpha$ such that $W_{\alpha}\neq0$. We name it $\lambda$. For every $\beta\in\Phi^+$, we have $W_{\lambda+\beta}=\{ 0 \}$. In particular, if $v_{\lambda}\in W_{\lambda}$,
\begin{equation}
	x_{\alpha}x_{\lambda}=0
\end{equation}
for every $\alpha\in\Phi^+$, and, of course,
\begin{equation}
	hv_{\lambda}=\lambda(h)v_{\lambda}.
\end{equation}
On the other hand, for every vector $v\in W$, and for $v_{\lambda}$ in particular, the space $\mU(\lG)v$ is invariant, so
\begin{equation}
	W=\mU(\lG)v_{\lambda}
\end{equation}
by irreducibility. One say that $W$ is the \defe{cyclic module}{cyclic!module} generated by $v_{\lambda}$.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Choice of basis}
%---------------------------------------------------------------------------------------------------------------------------



\begin{theorem}     \label{ThoBaseUGxxmono}
	Let $\lG$ be a Lia algebra on a field of characteristic zero. If $\{ x_i \}$ is an ordered basis of $\lG$, then
	\begin{equation}
		\{ x_{i_1}\cdots x_{i_n}\tq i_1\leq\ldots\leq i_n \}
	\end{equation}
	is a basis for the universal enveloping algebra $\mU(\lG)$ of $\lG$.
\end{theorem}
One can find a proof in \cite{DirkEnvFiniteDimNilLieAlg}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Roots and highest weight vectors}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropoIrrrgenffflamble}
	An irreducible cyclic module is generated by the elements of the form $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$.
\end{proposition}

\begin{proof}
	From theorem~\ref{ThoBaseUGxxmono}, the monomials of the form
	\begin{equation}
		(f_1^{i_1}\cdots f_m^{i_m})\cdot (h_1^{j_1}\cdots h_l^{j_l})\cdot (e_1^{k_1}\cdots e_m^{k_m})
	\end{equation}
	form a basis of $\mU(\lG)$. When one act with such an element on $v_{\lambda}$, the $e_i$ kill it, while the $h_i$ do not act (a part of changing the norm). Thus, in fact, the module $W$ is generated by the only elements $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$
\end{proof}
In very short, one can write
\begin{equation}        \label{EqWnmoinvlambldarootmodul}
	W=(\lN^-)^nv_{\lambda}.
\end{equation}
Since $f_kv_{\alpha}\in\lG_{\alpha-\alpha_k}$, we have
\begin{equation}        \label{Eqfmlaphamoinsmouns}
	f_1^{i_1}\cdot f_m^{i_m}v_{\lambda}\in\lG_{\lambda-(i_m\alpha_m-\ldots i_1\alpha_1)}.
\end{equation}
The set of roots is ordered by
\begin{equation}
	\begin{aligned}
		\mu_1 & \prec\mu_2 & \text{iff} &  & \mu_2-\mu_1 & =\sum_i k_i\alpha_i
	\end{aligned}
\end{equation}
with $\alpha_i>0$ and with $k_i\in\eN$. Equation \eqref{Eqfmlaphamoinsmouns} means that
\begin{equation}
	\mu\prec\lambda
\end{equation}
for every weight $\mu$ of $W$.

\begin{definition}
	Let $\lG$ be a finite dimensional Lia algebra. A \defe{cyclic module of highest weight}{module!highest weight} for $\lG$ is a module (not specially of finite dimension) in which there exists a vector $v_+$ such that $x_+v_+=0$ for every $x_+\in\lN^+$ and $hv_+=\lambda(h)v_+$ for every $h\in\lH$.
\end{definition}

\begin{proposition}
	Every submodule of a cyclic highest weight module is a direct sum of weight spaces.
\end{proposition}
\begin{proof}
	No proof.
\end{proof}

From the relation $x_+v_+=0$, we know that all the weight spaces satisfy $V_{\mu}$ satisfy $\mu\prec\lambda$, and, since a module is the sum of all its submodules,
\begin{equation}        \label{EqVsumValpha}
	V=\bigoplus V_{\mu}.
\end{equation}
Notice that if $v_+$ is in a submodule, then that submodule is the whole $V$, thus the sum of two proper submodules is a proper submodule. We conclude that $V$ has an unique maximal submodule, and has thus an unique irreducible quotient.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dominant weight}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDomiunSei}

We know that every representation is defined by a highest weight. The following proposition shows that every root cannot be a highest weight of an irreducible representation.

\begin{proposition}[\cite{Anupam}]
	The highest weight of an irreducible representation of a simple complex Lie algebra is an integral dominant weight.
\end{proposition}

\begin{proof}
	Let \( \alpha_i\) be a simple root and consider the corresponding copy of \( \gsl(2,\eC)\) generated by \( \{ e_i,f_i,h_i \}\) (see proposition~\ref{PropWEzZYzC}). The following part of \( L(\Lambda)\) is a \( \gsl(2,\eC)_i\)-module:
	\begin{equation}
		V(\alpha_i)=\bigoplus_{n\in\eZ}V_{\Lambda+n\alpha_i}=V_{\Lambda}\oplus V_{\Lambda-\alpha_i}\oplus V_{\Lambda-2\alpha_i}\oplus\ldots\oplus V_{\Lambda-r\alpha_i}
	\end{equation}
	for some positive integer \( r\). Notice that the sum over \( n\in\eZ\) does not contain terms with \( n<0\) because \( \Lambda\) being an highest weight, \( V_{\Lambda+k\alpha_i}=\emptyset\) when \( k>0\). We know that in a \( \gsl(2,\eC)\)-module the eigenvalues of \( h\) run from \( -m\) to \( m\) (see equations \eqref{EqReprezgsldeuxC} for example). Thus here
	\begin{equation}
		\Lambda(h_i)=-(\Lambda-r\alpha_i)(h_i).
	\end{equation}
	By construction \( \alpha_i(h_i)=2\), so \( \Lambda(h_i)=r\) and the proof is finished.
\end{proof}

\begin{proposition}
	If \( \Lambda\) is the highest weight of the representation \( L(\Lambda)\) of the complex simple Lie algebra \( \lG\) and if \( w_0\) is the longest elements of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{proposition}

\begin{proof}
	First remember that whenever \( \lambda\) is a weight of a representation and \( w\) is an element of the Weyl group, the root \( w\lambda\) is a weight\quext{To be proved.}; in particular \( w_0\Lambda\) is a weight of \( L(\Lambda)\).   Let \( v\in L(\Lambda)_{w_0\Lambda}\); we want to show that \( X_i^-v=0\).

	If \( X_i^-v\neq 0\), then \( w_0\Lambda-\alpha_i\) is a weight and \( w_0\big( w_0\Lambda-\alpha_i \big)=\Lambda-w_0\alpha_i\) is a weight too. Here we used the fact that \( w_0^2=\id\).
\end{proof}

\begin{probleme}
	Still to be shown:
	\begin{enumerate}
		\item
		      \( w\lambda\) is a weight
		\item
		      \( w_0^2=\id\)
	\end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Verma modules}
%---------------------------------------------------------------------------------------------------------------------------

Let us consider
\begin{equation}
	\lB=\lH\oplus\lN^+,
\end{equation}
and take $\alpha\in\lH^*$. Now, we define $\eC_{\alpha}$ as the vector space $\eC$ (one dimensional, generated by $z_+\in\eC$) equipped with the following action of $\lB$:
\begin{equation}
	\big( h+\sum_{\mu\prec 0}x_{\mu} \big)z_+=\alpha(h)z_+.
\end{equation}
The vector space $\eC_{\alpha}$ becomes a left $\mU(\lB)$-module. On the other hand, $\mU(\lG)$ is a free right $\mU(\lB)$-module because $\mU(\lB)\cup\mU(\lG)\subseteq\mU(\lG)$. As $\mU(\lB)$-module, a basis of $\mU(\lG)$ is given by $\lN^-$, i.e. by $\{ f_1^{i_1}\cdots f_m^{i_l} \}$. The \defe{Verma module}{Verma module} is the cyclic module
\begin{equation}
	\Verm(\alpha)=\mU(\lG)\otimes_{\mU(\lB)}\eC_{\alpha}
\end{equation}
which has a highest weight vector $v_{\lambda}=1\otimes z_+$. The tensor product over $\mU(\lB)$ beans that, when $X\in\mU(\lG)$, then
\begin{equation}
	\big( h+\sum_{\mu}x_{\mu} \big)X\otimes_{\mU(\lB)}zz_+=X\otimes\big( h+\sum_{\mu}x_{\mu} \big)zz_+=X\otimes_{\mU(\lG)}z\alpha(h)z_+=\alpha(h)X\otimes_{\mU(\lB)}zz_+.
\end{equation}
The Verma module is generated by $1\otimes z_+$ and the fact that
\begin{equation}
	zX(1\otimes z_+)=X\otimes zz_+.
\end{equation}

\begin{proposition}
	Two irreducible cyclic modules with same highest weight are isomorphic.
\end{proposition}

\begin{proof}
	Let $V$ and $W$ be two highest weight cyclic modules with highest weight $\lambda$ and highest weight vectors $v_{\lambda}$ and $w_{\lambda}$. In the module $V\oplus W$, the vector $v_{\lambda}\oplus w_{\lambda}$ is a highest weight vector of weight $\lambda$. Let us consider the module
	\begin{equation}
		Z=\mU(\lG)(v_{\lambda}\oplus w_{\lambda}).
	\end{equation}
	That module is a highest weight cyclic module. The projections onto $V=Z/W$ and $W=Z/V$ are non vanishing surjective homomorphisms, so $V$ and $W$ are irreducible quotients of $Z$. But we saw bellow equation \eqref{EqVsumValpha} that $Z$ can only accept one irreducible quotient. Thus $V$ and $W$ are isomorphic.
\end{proof}
We denote by $\Irr_{\lG}(\lambda)$\nomenclature{$\Irr_{\lG}(\lG)$}{the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.} the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.
