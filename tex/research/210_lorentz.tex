% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014, 2019-2022, 2024
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{The group \texorpdfstring{$SU(2)$}{SU2}}
%--------------------------------------------------

The group \( \SU(2)\) is already defined in \ref{DEFooVIQUooQbnYMu}.

\begin{proposition}[\cite{BIBooUXTFooXTeMOn,BIBooMXBZooFCLGYe}]       \label{PROPooZMPLooUFyAPW}
	The matrices of \( \SU(2)\) are
	\begin{equation}
		\SU(2)=\{ \begin{pmatrix}
			\alpha & -\bar \beta \\
			\beta  & \bar \alpha
		\end{pmatrix}\tq \alpha,\beta\in \eC,| \alpha |^2+| \beta |^2=1\}.
	\end{equation}
\end{proposition}

\begin{proof}
	We initiate with a matrix \( U=\begin{pmatrix}
		\alpha & \beta  \\
		\gamma & \delta
	\end{pmatrix}\in \eM(2,\eC)\). Then we impose the conditions. The unitary property gives:
	\begin{equation}
		UU^{\dag}=
		\begin{pmatrix}
			\alpha & \beta  \\
			\gamma & \delta
		\end{pmatrix}
		\begin{pmatrix}
			\oalpha & \ogamma \\
			\obeta  & \odelta
		\end{pmatrix}
		=
		\begin{pmatrix}
			\alpha\oalpha+\beta\obeta         & \alpha\bar \gamma+\beta\bar\delta \\
			\gamma\bar \alpha+\delta\bar\beta & \gamma\ogamma+\delta\odelta
		\end{pmatrix}
		\stackrel{!}{=}
		\begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix}.
	\end{equation}
	Among with the determinant conditions, we have the system
	\begin{subequations}        \label{SUBEQSooGUDNooOoxdSO}
		\begin{numcases}{}
			\alpha\delta-\gamma\beta=1\\
			| \alpha |^2+| \beta |^1=1\\
			| \gamma |^2+| \delta |^2=1\\
			\alpha\bar \gamma+\beta\bar\delta=0.        \label{SUBEQooSPRRooWjAUNi}
		\end{numcases}
	\end{subequations}
	We multiply \eqref{SUBEQooSPRRooWjAUNi} by \( \gamma\), and we substitute \( \gamma\bar \gamma=1-| \delta |^2\)  and \( \gamma\beta=\alpha\delta-1\). What we get is
	\begin{equation}
		\alpha(1-| \delta |^2)+(\alpha\delta-1)\bar \delta=0.
	\end{equation}
	If you develop the products, you see some simplifications and you remain with \( \delta=\bar \alpha\).

	Now we substitute \( \delta=\bar \alpha\) in \eqref{SUBEQooSPRRooWjAUNi} again. We obtain
	\begin{equation}        \label{EQooNOVWooYSTXqJ}
		\alpha(\bar \gamma+\beta)=0.
	\end{equation}
	There are two possibilities: \( \alpha=0\) or \( \alpha\neq 0\).
	\begin{subproof}
		\spitem[If \( \alpha\neq 0\)]
		In that case the equality \eqref{EQooNOVWooYSTXqJ} produces \( \gamma=-\bar\beta\) and the result is proved.
		\spitem[If \( \alpha=0\)]
		The system \eqref{SUBEQSooGUDNooOoxdSO} reduces to
		\begin{subequations}
			\begin{numcases}{}
				\gamma\beta=-1 \label{SUBEQooVCDOooTGejzi}\\
				| \beta |^2=1\\
				| \gamma |^2=1.     \label{SUBEQooWJJMooItqDsi}
			\end{numcases}
		\end{subequations}
		There exist \( \theta\in \eR\) such that \( \beta= e^{i\theta}\). The equation \eqref{SUBEQooVCDOooTGejzi} then shows that \( \gamma=- e^{-i\theta}\). The condition \eqref{SUBEQooWJJMooItqDsi} is automatically satisfied. At the end we have the matrix
		\begin{equation}
			U=\begin{pmatrix}
				0          & \beta \\
				-\bar\beta & 0
			\end{pmatrix}.
		\end{equation}
	\end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{\texorpdfstring{$ \SU(2)$}{SU(2)} as compact group}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooGLPQooKOfrjl}
	The group \( \SU(2)\) is compact in \( \GL(2)\).
\end{proposition}

\begin{proof}
	We consider the compact part
	\begin{equation}
		K=\{ (\alpha,\beta)\in \eC^2\tq | \alpha |^2+| \beta |^2=1 \}.
	\end{equation}

	The proposition \ref{PROPooZMPLooUFyAPW} shows that \( \SU(2)=f(K)\) with
	\begin{equation}
		\begin{aligned}
			f\colon K      & \to \SU(2)                   \\
			(\alpha,\beta) & \mapsto \begin{pmatrix}
				                         \alpha & -\bar\beta  \\
				                         \beta  & \bar \alpha
			                         \end{pmatrix}.
		\end{aligned}
	\end{equation}
	This map is continuous if we consider on \( \SU(2)\) the norm topology. Since the topology of \( \SU(2)\) as manifold is the one of the norm topology (proposition \ref{PROPooQJANooMaqLvV}\ref{ITEMooZWFDooWPiScd}), the map is continuous. As image of a compact by a continuous map, the manifold \( \SU(2)\) is compact.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pauli matrices}
%---------------------------------------------------------------------------------------------------------------------------

We denote by \( V\) the set of the hermitian traceless matrices. These are the elements \( u\in \eM(2,\eC)\) such that
\begin{subequations}
	\begin{align}
		u^{\dag}  & =u  \\
		\trace(u) & =0.
	\end{align}
\end{subequations}
This is not the Lie algebra \( \su(2)\) because the elements of \( \su(2)\) are anti-hermitian\footnote{Proposition \ref{PROPooSERWooFtxBgV}.}. This set \( V\) is a vector space over \( \eR\), but not over the field \( \eC\) because if \( u\in V\), then \( (iu)^{\dag}=-iu^{\dag}=-iu\neq iu\).

\begin{definition}      \label{DEFooRNTDooTVkPtB}
	We consider the following matrices:
	\begin{equation}
		\begin{aligned}[]
			\sigma_0=\begin{pmatrix}
				         1 & 0 \\
				         0 & 1
			         \end{pmatrix} &  &
			\sigma_1=\begin{pmatrix}
				         0 & 1 \\
				         1 & 0
			         \end{pmatrix}, &  &
			\sigma_2=\begin{pmatrix}
				         0 & -i \\
				         i & 0
			         \end{pmatrix}, &  &
			\sigma_3=\begin{pmatrix}
				         1 & 0  \\
				         0 & -1
			         \end{pmatrix}.
		\end{aligned}
	\end{equation}
	The \defe{Pauli matrices}{Pauli matrices} are the matrices \( \sigma_1\), \( \sigma_2\) and \( \sigma_3\).
\end{definition}
The matrix \( \sigma_0=\id\) is introduced for later use.

For the sake of notations, we write \( \sigma\) the vector of \( V^3\) given by \( \sigma=(\sigma_1, \sigma_2, \sigma_3)\). This allows us to write combinations like
\begin{equation}        \label{EQooXNRGooZaRQoZ}
	a\cdot \sigma=a_1\sigma_1+a_2\sigma_2+a_3\sigma_3
\end{equation}
when \( a\in \eR^3\). It must be noticed however that the notation «\( a\cdot \sigma\)» is not a scalar product. In particular, the formula \eqref{EQooXNRGooZaRQoZ} depends on the chosen basis \( \{ \sigma_i \}\) of \( V\) and \( \{ e_i \}\) on \( \eR^3\).

\begin{lemma}       \label{LEMooZNCQooLgoReX}
	The Pauli matrice form a basis\footnote{Definition \ref{DEFooNGDSooEDAwTh}.} of the real vector space \( V\) of hermitian traceless matrices.
\end{lemma}

\begin{proof}
	An element of \( V\) is a matrice of the form
	\begin{equation}
		u=\begin{pmatrix}
			a & b \\
			c & d
		\end{pmatrix}
	\end{equation}
	with \( a,b,c,d\in \eC\). The condition \( u=u^{\dag}\) imposes the relations \( a=\bar a\), \( d=\bar d\) and \( c=\bar b\), so that
	\begin{equation}
		u=\begin{pmatrix}
			x      & z \\
			\bar z & y
		\end{pmatrix}.
	\end{equation}
	The trace condition imposes \( x=-y\). Finally a general element of \( V\) has the form
	\begin{equation}
		u=\begin{pmatrix}
			x      & z  \\
			\bar z & -x
		\end{pmatrix}
	\end{equation}
	with \( x\in \eR\) and \( z\in \eC\). We have:
	\begin{equation}
		u=-\imag(z)\sigma_1+\real(z)\sigma_2+x\sigma_3.
	\end{equation}
	This proves that \( \{ \sigma_i \}_{i=1,2,3}\) spans \( V\).

	We still have to prove that \( \{ \sigma_i \}_{i=1,2,3}\) is free. For that, consider \( a,b,c\in \eR\) such that \( a\sigma_1+b\sigma_2+c\sigma_3=0\):
	\begin{equation}
		\begin{pmatrix}
			c    & a-bi \\
			a+bi & -c
		\end{pmatrix}=\begin{pmatrix}
			0 & 0 \\
			0 & 0
		\end{pmatrix}.
	\end{equation}
	We immediately deduce \( c=0\), \( a+bi=0\) and \( a-bi=0\). Thus \( a=b=c=0\).
\end{proof}

Notice that the hermitian matrices do not form a vector space over \( \eC\) because, if \( X\) is hermitian,
\begin{equation}
	(\lambda X)^{\dag}=\bar \lambda X^{\dag}=\bar \lambda X\neq \lambda X.
\end{equation}

\begin{lemma}
	The matrices \( i\sigma_k\) are unitary.
\end{lemma}

\begin{proof}
	A simple computation show that \( \sigma_k^2=\mtu\) and \( \sigma_k^{\dag}=\sigma_k\), so that
	\begin{equation}
		(i\sigma_k)(i\sigma_k)^{\dag}=(i\sigma_k)(-i\sigma_k)=\sigma_k^2=\mtu.
	\end{equation}
\end{proof}


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Some relations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{lemma}       \label{LEMooIBJMooTYnooZ}
	For every \( i,j=1,2,3\) we have the formula
	\begin{equation}
		\sigma_i\sigma_j=\delta_{ij}\mtu+i\sum_m\epsilon_{ijm}\sigma_m.
	\end{equation}
	In particular \( \sigma_i^2=\mtu\).
\end{lemma}

\begin{proof}
	Explicit matricial computation.
\end{proof}

\begin{lemma}       \label{LEMooJRWXooMkzRnk}
	We have the commutator\footnote{We are adult here; I believe you will not confuse the \( i\) of the index and the \( i\) of the imaginary numbers.}
	\begin{equation}
		[\sigma_i,\sigma_j]=2i\sum_k\epsilon_{ijk}\sigma_k.
	\end{equation}
\end{lemma}

\begin{proof}
	This is a computation using the lemma \ref{LEMooIBJMooTYnooZ}:
	\begin{equation}
		[\sigma_i,\sigma_j]=\delta_{ij}\mtu+\sum_ki\epsilon_{ijk}\sigma_k-\delta_{ji}\mtu-\sum_{k}i\epsilon_{jik}\sigma_k=2i\sum_k\epsilon_{ijk}\sigma_k
	\end{equation}
	where we used the fact that \( \epsilon_{jik}=-\epsilon_{ijk}\).
\end{proof}

\begin{lemma}       \label{LEMooLNCSooPHsVut}
	If \( a,b\in \eR^3\) we have
	\begin{equation}
		(a\cdot \sigma)(b\cdot \sigma)=(a\cdot b)\mtu+i(a\times b)\cdot \sigma.
	\end{equation}
\end{lemma}

\begin{proof}
	We use lemme \ref{LEMooIBJMooTYnooZ}:
	\begin{subequations}
		\begin{align}
			(a\cdot b)(b\cdot \sigma) & =\big( \sum_i a_i\sigma_i \big)(\sum_jb_j\sigma_j)              \\
			                          & =\sum_{ij}a_ib_j\sigma\sigma_j                                  \\
			                          & =\sum_{ij}a_ib_j(\delta_{ij}\mtu+\sum_mi\epsilon_{ijm}\sigma_m)
		\end{align}
	\end{subequations}
	Then we use the property \( \sum_{kl}a_kb_l\epsilon_{klm}=(a\times b)_m\).
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Isomorphism with \( \eR^3\)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The following lemme does not aims to provide a norm on \( V\). The norm on \( V\) is already the operator norm:
\begin{equation}
	\|X  \|=\sup_{| z |=1}| Xz |
\end{equation}
where \( z\in \eC\) and \( X\in V\). This is not something new.

In the same perspective, the elements of \( \SU(2)\) are normed to \( 1\) because, if \( U\in \SU(2)\),
\begin{equation}
	\| U \|_{\SU(2)}=\sup_{| z |=1}| Uz |=\sup\langle Uz, Uz\rangle =\sup_{| z=1 |}\langle U^{\dag}Uz, z\rangle =\sup_{| z |=1} \langle z, z\rangle=1.
\end{equation}

\begin{lemma}       \label{LEMooRFBTooIRDbEq}
	The map
	\begin{equation}
		\begin{aligned}
			\phi\colon \eR^3 & \to V                 \\
			a                & \mapsto a\cdot \sigma
		\end{aligned}
	\end{equation}
	is a vector space isomorphism and satisfies
	\begin{equation}
		\det\big( \phi(x) \big)=-\| x \|^2.
	\end{equation}
\end{lemma}

\begin{proof}
	Some immediate facts:
	\begin{itemize}
		\item \( \phi\) is linear,
		\item \( \phi\) is bijective because \( \{ \sigma_i \}_{i=1,2,3}\) is a basis (lemme \ref{LEMooZNCQooLgoReX}).
		\item Thus \( \phi\) is a vector space isomorphism.
	\end{itemize}
	The formula \( \det\big( \phi(x) \big)=-\| x \|^2\) is a computation:
	\begin{subequations}
		\begin{align}
			\det\big( \phi(x) \big) & =\det\begin{pmatrix}
				                               x_3      & x_1-ix_2 \\
				                               x_1+ix_2 & -x_3
			                               \end{pmatrix}     \\
			                        & =-x_3^2-(x_1-ix_2)(x_1+ix_2) \\
			                        & =-(x_1^2+x_2^2+x_3^2)        \\
			                        & =-\| x \|^2.
		\end{align}
	\end{subequations}
\end{proof}

Some more properties about the Pauli matrices and the map \( \phi\).
\begin{proposition}[\cite{ooJQZGooElGniq}]
	Let \( x,y\in \eR^3\). We have
	\begin{enumerate}
		\item       \label{ITEMooDDRNooGZASBN}
		      $[\sigma_i,\sigma_j]=2i\sum_k\epsilon_{ijk}\sigma_k$.
		\item       \label{ITEMooXORKooXFwQhR}
		      $\phi(x\times y)=\frac{1}{ 2i }[\phi(x),\phi(y)]$.
		\item       \label{ITEMooREMBooLPVnxz}
		      \( \tr\big( \phi(x)\phi(y) \big)=2x\cdot y\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Several points.

	\begin{subproof}
		\spitem[Formula \ref{ITEMooDDRNooGZASBN}]
		We use the product formula of lemma \ref{LEMooIBJMooTYnooZ}:
		\begin{equation}
			[\sigma_i,\sigma_j]=\sigma_i\sigma_j-\sigma_j\sigma_i=\delta_{ij}\id+i\sum_k\epsilon_{ijk}\sigma_k-\delta_{ij}\id-i\sum_k\epsilon_{jik}\sigma_k.
		\end{equation}
		Using the fact that \( \epsilon_{ijk}=-\epsilon_{jik}\) we get the result.
		\spitem[Formula \ref{ITEMooXORKooXFwQhR}]
		We have
		\begin{equation}
			[\phi(x),\phi(y)]=[x\cdot \sigma,y\cdot \sigma]=\sum_{ij}x_iy_j[\sigma_i,\sigma_j].
		\end{equation}
		Substituting the first result and using the formula \( \sum_{ij}x_iy_j\epsilon_{ijk}=(x\times y)_k\)\footnote{Definition \ref{DEFooTNTNooRjhuJZ}.} we get
		\begin{equation}
			[\phi(x),\phi(y)] = \sum_{ijk}x_iy_j2i\epsilon_{ijk}\sigma_k=2i\sum_k(x\times y)_k\sigma_k=2i(x\times y)\cdot \sigma=2i\phi(x\times y).
		\end{equation}
		\spitem[Formula \ref{ITEMooREMBooLPVnxz}]
		Using formula of lemma \ref{LEMooLNCSooPHsVut},
		\begin{subequations}
			\begin{align}
				\tr\big( \phi(x)\phi(y) \big) & =\tr\big( (x\cdot \sigma)(y\cdot \sigma) \big)                                             \\
				                              & =\sum_i\big[ (x\cdot \sigma)(y\cdot \sigma) \big]_{ii}                                     \\
				                              & =\tr\big( (x\cdot y)\mtu_2+i(x\times y)\cdot \sigma \big)      \label{SUBEQooRJKRooNrLhhV} \\
				                              & =(x\cdot y)\tr(\mtu_2)                                                                     \\
				                              & =2(x\cdot y).
			\end{align}
		\end{subequations}
		We used the fact that the Pauli matrice have vanishing trace, so that the second term in \eqref{SUBEQooRJKRooNrLhhV} is zero.
	\end{subproof}
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Path connection}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{proposition}     \label{PROPooLEKXooSXPhRX}
	The Lie groups \( \SO(3)\) and \( \SU(2)\) are path connected.
\end{proposition}

\begin{proof}
	Particular cases of theorem \ref{THOooYQFNooPaYmaP} and proposition \ref{PROPooTVHJooBRmUCd}.
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{One representation}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We still consider \( V\), the real vector space of hermitian matrices with vanishing trace. Thanks to the lemma \ref{LEMooRFBTooIRDbEq} we define the following norm on \( V\):
\begin{equation}
	\| X \|=\| \phi^{-1}(X) \|_{\eR^3}=-\det(X).
\end{equation}

For the sake of notational convenience in the proof of the next proposition, we introduce the maps \( L\) and \( R\).
\begin{lemma}       \label{LEMooQVYXooQFNaGc}
	The maps
	\begin{equation}
		\begin{aligned}
			L\colon \GL(V) & \to \End\big( \End(V) \big) \\
			L(A)X          & =AX
		\end{aligned}
	\end{equation}
	and
	\begin{equation}
		\begin{aligned}
			R\colon \GL(V) & \to \End\big( \End(V) \big) \\
			R(A)X          & =XA
		\end{aligned}
	\end{equation}
	are continuous.
\end{lemma}

\begin{proof}
	We prove our statement for \( L\). Let \( A_k\stackrel{\End(V)}{\longrightarrow}A\). We want to prove that
	\begin{equation}
		\| L(A_k)-L(A) \|_{\End\big( \End(V) \big)}\to 0.
	\end{equation}
	Using the definition of the operator norm\footnote{Definition \ref{DefNFYUooBZCPTr}.}, and the fact that it is an algebra norm (lemme \ref{LEMooFITMooBBBWGI}),
	\begin{subequations}
		\begin{align}
			\| L(A_k)-L(A) \| & =\sup_{X\in \End(V)}\frac{ \| L(A_k)X-L(A)X \| }{ \| X \| }      \\
			                  & =\sup_{X\in \End(V)}\frac{ \| (A_k-A)X \|_{\End(V)} }{ \| X \| } \\
			                  & \leq \sup_{X\in\End(V)}\| A_k-A \|                               \\
			                  & \to 0.
		\end{align}
	\end{subequations}
	Thus \( L\) is continuous by proposition \ref{PROPooJYOOooZWocoq}.
\end{proof}

\begin{proposition}     \label{PROPooRQUZooAoZzwx}
	We still consider \( V\), the real vector space of hermitian matrices\footnote{Definition \ref{DEFooOKGXooFCzCHu}.} with vanishing trace. Let
	\begin{equation}
		\begin{aligned}
			\rho\colon \SU(2) & \to \End(V)  \\
			\rho(U)X          & =UXU^{\dag}.
		\end{aligned}
	\end{equation}
	Then
	\begin{enumerate}
		\item           \label{ITEMooLZBSooZUQGgJ}
		      The map \( \rho\) is well defined: \( \rho(U)X\in V\) for every \( U\in \SU(2)\) and \( X\in V\).
		\item
		      The map \( \rho\) is a representation of \( \SU(2)\) on \( V\) by isometries,
		\item       \label{ITEMooBZUQooNXNVfs}
		      for each \( U\) the map \( \rho(U)\colon V\to V\) is continuous,
		\item       \label{ITEMooGHZYooQuabWb}
		      the map \( \rho\colon \SU(2)\to \End\big( \End(V)\big) \) is continuous.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Let us prove that \( \rho(U)X\in V\). First, using the properties of lemma \ref{LEMooJYGRooPTMZwY},
	\begin{equation}
		(UXU^{\dag})^{\dag}=(U^{\dag})^{\dag}X^{\dag}U^{\dag}=UXU^{\dag}.
	\end{equation}
	Then, with the cyclic invariance of the trace (lemma \ref{LEMooUXDRooWZbMVN}),
	\begin{equation}
		\tr(UXU^{\dag})=\tr(U^{\dag}UX)=\tr(X)=0.
	\end{equation}
	So \( UXU^{-1}=UXU^{\dag}\in V\).

	The fact that \( \rho(U)\) is linear is a small computation. It is a representation because
	\begin{equation}
		\rho(U_1)\rho(U_2)X=\rho(U_1)U_2XU_2^{\dag}=U_1U_2XU_2^{\dag}U_1^{\dag}=\rho(U_1U_2)X.
	\end{equation}

	For the isometry part, the determinant being multiplicative (proposition \ref{PROPooHQNPooIfPEDH}),
	\begin{equation}
		\| UXU^{\dag} \|=-\det(UXU^{\dag})=-\det(U)\det(X)\det(U^{\dag})=-| \det(U) |\det(X).
	\end{equation}
	Since \( U\in\SU(2)\) we have \( | \det(U) |=1\) and then \( \| \rho(U)X \|=\| X \|\).

	The last point to check is the continuity of \( \rho\colon \SU(2)\to \End(V)\). With the notations of lemma \ref{LEMooQVYXooQFNaGc} we have \( \rho(U)=L(U)\circ R(U^{\dag})\) while \( L(U)\) and \( R(U)\) are continuous\footnote{They are matrix multiplication.}. This is point \ref{ITEMooBZUQooNXNVfs} of continuity.

	The point \ref{ITEMooGHZYooQuabWb} of the continuity statement is more subtle. It is done in proposition \ref{PROPooJGNFooEwtNmJ}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Link with \texorpdfstring{$ \SO(3)$}{SO(3)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooYTTJooYpPYLT}]     \label{PROPooGEHAooPCReoU}
	The map
	\begin{equation}        \label{EQooSOZTooTIkONx}
		\begin{aligned}
			f\colon \SU(2) & \to \SO(3)                              \\
			U              & \mapsto \phi^{-1}\circ \rho(U)\circ\phi
		\end{aligned}
	\end{equation}
	where
	\begin{equation}
		\begin{aligned}
			\phi\colon \eR^3 & \to V                            \\
			x                & \mapsto \langle x, \sigma\rangle
		\end{aligned}
	\end{equation}
	is
	\begin{enumerate}
		\item
		      continuous,
		\item
		      a group homomorphism,
		\item       \label{ITEMooZSSHooDUCqSQ}
		      surjective,
		\item
		      \( \ker(f)=\{ \pm \mtu \}\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Several points.
	\begin{subproof}
		\spitem[Continuous]
		We know from proposition \ref{PROPooRQUZooAoZzwx} that \( U\mapsto \rho(U)\) is continuous. The inequality
		\begin{equation}
			\| \phi^{-1}\circ\rho(U_k)\circ \phi-\phi^{-1}\circ\rho(U)\circ\phi \|\leq \| \phi^{-1} \|\| \phi(U_k)-\rho(U) \|\| \phi \|
		\end{equation}
		shows that \( f\) is continuous too.
		\spitem[Group homomorphism]
		Let \( U_1,U_2\in \SU(2)\) and \( x\in \eR^3\). We have
		\begin{subequations}
			\begin{align}
				f(U_1U_2)x & =\phi^{-1}\big( U_1U_2\phi(x)U_2^{-1}U_1^{-1} \big)             \\
				           & =\phi^{-1}\Big( \rho(U_1)\big( U_2\phi(x)U_2^{-1} \big) \Big)   \\
				           & =\phi^{-1}\big( \rho(U_1)\circ\rho(U_2)\phi(x) \big)            \\
				           & =\big( \phi^{-1}\circ\rho(U_1)\circ\rho(U_2)\circ\phi \big)(x).
			\end{align}
		\end{subequations}
		We can suppress the dependency on \( x\) and continue:
		\begin{subequations}
			\begin{align}
				f(U_1U_2) & =\phi^{-1}\circ\rho(U_1)\circ\rho(U_2)\circ\phi                        \\
				          & =\phi^{-1}\circ\rho(U_1)\circ\phi\circ\phi^{-1}\circ\rho(U_2)\circ\phi \\
				          & =f(U_1)\circ f(U_2).
			\end{align}
		\end{subequations}
		Since \( \rho(\id)=\id\) we also have \( f(\id)=\id\). Thus \( f\) is a group homomorphism.
		\spitem[Surjective]
		Elements of \( \SO(3)\) are compositions of two reflexions by corollary \ref{CORooJCURooSRzSFb}. A generic element of \( \SO(3)\) has the form \(ST \) where \( S\) and \( T\) are reflexions. They have the form
		\begin{equation}
			\begin{aligned}
				S\colon \eR^3 & \to \eR^3                  \\
				x             & \mapsto x-2(x\cdot n_1)n_1
			\end{aligned}
		\end{equation}
		and
		\begin{equation}
			\begin{aligned}
				T\colon \eR^3 & \to \eR^3                  \\
				x             & \mapsto x-2(x\cdot n_2)n_2
			\end{aligned}
		\end{equation}
		with \( \| n_1 \|=\| n_2 \|=1\).

		Let \( M= \phi(n_1) =n_1\cdot \sigma\) and \( Q=\phi(n_2)=n_2\cdot \sigma\). We will prove that \( MQ\in \SU(2)\) and \( f(MQ)=S\circ T\).

		\begin{subproof}
			\spitem[\( M^2=\mtu\)]
			We have
			\begin{equation}
				M^2=(n_1\cdot n_1)\mtu_2+i(n_1\times n_1)\cdot \sigma=\mtu_2.
			\end{equation}
			\spitem[\( \det(M)=-1\)]
			We know from lemma \ref{LEMooRFBTooIRDbEq} that \( \det(M)=\det\big( \phi(n_1) \big)=-\| n_1 \|^2=-1\).
			\spitem[\( MQ\in \SU(2)\)]
			First, \( \det(MQ)=\det(M)\det(Q)=-1\). Second, since \( M\) and \( Q\) are hermitian, \( (MQ)^{\dag}=Q^{\dag}M^{\dag}=QM\) and then
			\begin{equation}
				(MQ)^{\dag}(MQ)=QMMQ=\mtu_2
			\end{equation}
			because \( M^2=Q^2=1\).
			\spitem[\( \phi(Sx)=-M\phi(x)M\)]
			Let \( x\in \eR^3\). We have
			\begin{subequations}        \label{EQooSHKEooAhOxfH}
				\begin{align}
					\phi(Sx) & =\phi\big( x-2(x\cdot n_1)n_1 \big) \\
					         & =\phi(x)-2(x\cdot n_1)\phi(n_1)     \\
					         & =\phi(x)-2(x\cdot n_1)M.
				\end{align}
			\end{subequations}
			Using the formula \( (a\cdot \sigma)(b\cdot \sigma)=(a\cdot b)\mtu+i(a\times b)\cdot \sigma\) we have
			\begin{equation}
				\phi(x)M=\phi(x)\phi(n_1)=(x\cdot n_1)\mtu+i(x\times n_1)\cdot \sigma
			\end{equation}
			and
			\begin{equation}
				M\phi(x)=(n_1\cdot \sigma)(x\cdot \sigma)=(n_1\cdot x)\mtu+i(n_1\times x)\cdot \sigma,
			\end{equation}
			si that
			\begin{equation}
				\phi(x)M+M\phi(x)=2(x\cdot n_1)\mtu.
			\end{equation}
			Multiplying that by \( M\) and using \( M^2=\mtu\) we deduce
			\begin{equation}
				\phi(x)=2(x\cdot n_1)M-M\phi(x)M.
			\end{equation}
			Now we substitute that into \eqref{EQooSHKEooAhOxfH} in order to see that
			\begin{equation}
				\phi(Sx)=-M\phi(x)M.
			\end{equation}
			\spitem[Conclusion (surjective)]
			We can now compute the action of \( f(MQ)\) on \( x\in \eR^3\):
			\begin{subequations}
				\begin{align}
					f(MQ)x & =\big( \phi^{-1}\circ\rho(MQ)\circ\phi \big)x \\
					       & =\phi^{-1}\big( MQ\phi(x)QM \big)             \\
					       & =\phi^{-1}\big( M\phi(Tx)M \big)              \\
					       & =\phi^{-1}\big( \phi(STx) \big)               \\
					       & =STx.
				\end{align}
			\end{subequations}
			So we have \( f(MQ)=ST\) and \( f\) is surjective.
		\end{subproof}
		\spitem[Kernel]
		A less technological proof will be given for fun in the lemma \ref{LEMooQKCQooTIaesa}.
		Let \( U\in \SU(2)\) be such that \( f(U)=\mtu_3\in \SO(3)\). For every \( x\in \eR^3\) we have \( x=f(U)x\) while
		\begin{equation}
			f(U)x=\phi^{-1}\big( U\phi(x)U^{-1} \big).
		\end{equation}
		We conclude that \( U\phi(x)U^{-1}=\phi(x)\) for every \( x\in \eR^3\). Since \( f\) is surjective on the vector space \( V\) of hermitian matrices with vanishing trace, we have
		\begin{equation}
			UXU^{\dag}=X
		\end{equation}
		for every \( X\in V\). In particular \( UX=XU\). Since the matricial product is continuous, we can commute \( U\) and the infinite sum and get
		\begin{equation}
			U\sum_{k=0}^{\infty}\frac{ (iX)^k }{ k! }=\lim_{N\to \infty} \sum_{k=0}^N\frac{ U(iX)^k }{ k! }=\lim_{N\to \infty} \sum_{k=0}^N\frac{ (iX)^kU }{ k! }= e^{iX}U.
		\end{equation}
		So we have \( [U, e^{iX}]=0\) for every \( X\in V\). Since proposition \ref{PROPooZBJSooEIguXR} says that every element of \( \SU(2)\) is the exponential of an element in \( V\) the element \( U\) is in the center of \( \SU(2)\). The center of \( \SU(2)\) is \( \{ \pm\id \}\) by the proposition \ref{PROPooLMGHooKrKpsa}.

		Until now we have \( \ker(f)\subset \{ \id,-\id \}\). It is a simple verification to check that \( \{ \id,-\id \}\) are in the kernel of \( f\). We conclude that \( \ker(f)=\{ \pm\mtu_2 \}\).
	\end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooDKPTooBnLflt}
	We have the group isomorphism
	\begin{equation}
		\SO(3)=\frac{ \SU(2) }{ \eZ_2 }.
	\end{equation}
\end{proposition}

\begin{proof}
	We use the first isomorphism theorem \ref{ThoPremierthoisomo} with \( \theta\) being the map \( f\colon \SU(2)\to \SO(3)\) defined by the proposition \ref{PROPooGEHAooPCReoU}. It says that
	\begin{equation}
		\frac{ \SU(2) }{ \ker(f) }=\Image(f).
	\end{equation}
	The known properties of \( f\) are that \( \ker(f)=\eZ_2\) and \( \Image(f)=\SO(3)\). This is the expected result.
\end{proof}

\begin{lemma}       \label{LEMooSYGUooVWxGYX}
	The images of the unitary matrices \( i\sigma_k\) by \( f\) are
	\begin{equation}
		\begin{aligned}[]
			f(i\sigma_1)=\begin{pmatrix}
				             1 &    &    \\
				               & -1 &    \\
				               &    & -1
			             \end{pmatrix},
			f(i\sigma_2)=\begin{pmatrix}
				             -1 &   &    \\
				                & 1 &    \\
				                &   & -1
			             \end{pmatrix},
			f(i\sigma_3)=\begin{pmatrix}
				             -1 &    &   \\
				                & -1 &   \\
				                &    & 1
			             \end{pmatrix}.
		\end{aligned}
	\end{equation}
\end{lemma}

\begin{proof}
	We know that
	\begin{equation}
		\phi(x)=\begin{pmatrix}
			x_3      & x_1-ix_2 \\
			x_1+ix_2 & -x_3
		\end{pmatrix}.
	\end{equation}
	We have
	\begin{equation}
		(i\sigma_1)\phi(x)(i\sigma_1)^{\dag}=\sigma_1\phi(x)\sigma_1=\begin{pmatrix}
			-x_3     & x_1+ix_2 \\
			x_1-ix_2 & x_3
		\end{pmatrix}=\phi\begin{pmatrix}
			x_1  \\
			-x_2 \\
			-x_3
		\end{pmatrix}.
	\end{equation}
	This shows that
	\begin{equation}
		f(i\sigma_1)=\begin{pmatrix}
			x_1  \\
			-x_2 \\
			-x_3
		\end{pmatrix},
	\end{equation}
	so that the matrix of \( i\sigma_1\) is
	\begin{equation}
		f(i\sigma_1)=\begin{pmatrix}
			1 &    &    \\
			  & -1 &    \\
			  &    & -1
		\end{pmatrix}.
	\end{equation}
	The same kind of computations provide the result.
\end{proof}

\begin{proposition}
	Let \( f\colon \SU(2)\to \SO(3)\) be the map of the proposition \ref{PROPooGEHAooPCReoU}:
	\begin{equation}
		\begin{aligned}
			f\colon \SU(2) & \to \SO(3)                        \\
			f(U)           & =\phi^{-1}\circ\rho(U)\circ \phi.
		\end{aligned}
	\end{equation}
	There exist no group homomorphism \( g\colon \SO(3)\to \SU(2)\) such that \( f\circ g=\id\).
\end{proposition}

\begin{proof}
	Let \( g\) be such an homomorphism and let's derive a contradiction. Since \( g\) is an homomorphism it satisfies \( g(\mtu_3)=\mtu_2\). Let
	\begin{equation}
		T_x=\begin{pmatrix}
			1 & 0  & 0  \\
			0 & -1 & 0  \\
			0 & 0  & -1
		\end{pmatrix}\in \SO(3).
	\end{equation}
	The map \( f\) is surjective, so there exist \( U\in \SU(2)\) such that \( f(U)=T_x\). From lemma \ref{LEMooSYGUooVWxGYX} we have
	\begin{equation}
		f(i\sigma_1)=f(-i\sigma_1)=T_x.
	\end{equation}
	Thus \( g(T_x)=i\sigma_1\) or \( g(T_x)=-i\sigma_1\). In both cases we have a contradiction. Indeed, since \( T_x^2=\mtu\) and \( g(T_x^2)=g(T_x)^2\) we must have \( g(T_x)^2=\mtu\) while
	\begin{equation}
		(i\sigma_1)^2=(-i\sigma_1)^2=\mtu.
	\end{equation}
\end{proof}

\begin{lemma}       \label{LEMooRCSSooTvAaJY}
	Let \( \alpha_0\in \SO(3)\) and \( U\in \SU(2)\) such that \( f(U)=\alpha_0\). There exists a neighborhood \(\mO\) of \( \alpha_0\) in \( \SU(2)\) such that
	\begin{equation}
		f^{-1}(\mO)=V_1\cup V_2
	\end{equation}
	where \( V_1\) is a neighborhood of \( U\), \( V_2\) is a neighborhood of \( -U_1\) and \( V_1\cap V_2=\emptyset\).
\end{lemma}

\begin{proof}
	We know that \( f(U)=f(-U)=\alpha_0\). Let \( W_1\) be a neighborhood of \( U\) and \( W_2\) be a neighborhood of \( -U\) such that \( W_1\cap W_2=\emptyset\).

	The part \( -W_2\) is a neighborhood of \( U\). we consider \( V_1\), a neighborhood of \( U\) contained in \( W_1\cap -W_2\). Then we set \( V_2=-V_1\). This is a neighborhood of \(-U\) contained in \( W_2\).

	Thus we have \( V_1\cap V_2=\emptyset\) and \( f(V_1)=f(V_2)\) is a neighborhood of \( \alpha_0\). It remains to define \( \mO=f(V_1)\).
\end{proof}

\begin{proposition}       \label{PROPooHQENooUsQeiZ}
	Let \( U\in \SU(2)\) be such that
	\begin{equation}
		UX=XU
	\end{equation}
	for every \( X\in V\). Then \( U\in\{\mtu,-\mtu\}\).
\end{proposition}

\begin{proof}
	The proof is a simple computation. Let \( a,b,c,d\in \eC\) such that \( U=\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}\). We have
	\begin{equation}
		U\sigma_1=\begin{pmatrix}
			a & b \\
			c & d
		\end{pmatrix}\begin{pmatrix}
			0 & 1 \\
			1 & 0
		\end{pmatrix}=\begin{pmatrix}
			b & a \\
			d & c
		\end{pmatrix}
	\end{equation}
	while
	\begin{equation}
		\sigma_1U=\begin{pmatrix}
			c & d \\
			a & b
		\end{pmatrix}.
	\end{equation}
	We deduce \( b=c\) and \( a=d\) and \( U=\begin{pmatrix}
		a & b \\
		b & a
	\end{pmatrix}\). Taking that into account, the same work with \( \sigma_2\) provides
	\begin{equation}
		U\sigma_2=\begin{pmatrix}
			bi & -ia \\
			ia & -ib
		\end{pmatrix}
	\end{equation}
	and
	\begin{equation}
		\sigma_2U=\begin{pmatrix}
			-bi & -ia \\
			ia  & bi
		\end{pmatrix},
	\end{equation}
	so that \( b=0\). Now \( U=\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}\) for some \( a\in \eC\).

	The constrain \( U\sigma_3=\sigma_3U\) does not provide new informations.

	Since \( U\in \SU(2)\) we have \( \det(U)=1\) which implies \( a^2=1\), which in turn means \( a=\pm1\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooBHVBooEPbWwZ}
	Let \( U_k\in \SU(2)\) such that
	\begin{enumerate}
		\item
		      \( \rho_{U_k}\stackrel{\End(V)}{\longrightarrow}\id_V\)
		\item
		      there exists a neighborhood of \( -\mtu\) in \( \SU(2)\) which contains no element \( U_k\).
	\end{enumerate}
	Then we have \( U_k\stackrel{\SU(2)}{\longrightarrow}\mtu\).
\end{lemma}

\begin{proof}
	Let \( A_k\) be a converging subsequence of \( U_k\) (we will see later that it exists) with \( A_k\stackrel{\SU(2)}{\longrightarrow}A\). For each \( X\in V\) we have \( A_kXA_k^{-1}\stackrel{\SU(2)}{\longrightarrow}X\), so that
	\begin{subequations}
		\begin{align}
			\| A_kX-XA_k \| & =\| A_kXA_k^{-1}A_k-XA_k \|       \\
			                & \leq\| A_kXA_k^{-1}-X \|\| A_k \| \\
			                & \leq \| A_kXA_k^{-1}-X \|M\to 0
		\end{align}
	\end{subequations}
	where \( M\) is some constant majoration of \( \| A_k \|\). Thus we have \( A_kX-XA_k\to 0\) which means
	\begin{equation}
		XA=AX.
	\end{equation}
	If it is true for every \( X\), we conclude that \( A=\pm\mtu\) (proposition \ref{PROPooHQENooUsQeiZ}). Since there is a neighborhood of \( -\mtu\) in which there are no elements \( U_k\), we cannot have \( A_k\to -\mtu\), so we have \( A=\mtu\).

	Now \( U_k\) is a sequence in the compact \( \SU(2)\) (proposition \ref{PROPooGLPQooKOfrjl}), so that every subsequence has a converging subsequence\footnote{Bolzano-Weierstrass \ref{THOooZJWLooAtGMxD}.}. We are in the case of the lemma \ref{LEMooSJKMooKSiEGq} and we conclude \( U_k\stackrel{\SU(2)}{\longrightarrow}\mtu\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooMNWSooAjmBQK}
	Let \( U\in \SU(2)\). We consider the linear map\footnote{See proposition \ref{PROPooRQUZooAoZzwx}.}
	\begin{equation}
		\begin{aligned}
			\rho_U\colon V & \to V             \\
			v              & \mapsto UvU^{-1}.
		\end{aligned}
	\end{equation}
	Then \( \| \rho_U \|\leq 1\).
\end{lemma}

\begin{proof}
	By definition, the norme of \( \rho_U\colon V\to V\) is
	\begin{equation}
		\| \rho_U \|=\sup_{\| v \|=1}\| \rho_Uv \|=\sup_{\| v \|=1}\| UvU^{-1} \|.
	\end{equation}
	In the last expression, the norms are in \( \End(\eC^2)\) because \( U\) and \( v\) are both \( 2\times 2\) complex matrices. The operator norm is an algebra norm\footnote{Lemma \ref{LEMooFITMooBBBWGI}.}, so that
	\begin{equation}
		\| UvU^{-1} \|\leq \| U \|\| U^{-1} \|\| v \|=\| v \|
	\end{equation}
	because the éléments of \( \SU(2)\) are normed to \( 1\). Thus
	\begin{equation}
		\| \rho_1 \|\leq \sup_{\| v \|=1}\| v \|=1.
	\end{equation}
\end{proof}

\begin{lemma}       \label{LEMooHPQQooIGwljm}
	Let \( U_k\) be a sequence in \(\SU(2)\) and \( U\in \SU(2)\) such that
	\begin{enumerate}
		\item
		      \( \rho_{U_k}\stackrel{\End(V)}{\longrightarrow}\rho_U\)
		\item
		      there exists a neighborhood\footnote{Ultimately, the topological properties of $ \SU(2)$ are given by its analytic structure defined in the proposition \ref{PROPooPXRJooLNnMFn}.} of \( -A\) in \( \SU(2)\) which contains no element \( U_k\).
	\end{enumerate}
	Then we have \( U_k\stackrel{\SU(2)}{\longrightarrow}U\).
\end{lemma}

\begin{proof}
	Several steps.
	\begin{subproof}
		\spitem[\( \rho_{U_k^{-1}U}\to\id\)]
		We start by proving that \( \rho_{U_k^{-1} U}\to \id\). For each \( v\in V\) we have
		\begin{equation}
			\| \rho_{U_k^{-1} U}v-v \|=\| \rho_{U_k^{-1}}\big( \rho_Uv-\rho_{U_k}v \big) \|\leq \| \rho_{U_k^{-1}}\| \rho_Uv-\rho_{U_k}v \| \|
		\end{equation}
		Thus we have
		\begin{subequations}
			\begin{align}
				\| \rho_{U_k^{-1}U}-\id \| & =\sup_{\| v \|=1}\| \rho_{U_k^{-1}U}v-v \|                          \\
				                           & \leq \| \rho_{U_k^{-1}} \|\sup_{\| v \|=1}\| \rho_Uv-\rho_{U_k}v \| \\
				                           & =\| \rho_{U_k} \|\| \rho_u-\rho_{U_k} \|                            \\
				                           & \leq \| \rho_U-\rho_{U_k} \|.
			\end{align}
		\end{subequations}
		We used lemme \ref{LEMooMNWSooAjmBQK}. In conclusion,
		\begin{equation}
			\| \rho_{U_k^{-1}U}-\id \|\leq \| \rho_U-\rho_{U_k} \|\to 0.
		\end{equation}
		\spitem[No neighborhood of \( -\mtu\)]
		We prove that there exists a neighborhood of \( -\mtu\) which contains no elements of the sequence \( U_k^{-1}U\). Suppose that each neighborhood of \( -\mtu\) contains one of the \( U_k^{-1}U\). At this point we have a subsequence \( (B_k)\) of \( (U_k)\) such that
		\begin{equation}
			B_k^{-1}U\to-\mtu.
		\end{equation}
		Since the multiplication and the inverse are continuous operations\footnote{The group \( \SU(2)\) is a Lie group, proposition \ref{PROPooPXRJooLNnMFn}\ref{ITEMooHZQRooFGCVjP}.} we also have
		\begin{equation}
			B_k^{-1}\to-U^{-1}
		\end{equation}
		and
		\begin{equation}
			B_k\to -U.
		\end{equation}
		This prove that for every neighborhood of \( -U\) we have a \( B_k\) and then a \( U_k\), which is a contradiction with the hypothesis.
		\spitem[Conclusion]
		The sequence \( U_k^{-1}U\) satisfies the lemma \ref{LEMooBHVBooEPbWwZ} and we conclude \( U_k^{-1}U\to \mtu\). Thus \( U_k\to U\).
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooYTTJooYpPYLT,MonCerveau}]        \label{PROPooHCVZooMOSzTm}
	Let \( \alpha_0\in \SO(3)\) and \( \mO\) be a neighborhood of \( \alpha_0\) such that \( f^{-1}(\mO)=V_1\cup V_2\) with \( V_1\cap V_2=\emptyset\)\footnote{Such choice is possible by the lemma \ref{LEMooRCSSooTvAaJY}.}.

	The map
	\begin{equation}
		\begin{aligned}
			\varphi\colon \mO & \to\SU(2)                      \\
			\alpha            & \mapsto f^{-1}(\alpha)\cap V_1
		\end{aligned}
	\end{equation}
	is continuous.
\end{proposition}

\begin{proof}
	Let \( \sigma_k\stackrel{\SO(3)}{\longrightarrow} \alpha\) with \( \alpha_k\in \mO\). We have to prove that \( \varphi(\alpha_k)\stackrel{\SU(2)}{\longrightarrow}\varphi(\alpha)\).
	\begin{subproof}
		\spitem[General setting]
		First we suppose that \( \alpha_k\) converges to the identity. For each \( k\) we have
		\begin{equation}
			f\big( \varphi(\alpha_k) \big)=\alpha_k,
		\end{equation}
		with the map \eqref{EQooSOZTooTIkONx}. That means, for each \( k\):
		\begin{equation}
			\phi^{-1}\circ\rho_{\varphi(\alpha_k)}\circ \phi=\alpha_k,
		\end{equation}
		or
		\begin{equation}
			\rho_{\varphi(\alpha_k)}=\varphi\circ \alpha_k\circ\phi^{-1}
		\end{equation}
		as operator on \( V\).
		\spitem[Norm convergence]
		We have the following computation:
		\begin{subequations}
			\begin{align}
				\| \rho_{\varphi(\alpha_k)}-\rho_{\varphi(\alpha)} \|_{\End(V)} & =\| \phi\circ\alpha_k\circ\phi^{-1}-\phi\circ\alpha\circ\phi^{-1} \| \\
				                                                                & =\| \phi\circ(\alpha_k-\alpha)\circ\phi^{-1} \|                      \\
				                                                                & \leq \| \phi \|\| \alpha_k-\alpha \|\| \phi^{-1} \|\to0.
			\end{align}
		\end{subequations}
		This shows that
		\begin{equation}        \label{EQooCEFUooTCoczi}
			\rho_{\varphi(\alpha_k)}\to \rho_{\varphi(\alpha)}.
		\end{equation}

		\spitem[Conclusion]

		The sequence \( U_k=\varphi(\alpha_k)\) and the element \( U=\varphi(\alpha)\) satisfy the lemma \ref{LEMooHPQQooIGwljm}, so that \( \varphi(\alpha_k)\to \varphi(\alpha)\).
	\end{subproof}
\end{proof}

\begin{proposition}
	The map \( f\colon \SU(2)\to \SO(3)\) is a representation of \( \SU(2)\) on \( \eR^3\), but is not faithful\footnote{Définition \ref{DEFooAFSAooGDSDBb}.}.
\end{proposition}

\begin{proof}
	The function \( f\) is written as
	\begin{equation}
		f(U)=\phi^{-1}\circ\rho_U\circ\phi.
	\end{equation}
	On the other hand, we have
	\begin{equation}
		\rho_{U_1U_2}v=U_1U_2v(U_1U_2)^{-1}=U_1U_2vU_2^{-1}U_1^{-1}=(\rho_{U_1}\circ\rho_{U_2})v.
	\end{equation}
	Thus
	\begin{equation}
		f(U_1U_2)=\phi^{-1}\circ\rho_{U_1}\circ\rho_{U_2}\circ\phi=\underbrace{\phi^{-1}\rho_{U_1}\phi}_{f(U_1)}\underbrace{phi^{-1}\rho_{U_2}\phi}_{f(U_2)}=f(U_1)\circ f(U_2).
	\end{equation}
	Thus \( f\) is a representation.

	It is not faithful because \( f(\mtu)=f(-\mtu)=\id\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{The Lie algebras \texorpdfstring{\( \su(2)\)}{su(2)} and \texorpdfstring{$ \so(3)$}{so(n)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}   \label{PROPooSERWooFtxBgV}
	The Lie algebra \( \su(n)\) of \( \SU(n)\) is isomorphic to the algebra of traceless anti-hermitian matrices\footnote{See the definitions \ref{DEFooKDCPooZOJsMD} and \ref{DEFooJJVIooDUBwDJ}.}.
	\begin{equation}
		\su(n)=\{X\in\gl(2,\eC)\tq X^{\dag}=-X,\tr(X)=0\}.
	\end{equation}
\end{proposition}

\begin{proof}
	Let consider $G=\SU(n)$; the elements are complexes $n\times n$ matrices $U$ such that $UU^{\dag}=\mtu$ and $\det U=1$. An element of the Lie algebra is given by a path $\dpt{u}{\eR}{G}$ in the group with $u(0)=\mtu$. Since \( \SU(n)\) is a Lie subgroup of \( \GL(n,\eC)\)\footnote{Proposition \ref{PROPooPXRJooLNnMFn}\ref{ITEMooHZQRooFGCVjP}.}, by the proposition \ref{PROPooSQHLooGQAykc}, it is sufficient to compute the usual derivative of such a path.
	Since for all $t$, $u(t)u(t)^{\dag}=\mtu$,
	\begin{subequations}
		\begin{align}
			0 & =\Dsdd{u(t)u(t)^{\dag}}{t}{0}                              \\
			  & =u(0)\Dsdd{u(t)^{\dag}}{t}{0}+\Dsdd{u(t)}{t}{0}u(0)^{\dag} \\
			  & =[d_tu(t)]^{\dag}+[d_tu(t)].
		\end{align}
	\end{subequations}
	So a general element of the Lie algebra $\su(n)$ is an anti-hermitian matrix.

	An element of \( \SU(n)\) has also a determinant equal to \( 1\). What condition does it implies on the elements of the Lie algebra?

	Let \( X\) be an element of \( \su(2)\). For each \( t\), the element \(  e^{tX}\) is part of the Lie group and satisfy \( \det( e^{tX})=1\). Using the formula\footnote{Corollary \ref{CORooOKKSooHrsYOs}.}
	\begin{equation}
		\Dsdd{ \det( e^{tX}) }{t}{0}=\tr(X)
	\end{equation}
	we deduce \( \tr(X)=0\).

	An other way to prove the same result is to consider a path in \( \SU(n)\) and derive; let's do it. If \( g(t)\) is a path in \( \SU(n)\) with \( g(0)=\mtu\). For each \( t\) we have \( \det\big( g(t) \big)=1\).

	Using the formula expression the determinant with the minors,
	\begin{equation}
		\det\begin{pmatrix}
			g_{11}(t) & g_{12}(t) & \ldots \\
			f_{21}(t) & g_{22}(t) & \ldots \\
			\vdots    & \vdots    & \ddots
		\end{pmatrix}=g_{11}(t)M_{11}(t)+g_{12}(t)M_{12}(t)+\ldots=1
	\end{equation}
	where \( M_{ij}\) is the minor of \( g\). If we derive the left hand side we get
	\begin{equation}
		g'_{11}(0)M_{11}(0)+g_{11}(0)M'_{11}(0)+g'_{12}(0)M_{12}(0)+g_{12}(0)M'_{12}(0)+\ldots
	\end{equation}
	where the numbers \( g'_{ij}(0)\) are the matrix entries of the tangent matrix, that is the matrix elements of a general element in \( \gsu(n)\). Since \( g(0)=\mtu\) we have \( M_{11}(0)=1\), \( g_{11}(0)=1\), \( M_{12}(0)=0\) and \( g_{12}(0)=0\). Thus we have
	\begin{equation}
		(\det g)'(0)=X_{11}+M'_{11}(0)
	\end{equation}
	where \( X=g'(0)\). By induction we found that the trace of \( X\) appears. Thus the elements of \( \gsu(n)\) have vanishing trace.
\end{proof}

\begin{normaltext}
	The space \( V\) spanned by the matrices \( \sigma_i\) is not \( \su(2)\).
\end{normaltext}

\begin{proposition}     \label{PROPooDNNEooMOdrkq}
	The Lie algebra \( \so(n)\) is the vector space of antisymmetric matrices.
\end{proposition}

\begin{proof}
	As said in the proposition \ref{PROPooSQHLooGQAykc}, the Lie algebra \( \so(n)\) can be seen as \( \SO(n)'\), the Lie algebra of the matrices obtained by componentwise derivate paths in \( \SO(n)\).

	\begin{subproof}
		\spitem[Inclusion in one sense]
		So let be a path \( g\colon \eR\to \SO(n)\). For each \( t\) we have the equality
		\begin{equation}
			g(t)g(t)^t=\mtu.
		\end{equation}
		We differentiate that equation with respect to \( t\) at \( t=0\) taking into account \( g(0)=\mtu\):
		\begin{equation}
			g'(0)+g'(0)^t=0.
		\end{equation}
		This shows that the matrices of the Lie algebra \( \so(n)\) are skew-symmetric.
		\spitem[Inclusion in the other sense]
		Now we prove that every skew-symmetric matrix is of the form \( \gamma'(0)\) for some path \( \gamma\colon \eR\to \SO(n) \). Let \( X\) be a skew-symmetric matrix. We consider the path
		\begin{equation}
			\gamma(t)= e^{tX}
		\end{equation}
		defined in the proposition \ref{PropPEDSooAvSXmY}. We have to prove that \( \gamma(t)\in \SO(3)\) for every \( t\) (at least in a neighborhood of \( t=0\)) and that \( \gamma(0)=\mtu\).
		\begin{subproof}
			\spitem[\( \gamma(0)=\mtu\)]
			This is the proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooCVALooEfLQCyI}.
			\spitem[\( \gamma(t)\) is orhogonal]
			By proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooEOSMooQWjcjA} we know that
			\begin{equation}
				( e^{tX})^t= e^{tX^t}.
			\end{equation}
			Since \( X\) is skew-symmetric we also have \( [X,X^t]=0\), because
			\begin{equation}
				(XX^t)_{ij}=\sum_kX_{ik}X^t_{kj}=\sum_kX_{ik}X_{jk}
			\end{equation}
			while
			\begin{equation}
				(X^tX)_{ij}=\sum_kX^t_{ik}X_{kj}=\sum_kX_{ki}X_{kj}=\sum_kX_{ik}X_{jk}.
			\end{equation}
			The last equality accounts the fact that \( X\) is skew-symmetric. Since \( X\) and \( X^t\) commute we can use the theorem \ref{THOooXCPEooYGyLOp}:
			\begin{equation}
				e^{tX}( e^{tX})^t= e^{tX} e^{tX^t}= e^{t(X+X^t)}= e^{0}=\mtu.
			\end{equation}
			\spitem[\( \gamma(t)\) is special] We prove that \( \det\big( \gamma(t) \big)=1\). The proposition \ref{PROPooZUHOooQBwfZq} provides
			\begin{equation}
				\det( e^{tX})= e^{\tr(tX)}= e^{0}=1.
			\end{equation}
			By the way, these equalities are equalities in \( \eR\), not equalities on \( \GL(n,\eR)\).
			\spitem[Pause]
			We finished to prove that \( \gamma(t)\in \SO(n)\) for every \( t\in \eR\). We still have to prove that \( \gamma'(0)=X\).
			\spitem[\( \gamma'(0)=X\)]
			This is from proposition \ref{PROPooSDNNooQtHkhA} :
			\begin{equation}
				\Dsdd{  e^{tX} }{t}{0}=X.
			\end{equation}
		\end{subproof}
	\end{subproof}
\end{proof}

\begin{normaltext}
	Notice that antisymmetric matrices are automatically with vanishing trace.
\end{normaltext}

\begin{proposition}     \label{PROPooHOOLooOrcquD}
	Two Lie algebras.
	\begin{enumerate}
		\item       \label{ITEMooFSTMooGSjovL}
		      The Lie algebra of \( \gU(n)\) is the set \( \gu(n)\) of anti-hermitian matrices.
		\item       \label{ITEMooYEFMooRmGmlF}
		      The Lie algebra of \( \SU(n)\) is the set \( \su(n)\) of anti-hermitian matrices with vanishing trace\footnote{Just to be clear: as set this is the skew-hermitian matrices. As vector space, this is a real vector space. The fact to be skew-hermitian is not preserved by a multiplication by \( i\).}.
		\item           \label{ITEMooXXTRooQZzCfs}
		      A basis of \( \su(2)\) is given by the matrices \( t_k=-i\sigma_k\) where \( \sigma_k\) are the Pauli matrices\footnote{Definition \ref{DEFooRNTDooTVkPtB}.}, that is
		      \begin{equation}
			      \begin{aligned}[]
				      t_1=i\sigma_1=\begin{pmatrix}
					                    0 & i \\
					                    i & 0
				                    \end{pmatrix} &  & t_2=i\sigma_2=\begin{pmatrix}
					                                                     0  & 1 \\
					                                                     -1 & 0
				                                                     \end{pmatrix} &  & t_3=i\sigma_3=\begin{pmatrix}
					                                                                                      i & 0  \\
					                                                                                      0 & -i
				                                                                                      \end{pmatrix}.
			      \end{aligned}
		      \end{equation}
		\item
		      The commutation relations in \( \su(2)\) are
		      \begin{equation}        \label{EQooFJIDooRtQGjA}
			      [t_i,t_j]=2\sum_k\epsilon_{ijk}t_k.
		      \end{equation}
	\end{enumerate}
\end{proposition}

\begin{proof}
	The point \ref{ITEMooFSTMooGSjovL} is the same kind of proof that the one of proposition \ref{PROPooDNNEooMOdrkq}; the only difference is that one starts with \( g(t)g(t)^{\dag}=\mtu\). Then one use \(  e^{X^{\dag}}=( e^{X})^{\dag}\).

	The point \ref{ITEMooYEFMooRmGmlF}, is already proved in the proposition \ref{PROPooSERWooFtxBgV}.

	For the point \ref{ITEMooXXTRooQZzCfs}, an explicit computation shows that the matrices \( t_k\) belong to \( \su(2)\) and are linearly independent. Now there are two ways to proceed.

	One way is to prove that \( \su(2)\) has dimension \( 3\). For that, you can write down an explicit manifold structure on \( \SU(2)\) and show that it is a manifold of dimension \( 3\). Then the Lie algebra has the same dimension.

	An other way is to make it by hand. We consider a matrix \( \begin{pmatrix}
		a & c \\
		d & b
	\end{pmatrix}\in \eM(\eC,2)\). The fact to be traceless imposes \( a=-b\). Then we have
	\begin{equation}
		\begin{pmatrix}
			a & c  \\
			d & -a
		\end{pmatrix}^{\dag}=\begin{pmatrix}
			\bar a & \bar d  \\
			\bar c & -\bar a
		\end{pmatrix}.
	\end{equation}
	The condition to be skew-hermitian is
	\begin{equation}
		\begin{pmatrix}
			a & c  \\
			d & -a
		\end{pmatrix}=-\begin{pmatrix}
			\bar a & \bar d  \\
			\bar c & -\bar a
		\end{pmatrix}.
	\end{equation}
	That provide the constrains that \( a\) is purely imaginary and that, if \( c=x+iy\), then \( d=-x+iy\). Thus a generic matrix in \( \su(2)\) is given by
	\begin{equation}
		\begin{pmatrix}
			\lambda i & x+iy       \\
			-x+iy     & -\lambda i
		\end{pmatrix}=\lambda\begin{pmatrix}
			i & 0  \\
			0 & -i
		\end{pmatrix}+x\begin{pmatrix}
			0  & 1 \\
			-1 & 0
		\end{pmatrix}+y\begin{pmatrix}
			0 & i \\
			i & 0
		\end{pmatrix}=x(i\sigma_2)+y(i\sigma_1)+\lambda(i\sigma_3).
	\end{equation}
	with \( x,y,\lambda\in \eR\). That shows that \(  \{ i\sigma_k \} \) is a basis of \( \su(2)\).  In the same time this is a proof a proof that \( \su(2)\) has dimension \( 3\).

	The lemma \ref{LEMooJRWXooMkzRnk} provides the commutators for the Pauli matrices. We can adapt them for our basis of \( \su(2)\):
	\begin{equation}
		[t_i,t_j]=[i\sigma_i , i\sigma_j ]=-[\sigma_i,\sigma_j]=-2i\sum_k\epsilon_{ijk}\sigma_k=2\sum_k\epsilon_{ijk}t_k.
	\end{equation}
\end{proof}

\begin{proposition}
	The Lie algebras \( \su(2)\) and \( \so(3)\) are isomorphic.
\end{proposition}

\begin{proof}
	The propositions \ref{PROPooDNNEooMOdrkq} and \ref{PROPooHOOLooOrcquD} provide a description of \( \su(2)\) and \( \so(3)\). The easiest way to prove the isomorphism is to show an explicit isomorphism. A basis of \( \so(3)\) is
	\begin{equation}
		\begin{aligned}[]
			O_1 & =\begin{pmatrix}
				       0 & 0  & 0 \\
				       0 & 0  & 1 \\
				       0 & -1 & 0
			       \end{pmatrix}, & O_2 & =\begin{pmatrix}
				                               0  & 0 & 1 \\
				                               0  & 0 & 0 \\
				                               -1 & 0 & 0
			                               \end{pmatrix}, & O_3 & =\begin{pmatrix}
				                                                       0  & 1 & 0 \\
				                                                       -1 & 0 & 0 \\
				                                                       0  & 0 & 0
			                                                       \end{pmatrix}.
		\end{aligned}
	\end{equation}
	These matrices satisfy
	\begin{equation}        \label{EQooWJMUooOtFAkW}
		[O_i,O_j]=\sum_k\epsilon_{ijk}O_k.
	\end{equation}
	A basis of \( \su(2)\) is given by \( t_k=i\sigma_k\). Our isomorphism is
	\begin{equation}
		\begin{aligned}
			\varphi\colon \so(3) & \to \su(2)                        \\
			O_i                  & \mapsto -\frac{ i\sigma_k }{ 2 }.
		\end{aligned}
	\end{equation}
	The fact that \( \varphi\) is a bijection derives from the fact that it maps a basis on a basis. We have to check that \( \varphi\) is a morphism, that is
	\begin{equation}
		\big[ \varphi(O_i),\varphi(O_j) \big]=\varphi\big( [O_i,O_j] \big).
	\end{equation}
	This is done by virtue of the commutators \eqref{EQooWJMUooOtFAkW} and of lemma \ref{LEMooJRWXooMkzRnk}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Irreducible representations of \texorpdfstring{$\gsl(2,\eC)$}{sl(2,C)}}
%---------------------------------------------------------------------------------------------------------------------------

We are not here to joke or to be funny. We are here to make quantum fields theory. So we need (among maaaaaany other things) the irreducible representations of the groups \( \SL(2,\eC)\) and \( \SU(2)\). Here is a good news: at the Lie algebra level, these two are more or less related by the lemma \ref{LEMooIGAFooTSUsJR}.

\begin{lemma}[\cite{BIBooUXTFooXTeMOn}]     \label{LEMooVEJZooUVNdmE}
	As sets,
	\begin{equation}
		\su(2)_{\eC}=\su(2)\otimes_{\eR}\eC=\gsl(2,\eC)=\{ \begin{pmatrix}
			\alpha & \beta   \\
			\gamma & -\alpha
		\end{pmatrix}\tq \alpha,\beta,\gamma\in \eC\}.
	\end{equation}
	The first equality is a definition for the notation \( \su(2)_{\eC}\).
\end{lemma}

\begin{proof}
	A basis of \( \su(2)\) is \( \{ t_1,t_2,t_3 \}\); so \( \su(2)_{\eC}=\eC t_1\oplus \eC t_2\oplus \eC t_3\), that is
	\begin{subequations}
		\begin{align}
			\su(2)_\eC & =\{ \begin{pmatrix}
				                 0  & ix \\
				                 ix & 0
			                 \end{pmatrix}+\begin{pmatrix}
				                               0  & y \\
				                               -y & 0
			                               \end{pmatrix}+\begin{pmatrix}
				                                             iz & 0   \\
				                                             0  & -iz
			                                             \end{pmatrix}\tq x,y,z\in \eC\} \\
			           & =\{\begin{pmatrix}
				                iz   & ix+y \\
				                ix-y & -iz
			                \end{pmatrix}\rq x,y,z\in \eC\}                              \\
			           & =\{\begin{pmatrix}
				                \alpha & \beta   \\
				                \gamma & -\alpha
			                \end{pmatrix}\tq \alpha,\beta,\gamma\in \eC\}.
		\end{align}
	\end{subequations}
	In order to determine the Lie algebra \( \gsl(2,\eC)\) of \( \SL(2,\eC)\) we use the proposition \ref{PROPooSQHLooGQAykc} to allow ourself to work at the matrix level. Let \( g\) be a smooth path in \( \SL(2,\eC)\) such that \( g(0)=\mtu\). A generic element of \( \gsl(2,\eC)\) has the form \( g'(0)\). We have
	\begin{equation}
		g(t)=\begin{pmatrix}
			\alpha(t) & \beta(t)  \\
			\gamma(t) & \delta(t)
		\end{pmatrix}
	\end{equation}
	with
	\begin{equation}        \label{EQooMNXMooVkbfDg}
		\alpha(t)\delta(t)-\gamma(t)\beta(t)=1
	\end{equation}
	for every \( t\).  Moreover \( g(0)=\mtu\) implies \( \alpha(0)=1\), \( \beta(0)=0\), \( \gamma(0)=0\) and \( \delta(0)=1\). Now we differentiate \eqref{EQooMNXMooVkbfDg} with respect to \( t\) at \( t=0\) :
	\begin{equation}
		\alpha'(0)\delta(0)+\alpha(0)\delta'(0)-\gamma'(0)\beta(0)-\gamma(0)\beta'(0)=0
	\end{equation}
	which reduces to \( \alpha'(0)+\delta'(0)=0\). An element of \( \gsl(2,\eC)\) is thus of the form \( \begin{pmatrix}
		a & b  \\
		c & -a
	\end{pmatrix}\) with \( a,b,c\in \eC\).
\end{proof}

\begin{normaltext}
	Lemma \ref{LEMooVEJZooUVNdmE} speaks about the \emph{sets} of \( \su(2)_{\eC}\) and \( \gsl(2,\eC)\). The Lie bracket, in both cases, is the matrix commutator. As a vector space, one can consider on \( \gsl(2,\eC)\) a vector space structure on \( \eC\) or on \( \eR\). If you want \( \{ t_1, t_2, t_3 \}\) to be a basis, you have to consider the complex linear combinations. If you really want real linear combinations, you need a larger basis.
\end{normaltext}

We consider the following basis for \( \gsl(2,\eC)\):
\begin{subequations}        \label{EQSooORIBooAsgdDp}
	\begin{align}
		h_3 & =\begin{pmatrix}
			       1/2 & 0    \\
			       0   & -1/2
		       \end{pmatrix}   \\
		h_+ & =\begin{pmatrix}
			       0 & \sqrt{ 2 }/2 \\
			       0 & 0
		       \end{pmatrix} \\
		h_- & =\begin{pmatrix}
			       0            & 0 \\
			       \sqrt{ 2 }/2 & 0
		       \end{pmatrix}
	\end{align}
\end{subequations}
They satisfy the commutation relations
\begin{subequations}        \label{SUBEQSooXMMVooKtnRXW}
	\begin{align}
		[h_3,h_+] & =h_+  \\
		[h_3,h_-] & =-h_- \\
		[h_+,h_-] & =h_3.
	\end{align}
\end{subequations}

\begin{lemma}[\cite{BIBooUXTFooXTeMOn}]     \label{LEMooDGUYooPUkDNr}
	Let \( (\rho, V)\) be a representation of \( \gsl(2,\eC)\). If \( V_{\lambda}\) is the eigenspace of the eigenvalue \( \lambda\in \eC\) for \( \rho(h_3)\), then
	\begin{subequations}
		\begin{align}
			\rho(h_+)V_{\lambda} & \subset V_{\lambda+1} \\
			\rho(h_-)V_{\lambda} & \subset V_{\lambda-1}
		\end{align}
	\end{subequations}
\end{lemma}

\begin{proof}
	Let \( w\in V_{\lambda}\). We test the eigenvalue of \( \rho(h_3)\) on \( \rho(H_+)w\):
	\begin{subequations}
		\begin{align}
			\phi(h_3)\rho(h_+)w & =\big( [\rho(h_3),\rho(h_+)]+\rho(h_+)\rho(h_3) \big) \\
			                    & =\rho(h_+)w+\lambda\rho(h_+)w                         \\
			                    & =(\lambda+1)\rho(h_+)w,
		\end{align}
	\end{subequations}
	so that \( \phi(h_+)w\in V_{\lambda+1}\).

	The computation is the same for the other one.
\end{proof}

\begin{lemma}           \label{LEMooWXDYooUyijnm}
	Let \( (V,\rho)\) be a finite dimensional representation of \( \gsl(2,\eC)\) over the complex vector space \( V\). There exists \( \lambda_0\in \eC\) such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}=\{ 0 \}\).
\end{lemma}

\begin{proof}
	A vector \( w\in V\) belong to \( V_{\lambda}\) if \( \rho(h_3)w=\lambda w\), which meas that \( \big( \rho(h_3)-\lambda\id \big)w=0\). The equation \( \det\big( \rho(h_3)-\lambda\id \big)\) has (at least) one solution \( \lambda\in \eC\). So there exists \( \lambda\in \eC\) such that \( V_{\lambda}\neq \{ 0 \}\).

	Let \( \lambda\) be such a number and a non vanishing vector \( w\in V_{\lambda}\).

	Now the sequence of elements \( w_k= \rho(h_+)w   \) satisfy \( w_k\in V_{\lambda+k}\). Since \( V\) is finite dimensional only a finite number of the \( V_{\lambda+k}\) are different to \( \{ 0 \}\). The space \( V_{\lambda}\) on the other hand contains \( w\neq 0\). Let \( k_0\) be the lowest natural such that \( V_{\lambda+k_0}=\{ 0 \}\). What we have is \( V_{\lambda+k_0-1}\neq \{ 0 \}\) and \( V_{\lambda_0+k_0}=\{ 0 \}\).

	The proposition is done with \( \lambda_0=\lambda+k_0\).
\end{proof}

\begin{proposition}[\cite{BIBooUXTFooXTeMOn}]      \label{PROPooZCAOooHHGxQk}
	Let \( (\rho, V)\) be a finite dimensional complex representation of \( \gsl(2,\eC)\). Let \( \lambda_0\) be such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}=\{ 0 \}\). Let \( w_0\in V_{\lambda_0}\) and
	\begin{equation}
		w_k=\rho(h_-)^kw_0.
	\end{equation}
	Then
	\begin{enumerate}
		\item       \label{ITEMooBPPFooKdGyqO}
		      \( w_k\in V_{\lambda_0-k}\)
		\item       \label{ITEMooHNULooHoTgEa}
		      \( \rho(h_+)w_k=\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1}\)
		\item       \label{ITEMooHDAPooClASpy}
		      There exists \( n\in \eN\) such that \( w_n\neq 0\) and \( w_{n+1}=0\).
		\item       \label{ITEMooJBZFooGqallS}
		      \( \lambda_0=n/2\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Notice that the existence of \( \lambda_0\) such that \( V_{\lambda_0}\neq \{ 0 \}\) and \( \rho(h_+)V_{\lambda_0}\neq\{ 0 \}\) is provided by lemma \ref{LEMooWXDYooUyijnm}.
	\begin{subproof}
		\spitem[For \ref{ITEMooBPPFooKdGyqO}]
		By recursion, using lemma \ref{LEMooDGUYooPUkDNr}.
		\spitem[For \ref{ITEMooHNULooHoTgEa}]
		We'll have a recursion. Just to be clear here are two facts that are not yet proved:
		\begin{itemize}
			\item The spaces \( V_{\lambda_0+k}\) are one-dimensional.
			\item \( \rho(h_+)w_{k+1}\) is a multiple of \( w_k\).
		\end{itemize}
		We will now prove by recursion that the first fact is true. The second one is, in general, false. We will see later that it is true when the representation is irreducible.

		Ok. So let's begin our work. For \( k=0\) we already have
		\begin{equation}
			\rho(h_+)w_0=0.
		\end{equation}
		Let work out the case of \( k=1\).
		\begin{subequations}
			\begin{align}
				\rho(h_+)w_1 & =\rho(h_+)\rho(h_-)w_0                                                             \\
				             & =\big( \underbrace{[\rho(h_+),\rho(h_-)]}_{=\rho(h_3)}+\rho(h_-)\rho(h_+) \big)w_0 \\
				             & =\rho(h_3)w_0                                                                      \\
				             & =\lambda_0w_0.
			\end{align}
		\end{subequations}

		For the recursion, suppose that \( \rho(h_+)w_k=f(k)w_{k-1}\) for some function \( f\colon \eN\to \eC\). Then we compute \( \rho(h_+)w_{k+1}\):
		\begin{subequations}
			\begin{align}
				\rho(h_+)w_{k+1}=\rho(h_+)\rho(h_-)w_k & =\big( \underbrace{[\rho(h_+),\rho(h_-)]}_{=\rho(h_3)}+\rho(h_-)\rho(h_+) \big)w_k \\
				                                       & =\lambda_0w_k+f(k)\rho(h_-)w_{k-1}                                                 \\
				                                       & =\big( \lambda_0+f(k) \big)w_k.
			\end{align}
		\end{subequations}
		This shows that \( \rho(h_+)w_{k+1}\) is a multiple of \( w_k\) and that the proportionality factor \( f(k)\) satisfy
		\begin{subequations}        \label{SUBEQSooHGQNooRjMCap}
			\begin{numcases}{}
				f(k+1)=f(k)+\lambda_0-k\\
				f(1)=\lambda_0.
			\end{numcases}
		\end{subequations}
		The function \( f\) is defined by recursion and you see that at each step \( k\), we substrat \( k\) and add \( \lambda_0\). The guess is
		\begin{equation}
			f(k)=k\lambda_0-\frac{ k(k-1) }{ 2 }.
		\end{equation}
		Check that this satisfy \eqref{SUBEQSooHGQNooRjMCap}.

		\spitem[For \ref{ITEMooHDAPooClASpy}]
		The sequence of elements \( w_k\in V_{\lambda_0-k}\) has to finish on \( 0\) because the space \( V\) is finite dimensional.
		\spitem[For \ref{ITEMooJBZFooGqallS}]
		Let \( n\in \eN\) such that \( w_n\neq 0\) and \( w_{n+1}=0\). This means \( f(k+1)=0\). Solving
		\begin{equation}
			(n+1)\big( \lambda_0+\frac{ 1-(n+1) }{ 2 } \big)=0
		\end{equation}
		we get \( \lambda_0=n/2\).
	\end{subproof}
\end{proof}

This is quite an achievement because we proved not only that \( \rho(h_3)\) has a real eigenvalue, but that it has an eigenvalue in \( \eN/2\).

\begin{proposition}     \label{PROPooDAIQooPZVjju}
	Let \( \lambda_0\) and \( w_0\) be as before. We suppose that the representation is irreducible. Then
	\begin{equation}
		V=\Span\{  \rho(h_-)^kw_0 \}_{k=0,\ldots, 2\lambda_0}
	\end{equation}

	The eigenspaces of \( \rho(h_3)\) are one-dimensional.
\end{proposition}

\begin{proof}
	Let \(  W=\Span\{  \rho(h_-)^kw_0 \}_{k=0,\ldots, 2\lambda_0}\). This space is invariant under \( \rho\) because of the definitions and the proposition \ref{PROPooZCAOooHHGxQk}:
	\begin{equation}
		\begin{aligned}[]
			\rho(h_3)w_k & =(\lambda_0-k)w_k                      \\
			\rho(h_+)w_k & =\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1} \\
			\rho(h_-)w_k & =w_{k+1}
		\end{aligned}
	\end{equation}
	with the convention that \( w_{k+1}\) could be \( 0\).

	Since \( W\) is a non trivial invariant subspace, it has to be \( V\). So \( W=V\).
\end{proof}

\begin{normaltext}
	In the proposition \ref{PROPooZCAOooHHGxQk} and \ref{PROPooDAIQooPZVjju}, the vectors \( w_k\) are more or less enumerated in the reverse order: the larger \( k\) is, the lower is the eigenvalue. That leads to missleading formula like \( \rho(h_-)v_k=v_{K+1}\). In the following theorem, we make it in the correct order and one has to think \( v_m\) as being \( w_{\lambda_0-m}\).

	Notice that up to now, the results we have collected are «if a representation of \( \gsl(2,\eC)\) exists». The next theorem \ref{THOooSRQYooXQDZpT} will show that a representation exists.
\end{normaltext}

Here is the theorem which provides every irredicible finite-dimensional representations of the Lie algebra \( \gsl(2,\eC)=\su(2)_{\eC}\).
\begin{theorem}     \label{THOooSRQYooXQDZpT}
	Let \( j\in \eN/2\). Let \( V_j\) be a complex vector space of dimension \( 2j+1\); we label a basis of \( V_j\) in the following way: \( \{ v_m \}_{m=j,j-1,\ldots, -j}\).

	We define the map \( \rho_j\colon \gsl(2,\eC)\to \End(V_j)\) by
	\begin{subequations}
		\begin{align}
			\rho_j(h_3)v_m & =mv_m,                                                 \\
			\rho_j(h_+)v_m & =\begin{cases}
				                  0                                & \text{if } m=j     \\
				                  \frac{ 1 }{2}(j-m)(j+m+1)v_{m+1} & \text{otherwise },
			                  \end{cases} \\
			\rho_j(h_-)v_m & =\begin{cases}
				                  v_{m-1} & \text{if } m\neq -j \\
				                  0       & \text{if } m=-j.
			                  \end{cases}
		\end{align}
	\end{subequations}
	Two statements.
	\begin{enumerate}
		\item
		      The map \( \rho_j\) is a representation of \( \gsl(2,\eC)\).
		\item
		      Every finite dimensional complex irreducible representation is isomorphic to \( \rho_j\) for some \( j\in \eN/2\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	For the first item we have to check the algebra of \( \gsu(2)\) given by \eqref{SUBEQSooXMMVooKtnRXW}. There are three computations.

	We begin to check \( [\rho_j(h_3), \rho_j(h_+)]=\rho_j(h_+)\). The bracket in the left-hand side is the commutator of operators in \( \End(V)\). We have:
	\begin{subequations}
		\begin{align}
			\big( \rho_j(h_3)\rho_j(h_+)-\rho(h_+)\rho(h_3) \big)v_m & =\rho_j(h_3)\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}-m\rho_j(h_+)v_m \\
			                                                         & =(m+1)\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}                       \\
			                                                         & \quad-m\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}                      \\
			                                                         & =\frac{ 1 }{2}(j-m)(j+m+1)v_{m+1}                            \\
			                                                         & =\rho_j(h_+)v_m.
		\end{align}
	\end{subequations}
	The two other ones are checks with the same kind of computations.

	For the second item, we consider an irreducible finite-dimensional representations \( (\rho,V)\) of \( \gsl(2,\eC)\). Combining the propositions  \ref{PROPooZCAOooHHGxQk} and \ref{PROPooDAIQooPZVjju} we have:
	\begin{itemize}
		\item A vector \( w_0\in V\) such that \( \rho(h_+)w_0=0\).
		\item Letting \( n=2\lambda_0\) we have \( n\in \eN\),
		\item we let \( j=\lambda_0\) (pure notational purpose),
		\item \( V=\Span\{ w_k=\rho(h_-)^kw_0 \}\),
		\item  \( w_k\in V_{\lambda_0-k}\) where \( V_{\lambda}\) is the eigenspace of \( \rho(h_3)\) for the eigenvalue \( \lambda\),
		\item \( w_k\neq 0\) if and only if \( k=0,\ldots, n\), so \( \dim(V)=2n+1\)
		\item \( \rho(h_3)w_k=(\lambda_0-k)w_k\)
		\item \( \rho(h_+)w_k=\frac{ 1 }{2}k(2\lambda_0+1-k)w_{k-1}\)
	\end{itemize}
	We choose \( j=\lambda_0\in \eN/2\). Do you believe that the map
	\begin{equation}
		\begin{aligned}
			\phi\colon V & \to V_j         \\
			w_k          & \mapsto v_{j-k}
		\end{aligned}
	\end{equation}
	provides an equivalence of representations between \( \rho_j\) and \( \rho\) ? No ? Ok. We check that for every \( X\in \gsl(2,\eC)\) we have
	\begin{equation}
		\phi\circ\rho(X)=\rho_j(X)\circ \phi.
	\end{equation}
	For \( X=h_3\) we have
	\begin{equation}
		\phi\circ\rho(h_3)w_k=\phi\big( (\lambda_0-k)w_k \big)=(\lambda_0-k)v_{j-k}=(j-k)v_{j-k}
	\end{equation}
	while
	\begin{equation}
		\rho_j(h_3)\phi(w_k)=\rho_j(h_3)v_{j-k}=(j-k)v_{j-k}.
	\end{equation}
	Ok for the first one. Next: \( X=h_+\). We have
	\begin{equation}
		\phi\circ\rho(h_+)w_k=\phi\big( \frac{ 1 }{2}k(2j+1-k)w_{k-1} \big)=\frac{ 1 }{2}k(2j+1-k)v_{j-k+1}
	\end{equation}
	while
	\begin{equation}
		\rho_j\phi(w_k)=\rho_j(h_+)v_{j-k}=\frac{ 1 }{2}\big( j-(j-k) \big)\big( j+(j-k)+1 \big)v_{j-k+1}=\frac{ 1 }{2}k(2j-k+1)v_{j-k+1}.
	\end{equation}
	Ok again. And last one: \( X=h_-\); we have
	\begin{equation}
		\phi\circ\rho(h_-)w_k=\phi(w_{k+1})=v_{j-k-1}
	\end{equation}
	while
	\begin{equation}
		\rho_j(h_-)\phi(w_k)=\rho_j(h_-)v_{j-k}=v_{j-k-1}.
	\end{equation}
	Done\footnote{Now that we reached the end, I recognize that I did not belive neither until the last check.}.
\end{proof}

\begin{normaltext}
	The representations \( \rho_j\) of the theorem \ref{THOooSRQYooXQDZpT} are not yet hermitian for two reasons.
	\begin{itemize}
		\item The representations we expect to be hermitian are the ones of \( \su(2)\). The basis \( \{ h_2,h_{+}, h_{-} \}\) of \( \su(2)_{\eC}\) defined by \eqref{EQSooORIBooAsgdDp} is made of elements which do not belong to \( \su(2)\).
		\item We did not defined a scalar product on \( V_j\); thus the notion of «hermitian» makes no sene.
	\end{itemize}
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Representations of \texorpdfstring{$ \su(2)$}{su(2)}}
%---------------------------------------------------------------------------------------------------------------------------

We know every representations of \( \su(2)_{\eC}\) by the theorem \ref{THOooSRQYooXQDZpT}. Let \( \rho\colon \su(2)\to \End(V)\) be an irreducible representation of \( \su(2)\). By lemma \ref{LEMooIGAFooTSUsJR}, there exists an irreducible representation \( \rho'\colon \su(2)_{\eC}\to \End(V)\) such that \( \rho=\rho'|_{\su(2)}\).

Thus there exists a \( j\) such that \( \rho(X)=\rho_j(X)\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Haar measure on \texorpdfstring{$\SU(2)$}{SU2}}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The quaternion\index{quaternion} field $\eH$ can be embed in $\eM_2(\eC)$ as a genera element reads
\begin{equation}
	q=
	\begin{pmatrix}
		\alpha     & \beta      \\
		-\bar\beta & \bar\alpha
	\end{pmatrix}
\end{equation}
with $\alpha$, $\beta\in\eC$. Under that isomorphism, we have
\[
	| q |^2=| \alpha |^2+| \beta |^2=\det q.
\]
Thus we have the identification
\begin{equation}
	\SU(2)=\{ q\in\eH\tq | q |=1 \}.
\end{equation}
We can act on $\eH$ by $\SU(2)\times \SU(2)$ by
\begin{equation}
	(u,v)\cdot q=uqv^{-1}
\end{equation}
for every $(u,v)\in \SU(2)\times\SU(2)$ and $q\in\eH$. That action defines an homomorphism from $\SU(2)\times\SU(2)$ onto $O(4)$.

\begin{proposition}
	The previously defined homomorphism
	\[
		\phi\colon \SU(2)\times\SU(2)\to O(4).
	\]
	is surjective over $\SO(4)$ (which is the identity component of $O(4)$) and, moreover, the kernel is $\big\{  (e,e),(-e,-e) \big\}$.
\end{proposition}

\begin{proof}
	The group $\SU(2)\times \SU(2)$ being connected, its image can only be included in $\SO(4)$. Let us first determine the kernel of $\phi$. If $(u,v)\in\ker\phi$, we have $uqv^{-1}=q$ for every $q\in\eH$. In particular, with $q=1$, we find $u=v$. Then the relation $uqu^{-1}=q$ means that $u$ belongs to the center of $\eH$, which is $\eR$. We conclude that $u=\pm 1$. That proves that $\ker\phi=\big\{  (e,e),(-e,-e) \big\}$.

	The differential $(d\phi)_{(e,e)}$ is an homomorphism
	\[
		d\phi\colon \gsu(2)\oplus\gsu(2)\to \so(4).
	\]
	Let $(S,T)\in\gsu(2)\oplus\gsu(2)$, we have
	\[
		d\phi(S,T)q=\Dsdd{ \phi( e^{t(S,T)})q }{t}{0}=\Dsdd{ \phi( e^{tS}, e^{tT})q }{t}{0}=\Dsdd{  e^{tS}q e^{-tT} }{t}{0}=Sq-qT,
	\]
	on which one sees that $d\phi$ is injective. Moreover we have $\dim\big( \gsu(2)\oplus\gsu(2) \big)=6=\dim\so(4)$. An injective map between vector space of same dimension being an isomorphism, the image of $\phi$ contains a neighborhood of identity in $\SO(4)$. From connectedness of $\SO(4)$, that neighborhood generates the whole group (see proposition~\ref{PropUssGpGenere}), so that $\phi$ is in fact surjective.
\end{proof}

Since the map $\phi\colon \SU(2)\times \SU(2)\to \SO(4)$ is a surjective homomorphism with a discrete kernel, we have an isomorphism at the algebra level:
\[
	\so(4)\simeq \gsu(2)\oplus\gsu(2).
\]

\subsection{Building some representations for \texorpdfstring{$\SU(2)$}{SU2}}
%/////////////////////////////////////////////////////////////////////////////////

Since $\SU(2)$ acts on $\eC^2$, we can build a representation of $\SU(2)$ on functions on $\eC^2$. We define $\dpt{T}{SU(2)}{\End\big(\Cinf(\eC^2)\big)}$ by
\[
	(T(U)f)(\xi)=f(U^{-1}\xi),
\]
if $f\in\Cinf\big(\eC^2\big)$, $\xi\in\eC^2$ and $U\in SU(2)$.

Let $V_j$ be the space of the homogeneous polynomials of degree $j$ on $\eC^2$; a basis of this space is given by the $\phi_{pq}$, $p+q=2j$ defined by
\begin{equation}
	\phi_{pq}(\xi)=\xi_1^p\xi_2^q
\end{equation}
($\xi=\xi_1+i\xi_2$). If $j$ is fixed, we will often write $\phi_m$ instead of $\phi_{pq}$. The signification is $p=j+m$, $q=j-m$, and $m$ takes its values in $-j,\ldots,j$. Note that $p-q=2m$. It is clear that if $A$ is any invertible $2\times 2$ matrix , and $f\in V_j$, then
\[
	\rho(A)f:=f(A^{-1} \cdot)
\]
is still an element of $V_j$. This representation $\rho$ is defined on the whole $\Cinf(\eC^2)$. We will descent it to $V_j$ later.
Now, we fix $j$ and a $m$ between $-j$ and $j$.

Consider the diagonal matrix
\[   U_{-\theta}=\begin{pmatrix}
		e^{-i\theta} & 0           \\
		0            & e^{i\theta}
	\end{pmatrix} \in\SU(2).
\]
One has
\begin{equation}
	\left(\rho(U_{-\theta})\phi_{pq}\right)(\xi)=\phi_{pq}
	\begin{pmatrix}
		e^{i\theta}\xi_1 \\
		e^{-i\theta}\xi_2
	\end{pmatrix}
	= e^{pi\theta} e^{-qi\theta}\xi_1^p\xi_2^q
	=e^{2mi\theta}\phi_{pq}(\xi).
\end{equation}
First conclusion: the $\phi$'s are eigenvectors of $\rho(U_{-\theta})$ because
\[
	\rho(U_{-\theta})\phi_m=e^{2mi\theta}\phi_m.
\]
Second, the trace of $\rho(U_{-\theta})$ is
\begin{equation}
	\chi_j(\theta)=\sum_{m=-s}^{s}e^{2mi\theta}.
\end{equation}
By the way, the $\chi_j$ are the characters of the representation $\rho$.

From considerations about the Haar\quextproj{} invariant measure on $\SU(2)$, one knows that the good notion product between functions is:
\begin{equation}
	(f_1,f_2)_{\SU(2)}=\frac{2}{\pi}\int_0^{\pi}f_1(\theta)\overline{ f_2(\theta) }\sin^2\theta\,d\theta,
\end{equation}
so that $(\chi_j,\chi_j)=1$. This and the fact that $\SU(2)$ is compact make the theorem of Peter-Weyl (cf. \cite{Sternberg}) applicable, thus the restrictions of $\rho$ to the $V_j$'s are irreducible and moreover, these provide \emph{all} the irreducible representations.

\subsection{Special case: \texorpdfstring{$j=\frac{1}{2}$}{j=1/2}}
%//////////////////////////////////////////////////////////////////////

Consider a matrix $A\in\SU(2)$:
\begin{equation}
	A=\begin{pmatrix}
		\oalpha & -\beta \\
		\obeta  & \alpha
	\end{pmatrix},\qquad
	A^{-1}=\begin{pmatrix}
		\alpha  & \beta   \\
		-\obeta & \oalpha
	\end{pmatrix}.
\end{equation}
A basis of $V_{\frac{1}{2}}$ is given by $\phi_{10}$ and $\phi_{01}$. Let us see how $\rho(A)$ acts on. Since
\[
	A^{-1}\begin{pmatrix}
		\xi_1 \\
		\xi_2
	\end{pmatrix}=
	\begin{pmatrix}
		\alpha\xi_1+\beta\xi_2 \\
		-\obeta\xi_1+\oalpha\xi_2
	\end{pmatrix},
\]
we find
\begin{equation}
	\begin{split}
		(\rho(A)\phi_{10})(\xi)&=\alpha\xi_1+\beta\xi_2=(\alpha\phi_{10}+\beta\phi_{01})(\xi)\\
		(\rho(A)\phi_{01})(\xi)&=-\obeta\xi_1+\oalpha\xi_2=(-\obeta\phi_{10}+\oalpha\phi_{01})(\xi).
	\end{split}
\end{equation}
Thus in the basis $\{\phi_{10},\phi_{01}\}$, the matrix of $\rho(A)$ is given by
\begin{equation}
	\rho(A)=\begin{pmatrix}
		\alpha & -\obeta \\
		\beta  & \oalpha
	\end{pmatrix}=\overline{A}.
\end{equation}

Up to here, we were looking at the representation $\rho$ of $\SU(2)$ on the whole set of functions on $\eC^2$, and more precisely, its restriction to $V_j$. We could define the representation $\rho_{\frac{1}{2}}$ as $\rho_{\frac{1}{2}}=\rho|_{V_{\frac{1}{2}}}$, but we will not do it. Our definition is
\begin{equation}
	\rho_{\frac{1}{2}}(A)=\rho(\overline{A})|_{V_{\frac{1}{2}}}.
\end{equation}
Note that
\[
	\begin{pmatrix}
		0 & -1 \\
		1 & 0
	\end{pmatrix}
	A
	\begin{pmatrix}
		0  & 1 \\
		-1 & 0
	\end{pmatrix}=\overline{A},
\]
thus the representation $A\to\rho(\overline{A})|_{V_{\frac{1}{2}}}$ is equivalent to $A\to\rho(A)|_{V_{\frac{1}{2}}}$. This equivalence can also be seen because these two representations have the same characters\quextproj.

The basis $\phi_{pq}$ is orthogonal; we will build an orthonormal one: $e_m(\xi)$ is the vector whose coordinates are
\begin{equation}
	e_m^j(\xi)=\frac{ \xi_1^{j+m}\xi_2^{j-m} }{\sqrt{ (j+m)!(j-m)! }}
\end{equation}
for $m=-j,-j+1,\ldots,j$. The metric to take in order to define $(e_m,e_n)$ is the unique one on $V_j$ which is $\SU(2)$-invariant.

The Newton's formula for the binomial yields:
\begin{equation}
	\begin{split}
		\sum_{m=-s}^s e_m(\xi)\overline{e_m(\eta)}
		&=\sum\frac{ \xi_1^{j+m}\xi_2^{j-m}\oeta_1^{j+m}\oeta_2^{j-m} }{(j+m)!(j-m)!}\\
		&=\us{(2j)!}(\xi_1\oeta_1+\xi_2\oeta_2)^2\\
		&=\us{(2j)!}\scal{\xi}{\eta}^{2j}.
	\end{split}
\end{equation}
But we know that $A\in\SU(2)$ preserves the scalar product: $\scal{A\xi}{A\eta}=\scal{\xi}{\eta}$. Therefore:
\begin{equation}\label{eq:produit_e_m}
	\sum (\rho_j(A)e_m)(\xi)\overline{ (\rho_j(A)e_m)(\eta) }=\sum e_m(\xi)\overline{e_m(\eta)}.
\end{equation}
Now, instead of considering the matrices $\rho_j(A)$ on $V_j$ for the basis $\phi_m$, we looks at the ones with respect to the basis $e_m$:
\begin{equation}
	\rho_j(A)e_m=r(A)^k_me_k;
\end{equation}
in others words, we looks at the representation $A\to r(A)$. The equations \eqref{eq:produit_e_m} makes
\[
	\sum_{m=-j}^j\left(
	r(A)^l_me_l(\xi)\overline{ r(A)^k_me_k(\eta)   }
	-\delta^l_me_l(\xi)\delta^k_me_k(\eta)
	\right)=0.
\]
Since the functions
\begin{equation}
	\begin{aligned}
		e_k\otimes\overline{e_l}\colon \eC^2\times\eC^2 & \to \eC                               \\
		(\xi,\eta)                                      & \mapsto  e_k(\xi)\overline{e_l(\eta)}
	\end{aligned}
\end{equation}
are linearly independent, one gets $\sum_m r(A)^k_l\overline{r(A)^l_m}=\delta^{kl}$, or
\begin{equation}
	r(A)r(A)^*=\mtu,
\end{equation}
the conclusion is that in this basis, the matrices $\rho_j(A)$ are unitary.

\subsection{Clebsch-Gordan}
%//////////////////////////////////////////////////////////////////////

From the knowledge of the characters of $\rho_j$, one can decompose the product $\rho_s\otimes\rho_r$ into irreducible representations. For example,
\[
	V_{\frac{1}{2}}\otimes V_{\frac{1}{2}}=V_0\oplus V_1.
\]
More generally,
\begin{equation}
	V_s\otimes V_r=V_{|r-s|} \oplus V_{|r-s|+1}\oplus\ldots\oplus V_{r+s}.
\end{equation}
For this reason, the representation $\rho_j$ is sometimes called the \defe{spin $j$}{spin!representation!of $\SU(2)$} representation of $\SU(2)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Representations of \texorpdfstring{$\SO(3)$}{SO3}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The group $\SO(3)$ is strongly linked with $SU(2)$ by the following property:
\begin{equation}
	\SO(3)=\frac{SU(2)}{\eZ_2}.
\end{equation}
proved in proposition \ref{PROPooDKPTooBnLflt}.

\begin{lemma}\label{lem:SO_3}
	A representation $\rho_j$ of $SU(2)$ is a representation of $\SO(3)$ if and only if $\rho_j(X)=\id$ for any $X$ in the kernel of the homomorphism $SU(2)\to \SO(3)$, namely: $\rho_j(\pm\mtu)=\id$.
\end{lemma}

\begin{proof}
	We consider $\dpt{\rho_j}{SU(2)}{\End{V_j}}$. By proposition \ref{PROPooDKPTooBnLflt} we have \( SO(3)=\SU(2)/\eZ_2\) and there exists a group homomorphism\footnote{Defined and studied in proposition \ref{PROPooGEHAooPCReoU}.} $\dpt{\psi}{SU(2)}{\SO(3)}$ such that $\psi(\mtu)=\psi(-\mtu)=\mtu$, which is an important equation because it ensures us that the rest of the expressions are well defined with respect to the class representative.

	If $\rho_j(-\mtu)=\mtu$, we define $\dpt{d_j}{\SO(3)}{\End{V}}$ by $d_j([x])=\rho_j(x)$ (check that this is well defined). With this,
	\[
		d_j([x])d_j([y])=\rho_j(x)\rho_j(y)=\rho_j(xy)=d_j([xy]).
	\]

	Now let us suppose that $d_j([x])=\rho_j(x)$ is a representation. Thus
	\[
		\rho_j(x)=d_j([x])=d_j([-x])=\rho_j(-x)=\rho_j(-\mtu)\rho_j(x),
	\]
	so $\rho_j(-\mtu)=\id_{V_j}$.
\end{proof}

Moreover, any representation of $\SO(3)$ comes from a representation $\tilde\rho$ of $SU(2)$ by setting $\tilde\rho(-\mtu)=\id$ and $\tilde\rho(x)=\rho([x])$.

Now, we research the representations of $SU(2)$ for which the matrix $-\mtu$ is represented by the identity operator. These will be representations of $\SO(3)$. The spin $j$ representations of $SU(2)$ is given by
\begin{equation}
	\rho_j(X)\phi_{pq}(\xi)=\phi_{pq}(X^{-1}\xi).
\end{equation}
With $X=-\mtu$, this gives: $\phi_{pq}(-\xi)=(-1)^{p+q}\phi_{pq}(\xi)$. If we want it to be equal to $\phi_{pq}(\xi)$, we need $p+q=2j$ even. This is true if and only if $j\in\eN$.

\begin{normaltext}      \label{NORMooHWAYooPlSDOp}
	The conclusion is that the irreducible representations of $\SO(3)$ are the integer spin irreducible representations of $\SU(2)$. Note that the non relativistic mechanics has $\SO(3)$ as group of space symmetry. Thus there are no hope to find any half integer spin in a non relativistic theory.
\end{normaltext}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Lorentz group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
	We consider the vector space \( \eR^4\) with its usual scalar product \( \langle ., .\rangle \) which is positive defined. Using the matrix\footnote{The plus/minus convention is, by  far, not universal. As an example, \cite{BIBooYTTJooYpPYLT} uses the opposite.}
	\begin{equation}
		\eta=\begin{pmatrix}
			1 &    &    &    \\
			  & -1 &    &    \\
			  &    & -1 &    \\
			  &    &    & -1
		\end{pmatrix},
	\end{equation}
	we introduce the \defe{Minkowskian product}{Minkowsky product}
	\begin{equation}    \label{EQooQAXNooXhGUQV}
		x\cdot y=\langle \eta x, y\rangle .
	\end{equation}
	This product is not positive defined and is often called «pseudo-scalar product».
\end{definition}

\begin{lemma}       \label{LEMooICEYooNcjJjD}
	A map \( \Lambda\colon \eR^4\to \eR^4\) such that
	\begin{equation}
		\Lambda x\cdot \Lambda y=x\cdot y
	\end{equation}
	for every \( x,y\in  \eR^4\) is linear.
\end{lemma}

\begin{proof}
	The bilinear form
	\begin{equation}
		\begin{aligned}
			b\colon \eR^4\times \eR^4 & \to \eR          \\
			x,y                       & \mapsto x\cdot y
		\end{aligned}
	\end{equation}
	is non degenerated. An element of \( \gO(3,1)\) satisfy \( b(\Lambda x,\Lambda y)=b(x,y)\). Thus the theorem \ref{ThoDsFErq} says that \( \Lambda\) must be linear.
\end{proof}

\begin{lemmaDef}
	The set of maps \( \Lambda\colon \eR^4\to \eR^4\) such that
	\begin{equation}        \label{EQooLPXWooNgrAXz}
		\Lambda x\cdot \Lambda y=x\cdot y
	\end{equation}
	for every \( x,y\in  \eR^4\) is a group.

	This group is named the \defe{Lorentz group}{Lorentz group} and is denoted by \( \gO(3,1)\) or \( L\).
\end{lemmaDef}

\begin{proof}
	From lemma \ref{LEMooICEYooNcjJjD} we know that the elements of \( \gO(3,1)\) are linear operators.
	\begin{enumerate}
		\item
		      The identify is part of \( \gO(3,1)\).
		\item
		      The product is associative.
		\item
		      The only tricky part is to prove that if \( \Lambda\in \gO(3,1)\), then \( \Lambda\) is invertible and \( \Lambda^{-1}\in \gO(3,1)\).
	\end{enumerate}
	Let \( z\neq 0\in \eR^4\) being such that \( \Lambda z=0\), then there exists \( y\in \eR^4\) such that \( z\cdot y\neq 0\) while obviously \( \Lambda z\cdot \Lambda y=0\). Thus every element in \(  \gO(3,1)\) is invertible.

	Let \( \Lambda\in \gO(3,1)\), \( x,y\in \eR^4\). Using the fact that \( \Lambda\in \gO(3,1)\) we have
	\begin{equation}
		x\cdot y= \Lambda(\Lambda^{-1} x)\cdot\Lambda(\Lambda^{-1}y)=\Lambda^{-1}x\cdot \Lambda^{-1}y,
	\end{equation}
	so that \( \Lambda^{-1}\in \gO(3,1)\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Adjoint map}
%---------------------------------------------------------------------------------------------------------------------------

We have a notational issue here. We already defined the adjoint map \( A^*\) by
\begin{equation}
	\langle Ax, y\rangle =\langle x, A^*y\rangle
\end{equation}
for the usual scalar product. Since the main product we consider now in \( \eR^4\), is the Minkowskian one, we will define \( \Lambda^*\) by \( \Lambda x\cdot y=x\cdot \Lambda^*y\) and leave the notation \( A^t\) for the adjoint with respect to the usual scalar product.

\begin{propositionDef}
	Let \( A\in \aL(\eR^4)\). There exists a unique operator \( B\in \aL(\eR^4)\) such that
	\begin{equation}
		Ax\cdot y=x\cdot By
	\end{equation}
	for every \( x,y\in\eR^4\).

	This operator is called \defe{adjoint}{adjoint in Minkowsky space}, is written \( A^*\) and is given by
	\begin{equation}        \label{EQooPFPGooXiGcXs}
		A^*=\eta A^t\eta.
	\end{equation}
\end{propositionDef}

\begin{proof}
	For the existence, we just have to check that \( B=\eta A^t\eta\) works. Using the fact that \( \eta^t=\eta\) and \( \eta^2=\mtu\),
	\begin{subequations}
		\begin{align}
			x\cdot \eta A^t\eta y & =\langle \eta x, \eta A^t\eta y\rangle \\
			                      & =\langle x, A^t\eta y\rangle           \\
			                      & =\langle Ax, \eta y\rangle             \\
			                      & =\langle \eta Ax, y\rangle             \\
			                      & =Ax\cdot y.
		\end{align}
	\end{subequations}
	For the unicity, we suppose $Ax\cdot y=x\cdot By$ for every \( x,y\in \eR^4\). We have
	\begin{equation}
		x\cdot By=Ax\cdot y=x\cdot \eta A^t\eta y
	\end{equation}
	Since the product is non degenerate, this implies \( B=\eta A^t\eta\).
\end{proof}

\begin{lemma}       \label{LEMooVRWJooPsDRwU}
	The adjoint operator satisfy \( (\Lambda^*)^*=\Lambda\).
\end{lemma}

\begin{proof}
	We use the formula \eqref{EQooPFPGooXiGcXs}: $(\Lambda^*)^*=\eta (\Lambda^*)^t\eta=\eta(\eta \Lambda^t\eta)^t\eta=\eta\eta^t\Lambda\eta^t\eta=\Lambda$.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooDLWDooWCXlWq}
	If \( \Lambda\in \aL(\eR^4,\eR^4)\), the following are equivalent:
	\begin{enumerate}
		\item \( \Lambda\in\gO(3,1)\),      \label{ITEMooWHGKooFPfujT}
		\item \( \Lambda^*\in \gO(3,1)\),   \label{ITEMooNISDooMajEMS}
		\item \( \Lambda^*\Lambda=\mtu\),       \label{ITEMooNLZGooUINRiP}
		\item \( \Lambda\Lambda^*=\mtu\),       \label{ITEMooFFRVooOwLmnz}
		\item \( \Lambda^t\eta\Lambda=\eta\),       \label{ITEMooOYTDooCWImBJ}
		\item \( \Lambda\eta\Lambda^t=\eta\).       \label{ITEMooAEEYooDiJuEi}
	\end{enumerate}
\end{lemma}

\begin{proof}
	We prove the equivalences.
	\begin{subproof}
		\spitem[\ref{ITEMooWHGKooFPfujT} implies \ref{ITEMooNLZGooUINRiP}]
		Since \( \Lambda\in \gO(3,1)\) we have $x\cdot y=\Lambda x\cdot \Lambda y=x\cdot \Lambda^*\Lambda y$ for every \( x,y\in \eR^4\). This implies \( \Lambda^*\Lambda=\mtu\).
		\spitem[\ref{ITEMooNLZGooUINRiP} implies \ref{ITEMooWHGKooFPfujT}]
		Since \( \Lambda^*\Lambda=\mtu\) and the fact that \( (\Lambda^*)^*\) (lemma \ref{LEMooVRWJooPsDRwU}) we have
		\begin{equation}
			x\cdot y=\Lambda^*\Lambda x\cdot y=\Lambda x\cdot \Lambda y.
		\end{equation}
		This shows that \( \Lambda\in \gO(3,1)\).
		\spitem[\ref{ITEMooNLZGooUINRiP} if and only if \ref{ITEMooFFRVooOwLmnz}]
		Le corolaire \ref{CORooNFJLooJtzFwN} nous dit que \( AB=\mtu\) if and only if \( BA=\mtu\).
		\spitem[\ref{ITEMooNISDooMajEMS} if and only if \ref{ITEMooFFRVooOwLmnz}]
		Same proof as \ref{ITEMooWHGKooFPfujT} if and only if \ref{ITEMooNLZGooUINRiP}.
	\end{subproof}
	At this point, we have the equivalence between \ref{ITEMooWHGKooFPfujT}, \ref{ITEMooNISDooMajEMS}, \ref{ITEMooNLZGooUINRiP} and \ref{ITEMooFFRVooOwLmnz}.
	\begin{subproof}
		\spitem[\ref{ITEMooNLZGooUINRiP} implies \ref{ITEMooOYTDooCWImBJ}]
		We plug the expression \eqref{EQooPFPGooXiGcXs} of the adjoint in the equation \( \Lambda^*\Lambda=\mtu\):
		\begin{equation}
			\mtu=\Lambda^*\Lambda=\eta\Lambda^t\eta\Lambda.
		\end{equation}
		Multiplying by \( \eta\) on the left, we get the result.
		\spitem[\ref{ITEMooOYTDooCWImBJ} implies \ref{ITEMooNLZGooUINRiP}]
		We write \( \Lambda^t\eta\Lambda=\eta\) and we multiply by \( \eta\).
		\spitem[\ref{ITEMooFFRVooOwLmnz} if and only if \ref{ITEMooAEEYooDiJuEi}]
		It is the same as \ref{ITEMooOYTDooCWImBJ} if and only if \ref{ITEMooNLZGooUINRiP}.
	\end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Structure}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooBMDUooHoYYae}
	Elements \( \Lambda\in\gO(3,1)\) satisfy \( \det(\Lambda)=\pm1\).
\end{lemma}

\begin{proof}
	Taking the determinant on both sides of \( \Lambda^*\eta\Lambda=\eta\) (lemma \ref{LEMooDLWDooWCXlWq}), we get \( \det(\Lambda^*)\det(\Lambda)=1\). Since the operator \( \Lambda\) is real, the proposition \ref{PROPooSHZMooGwdfBd} says that \( \det(\Lambda^*)=\det(\Lambda)^*=\det(\Lambda)\). Thus \( \det(\Lambda)^2=1\). The result follows.
\end{proof}

\begin{lemma}       \label{LEMooHRNXooJOgfpy}
	Let \( \Lambda\in \gO(3,1)\). We have
	\begin{subequations}
		\begin{align}
			\Lambda_{00}^2-\sum_{k=0}^3(\Lambda_{k0})^2 & =1      \label{SUBEQooFLECooUFvwOy} \\
			\Lambda_{00}^2-\sum_{k=0}^3(\Lambda_{0k})^2 & =1  \label{SUBEQooBLTOooPUTztZ}
		\end{align}
	\end{subequations}
	In particular, \( \Lambda_{00}^2\geq 1\).
\end{lemma}

\begin{proof}
	Just write the \( 00\) component of the equation \( \Lambda^t\eta\Lambda=\eta\) (lemma \ref{LEMooDLWDooWCXlWq}):
	\begin{subequations}
		\begin{align}
			1 & =\sum_{kl}\Lambda^t_{0k}\underbrace{\eta_{kl}}_{\eta_{kk}\delta_{kl}}\Lambda_{l0} \\
			  & =\sum_k\Lambda_{0k}^t\eta_{kk}\Lambda_{k0}                                        \\
			  & =\Lambda_{00}^t\Lambda_{00}-\sum_{k=1}^3\Lambda^t_{0k}\Lambda_{k0}                \\
			  & =\Lambda_{00}^2+\sum_{k=1}^3\Lambda_{k0}^2.
		\end{align}
	\end{subequations}
	This is \eqref{SUBEQooFLECooUFvwOy}; the same computation from \( \Lambda\eta\Lambda^t=\eta\) provides \eqref{SUBEQooBLTOooPUTztZ}.
\end{proof}

\begin{lemma}       \label{LEMooEKXWooLEMBIj}
	The set
	\begin{equation}
		L^{\uparrow}=\gO(3,1)^{\uparrow}=\{ \Lambda\in\gO(3,1)\tq \Lambda_{00}\geq 1 \}
	\end{equation}
	is a subgroup of \( \gO(3,1)\).

	Elements of \( \gO(3,1)^{\uparrow}\) are said \defe{orthochronous}{orthochronous}. We will not use the notation \( L^{\uparrow}\).
\end{lemma}

\begin{proof}
	Let \( A,B\in L^{\uparrow}\). We have
	\begin{equation}        \label{EQooBUMZooFGCDEa}
		(AB)_{00}=\sum_{k=0}^3A_{0k}B_{k0}=A_{00}B_{00}+\sum_{k=1}^3A_{0k}B_{k0}.
	\end{equation}
	Using lemma \ref{LEMooHRNXooJOgfpy}, we write
	\begin{equation}        \label{EQooBFWTooZNIdlP}
		A_{00}^2=1+\sum_{k=1}^3A_{k0}^2
	\end{equation}
	and
	\begin{equation}
		B_{00}^2=1+\sum_{k=1}^3B_{0k}^2.
	\end{equation}
	Since \( A_{00}\geq 1\), taking the square root of \eqref{EQooBFWTooZNIdlP} does not require additional caution:
	\begin{equation}
		A_{00}>\sqrt{ \sum_{k=1}^3A_{k0}^2 }.
	\end{equation}
	The same holds for \( B\). Using these (strict) inequalities in \eqref{EQooBUMZooFGCDEa} we have
	\begin{subequations}
		\begin{align}
			(AB)_{00}>\sqrt{ \sum_{k=1}^3A_{k0}^2 }\sqrt{ \sum_{k=1}^3B_{0k}^2 }+\sum_{k=1}^3A_{0k}B_{k0}.
		\end{align}
	\end{subequations}
	If we set \( a=\sum_{k=1}^3A_{k0}e_k\) and \( b=\sum_{k=1}^3B_{0k}e_k\) (here \( e_i\in \eR^3\)), we have
	\begin{equation}
		(AB)_{00}>\| a \|\| b \|+\langle a, b\rangle \\
		\geq \| a \|\| b \|-| \langle a, b\rangle  |\\
		\geq 0
	\end{equation}
	because of the Cauchy-Schwarz identity, theorem \ref{ThoAYfEHG}. The strict inequality \( (AB)_{00}>0\) implies the inequality \( (AB)_{00}\geq 1\) because \( AB\in \gO(3,1)\) (see lemma \ref{LEMooHRNXooJOgfpy}).
\end{proof}

\begin{lemma}       \label{LEMooLJMMooOXCyOl}
	About adjoint.
	\begin{enumerate}
		\item
		      If \( \Lambda\in \gO(3,1)\), then \( \Lambda^*\in \gO(3,1)\).
		\item
		      If \( \Lambda\in \SO(3,1)\), then \( \Lambda^*\in \SO(3,1)\).
		\item
		      If \( \Lambda\in \gO(3,1)^{\uparrow}\), then \( \Lambda^*\in \gO(3,1)^{\uparrow}\).
	\end{enumerate}
	In particular, if \( \Lambda\in \SO(3,1)^{\uparrow}\), then \( \Lambda^*\in \SO(3,1)^{\uparrow}\).
\end{lemma}

\begin{proof}
	Just compute \( \Lambda^*_{00}\) and \( \det(\Lambda^*)\) with the formula \eqref{EQooPFPGooXiGcXs}.
\end{proof}

\begin{definition}          \label{DEFooVQLPooWyINoc}
	A \defe{boost}{boost} in the direction \( x\) is an element \( \Lambda\in \SO(3,1)^{\uparrow}\) such that \( \Lambda(e_2)=e_2\) and \( \Lambda(e_3)=e_2\). In other words, this is a transformations which only involves the components \( t\) and \( x\).

	The boost in the directions \( y\) and \( z\) are defined in a similar way.

	A \defe{spatial rotation}{spatial rotation} is an element \( \Lambda\in \gO(3,1)\) such that \( \Lambda(e_0)=e_0\).
\end{definition}

\begin{lemma}
	An operator \( \Lambda\colon \eR^4\to \eR^4\) is a boost if and only if there exists \( \gamma\in \eR\) such that the matrix of \( \Lambda\) has the form
	\begin{equation}
		\Lambda=\begin{pmatrix}
			\cosh(\gamma) & \sinh(\gamma) & 0 & 0 \\
			\sinh(\gamma) & \cosh(\gamma) & 0 & 0 \\
			0             & 0             & 1 & 0 \\
			0             & 0             & 0 & 1
		\end{pmatrix}.
	\end{equation}
\end{lemma}

\begin{proof}
	It is easy to see that the proposed matrix is a boost. The only difficult part is the direct sense. We suppose that \( \Lambda\) is a boost. The conditions \( \Lambda e_2=e_2\) and \( \Lambda e_3=e_3\) imply that the matrix of \( \Lambda\) has the form
	\begin{equation}
		\Lambda=\begin{pmatrix}
			. & . & 0 & 0 \\
			. & . & 0 & 0 \\
			. & . & 1 & 0 \\
			. & . & 0 & 1
		\end{pmatrix}
	\end{equation}
	where the dots are to be determined. Since \( \Lambda\in\gO(3,1)\) we have
	\begin{equation}
		0=e_0\cdot e_2=\Lambda e_0\cdot \Lambda e_2=\Lambda e_0\cdot e_2=-\Lambda_{20}.
	\end{equation}
	The same shows that \( \Lambda_{20}=\Lambda_{21}=\Lambda_{30}=\Lambda_{31}=0\). The matrix of \( \Lambda\) is block diagonal:
	\begin{equation}
		\Lambda=\begin{pmatrix}
			a & c & 0 & 0 \\
			b & d & 0 & 0 \\
			0 & 0 & 1 & 0 \\
			0 & 0 & 0 & 1
		\end{pmatrix}
	\end{equation}
	where \( a\), \( b\), \( c\) and \( d\) are still to be determined. Here are the constrains.

	First the operator \( \Lambda\) preserve the product, so that \( \Lambda e_0\cdot \Lambda e_0=1\), \( \Lambda e_1\cdot \Lambda e_1=-1\), and \( \Lambda e_0\cdot \Lambda e_1=0\). These conditions are translated into
	\begin{subequations}
		\begin{align}
			a^2-c^2 & =1  \label{SUBEQooWEJSooPWfmNS}      \\
			d^2-b^2 & =1     \label{SUBEQooKLZFooCovszD}   \\
			ab-cd   & =0.      \label{SUBEQooUZQWooUxUCSe}
		\end{align}
	\end{subequations}
	Second, the operator \( \Lambda\) has determinant equals to \( 1\):
	\begin{equation}        \label{EQooJAEKooTCZaIG}
		ad-bc=1,
	\end{equation}
	and finally the element \( \Lambda\) is orthochronous: \( \Lambda_{00}\geq 0\), so that
	\begin{equation}        \label{EQooQCMYooPhHeas}
		a\geq 0.
	\end{equation}

	The proposition \ref{PROPooWEHGooOBqSHY} about hyperbolic functions along with the conditions \eqref{SUBEQooWEJSooPWfmNS} and \ref{SUBEQooKLZFooCovszD} show that there exist \( x\in \eR\), \( y\in \eR\), \( \sigma\in\{ \pm1 \}\) and \( \epsilon\in\{ \pm 1 \}\) such that
	\begin{subequations}
		\begin{align}
			a & =\sigma\cosh(x)    \\
			b & =\sinh(y)          \\
			c & =\sinh(x)          \\
			d & =\epsilon\cosh(y).
		\end{align}
	\end{subequations}

	\begin{subproof}
		\spitem[\( \sigma=1\)]
		The condition \eqref{EQooQCMYooPhHeas} show that \( \sigma=1\) because the hyperbolic cosine is always strictly positive.

		\spitem[\( \epsilon=1\)]
		The determinant condition \eqref{EQooJAEKooTCZaIG} provides
		\begin{equation}
			\epsilon\cosh(x)\cosh(y)-\sinh(x)\sinh(y)=1.
		\end{equation}
		If \( \epsilon=-1\) we are left with
		\begin{equation}
			1=-(\cosh(x)\cosh(y)+\sinh(x)\sinh(y))
		\end{equation}
		Using the formula of proposition \ref{PROPooUNHHooIksdoJ}\ref{ITEMooOJRFooUCUaDl} we get $1=-\cosh(x+y)$ which is impossible because the hyperbolic cosine is always positive.
		\spitem[\( x=y\)]
		The orhogonality condition \eqref{SUBEQooUZQWooUxUCSe} implies
		\begin{equation}
			0=\cosh(x)\sinh(y)-\sinh(x)\cosh(y)=-\sinh(x-y).
		\end{equation}
		Since the hyperbolic sine is bijective (proposition \ref{PROPooQLNYooIIOdvm}) we deduce \( x-y=0\) and then \( x=y\).
	\end{subproof}
\end{proof}

We define the projection from \( \eR^4\) to \( \eR^3\) as
\begin{equation}
	\pr(x)=(x_1,x_2,x_3)\in \eR^3
\end{equation}
and, if \( b\in \eR^3\) we write
\begin{equation}
	\bar b=(0,b_1,b_2,b_3).
\end{equation}
So if \( b\in \eR^3\) we have
\begin{equation}        \label{EQooOIBWooAvxfYz}
	\bar b\cdot x=-\langle b, \pr(x)\rangle .
\end{equation}

\begin{proposition}[Standard decomposition\cite{BIBooYTTJooYpPYLT}]     \label{PROPooYADMooQOTpWX}
	Every operator \( \Lambda\in \SO(1,3)^{\uparrow}\) can be decomposed as
	\begin{equation}
		\Lambda=RLS
	\end{equation}
	where \( R\) and \( S\) are spatial rotations and \( L\) is a boost in the \( x\) direction\footnote{Definition \ref{DEFooVQLPooWyINoc}.}.
\end{proposition}

\begin{proof}
	We initiate with \( a=\pr(\Lambda e_0)\). If \( a\neq 0\) we define \( b_1=\frac{ a }{ \| a \| }\in \eR^3\) and we consider \( b_2\) and \( b_3\) in \( \eR^3\) such that \( \{ b_1, b_2, b_3 \}\) is an orthonormal basis of \( \eR^3\) with the same orientation\footnote{Definition \ref{DEFooNVRHooEBHUSu}.} as the canonical basis..

	\begin{subproof}
		\spitem[The first spatial rotation]
		Now we define the spatial rotation \( R\colon \eR^4\to \eR^4\) by
		\begin{subequations}
			\begin{numcases}{}
				Re_0=e_0\\
				Re_i=\bar b_i.
			\end{numcases}
		\end{subequations}
		Since the basis \( \{ \bar b_i \}\) is positive-oriented, the determinant of \( R\) is positive\footnote{Proposition \ref{PROPooNBAXooKNUrnk}.} and since \( Re_0=e_0\), we have \( R_{00}=1\), so that \( R\in\SO(3,1)^{\uparrow}\).

		\spitem[One property]
		We prove that \( \Lambda^*Re_i\cdot e_0-0\) for \( i=2,3\). For that:
		\begin{subequations}
			\begin{align}
				\Lambda^*Re_i\cdot e_0 & =\Lambda^*\bar b_i\cdot e_0            \\
				                       & =\bar b_i\cdot \Lambda e_0             \\
				                       & =-\langle b_i, \pr(\Lambda e_0)\rangle \\
				                       & =-\langle b_i, \| a \|  b_1\rangle     \\
				                       & =0.
			\end{align}
		\end{subequations}
		We used the relation \eqref{EQooOIBWooAvxfYz} and the fact that \( \{ b_i \} \) is an orthonormal basis of \( \eR^3\).
		\spitem[A new basis]
		We define the following vectors:
		\begin{subequations}
			\begin{align}
				f_0 & =e_0            \\
				f_2 & =\Lambda^*Re_2  \\
				f_3 & =\Lambda^*Re_3.
			\end{align}
		\end{subequations}
		We check that these vectors are orthonormal. First:
		\begin{subequations}
			\begin{align}
				f_0\cdot f_2 & =e_0\cdot f_2                          \\
				             & =e_0\cdot \Lambda^*Re_2                \\
				             & = Re_2\cdot \Lambda e_0                \\
				             & =\bar b_2\cdot \Lambda e_0             \\
				             & =-\langle b_2, \pr(\Lambda e_0)\rangle \\
				             & =0.
			\end{align}
		\end{subequations}
		We get \( f_0\cdot f_3=0\) in the same way. Second:
		\begin{equation}
			f_2\cdot f_3=\Lambda^*Re_2\cdot \Lambda^*Re_3=e_2\cdot e_3=0.
		\end{equation}
		Now we fix \( f_1\) in such a way that \( \{ f_0,f_1,f_2,f_3 \}\) is a pseudo-orthonormal basis of \( (\eR^4,\cdot)\). Up to redefinition \( f_1\to -f_1\) we also suppose that \( \{ f_1,f_2,f_3 \}\) is a basis of \( \eR^3\) with the same orientation as the canonical basis.
		\spitem[The second spatial rotation]
		We define the spatial rotation \( S\colon \eR^4\to \eR^4\) by
		\begin{equation}
			Sf_i=e_i
		\end{equation}
		for \( i=0,1,2,3\).

		Due to our choice of orientation for \( f_1\) we have \( S\in \SO(3,1)^{\uparrow}\).
		\spitem[Boost]
		We show that \( S\Lambda^*R\) is a boost in the \( x\) direction. We have
		\begin{equation}
			S\Lambda^*Re_2=Sf_2=e_2
		\end{equation}
		and the same for \( e_3\): \( S\Lambda^*Re_3=e_3\).

		We made some choices such that \( S\) and \( R\) belong to \( \SO(3,1)^{\uparrow}\). Moreover by hypothesis \( \Lambda\in \SO(3,1)^{\uparrow}\) and the lemma \ref{LEMooLJMMooOXCyOl} implies that \( \Lambda^*\in \SO(3,1)^{\uparrow}\). The whole shows that \( S\Lambda^*R\) is a boost.

		Finally, \( \Lambda^*=S^{-1}LR^{-1}\) and taking onto account the fact that the adjoint is the inverse (lemma \ref{LEMooDLWDooWCXlWq}),
		\begin{equation}        \label{EQooSJNLooWZztHU}
			\Lambda=RL^{-1}S.
		\end{equation}
		The operator \( L^{-1}\) is a boost because \( L\) is a boost.
	\end{subproof}
	The decomposition \eqref{EQooSJNLooWZztHU} is the requested one.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isomorphism $\SO(3,1)^{\uparrow}= \SL(2,\eC)/ \eZ_2 $}.
%---------------------------------------------------------------------------------------------------------------------------

We are now going to prove the isomorphism
\begin{equation}
	\SO(3,1)^{\uparrow}=\frac{ \SL(2,\eC) }{ \eZ_2 }.
\end{equation}

We know from lemma \ref{LEMooZNCQooLgoReX} that the Pauli matrices form a basis of the hermitian \( 2\times 2\) matrices. Here we introduce a new Pauli matrix \( \sigma_0=\id\) :
\begin{equation}
	\begin{aligned}[]
		\sigma_0=\begin{pmatrix}
			         1 & 0 \\
			         0 & 1
		         \end{pmatrix}, &  &
		\sigma_1=\begin{pmatrix}
			         0 & 1 \\
			         1 & 0
		         \end{pmatrix}, &  &
		\sigma_2=\begin{pmatrix}
			         0 & -i \\
			         i & 0
		         \end{pmatrix}, &  &
		\sigma_3=\begin{pmatrix}
			         1 & 0  \\
			         0 & -1
		         \end{pmatrix}.
	\end{aligned}
\end{equation}
We denote by \( \mH\) the spanned vector space \( \mH=\Span\{ \sigma_i \}_{i=0,1,2,3}\).

\begin{normaltext}
	We recall that \( x\cdot x=x_0^2-\sum_{i=1}^3x_i^2\); we could write it as \( \| x \|^2\) but we cannot figure out what \( \| x \| \) should mean.
\end{normaltext}

\begin{lemma}       \label{LEMooXKHYooFTzHhg}
	We define
	\begin{equation}
		\begin{aligned}
			\phi\colon \eR^4   & \to \mH                    \\
			\sum_{i=0}^3x_ie_i & \mapsto \sum_ix_i\sigma_i.
		\end{aligned}
	\end{equation}
	Then
	\begin{enumerate}
		\item
		      The map \( \phi\) is a vector space isomorphism
		\item       \label{ITEMooISFKooItVpre}
		      We have \( \det\big( \phi(x) \big)=x\cdot x\) for every \( x\in \eR^4\).
	\end{enumerate}
\end{lemma}

\begin{normaltext}
	We will often write \( \| x \|^2\) for \( x\cdot x\), but we know that this is an abuse since we don't really want to define \( \| x \|\) itself.

	In an euclidian space like \( \eR\) endowed with its scalar product, we always have \( x\cdot x>0\) so that one can define \( \| x \|=\sqrt{ x\cdot x }\).

	In the Minkowskian space, the situation is quite different because the number \( x\cdot x\) can be positive as well as negative. Thus we will often use the notation \( \| x \|^2\), and if you realty want to, \( \| x \|=\sqrt{ x\cdot x }\) with the convention that \( \sqrt{ - r }=i\sqrt{ r }\) for \( r\in \eR^+\). So, if you want, \( \| e_1 \|=i\).
\end{normaltext}

\begin{proof}
	We have
	\begin{equation}
		\phi(x)=\begin{pmatrix}
			x_0+x_3  & x-1-ix_2 \\
			x_1+ix_2 & x_0-x_3
		\end{pmatrix}
	\end{equation}
	and then
	\begin{subequations}
		\begin{align}
			\det\big( \phi(x) \big) & =(x_0+x_3)(x_0-x_3)-(x_1+ix_2)(x_1-ix_2) \\
			                        & =x_0^2-x_3^2-x_1^2-x_2^2.
		\end{align}
	\end{subequations}
\end{proof}

\begin{lemma}       \label{LEMooHPSJooEVIaoE}
	Let \( S\in \SL(2,\eC)\). We define the map
	\begin{equation}
		\begin{aligned}
			g_S\colon \mH & \to \eM(2,\eC)      \\
			X             & \mapsto SXS^{\dag}.
		\end{aligned}
	\end{equation}
	\begin{enumerate}
		\item
		      The map \( g_S\) is linear,
		\item
		      the target space is included in \( \mH\) : \( g_S(\mH)\subset \mH\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	For the linearity,
	\begin{equation}
		g_S(X+Y)=S(X+Y)S^{\dag}=SXS^{\dag}+SYS^{\dag}=g_S(X)+g_S(Y)
	\end{equation}
	and
	\begin{equation}
		g_S(\lambda X)=S\lambda XS^{\dag}=\lambda SXS^{\dag}=\lambda g_S(X).
	\end{equation}
	Since the map \( g_S\) is linear, it is sufficient to check that \( g_S(\sigma_i)\in \mH\) for each \( i=0,1,2,3\). Each of these matrices is hermitian, we have
	\begin{equation}
		g_S(\sigma_i)^{\dag}=(S\sigma_iS^{\dag})^{\dag}=S\sigma_i^{\dag}S^{\dag}=S\sigma_iS^{\dag}=g_S(\sigma_i).
	\end{equation}
	Thus we have \( g_S(\sigma_i)=g_S(\sigma_i)\) and we deduce that \( g_S(\sigma_i)\) is hermitian.

	When \( i=0\) we have \( g_S(\sigma_0)=S\sigma_0S^{\dag}=SS^{\dag}=\mtu=\sigma_0\).

	When \( i\neq 0\), using the cyclic invariance of the trace\footnote{Lemma \ref{LEMooUXDRooWZbMVN}.}, we also have
	\begin{equation}
		\tr\big( g_S(\sigma_i) \big)=\tr(S\sigma_i S^{\dag})=\tr(S^{\dag}S\sigma_i)=\tr(\sigma_i)=0.
	\end{equation}
	Thus \( g_S(\sigma_i)\) is an hermitian matrix with null trace, so that \( g_S(\sigma_i)\in \mH\).
\end{proof}

Now we can consider the map \( g_S\colon \mH\to \mH\).

\begin{lemma}       \label{LEMooEDIMooNIURdn}
	Let \( S\in \SL(2,\eC)\). We consider the map of the lemma \ref{LEMooHPSJooEVIaoE}
	\begin{equation}
		\begin{aligned}
			g_S\colon \mH & \to \mH            \\
			X             & \mapsto SXS^{\dag}
		\end{aligned}
	\end{equation}
	Then \( \det\big( g_S(X) \big)=\det(X)\).
\end{lemma}

\begin{proof}
	We use the proposition \ref{PROPooWVJFooTmqoec} :
	\begin{equation}
		\det(SXS^{\dag})=\det(S)\det(X)\det(S^{\dag})=\det(X)
	\end{equation}
	because the determinant of \( S\in\SL(2,\eC)\) is \( 1\).
\end{proof}

\begin{lemma}       \label{LEMooXDPPooPImUQX}
	For \( S\in \SL(2,\eC)\) we define
	\begin{equation}
		\begin{aligned}
			\Lambda(S)\colon \eR^4 & \to \eR^4                                            \\
			x                      & \mapsto \phi^{-1}\Big( g_S\big( \phi(x) \big) \Big).
		\end{aligned}
	\end{equation}
	Then \( \Lambda(S)\in \gO(3,1)\).
\end{lemma}

\begin{proof}
	The lemma \ref{LEMooXKHYooFTzHhg}\ref{ITEMooISFKooItVpre} shows that \( \det\big( \phi(y) \big)=\| y \|^2\) for every \( y\in \eR^4\). We use that equation with \( y=\Lambda(S)x\) :
	\begin{subequations}
		\begin{align}
			\| \Lambda(S)x \|^2 & =\| \phi g_S\phi^{-1} x \|^2  \\
			                    & =\det(\phi\phi^{-1}g_S\phi x) \\
			                    & =\det(g_S\phi x)              \\
			                    & =\det\big( \phi(x) \big)      \\
			                    & =\| x \|^2
		\end{align}
	\end{subequations}
	We used the fact that \( \det\big( g_S(X) \big)=\det(X)\) by lemma \ref{LEMooEDIMooNIURdn}.
\end{proof}

\begin{lemma}           \label{LEMooJBYDooSFEUFr}
	The map \( \Lambda\colon \SL(2,\eC)\to \gO(3,1)\) is a group homomorphism.
\end{lemma}

\begin{proof}
	Let \( S,T\in \SL(2,\eC)\) and \( X\in \mH\). We have
	\begin{equation}
		g_{ST}(X)=STX(ST)^{\dag}=STXT^{\dag}S^{\dag}=Sg_T(X)S^{\dag}=(g_Sg_T)(X).
	\end{equation}
	Now we can prove that \( \Lambda\) is an homomorphism:
	\begin{equation}
		\Lambda(ST)=\phi g_{ST}\phi^{-1}=\phi g_Sg_T\phi^{-1}=\phi g_S\phi^{-1}\phi g_T\phi^{-1}=\Lambda(S)\Lambda(T).
	\end{equation}
\end{proof}

\begin{lemma}       \label{LEMooTLQKooGntuRH}
	The map \( \Lambda\colon \SL(2,\eC)\to \gO(3,1)\) is continuous.
\end{lemma}

\begin{proof}
	In several steps.
	\begin{subproof}
		\spitem[If $ S_i\stackrel{\SL(2,\eC)}{\longrightarrow}\mtu$ then $g_{S_i}\stackrel{\End(\mH)}{\longrightarrow}\id$]
		For each \( X\in \mH\) we have \( g_{S_i}X=S_iXS_i^{\dag}\). Using the fact that the matrix product is continus (proposition \ref{PROPooOEETooPhqWuf}), we deduce
		\begin{equation}
			\lim_{i\to \infty} g_{S_i}(X)=X.
		\end{equation}
		The proposition \ref{PROPooDRHMooYzXbkl} concludes that \( g_{S_i}\stackrel{\End(\mH)}{\longrightarrow}\id\).
		\spitem[If $ S_i\stackrel{\SL(2,\eC)}{\longrightarrow}\mtu$ then $ \Lambda(S_i)\stackrel{\gO(3,1)}{\longrightarrow}\id$]
		For each \( i\) we have
		\begin{equation}
			\Lambda(S_i)=\phi^{-1}\circ g_{S_i}\circ \phi.
		\end{equation}
		Since \( \phi\) and \( \phi^{-1}\) are continuous, they commute with the limit and we have
		\begin{equation}
			\lim_{i\to \infty} \Lambda(S_i)=\phi^{-1}\circ\lim_{i\to \infty} g_{S_i}\circ\phi=\phi^{-1}\circ\id\circ \phi=\id.
		\end{equation}

		\spitem[If $ S_i\stackrel{\SL(2,\eC)}{\longrightarrow}g  $ then $ \Lambda(S_i)\stackrel{\gO(3,1)}{\longrightarrow}\Lambda(g)  $  ]
		We successiveny have :
		\begin{subequations}
			\begin{align}
				S_i                         & \stackrel{\SL(2,\eC)}{\longrightarrow}g     \label{SUBEQooPXXZooYJrxqk}                            \\
				g^{-1}S_i                   & \stackrel{\SL(2,\eC)}{\longrightarrow}\mtu        \label{SUBEQooWRHNooNPPceW}                      \\
				\Lambda(g^{-1} S_i)         & \stackrel{\gO(3,1)}{\longrightarrow}\id \label{SUBEQooUJAKooRtfQzE}                                \\
				\Lambda(g^{-1})\Lambda(S_i) & \stackrel{\gO(3,1)}{\longrightarrow}\id \label{SUBEQooUNJDooOwTFhk}                                \\
				\Lambda(S_i)                & \stackrel{\gO(3,1)}{\longrightarrow}\Lambda(g^{-1})^{-1}=\Lambda(g).   \label{SUBEQooSNAVooEeQlHz}
			\end{align}
		\end{subequations}
		Justifications:
		\begin{itemize}
			\item For \eqref{SUBEQooWRHNooNPPceW}, the group law is continuous; thus we can multiply by \( g^{-1}\) to both sides
			\item For \eqref{SUBEQooUJAKooRtfQzE}, we apply \( \Lambda\) and we use the previous step.
			\item For \eqref{SUBEQooUNJDooOwTFhk}, we use the fact that \( \Lambda\) is an homomorphism (lemma \ref{LEMooJBYDooSFEUFr})
			\item For \eqref{SUBEQooSNAVooEeQlHz}, we multiply both sides by \( \Lambda(g^{-1})^{-1}\) and \( \Lambda\) and we use once again the fact that \( \Lambda\) is an homomorphism : \( \Lambda(h)^{-1}=\Lambda(h^{-1})\).
		\end{itemize}
		\spitem[$ \Lambda$ is continuous  ]
		Since \( \SL(2,\eC)\) and \( \gO(3,1)\) are metric spaces, the proposition \ref{PropXIAQSXr} makes sequential continuity (the one we ) equivalent to the continuity. Thus the convergence \eqref{SUBEQooSNAVooEeQlHz} says that \( \Lambda\) is continuous.
	\end{subproof}
\end{proof}


\begin{lemma}
	We have \( \Lambda\big( \SL(2,\eC) \big)\subset \SO(3,1)^{\uparrow}\).
\end{lemma}

\begin{proof}
	The group \( \SL(2,\eC)\) is connected from proposition \ref{PROPooALQCooLZCKrH}, and \( \Lambda\) is continuous by lemme \ref{LEMooTLQKooGntuRH}. Thus the image of \( \SL(2,\eC)\) by \( \Lambda\) is connected by lemma \ref{LemConncontconn}.

	Thus \( \Lambda\big( \SL(2,\eC) \big)\) is contained in the part of \( \SO(3,1)\) which is connected to \( \mtu\), namelly in \( \SO(3,1)^{\uparrow}\).
\end{proof}

Before to prove that \( \Lambda\colon \SL(2,\eC)\to \SO(3,1)^{\uparrow}\) is surjective, we need some computations.

\begin{lemma}
	We have
	\begin{equation}
		e^{t\sigma_1}=\cosh(t )\mtu+\sinh( t )\sigma_1
	\end{equation}
	where \( \sigma_1\) is the Pauli matrix \eqref{DEFooRNTDooTVkPtB}.
\end{lemma}

\begin{proof}
	We have \( \sigma_1^2=\mtu\) and, by definition,
	\begin{equation}
		e^{t\sigma_1}=\sum_{k\in \eN}\frac{ t^k\sigma_1^k }{ k! }.
	\end{equation}
	We use the proposition \ref{PROPooJLQAooAEbIvZ} to split the sum into even and odd terms:
	\begin{subequations}
		\begin{align}
			e^{t\sigma_1} & =\sum_{k\text{ even}}\frac{ t^k\sigma_1^k }{ k! }+  \sum_{k\text{ odd}}\frac{ t^k\sigma_1^k }{ k! } \\
			              & =\sum_{k\text{ even}}\frac{ t^k }{ k! }\sigma_1+  \sum_{k\text{ odd}}\frac{ t^k }{ k! }\mtu         \\
			              & =\sinh(t)\sigma_1+\cosh(t)\mtu.
		\end{align}
	\end{subequations}
\end{proof}

\begin{lemma}       \label{LEMooPKYXooWGZkkG}
	Some results with the matrices \( \{ \sigma_i \}_{i=0,\ldots, 3}\).
	\begin{enumerate}
		\item
		      \(  e^{\gamma \sigma_1/2}\sigma_0 e^{\gamma\sigma_1/2}= e^{\gamma\sigma_1}\)
		\item
		      \(  e^{\gamma \sigma_1/2}\sigma_1 e^{\gamma\sigma_1/2}= \cosh(\gamma)\sigma_1+\sinh(\gamma)\mtu\)
		\item
		      \(  e^{\gamma \sigma_1/2}\sigma_2 e^{\gamma\sigma_1/2}= \sigma_2\)
		\item
		      \(  e^{\gamma \sigma_1/2}\sigma_3 e^{\gamma\sigma_1/2}= \sigma_3\)
	\end{enumerate}
\end{lemma}

\begin{proof}
	These are computations using the formulas of lemma \eqref{LEMooIBJMooTYnooZ} as well as the hyperbolic trigonometry relations of proposition \ref{PROPooUNHHooIksdoJ}. As an example,
	\begin{subequations}
		\begin{align}
			 & \big( \cosh(\gamma/2)\mtu+\sinh(\gamma/2)\sigma_1 \big)\sigma_2\big( \cosh(\gamma/2)\mtu+\sinh(\gamma/2)\sigma_1 \big)                    \\
			 & \quad=
			\big( \cosh(\gamma/2)\sigma_2+i\sinh(\gamma/2)\sigma_3 \big)\big( \cosh(\gamma/2)\mtu+\sinh(\gamma/2)\sigma_1 \big)                          \\
			 & \quad=\cosh(\gamma/2)\sigma_2-i\cosh(\gamma/2)\sinh(\gamma/2)\sigma_3+i\sinh(\gamma/2)\cosh(\gamma/2)\sigma_3+i\sinh^2(\gamma/2)i\sigma_2 \\
			 & \quad=\big( \cosh^2(\gamma/2)-\sinh^2(\gamma/2) \big)\sigma_2                                                                             \\
			 & \quad=\sigma_2.
		\end{align}
	\end{subequations}
\end{proof}

The following lemma provides a clearly non trivial element in \( \Lambda\big( \SL(2,\eC) \big)\).
\begin{lemma}[\cite{BIBooYTTJooYpPYLT}]     \label{LEMooGURFooRTBBmi}
	Let \( \gamma\in \eR\). We write \( S(\gamma)= e^{\gamma\sigma_1/2}\). We have
	\begin{equation}
		\Lambda\big( S(\gamma) \big)=\begin{pmatrix}
			\cosh(\gamma) & \sinh(\gamma) & 0 & 0 \\
			\sinh(\gamma) & \cosh(\gamma) & 0 & 0 \\
			0             & 0             & 1 & 0 \\
			0             & 0             & 0 & 1
		\end{pmatrix}.
	\end{equation}
\end{lemma}

\begin{proof}
	Let \( x\in \eR^4\). We have
	\begin{subequations}
		\begin{align}
			\Lambda\big( S(\gamma) \big)x & =(\phi^{-1}g_{S(\gamma)}\phi)(x)                                   \\
			                              & =(\phi^{-1}\circ g_{S(\gamma)})\big( \sum_{k=0}^3x_k\sigma_k \big) \\
			                              & =\sum_kx_k\phi^{-1}\big( S(\gamma)\sigma_kS(\gamma)^{\dag} \big)   \\
			                              & =\sum_kx_k\phi^{-1}\big( S(\gamma)\sigma_kS(\gamma) \big).
		\end{align}
	\end{subequations}
	At this point, we use the relations of lemma \ref{LEMooPKYXooWGZkkG}. We have:
	\begin{subequations}
		\begin{align}
			\Lambda\big( S(\gamma) \big)x & =x_0\phi^{-1}\big(  e^{\gamma\sigma_1} \big)+x_1\phi^{-1}\big( \cosh(\gamma)\sigma_1+\sinh(\gamma)\mtu \big)
			+x_2\phi^{-1}(\sigma_2)+x_3\phi^{-1}(\sigma_3)                                                                                               \\
			                              & =x_0\cosh(\gamma)e_0+x_0\sinh(\gamma)e_1+x_1\cosh(\gamma)e_1+x_1\sinh(\gamma)e_0+x_2e_2+x_3e_3               \\
			                              & =\big( x_0\cosh(\gamma)+x_1\sinh(\gamma) \big)e_0                                                            \\
			                              & \quad+\big( x_0\sinh(\gamma)+x_1\cosh(\gamma) \big)e_1                                                       \\
			                              & \quad+x_2e_2                                                                                                 \\
			                              & \quad+x_3e_3.
		\end{align}
	\end{subequations}
	The matrix of \( \Lambda\big( S(\gamma) \big)\) is now easy to write.
\end{proof}

\begin{lemma}       \label{LEMooTZPAooWGjMgU}
	We have \( \SO(3)\subset \Lambda\big( \SL(2,\eC) \big)\) where \( \SO(3)\) denotes the block-diagonal matrices of the form
	\begin{equation}
		\begin{pmatrix}
			1 & 0      \\
			0 & \bar A
		\end{pmatrix}
	\end{equation}
	in which \( \bar A\) is a \( 3\times 3\) matrix of \( \SO(3)\).
\end{lemma}

\begin{proof}
	A matrix of \( \SO(3)\) in our context has the form
	\begin{equation}
		\begin{pmatrix}
			1               & \begin{pmatrix}
				                  0 & 0 & 0
			                  \end{pmatrix} \\
			\begin{pmatrix}
				0 \\
				0 \\
				0
			\end{pmatrix} & \bar A
		\end{pmatrix}
	\end{equation}
	where \( \bar A\) is a \( 3\times 3\) matrix of the authentic \( \SO(3)\). Now we use the proposition \ref{PROPooGEHAooPCReoU}\ref{ITEMooZSSHooDUCqSQ} addind bars everywhere to distinguish the objects about \( \eR^3\) from the objects about \( \eR^4\). There exists an element \( U\in \SU(2)\) such that
	\begin{equation}
		\big( \bar\phi^{-1}\circ\rho(U)\circ\bar \phi \big)(\bar y)=\bar A\bar y
	\end{equation}
	for every \( \bar y\in \eR^3\). In other words,
	\begin{equation}
		\bar\phi^{-1}\big( U\bar\phi(\bar y)U^{\dag} \big)=\bar A\bar y.
	\end{equation}
	We know that \( \SU(2)\subset \SL(2,\eC)\) and we will prove that \( \Lambda(U)=A\), that is \( \Lambda(U)y=Ay\) for every \( y\in \eR^4\). Since \( \Lambda(U)\) and \( A\) are linear, we can prove the cases \( y=e_k\) with \( k=0\) and \( k\neq 0\) separately.
	\begin{subproof}
		\spitem[$ y=e_0$]
		We have
		\begin{subequations}
			\begin{align}
				\Lambda(U)e_0 & =\phi^{-1}\big( U\phi(e_0) U^{\dag}\big) \\
				              & =\phi^{-1}\big( U\sigma_0U^{\dag} \big)  \\
				              & =\phi^{-1}(\sigma_0)                     \\
				              & =e_0.
			\end{align}
		\end{subequations}
		because \( \sigma_0=\id\).
		\spitem[$ y=e_k$ with $ k\neq 0$]
		We have
		\begin{equation}
			f(U)e_k=\phi^{-1}\big( U\phi(e_k)U^{\dag} \big)=\phi^{-1}(U\sigma_k U^{\dag}).
		\end{equation}
		Since \( k\neq 0\), we know from proposition \ref{PROPooRQUZooAoZzwx}\ref{ITEMooLZBSooZUQGgJ} that \( U\sigma_k U^{\dag}\in V\). Thus it makes sense to consider \( \bar \phi^{-1}\big(U\sigma_k U^{\dag}\big)\), and in fact we have
		\begin{equation}
			\phi^{-1}\big( U\sigma_k U^{\dag} \big)=\begin{pmatrix}
				0 \\
				\bar\phi(U\sigma_kU^{\dag})
			\end{pmatrix}=\begin{pmatrix}
				0 \\
				\bar A\bar e_k
			\end{pmatrix}=Ae_k.
		\end{equation}
	\end{subproof}
	It is now proven that \( A=\Lambda(U)\), so that \( \SO(3)\subset \Lambda\big( \SL(2,\eC) \big)\).
\end{proof}
Notice that we even have \( \SO(3)\subset \Lambda\big( \SU(2) \big)\).

\begin{proposition}     \label{PROPooFPSLooSzvSYF}
	The map \( \Lambda\colon \SL(2,\eC)\to \SO(1,3)^{\uparrow}\) is surjective.
\end{proposition}

\begin{proof}
	Let \( A\in \SO(1,3)^{\uparrow}\). The standard decomposition \ref{PROPooYADMooQOTpWX} provides two rotations \( R\) and \( S\) and a boost \( L\) such that \( A=RLS\). The lemmas \ref{LEMooTZPAooWGjMgU} and \ref{LEMooGURFooRTBBmi} show that \( R\), \( S\) and \( L\) are part of \( \Lambda\big( \SL(2,\eC) \big)\). Since \( \Lambda\) is an homomorphism (lemma \ref{LEMooJBYDooSFEUFr}), we have the result.
\end{proof}

\begin{lemma}       \label{LEMooQKCQooTIaesa}
	We have \( \ker(\Lambda)=\eZ_2\).
\end{lemma}

\begin{proof}
	We can use the same proof as the one given in \ref{PROPooGEHAooPCReoU}, but we present here a simpler one. Let \( S\in \SL(2,\eC)\) be an element of \( \ker(\Lambda)\), that is \( \Lambda(S)=\id\), or
	\begin{equation}
		\Lambda(S)y=\phi^{-1}\big( S\phi(y)S^{\dag} \big)=y.
	\end{equation}
	We begin our investigations with \( y=e_0\). Since \( \phi(e_0)=\sigma_0=\id\), it provides \( \phi^{-1}(SS^{\dag})=e_0=\sigma_0\), so that
	\begin{equation}
		SS^{\dag}=\id.
	\end{equation}
	We conclude that \( S\in \SU(2)\). From here we can continue as in proposition \ref{PROPooGEHAooPCReoU}, but we provide a simpler computation (just for fun).

	For every \( i=1,2,3\) we have \( \phi^{-1}\big( S\phi(e_i)S^{\dag} \big)=e_i\). We apply \( \phi\) on both sides:
	\begin{equation}        \label{EQooFMQQooDHtAGl}
		S\sigma_iS^{\dag}=\phi(e_i)=\sigma_i.
	\end{equation}
	Since \( S\in \SU(2)\) we have \( S^{\dag}=S^{-1}\). We multiply both sides of \eqref{EQooFMQQooDHtAGl} by \( S\):
	\begin{equation}
		S\sigma_i=\sigma_iS.
	\end{equation}
	Let \( S=\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}\) with \( a,b,c,d\in \eC\). We have
	\begin{subequations}
		\begin{align}
			S\sigma_1 & =\begin{pmatrix}
				             b & a \\
				             d & c
			             \end{pmatrix} \\
			\sigma_1S & =\begin{pmatrix}
				             c & d \\
				             a & b
			             \end{pmatrix},
		\end{align}
	\end{subequations}
	so that \( b=c\) and \( a=d\) and we are left with \( S=\begin{pmatrix}
		a & b \\
		b & a
	\end{pmatrix}\). The same with \( \sigma_2\):
	\begin{subequations}
		\begin{align}
			S\sigma_2=i\begin{pmatrix}
				           b & -a \\
				           a & -b
			           \end{pmatrix}    \\
			\sigma_2S & =i\begin{pmatrix}
				              -b & -a \\
				              a  & b
			              \end{pmatrix},
		\end{align}
	\end{subequations}
	so that \( b=0\) and we are left with \( S=\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}\).
	Since \( S\in \SU(2)\) we have \( 1=\det(S)=a^2\).

	Thus \( a\) is an element of \( \eC\) such that \( a^2=1\). Theorem \ref{ThoSVZooMpNANi} says that a polynomial of degree \( 2\) has at most \( 2\) roots. Thus \( a^2=1\) implies \( a=\pm 1\). No other possibilities. Thus \( S=\pm\mtu\). It is easy to check that \( \mtu\) and \( -\mtu\) are part of \( \ker(\Lambda)\).

	We conclude that \( \ker(\Lambda)=\pm\mtu=\eZ_2\).
\end{proof}

\begin{theorem}
	We have the group isomorphism
	\begin{equation}
		\SO(1,3)^{\uparrow}=\frac{ \SL(2,\eC) }{ \eZ_2 }.
	\end{equation}
\end{theorem}

\begin{proof}
	The proof follows the same lines as the proof of proposition \ref{PROPooDKPTooBnLflt}. We use the first isomorphism theorem \ref{ThoPremierthoisomo} with \( \theta\) being the map \( \Lambda\colon \SL(2,\eC)\to \SO(1,3)\) defined by the lemma \ref{LEMooXDPPooPImUQX}. It says that
	\begin{equation}
		\frac{ \SL(2,\eC) }{ \ker(\Lambda) }=\Image(\Lambda).
	\end{equation}
	Lemma \ref{LEMooQKCQooTIaesa} says that \( \ker(\Lambda)=\eZ_2\) and proposition \ref{PROPooFPSLooSzvSYF} says that \( \Image(\Lambda)=\SO(1,3)^{\uparrow}\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Lie group structure}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooISDKooUnafZR}
	Some manifolds.
	\begin{enumerate}
		\item
		      The part \( \gO(3,1)\) is a \( 6\)-dimensional submanifold of \( \GL(4)\).
		\item
		      The part \( \SO(3,1)\) is a \( 6\)-dimensional submanifold of \( \gO(3,1)\).
		\item
		      The part \( \SO(3,1)^{\uparrow}\) is a \( 6\)-dimensional submanifold of \( \SO(3,1)\).
	\end{enumerate}
	%TODOooLYOLooPaInpH
\end{lemma}
Recall that \( \GL(n\eR)\) is an analytic Lie group by proposition \ref{PROPooPZABooXxQkFi}.


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Lie algebra of \( \SO(3,1)\)}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooEEDYooKuRxbW}
	If \( \omega\in \eM(4,\eR)\) satisfy \( \eta\omega=-\omega^t\eta\), then for every \( k\in \eN\) we have
	\begin{equation}
		\eta\omega^k=(-1)^k(\omega^t)^k\eta.
	\end{equation}
\end{lemma}

\begin{proof}
	By induction. For \( k=0\) or \( k=1\) it's okay. For the induction,
	\begin{equation}
		\eta\omega^{k+1}=\eta\omega^k\omega=(-1)^k(\omega^t)^k\eta\omega=(-1)^k(\omega^t)^k(-)\omega^t\eta=(-1)^{k+1}(\omega^t)^{k+1}\eta.
	\end{equation}
\end{proof}

\begin{proposition}[\cite{MonCerveau,BIBooYTTJooYpPYLT}]        \label{PROPooGNHJooKILwuI}
	The Lie algebra \( \so(3,1)\) of \( \SO(3,1)\) is the set of the \( 4\times 4\) matrices \( \omega\) satisfying
	\begin{equation}
		\omega^t\eta+\eta\omega=0.
	\end{equation}
\end{proposition}

\begin{proof}
	We are going to determine the set \( \SO(3,1)'\) of the componentwise derivative of paths of matrices in \( \SO(3,1)\). See \ref{NORMooHZGKooJEiamo} and the proposition \ref{PROPooSQHLooGQAykc}.

	Let
	\begin{equation}
		\mA=\{ \omega\in\eM(4,\eR)\tq \omega^t\eta+\eta\omega=0 \}.
	\end{equation}

	\begin{subproof}
		\spitem[\( SO(3,1)'\subset \mA\)]

		Let \( \Lambda\) be a path in \( \SO(3,1)\); for each \( t\in \eR\) we have
		\begin{equation}
			\Lambda(t)^t\eta\Lambda(t)=\eta.
		\end{equation}
		Taking the componentwise derivative at \( t=0\) we have
		\begin{equation}
			\Lambda'(0)^t\eta\Lambda(0)+\Lambda(0)^t\eta\Lambda'(0)=0.
		\end{equation}
		Notice that the derivative and the transposition commute. Since \( \Lambda(0)=\mtu\), the element \( \omega=\Lambda'(0)\) satisfy
		\begin{equation}
			\omega\eta+\omega^t\eta=0.
		\end{equation}
		This shows that the elements \( \omega\) of \( \so(3,1)\) satisfy \( \omega^t\eta+\eta\omega=0\).

		\spitem[\( \mA\subset \SO(3,1)\)]
		Let \( \omega\in \mA\). We consider the path \( \Lambda(t)= e^{\omega t}\). Here the exponential is a «normal» exponential of matrix\footnote{Definition \ref{DEFooSFDUooMNsgZY}.}. The proposition \ref{PROPooSDNNooQtHkhA} says that \( \Lambda'(0)=\omega\). We have to prove that \( \Lambda(t)\in \SO(3,1)\) for every \( t\) in a neighborhood of \( 0\). For that, our first task is to prove that \( \eta e^{t\omega}= e^{-t\omega^t}\eta\) :
		\begin{subequations}
			\begin{align}
				\eta e^{t\omega} & =\eta\sum_{k=0}^{\infty}\frac{ t^k\omega^k }{ k! }                             \\
				                 & =\sum_k\frac{ t^k }{ k! }\eta\omega^k          \label{SUBEQooBFPHooSVCAoO}     \\
				                 & =\sum_k\frac{ t^k }{ k! }(-1)^k(\omega^t)^k\eta    \label{SUBEQooGDTPooZPNJOg} \\
				                 & = e^{-t\omega^t}\eta        \label{SUBEQooMZOAooJFqwgT}
			\end{align}
		\end{subequations}
		Justifications:
		\begin{itemize}
			\item For \eqref{SUBEQooBFPHooSVCAoO}. Proposition \ref{PROPooMZZQooEhQsgQ}.
			\item For \eqref{SUBEQooGDTPooZPNJOg}. Lemma \ref{LEMooEEDYooKuRxbW}.
			\item For \eqref{SUBEQooMZOAooJFqwgT}. Proposition \ref{PROPooMZZQooEhQsgQ} again.
		\end{itemize}

		We are now able to prove that \( \Lambda(t)\in \SO(3,1)\). Using propositions \ref{PROPooFLHPooRhLiZE}\ref{ITEMooEOSMooQWjcjA} and \ref{PROPooRERRooMutKcg}, we see that
		\begin{equation}
			\Lambda(t)^t\eta\Lambda(t)= e^{t\omega^t} e^{-\omega^t}\eta=\eta.
		\end{equation}
		This proves that \( \Lambda(t)\in \gO(3,1)\).

		For the determinant, \( \det\big( \Lambda(0) \big)=1\) and \( t\mapsto \det\big( \Lambda(t) \big)\) is continuous. Lemma \ref{LEMooBMDUooHoYYae} says that \( \det\big( \Lambda(t) \big)\) can take only the values \( \pm1\). By continuity, \( \det\big( \Lambda(t) \big)=1\) for every \( t\).
	\end{subproof}
\end{proof}

\begin{normaltext}
	The link between the matrices \( \omega\) satisfying \( \omega^t\eta+\eta\omega=0\) and the authentic Lie algebra of \( \SO(3,1)\) is that the element \( \omega\in T_e\SO(3,1)\) associated with the matrix \( \omega\) is given by
	\begin{equation}
		\omega(f)=\Dsdd{ f( e^{\omega t}) }{t}{0}
	\end{equation}
	when \( f\) is a function on \( \SO(3,1)\).
\end{normaltext}

\begin{lemma}[\cite{MonCerveau,BIBooYTTJooYpPYLT,BIBooNYSGooCxjgGI}]        \label{LEMooVAYBooHcPKHU}
	Let the matrices \( \{ M_{\lambda\mu} \}_{\lambda,\mu=0,1,2,3}\) given by the following matrix elements
	\begin{equation}
		(M_{\lambda\mu})_{\alpha\beta}=\delta_{\mu\alpha}\eta_{\lambda\beta}-\delta_{\lambda\alpha}\eta_{\mu\beta}.
	\end{equation}
	\begin{enumerate}
		\item
		      They belong to \( \so(3,1)\).
		\item
		      The commutator is given by
		      \begin{equation}
			      [M_{\lambda\mu}, M_{\rho\sigma}]=\eta_{\mu\sigma}M_{\lambda\rho}-\eta_{\mu\rho}M_{\lambda\sigma}+\eta_{\lambda\rho}M_{\mu\sigma}-\eta_{\lambda\sigma}M_{\mu\rho}.
		      \end{equation}
	\end{enumerate}
\end{lemma}

\begin{proof}
	By proposition \ref{PROPooGNHJooKILwuI}, we have to prove that \( M_{\lambda\mu}\eta\) is antisymmetric. Here you are:
	\begin{subequations}
		\begin{align}
			(M_{\lambda\mu}\eta)_{ij} & =\sum_{k=0}^3(M_{\lambda\mu})_{ik}\eta_{kj}                                                        \\
			                          & =\sum_k(\delta_{i\mu}\eta_{\lambda k}-\delta_{\lambda i}\eta_{\mu k})\eta_{kj}                     \\
			                          & =\delta_{i\mu}(\eta\eta)_{\lambda j}-\delta_{\lambda i}(\eta\eta)_{\mu k}                          \\
			                          & =\delta_{i\mu}\delta_{\lambda j}-\delta_{\lambda i}\delta_{\mu_ j}     \label{SUBEQooWIRYooLnzyMw}
		\end{align}
	\end{subequations}
	We used the fact that \( \eta\eta=\delta\). The expression \eqref{SUBEQooWIRYooLnzyMw} is antisymmetric with respect to \( i,j\).

	The commutation relation is a computations:
	\begin{subequations}
		\begin{align}
			[M_{\lambda\mu},M_{\rho\sigma}]_{ij} & =\sum_k(M_{\lambda\mu})_{ik}(M_{\rho\sigma})_{kj}-\sum_k(M_{\rho\sigma})_{ik}(M_{\lambda\mu})_{kj}                                                            \\
			                                     & =\delta_{i\mu}\eta_{\lambda\sigma}\eta_{\rho j}-\delta_{i\mu}\eta_{\lambda\rho}\eta_{\sigma j}
			-\delta_{\lambda i}\eta_{\mu\sigma}\eta_{\rho j}+\delta_{\lambda i}\eta_{\mu\rho}\eta_{\sigma j}                                                                                                     \\
			                                     & \quad-\delta_{i\sigma}\eta_{\rho\mu}\eta_{\lambda i}+\delta_{i\sigma}\eta_{\rho\lambda}\eta_{\mu j}
			+\delta_{\rho i}\eta_{\sigma\mu}\eta_{\lambda j}-\delta_{\rho i}\eta_{\sigma\lambda}\eta_{\mu j}                                                                                                     \\
			                                     & =\eta_{\lambda\sigma}(M_{\rho\mu})_{ij}+\eta_{\lambda\rho}(M_{\mu\sigma})_{ij}+\eta_{\mu\rho}(M_{\sigma\lambda})_{ij}+\eta_{\sigma\mu}(M_{\lambda\rho})_{ij}.
		\end{align}
	\end{subequations}
	Keep in mind that \( M_{\lambda\mu}=-M_{\mu\lambda}\) and you get the result.
\end{proof}

\begin{lemma}
	We set
	\begin{equation}
		\begin{aligned}[]
			M_1 & =M_{32} &  &  & N_1 & =M_{01} \\
			M_2 & =M_{13} &  &  & N_2 & =M_{02} \\
			M_3 & =M_{21} &  &  & N_3 & =M_{03}
		\end{aligned}
	\end{equation}
	We have the commutation relations
	\begin{subequations}
		\begin{align}
			[M_i,M_j] & =\sum_k\epsilon_{ijk}M_k  \\
			[M_i,N_j] & =\sum_k\epsilon_{ijk}N_k  \\
			[N_i,N_j] & =-\sum_k\epsilon_{ijk}M_k \\
		\end{align}
	\end{subequations}
\end{lemma}

\begin{proof}
	Direct computation using the commutators of lemma \ref{LEMooVAYBooHcPKHU}. An alternative is to ask Sage\cite{Sage} to make the job.

	First define the matrices:
	\lstinputlisting{tex/sage/sageSnip020.py}

	The make the checks:
	\lstinputlisting{tex/sage/sageSnip021.sage}
\end{proof}


\begin{lemma}
	We set
	\begin{subequations}
		\begin{align}
			L_i & =\frac{ 1 }{2}(M_i+iN_i) \\
			S_i & =\frac{ 1 }{2}(M_i-iN_i)
		\end{align}
	\end{subequations}
	and
	\begin{subequations}
		\begin{align}
			\mB_1 & =\Span_{\eC}\{ L_i \} \\
			\mB_2 & =\Span_{\eC}\{ S_i \}
		\end{align}
	\end{subequations}
	\begin{enumerate}
		\item
		      The sets \( \mB_1\) et \( \mB_2\) are Lie algebras.
		\item
		      The commutation relations are
		      \begin{subequations}
			      \begin{align}
				      [L_i,L_j] & =\sum_k\epsilon_{ijk}L_k   \\
				      [S_i,S_j] & =\sum_{k}\epsilon_{ijk}S_k \\
				      [L_i,S_j] & =0.
			      \end{align}
		      \end{subequations}
	\end{enumerate}
\end{lemma}

\begin{proof}
	It is sufficient to check the commutation relations. One again, these are only computations. You can do it with Sage:
	\lstinputlisting{tex/sage/sageSnip022.sage}
\end{proof}
