% This is part of Mes notes de mathématique
% Copyright (c) 2008-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminants}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecGYzHWs}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications multilinéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Application multilinéaire]       \label{DefFRHooKnPCT}
	Soient des espaces vectoriels \( V_1,\ldots,V_k\) ainsi qu'un espace vectoriel \( W\). Une application \(T \colon V^k\to W  \) est dite \defe{\( k\)-linéaire}{application \( k\)-linéaire} ou \defe{\( k\)-multilinéaire}{application \( k\)-multilinéaire} si pour tout \( (v_1, \ldots,v_k)\in V_1\times \ldots\times V_k\) les applications
	\begin{equation}
		\begin{aligned}
			T_i\colon V_i & \to W                                                 \\
			x             & \mapsto T(v_1,\ldots, v_{i-1}, x,v_{i+1},\ldots, v_k)
		\end{aligned}
	\end{equation}
	sont linéaires.

	En particulier lorsque \( k=2\), nous parlons d'applications \defe{bilinéaires}{bilinéaire}.

	Nous notons \( \aL(V_1,\ldots,V_k;W)\) ou \( \aL_k(V_1,\ldots,V_k;W)\) l'ensemble des applications \( k\)-linéaires de \( V_1\times \ldots \times V_k\) vers \( W\). Lorsque tout les \( V_i\) sont identiques nous notons \( \aL_k(V,W)\).
\end{definition}


\begin{definition}	\label{DEFooZFCKooGIoYrE}
	Soit \( E\), un \( \eK\)-espace vectoriel. Une forme multilinéaire \(f \colon E^k\to \eK  \) est \defe{antisymétrique}{formle multilinéaire antisymétrique} si
	\begin{equation}
		f(x_1,\ldots,x_i,\ldots,x_j,\ldots,x_k)=-f(x_1,\ldots,x_j,\ldots,x_i,\ldots,x_k)
	\end{equation}
	pour tout couple \( (i,j)\).
\end{definition}

\begin{normaltext}
	Il serait une erreur\quext{Mais je crois que c'est fait à plusieurs endroits dans le Frido. Faire attention.} de noter \( \aL(V\times V,\eR)\) l'ensemble de applications bilinéaires sur \( V\). En effet, \( \aL(V\times V,\eR)\) est l'ensemble des applications linéaires \( V\times V\to \eR\).

	Si \( S\in \aL(V\times V,\eR)\), il n'y a pas de formules particulières pour \( S(u,\lambda v)\). Par contre si \( T\in\aL_2(V,W)\), alors
	\begin{equation}
		T(u,\lambda v)=\lambda T(u,v).
	\end{equation}
	Nous avons par contre
	\begin{equation}
		T(\lambda u,\lambda v)=\lambda^2 T(u,v)
	\end{equation}
	alors que
	\begin{equation}
		S(\lambda u,\lambda v)=  S\big( \lambda(u,v) \big) =\lambda S(u,v).
	\end{equation}

	Certains\cite{BIBooDEEYooRGFyDD} notent \( \otimes^kV^*\) l'espace des formes \( k\)-linéaires sur \( V\), voir la proposition \ref{PROPooIODGooYajpiy}.
\end{normaltext}


\begin{example}
	Soit une application linéaire \(A \colon \eR^n\to \eR^m  \). L'application de \( \eR^m\times \eR^n\) dans \( \eR\) associée à \( A\) définie par
	\begin{equation}
		\begin{aligned}
			T_A\colon \eR^n\times \eR^m & \to \eR           \\
			x,y                         & \mapsto x\cdot Ay
		\end{aligned}
	\end{equation}
	est une forme bilinéaire.

	Elle est aussi donnée par la formule
	\begin{equation}
		T_A(x,y)= x\cdot Ay =\sum_{i,j}A_{ij}x_i y_j.
	\end{equation}
\end{example}

\begin{proposition}	\label{PROPooXIWIooWVeEJo}
	Si \(T \colon (\eR^n)^k\to \eR  \) est une application \( k\)-multilinéaire, alors nous avons
	\begin{equation}
		T\big( x^{(1)},\ldots,x^{(k)} \big)=\sum_{i_1,\ldots,i_k=1}^nS_{i_1,\ldots,i_k}x^{(1)}_{i_1}\ldots x^{(k)}_{i_k}
	\end{equation}
	où \( S_{i_1,\ldots,i_k}=T(e_{i_1},\ldots,e_{i_k})\).
\end{proposition}

\begin{proof}
	Pour chaque \( j\) nous posons \( x^{(j)}=\sum_lx^{(j)}_le_l\) et en utilisant la multilinéarité,
	\begin{equation}
		T(x^{(1)},\ldots,x^{(k)})=T\big( \sum_{i_1}x_{i_1}^{(1)}e_{i_1},\ldots,\sum_{i_k}x_{i_k}^{(k)}e_{i_k} \big)=\sum_{i_1,\ldots,i_k=1}^nx_{i_1}^{(1)}\ldots x_{i_k}^{(k)}T(e_{i_1},\ldots,e_{i_k}).
	\end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooDBHGooSzhdOn}
	Soit \( E=\{ (\alpha_1,\ldots,\alpha_n)\in \eN^n \tq \sum_i\alpha_i=k \}\). Nous définissons la relation d'équivalence \( \alpha\sum\beta\) lorsque il existe une permutation \( \sigma\in S_n\) telle que \( \alpha_i=\beta_{\sigma(i)}\) pour tout \( i\).

	Alors pour tout \( \alpha\in E\) nous avons
	\begin{equation}
		\Card([\alpha])=\frac{ k! }{ \alpha_1!\ldots \alpha_n! }.
	\end{equation}
	%TODOooPWHJooRDtuau. Prouver ça.
\end{lemma}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooIEXNooPOHokX}
	Si \(T \colon (\eR^n)^k\to \eR  \) est une application \( k\)-multilinéaire. Nous posons
	\begin{equation}
		\begin{aligned}
			S\colon \eR^n & \to \eR                \\
			x             & \mapsto T(x,\ldots,x).
		\end{aligned}
	\end{equation}
	Alors
	\begin{equation}
		S(x)=\sum_{| \alpha |=k}a_{\alpha}x^{\alpha}
	\end{equation}
	avec
	\begin{equation}
		a_{\alpha}=\frac{ k! }{ \alpha! }S_{\alpha}
	\end{equation}
	où \( S_{\alpha}\) est celui donné dans la proposition \ref{PROPooXIWIooWVeEJo}.
	%TODOooZGUCooInMNQt. Prouver ça.
\end{lemma}



%-------------------------------------------------------
\subsection{Continuité}
%----------------------------------------------------


\begin{lemma}[\cite{BIBooFWFPooGfRwtt}]	\label{LEMooGQWQooUfqURv}
	Soit une forme \( n\)-multilinéaire \( \alpha\in\aL_n(E_1,\ldots,E_n;F)\). Nous avons la formule
	\begin{equation}
		\alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n)=\sum_{i=1}^n\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n).
	\end{equation}
\end{lemma}

\begin{proof}
	Nous y allons par récurrence.
	\begin{subproof}
		\spitem[\( n=2\)]
		%-----------------------------------------------------------
		Vu que \( \alpha\) est multilinéaire, nous avons \( \alpha(x_1,x_2)=\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2)\). Avec ça, nous avons
		\begin{subequations}
			\begin{align}
				\alpha(x_1,x_2)-\alpha(a_1,a_2) & =\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2)-\alpha(a_1,a_2) \\
				                                & =\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2-a_2).
			\end{align}
		\end{subequations}
		\spitem[La récurrence]
		%-----------------------------------------------------------
		Nous supposons que la formule est valable pour toute application \( n\)-multilinéaire, et nous considérons une application \( n+1\)-multilinéaire \( \alpha\in\aL_{n+1}(E,F)\). Nous considérons aussi des éléments \( x_i, a_i\in E_i\) pour \( i=1,\ldots,n+1\). Nous posons
		\begin{equation}
			\begin{aligned}
				\alpha'\colon E_1\times\ldots\times E_n & \to F                                   \\
				y_1,\ldots,y_n                          & \mapsto \alpha(y_1,\ldots,y_n,x_{n+1}).
			\end{aligned}
		\end{equation}
		Cela est une application \( n\)-multilinéaire. Nous avons ce calcul :
		\begin{subequations}
			\begin{align}
				 & \alpha(x_1,  \ldots,x_{n+1})  -\alpha(a_1,\ldots,a_{n+1})      \nonumber                                                      \\& =\alpha'(x_1,\ldots,x_n)-\alpha'(a_1,\ldots,a_n)+\alpha'(a_1,\ldots,a_n)-\alpha(a_1,\ldots,a_{n+1}) \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha'(a_1,\ldots,a_n)-\alpha(a_1,\ldots,a_{n+1})        \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha(a_1,\ldots,a_n,x_{n+1})-\alpha(a_1,\ldots,a_{n+1}) \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha(a_1,\ldots,a_n,x_{n+1}-a_{n+1})                    \\
				 & =\sum_{i=1}^{n+1}\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)
			\end{align}
		\end{subequations}
	\end{subproof}
\end{proof}


\begin{proposition}[\cite{BIBooINSNooUzJOru}]	\label{PROPooDQBOooByBvmj}
	Soient des espaces vectoriels normés \( E_1,\ldots, E_n\) et \( F\). Une application \( n\)-multilinéaire \(\alpha \colon E_1\times \ldots E_n\to F  \) est continue\footnote{Pour la topologie sur \( E_1\times\ldots\times E_n\), c'est celle de la norme produit \ref{LEMooFQMSooLmdIvD}.} si et seulement si il existe \( \lambda>0\) tel que
	\begin{equation}		\label{EQooGTVEooZsvzAM}
		\| \alpha(x_1,\ldots,x_n) \|\leq \lambda \| x_1 \|\ldots \| x_n \|.
	\end{equation}
\end{proposition}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\( \Rightarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( \alpha\) est continue. En particulier, elle est continue en \( (0,\ldots,0)\in E_1\times \ldots\times E_n\). Nous considérons \( r>0\) tel que
		\begin{equation}
			\alpha\big( B(0,2r) \big)\subset B(0,1).
		\end{equation}
		Nous posons \( \lambda=1/r^{n}\), et nous allons voir que ça fait le boulot. Soit \( (x_1,\ldots,x_n)\in E_1\times \ldots \times E_n\) avec \( x_i\neq 0\) pour tout \( i\). Nous posons \( z_i=rx_i/\| x_i \|\), de telle sorte que \( \| z_1 \|=\ldots =\| z_n \|=r\) et donc que \( (z_1,\ldots,z_n)\in B(0,2r)\) et donc
		\begin{equation}
			\| \alpha(z_1,\ldots,z_n) \|<1.
		\end{equation}
		Mais \( \alpha(z_1,\ldots,z_n)=r^n/\prod_i\| x_i \|\). Donc avec \( \lambda=1/r^n\) nous trouvons
		\begin{equation}
			\| \alpha(x_1,\ldots,x_n) \|\leq \lambda \prod_{i}\| x_i \|.
		\end{equation}

		Si \( x_i=0\), alors c'est encore plus simple parce que la relation \eqref{EQooGTVEooZsvzAM} se réduit à \( 0\leq 0\).
		\spitem[\( \Leftarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( \lambda>0\) vérifie \( \| \alpha(x_1,\ldots,x_n) \|\leq \lambda \prod_i\| x_i \|\) pour tout \( (x_1,\ldots,x_n)\in E_1\times \ldots\times E_n\). En utilisant le lemme \ref{LEMooGQWQooUfqURv}, nous avons, pour tout \( x_i\) et \( a_i\) :
		\begin{subequations}
			\begin{align}
				\| \alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n) \| & =\| \sum_{i=1}^n\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n) \|                                                             \\
				                                                    & \leq \sum_{i=1}^n  \lambda  \| x_i-a_i \|\cdot \prod_{j=1}^{i-1}\| a_j \|\cdot\prod_{j=i+1}^n\| x_j \|.		\label{SUBEQooYDGCooJFBaxN}
			\end{align}
		\end{subequations}
		Et maintenant nous prenons \( 0<\epsilon<1\) et \( (x_1,\ldots,x_n)\in B\big( (a_1,\ldots,a_n),\epsilon \big)\), de telle sorte que \( \| x_i-a_i \|<\epsilon\) pour tout \( i\) et que
		\begin{equation}
			\| x_i \|<\| a_i \|+\epsilon\leq \| a_i \|+1.
		\end{equation}
		Nous posons \( M=\max_{j=1,\ldots,n}\| a_j \|\) : \( \| x_j \|\leq \| a_i \|+1\leq M+1\) et nous majorons sauvagement chacun des produits de \eqref{SUBEQooYDGCooJFBaxN} par \( (M+1)^n\) :
		\begin{subequations}
			\begin{align}
				\| \alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n) \| & \leq \sum_{i=1}^n\lambda \| x_i-a_i \|(M+1)^{2n} \\
				                                                    & \leq n\lambda \epsilon (M+1)^{2n}.
			\end{align}
		\end{subequations}
		Nous avons donc bien prouvé que \( \alpha(x_1,\ldots,x_n)\stackrel{F}{\longrightarrow} \alpha(a_1,\ldots,a_n) \) lorsque \( (x_1,\ldots,x_n)\stackrel{ E_1\times \ldots\times E_n}{\longrightarrow} (a_1,\ldots,a_n) \).

	\end{subproof}
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Formes multilinéaires alternées}
%---------------------------------------------------------------------------------------------------------------------------


\begin{definition}\index{déterminant!forme linéaire alternée}       \label{DEFooYWOBooUGJojy}
	Soit \( E\), un \( \eK\)-espace vectoriel. Une application multilinéaire \defe{alternée}{forme multilinéaire alternée} sur \( E\) est une application \( k\)-linéaire \( f\colon E^k\to \eK\) telle que \( f(v_1,\ldots, v_k)=0\) dès que \( v_i=v_j\) pour certains \( i\neq j\).

	Une application multilinéaire alternée est une \defe{forme linéaire}{forme linéaire}.
\end{definition}


\begin{lemma}[\cite{BIBooLFTJooKfWQww}]   \label{LemHiHNey}
	Une forme \( k\)-linéaire alternée est antisymétrique\footnote{Antisymétrique, définition \ref{DEFooZFCKooGIoYrE}.}. Si \( \eK\) est de caractéristique différente de \( 2\), alors une forme antisymétrique est alternée\footnote{Définition \ref{DEFooYWOBooUGJojy}.}.
\end{lemma}

\begin{proof}
	Soit \( f\) une forme alternée; quitte à fixer toutes les autres variables, nous pouvons travailler avec une \( 2\)-forme et simplement montrer que \( f(x,y)=-f(y,x)\). Pour ce faire nous écrivons
	\begin{equation}
		0=f(x+y,x+y)=f(x,x)+f(x,y)+f(y,x)+f(y,y)=f(x,y)+f(y,x).
	\end{equation}

	Pour la réciproque, si \( f\) est antisymétrique, alors \( f(x,x)=-f(x,x)\). Cela montre que \( f(x,x)=0\) lorsque \( \eK\) est de caractéristique différente de deux.
\end{proof}

\begin{proposition}[\cite{GQolaof}] \label{ProprbjihK}
	Soit \( E\), un \( \eK\)-espace vectoriel de dimension \( n\), où la caractéristique de \( \eK\) n'est pas deux. L'espace des \( n\)-formes multilinéaires alternées sur \( E\) est de \( \eK\)-dimension \( 1\).
\end{proposition}
\index{groupe!permutation}
\index{groupe!et géométrie}
\index{espace!vectoriel!dimension}
\index{rang}
\index{déterminant}
\index{dimension!\( n\)-formes multilinéaires alternées}

\begin{proof}
	Soient \( \{ e_i \}\), une base de \( E\), une \( n\)-forme linéaire alternée \( f\colon E\to \eK\) ainsi que des vecteurs \( (v_1,\ldots, v_n)\) de \( E\). Nous pouvons les écrire dans la base
	\begin{equation}
		v_j=\sum_{i=1}^n\alpha_{ij}e_i
	\end{equation}
	et alors exprimer \( f\) par
	\begin{subequations}
		\begin{align}
			f(v_1,\ldots, v_n) & =f\big( \sum_{i_1=1}^n\alpha_{1i_1}e_{i_1},\ldots, \sum_{i_n=1}^n\alpha_{ni_n}e_{i_n} \big) \\
			                   & =\sum_{i_1,\ldots, i_n}\alpha_{1i_1}\ldots \alpha_{ni_n}f(e_{i_1},\ldots, e_{i_n}).
		\end{align}
	\end{subequations}
	Étant donné que \( f\) est alternée, les seuls termes de la somme sont ceux dont les \( i_k\) sont tous différents, c'est-à-dire ceux où \( \{ i_1,\ldots, i_n \}=\{ 1,\ldots, n \}\). Il y a donc un terme par élément du groupe des permutations \( S_n\) et
	\begin{equation}
		f(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}f(e_{\sigma(1)},\ldots, e_{\sigma(n)}).
	\end{equation}
	En utilisant encore une fois le fait que la forme \( f\) soit alternée, \( f=f(e_1,\ldots, e_n)\Pi\) où
	\begin{equation}
		\Pi(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}.
	\end{equation}
	Pour rappel, la donnée des \( v_i\) est dans les nombres \( \alpha_{ij}\).

	L'espace des \( n\)-formes alternées est donc \emph{au plus} de dimension \( 1\). Pour montrer qu'il est exactement de dimension \( 1\), il faut et suffit de prouver que \( \Pi\) est alternée. Par le lemme~\ref{LemHiHNey}, il suffit de prouver que cette forme est antisymétrique\footnote{C'est ici que joue l'hypothèse sur la caractéristique de \( \eK\).}.

	Soient donc \( v_1,\ldots, v_n\) tels que \( v_i=v_j\). En posant \( \tau=(1i)\) et \( \tau'=(2j)\) et en sommant sur \( \sigma\tau\tau'\) au lieu de \( \sigma\), nous pouvons supposer que \( i=1\) et \( j=2\). Montrons que \( \Pi(v,v,v_3,\ldots, v_n)=0\) en tenant compte que \( \alpha_{i1}=\alpha_{i2}\) :
	\begin{subequations}
		\begin{align}
			\Pi(v,v,v_3,\ldots, v_n) & =\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n}                                            \\
			                         & =\sum_{\sigma\in S_n}\epsilon(\sigma\tau)\alpha_{\sigma\tau(1)1}\alpha_{\sigma\tau(2)2}\alpha_{\sigma\tau(3)3}\ldots \alpha_{\sigma\tau(n)n} & \text{où } \tau=(12) \\
			                         & =-\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n}                                           \\
			                         & =-\Pi(v,v,v_3,\ldots, v_n).
		\end{align}
	\end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant d'une famille de vecteurs}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons un corps \( \eK\) et l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\).

\begin{definition}[Déterminant d'une famille de vecteurs\cite{MathAgreg}]\label{DEFooODDFooSNahPb}
	Le \defe{déterminant}{déterminant!d'une famille de vecteurs} de la famille de vecteurs \( (v_1,\ldots, v_n)\) dans la base \( B=\{ b_1,\ldots, b_n \}\) est l'élément de \( \eK\)
	\begin{equation}        \label{EQooOJEXooXUpwfZ}
		\det_{(b_1,\ldots, b_n)}(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nb^*_{\sigma(i)}(v_i)
	\end{equation}
	où
	\begin{itemize}
		\item
		      la somme porte sur le groupe symétrique,
		\item
		      le nombre \( \epsilon(\sigma)\) est la signature\footnote{Définition \ref{DEFooYDUHooKIXGNW}.} de la permutation \( \sigma\),
		\item
		      les éléments \( \{ b^*_i \}\) sont la base duale\footnote{Définition \ref{DEFooTMSEooZFtsqa}.} de \( \{ b_i \}\).
	\end{itemize}
\end{definition}

\begin{normaltext}
	La base \( \{ e_i \}\) est la base canonique de \( \eK^n\), et l'élément \( e_k^*\) est la forme linéaire définie par
	\begin{equation}
		\begin{aligned}
			e_k^*\colon \eK^n & \to \eK      \\
			\sum_ix_ie_i      & \mapsto x_k.
		\end{aligned}
	\end{equation}
	Il n'est pas sous-entendu que \( \eK^n\) ait un produit scalaire. Il n'est donc pas autorisé de dire que \( \{ e_i \}\) est une base orthonormée et que \( e^*_k(x)=\langle e_k, x\rangle \). Ce genre d'égalités sont vraies dans le cas \( \eK=\eR\), mais n'ont pas de sens en général.

	Le lemme \ref{LEMooEZFIooXyYybe} va un peu parler du cas où \( \eK^n\) est muni d'une base orthonormée.
\end{normaltext}

\begin{lemma}[\cite{MathAgreg}]     \label{LemJMWCooELZuho}
	Les propriétés du déterminant. Soit \( B\) une base de \( E\).
	\begin{enumerate}
		\item\label{ITEMooAHOHooDZgtSB}
		      L'application \( \det_B\colon E^n\to \eK\) est \( n\)-linéaire.
		\item\label{ITEMooTXXBooBmDtzd}
		      L'application \( \det_B\colon E^n\to \eK\) est \( n\)-linéaire est antisymétrique et alternée\footnote{Alternée, définition \ref{DEFooYWOBooUGJojy}. En caractéristique \( 2\), alternée n'est pas équivalent à symétrique.}.
		\item   \label{ITEMooNFJTooTqGoPr}
		      Pour toute base, \( \det_B(B)=1\).
		\item   \label{ITEMooALRQooDvBzDQ}
		      Le déterminant ne change pas si on remplace un vecteur par une combinaison linéaire des autres :
		      \begin{equation}
			      \det_B(v_1,\ldots, v_n)=\det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big).
		      \end{equation}
		\item   \label{ITEMooQTTRooMbzqyW}
		      Si on permute les vecteurs,
		      \begin{equation}
			      \det_B(v_1,\ldots, v_n)=\epsilon(\sigma)\det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)}).
		      \end{equation}
		\item   \label{ITEMooIPIDooTrerVF}
		      Si \( B'\) est une autre base :
		      \begin{equation}        \label{EqAWICooBLTTOY}
			      \det_B=\det_B(B')\det_{B'}
		      \end{equation}
		\item   \label{ITEMooXKTAooXynFTE}
		      Nous avons aussi la formule \( \det_{B}(B')\det_{B'}(B)=1\).
		\item\label{ItemDWFLooDUePAf}
		      Les vecteurs \( \{ v_1,\ldots, v_n \}\) forment une base si et seulement si \( \det_B(v_1,\ldots, v_n)\neq 0\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[\ref{ITEMooAHOHooDZgtSB}]
		En posant \( v_1=x_1+\lambda x_2\) nous avons
		\begin{subequations}
			\begin{align}
				\det_B(x_1+\lambda x_2,v_2,\ldots, v_n) & =\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^ne^*_{\sigma(i)}(v_i)                                              \\
				                                        & =\sum_{\sigma}\epsilon(\sigma)\Big( e^*_{\sigma(1)}(x_1+\lambda x_2) \Big)\prod_{i=2}^ne^*_{\sigma(i)}(v_i).
			\end{align}
		\end{subequations}
		À partir de là, la linéarité de \( e^*_{\sigma(1)}\) montre que \( \det_B\) est linéaire en son premier argument. Pour les autres arguments, le même calcul tient.

		\spitem[\ref{ITEMooTXXBooBmDtzd}]

		Nous prouvons à présent que \( \det\) est alternée. Si votre corps est de caractéristique différente de deux, vous pouvez lire la proposition \ref{PROPooXNLDooGGkHpd}.

		Supposons \( v_k=v_l\), et considérons la permutation \( \beta=(k,l)\). Nous savons par la proposition \ref{PROPooZOWBooIMxxlj} que \( S_n=A_n\cup A_n\beta\). Cela nous permet de décomposer la somme sur \( S_n\) en deux parties :
		\begin{equation}        \label{EQooWFHQooTrTTWl}
			\sum_{\sigma\in S_n}(-1)^{\sigma}\prod_i\epsilon_{\sigma(i)}^*(v_i)=\sum_{\sigma\in A_n}(-1)^{\sigma}\prod_i\epsilon_{\sigma(i)}^*(v_i)+\sum_{\sigma\in A_n}(-1)^{\sigma\beta}\prod_i\epsilon_{(\sigma\beta)(i)}^*(v_i).
		\end{equation}
		D'abord \( (-1)^{\sigma}=1\) et \( (-1)^{\sigma\beta}=-1\). Ensuite, pour un \( \sigma\in A_n\) donné, nous avons
		\begin{subequations}
			\begin{align}
				\prod_i\epsilon^*_{(\sigma\beta)(i)}(v_i) & =\epsilon_{(\sigma\beta)(k)}^*(v_k)\epsilon^*_{(\sigma\beta)(l)}(v_l)\prod_{\stackrel{i\neq k}{i\neq l}}\epsilon_{(\sigma\beta)(i)}^*(v_i) \\
				                                          & =\epsilon^*_{\sigma(l)}(v_k)\epsilon^*_{\sigma(k)}(v_l)\prod_{\substack{i\neq k                                                            \\i\neq l}}\epsilon^*_{\sigma(i)}(v_i)\\
				                                          & =\epsilon^*_{\sigma(l)}(v_l)\epsilon^*_{\sigma(k)}(v_k)\prod_{\substack{i\neq k                                                            \\i\neq l}}\epsilon^*_{\sigma(i)}(v_i)\\
				                                          & =\prod_i\epsilon^*_{\sigma(i)}(v_i).
			\end{align}
		\end{subequations}
		Donc les deux termes de la somme \eqref{EQooWFHQooTrTTWl} ne diffèrent que par un signe. Elle est donc nulle, et la forme déterminant est alternée.

		La fonction \( \det\) est antisymétrique parce que alternée, voir le lemme \ref{LemHiHNey}.

		\spitem[\ref{ITEMooNFJTooTqGoPr}]
		Nous avons
		\begin{equation}
			\det_B(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\underbrace{e_{\sigma(i)}^*(e_i)}_{=\delta_{\sigma(i),i}}.
		\end{equation}
		Si \( \sigma\) n'est pas l'identité, le produit contient forcément un facteur nul. Il ne reste de la somme que \( \sigma=\id\) et le résultat est \( 1\).
		\spitem[\ref{ITEMooALRQooDvBzDQ}]
		Vu que \( \det_B\) est linéaire en tous ses arguments,
		\begin{equation}
			\det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big)=\det_B(v_1,\ldots, v_n)+\sum_{s=2}^na_s\det_B(v_s,v_2,\ldots, v_n).
		\end{equation}
		Chacun des termes de la somme est nul parce qu'il y a répétition de \( v_s\) parmi les arguments alors que la forme est alternée.
		\spitem[\ref{ITEMooQTTRooMbzqyW}]
		Nous devons calculer \( \det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)})\), et pour y voir plus clair nous posons \( w_i=v_{\sigma(i)}\). Alors :
		\begin{subequations}
			\begin{align}
				\det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)}) & =\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(w_i)            \\
				                                            & =\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_{\sigma(i)})  \\
				                                            & =\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma^{-1}\sigma'(i)}(v_i) \\
				                                            & =\sum_{\sigma'}\epsilon(\sigma\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_i)      \\
				                                            & =\epsilon(\sigma)\det_B(v_1,\ldots, v_n).
			\end{align}
		\end{subequations}
		Justifications : nous avons d'abord modifié l'ordre des éléments du produit et ensuite l'ordre des éléments de la somme. Nous avons ensuite utilisé le fait que \( \epsilon\colon S_n\to \{ 0,1 \}\) était un morphisme de groupe (proposition~\ref{ProphIuJrC}).
		\spitem[\ref{ITEMooIPIDooTrerVF}]
		Étant donné que l'espace des formes multilinéaires alternées est de dimension \( 1\), il existe un \( \lambda\in \eK\) tel que \( \det_B=\lambda\det_{B'}\). Appliquons cela à \( B'\) :
		\begin{equation}
			\det_B(B')=\lambda\det_{B'}(B'),
		\end{equation}
		donc \( \lambda=\det_B(B')\).
		\spitem[\ref{ITEMooXKTAooXynFTE}]
		Il suffit d'appliquer l'égalité précédente à \( B\) en nous souvenant que \( \det_B(B)=1\).
		\spitem[\ref{ItemDWFLooDUePAf}]
		Si \( B'=\{ v_1,\ldots, v_n \}\) est une base alors \( \det_B(B')\neq 0\), sinon il n'est pas possible d'avoir \( \det_B(B')\det_{B'}(B)=1\).

		À l'inverse, si \( B'\) n'est pas une base, c'est que \( \{ v_1,\ldots, v_n \}\) est liée par la proposition \ref{PROPooVEVCooHkrldw}. Il y a donc moyen de remplacer un des vecteurs par une combinaison linéaire des autres. Le déterminant s'annule alors.
	\end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooXNLDooGGkHpd}
	Si la caractéristique du corps de base n'est pas deux, le déterminant est antisymétrique et alterné.
\end{proposition}

\begin{proof}
	Si la caractéristique du corps de base n'est pas deux, une forme antisymétrique est alternée (lemme \ref{LemHiHNey}).

	Pour prouver que le déterminant est antisymétrique, remarquez que permuter \( v_k\) et \( v_l\) revient à calculer le nombre \( \det_B( v_{\sigma_{kl}(1)},\ldots, v_{\sigma_{kl}(n)} )\) au lieu de \( \det_B(v_1,\ldots, v_n)\). Cela revient à changer la somme \( \sum_{\sigma}\) en \( \sum_{\sigma\circ\sigma_{kl}}\). Cela multiplie \( \epsilon(\sigma)\) par \( -1\) parce qu'on ajoute une permutation.

	Donc le déterminant est antisymétrique. Nous en déduisons qu'il est alterné parce que, en permutant trivialement \( v_1\) et \( v_1\), nous obtenons \( \det_B(v_1,v_1)=-\det_B(v_1,v_1)\). Si le corps est de caractéristique différente de deux, cela implique que \( \det_B(v_1,v_1)=0\).
\end{proof}

D'après la proposition~\ref{ProprbjihK}, il existe une unique forme \( n\)-linéaire alternée égale à \( 1\) sur \( B\), et c'est \( \det_B\colon E^n\to \eK\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant d'un endomorphisme}
%---------------------------------------------------------------------------------------------------------------------------

L'interprétation géométrique du déterminant en termes d'aires et de volumes est donnée après le théorème~\ref{ThoBVIJooMkifod}. Pour un déterminant entre deux espaces différents, voir la proposition \ref{PROPooQCPJooFSzaPc}.

\begin{normaltext}
	Voici une définition du déterminant d'une application linéaire \(f \colon E\to E  \). Comme expliqué dans \cite{BIBooJLVVooEdGshG}, il est peu probable qu'il existe une définition satisfaisante de déterminant pour une application linéaire \(f \colon E\to F  \), même si \( E\) et \( F\) ont une même dimension finie.

	Cela soit dit en passant, ça rend plus compliqué que prévu la preuve que \( \GL(E,F)\) est ouvert dans \( \aL(E,F)\). Il ne suffira pas de dire que le déterminant est une application continue. Voir la proposition \ref{PROPooQCPJooFSzaPc}.
\end{normaltext}

\begin{lemmaDef}[Déterminant d'un endomorpisme]       \label{LEMooQTRVooAKzucd}
	Si \( f\colon E\to E\) est un endomorphisme, et si les parties \( B\) et \( B'\) sont deux bases, alors\footnote{Définition de \( \det_B(B')\), \ref{DEFooODDFooSNahPb}.}
	\begin{equation}
		\det_B\big( f(B) \big)=\det_{B'}\big( f(B') \big).
	\end{equation}
	Ce nombre, indépendant de la base choisie est nommé le \defe{déterminant}{déterminant!d'un endomorphisme} de \( f\) et est noté \( \det(f)\).
\end{lemmaDef}

\begin{proof}
	L'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon E^n & \to \eK                                         \\
			v_1,\ldots, v_n   & \mapsto \det_B\big( f(v_1),\ldots, f(v_n) \big)
		\end{aligned}
	\end{equation}
	est \( n\)-linéaire et alternée; il existe donc \( \lambda\in \eK\) tel que \( \varphi=\lambda\det_B\). En appliquant cela à \( B\) :
	\begin{equation}
		\det_B\big( f(B) \big)=\lambda \det_B(B)=\lambda.
	\end{equation}
	Nous avons donc déjà prouvé que \( \lambda=\det_B\big( f(B) \big)\), c'est-à-dire
	\begin{equation}
		\det_B\big( f(v) \big)=\det_B\big( f(B) \big)\det_B(v).
	\end{equation}

	Nous allons maintenant introduire \( B'\) là où il y a du \( v\) en utilisant les formules \eqref{EqAWICooBLTTOY} :
	\begin{subequations}
		\begin{align}
			\det_B\big( f(v) \big) & =\det_B(B')\det_{B'}\big( f(v) \big) \\
			\det_B(v)=\det_B(B')\det_{B'}(v).
		\end{align}
	\end{subequations}
	Nous obtenons
	\begin{equation}
		\det_{B'}\big( f(v) \big)=\det_B\big( f(B) \big)\det_{B'}(v).
	\end{equation}
	Et on applique cela à \( v=B'\) :
	\begin{equation}
		\det_{B'}\big( f(B') \big)=\det_B\big( f(B) \big)\underbrace{\det_{B'}(B')}_{=1}.
	\end{equation}
\end{proof}

\begin{proposition}     \label{PropYQNMooZjlYlA}
	Principales propriétés géométriques du déterminant d'un endomorphisme.
	\begin{enumerate}
		\item   \label{ItemUPLNooYZMRJy}
		      Si \( f\) et \( g\)  sont des endomorphismes, alors \( \det(f\circ g)=\det(f)\det(g)\).
		\item       \label{ITEMooNZNLooODdXeH}
		      L'endomorphisme \( f\) est un automorphisme\footnote{Endomorphisme inversible, définition~\ref{DEFooOAOGooKuJSup}.} si et seulement si \( \det(f)\neq 0\).\index{déterminant!et inversibilité}
		\item   \label{ITEMooZMVXooLGjvCy}
		      Si \( \det(f)\neq 0\) alors \( \det(f^{-1})=\det(f)^{-1}\).
		\item       \label{ItemooPJVYooYSwqaE}
		      L'application \( \det\colon \GL(E)\to \eK\setminus\{ 0 \}\) est un morphisme de groupe.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Point par point.
	\begin{enumerate}
		\item
		      Nous considérons l'application
		      \begin{equation}
			      \begin{aligned}
				      \varphi\colon E^n & \to \eK                         \\
				      v                 & \mapsto \det_B\big( f(v) \big).
			      \end{aligned}
		      \end{equation}
		      Comme d'habitude nous avons \( \varphi(v)=\lambda\det_B(v)\). En appliquant à \( B\) et en nous souvenant que \( \det_B(B)=1\) nous avons
		      \( \det_B\big( f(B) \big)=\lambda\). Autrement dit :
		      \begin{equation}
			      \lambda=\det(f).
		      \end{equation}
		      Calculons à présent \( \varphi\big( g(B) \big)\) : d'une part,
		      \begin{equation}
			      \varphi\big( g(B) \big)=\det_B\big( (f\circ g)(B) \big)
		      \end{equation}
		      et d'autre part,
		      \begin{equation}
			      \varphi\big( g(B) \big)=\lambda\det_B\big( g(B) \big)=\lambda\det(g)
		      \end{equation}
		      En égalisant et en reprenant la la valeur déjà trouvée de \( \lambda\),
		      \begin{equation}
			      \det\big((f\circ g)(B) \big)=\det(f)\det(g),
		      \end{equation}
		      ce qu'il fallait.
		\item
		      Supposons que \( f\) soit un automorphisme. Alors si \( B\) est une base, \( f(B) \) est une base. Par conséquent \( \det(f)=\det_B\big( f(B) \big)\neq 0\) parce que \( f(B)\) est une base (lemme~\ref{LemJMWCooELZuho}\ref{ItemDWFLooDUePAf}).

		      Réciproquement, supposons que \( \det(f)\neq 0\). Alors si \( B\) est une base quelconque nous avons \( \det_B\big( f(B) \big)\neq 0\), ce qui est uniquement possible lorsque \( f(B)\) est une base. L'application \( f\) transforme donc toute base en une base et est alors un automorphisme d'espace vectoriel.
		\item
		      Vu que le déterminant de l'identité est \( 1\) et que \( f\) est inversible, \( 1=\det(f\circ f^{-1})=\det(f)\det(f^{-1})\).
	\end{enumerate}
\end{proof}

\begin{proposition}     \label{PROPooFKDXooKMSolt}
	Soient deux espaces vectoriels \( E\) et \( F\) de dimension finies \( n\) et \( m\) sur le corps \( \eK\) munis de bases \( \{e_i\}\) et \( \{f_{\alpha}\}\). À une matrice \( A\in \eM(m\times n,\eK)\) nous associons l'application linéaire\footnote{Dont nous avons déjà beaucoup parlé entre autres dans la proposition \ref{PROPooCSJNooEqcmFm}.}
	\begin{equation}
		f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}.
	\end{equation}

	Alors, en ce qui concerne les déterminants\footnote{Définition \ref{LEMooQTRVooAKzucd} pour les applications linéaires et \ref{DEFooYCKRooTrajdP} pour les matrices.}, nous avons
	\begin{enumerate}
		\item
		      \( \det(f_A)=\det(A)\)
		\item
		      \( \det(f_{AB})=\det(f_A)\det(f_B)\)
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous devons étudier la formule
	\begin{equation}
		\det(f_A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne_{\sigma(i)}^*\big( f_A(e_i) \big).
	\end{equation}
	En premier lieu nous avons
	\begin{equation}
		f_A(e_i)=\sum_{jk}A_{jk}(e_i)_ke_j=\sum_jA_{ji}e_j.
	\end{equation}
	Nous avons alors
	\begin{equation}
		e_{\sigma(i)}^*\big( f_A(e_i) \big)=\sum_jA_{ji}\underbrace{e^*_{\sigma(i)}(e_j)}_{\delta_{j\sigma(i)}}=A_{\sigma(i)i}.
	\end{equation}
	Au final,
	\begin{equation}
		\det(f_A)=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^nA_{\sigma(i)i}=\det(A^t)=\det(A)
	\end{equation}
	où la dernière égalité est autorisée par le lemme \ref{LEMooCEQYooYAbctZ}.

	Cela prouve la formule \( \det(f_A)=\det(A)\).

	En ce qui concerne la seconde formule, il s'agit de se souvenir de la proposition \ref{PROPooCSJNooEqcmFm} qui donne \( f_{AB}=f_A\circ f_B\), et ensuite de la proposition \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} qui donne \( \det(f_A\circ f_B)=\det(f_A)\det(f_B)\).
\end{proof}


\begin{example}
	Le déterminant de Vandermonde (proposition~\ref{PropnuUvtj}) est alterné, semi-symétrique et non symétrique. Le fait qu'il soit alterné est le fait qu'il soit un déterminant. Étant donné qu'il est alterné, il est semi-symétrique parce que sur \( A_n\), nous avons \( \epsilon=1\). Étant donné qu'il est alterné, il change de signe sous l'action des éléments impairs de \( S_n\) et n'est donc pas symétrique.
\end{example}

\begin{proposition}\index{action de groupe} \label{PropUDqXax}
	Un polynôme semi-symétrique \( f\in \eK[T_1,\ldots, T_n]\) se décompose de façon unique en
	\begin{equation}
		f=P+VQ
	\end{equation}
	où \( P\) et \( Q\) sont deux polynômes symétriques.
\end{proposition}
\index{groupe!permutation}
\index{polynôme!symétrique}

\begin{proof}

	Nous commençons par prouver l'unicité en montrant que si \( f=P+VQ\) avec \( P\) et \( Q\) symétrique, alors \( P\) et \( Q\) sont donnés par des formules explicites en termes de \( f\).


	Si \( \sigma_1\) et \( \sigma_2\) sont deux permutations impaires de \( \{ 1,\ldots, n \}\), alors \( \sigma_1\cdot f=\sigma_2\cdot f\) parce que l'élément \( \sigma_2^{-1}\sigma_1\) est pair (proposition~\ref{ProphIuJrC}), de telle sorte que \( \sigma_2^{-1}\sigma_1\cdot f=f\). Nous posons donc \( g=\tau\cdot f\) où \( \tau\) est une permutation impaire quelconque -- par exemple une transposition.

	Vu que \( V\) est alternée et que \( \tau\) est une transposition nous avons
	\begin{equation}
		g=\tau\cdot f=P-VQ.
	\end{equation}
	Donc \( f+g=2P\) et \( f-g=2VQ\). Cela donne \( P\) et \( Q\) en termes de \( f\) et \( g\), et donc l'unicité.

	Attention : cela ne donne pas un moyen de prouver l'existence parce que rien ne prouve pour l'instant que \( f-g\) peut effectivement être écrit sous la forme \( VQ\), c'est-à-dire que \( f-g\) soit divisible par \( V\). C'est cela que nous allons nous atteler à démontrer maintenant.

	Nous commençons par prouver que \( f+g\) est symétrique et \( f-g\) alterné. Si \( \sigma\) est une transposition,
	\begin{equation}
		\sigma\cdot(f+g)=\sigma\cdot f+\sigma\tau\cdot f=g+f
	\end{equation}
	parce que \( \sigma\tau\) est pair. De la même façon,
	\begin{equation}
		\sigma\cdot(f-g)=g-f=\epsilon(\sigma)(f-g).
	\end{equation}
	Dans les deux cas nous concluons en utilisant le fait que toute permutation est un produit de transpositions (proposition~\ref{PropPWIJbu}) et que \( \epsilon\) est un homomorphisme.

	Soient maintenant deux entiers \( h<k\) dans \( \{ 1,\ldots, n \}\) et l'anneau
	\begin{equation}
		\big( \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \big)[T_k].
	\end{equation}
	Cet anneau contient le polynôme \( T_k-T_h\) où \( T_k\) est la variable et \( T_h\) est un coefficient. Nous faisons la division euclidienne de \( f-g\) par  \( T_k-T_h\) parce que nous avons dans l'idée de faire arriver le déterminant de Vandermonde et donc le produit de toutes les différences \( T_k-T_h\) :
	\begin{equation}    \label{EqSHdgrG}
		f-g=(T_k-T_h)q+r
	\end{equation}
	où \( \deg_{T_k}r<1\), c'est-à-dire que \( r\) ne dépends pas de \( T_k\). Nous revoyons maintenant l'égalité \eqref{EqSHdgrG} dans \( \eK[T_1,\ldots, T_n]\) et nous y appliquons la transposition \( \tau_{kh}\). Nous savons que \( \tau_{kh}(f-g)=-(f-g)\) et \( \tau_{kh}(T_k-T_h)=-(T_k-T_h)\), et donc
	\begin{equation}    \label{EqVOhjKB}
		-(f-g)=-(T_k-T_h)\tau_{kh}\cdot   q+\tau_{kh}\cdot r
	\end{equation}
	où \(\tau_{kh}\cdot r\) ne dépend pas de \( T_h\). Nous appliquons à \eqref{EqVOhjKB} l'application
	\begin{equation}
		\begin{aligned}
			t\alpha\colon \eK[T_1,\ldots, T_n]        & \to \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \\
			\alpha(PT_1,\ldots, \hat T_k,\ldots, T_n) & =P(T_1,\ldots, T_h,\ldots, T_n).
		\end{aligned}
	\end{equation}
	Cette application vérifie \( \alpha\big( \tau_{kh}\cdot r \big)=\alpha(r)\) et nous avons
	\begin{equation}
		-\alpha(f-g)=\alpha(r).
	\end{equation}
	Puis en appliquant \( \alpha\) à la relation \( f-g=(T_k-T_h)q+r\), nous trouvons
	\begin{equation}
		\alpha(f-g)=\alpha(r),
	\end{equation}
	et par conséquent \( \alpha(r)=0\). Ici nous utilisons l'hypothèse de caractéristique différente de deux. Dire que \( \alpha(r)=0\), c'est dire que \( r\) est divisible par \( T_k-T_h\), mais \( r\) étant de degré zéro en \( T_k\), nous avons \( r=0\). Par conséquent \( T_k-T_h\) divise \( f-g\) pour tout \( h<k\), et nous pouvons définir un polynôme \( Q\) par
	\begin{equation}    \label{EqrnbgdA}
		f-g=2Q\prod_{h<k}\prod_{k\leq n}(T_k-T_h)=2Q(T_1,\ldots, T_n)V(T_1,\ldots, T_n),
	\end{equation}
	où nous avons utilisé la formule du déterminant de Vandermonde de la proposition~\ref{PropnuUvtj}.

	Étant donné que \( f+g\) est un polynôme symétrique, nous allons aussi poser \( f+g=2P\) avec \( P\) symétrique.

	Montrons à présent que \( Q\) est un polynôme symétrique. Soit \( \sigma\in S_n\); vu que nous savons déjà que \( f-g\) est alternée, nous avons
	\begin{equation}    \label{EqpSPEyq}
		\sigma\cdot (f-g)=\epsilon(\sigma)(f-g)=\epsilon(\sigma)2QV,
	\end{equation}
	Mais en appliquant \( \sigma\) à l'équation \eqref{EqrnbgdA},
	\begin{subequations}
		\begin{align}
			\sigma\cdot (f-g) & =2(\sigma\cdot V)(T_1,\ldots, ,T_n)(\sigma\cdot Q)(T_1,\ldots,T_n)    \\
			                  & =2\epsilon(\sigma)V(T_1,\ldots, T_n)(\sigma\cdot Q)(T_1,\ldots, T_n).
		\end{align}
	\end{subequations}
	Nous égalisons cela avec \eqref{EqpSPEyq} et nous souvenant que l'anneau \( \eK[T_1,\ldots, T_n]\) est intègre par le théorème \ref{ThoBUEDrJ}. Ensuite nous simplifions par \( 2\epsilon(\sigma)V\) pour obtenir
	\begin{equation}
		Q=\sigma\cdot Q,
	\end{equation}
	c'est-à-dire que \( Q\) est symétrique.

	Au final nous avons \( f+q=2P\) et \( f-g=2VQ\) avec \( P\) et \( Q\) symétriques. En faisant la somme,
	\begin{equation}
		f=P+VQ.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Gram}
%---------------------------------------------------------------------------------------------------------------------------

Si \( x_1,\ldots, x_r\) sont des vecteurs d'un espace vectoriel, alors le \defe{déterminant de Gram}{déterminant!Gram}\index{Gram (déterminant)} est le déterminant
\begin{equation}
	G(x_1,\ldots, x_r)=\det\big( \langle x_i, x_j\rangle  \big).
\end{equation}
Notons que la matrice est une matrice symétrique.

\begin{proposition}\label{PropMsZhIK}
	Si \( F\) est un sous-espace vectoriel de base \( \{ x_1,\ldots, x_n \}\) et si \( x\) est un vecteur, alors le déterminant de Gram est un moyen de calculer la distance entre \( x\) et \( F\) par
	\begin{equation}
		d(x,F)^2=\frac{ G(x,x_1,\ldots, x_n)}{ G(x_1,\ldots, x_n) }.
	\end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Cauchy}
%---------------------------------------------------------------------------------------------------------------------------

Soient des nombres \( a_i\) et \( b_i\) (\( i=1,\ldots, n\)) tels que \( a_i+b_j\neq 0\) pour tout couple \( (i,j)\). Le \defe{déterminant de Cauchy}{déterminant!de Cauchy}\index{Cauchy!déterminant} est
\begin{equation}
	D_n=\det\left( \frac{1}{ a_i+b_j } \right).
\end{equation}

\begin{proposition}[\cite{RollandRobertjyYDzY}] \label{ProptoDYKA}
	Le déterminant de Cauchy est donné par la formule
	\begin{equation}
		D_n=\frac{ \prod_{i<j}(a_j-a_i)\prod_{i<j}(b_j-b_i) }{ \prod_{ij}(a_i+b_j) }.
	\end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSQBJfr}

\begin{definition}[Matrice de Sylvester, résultant\cite{BIBooCYHXooCRvJJh}]        \label{DEFooHGZDooXIvTaP}
	Soient \( P\) et \( Q\) deux polynômes non nuls, de degrés respectifs \( n\) et \( m\) :
	\begin{subequations}
		\begin{align}
			P(x)=p_0+p_1x+\cdots +p_nx^n \\
			Q(x)=q_0+q_1x+\cdots +q_mx^m.
		\end{align}
	\end{subequations}
	La \defe{matrice de Sylvester}{matrice!de Sylvester}\index{Sylvester (matrice)} associée à \( P\) et \( Q\) est la matrice carrée \( m+n\times m+n\) définie ainsi :
	\begin{enumerate}
		\item
		      la première ligne est formée des coefficients de \( P\), suivis de 0 :
		      \begin{equation}
			      \begin{pmatrix} p_n & p_{n-1} & \cdots & p_1 & p_0 & 0 & \cdots & 0 \end{pmatrix} ;
		      \end{equation}
		\item la seconde ligne s'obtient à partir de la première par permutation circulaire vers la droite ;
		\item les \( (m-2)\) lignes suivantes s'obtiennent en répétant la même opération ;
		\item la ligne \( (m+1)\) est formée des coefficients de \( Q\), suivis de 0 :
		      \begin{equation}
			      \begin{pmatrix} q_m & q_{m-1} & \cdots & q_1 & q_0 & 0 & \cdots & 0 \end{pmatrix} ;
		      \end{equation}
		\item les \( (n-1)\) lignes suivantes sont formées par des permutations circulaires.
	\end{enumerate}
	Le déterminant de la matrice de Sylvester associée à \( P\) et \( Q\) est appelé le \defe{résultant}{résultant} de \( P\) et \( Q\) et noté \( \res(P,Q)\)\nomenclature[A]{\( \res(P,Q)\)}{résultant des polynômes \( P\) et \( Q\)}.
\end{definition}


Ainsi dans le cas \( n=4\) et \( m=3\), la matrice obtenue est
\begin{equation}    \label{EqPEgtle}
	S_{p,q}=\begin{pmatrix}
		p_4 & p_3 & p_2 & p_1 & p_0 & 0   & 0   \\
		0   & p_4 & p_3 & p_2 & p_1 & p_0 & 0   \\
		0   & 0   & p_4 & p_3 & p_2 & p_1 & p_0 \\
		q_3 & q_2 & q_1 & q_0 & 0   & 0   & 0   \\
		0   & q_3 & q_2 & q_1 & q_0 & 0   & 0   \\
		0   & 0   & q_3 & q_2 & q_1 & q_0 & 0   \\
		0   & 0   & 0   & q_3 & q_2 & q_1 & q_0 \\
	\end{pmatrix}.
\end{equation}

Attention : si \( P\) est de degré \( n\) et \( Q\) de degré \( m\), il y a \( m\) lignes pour \( P\) et \( n\) pour \( Q\) dans le déterminant du résultant (et non le contraire).

\begin{lemma}[\cite{QQuRUzA}]       \label{LemBFrhgnA}
	Si \( P\) et \( Q\) sont deux polynômes de degrés \( n\) et \( m\) à coefficients dans l'anneau \( \eA\), alors pour tout \( \lambda\in \eA\),
	\begin{subequations}
		\begin{align}
			\res(\lambda P,Q) & =\lambda^m\res(P,Q)  \\
			\res(P,\lambda Q) & =\lambda^n\res(P,Q).
		\end{align}
	\end{subequations}
\end{lemma}

\begin{proof}
	Cela est simplement un comptage du nombre de lignes. Il y a \( m\) lignes contenant les coefficients de \( P\); donc prendre \( \lambda P\) revient à multiplier \( m\) lignes dans un déterminant et donc le multiplier par \( \lambda^m\).
\end{proof}

L'équation de Bézout \eqref{EqkbbzAi}\index{théorème!Bézout!utilisation} peut être traitée avec une matrice de Sylvester. Soient \( P\) et \( Q\), deux polynômes donnés et à résoudre l'équation
\begin{equation}    \label{EqSsyXOo}
	xP+yQ=0
\end{equation}
par rapport aux polynômes inconnus \( x\) et \( y\) dont les degrés sont \( \deg(x)<\deg(Q)\) et \( \deg(y)<\deg(P)\). Si nous notons \( \tilde x\) et \( \tilde y\) la liste des coefficients de \( x\) et \( y\) (dans l'ordre décroissant de degré), nous pouvons récrire l'équation \eqref{EqSsyXOo} sous la forme
\begin{equation}
	S_{PQ}^t\begin{pmatrix}
		\tilde x \\
		\tilde y
	\end{pmatrix}=0.
\end{equation}
Pour s'en convaincre, écrivons pour les polynômes de l'exemple \eqref{EqPEgtle} :
\begin{equation}
	\begin{pmatrix}
		p_4 & 0   & 0   & q_3 & 0   & 0   & 0   \\
		p_3 & p_4 & 0   & q_2 & q_3 & 0   & 0   \\
		p_2 & p_3 & p_4 & q_1 & q_2 & q_3 & 0   \\
		p_1 & p_2 & p_3 & q_0 & q_1 & q_2 & q_3 \\
		p_0 & p_1 & p_2 & 0   & q_0 & q_1 & q_2 \\
		0   & p_0 & p_1 & 0   & 0   & q_0 & q_1 \\
		0   & 0   & p_0 & 0   & 0   & 0   & q_0 \\
	\end{pmatrix}\begin{pmatrix}
		x_2 \\
		x_1 \\
		x_0 \\
		y_3 \\
		y_2 \\
		y_1 \\
		y_0
	\end{pmatrix}=
	\begin{pmatrix}
		x_2p_4+y_3q_3               \\
		p_3x_2+p_4x_1+q_2y_3+q_3y_2 \\
		\\
		\\
		\vdots                      \\
		\\
	\end{pmatrix}
\end{equation}
Nous voyons que sur la ligne numéro \( k\) (en partant du bas et en numérotant de à partir de zéro) nous avons les produits \( p_ix_j\) et \( q_iy_j\) avec \( i+j=k\). La colonne de droite représente donc bien les coefficients du polynôme \( xP+yQ\).


\begin{proposition} \label{PropAPxzcUl}
	Le résultant de deux polynômes est non nul si et seulement si les deux polynômes sont premiers entre eux.
\end{proposition}
\index{déterminant!résultant}

Un polynôme \( P\) a une racine double en \( a\) si et seulement si \( P\) et \( P'\) ont \( a\) comme racine commune, ce qui revient à dire que \( P\) et \( P'\) ne sont pas premiers entre eux.

Une application importante de ces résultats sera le théorème de Rothstein-Trager~\ref{ThoXJFatfu} sur l'intégration de fractions rationnelles.

\begin{example}
	Si nous prenons \( P=aX^2+bX+c\) et \( P'=2aX+b\) alors la taille de la matrice de Sylvester sera \( 2+1=3\) et
	\begin{equation}
		S_{P,P'}=\begin{pmatrix}
			a  & b  & c \\
			2a & b  & 0 \\
			0  & 2a & b
		\end{pmatrix}.
	\end{equation}
	Le résultant est alors
	\begin{equation}
		\res(P,P')=-a(b^2-4ac).
	\end{equation}
	Donc un polynôme du second degré a une racine double si et seulement si \( b^2-4ac=0\). Cela est un résultat connu depuis longtemps mais qui fait toujours plaisir à revoir.
\end{example}

La matrice de Sylvester permet aussi de récrire l'équation de Bézout pour les polynômes; voir le théorème~\ref{ThoBezoutOuGmLB} et la discussion qui s'ensuit.

Une proposition importante du résultant est qu'il peut s'exprimer à l'aide des racines des polynômes.
\begin{proposition} \label{PropNDBOGNx}
	Si
	\begin{subequations}
		\begin{align}
			P(X) & =a_p\prod_{i=1}^p(X-\alpha_i) \\
			Q(X) & =b_q\prod_{j=1}^q(X-\beta_i)
		\end{align}
	\end{subequations}
	alors nous avons les expressions suivantes pour le résultant :
	\begin{equation}        \label{EqCFUumjx}
		\res(P,Q)=a_p^qb_q^p\prod_{i=1}^p\prod_{j=1}^q(\beta_j-\alpha_i)=b_q^p\prod_{j=1}^qP(\beta_j)=(-1)^{pq}a_p^q\prod_{i=1}^pQ(\alpha_i).
	\end{equation}
\end{proposition}

\begin{proof}
	Si \( P\) et \( Q\) ne sont pas premiers entre eux, d'une part la proposition~\ref{PropAPxzcUl} nous dit que \( \res(P,Q)=0\) et d'autre part, \( P\) et \( Q\) ont un facteur irréductible en commun, ce qui  signifie que nous devons avoir un des \( X-\alpha_i\) égal à un des \( X-\beta_j\). Autrement dit, nous avons \( \alpha_i=\beta_j\) pour un couple \( (i,j)\). Par conséquent tous les membres de l'équation \eqref{EqCFUumjx} sont nuls.

	Nous supposons donc que \( P\) et \( Q\) sont premiers entre eux. Nous commençons par supposer que les polynômes \( P\) et \( Q\) sont unitaires, c'est-à-dire que \( a_p=b_q=1\). Nous considérons alors l'anneau
	\begin{equation}
		\eA=\eZ[\alpha_1,\ldots, \alpha_p,\beta_1,\ldots, \beta_q].
	\end{equation}
	Dans cet anneau, l'élément \( \beta_j-\alpha_i\) est irréductible (tout comme \( X-Y\) est irréductible dans \( \eZ[X,Y]\)). Le résultant \( R=\res(P,Q)\) est un élément de \( \eA\) parce que tous leurs coefficients peuvent être exprimés à l'aide des \( \alpha_i\) et des \( \beta_j\). Dans \( \eA\), l'élément \( \beta_j-\alpha_i\) divise \( R\). En effet lorsque \( \beta_j=\alpha_i\), le déterminant définissant le résultant est nul, ce qui signifie que \( \beta_j-\alpha_i\) est un facteur irréductible de \( R\).

	Par conséquent il existe un polynôme \( T\in \eA\) tel que
	\begin{equation}
		R=T(\alpha_1,\ldots, \beta_q)\prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i).
	\end{equation}
	Comptons les degrés. Pour donner une idée de ce calcul de degré, voici comment se présente, au niveau des dimensions, le déterminant :
	\begin{equation}  \label{EqJCaATOH}
		\xymatrix{%
		\ar@{<->}[rrr]^{p+1}&&&& \ar@{<->}[r]^{q-1}  &\\
		a_p\ar@{.}[rrd] &a_{p-1}\ar@{.}[rr]  &  & a_0\ar@{.}[rrd] & 0\ar@{.}[r]&0&\ar@{<->}[d]^q \\
		0\ar@{.}[r]&0&a_p\ar@{.}[rr]&&a_1&a_0&\\
		\ar@{<->}[rrrrr]_{p+q}&&&&&&
		}
	\end{equation}
	si les \( a_i\) sont les coefficients de \( P\). Mais chacun des \( a_i\) est de degré \( 1\) en les \( \alpha_i\), donc le déterminant dans son ensemble est de degré \( q\) en les \( \alpha_i\), parce que \( R\) contient \( q\) lignes telles que \eqref{EqJCaATOH}. Le même raisonnement montre que \( R\) est de degré \( p\) en les \( \beta_j\). Par ailleurs le polynôme \( \prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i)\) est de degré \( p\) en les \( \beta_j\) et \( q\) en les \( \alpha_i\). Nous en déduisons que \( T\) doit être un polynôme ne dépendant pas de \( \alpha_i\) ou de \( \beta_j\).

	Nous pouvons donc calculer la valeur de \( T\) en choisissant un cas particulier. Avec \( P(X)=X^p\) et \( Q(X)=X^q+1\), il est vite vu que \( R(P,Q)=1\) et donc que \( T=1\).

	Si les polynômes \( P\) et \( Q\) ne sont pas unitaires, le lemme~\ref{LemBFrhgnA} nous permet de conclure.

\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Kronecker}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons \( K_n\) l'ensemble des polynômes de \( \eZ[X]\)
\begin{enumerate}
	\item
	      unitaires de degré \( n\),
	\item
	      dont les racines dans \( \eC\) sont de modules plus petits ou égaux à \( 1\),
	\item
	      et qui ne sont pas divisés par \( X\).
\end{enumerate}
Un tel polynôme s'écrit sous la forme
\begin{equation}
	P=X^n+\sum_{k=0}^{n-1}a_kX^k.
\end{equation}

\begin{theorem}[Kronecker\cite{KXjFWKA}]    \label{ThoOWMNAVp}
	Les racines des éléments de \( K_n\) sont des racines de l'unité.
\end{theorem}
\index{théorème!Kronecker}
\index{polynôme!à plusieurs indéterminées}
\index{résultant!utilisation}
\index{polynôme!symétrique}

\begin{proof}
	Vu que \( \eC\) est algébriquement clos
	nous pouvons considérer les racines \( \alpha_1,\ldots, \alpha_n\) de \( P\) dans \( \eC\). Nous les considérons avec leurs multiplicités.
	%TODO : lorsqu'on aura démontré que \eC est algébriquement clos, il faudra le référentier ici.

	Soit \( R=X^n+\sum_{k=0}^{n-1}b_kX^k\) un élément de \( K_n\) dont nous notons \( \beta_1,\ldots, \beta_n\) les racines dans \( \eC\). Les relations coefficients-racines stipulent que
	\begin{equation}
		b_k=\sum_{1\leq i_1<\ldots <i_{n-k}\leq n}\prod_{j=1}^{n-k}\beta_{i_j}.
	\end{equation}
	En prenant le module et en se souvenant que \( | \beta_{l} |\leq 1\) pour tout \( l\), nous trouvons que
	\begin{equation}
		| b_k |\leq\binom{ n }{ n-k }.
	\end{equation}
	Mais comme \( b_k\in \eZ\), nous avons
	\begin{equation}
		b_k\in\big\{    -\binom{ n }{ n-k },-\binom{ n }{ n-k }+1,\ldots, 0,\ldots,\binom{ n }{ n-k }   \big\}
	\end{equation}
	qui est de cardinal \( 2\binom{ n }{ n-k }+1\). Nous avons donc
	\begin{equation}
		\Card(K_n)\leq\prod_{k=0}^{n-1}\big( 1+\binom{ n }{ n-k } \big)<\infty.
	\end{equation}
	La conclusion jusqu'ici est que \( K_n\) est un ensemble fini.

	Pour chaque \( k\in \eN^*\) nous considérons les polynômes
	\begin{subequations}
		\begin{align}
			P_k & =\prod_{i=1}^n(X-\alpha_i^k) \\
			Q_k & =X^k-Y\in \eZ[X,Y],
		\end{align}
	\end{subequations}
	et puis nous considérons le résultant \( R_k=\res_X(P,Q_k)\in \eZ[Y]\) :
	\begin{equation}
		R_k=\res_X(P,Q_k)=
		\begin{pmatrix}
			1      & a_{n-1} & \cdots  & a_0    & 0       & \cdots  & 0       & 0      & 0   \\
			0      & 1       & a_{n-1} & \cdots & a_0     & 0       & \cdots  & 0      & 0   \\
			\vdots & \ddots  & \ddots  & \ddots &         & \ddots  &                        \\
			0      & \cdots  & 0       & 1      & a_{n-1} & \cdots  & a_0     & 0      & 0   \\
			0      & \cdots  & 0       & 0      & 1       & a_{n-1} & \cdots  & a_0    & 0   \\
			0      & \cdots  & 0       & 0      & 0       & 1       & a_{n-1} & \cdots & a_0 \\
			\\
			1      & 0       & \ldots  & 0      & -Y      & 0       & \ldots  & 0      & 0   \\
			0      & 1       & 0       & \ldots & 0       & -Y      & 0       & \ldots & 0   \\
			       &         & \ddots  &        &         & \ddots  & \ddots                 \\
			0      & \cdots  & 0       & 1      & 0       & \cdots  & 0       & -Y     & 0   \\
			0      & 0       & \cdots  & 0      & 1       & 0       & \cdots  & 0      & -Y
		\end{pmatrix}
	\end{equation}
	Cela est un polynôme en \( Y\) dont le terme de plus haut degré est \( (-1)^nY^n\). Les petites formules de la proposition~\ref{PropNDBOGNx} nous permettent d'exprimer \( R_k(Y)\) en termes des racines de \( P\) :
	\begin{equation}
		R_k(Y)=\prod_{i=1}^nQ_k(\alpha_i)=\prod_{i=1}^n(\alpha_i^k-Y)=(-1)^n\prod_{i=1}^n(Y-\alpha_i^k)=(-1)^nP_k(Y).
	\end{equation}
	Vu que \( P\in K_n\) nous savons que les \( \alpha_i\) ne sont pas tous nuls; donc \( P_k\in K_n\). Cependant nous avons vu que \( K_n\) est un ensemble fini; donc parmi les \( P_k\), il y a des doublons (et pas un peu)\quext{Ici dans \cite{KXjFWKA}, il déduit qu'on a un \( k\) tel que \( P_k=P_1=P\). Moi je vois pourquoi on a un \( k\) et un \( l\) tels que \( P_k=P_l\), mais pourquoi on peut en trouver un spécialement égal au premier ? Une réponse à cette question permettrait de solidement réduire la lourdeur de la suite de la preuve.}. Nous regardons même l'ensemble des \( P_{2^n}\) dans lequel nous pouvons en trouver deux les mêmes. Soit \( l>k\) tels que \( P_{2^k}=P_{2^l}\). Si \( \alpha\) est racine de \( P_{2^k}\), alors il est de la forme \( \alpha=\beta^{2^k}\) pour une certaine racines \( \beta\) de \( P\). Par conséquent
	\begin{equation}    \label{EqBEgJtzm}
		\alpha^{2^l/2^k}=\alpha^{2^{l-k}}
	\end{equation}
	est racine de \( P_{2^l}\). Notons que dans cette expression il n'y a pas de problèmes de définition d'exposant fractionnaire dans \( \eC\) parce que \( l>k\). Vu que \eqref{EqBEgJtzm} est racine de \( P_{2^l}\), il est aussi racine de \( P_{2^k}\). Donc
	\begin{equation}
		\big( \alpha^{2^{l-k}} \big)^{2^{l-k}}=\alpha^{2^{2(l-k)}}
	\end{equation}
	est racine de \( P_{2^l}\) et donc de \( P_{2^k}\). Au final nous savons que tous les nombres de la forme \( \alpha^{2^{n(l-k)}}\) sont racines de \( P_{2^k}\). Mais comme \( P_{2^k}\) a un nombre fini de racines, nous pouvons en trouver deux égales. Si nous avons
	\begin{equation}
		\alpha^{2^{n(l-k)}}=\alpha^{2^{m(l-k)}}
	\end{equation}
	pour certains entiers \( m>n\), alors
	\begin{equation}
		\alpha^{2^{n(l-k)}-2^{m(l-k)}}=1,
	\end{equation}
	ce qui prouve que \( \alpha\) est une racine de l'unité. Nous avons donc prouvé que toutes les racines de \( P_{2^k}\) sont des racines de l'unité et donc que les racines de \( P\) sont racines de l'unité.
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Orientation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Cas vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{BIBooEMJPooWLMWSd}]        \label{DEFooNVRHooEBHUSu}
	Soient deux bases \( \mB\) et \( \mB'\) d'un espace vectoriel réel \( E\). Nous définissons la relation \( \mB\sim \mB'\) si et seulement si \( \det_{\mB}(\mB')>0\)\footnote{Définition \ref{DEFooODDFooSNahPb}.}.

	Cela est une relation d'équivalence\footnote{Définition \ref{DefHoJzMp}.} sur l'ensemble des bases de \( E\), et les classes sont les \defe{orientations}{orientation} de \( E\).
\end{propositionDef}

\begin{proof}
	Tout est dans le lemme \ref{LemJMWCooELZuho}. D'abord quand \( \mB\) et \( \mB'\) sont des bases, \( \det_{\mB}(\mB')\neq 0\) ensuite, nous passons en revue les points qu'il faut pour être une relation d'équivalence.
	\begin{enumerate}
		\item
		      \( \mB\sim \mB\) parce que \( \det_{\mB}(\mB)=1>0\).
		\item
		      Vu que \( \det_{\mB}(\mB')=\frac{1}{ \det_{\mB'}(\mB) }\), les deux sont positifs en même temps ou pas du tout.
		\item
		      Si \( \mB\sim \mB'\) et \( \mB'\sim\mB''\), alors en utilisant la formule
		      \begin{equation}
			      \det_{\mB}(\mB'')=\det_{\mB}(\mB')\det_{\mB'}(\mB''),
		      \end{equation}
		      nous voyons que \( \det_{\mB}(\mB'')>0\).
	\end{enumerate}
\end{proof}

\begin{lemma}
	Soit un espace vectoriel réel \( E\). L'ensemble des bases de \( E\) possède exactement deux orientations\footnote{Définition \ref{DEFooNVRHooEBHUSu}.}
\end{lemma}

\begin{proof}
	Nous considérons une base \( \mB=(e_1,\ldots, e_n)\)\footnote{Nous notons \( (e_1,e_2)\) et non \( \{ e_1,e_2 \}\) parce que l'ordre est important.} à partir de laquelle nous définissons une autre base : \( \mB'=( -e_1,e_2,\ldots, e_n )\). Nous allons prouver que ces deux bases ne sont pas équivalentes, et que toute base de \( E\) est équivalente soit à \( \mB\) soit à \( \mB'\).

	\begin{subproof}
		\spitem[Au moins deux classes]
		Le fait que \( \det_{\mB}(\mB')=-1\) vient du fait que \( \det_{\mB}(\mB)=1\) et que l'application \( \det_{\mB}\) est \( n\)-linéaire; en multipliant par \( -1\) le premier argument, la valeur du déterminant est multipliée par \( -1\).

		Donc les bases \( \mB\) et \( \mB'\) ne sont pas équivalentes et il existe au moins deux classes.

		\spitem[Au plus deux classes]
		Nous montrons à présent que toute base est équivalente soit à \( \mB\) soit à \( \mB'\). Supposons que \( \mB''\) ne soit pas équivalente à \( \mB\), c'est-à-dire que \( \det_{\mB}(\mB'')<0\). Nous utilisons encore la formule \eqref{EqAWICooBLTTOY},
		\begin{equation}
			\underbrace{\det_{\mB}(\mB'')}_{<0}=\underbrace{\det_{\mB}(\mB')}_{<0}\det_{\mB'}(\mB''),
		\end{equation}
		et nous déduisons que \( \det_{\mB'}(\mB'')>0\).
	\end{subproof}
\end{proof}

\begin{normaltext}
	Vu qu'il n'y a que deux classes d'équivalence parmi les bases, nous pouvons utiliser le vocable «avoir la même orientation que» ou «avoir l'orientation contraire de». Ce n'est pas ambigu.
\end{normaltext}

\begin{proposition}[\cite{BIBooEMJPooWLMWSd}]
	Si \( \mB\) est une base de l'espace vectoriel \( E\) de dimension \( n\), et si \( \tau\) est une transposition\footnote{Définition \ref{DEFooXNAFooGTbTTJ}.} de \( S_n\), alors la base \( \tau(\mB)\) est de sens contraire.
\end{proposition}

\begin{proof}
	Le lemme \ref{LemJMWCooELZuho}\ref{ITEMooTXXBooBmDtzd} dit que \( \det_{\mB}\) est une forme anti-symétrique; donc
	\begin{equation}
		\det_{\mB}(\mB')=-\det_{\mB}\big( \tau(\mB)' \big).
	\end{equation}
	Si l'un est positif, l'autre est négatif. Elles ont donc des orientations contraires.
\end{proof}

\begin{corollary}
	Si \( \mB\) est une base de l'espace vectoriel \( E\) de dimension \( n\), et si \( \sigma\in S_n\), la base \( \sigma(\mB)\) a même orientation que \( \mB\) si et seulement si \( \sigma\in A_n\).
\end{corollary}

\begin{proof}
	Notons \( c_1\) la classe d'orientation de \( \mB\) et \( c_2\) l'autre classe. La permutation \( \sigma\) se décompose en produit de transpositions dont la parité est fixée (proposition \ref{PROPooKRHEooAxtmRv}). Posons \( \sigma=\tau_k\ldots \tau_1\).

	En posant \( \mB_0=\mB\) et \( \mB_{l+1}=\tau_{l+1}(\mB_l)\), pour tout \( l\), la base \( \mB_l\) est d'orientation contraire à celle de la base \( \mB_{l-1}\). Une base sur deux a l'orientation de \( \mB\) et l'autre sur deux a l'orientation contraire.

	Donc \( \sigma(\mB)\) a la même orientation que \( \mB\) si et seulement si \( k\) est pair. Mais \( \sigma\in A_n\) si et seulement si \( k\) est pair. C'est bon.
\end{proof}

\begin{propositionDef}[\cite{BIBooEMJPooWLMWSd}]        \label{PROPooNBAXooKNUrnk}
	Soit un espace vectoriel réel, et un endomorphisme \( f\) de \( E\). Deux définitions.
	\begin{enumerate}
		\item       \label{ITEMooOAXFooLIPHlW}
		      L'endomorphisme \( f\) est \defe{direct}{endomorphisme direct} si sont déterminant est strictement positif.
		\item       \label{ITEMooNKYCooXTgKJA}
		      L'endomorphisme \defe{préserve l'orientation}{endomorphisme préserve l'orientation} si il transforme toute base de \( E\) en une base de même orientation.
	\end{enumerate}
	Un endomorphisme est direct si et seulement si il préserve l'orientation.
\end{propositionDef}

\begin{proof}
	En deux sens.
	\begin{subproof}
		\spitem[Direct implique préserve l'orientation]
		Soit une base \( \mB\) de \( E\) et un endomorphisme direct \( u\). D'abord, \( u\) est inversible du fait que son déterminant est non nul par la proposition \ref{PropYQNMooZjlYlA}\ref{ITEMooNZNLooODdXeH}. Donc \( u\) transforme une base en une base par le lemme \ref{LEMooDJSIooYcsvhO}.

		La définition \ref{LEMooQTRVooAKzucd} du déterminant de \( u\) est que
		\begin{equation}
			\det(u)=\det_{\mB}\big( u(\mB) \big)>0.
		\end{equation}
		Donc \( \mB\) et \( u(\mB)\) ont même orientation.
		\spitem[Préserve l'orientation implique direct]
		Le fait que \( u\) préserve l'orientation signifie en particulier qu'il transforme une base en une base et qu'il est inversible par le lemme \ref{LEMooDJSIooYcsvhO}.

		Donc si \( \mB\) est une base, \( u(\mB)\) est encore une base et nous avons, parce que \( \mB\) et \( u(\mB)\) ont même orientation,
		\begin{equation}
			0<\det_{\mB}\big( u(\mB) \big)=\det(u).
		\end{equation}
	\end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Cas affine}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooOTFPooIVkHFP}
	Nous disons qu'une application affine \( f\colon \affE\to \affE\) \defe{préserve l'orientation}{préserve l'orientation} si sa partie linéaire préserve l'orientation.
\end{definition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Hermitien, orthogonal, adjoint}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


\begin{normaltext}      \label{NORMooWGEJooCtGtqZ}
	Une des choses à retenir de la définition de l'opérateur adjoint est que la notion de \( A^*\) dépend du produit scalaire considéré.

	Il se fait que le plus souvent, sur \( \eR^n\), nous considérons le produit scalaire usuel et la base canonique. De ce fait, les notions d'opérateur adjoint et d'opérateur transposés se confondent avec la notion de matrice transposée. Ce sont pourtant, en général, trois notions distinctes.
\end{normaltext}

\begin{propositionDef}[Définition de la transposée\cite{MonCerveau}]\label{DEFooROVNooFlTbSK}
	Soient deux espaces vectoriels euclidiens ou hermitiens \( E\) et \( F\) et une application linéaire \( A\colon E\to F\).
	\begin{enumerate}
		\item       \label{ITEMooRUZWooSZgGnf}
		      Il existe une unique application linéaire \( B\colon F\to E\) telle que
		      \begin{equation}        \label{EQooHWYKooFzAGgB}
			      \langle Ax, y\rangle_F=\langle x, By\rangle_E
		      \end{equation}
		      pour tout \( x\in E\) et \( y\in F\).
		\item   \label{ITEMooXXEUooPtfPKY}
		      Si \( \{ e_i \}\) est une base orthonormée de \( E\) et \( \{ f_{\alpha} \}\) est une base orthonormée de \( F\), alors la matrice de \( A\) et \( B\) pour ces bases sont liées par
		      \begin{equation}       \label{EQooUSNVooQtRNGL}
			      B_{i\alpha}=A_{\alpha i}.
		      \end{equation}
	\end{enumerate}
	L'application \( B\) ainsi définie set nommée \defe{adjoint}{adjoint} de \( A\) et sera notée \( B=A^*\).
\end{propositionDef}

\begin{proof}
	Pour l'unicité, nous écrivons la condition avec \( x=e_j\) pour obtenir :
	\begin{equation}
		\langle Ae_j, y\rangle = \langle e_j, By\rangle =(By)_j
	\end{equation}
	c'est-à-dire que les coefficients \( B(y)_j\) de \( B(y)\) dans la base canonique sont fixés par la condition.

	Pour l'existence, il suffit de vérifier que poser
	\begin{equation}
		B(y)=\sum_j\langle Ae_j, y\rangle e_j
	\end{equation}
	fonctionne. Pour cela il faut utiliser la bilinéarité du produit scalaire et le fait que \( \langle x, e_j\rangle =x_j\). Nous avons :
	\begin{subequations}
		\begin{align}
			\langle x, B(y)\rangle & =\langle x, \sum_j\langle A(e_j), y\rangle e_j\rangle \\
			                       & =\sum_j\langle A(e_j), y\rangle \langle x, e_j\rangle \\
			                       & =\sum_j\langle A(x_je_j), y\rangle                    \\
			                       & =\langle A(x), y\rangle .
		\end{align}
	\end{subequations}

	En ce qui concerne la matrice de l'application \( B\) ainsi définie, nous écrivons la condition \eqref{EQooHWYKooFzAGgB} avec \( y=e'_{\alpha}\) et \( x=e_i\), de telle sorte que
	\begin{equation}
		A(x)=A(e_i)=\sum_{\beta}A_{\beta i}e'_{\beta}
	\end{equation}
	et
	\begin{equation}
		B(y)=B(e'_{\alpha})=\sum_jB_{j\alpha}e_j.
	\end{equation}
	Alors nous avons :
	\begin{equation}
		\sum_{\beta}A_{\beta i}\langle e'_{\beta}, e'_{\alpha}\rangle =\sum_j B_{j\alpha}\langle e_i, e_j\rangle ,
	\end{equation}
	donc
	\begin{equation}
		A_{\alpha i}=B_{i \alpha}.
	\end{equation}
\end{proof}

\begin{normaltext}
	À cause de l'expression \eqref{EQooUSNVooQtRNGL} pour la matrice de \( A^*\), cette application est souvent appelé \defe{transposé}{transposé} de \( A\) et noté \( A^t\). Nous allons cependant voir plus tard (définition\ref{DefooZLPAooKTITdd}) que la transposée de \( A\) est une application \( A^t\colon F^*\to E^*\). Il nous arrivera cependant d'écrire des égalités comme \( \langle Ax, y\rangle=\langle x, A^ty\rangle  \).
\end{normaltext}

\begin{proposition}     \label{PROPooSHZMooGwdfBd}
	En ce qui concerne le déterminant,
	\begin{equation}
		\det(A^*)=\det(A)^*
	\end{equation}
	où l'étoile à droite dénote la conjugaison complexe dans \( \eC\).
\end{proposition}

\begin{proof}
	Écrivons l'expression explicite \eqref{EQooOJEXooXUpwfZ} du déterminant. Le tout avec la base canonique :
	\begin{equation}
		\det(A)=\det_{(e_1,\ldots, e_n)}(Ae_1,\ldots, Ae_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne_{\sigma(i)}^*(Ae_i).
	\end{equation}
	Mais nous pouvons développer :
	\begin{equation}
		e^*_{\sigma(i)}(Ae_i)=\langle e_{\sigma(i)}, Ae_i\rangle =\langle A^*e_{\sigma(i)}, e_i\rangle =\langle e_i, A^*e_{\sigma(i)}\rangle^*=e_i^*(A^*e_{\sigma(i)})^*.
	\end{equation}
	Notez que dans la dernière expression, les trois \( {}^*\) ont trois significations différentes. Par conséquent,
	\begin{equation}
		\det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_i^*(A^*e_{\sigma(i)})^*.
	\end{equation}
	Mais \( e_i^*(A^*e_{\sigma(i)})=e_{\sigma(j)}^*(A^*e_{j})\) pour \( j=\sigma(i)\), donc le produit ne change pas si on déplace le \( \sigma\) :
	\begin{equation}
		\det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_{\sigma(i)}^*(A^*e_{i})^*=\det(A^*)^*.
	\end{equation}
	Nous avons donc \( \det(A)=\det(A^*)^*\), c'est-à-dire \( \det(A)^*=\det(A^*)\). Pour information, la dernière étoile est la conjugaison complexe.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooVPSYooRuoEFi}
	Si \( A\colon E_2\to E_3\) et \( B\colon E_1\to E_2\) sont des applications linéaires, alors
	\begin{equation}
		(AB)^*=B^*A^*
	\end{equation}
	où la «multiplication» est la composition.
\end{proposition}

\begin{proof}
	L'existence de \( (AB)^*\), de \( A^*\) et de \( B^*\) ne donne pas lieu à débat parce que la proposition \ref{DEFooROVNooFlTbSK} ne souffre pas de discussions. La propriété que \( (AB)^*\) est unique a avoir est que
	\begin{equation}
		\langle ABx, y\rangle =\langle x, (AB)^*y\rangle
	\end{equation}
	pour tout \( x\in E_1\) et \( y\in E_3\). Or l'application \( B^*A^*\) possède également cette propriété parce que
	\begin{equation}
		\langle x, B^*A^*y\rangle =\langle Bx, A^*y\rangle =\langle ABx, y\rangle .
	\end{equation}
	La partie unicité de la proposition \ref{DEFooROVNooFlTbSK} nous impose donc d'accepter que les applications \( (AB)^*\) et \( B^*A^*\) sont en réalité les mêmes\footnote{Et ce même si vous croyez les avoir déjà vu ensemble dans la même pièce.}.
\end{proof}

\begin{normaltext}
	Un grand moment d'utilisation de la notion d'adjoint pour un opérateur non carré sera la définition d'une intégrale sur une variété; en particulier dans la proposition \ref{PROPooOAHWooAfxvyv}.
\end{normaltext}

\begin{lemma}
	Si \( E\) est un espace euclidien, un endomorphisme \( f\colon E\to E\) est autoadjoint si et seulement si pour tout \( x,y\in E\) nous avons \( \langle x, f(y)\rangle=\langle f(x), y\rangle  \).
\end{lemma}

\begin{proof}
	Dans le sens direct, nous avons
	\begin{equation}
		\langle f(x), y\rangle =\langle x, f^*(y)\rangle =\langle x, f(y)\rangle .
	\end{equation}
	La première égalité est la définition de \( f^*\) et la seconde est l'hypothèse \( f=f^*\).

	Dans l'autre sens, l'hypothèse est que l'endomorphisme \( f\) vérifie \( \langle x, f(y)\rangle =\langle f(x), y\rangle \). Mais la proposition \ref{DEFooROVNooFlTbSK}\ref{ITEMooRUZWooSZgGnf} spécifie que \( f^*\) est l'unique endomorphisme à satisfaire cette égalité. Donc \( f=f^*\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Opérateur orthogonal, matrice orthogonale}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYKCSooURQDoS}
	Un opérateur est \defe{orthogonal}{orthogonal!opérateur} lorsque \( A^*=A^{-1}\) où \( A^*\) est l'adjoint de \( A\) définit en~\ref{DEFooROVNooFlTbSK}.
\end{definition}

\begin{definition}      \label{DEFooUHANooLVBVID}
	Une matrice \( U\) est \defe{orthogonale}{matrice!orthogonale}\index{orthogonal!matrice} si \( U^t=U^{-1}\). Le \defe{groupe orthogonal}{groupe!orthogonal} noté \( \gO(n)\) est l'ensemble des matrices orthogonales \( n\times n\).
\end{definition}

\begin{lemma}       \label{LEMooSSALooSBFzJb}
	Soit un opérateur \( A\colon \eR^n\to \eR^n\) muni du produit scalaire usuel. Il est orthogonal si et seulement si sa matrice dans la base canonique est orthogonale\footnote{Définition~\ref{DEFooUHANooLVBVID}.}.
\end{lemma}

\begin{proof}
	Soit la base canonique \( \{ e_i \}_{i=1,\ldots, n}\) de \( \eR^n\). Nous avons
	\begin{equation}
		\langle AA^*e_i, e_j\rangle =\langle e_i, e_j\rangle =\delta_{ij},
	\end{equation}
	donc \( \big( (AA^*)e_i \big)_j=\delta_{ij}\), ou encore \( (AA^*)_{ij}=\delta_{ij}\), ce qui signifie que la matrice \( AA^*\) est l'identité.
\end{proof}

\begin{proposition}[Thème~\ref{THMooVUCLooCrdbxm}]     \label{PropKBCXooOuEZcS}
	À propos de matrices orthogonales.
	\begin{enumerate}
		\item       \label{ITEMooHSTAooIbVrwa}
		      L'ensemble des matrices réelles orthogonales forme un groupe noté \( \gO(n,\eR)\)\nomenclature[B]{\( \gO(n,\eR)\)}{le groupe des matrices orthogonales}.
		\item
		      Si \( A\) est une matrice orthogonale, alors \( \det(A)=\pm 1\).
		\item       \label{ITEMooOWMBooHUatNb}
		      Le groupe \( \gO(n)\) est le groupe des isométries linéaires\footnote{Au sens où, parmi les applications linéaires, les isométries sont les éléments de \( \gO(n)\). À part ça, il y a aussi les translations, mais c'est une autre histoire qui vous sera contée une autre fois.} de \( \eR^n\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Commençons par prouver que \( \gO(n,\eR)\) est un groupe. La matrice \( \mtu\) est orthogonale. De plus si \( A\) et \( B\) sont orthogonale, la matrice produit \( AB\) est orthogonale :
	\begin{equation}
		(AB)(AB)^t=ABB^tA^t=A\mtu A^t=\mtu.
	\end{equation}
	Nous avons donc bien un groupe.

	En ce qui concerne le déterminant, \( AA^t=\mtu\) donne \( \det(A)\det(A^t)=1\), mais la proposition~\ref{PROPooSHZMooGwdfBd} dit que \( \det(A)=\det(A^t)\), donc \( \det(A)^2=1\). D'où le fait que \( \det(A)=\pm 1\).

	D'autre part si \( A\) est une isométrie de \( \eR^n\) alors pour tout \( x,y\in \eR^n\) nous avons \( \langle Ax, Ay\rangle =\langle x, y\rangle \). En particulier,
	\begin{equation}
		\langle A^tAx, y\rangle =\langle x, y\rangle
	\end{equation}
	pour tout \( x,y\in \eR^n\). En prenant \( y=e_i\) nous trouvons
	\begin{equation}
		(A^tAx)_i=x_i,
	\end{equation}
	ce qui signifie que pour tout \( x\), \( A^tAx=x\), ou encore que \( A^tA\) est l'identité.

	Réciproquement si \( A^tA\) est l'identité nous avons
	\begin{equation}
		\langle x, y\rangle =\langle A^tAx, y\rangle =\langle Ax, Ay\rangle ,
	\end{equation}
	ce qui prouve que \( A\) est une isométrie.
\end{proof}

En ce qui concerne les valeurs propres des matrices de \( \gO(n)\) ainsi que leurs formes canoniques (avec des fonctions trigonométriques) pour \( \gO(3)\) et \( \SO(3)\), ce sera pour la proposition~\ref{PROPooVEJGooWnqtMm} et ce qui s'ensuit.

\begin{definition}      \label{DEFooJLNQooBKTYUy}
	Le sous-groupe des matrices orthogonales de déterminant \( 1\) est le groupe \defe{spécial orthogonal}{groupe!spécial orthogonal} noté \( \SO(n)\).
\end{definition}
