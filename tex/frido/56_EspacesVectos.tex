% This is part of Mes notes de mathématique
% Copyright (c) 2008-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit scalaire, produit hermitien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Définie positive, thème~\ref{THEMEooYEVLooWotqMY}]      \label{DEFooJIAQooZkBtTy}
    Si $g$ est une application bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} sur un espace vectoriel \( E\) nous disons qu'elle est
    \begin{enumerate}
        \item
            \defe{définie positive}{application!définie positive} si $g(x,x)\geq 0$ pour tout $x\in E$ et $g(x,x)=0$ si et seulement si $x=0$.
        \item
            \defe{semi-définie positive}{application!semi-définie positive} si $g(x,x)\geq 0$ pour tout $x\in E$. Nous dirons aussi parfois qu'elle est simplement «positive».
        \end{enumerate}
\end{definition}
Cela est évidemment à lier à la définition~\ref{DefAWAooCMPuVM} et la proposition~\ref{PROPooUAAFooEGVDRC} : une application bilinéaires est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{definition}\label{DefVJIeTFj}
    Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel réel est une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition~\ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

La définition suivante est utile pour celles qui veulent faire de la relativité\footnote{Voir le théorème \ref{THOooYHDWooWxVovH} qui établit les transformations de Lorentz.}.
\begin{definition}      \label{DEFooLPBGooXLxubc}
    Un \defe{produit pseudo-scalaire}{produit pseudo-scalaire} sur un espace vectoriel réel est une forme bilinéaire et symétrique.
\end{definition}

Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj}
    Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel de dimension finie muni d'un produit scalaire (définition~\ref{DefVJIeTFj}).
\end{definition}
Avouez que c'est drôle qu'un espace vectoriel est euclidien lorsqu'il possède une \emph{multiplication} alors qu'un anneau est euclidien lorsqu'il possède une \emph{division} (voir la définition~\ref{DefAXitWRL}). C'est pas très profond, mais si ça peut vous servir de moyen mnémotechnique\ldots

\begin{definition}[\cite{ooJUXBooVrwvfP}]  \label{DefMZQxmQ}
    Soit \( E\) est un espace vectoriel sur \( \eC\). Une application \( \langle ., .\rangle \colon E\times E\to \eC\) est \defe{sesquilinéaire à droite}{sesquilinéaire} si pour tout \( x,y\in E\) et pour tout \( \lambda\in \eC\),
    \begin{enumerate}
        \item
            \( \langle \lambda x, y\rangle =\lambda\langle x,y, \rangle =\langle x, \bar\lambda y\rangle \),
        \item
            \( \langle x+y, z\rangle =\langle x, y\rangle+\langle y, z\rangle  \),
        \item
            \( \langle x, y+z\rangle =\langle x, y\rangle +\langle x, z\rangle \).
    \end{enumerate}
    Cette forme est \defe{hermitienne}{hermitienne} si de plus
    \begin{equation}
        \langle x, y\rangle =\overline{ \langle y, x\rangle  }.
    \end{equation}
    Un \defe{produit scalaire hermitien}{hermitien!produit scalaire} est une forme hermitienne strictement définie positive, c'est-à-dire telle que \( \langle x, x\rangle \geq 0\) pour tout \( x\in E\) et \( \langle x, x\rangle =0\) si et seulement si \( x=0\).
\end{definition}

\begin{example}
    L'ensemble \( E=\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\)  est muni d'une forme sesquilinéaire
    \begin{equation}    \label{EqFormSesqQrjyPH}
        \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
    \end{equation}
    pour tout \( x,y\in\eC^n\). Cela est un espace vectoriel hermitien.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Expressions matricielles}
%---------------------------------------------------------------------------------------------------------------------------

Une forme bilinéaire symétrique sur \( \eR^n\) s'é

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme, produit scalaire et Cauchy-Schwarz}
%---------------------------------------------------------------------------------------------------------------------------

Dans la suite, le produit scalaire de \( x\) et \( y\) pourra être noté indifféremment par \( x\cdot y\), \( \langle x, y\rangle \) ou \( b(x,y)\) lorsque une forme bilinéaire est donnée.

Nous commençons par prouver qu'un produit scalaire étant donné, nous pouvons définir une norme par la formule \( \| x \|^2=\langle x, x\rangle \). Pour cela nous aurons besoin de l'inégalité de Cauchy-Schwarz.

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas réel]      \label{ThoAYfEHG}
    Soit un espace vectoriel muni d'un produit scalaire \( (x,y)\mapsto x\cdot y\). En posant\footnote{Attention à la notation : pour l'instant nous ne savons pas que c'est une norme. Ce sera justifié dans la proposition~\ref{PropEQRooQXazLz}.}
    \begin{equation}
        \| x \|=\sqrt{ x\cdot x },
    \end{equation}
    nous avons
    \begin{equation}        \label{EQooZDSHooWPcryG}
		| x\cdot y |\leq \| x \|\| y \|.
	\end{equation}
    Nous avons une égalité si et seulement si \( x\) et \( y\) sont multiples l'un de l'autre.
\end{theorem}
\index{Cauchy-Schwarz}
\index{inégalité!Cauchy-Schwarz}

\begin{proof}
	Étant donné que les deux membres de l'inéquation sont positifs, nous allons travailler en passant au carré afin d'éviter les racines carrés dans le second membre.

	Nous considérons le polynôme
	\begin{equation}
		P(t)=\| x+ty \|^2=(x+ty)\cdot(x+ty)=x\cdot x+x\cdot ty+ty\cdot x+t^2y\cdot y.
	\end{equation}
    En utilisant la bilinéarité (pour sortir les \( t\)) et la symétrique du produit scalaire, puis en ordonnant les termes selon les puissances de $t$,
	\begin{equation}
		P(t)=\| y \|^2t^2+2(x\cdot y)t+\| x \|^2.
	\end{equation}
    %TODOooRUEZooGCVyQZ : faire la résolution.
	Cela est un polynôme du second degré en $t$ dont le signe est toujours positif (ou nul). Par conséquent le discriminant\footnote{Le fameux $b^2-4ac$.} doit être négatif ou nul. Nous avons donc
	\begin{equation}
		\Delta=4(x\cdot y)^2-4\| x \|^2\| y \|^2\leq 0,
	\end{equation}
	ce qui donne immédiatement
	\begin{equation}
		(x\cdot y)^2\leq\| x \|^2\| y \|^2.
	\end{equation}

    En ce qui concerne le cas d'égalité, si nous avons \( x\cdot y=\| x \|\| y \|\), alors le discriminant \( \Delta\) ci-dessus est nul et le polynôme \( P\) admet une racine double \( t_0\). Pour cette valeur nous avons
    \begin{equation}
        P(t_0)=| x+t_0y |=0,
    \end{equation}
    ce qui implique \( x+t_0y=0\) et donc que \( x\) et \( y\) sont liés.
\end{proof}

\begin{proposition} \label{PropEQRooQXazLz}
    Si \( x,y\mapsto x\cdot y\) est un produit scalaire sur un espace vectoriel \( E\), alors \( \| x \|=\sqrt{x\cdot x}\) est une norme\footnote{Définition~\ref{DefNorme}.} vérifiant l'identité du parallélogramme :
    \begin{equation}        \label{EqYCLtWfJ}
        \| x-y \|^2+\| x+y \|^2=2\| x \|^2+2\| y \|^2.
    \end{equation}
\end{proposition}

\begin{proof}
    Prouvons l'inégalité triangulaire\index{inégalité!triangulaire!produit scalaire}. Si \( x,y\in E\) nous avons
    \begin{equation}
        \| x+y \|=\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}.
    \end{equation}
    Par l'inégalité de Cauchy-Schwarz, théorème~\ref{ThoAYfEHG} nous avons aussi
    \begin{equation}
        \| x \|^2+\| y \|^2+2x\cdot y\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|=\big( \| x \|+\| y \| \big)^2,
    \end{equation}
    donc
    \begin{equation}
        \| x+y \|\leq \sqrt{\big( \| x \|+\| y \| \big)^2}=\| x \|+\| y \|.
    \end{equation}

    La seconde assertion est seulement un calcul :
			\begin{equation}
				\begin{aligned}[]
					\| x-y \|^2+\| x+y \|^2&=(x-y)\cdot (x-y)+(x+y)\cdot(x+y)\\
					&=x\cdot x-x\cdot y-y\cdot x+y\cdot y\\
					&\quad +x\cdot x+x\cdot y+y\cdot x+y\cdot y\\
					&=2x\cdot x+2y\cdot y\\
					&=2\| x \|^2+2\| y \|^2.
				\end{aligned}
			\end{equation}
\end{proof}

\begin{normaltext}
    Un produit scalaire fourni donc toujours une norme et donc une topologie. Il ne faudrait cependant pas croire que toute norme dérive d'un produit scalaire, même pas en dimension finie. Et ce, malgré l'équivalence de toutes les normes du théorème~\ref{ThoNormesEquiv} dont vous avez déjà peut-être entendu parler.
\end{normaltext}

\begin{example}     \label{EXooCAPYooMgOSyH}
    Sur \( \eR^2\), l'application \( N(x,y)=| x |+| y |\) est une norme\footnote{Proposition~\ref{PROPooCLZRooIRxCnZ}.}. Nous allons voir qu'elle ne dérive pas d'un produit scalaire en montrant qu'elle ne vérifie pas l'identité du parallélogramme.

    Voici un petit bout de code qui nous permet de ne pas faire de recherches à la main :
    \lstinputlisting{tex/sage/sageSnip018.sage}

    Il est vite vu qu'avec \( v=(-1,1)\) et \( w=(1,1)\), l'identité du parallélogramme n'est pas vérifiée.
\end{example}

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas complexe\cite{HilbertLi}]
     Soit un espace vectoriel complexe muni d'un produit scalaire \( \langle ., .\rangle \). Alors pour tout vecteurs \( x,y\) nous avons
     \begin{equation}
         | \langle x, y\rangle  |\leq \| x \|\| y \|.
     \end{equation}
\end{theorem}

\begin{proof}
    Si \( \langle x, y\rangle =0\), le résultat est évident; nous supposons que non. Nous posons
    \begin{equation}
        \theta=\frac{ \langle x, y\rangle  }{ | \langle x, y\rangle  | }.
    \end{equation}
    C'est un élément de \( \eC\) de norme \( 1\). Nous avons
    \begin{equation}
        \langle \frac{1}{ \theta }x, y\rangle =\frac{ | \langle x, y\rangle  | }{ \langle x, y\rangle  }\langle x, y\rangle =| \langle x, y\rangle  |\geq 0
    \end{equation}
    où le symbole «\( \geq\)» signifie «est réel et positif». Nous posons \( x'=\frac{1}{ \theta }x\) et nous considérons \( t\in \eR\). Remarquons que \( \| x' \|^2=\| x \|^2\) :
    \begin{equation}
        \| x' \|^2=\langle x', x'\rangle =\frac{1}{ \theta\bar\theta }\langle x, x\rangle =\| x \|^2
    \end{equation}
    parce que \( | \theta |=1\).

    En utilisant le fait que \( \langle a, b\rangle +\langle b, a\rangle =\real(\langle a, b\rangle )\) nous avons :
    \begin{subequations}
        \begin{align}
            0\leq \| x'+ty \|^2&=\| x' \|^2+t\langle x', y\rangle +t\langle y, x'\rangle +t^2\| y \|^2\\
            &=\| y \|^2t^2+2\real(\langle x', y\rangle )t+\| x' \|^2.
        \end{align}
    \end{subequations}
    Cela est un polynôme de degré \( 2\) en \( t\) qui n'est jamais strictement négatif. Autrement dit, il a au maximum une seule racine, ce qui signifie que son discriminant est négatif ou nul :
    \begin{equation}
        \real(\langle x', y\rangle )^2-\| y \|^2\| x' \|^2\leq 0.
    \end{equation}
    Mais nous avons choisi \( x'\) de telle sorte que \( \langle x', y\rangle =| \langle x, y\rangle  |\in \eR\) et \( \| x' \|^2=\| x \|^2\); nous avons donc
    \begin{equation}
        | \langle x, y\rangle  |^2\leq \| x \|^2\| y \|^2,
    \end{equation}
    comme il se devait.
\end{proof}


\begin{lemma}[\cite{KXjFWKA}]   \label{LemLPOHUme}
    Soit \( V\) un espace vectoriel muni d'un produit scalaire et de la norme associée. Si \( x,y\in V\) satisfont à \( \| x+y \|=\| x \|+\| y \|\), alors il existe \( \lambda\geq 0\) tel que \( x=\lambda y\).
\end{lemma}

\begin{proof}
    Quitte à raisonner avec \( x/\| x \|\) et \( y/\| y \|\), nous supposons que \( \| x \|=\| y \|=1\). Dans ce cas l'hypothèse signifie que \( \| x+y \|^2=4\). D'autre part en écrivant la norme en terme de produit scalaire,
    \begin{equation}
        \| x+y \|^2=\| x \|^2+\| y \|^2+2\langle x, y\rangle ,
    \end{equation}
    ce qui nous mène à affirmer que \( \langle x, y\rangle =1=\| x \|\| y \|\). Nous sommes donc dans le cas d'égalité de l'inégalité de Cauchy-Schwarz\footnote{Théorème~\ref{ThoAYfEHG}.}, ce qui nous donne un \( \lambda\) tel que \( x=\lambda y\). Étant donné que \( \| x \|=\| y \|=1\) nous avons obligatoirement \( \lambda=\pm 1\), mais si \( \lambda=-1\) alors \( \langle x, y\rangle =-1\), ce qui est le contraire de ce qu'on a prétendu plus haut. Par soucis de cohérence, nous allons donc croire que \( \lambda=1\).
\end{proof}

\begin{proposition}			\label{PropVectsOrthLibres}
	si $v_1,\cdots,v_k$ sont des vecteurs non nuls, orthogonaux deux à deux, alors ces vecteurs forment une famille libre.
\end{proposition}

\begin{definition}[Isométrie, thème~\ref{THMooVUCLooCrdbxm}]      \label{DEFooGGTYooXsHIZj}
    Une \defe{isométrie}{isométrie!forme bilinéaire} d'une forme bilinéaire \( b\) sur l'espace vectoriel \( E\) est une application bijective \( f\colon E\to E\) telle que \( b\big( f(u),f(v) \big)=b(u,v)\) pour tout \( u,v\in E\).
\end{definition}
En particulier une isométrie d'un espace euclidien est une application bijective qui préserve le produit scalaire. La proposition \ref{PROPooSYQMooEnZFdp} a déjà fixé une condition pour qu'une application linéaire soit une isométrie. En gros, en termes de matrices, c'est 
\begin{equation}
a^t\eta A=\eta
\end{equation}
pour que la matrice \( A\) soit une isométrie de la forme bilinéaire \( \eta\).

\begin{lemma}       \label{LEMooYXJZooWKRFRu}
    Une isométrie d'un espace euclidien fixe l'origine.
\end{lemma}

\begin{proof}
    Soit une isométrie \( f\) d'un espace euclidien : \( f(x)\cdot f(y)=x\cdot y\) pour tout \( x,y\in E\). En particulier pour \( x=0\) nous avons
    \begin{equation}
        f(0)\cdot f(y)=0
    \end{equation}
    pour tout \( y\). Vu que \( f\) est une bijection, nous avons \( f(0)\cdot x=0\) pour tout \( x\). Comme le produit scalaire est non dégénéré cela implique que \( f(0)=0\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et angles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Propriétés du produit scalaire]
	Si $X$ et $Y$ sont des vecteurs de $\eR^3$, alors
	\begin{description}
		\item[Symétrie] $X\cdot Y=Y\cdot X$;
		\item[Linéarité] $(\lambda X+\mu X')\cdot Y=\lambda(X\cdot Y)+\mu(X'\cdot Y)$ pour tout $\lambda$ et $\mu$ dans $\eR$;
		\item[Défini positif] $X\cdot X\geq 0$ et $X\cdot X=0$ si et seulement si $X=0$.
	\end{description}
\end{proposition}
Note : lorsque nous écrivons $X=0$, nous voulons voulons dire $X=\begin{pmatrix}
	0	\\
	0	\\
	0
\end{pmatrix}$.


\begin{definition}
	La \defe{norme}{norme!vecteur} du vecteur $X$, notée $\| X \|$, est définie par
	\begin{equation}
		\| X \|=\sqrt{X\cdot X}=\sqrt{x^2+y^2+z^2}
	\end{equation}
	si $X=(x,y,z)$. Cette norme sera parfois nommée «norme euclidienne».
\end{definition}
Cette définition est motivée par le théorème de Pythagore. Le nombre $X\cdot X$ est bien la longueur de la «flèche» $X$. Plus intrigante est la définition suivante :
\begin{definition}
	Deux vecteurs $X$ et $Y$ sont \defe{orthogonaux}{orthogonal!vecteur} si $X\cdot Y=0$.
\end{definition}
Cette définition de l'orthogonalité est motivée par la proposition suivante.

\begin{proposition}		\label{PropProjScal}
	Si nous écrivons $\pr_Y$  l'opération de projection sur la droite qui sous-tend $Y$, alors nous avons
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	Les vecteurs $X$ et $Y$ sont des flèches dans l'espace. Nous pouvons choisir un système d'axe orthogonal tel que les coordonnées de $X$ et $Y$ soient
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x	\\
				y	\\
				0
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				l	\\
				0	\\
				0
			\end{pmatrix}
		\end{aligned}
	\end{equation}
	où $l$ est la longueur du vecteur $Y$. Pour ce faire, il suffit de mettre le premier axe le long de $Y$, le second dans le plan qui contient $X$ et $Y$, et enfin le troisième axe dans le plan perpendiculaire aux deux premiers.

	Un simple calcul montre que $X\cdot Y=xl+y\cdot 0+0\cdot 0=xl$. Par ailleurs, nous avons $\| \pr_YX \|=x$. Par conséquent,
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ l }=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proof}

\begin{corollary}
	Si la norme de $Y$ est $1$, alors le nombre $X\cdot Y$ est la longueur de la projection de $X$ sur $Y$.
\end{corollary}

\begin{proof}
	Poser $\| Y \|=1$ dans la proposition~\ref{PropProjScal}.
\end{proof}

\begin{remark}
    Outre l'orthogonalité, le produit scalaire permet de savoir l'angle entre deux vecteurs à travers la définition~\ref{DEFooSVDZooPWHwFQ}. D'autres interprétations géométriques du déterminant sont listées dans le thème~\ref{THMooUXJMooOroxbI}.
\end{remark}

Nous sommes maintenant en mesure de déterminer, pour deux vecteurs quelconques $u$ et $v$, la projection orthogonale de $u$ sur $v$. Ce sera le vecteur $\bar u$ parallèle à $v$ tel que $u-\bar u$ est orthogonal à $v$. Nous avons donc
\begin{equation}
    \bar u=\lambda v
\end{equation}
et
\begin{equation}
    (u-\lambda v)\cdot v=0.
\end{equation}
La seconde équation donne $u\cdot v-\lambda v\cdot v=0$, ce qui fournit $\lambda$ en fonction de $u$ et $v$ :
\begin{equation}
    \lambda=\frac{ u\cdot v }{ \| v \|^2 }.
\end{equation}
Nous avons par conséquent
\begin{equation}
    \bar u=\frac{ u\cdot v }{ \| v \|^2 }v.
\end{equation}
Armés de cette interprétation graphique du produit scalaire, nous comprenons pourquoi nous disons que deux vecteurs sont orthogonaux lorsque leur produit scalaire est nul.

Nous pouvons maintenant savoir quel est le coefficient directeur d'une droite orthogonale à une droite donnée. En effet, supposons que la première droite soit parallèle au vecteur $X$ et la seconde au vecteur $Y$. Les droites seront perpendiculaires si $X\cdot Y=0$, c'est-à-dire si
\begin{equation}
	\begin{pmatrix}
		x_1	\\
		y_1
	\end{pmatrix}\cdot\begin{pmatrix}
		y_1	\\
		y_2
	\end{pmatrix}=0.
\end{equation}
Cette équation se développe en
\begin{equation}		\label{Eqxuyukljsca}
	x_1y_1=-x_2y_2.
\end{equation}
Le coefficient directeur de la première droite est $\frac{ x_2 }{ x_1 }$. Isolons cette quantité dans l'équation \eqref{Eqxuyukljsca} :
\begin{equation}
	\frac{ x_2 }{ x_1 }=-\frac{ y_1 }{ y_2 }.
\end{equation}
Donc le coefficient directeur de la première est l'inverse et l'opposé du coefficient directeur de la seconde.

\begin{example}
	Soit la droite $d\equiv y=2x+3$. Le coefficient directeur de cette droite est $2$. Donc le coefficient directeur d'une droite perpendiculaires doit être $-\frac{ 1 }{ 2 }$.
\end{example}

\begin{proof}[Preuve alternative]
	La preuve peut également être donnée en ne faisant pas référence au produit scalaire. Il suffit d'écrire toutes les quantités en termes des coordonnées de $X$ et $Y$. Si nous posons
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x_1	\\
				x_2	\\
				x_2
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				y_1	\\
				y_2	\\
				y_3
			\end{pmatrix},
		\end{aligned}
	\end{equation}
	l'inégalité à prouver devient
	\begin{equation}
		(x_1y_1+x_2y_2+x_3y_3)^2\leq (x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2).
	\end{equation}
	Nous considérons la fonction
	\begin{equation}
		\varphi(t)=(x_1+ty_1)^2+(x_2+ty_2)^2+(x_3+ty_3)^2
	\end{equation}
	En tant que norme, cette fonction est évidemment positive pour tout $t$. En regroupant les termes de chaque puissance de $t$, nous avons
	\begin{equation}
		\varphi(t)=(y_1^2+y_2^2+y_3^2)t^2+2(x_1y_1+x_2y_2+x_3y_3)t+(x_1^2+x_2^2+x_3^2).
	\end{equation}
	Cela est un polynôme du second degré en $t$. Par conséquent le discriminant doit être négatif. Nous avons donc
	\begin{equation}
		4(x_1y_1+x_2y_2+x_3y_3)^2-(x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2)\leq 0.
	\end{equation}
	La thèse en découle aussitôt.
\end{proof}

\begin{proposition}     \label{PROPooVSVMooZrqxdc}
	La norme euclidienne a les propriétés suivantes :
	\begin{enumerate}
		\item
			Pour tout vecteur $X$ et réel $\lambda$,  $\| \lambda X \|=| \lambda |\| X \|$. Attention à ne pas oublier la valeur absolue !
		\item
			Pour tout vecteurs $X$ et $Y$, $\| X+Y \|\leq \| X \|+\| Y \|$.
	\end{enumerate}
\end{proposition}

\begin{proof}
    Pour le second point, nous avons les inégalités suivantes :
	\begin{subequations}
		\begin{align}
			\| X+Y \|^2&=\| X \|^2+\| Y \|^2+2X\cdot Y\\
			&\leq\| X \|^2+\| Y \|^2+2|X\cdot Y|\\
			&\leq\| X \|^2+\| Y \|^2+2\| X \|\| Y \|\\
			&=\big( \| X \|+\| Y \| \big)^2
		\end{align}
	\end{subequations}
    Nous avons utilisé d'abord la majoration $| x |\geq x$ qui est évidente pour tout nombre $x$; et ensuite l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Pythagore}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons donner une preuve du théorème de Pythagore.

\begin{theorem}[Pythagone\cite{MonCerveau}]     \label{THOooHXHWooCpcDan}
    Soient \( A,B,S\in \eR^2\) un triangle rectangle en \( A\), c'est à dire tel que
    \begin{equation}        \label{EQooRAWAooBxlBcZ}
        (B-A)\cdot (A-S)=0.
    \end{equation}
    Alors
    \begin{equation}
        \| S-B \|^2=\| S-A \|^2+\| B-A \|^2.
    \end{equation}
\end{theorem}

\begin{proof}
    En développant l'hypothèse \eqref{EQooRAWAooBxlBcZ} nous avons :
    \begin{equation}    \label{EQooYTDGooXzYQwi}
        B\cdot A-B\cdot S-\| A \|^2+A\cdot S=0.
    \end{equation}
    Et de même,
    \begin{equation}
        \| S-B \|^2=(S-B)\cdot(S-B)=\| S \|^2-2B\cdot S+\| B \|^2.
    \end{equation}
    En substituant dans cette dernière \( B\cdot S\) par \( B\cdot S=B\cdot A -\| A \|^2+A\cdot S \) tirée de \eqref{EQooYTDGooXzYQwi}, nous trouvons
    \begin{equation}
        \| S-B \|^2=\| S \|^2-2B\cdot A+2\| A \|^2-2A\cdot S+\| B \|^2=\| S-A \|^2+\| B-A \|^2.
    \end{equation}
\end{proof}

Je profite de l'occasion pour montrer mon scepticisme quant aux preuves de Pythagore basées sur différents pliages et découpages des carrés construits sur les côtés du triangle. Pour autant que je le sache, la géométrie dans «le plan» (c'est à dire pas dans \( \eR^2\) muni de son produit scalaire) ne définit pas «longueur» et «aire». Donc bon \ldots Il y a peut-être moyen de s'en sortir, mais je ne le connais pas.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooTNTNooRjhuJZ}
	Soient $u$ et $v$, deux vecteurs de $\eR^3$. Le \defe{produit vectoriel}{produit!vectoriel} de $u$ et $v$ est le vecteur $u\times v$ défini par
    \begin{equation}        \label{EQooCUJRooFuFPaZ}
		u\times v=\det\begin{pmatrix}
			e_1	&	e_2	&	e_3	\\
			u_1	&	u_2	&	u_3	\\
			v_1	&	v_2	&	v_3
		\end{pmatrix}
    \end{equation}
	où les vecteurs $e_1$, $e_2$ et $e_3$ sont les vecteurs de la base canonique de $\eR^3$.
\end{definition}

\begin{lemma}
    Le produit vectoriel \( u\times v\) est également exprimé par
    \begin{subequations}
        \begin{align}
            u\times v&=(u_2v_3-u_3v_2)e_1+(u_3v_1-u_1v_3)e_2+(u_1v_2-u_2v_1)e_3     \label{SEBEQooVROKooRpUOIr}\\
                &=\sum_{i,j,k}\epsilon_{ijk}v_iw_je_k
        \end{align}
    \end{subequations}
    où $\epsilon_{ijk}$ est défini par $\epsilon_{xyz}=1$ et ensuite $\epsilon_{ijk}$ est $1$ ou $-1$ suivant que la permutation des $x$, $y$ et $z$ est paire ou impaire. C'est-à-dire que \( \epsilon_{ijk}\) est la signature de la permutation qui amène \( (1,2,3)\) sur \( (i,j,k)\).
\end{lemma}

\begin{proof}
    Il s'agit seulement de développer explicitement le déterminant \eqref{EQooCUJRooFuFPaZ}.
\end{proof}

Une des principales utilités du produit vectoriel est donnée dans la proposition suivante.
\begin{proposition}     \label{PROPooIQMTooFHNjfu}
    Si \( u\) et \( v\) sont des vecteurs de \( \eR^3\) alors le vecteur \( u\times v\) est perpendiculaire à \( u\) et à \( v\).
\end{proposition}
La chose importante à retenir est que le produit vectoriel permet de construire un vecteur simultanément perpendiculaire à deux vecteurs donnés. Le vecteur $u\times v$ est donc linéairement indépendant de $u$ et $v$. En pratique, si $u$ et $v$ sont déjà linéairement indépendants, alors le produit vectoriel permet de compléter une base de $\eR^3$.

\begin{lemmaDef}
    Nous avons l'égalité suivante pour tout \( u,v,w\in \eR^3\) :
    \begin{equation}        \label{EQooKJYUooSQgfXU}
        (u\times v)\cdot w=\det\begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix}.
    \end{equation}
    Le résultat est nommé le \defe{produit mixte}{produit!mixte} de trois vecteurs de \( \eR^3\).
\end{lemmaDef}

\begin{normaltext}
    Nous avons donné un nom à la combinaison \( (u\times v)\cdot w\). J'imagine que vous voyez pourquoi nous ne considérons pas la combinaison $(u\cdot v)\times w$.
\end{normaltext}

Le lemme suivant donne un moyen compliqué et peu pratique de calculer la valeur absolue du produit mixte. La formule \eqref{EQooWZUQooYydphW} ne sera utilisée que pour faire le lien entre un jacobien et un élément de volume en dimension trois lorsque nous verrons les intégrales sur des variétés. Voir l'équation \eqref{EQooYIJSooHtkXfu}. 

% TODO lier à la vraie définition de l'intégrale sur une carte quand elle sera faite.
% TODOooWZMDooEJhpNS
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooSMWNooCmEZeY}
    Le produit mixte peut également être exprimé par
    \begin{equation}        \label{EQooWZUQooYydphW}
           |(u\times v)\cdot w|^2=\det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
    Si nous notons 
    \begin{equation}
        a= \begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix},
    \end{equation}
    il faut simplement remarquer que
    \begin{equation}
           \begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=aa^t.
    \end{equation}
    Donc au niveau des déterminants, en utilisant les propositions \ref{PROPooHQNPooIfPEDH} et le lemme \ref{LEMooCEQYooYAbctZ} nous avons
    \begin{equation}
           \det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=\det(aa^t)=\det(a)\det(a^t)=\det(a)^2.
    \end{equation}
    Et maintenant, par définition, \( \det(a)=(u\times w)\cdot w\). Donc le résultat annoncé.
\end{proof}

\begin{proposition}		 \label{PropScalMixtLin}
	Les applications produit scalaire, vectoriel et mixte sont multilinéaires. Spécifiquement, nous avons les propriétés suivantes.
	\begin{enumerate}
		\item
			Les applications produit scalaire et vectoriel sont bilinéaires. C'est-à-dire que pour tout vecteurs $a$, $b$, $c$ et pour tout nombre $\alpha$ et $\beta$ nous avons
    \begin{equation}
        \begin{aligned}[]
            a\times (\alpha b +\beta c)&=\alpha(a\times b)+\beta(a\times c)\\
            (\alpha a+\beta b)\times c&=\alpha(a\times c)+\beta(b\times c).
        \end{aligned}
    \end{equation}

        \item
            Le produit mixte est trilinéaire.
		\item
			Le produit vectoriel est antisymétrique, c'est-à-dire $u\times v=-v\times u$.
		\item
			Nous avons $u\times v=0$ si et seulement si $u$ et $v$ sont colinéaires, c'est-à-dire si et seulement si l'équation $\alpha u+\beta v=0$ a une solution différente de la solution triviale $(\alpha,\beta)=(0,0)$.
		\end{enumerate}
\end{proposition}

\begin{proposition}[Identité de Lagrange\cite{ooHFUZooGakvHi}]     \label{PROPooMXAIooJureOD}
    Si \( x,y\in \eR^n\), alors
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
    \end{equation}
    Et si \( n=3\) alors
    \begin{equation}
        \| x\times y \|=\| y \|^2\| y \|^2-(x\cdot y)^2.
    \end{equation}
\end{proposition}

\begin{proof}
    C'est un calcul. D'abord nous avons
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_ix_i^2\sum_jy_j^2-\big( \sum_k x_ky_k  \big)^2=\sum_{ij}x_i^2y_j^2-\sum_{kl}x_ky_kx_ly_l.
    \end{equation}
    Ensuite nous coupons les sommes de la façon suivante
    \begin{equation}
        \sum_{ij}=\sum_j\sum_{i<j}+\sum_j(i=j)+\sum_j\sum_{i>j}
    \end{equation}
    pour obtenir
    \begin{equation}
        \begin{aligned}[]
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\sum_{i<j}x_i^2y_j^2+\sum_jx_j^2y_j^2+\sum_j\sum_{i>j}x_i^2y_j^2\\
                &\quad-\sum_l\sum_{k<l}x_ky_kx_ly_l-\sum_kx_k^2y_k^2-\sum_l\sum_{k>l}x_ky_kx_ly_l.
        \end{aligned}
    \end{equation}
    Il y a deux termes qui se simplifient. Notez que si \( A_{kl}\) est symétrique en \( kl\) nous avons
    \begin{equation}
        \sum_l\sum_{k<l}A_{kl}=\sum_k\sum_{l<k}A_{lk}=\sum_k\sum_{l<k}A_{kl}.
    \end{equation}
    La première égalité était seulement un renommage des indices. Le coup des indices symétriques est justement ce qu'il se passe dans les deux termes en\( x_ky_kx_ly_l\), donc nous les regroupons :
    \begin{subequations}
        \begin{align}
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\big( \sum_{i<j}x_i^2x_j^2+\sum_{i>j}x_i^2y_j^2-2\sum_{i>j}x_iy_ix_jy_j \big)\\
            &=\sum_j\sum_{i<j}(x_i^2y_j^2+x_j^2y_i^2-2x_iy_ix_jy_j)\\
            &=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
        \end{align}
    \end{subequations}
    Voila qui prouve la première formule. Pour la seconde, il faut seulement poser \( n=3\) et écrire les sommes explicitement.

    \begin{itemize}
        \item 
    Pour \( j=1\), la somme sur \( i\) est \( \sum_{i<1}\), c'est-à-dire aucun termes.
\item
    Pour \( j=2\), il y a seulement \( i=1\), donc le terme \( (x_1y_2-x_2y_1)^2\).

\item
    Pour \( j=3\), il y a les termes \( i=1\) et \( i=2\), donc les termes \( (x_1y_3-x_3y_1)^2+(x_2y_3-x_3y_2)^2\).
    \end{itemize}
    Ces trois termes collectés sont justement les composants (au carré) de \( x\times y\) données dans la formule \eqref{SEBEQooVROKooRpUOIr}.
\end{proof}

Les trois vecteurs de base $e_x$, $e_y$ et $e_y$ ont des produits vectoriels faciles à retenir :
\begin{equation}
    \begin{aligned}[]
        e_x\times e_y&=e_z\\
        e_y\times e_z&=e_x\\
        e_z\times e_x&=e_y
    \end{aligned}
\end{equation}

Les deux formules suivantes, qui mêlent le produit scalaire et le produit vectoriel, sont souvent utiles en analyse vectorielle :
\begin{equation}
	\begin{aligned}[]
		(u\times v)\cdot w&=u\cdot(v\times w)\\
		(u\times v)\times w&=-(v\cdot w)u+(u\cdot w)v		\label{EqFormExpluxxx}
	\end{aligned}
\end{equation}
pour tout vecteurs $u$, $v$ et $w$ dans $\eR^3$. Nous les admettons sans démonstration. La seconde formule est parfois appelée \defe{formule d'expulsion}{formule!d'expulsion (produit vectoriel)}.

\begin{example}
    Calculons le produit vectoriel $v\times w$ avec
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                3    \\
                -1    \\
                1
            \end{pmatrix}&w=\begin{pmatrix}
                1    \\
                2    \\
                -1
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Les vecteurs s'écrivent sous la forme $v=3e_x-e_y+e_z$ et $w=e_x+2e_y-e_z$. Le produit vectoriel s'écrit
    \begin{equation}
        \begin{aligned}[]
            (3e_x-e_y+e_z)\times (e_x+2e_y-e_z)&=6e_x\times e_y-3e_x\times e_z\\
                                &\quad -e_y\times e_x + e_y\times e_z\\
                                &\quad + e_z\times e_x + 2e_z\times e_y\\
                                &=6e_z+3e_y+e_z+e_x+e_y-2e_x\\
                                &=-e_x+4e_y+7e_z.
        \end{aligned}
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si $a$, $b$ et $c$ sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre $a\cdot(b\times c)$. En écrivant le produit vectoriel sous forme de somme de trois déterminants $2\times 2$, nous avons
\begin{equation}
    \begin{aligned}[]
        a\cdot& (b\times c)\\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
        \begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}e_x-\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}e_y+\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\right)\\
        &=a_1\begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}-a_2\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}+a_3\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\\
        &=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3    \\
            c_1    &   c_2    &   c_3
        \end{vmatrix}.
    \end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
    a\cdot (b\times c)=\begin{vmatrix}
        a_1    &   a_2    &   a_3    \\
        b_1    &   b_2    &   b_3    \\
        c_1    &   c_2    &   c_3
    \end{vmatrix}.
\end{equation}

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}     \label{PROPooTUVKooOQXKKl}
    Le produit vectoriel $a\times b$ est un vecteur orthogonal à $a$ et $b$.
\end{proposition}

\begin{proof}
    Vérifions que $a\perp (a\times b)$. Pour cela, nous calculons $a\cdot (a\times b)$, c'est-à-dire le produit mixte
    \begin{equation}
        a\cdot(a\times b)=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3
        \end{vmatrix}=0.
    \end{equation}
    L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

Ces résultats admettent une intéressante généralisation.
\begin{lemma}       \label{LEMooFRWKooVloCSM}
    Soit \( X\in \eR^n\) ainsi que \( v_1,\ldots, v_{n-1}\in \eR^n\). Alors
    \begin{enumerate}
        \item
            Nous avons
            \begin{equation}        \label{EQooMQNPooRHHBjz}
                \det(X,v_1,\ldots, v_{n-1})=X\cdot
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
        \item
            Le vecteur
            \begin{equation}
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
            est orthogonal à tous les \( v_i\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que les deux côtés de \eqref{EQooMQNPooRHHBjz} vus comme fonctions de \( X\), sont des applications linéaires de \( \eR^n\) dans \( \eR\), il suffit de vérifier l'égalité sur une base.

    Nous posons \( \tau_i\colon \eR^n\to \eR^{n-1}\),
    \begin{equation}
        \tau_i(v)_k=\begin{cases}
            v_k    &   \text{si } k<i\\
            v_{k+1}    &    \text{si } k\geq i\text{.}
        \end{cases}
    \end{equation}
    et nous avons d'une part
    \begin{equation}
        e_k\cdot
                \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =\det\begin{pmatrix}
                     \tau_kv_1   \\
                     \vdots   \\
                     \tau_kv_{n-1}
                 \end{pmatrix}
            \end{equation}
     et d'autre part,
     \begin{equation}
         \det(e_k,v_1,\ldots, v_{n-1})=\det
         \begin{pmatrix}
             0&&&\\
             \vdots&&&\\
             1&v_1&\cdots&v_{n-1}\\
             \vdots&&&\\
             0&&&
         \end{pmatrix}=\det(\tau_k v_1,\ldots, \tau_k v_{n-1}).
     \end{equation}
     La première assertion est démontrée.

     En ce qui concerne la seconde, il suffit d'appliquer la première et se souvenir qu'un déterminant est nul lorsque deux lignes sont égales\footnote{Corollaire \ref{CORooAZFCooSYINvBl}.}. En effet :
     \begin{equation}
         v_k\cdot \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =
                 \det(v_k,v_1,\ldots, v_n)=0.
     \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Procédé de Gram-Schmidt}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Procédé de Gram-Schmidt]    \label{PropUMtEqkb}
    Un espace euclidien possède une base orthonormée.
\end{proposition}
\index{espace!euclidien}
\index{Gram-Schmidt}

\begin{proof}
    Soit \( E\) un espace euclidien et \( \{ v_1,\ldots, v_n \}\), une base quelconque de \( E\). Nous posons d'abord
    \begin{equation}
        \begin{aligned}[]
            f_1&=v_1,&e_1&=\frac{ f_1 }{ \| f_1 \| }.
        \end{aligned}
    \end{equation}
    Ensuite
    \begin{equation}
        \begin{aligned}[]
            f_2&=v_2-\langle v_2, e_1\rangle e_1,&e_2&=\frac{ f_2 }{ \| f_2 \| }.
        \end{aligned}
    \end{equation}
    Notons que \( \{ e_1,e_2 \}\) est une base de \( \Span\{ v_1,v_2 \}\). De plus elle est orthogonale :
    \begin{equation}
        \langle e_1, f_2\rangle =\langle e_1, v_2\rangle -\langle v_2, e_1\rangle \underbrace{\langle e_1, e_1\rangle}_{=1} =0.
    \end{equation}
    Le fait que \( \| e_1 \|=\| e_2 \|=1\) est par construction. Nous avons donc donné une base orthonormée de \( \Span\{ v_1,v_2 \}\).

    Nous continuons par récurrence en posant
    \begin{equation}
        \begin{aligned}[]
            f_k&=v_k-\sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i,&e_k&=\frac{ f_k }{ \| f_k \| }.
        \end{aligned}
    \end{equation}
    Pour tout \( j<k\) nous avons
    \begin{equation}
        \langle e_j, f_k\rangle =\langle e_j, v_k\rangle -\sum_{i=1}^{k-1}\langle v_k, e_i\rangle \underbrace{\langle e_i, e_j\rangle}_{=\delta_{ij}} =0
    \end{equation}
\end{proof}
Cet algorithme de Gram-Schmidt nous donne non seulement l'existence de bases orthonormée pour tout espace euclidien, mais aussi le moyen d'en construire à partir de n'importe quelle base.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminants}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecGYzHWs}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Formes multilinéaires alternées}
%---------------------------------------------------------------------------------------------------------------------------

%  Lire http://www.les-mathematiques.net/phorum/read.php?2,302266

\begin{definition}\index{déterminant!forme linéaire alternée}
    Soit \( E\), un \( \eK\)-espace vectoriel. Une forme linéaire \defe{alternée}{forme linéaire!alternée}\index{alternée!forme linéaire} sur \( E\) est une application linéaire \( f\colon E\to \eK\) telle que \( f(v_1,\ldots, v_k)=0\) dès que \( v_i=v_j\) pour certains \( i\neq j\).
\end{definition}

\begin{lemma}   \label{LemHiHNey}
    Une forme linéaire alternée est antisymétrique. Si \( \eK\) est de caractéristique différente de \( 2\), alors une forme antisymétrique est alternée.
\end{lemma}

\begin{proof}
    Soit \( f\) une forme alternée; quitte à fixer toutes les autres variables, nous pouvons travailler avec une \( 2\)-forme et simplement montrer que \( f(x,y)=-f(y,x)\). Pour ce faire nous écrivons
    \begin{equation}
        0=f(x+y,x+y)=f(x,x)+f(x,y)+f(y,x)+f(y,y)=f(x,y)+f(y,x).
    \end{equation}

    Pour la réciproque, si \( f\) est antisymétrique, alors \( f(x,x)=-f(x,x)\). Cela montre que \( f(x,x)=0\) lorsque \( \eK\) est de caractéristique différente de deux.
\end{proof}

\begin{proposition}[\cite{GQolaof}] \label{ProprbjihK}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension \( n\), où la caractéristique de \( \eK\) n'est pas deux. L'espace des \( n\)-formes multilinéaires alternées sur \( E\) est de \( \eK\)-dimension \( 1\).
\end{proposition}
\index{groupe!permutation}
\index{groupe!et géométrie}
\index{espace!vectoriel!dimension}
\index{rang}
\index{déterminant}
\index{dimension!\( n\)-formes multilinéaires alternées}

\begin{proof}
    Soient \( \{ e_i \}\), une base de \( E\), une \( n\)-forme linéaire alternée \( f\colon E\to \eK\) ainsi que des vecteurs \( (v_1,\ldots, v_n)\) de \( E\). Nous pouvons les écrire dans la base
    \begin{equation}
        v_j=\sum_{i=1}^n\alpha_{ij}e_i
    \end{equation}
    et alors exprimer \( f\) par
    \begin{subequations}
        \begin{align}
            f(v_1,\ldots, v_n)&=f\big( \sum_{i_1=1}^n\alpha_{1i_1}e_{i_1},\ldots, \sum_{i_n=1}^n\alpha_{ni_n}e_{i_n} \big)\\
            &=\sum_{i,j}\alpha_{1i_1}\ldots \alpha_{ni_n}f(e_{i_1},\ldots, e_{i_n}).
        \end{align}
    \end{subequations}
    Étant donné que \( f\) est alternée, les seuls termes de la somme sont ceux dont les \( i_k\) sont tous différents, c'est-à-dire ceux où \( \{ i_1,\ldots, i_n \}=\{ 1,\ldots, n \}\). Il y a donc un terme par élément du groupe des permutations \( S_n\) et
    \begin{equation}
        f(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}f(e_{\sigma(1)},\ldots, e_{\sigma(n)}).
    \end{equation}
    En utilisant encore une fois le fait que la forme \( f\) soit alternée, \( f=f(e_1,\ldots, e_n)\Pi\) où
    \begin{equation}
        \Pi(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}.
    \end{equation}
    Pour rappel, la donnée des \( v_i\) est dans les nombres \( \alpha_{ij}\).

    L'espace des \( n\)-formes alternées est donc \emph{au plus} de dimension \( 1\). Pour montrer qu'il est exactement de dimension \( 1\), il faut et suffit de prouver que \( \Pi\) est alternée. Par le lemme~\ref{LemHiHNey}, il suffit de prouver que cette forme est antisymétrique\footnote{C'est ici que joue l'hypothèse sur la caractéristique de \( \eK\).}.

    Soient donc \( v_1,\ldots, v_n\) tels que \( v_i=v_j\). En posant \( \tau=(1i)\) et \( \tau'=(2j)\) et en sommant sur \( \sigma\tau\tau'\) au lieu de \( \sigma\), nous pouvons supposer que \( i=1\) et \( j=2\). Montrons que \( \Pi(v,v,v_3,\ldots, v_n)=0\) en tenant compte que \( \alpha_{i1}=\alpha_{i2}\) :
    \begin{subequations}
        \begin{align}
            \Pi(v,v,v_3,\ldots, v_n)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n}\\
            &=\sum_{\sigma\in S_n}\epsilon(\sigma\tau)\alpha_{\sigma\tau(1)1}\alpha_{\sigma\tau(2)2}\alpha_{\sigma\tau(3)3}\ldots \alpha_{\sigma\tau(n)n}&\text{où } \tau=(12)\\
            &=-\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n} \\
            &=-\Pi(v,v,v_3,\ldots, v_n).
        \end{align}
    \end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant d'une famille de vecteurs}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons un corps \( \eK\) et l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\).

\begin{definition}[Déterminant d'une famille de vecteurs\cite{MathAgreg}]\label{DEFooODDFooSNahPb}
    Le \defe{déterminant}{déterminant!d'une famille de vecteurs} de la famille de vecteurs \( (v_1,\ldots, v_n)\) dans la base \( B\) est l'élément de \( \eK\)
    \begin{equation}        \label{EQooOJEXooXUpwfZ}
        \det_{(e_1,\ldots, e_n)}(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne^*_{\sigma(i)}(v_i)
    \end{equation}
    où 
    \begin{itemize}
        \item 
            la somme porte sur le groupe symétrique, 
        \item
            le nombre \( \epsilon(\sigma)\) est la signature de la permutation \( \sigma\),
        \item
            les éléments \( \{ e_i \}\) forment la base canonique de \( \eK^n\).
        \item
            les éléments \( \{ e^*_i \}\) sont la base duale de \( \{ e_i \}\).
    \end{itemize}
    Nous le notons \( \det_{(e_1,\ldots, e_n)}(v_1,\ldots, v_n)\).
\end{definition}

\begin{normaltext}
    La base \( \{ e_i \}\) est la base canonique de \( \eK^n\), et l'élément \( e_k^*\) est la forme linéaire définie par
    \begin{equation}
        \begin{aligned}
            e_k^*\colon \eK^n&\to \eK \\
            \sum_ix_ie_i&\mapsto x_k. 
        \end{aligned}
    \end{equation}
    Il n'est pas sous-entendu que \( \eK^n\) ait un produit scalaire. Il n'est donc pas autorisé de dire que \( \{ e_i \}\) est une base orthonormée et que \( e^*_k(x)=\langle e_k, x\rangle \). Ce genre d'égalités sont vraies dans le cas \( \eK=\eR\), mais n'ont pas de sens en général.

    Le lemme \ref{LEMooEZFIooXyYybe} va un peu parler du cas où \( \eK^n\) est muni d'une base orthonormée.
\end{normaltext}

\begin{lemma}[\cite{MathAgreg}]     \label{LemJMWCooELZuho}
    Les propriétés du déterminant. Soit \( B\) une base de \( E\).
    \begin{enumerate}
        \item\label{ITEMooAHOHooDZgtSB}
            L'application \( \det_B\colon E^n\to \eK\) est \( n\)-linéaire.
        \item\label{ITEMooTXXBooBmDtzd}
            L'application \( \det_B\colon E^n\to \eK\) est \( n\)-linéaire est antisymétrique et alternée\footnote{En caractéristique \( 2\), alternée n'est pas équivalent à symétrique.}.
        \item   \label{ITEMooNFJTooTqGoPr}
            Pour toute base, \( \det_B(B)=1\).
        \item   \label{ITEMooALRQooDvBzDQ}
            Le déterminant ne change pas si on remplace un vecteur par une combinaison linéaire des autres :
            \begin{equation}
                \det_B(v_1,\ldots, v_n)=\det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big).
            \end{equation}
        \item   \label{ITEMooQTTRooMbzqyW}
            Si on permute les vecteurs,
            \begin{equation}
                \det_B(v_1,\ldots, v_n)=\epsilon(\sigma)\det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)}).
            \end{equation}
        \item   \label{ITEMooIPIDooTrerVF}
            Si \( B'\) est une autre base :
            \begin{equation}        \label{EqAWICooBLTTOY}
                \det_B=\det_B(B')\det_{B'}
            \end{equation}
        \item   \label{ITEMooXKTAooXynFTE}
            Nous avons aussi la formule \( \det_{B}(B')\det_{B'}(B)=1\).
        \item\label{ItemDWFLooDUePAf}
            Les vecteurs \( \{ v_1,\ldots, v_n \}\) forment une base si et seulement si \( \det_B(v_1,\ldots, v_n)\neq 0\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Point par point.
    \begin{subproof}
    \item[\ref{ITEMooAHOHooDZgtSB}]
            En posant \( v_1=x_1+\lambda x_2\) nous avons
            \begin{subequations}
                \begin{align}
                    \det_B(x_1+\lambda x_2,v_2,\ldots, v_n)&=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^ne^*_{\sigma(i)}(v_i)\\
                    &=\sum_{\sigma}\epsilon(\sigma)\Big( e^*_{\sigma(1)}(x_1+\lambda x_2) \Big)\prod_{i=2}^ne^*_{\sigma(i)}(v_i).
                \end{align}
            \end{subequations}
            À partir de là, la linéarité de \( e^*_{\sigma(1)}\) montre que \( \det_B\) est linéaire en son premier argument. Pour les autres arguments, le même calcul tient.

        \item[\ref{ITEMooTXXBooBmDtzd}]

            Nous prouvons à présent que \( \det\) est alternée. Si votre corps est de caractéristique différente de deux, vous pouvez lire \ref{NORMooAEJLooSnDBhy}.

            Supposons \( v_k=v_l\), et considérons la permutation \( \beta=(k,l)\). Nous savons par la proposition \ref{PROPooZOWBooIMxxlj} que \( S_n=A_n\cup A_n\beta\). Cela nous permet de décomposer la somme sur \( S_n\) en deux parties :
            \begin{equation}        \label{EQooWFHQooTrTTWl}
                \sigma_{\sigma\in S_n}(-1)^{\sigma}\prod_i\epsilon_{\sigma(i)}^*(v_i)=\sum_{\sigma\in A_n}(-1)^{\sigma}\prod_i\epsilon_{\sigma(i)}^*(v_i)+\sum_{\sigma\in A_n}(-1)^{\sigma\beta}\prod_i\epsilon_{(\sigma\beta)(i)}^*(v_i).
            \end{equation}
            D'abord \( (-1)^{\sigma}=1\) et \( (-1)^{\sigma\beta}=-1\). Ensuite, pour un \( \sigma\in A_n\) donné, nous avons
            \begin{subequations}
                \begin{align}
                    \prod_i\epsilon^*_{(\sigma\beta)(i)}(v_i)&=\epsilon_{(\sigma\beta)(k)}^*(v_k)\epsilon^*_{(\sigma\beta)(l)}(v_l)\prod\stackrel{i\neq k}{i\neq l}\epsilon_{(\sigma\beta)(i)}^*(v_i)\\
                    &=\epsilon^*_{\sigma(l)}(v_k)\epsilon^*_{\sigma(k)}(v_l)\prod_{\substack{i\neq k\\i\neq l}}\epsilon^*_{\sigma(i)}(v_i)\\
                    &=\epsilon^*_{\sigma(l)}(v_l)\epsilon^*_{\sigma(k)}(v_k)\prod_{\substack{i\neq k\\i\neq l}}\epsilon^*_{\sigma(i)}(v_i)\\
                    &=\prod_i\epsilon^*_{\sigma(i)}(v_i).
                \end{align}
            \end{subequations}
            Donc les deux termes de la somme \eqref{EQooWFHQooTrTTWl} ne diffèrent que par un signe. Elle est donc nulle, et la forme déterminant est alternée.    
        
            La fonction \( \det\) est antisymétrique parce que alternée, voir le lemme \ref{LemHiHNey}.

        \item[\ref{ITEMooNFJTooTqGoPr}]
            Nous avons
            \begin{equation}
                \det_B(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\underbrace{e_{\sigma(i)}^*(e_i)}_{=\delta_{\sigma(i),i}}.
            \end{equation}
            Si \( \sigma\) n'est pas l'identité, le produit contient forcément un facteur nul. Il ne reste de la somme que \( \sigma=\id\) et le résultat est \( 1\).
        \item[\ref{ITEMooALRQooDvBzDQ}]
            Vu que \( \det_B\) est linéaire en tous ses arguments,
            \begin{equation}
                \det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big)=\det_B(v_1,\ldots, v_n)+\sum_{s=2}^na_s\det_B(v_s,v_2,\ldots, v_n).
            \end{equation}
            Chacun des termes de la somme est nul parce qu'il y a répétition de \( v_s\) parmi les arguments alors que la forme est alternée.
        \item[\ref{ITEMooQTTRooMbzqyW}]
            Nous devons calculer \( \det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)})\), et pour y voir plus clair nous posons \( w_i=v_{\sigma(i)}\). Alors :
            \begin{subequations}
                \begin{align}
                    \det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)})&=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(w_i)\\
                    &=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_{\sigma(i)})\\
                    &=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma^{-1}\sigma'(i)}(v_i)\\
                    &=\sum_{\sigma'}\epsilon(\sigma\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_i)\\
                    &=\epsilon(\sigma)\det_B(v_1,\ldots, v_n).
                \end{align}
            \end{subequations}
            Justifications : nous avons d'abord modifié l'ordre des éléments du produit et ensuite l'ordre des éléments de la somme. Nous avons ensuite utilisé le fait que \( \epsilon\colon S_n\to \{ 0,1 \}\) était un morphisme de groupe (proposition~\ref{ProphIuJrC}).
        \item[\ref{ITEMooIPIDooTrerVF}]
            Étant donné que l'espace des formes multilinéaires alternées est de dimension \( 1\), il existe un \( \lambda\in \eK\) tel que \( \det_B=\lambda\det_{B'}\). Appliquons cela à \( B'\) :
            \begin{equation}
                \det_B(B')=\lambda\det_{B'}(B'),
            \end{equation}
            donc \( \lambda=\det_B(B')\).
        \item[\ref{ITEMooXKTAooXynFTE}]
            Il suffit d'appliquer l'égalité précédente à \( B\) en nous souvenant que \( \det_B(B)=1\).
        \item[\ref{ItemDWFLooDUePAf}]
            Si \( B'=\{ v_1,\ldots, v_n \}\) est une base alors \( \det_B(B')\neq 0\), sinon il n'est pas possible d'avoir \( \det_B(B')\det_{B'}(B)=1\).

            À l'inverse, si \( B'\) n'est pas une base, c'est que \( \{ v_1,\ldots, v_n \}\) est liée par la proposition \ref{PROPooVEVCooHkrldw}. Il y a donc moyen de remplacer un des vecteurs par une combinaison linéaire des autres. Le déterminant s'annule alors.
    \end{subproof}
\end{proof}

\begin{normaltext}      \label{NORMooAEJLooSnDBhy}
    Si la caractéristique du corps de base n'est pa deux, une forme antisymétrique est alternée (lemme \ref{LemHiHNey}). Il est alors plus facile de prouver que le déterminant est antisymétrique et d'en déduire qu'il est alterné.

    Permuter \( v_k\) et \( v_l\) revient à calculer le nombre \( \det_B( v_{\sigma_{kl}(1)},\ldots, v_{\sigma_{kl}(n)} )\) au lieu de \( \det_B(v_1,\ldots, v_n)\). Cela revient à changer la somme \( \sum_{\sigma}\) en \( \sum_{\sigma\circ\sigma_{kl}}\). Cela ajoute \( 1\) à \( \epsilon(\sigma)\) vu que l'on ajoute une permutation.

    Donc le déterminant est antisymétrique. Nous en déduisons qu'il est alterné parce que \( \det_B(v_1,v_1)=-\det_B(v_1,v_1)\) (permutation de \( v_1\) et \( v_1\)). Si le corps est de caractéristique différente de deux, cela implique que \( \det_B(v_1,v_1)=0\).
\end{normaltext}

D'après la proposition~\ref{ProprbjihK}, il existe une unique forme \( n\)-linéaire alternée égale à \( 1\) sur \( B\), et c'est \( \det_B\colon E^n\to \eK\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant d'un endomorphisme}
%---------------------------------------------------------------------------------------------------------------------------

L'interprétation géométrique du déterminant en termes d'aires et de volumes est donnée après la théorème~\ref{ThoBVIJooMkifod}.

\begin{lemmaDef}       \label{LEMooQTRVooAKzucd}      \label{DefCOZEooGhRfxA}
    Si \( f\colon E\to E\) est un endomorphisme, et si les parties \( B\) et \( B'\) sont deux bases, alors 
    \begin{equation}
        \det_B\big( f(B) \big)=\det_{B'}\big( f(B') \big).
    \end{equation}
    Ce nombre, indépendant de la base choisie est nommé le \defe{déterminant}{déterminant!d'un endomorphisme} de \( f\) et est noté \( \det(f)\).
\end{lemmaDef}

\begin{proof}
    L'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon E^n&\to \eK \\
            v_1,\ldots, v_n&\mapsto \det_B\big( f(v_1),\ldots, f(v_n) \big)
        \end{aligned}
    \end{equation}
    est \( n\)-linéaire et alternée; il existe donc \( \lambda\in \eK\) tel que \( \varphi=\lambda\det_B\). En appliquant cela à \( B\) :
    \begin{equation}
        \det_B\big( f(B) \big)=\lambda \det_B(B)=\lambda.
    \end{equation}
    Nous avons donc déjà prouvé que \( \lambda=\det_B\big( f(B) \big)\), c'est-à-dire
    \begin{equation}
        \det_B\big( f(v) \big)=\det_B\big( f(B) \big)\det_B(v).
    \end{equation}

    Nous allons maintenant introduire \( B'\) là où il y a du \( v\) en utilisant les formules \eqref{EqAWICooBLTTOY} :
    \begin{subequations}
        \begin{align}
            \det_B\big( f(v) \big)&=\det_B(B')\det_{B'}\big( f(v) \big)\\
            \det_B(v)=\det_B(B')\det_{B'}(v).
        \end{align}
    \end{subequations}
    Nous obtenons
    \begin{equation}
        \det_{B'}\big( f(v) \big)=\det_B\big( f(B) \big)\det_{B'}(v).
    \end{equation}
    Et on applique cela à \( v=B'\) :
    \begin{equation}
        \det_{B'}\big( f(B') \big)=\det_B\big( f(B) \big)\underbrace{\det_{B'}(B')}_{=1}.
    \end{equation}
\end{proof}

Couplé à la formule \eqref{EQooOJEXooXUpwfZ}, nous pouvons écrire la formule pratique à utiliser le plus souvent. 

\begin{lemma}       \label{LEMooEZFIooXyYybe}
    Soit un espace vectoriel euclidien\footnote{C'est-à-dire qu'il possède un produit scalaire, voir la définition \ref{DefLZMcvfj}.} \( E\) sur le corps \( \eK\). Si \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( E\) et si \( f\colon E\to E\) est un endomorphisme, alors
    \begin{equation}        \label{EQooQAZLooZutFUz}
        \det(f)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\langle e_{\sigma(i)}, f(e_i)\rangle.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous utilisons la définition \ref{LEMooQTRVooAKzucd} du déterminant d'un endomorphisme \( \det(f)=\det_B\big( f(B) \big)\) en prenant la liste des vecteurs \( \{ e_i \}\) comme \( B\). En l'occurrence, le \( i\)\ieme\ vecteur de la famille \( B\) est \( f(e_i)\).

    Vu que la base est orthonormée, nous avons \( e^*_k(v)=\langle e_k, v\rangle \) et donc aussi
    \begin{equation}
        e^*_{\sigma(i)}(v_i)=\langle e_{\sigma(i)}^*, f(e_i)\rangle.
    \end{equation}
\end{proof}

Et si vous avez tout suivi, vous aurez remarqué que les produits scalaires impliqués dans la formule \eqref{EQooQAZLooZutFUz} sont les éléments de la matrice de \( f\) dans la base \( \{ e_i \}\) parce que \( \langle e_i, f(e_j)\rangle \) est la composante \( i\) de l'image de \( e_j\) par \( f\). Si la matrice est composée en mettant en colonne les images des vecteurs de base, le compte est bon.

\begin{proposition}     \label{PropYQNMooZjlYlA}
    Principales propriétés géométriques du déterminant d'un endomorphisme.
    \begin{enumerate}
        \item   \label{ItemUPLNooYZMRJy}
            Si \( f\) et \( g\)  sont des endomorphismes, alors \( \det(f\circ g)=\det(f)\det(g)\).
        \item       \label{ITEMooNZNLooODdXeH}
            L'endomorphisme \( f\) est un automorphisme\footnote{Endomorphisme inversible, définition~\ref{DEFooOAOGooKuJSup}.} si et seulement si \( \det(f)\neq 0\).\index{déterminant!et inversibilité}
        \item   \label{ITEMooZMVXooLGjvCy}
            Si \( \det(f)\neq 0\) alors \( \det(f^{-1})=\det(f)^{-1}\).
        \item       \label{ItemooPJVYooYSwqaE}
            L'application \( \det\colon \GL(E)\to \eK\setminus\{ 0 \}\) est un morphisme de groupe.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
            Nous considérons l'application
            \begin{equation}
                \begin{aligned}
                    \varphi\colon E^n&\to \eK \\
                    v&\mapsto \det_B\big( f(v) \big).
                \end{aligned}
            \end{equation}
            Comme d'habitude nous avons \( \varphi(v)=\lambda\det_B(v)\). En appliquant à \( B\) et en nous souvenant que \( \det_B(B)=1\) nous avons
                $\det_B\big( f(B) \big)=\lambda$. Autrement dit :
                \begin{equation}
                    \lambda=\det(f).
                \end{equation}
            Calculons à présent \( \varphi\big( g(B) \big)\) : d'une part,
            \begin{equation}
                \varphi\big( g(B) \big)=\det_B\big( (f\circ g)(B) \big)
            \end{equation}
            et d'autre part,
            \begin{equation}
                \varphi\big( g(B) \big)=\lambda\det_B\big( g(B) \big)=\lambda\det(g)
            \end{equation}
            En égalisant et en reprenant la la valeur déjà trouvée de \( \lambda\),
            \begin{equation}
                \det\big(f\circ g)(B) \big)=\det(f)\det(g),
            \end{equation}
            ce qu'il fallait.
        \item
            Supposons que \( f\) soit un automorphisme. Alors si \( B\) est une base, \( f(B) \) est une base. Par conséquent \( \det(f)=\det_B\big( f(B) \big)\neq 0\) parce que \( f(B)\) est une base (lemme~\ref{LemJMWCooELZuho}\ref{ItemDWFLooDUePAf}).

            Réciproquement, supposons que \( \det(f)\neq 0\). Alors si \( B\) est une base quelconque nous avons \( \det_B\big( f(B) \big)\neq 0\), ce qui est uniquement possible lorsque \( f(B)\) est une base. L'application \( f\) transforme donc toute base en une base et est alors un automorphisme d'espace vectoriel.
        \item
            Vu que le déterminant de l'identité est \( 1\) et que \( f\) est inversible, \( 1=\det(f\circ f^{-1})=\det(f)\det(f^{-1})\).
    \end{enumerate}
\end{proof}

\begin{proposition}     \label{PROPooFKDXooKMSolt}
    Soient deux espaces vectoriels \( E\) et \( F\) de dimension finies \( n\) et \( m\) sur le corps \( \eK\) munis de bases \( \{e_i\}\) et \( \{f_{\alpha}\}\). À une matrice \( A\in \\eM(m\times n,\eK)\) nous associons l'application linéaire\footnote{Dont nous avons déjà beaucoup parlé entre autres dans la proposition \ref{PROPooCSJNooEqcmFm}.}
    \begin{equation}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}.
    \end{equation}
    
    Alors, en ce qui concerne les déterminants\footnote{Définition \ref{LEMooQTRVooAKzucd} pour les applications linéaires et \ref{DEFooYCKRooTrajdP} pour les matrices.}, nous avons
    \begin{enumerate}
        \item
            \( \det(f_A)=\det(A)\)
        \item
            \( \det(f_{AB})=\det(f_A)\det(f_B)\)
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous devons étudier la formule
    \begin{equation}
        \det(f_A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne_{\sigma(i)}^*\big( f_A(e_i) \big).
    \end{equation}
    En premier lieu nous avons
    \begin{equation}
        f_A(e_i)=\sum_{jk}A_{jk}(e_i)_ke_j=\sum_jA_{ji}e_j.
    \end{equation}
    Nous avons alors
    \begin{equation}
        e_{\sigma(i)}^*\big( f_A(e_i) \big)=\sum_jA_{ji}\underbrace{e^*_{\sigma(i)}(e_j)}_{\delta_{j\sigma(i)}}=A_{\sigma(i)i}.
    \end{equation}
    Au final,
    \begin{equation}
        \det(f_A)=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^nA_{\sigma(i)i}=\det(A^t)=\det(A)
    \end{equation}
    où la dernière égalité est autorisée par le lemme \ref{LEMooCEQYooYAbctZ}.

    Cela prouve la formule \( \det(f_A)=\det(A)\).

    En ce qui concerne la seconde formule, il s'agit de se souvenir de la proposition \ref{PROPooCSJNooEqcmFm} qui donne \( f_{AB}=f_A\circ f_B\), et ensuite de la proposition \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} qui donne \( \det(f_A\circ f_B)=\det(f_A)\det(f_B)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Vandermonde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]  \label{PropnuUvtj}
    Le \defe{déterminant de Vandermonde}{déterminant!Vandermonde}\index{Vandermonde (déterminant)} est le polynôme en \( n\) variables donné par
    \begin{equation}
        V(T_1,\ldots, T_n)=\det\begin{pmatrix}
             1   &   1    &   \ldots    &   1    \\
             T_1   &   T_2    &   \ldots    &   T_n    \\
             \vdots   &   \ddots    &   \ddots    &   \vdots    \\
             T_1^{n-1}   &   T_2^{n-1}    &   \ldots    &   T_n^{n-1}
         \end{pmatrix}=\prod_{1\leq i<j\leq n}(T_j-T_i).
    \end{equation}
    Notez que l'inégalité du milieu est stricte (sinon d'ailleurs l'expression serait nulle).
\end{proposition}

\begin{proof}
    Nous considérons le polynôme
    \begin{equation}
        f(X)=V(T_1,\ldots, T_{n-1},X)\in \big( \eK[T_1,\ldots, T_{n-1}] \big)[X].
    \end{equation}
    %TODOooHLQLooWXznGe Mettre une référence vers la proposition qu'il faut à ce «par conséquent».
    C'est un polynôme de degré au plus \( n-1\) en \( X\) et il s'annule aux points \( T_1,\ldots, T_{n-1}\). Par conséquent il existe \( \alpha\in \eK[T_1,\ldots, T_{n-1}]\) tel que
    \begin{equation}    \label{EqeVxRwO}
        f=\alpha(X-T_{n-1})\ldots(X-T_1).
    \end{equation}
    Nous trouvons \( \alpha\) en écrivant \( f(0)\). D'une part la formule \eqref{EqeVxRwO} nous donne
    \begin{equation}    \label{EqblwWMj}
        f(0)=\alpha(-1)^{n-1}T_1\ldots T_{n-1}.
    \end{equation}
    D'autre par la définition donne
    \begin{subequations}
        \begin{align}
            f(0)&=\det\begin{pmatrix}
                 1   &   \cdots    &   1    &   1    \\
                 T_1      &       &   T_{n-1}    &   0    \\
                 \vdots   &       &   \vdots    &   \vdots    \\
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}    &   0
             \end{pmatrix}\\
             &=(-1)^{n-1}\det\begin{pmatrix}
                 T_1   &   \ldots    &   T_{n-1}    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \ldots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}\det\begin{pmatrix}
                 1   &   \cdots    &   1    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}V(T_1,\ldots, T_{n-1})
        \end{align}
    \end{subequations}
    En égalisant avec \eqref{EqblwWMj}, nous trouvons \( \alpha=V(T_1,\ldots, T_{n-1})\), et donc
    \begin{equation}
        f=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(X-T_j)
    \end{equation}
    Enfin, une récurrence montre que
    \begin{subequations}
        \begin{align}
            V(T_1,\ldots, T_n)&=f(T_n)\\
            &=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(T_n-T_j)\\
            &=\prod_{k\leq n}\prod_{j\leq k-1}(T_k-T_j)\\
            &=\prod_{1\leq j<k\leq n}(T_i-T_j).
        \end{align}
    \end{subequations}
\end{proof}

\begin{example}
    Le déterminant de Vandermonde (proposition~\ref{PropnuUvtj}) est alterné, semi-symétrique et non symétrique. Le fait qu'il soit alterné est le fait qu'il soit un déterminant. Étant donné qu'il est alterné, il est semi-symétrique parce que sur \( A_n\), nous avons \( \epsilon=1\). Étant donné qu'il est alterné, il change de signe sous l'action des éléments impairs de \( S_n\) et n'est donc pas symétrique.
\end{example}

\begin{proposition}\index{action de groupe} \label{PropUDqXax}
    Un polynôme semi-symétrique \( f\in \eK[T_1,\ldots, T_n]\) se décompose de façon unique en
    \begin{equation}
        f=P+VQ
    \end{equation}
    où \( P\) et \( Q\) sont deux polynômes symétriques.
\end{proposition}
\index{groupe!permutation}
\index{polynôme!symétrique}

\begin{proof}

    Nous commençons par prouver l'unicité en montrant que si \( f=PVQ\) avec \( P\) et \( Q\) symétrique, alors \( P\) et \( Q\) sont donnés par des formules explicites en termes de \( f\).


    Si \( \sigma_1\) et \( \sigma_2\) sont deux permutations impaires de \( \{ 1,\ldots, n \}\), alors \( \sigma_1\cdot f=\sigma_2\cdot f\) parce que l'élément \( \sigma_2^{-1}\sigma_1\) est pair (proposition~\ref{ProphIuJrC}), de telle sorte que \( \sigma_2^{-1}\sigma_1\cdot f=f\). Nous posons donc \( g=\tau\cdot f\) où \( \tau\) est une permutation impaire quelconque -- par exemple une transposition.

    Vu que \( V\) est alternée et que \( \tau\) est une transposition nous avons
    \begin{equation}
        g=\tau\cdot f=P-VQ.
    \end{equation}
    Donc \( f+g=2P\) et \( f-g=2VQ\). Cela donne \( P\) et \( Q\) en terme de \( f\) et \( g\), et donc l'unicité.

    Attention : cela ne donne pas un moyen de prouver l'existence parce que rien ne prouve pour l'instant que \( f-g\) peut effectivement être écrit sous la forme \( VQ\), c'est-à-dire que \( f-g\) soit divisible par \( V\). C'est cela que nous allons nous atteler à démontrer maintenant.

    Nous commençons par prouver que \( f+g\) est symétrique et \( f-g\) alterné. Si \( \sigma\) est une transposition,
    \begin{equation}
        \sigma\cdot(f+g)=\sigma\cdot f+\sigma\tau\cdot f=g+f
    \end{equation}
    parce que \( \sigma\tau\) est pair. De la même façon,
    \begin{equation}
        \sigma\cdot(f-g)=g-f=\epsilon(\sigma)(f-g).
    \end{equation}
    Dans les deux cas nous concluons en utilisant le fait que toute permutation est un produit de transpositions (proposition~\ref{PropPWIJbu}) et que \( \epsilon\) est un homomorphisme.

    Soient maintenant deux entiers \( h<k\) dans \( \{ 1,\ldots, n \}\) et l'anneau
    \begin{equation}
        \big( \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \big)[T_k].
    \end{equation}
    Cet anneau contient le polynôme \( T_k-T_h\) où \( T_k\) est la variable et \( T_h\) est un coefficient. Nous faisons la division euclidienne de \( f-g\) par  \( T_k-T_h\) parce que nous avons dans l'idée de faire arriver le déterminant de Vandermonde et donc le produit de toutes les différences \( T_k-T_h\) :
    \begin{equation}    \label{EqSHdgrG}
        f-g=(T_k-T_h)q+r
    \end{equation}
    où \( \deg_{T_k}r<1\), c'est-à-dire que \( r\) ne dépends pas de \( T_k\). Nous revoyons maintenant l'égalité \eqref{EqSHdgrG} dans \( \eK[T_1,\ldots, T_n]\) et nous y appliquons la transposition \( \tau_{kh}\). Nous savons que \( \tau_{kh}(f-g)=-(f-g)\) et \( \tau_{kh}(T_k-T_h)=-(T_k-T_h)\), et donc
    \begin{equation}    \label{EqVOhjKB}
        -(f-g)=-(T_k-T_h)\tau_{kh}\cdot   q+\tau_{kh}\cdot r
    \end{equation}
    où \(\tau_{kh}\cdot r\) ne dépend pas de \( T_h\). Nous appliquons à \eqref{EqVOhjKB} l'application
    \begin{equation}
        \begin{aligned}
            t\alpha\colon \eK[T_1,\ldots, T_n]&\to \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \\
            \alpha(PT_1,\ldots, \hat T_k,\ldots, T_n)&=P(T_1,\ldots, T_h,\ldots, T_n).
        \end{aligned}
    \end{equation}
    Cette application vérifie \( \alpha\big( \tau_{kh}\cdot r \big)=\alpha(r)\) et nous avons
    \begin{equation}
        -\alpha(f-g)=\alpha(r).
    \end{equation}
    Puis en appliquant \( \alpha\) à la relation \( f-g=(T_k-T_h)q+r\), nous trouvons
    \begin{equation}
        \alpha(f-g)=\alpha(r),
    \end{equation}
    et par conséquent \( \alpha(r)=0\). Ici nous utilisons l'hypothèse de caractéristique différente de deux. Dire que \( \alpha(r)=0\), c'est dire que \( r\) est divisible par \( T_k-T_h\), mais \( r\) étant de degré zéro en \( T_k\), nous avons \( r=0\). Par conséquent \( T_k-T_h\) divise \( f-g\) pour tout \( h<k\), et nous pouvons définir un polynôme \( Q\) par
    \begin{equation}    \label{EqrnbgdA}
        f-g=2Q\prod_{h<k}\prod_{k\leq n}(T_k-T_h)=2Q(T_1,\ldots, T_n)V(T_1,\ldots, T_n),
    \end{equation}
    où nous avons utilisé la formule du déterminant de Vandermonde de la proposition~\ref{PropnuUvtj}.

    Étant donné que \( f+g\) est un polynôme symétrique, nous allons aussi poser \( f+g=2P\) avec \( P\) symétrique.

    Montrons à présent que \( Q\) est un polynôme symétrique. Soit \( \sigma\in S_n\); vu que nous savons déjà que \( f-g\) est alternée, nous avons
    \begin{equation}    \label{EqpSPEyq}
        \sigma\cdot (f-g)=\epsilon(\sigma)(f-g)=\epsilon(\sigma)2QV,
    \end{equation}
    Mais en appliquant \( \sigma\) à l'équation \eqref{EqrnbgdA},
    \begin{subequations}
        \begin{align}
            \sigma\cdot (f-g)&=2(\sigma\cdot V)(T_1,\ldots, ,T_n)(\sigma\cdot Q)(T_1,\ldots,T_n)\\
            &=2\epsilon(\sigma)V(T_1,\ldots, T_n)(\sigma\cdot Q)(T_1,\ldots, T_n).
        \end{align}
    \end{subequations}
    Nous égalisons cela avec \eqref{EqpSPEyq} et nous souvenant que l'anneau \( \eK[T_1,\ldots, T_n]\) est intègre par le théorème \ref{ThoBUEDrJ}. Ensuite nous simplifions par \( 2\epsilon(\sigma)V\) pour obtenir
    \begin{equation}
        Q=\sigma\cdot Q,
    \end{equation}
    c'est-à-dire que \( Q\) est symétrique.

    Au final nous avons \( f+q=2P\) et \( f-g=2VQ\) avec \( P\) et \( Q\) symétriques. En faisant la somme,
    \begin{equation}
        f=P+VQ.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Gram}
%---------------------------------------------------------------------------------------------------------------------------

Si \( x_1,\ldots, x_r\) sont des vecteurs d'un espace vectoriel, alors le \defe{déterminant de Gram}{déterminant!Gram}\index{Gram (déterminant)} est le déterminant
\begin{equation}
    G(x_1,\ldots, x_r)=\det\big( \langle x_i, x_j\rangle  \big).
\end{equation}
Notons que la matrice est une matrice symétrique.

\begin{proposition}\label{PropMsZhIK}
    Si \( F\) est un sous-espace vectoriel de base \( \{ x_1,\ldots, x_n \}\) et si \( x\) est un vecteur, alors le déterminant de Gram est un moyen de calculer la distance entre \( x\) et \( F\) par
    \begin{equation}
        d(x,F)^2=\frac{ G(x,x_1,\ldots, x_n)}{ G(x_1,\ldots, x_n) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Cauchy}
%---------------------------------------------------------------------------------------------------------------------------

Soient des nombres \( a_i\) et \( b_i\) (\( i=1,\ldots, n\)) tels que \( a_i+b_j\neq 0\) pour tout couple \( (i,j)\). Le \defe{déterminant de Cauchy}{déterminant!de Cauchy}\index{Cauchy!déterminant} est
\begin{equation}
    D_n=\det\left( \frac{1}{ a_i+b_j } \right).
\end{equation}

\begin{proposition}[\cite{RollandRobertjyYDzY}] \label{ProptoDYKA}
    Le déterminant de Cauchy est donné par la formule
    \begin{equation}
        D_n=\frac{ \prod_{i<j}(a_j-a_i)\prod_{i<j}(b_j-b_i) }{ \prod_{ij}(a_i+b_j) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSQBJfr}

La définition est pompée de \wikipedia{fr}{Matrice_de_Sylvester}{wikipédia}. Soient \( P\) et \( Q\) deux polynômes non nuls, de degrés respectifs \( m\) et \( n\) :
\begin{subequations}
    \begin{align}
        P(x)=p_0+p_1x+\cdots +p_nx^n\\
        Q(x)=q_0+q_1x+\cdots +q_mx^m.
    \end{align}
\end{subequations}
La \defe{matrice de Sylvester}{matrice!de Sylvester}\index{Sylvester (matrice)} associée à \( P\) et \( Q\) est la matrice carrée \( m+n\times m+n\) définie ainsi :
\begin{enumerate}
    \item
la première ligne est formée des coefficients de \( P\), suivis de 0 :
\begin{equation}
\begin{pmatrix} p_n & p_{n-1} & \cdots & p_1 & p_0 & 0 & \cdots & 0 \end{pmatrix} ;
\end{equation}
\item la seconde ligne s'obtient à partir de la première par permutation circulaire vers la droite ;
\item les $(m-2)$ lignes suivantes s'obtiennent en répétant la même opération ;
\item la ligne $(m+1)$ est formée des coefficients de \( Q\), suivis de 0 :
    \begin{equation}
    \begin{pmatrix} q_m & q_{m-1} & \cdots & q_1 & q_0 & 0 & \cdots & 0 \end{pmatrix} ;
    \end{equation}
    \item les $(m-1)$ lignes suivantes sont formées par des permutations circulaires.
\end{enumerate}

Ainsi dans le cas $n=4$ et $m=3$, la matrice obtenue est
\begin{equation}    \label{EqPEgtle}
S_{p,q}=\begin{pmatrix}
p_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0 \\
0 & p_4 & p_3 & p_2 & p_1 & p_0 & 0 \\
0 & 0 & p_4 & p_3 & p_2 & p_1 & p_0 \\
q_3 & q_2 & q_1 & q_0 & 0 & 0 & 0 \\
0 & q_3 & q_2 & q_1 & q_0 & 0 & 0 \\
0 & 0 & q_3 & q_2 & q_1 & q_0 & 0 \\
0 & 0 & 0 & q_3 & q_2 & q_1 & q_0 \\
\end{pmatrix}.
\end{equation}
Le déterminant de la matrice de Sylvester associée à \( P\) et \( Q\) est appelé le \defe{résultant}{résultant} de \( P\) et \( Q\) et noté \( \res(P,Q)\)\nomenclature[A]{\( \res(P,Q)\)}{résultat des polynômes \( P\) et \( Q\)}.

Attention : si \( P\) est de degré \( n\) et \( Q\) de degré \( m\), il y a \( m\) lignes pour \( P\) et \( n\) pour \( Q\) dans le déterminant du résultant (et non le contraire).

\begin{lemma}[\cite{QQuRUzA}]       \label{LemBFrhgnA}
    Si \( P\) et \( Q\) sont deux polynômes de degrés \( n\) et \( m\) à coefficients dans l'anneau \( \eA\), alors pour tout \( \lambda\in \eA\),
    \begin{subequations}
        \begin{align}
            \res(\lambda P,Q)&=\lambda^m\res(P,Q)\\
            \res(P,\lambda Q)&=\lambda^n\res(P,Q).
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Cela est simplement un comptage du nombre de lignes. Il y a \( m\) lignes contenant les coefficients de \( P\); donc prendre \( \lambda P\) revient à multiplier \( m\) lignes dans un déterminant et donc le multiplier par \( \lambda^m\).
\end{proof}

L'équation de Bézout \eqref{EqkbbzAi}\index{théorème!Bézout!utilisation} peut être traitée avec une matrice de Sylvester. Soient \( P\) et \( Q\), deux polynômes donnés et à résoudre l'équation
\begin{equation}    \label{EqSsyXOo}
    xP+yQ=0
\end{equation}
par rapport aux polynômes inconnus \( x\) et \( y\) dont les degrés sont \( \deg(x)<\deg(Q)\) et \( \deg(y)<\deg(P)\). Si nous notons \( \tilde x\) et \( \tilde y\) la liste des coefficients de \( x\) et \( y\) (dans l'ordre décroissant de degré), nous pouvons récrire l'équation \eqref{EqSsyXOo} sous la forme
\begin{equation}
    S_{PQ}^t\begin{pmatrix}
        \tilde x    \\
        \tilde y
    \end{pmatrix}=0.
\end{equation}
Pour s'en convaincre, écrivons pour les polynômes de l'exemple \eqref{EqPEgtle} :
\begin{equation}
    \begin{pmatrix}
        p_4    &   0    &   0    &   q_3    &   0    &   0    &   0\\
        p_3    &   p_4    &   0    &   q_2    &   q_3    &   0    &   0\\
        p_2    &   p_3    &   p_4    &   q_1    &   q_2    &   q_3    &   0\\
        p_1    &   p_2    &   p_3    &   q_0    &   q_1    &   q_2    &   q_3\\
        p_0    &   p_1    &   p_2    &   0    &   q_0    &   q_1    &   q_2\\
        0    &   p_0    &   p_1    &   0    &   0    &   q_0    &   q_1\\
        0    &   0    &   p_0    &   0    &   0    &   0    &   q_0\\
    \end{pmatrix}\begin{pmatrix}
        x_2    \\
        x_1  \\
        x_0  \\
        y_3   \\
        y_2    \\
        y_1    \\
        y_0
    \end{pmatrix}=
    \begin{pmatrix}
        x_2p_4+y_2q_3    \\
        p_3x_2+p_4x_1+q_2y_3+q_3y_2  \\
          \\
           \\
        \vdots    \\
            \\

    \end{pmatrix}
\end{equation}
Nous voyons que sur la ligne numéro \( k\) (en partant du bas et en numérotant de à partir de zéro) nous avons les produits \( p_ix_j\) et \( q_iy_j\) avec \( i+j=k\). La colonne de droite représente donc bien les coefficients du polynôme \( xP+yQ\).


\begin{proposition} \label{PropAPxzcUl}
    Le résultant de deux polynômes est non nul si et seulement si les deux polynômes sont premiers entre eux.
\end{proposition}
\index{déterminant!résultant}

Un polynôme \( P\) a une racine double en \( a\) si et seulement si \( P\) et \( P'\) ont \( a\) comme racine commune, ce qui revient à dire que \( P\) et \( P'\) ne sont pas premiers entre eux.

Une application importante de ces résultats sera le théorème de Rothstein-Trager~\ref{ThoXJFatfu} sur l'intégration de fractions rationnelles.

\begin{example}
    Si nous prenons \( P=aX^2+bX+c\) et \( P'=2aX+b\) alors la taille de la matrice de Sylvester sera \( 2+1=3\) et
    \begin{equation}
        S_{P,P'}=\begin{pmatrix}
              a  &   b    &   c    \\
            2a    &   b    &   0    \\
            0    &   2a    &   b
        \end{pmatrix}.
    \end{equation}
    Le résultant est alors
    \begin{equation}
        \res(P,P')=-a(b^2-4ac).
    \end{equation}
    Donc un polynôme du second degré a une racine double si et seulement si \( b^2-4ac=0\). Cela est un résultat connu depuis longtemps mais qui fait toujours plaisir à revoir.
\end{example}

La matrice de Sylvester permet aussi de récrire l'équation de Bézout pour les polynômes; voir le théorème~\ref{ThoBezoutOuGmLB} et la discussion qui s'ensuit.

Une proposition importante du résultant est qu'il peut s'exprimer à l'aide des racines des polynômes.
\begin{proposition} \label{PropNDBOGNx}
    Si
    \begin{subequations}
        \begin{align}
        P(X)&=a_p\prod_{i=1}^p(X-\alpha_i)\\
        Q(X)&=b_q\prod_{j=1}^q(X-\beta_i)
        \end{align}
    \end{subequations}
    alors nous avons les expressions suivantes pour le résultant :
    \begin{equation}        \label{EqCFUumjx}
        \res(P,Q)=a_p^qb_q^p\prod_{i=1}^p\prod_{j=1}^q(\beta_j-\alpha_i)=b_q^p\prod_{j=1}^qP(\beta_j)=(-1)^{pq}a_p^q\prod_{i=1}^pQ(\alpha_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Si \( P\) et \( Q\) ne sont pas premiers entre eux, d'une part la proposition~\ref{PropAPxzcUl} nous dit que \( \res(P,Q)=0\) et d'autre part, \( P\) et \( Q\) ont un facteur irréductible en commun, ce qui  signifie que nous devons avoir un des \( X-\alpha_i\) égal à un des \( X-\beta_j\). Autrement dit, nous avons \( \alpha_i=\beta_j\) pour un couple \( (i,j)\). Par conséquent tous les membres de l'équation \eqref{EqCFUumjx} sont nuls.

    Nous supposons donc que \( P\) et \( Q\) sont premiers entre eux. Nous commençons par supposer que les polynômes \( P\) et \( Q\) sont unitaires, c'est-à-dire que \( a_p=b_q=1\). Nous considérons alors l'anneau
    \begin{equation}
        \eA=\eZ[\alpha_1,\ldots, \alpha_p,\beta_1,\ldots, \beta_q].
    \end{equation}
    Dans cet anneau, l'élément \( \beta_j-\alpha_i\) est irréductible (tout comme \( X-Y\) est irréductible dans \( \eZ[X,Y]\)). Le résultant \( R=\res(P,Q)\) est un élément de \( \eA\) parce que tous leurs coefficients peuvent être exprimés à l'aide des \( \alpha_i\) et des \( \beta_j\). Dans \( \eA\), l'élément \( \beta_j-\alpha_i\) divise \( R\). En effet lorsque \( \beta_j=\alpha_i\), le déterminant définissant le résultant est nul, ce qui signifie que \( \beta_j-\alpha_i\) est un facteur irréductible de \( R\).

    Par conséquent il existe un polynôme \( T\in \eA\) tel que
    \begin{equation}
        R=\lambda(\alpha_1,\ldots, \beta_q)\prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i).
    \end{equation}
    Comptons les degrés. Pour donner une idée de ce calcul de degré, voici comment se présente, au niveau des dimensions, le déterminant :
    \begin{equation}  \label{EqJCaATOH}
    \xymatrix{%
        \ar@{<->}[rrr]^{p+1}&&&& \ar@{<->}[r]^{q-1}  &\\
        a_p\ar@{.}[rrd] &a_{p-1}\ar@{.}[rr]  &  & a_0\ar@{.}[rrd] & 0\ar@{.}[r]&0&\ar@{<->}[d]^q \\
        0\ar@{.}[r]&0&a_p\ar@{.}[rr]&&a_1&a_0&\\
        \ar@{<->}[rrrrr]_{p+q}&&&&&&
       }
    \end{equation}
    si les \( a_i\) sont les coefficients de \( P\). Mais chacun des \( a_i\) est de degré \( 1\) en les \( \alpha_i\), donc le déterminant dans son ensemble est de degré \( q\) en les \( \alpha_i\), parce que \( R\) contient \( q\) lignes telles que \eqref{EqJCaATOH}. Le même raisonnement montre que \( R\) est de degré \( p\) en les \( \beta_j\). Par ailleurs le polynôme \( \prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i)\) est de degré \( p\) en les \( \beta_j\) et \( q\) en les \( \alpha_i\). Nous en déduisons que \( T\) doit être un polynôme ne dépendant pas de \( \alpha_i\) ou de \( \beta_j\).

    Nous pouvons donc calculer la valeur de \( T\) en choisissant un cas particulier. Avec \( P(X)=X^p\) et \( Q(X)=X^q+1\), il est vite vu que \( R(P,Q)=1\) et donc que \( T=1\).

    Si les polynômes \( P\) et \( Q\) ne sont pas unitaires, le lemme~\ref{LemBFrhgnA} nous permet de conclure.

\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Kronecker}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons \( K_n\) l'ensemble des polynômes de \( \eZ[X]\)
\begin{enumerate}
    \item
        unitaires de degré \( n\),
    \item
        dont les racines dans \( \eC\) sont de modules plus petits ou égaux à \( 1\),
    \item
        et qui ne sont pas divisés par \( X\).
\end{enumerate}
Un tel polynôme s'écrit sous la forme
\begin{equation}
    P=X^n+\sum_{k=0}^{n-1}a_kX^k.
\end{equation}

\begin{theorem}[Kronecker\cite{KXjFWKA}]    \label{ThoOWMNAVp}
    Les racines des éléments de \( K_n\) sont des racines de l'unité.
\end{theorem}
\index{théorème!Kronecker}
\index{polynôme!à plusieurs indéterminées}
\index{résultant!utilisation}
\index{polynôme!symétrique}

\begin{proof}
    Vu que \( \eC\) est algébriquement clos
    nous pouvons considérer les racines \( \alpha_1,\ldots, \alpha_n\) de \( P\) dans \( \eC\). Nous les considérons avec leurs multiplicités.
%TODO : lorsqu'on aura démontré que \eC est algébriquement clos, il faudra le référentier ici.

    Soit \( R=X^n+\sum_{k=0}^{n-1}b_kX^k\) un élément de \( K_n\) dont nous notons \( \beta_1,\ldots, \beta_n\) les racines dans \( \eC\). Les relations coefficients-racines stipulent que
    \begin{equation}
        b_k=\sum_{1\leq i_1<\ldots <i_{n-k}\leq n}\prod_{j=1}^{n-k}\beta_{i_j}.
    \end{equation}
    En prenant le module et en se souvenant que \( | \beta_{l} |\leq 1\) pour tout \( l\), nous trouvons que
    \begin{equation}
        | b_k |\leq\binom{ n }{ n-k }.
    \end{equation}
    Mais comme \( b_k\in \eZ\), nous avons
    \begin{equation}
        b_k\in\big\{    -\binom{ n }{ n-k },-\binom{ n }{ n-k }+1,\ldots, 0,\cdots,\binom{ n }{ n-k }   \big\}
    \end{equation}
    qui est de cardinal \( \binom{ n }{ n-k }+1\). Nous avons donc
    \begin{equation}
        \Card(K_n)\leq\prod_{k=0}^{n-1}\big( 1+\binom{ n }{ n-k } \big)<\infty.
    \end{equation}
    La conclusion jusqu'ici est que \( K_n\) est un ensemble fini.

    Pour chaque \( k\in \eN^*\) nous considérons les polynômes
    \begin{subequations}
        \begin{align}
            P_k&=\prod_{i=1}^n(X-\alpha_i^k)\\
            Q_k&=X^k-Y\in \eZ[X,Y],
        \end{align}
    \end{subequations}
    et puis nous considérons le résultant \( R_k=\res_X(P,Q_k)\in \eZ[Y]\) :
    \begin{equation}
        R_k=\res_X(P,Q_k)=
        \begin{pmatrix}
            1&a_{n-1}&\cdots&a_0&0&\cdots&0&0&0\\
            0&1&a_{n-1}&\cdots&a_0&0&\cdots&0&0\\
            \vdots&\ddots&\ddots&\ddots&&\ddots&\\
            0&\cdots&0&1&a_{n-1}&\cdots&a_0&0&0\\
            0&\cdots&0&0&1&a_{n-1}&\cdots&a_0&0\\
            0&\cdots&0&0&0&1&a_{n-1}&\cdots&a_0\\
        \\
                    1&0&\ldots&0&-Y&0&\ldots&0&0\\
                    0&1&0&\ldots&0&-Y&0&\ldots&0\\
                &&\ddots&&&\ddots&\ddots\\
                    0&\cdots&0&1&0&\cdots&0&-Y&0\\
                    0&0&\cdots&0&1&0&\cdots&0&-Y
        \end{pmatrix}
    \end{equation}
    Cela est un polynôme en \( Y\) dont le terme de plus haut degré est \( (-1)^nY^n\). Les petites formules de la proposition~\ref{PropNDBOGNx} nous permettent d'exprimer \( R_k(Y)\) en termes des racines de \( P\) :
    \begin{equation}
        R_k(Y)=\prod_{i=1}^nQ_k(\alpha_i)=\prod_{i=1}^n(\alpha_i^k-Y)=(-1)^n\prod_{i=1}^n(Y-\alpha_i^k)=(-1)^nP_k(Y).
    \end{equation}
    Vu que \( P\in K_n\) nous savons que les \( \alpha_i\) ne sont pas tous nuls; donc \( P_k\in K_n\). Cependant nous avons vu que \( K_n\) est un ensemble fini; donc parmi les \( P_k\), il y a des doublons (et pas un peu)\quext{Ici dans \cite{KXjFWKA}, il déduit qu'on a un \( k\) tel que \( P_k=P_1=P\). Mois je vois pourquoi on a un \( k\) et un \( l\) tels que \( P_k=P_l\), mais pourquoi on peut en trouver un spécialement égal au premier ? Une réponse à cette question permettrait de solidement réduire la lourdeur de la suite de la preuve.}. Nous regardons même l'ensemble des \( P_{2^n}\) dans lequel nous pouvons en trouver deux les mêmes. Soit \( l>k\) tels que \( P_{2^k}=P_{2^l}\). Si \( \alpha\) est racine de \( P_{2^k}\), alors il est de la forme \( \alpha=\beta^{2^k}\) pour une certaine racines \( \beta\) de \( P\). Par conséquent
    \begin{equation}    \label{EqBEgJtzm}
        \alpha^{2^l/2^k}=\alpha^{2^{l-k}}
    \end{equation}
    est racine de \( P_{2^l}\). Notons que dans cette expression il n'y a pas de problèmes de définition d'exposant fractionnaire dans \( \eC\) parce que \( l>k\). Vu que \eqref{EqBEgJtzm} est racine de \( P_{2^l}\), il est aussi racine de \( P_{2^k}\). Donc
    \begin{equation}
        \big( \alpha^{2^{l-k}} \big)^{2^{l-k}}=\alpha^{2^{2(l-k)}}
    \end{equation}
    est racine de \( P_{2^l}\) et donc de \( P_{2^k}\). Au final nous savons que tous les nombres de la forme \( \alpha^{2^{n(l-k)}}\) sont racines de \( P_{2^k}\). Mais comme \( P_{2^k}\) a un nombre fini de racines, nous pouvons en trouver deux égales. Si nous avons
    \begin{equation}
        \alpha^{2^{n(l-k)}}=\alpha^{2^{m(l-k)}}
    \end{equation}
    pour certains entiers \( m>n\), alors
    \begin{equation}
        \alpha^{2^{n(l-k)}-2^{m(l-k)}}=1,
    \end{equation}
    ce qui prouver que \( \alpha\) est une racine de l'unité. Nous avons donc prouvé que toutes les racines de \( P_{2^k}\) sont des racines de l'unité et donc que les racines de \( P\) sont racines de l'unité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trouver la matrice d'une symétrie donnée}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMtrSym}

Les notions de déterminants, produit scalaire et vectoriels\footnote{Définitions~\ref{LEMooQTRVooAKzucd},~\ref{DefVJIeTFj} et~\ref{DEFooTNTNooRjhuJZ}.} donnent une bonne intuition géométrique des matrices. Nous pouvons alors chercher à trouver les matrices de quelques symétriques dans \( \eR^2\) ou \( \eR^3\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à un plan}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Comment trouver par exemple la matrice $A$ qui donne la symétrie autour du plan $z=0$ ? La définition d'une telle symétrie est que les vecteurs du plan $z=0$ ne bougent pas, tandis que les vecteurs perpendiculaires changent de signe. Ces informations vont permettre de trouver comment $A$ agit sur une base de $\eR^3$. En effet :
\begin{enumerate}

	\item
		Le vecteur $\begin{pmatrix}
			1	\\
			0	\\
			0
		\end{pmatrix}$ est dans le plan $z=0$, donc il ne bouge pas,

	\item
		le vecteur $\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix}$ est également dans le plan, donc il ne bouge pas non plus,

	\item
		et le vecteur $\begin{pmatrix}
			0	\\
			0	\\
			1
		\end{pmatrix}$ est perpendiculaire au plan $z=0$, donc il va changer de signe.

\end{enumerate}
Cela nous donne directement les valeurs de $A$ sur la base canonique et nous permet d'écrire
\begin{equation}
	A=\begin{pmatrix}
		1	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	0	&	-1
	\end{pmatrix}.
\end{equation}
Pour écrire cela, nous avons juste mit en colonne les images des vecteurs de base. Les deux premiers n'ont pas changé et le troisième a changé.

Et si maintenant on donne un plan moins facile que $z=0$ ? Le principe reste le même : il faudra trouver deux vecteurs qui sont dans le plan (et dire qu'ils ne bougent pas), et puis un vecteur qui est perpendiculaire au plan\footnote{Pour le trouver, penser au produit vectoriel.}, et dire qu'il change de signe.

Voyons ce qu'il en est pour le plan $x=-z$. Il faut trouver deux vecteurs linéairement indépendants dans ce plan. Prenons par exemple
\begin{equation}		\label{EqffudE}
	\begin{aligned}[]
		f_1&=\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix},&f_2&=\begin{pmatrix}
			1	\\
			0	\\
			-1
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons
\begin{equation}
	\begin{aligned}[]
		Af_1&=f_1\\
		Af_2&=f_2.
	\end{aligned}
\end{equation}
Afin de trouver un vecteur perpendiculaire au plan, calculons le produit vectoriel :
\begin{equation}
	f_3=f_1\times f_2=\begin{vmatrix}
		e_1	&	e_2	&	e_3	\\
		0	&	1	&	0	\\
		1	&	0	&	-1
	\end{vmatrix}=-e_1-e_3=\begin{pmatrix}
		-1	\\
		0	\\
		-1
	\end{pmatrix}.
\end{equation}
Nous avons
\begin{equation}
	Af_3=-f_3.
\end{equation}
Afin de trouver la matrice $A$, il faut trouver $Ae_1$, $Ae_2$ et $Ae_3$. Pour ce faire, il faut d'abord écrire $\{ e_1,e_2,e_3 \}$ en fonction de $\{ f_1,f_2,f_3 \}$. La première des équations \eqref{EqffudE} dit que
\begin{equation}
	f_1=e_2.
\end{equation}
Ensuite, nous avons
\begin{equation}
	\begin{aligned}[]
		f_2&=e_1-e_3\\
		f_3&=-e_1-e_3.
	\end{aligned}
\end{equation}
La somme de ces deux équations donne $-2e_3=f_2+f_3$, c'est-à-dire
\begin{equation}
	e_3=-\frac{ f_2+f_3 }{ 2 }
\end{equation}
Et enfin, nous avons
\begin{equation}
	e_1=\frac{ f_2-f_3 }{ 2 }.
\end{equation}

Maintenant nous pouvons calculer les images de $e_1$, $e_2$ et $e_3$ en faisant
\begin{equation}
	\begin{aligned}[]
		Ae_1&=\frac{ Af_2-Af_3 }{ 2 }=\frac{1 }{2}\begin{pmatrix}
			0	\\
			0	\\
			-2
		\end{pmatrix}=\begin{pmatrix}
			0	\\
			0	\\
			-1
		\end{pmatrix},\\
		Ae_2&=Af_1=f_1=\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix},\\
		Ae_3&=-\frac{ f_2-f_3 }{ 2 }=-\frac{ 1 }{2}\begin{pmatrix}
			2	\\
			0	\\
			0
		\end{pmatrix}=\begin{pmatrix}
			-1	\\
			0	\\
			0
		\end{pmatrix}.
	\end{aligned}
\end{equation}
La matrice $A$ s'écrit maintenant en mettant les trois images trouvées en colonnes :
\begin{equation}
	A=\begin{pmatrix}
		0	&	0	&	-1	\\
		0	&	1	&	0	\\
		-1	&	0	&	0
	\end{pmatrix}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à une droite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le principe est exactement le même : il faut trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Ensuite il faudra exprimer $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$ et $f_3$.

Le seul problème est de trouver les trois vecteurs $f_i$. Le premier est tout trouvé : c'est n'importe quel vecteur sur la droite. Pour les deux autres, il faut un peu ruser parce qu'il faut impérativement qu'ils soient perpendiculaire à la droite. Pour trouver $f_2$, on peut écrire
\begin{equation}
	f_2=\begin{pmatrix}
		1	\\
		0	\\
		x
	\end{pmatrix},
\end{equation}
et puis fixer le $x$ pour que le produit scalaire de $f_2$ avec $f_1$ soit nul. S'il n'y a pas moyen (genre si $f_1$ a sa troisième composante nulle), essayer avec $\begin{pmatrix}
	x	\\
	1	\\
	0
\end{pmatrix}$. Une fois que $f_2$ est trouvé (il y a des milliards de choix possibles), trouver $f_3$ est super facile : prendre le produit vectoriel entre $f_1$ et $f_2$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{En résumé}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
La marche à suivre est

\begin{enumerate}

	\item
		Trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Typiquement : des vecteurs qui sont sur l'axe ou le plan de symétrie, et puis des perpendiculaires. Pour la perpendiculaire, penser au produit scalaire et au produit vectoriel.

	\item
		Exprimer la base canonique $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$, $f_3$.

	\item
		Trouver $Ae_1$, $Ae_2$ et $Ae_3$ en utilisant leur expression en termes des $f_i$, et le fait que l'on connaisse l'action de $A$ sur les $f_i$.

	\item
		La matrice s'obtient en mettant les images des $e_i$ en colonnes.
\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Approximation}
%---------------------------------------------------------------------------------------------------------------------------

Le lemme suivant est surtout intéressant en dimension infinie.
\begin{lemma}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); il existe une suite \( (v_n)\) dans \( A\) telle que \( v_n\stackrel{V}{\longrightarrow}v\) et \( \| v_n \|\leq \| v \|\) pour tout \( n\).
\end{lemma}

\begin{proof}
    Vu que \( A\) est dense, il existe une suite \( a_n\) dans \( A\) telle que \( a_n\to v\). Ensuite il suffit de poser
    \begin{equation}
        v_n=\frac{ n }{ n+1 }\frac{ \| v \| }{ \| a_n \| }a_n.
    \end{equation}
    Par construction nous avons toujours
    \begin{equation}
        \| v_n \|=\frac{ n }{ n+1 }\| v \|\leq \| v \|.
    \end{equation}
    Et de plus, la norme étant continue\footnote{Où dans le calcul suivant nous utilisons la continuité de la norme ? Posez-vous la question.},
    \begin{equation}
        \lim_{n\to \infty} v_n=\lim_{n\to \infty} \frac{ n }{ n+1 }\lim_{n\to \infty} \frac{ \| v \| }{ \| v_n \| }\lim_{n\to \infty} v_n=v.
    \end{equation}

    Le fait que \( v_n\) soit dans \( A\) est dû au fait que \( A\) soit vectoriel.
\end{proof}

\begin{proposition}     \label{PROPooVEMGooYKhMFy}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); pour tout \( a\in \eR\) nous avons
    \begin{equation}
        \sup\{ | v\cdot a |\tq a\in A\text{ et }\| a \|\leq \lambda \}=\lambda\| v \|.
    \end{equation}
\end{proposition}

\begin{proof}
    D'abord pour tout \( a\in A\) vérifiant \( \| a \|\leq \lambda\) l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG} donne
    \begin{equation}
        | v\cdot a |\leq \| v \|\| a \|\leq \lambda\| v \|.
    \end{equation}
    Donc le supremum dont on parle est majoré par \( \lambda\| v \|\).

    Il nous faut l'inégalité dans l'autre sens. Par densité nous pouvons choisir une suite \( v_n\in A\) tel que \( v_n\to v\). Ensuite nous posons
    \begin{equation}
        a_n=\frac{ \lambda }{ \| v_n \| }v_n.
    \end{equation}
    Nous avons \( \| a_n \|=\lambda\) pour tout \( n\) et
    \begin{equation}
        | v\cdot a_n |=\frac{ \lambda }{ \| v_n \| }| v\cdot v_n |,
    \end{equation}
    et en passant à la limite,
    \begin{equation}
        \lim_{n\to \infty} | v\cdot a_n |=\frac{ \lambda }{ \| v \| }\| v\cdot v \|=\lambda\| v \|.
    \end{equation}
    Donc l'ensemble sur lequel nous prenons le supremum contient une suite convergente vers \( \lambda\| v \|\). Le supremum est donc au moins aussi grand que cela.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Hyperplans et formes linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}      \label{DEFooEWDTooQbUQws}
    Si \( E\) est un espace vectoriel de dimension \( n\), un \defe{hyperplan}{hyperplan} de \( E\) est un sous-espace vectoriel de dimension \( n-1\).
\end{definition}

\begin{proposition}[\cite{ooDYWYooBJkHuh}]      \label{PROPooVYJUooAWDQrZ}
    À propos d'hyperplans et de formes linéaires sur un espace vectoriel \( E\) sur le corps \( \eK\).
    \begin{enumerate}
        \item
            Si \( \varphi\) est une forme linéaire non nulle, alors \( \ker(\varphi)\) est un hyperplan.
        \item
            Si \( H\) est un hyperplan de \( E\), il existe une forme linéaire dont \( H\) est le noyau :
            \begin{equation}
                H=\ker(\varphi).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    En deux parties.
    \begin{enumerate}
        \item
            Soit un supplémentaire \( A\) de \( H\). Nous considérons la restriction \( \varphi_A\colon A\to \eK\). Vu que les éléments non nuls de \( A\) sont hors de \( H\), nous avons \( \varphi(x)\neq 0\) dès que \( x\) est non nul dans \( A\). Cela implique que \( \varphi_A\) est surjective.

            D'autre part, \( \varphi_A\) est également injective : si \( \varphi_A(x)=\varphi_A(y)\), alors \( \varphi_A(x-y)=0\), ce qui signifie que \( x-y=0\) ou encore que \( x=y\).

            Donc \( \varphi_A\) est un isomorphisme de \( \eK\)-espaces vectoriels; nous en déduisons par le corollaire \ref{CORooXIPKooWThOsr} que \( A\) est de dimension \( 1\) sur \( \eK\), parce que \( \eK\) est de dimension \( 1\).

        \item
            Nous utilisons le théorème de la base incomplète \ref{ThonmnWKs}\ref{ITEMooJIJSooGuJMdt} pour considérer une base \( \{ e_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \Span\{ e_1,\ldots, e_{n-1} \}=H\). Nous pouvons alors considérer la forme linéaire définie par
            \begin{equation}
                \varphi(e_i)=\begin{cases}
                    0    &   \text{si }  i=1,\ldots, n-1\\
                    1    &    \text{si } i=n.
                \end{cases}
            \end{equation}
            Cette forme vérifie \( \ker(\varphi)=H\).
    \end{enumerate}
\end{proof}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]
    Soit un espace vectoriel \( E\) de dimension finie \( n\geq 2\). Soit un sous-espace vectoriel \( V\) de \( E\) de dimension \( s\). Alors \( V\) est une intersection de \( n-s\) hyperplans de \( E\).
\end{proposition}

\begin{proof}
    Nous considérons une base de \( V\) que nous complétons\footnote{Théorème de la base incomplète, \ref{ThonmnWKs}\ref{ITEMooJIJSooGuJMdt}.} en une base de \( E\) : si \( x=\sum_{i=1}^nx_ie_i\), nous avons \( x\in V\) si et seulement si \( x_{s+1}=\ldots=x_n=0\). Nous considérons les formes linéaires
    \begin{equation}
        \begin{aligned}
            \varphi_i\colon E&\to \eR \\
            x&\mapsto x_i, 
        \end{aligned}
    \end{equation}
    et nous considérons les parties \( H_i=\ker(\varphi_i)\) qui sont de hyperplans par la proposition \ref{PROPooVYJUooAWDQrZ}. Les \( H_i\) avec \( s+1\leq i\leq n\) sont une famille de \( n-s\) hyperplans qui vérifient
    \begin{equation}
        V=\bigcap_{i=s+1}^n\ker(\varphi_i)
    \end{equation}
    parce que \( x\in \ker(\varphi_i)\) si et seulement si \( x_i=0\).

    Donc \( V\) peut être écrit comme intersection de \( n-s\) hyperplans de \( E\).
\end{proof}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]      \label{PROPooRCLNooJpIMMl}
    Soit un \( \eK\)-espace vectoriel \( E\) de dimension finie \( n\geq 2\). Si \( H_i\) sont des hyperplans de \( E\), alors
    \begin{equation}
        \dim\Big( \bigcup_{i=1}^mH_i \Big)\geq n-m.
    \end{equation}
\end{proposition}

\begin{proof}
    N'oubliez pas de prouver que \( \bigcap_{i=1}^mH_i\) est un espace vectoriel. À part ça, nous faisons une petite récurrence.
    \begin{subproof}
        \item[Pour \( m=2\)]
            Nous savons déjà par la proposition \ref{PROPooQCIXooHIyPPq} que
            \begin{equation}
                \dim(H_1\cap H_2)=\dim(H_1)+\dim(H_2)-\dim(H_1\cap H_2).
            \end{equation}
            De plus \( \dim(H_1+H_2)\leq n\). En remplaçant, par les valeurs,
            \begin{subequations}
                \begin{align}
                    \dim(H_1\cap H_2)&=\dim(H_1)+\dim(H_2)-\dim(H_1\cap H_2)\\
                    &=n-1+n-1-\dim(H_1+H_2)\\
                    &\geq 2n-2-n\\
                    &=n-2.
                \end{align}
            \end{subequations}
            Donc \( \dim(H_1\cap H_2)\geq n-2\).

        \item[La récurrence]
            Nous calculons \( \dim(H_1\cap\ldots\cap H_m\cap H_{m+1})\) en commençant encore par la proposition \ref{PROPooQCIXooHIyPPq} :
            \begin{subequations}
                \begin{align}
                    \dim(H_1\cap \ldots\cap H_m\cap H_{m+1})&=\underbrace{\dim(H_1\cap\ldots\cap H_m)}_{\leq n-m}+\dim(H_{m+1})\\
                        &\qquad -\underbrace{\dim\big( (H_1\cap\ldots H_m)+H_{m+1} \big)}_{\leq n}\\
                    &\geq n-m+(n-1)-n\\
                    &=n-m-1.
                \end{align}
            \end{subequations}
            C'est bon pour la récurrence.
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Hermitien, orthogonal, adjoint}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{propositionDef}[Définition de la transposée\cite{MonCerveau}]\label{DEFooROVNooFlTbSK}
    Soient deux espaces vectoriels euclidiens ou hermitiens \( E\) et \( F\) et une application linéaire \( A\colon E\to F\).
    \begin{enumerate}
        \item       \label{ITEMooRUZWooSZgGnf}
    Il existe une unique application linéaire \( B\colon F\to E\) telle que
    \begin{equation}        \label{EQooHWYKooFzAGgB}
        \langle Ax, y\rangle_F=\langle x, By\rangle_E
    \end{equation}
    pour tout \( x\in E\) et \( y\in F\).
\item   \label{ITEMooXXEUooPtfPKY}
 Si \( \{ e_i \}\) est une base orthonormée de \( E\) et \( \{ f_{\alpha} \}\) est une base orthonormée de \( F\), alors la matrice de \( A\) et \( B\) pour ces bases sont liées par
 \begin{equation}       \label{EQooUSNVooQtRNGL}
     B_{i\alpha}=A_{\alpha i}.
 \end{equation}
    \end{enumerate}
     L'application \( B\) ainsi définie set nommée \defe{adjoint}{adjoint} de \( A\) et sera notée \( B=A^*\). 
\end{propositionDef}

\begin{proof}
    Pour l'unicité, nous écrivons la condition avec \( x=e_j\) pour obtenir :
    \begin{equation}
        \langle Ae_j, y\rangle = \langle e_j, By\rangle =(By)_j
    \end{equation}
    c'est-à-dire que les coefficients \( B(y)_j\) de \( B(y)\) dans la base canonique sont fixés par la condition.

    Pour l'existence, il suffit de vérifier que poser
    \begin{equation}
        B(y)=\sum_j\langle Ae_j, y\rangle e_j
    \end{equation}
    fonctionne. Pour cela il faut utiliser la bilinéarité du produit scalaire et le fait que \( \langle x, e_j\rangle =x_j\). Nous avons :
    \begin{subequations}
        \begin{align}
            \langle x, B(y)\rangle &=\langle x, \sum_j\langle A(e_j), y\rangle e_j\rangle \\
            &=\sum_j\langle A(e_j), y\rangle \langle x, e_j\rangle \\
            &=\sum_j\langle A(x_je_j), y\rangle \\
            &=\langle A(x), y\rangle .
        \end{align}
    \end{subequations}

En ce qui concerne la matrice de l'application \( B\) ainsi définie, nous écrivons la condition \eqref{EQooHWYKooFzAGgB} avec \( y=e'_{\alpha}\) et \( x=e_i\), de telle sorte que
\begin{equation}
    A(x)=A(e_i)=\sum_{\beta}A_{\beta i}e'_{\beta}
\end{equation}
et
\begin{equation}
    B(y)=B(e'_{\alpha})=\sum_jB_{j\alpha}e_j.
\end{equation}
Alors nous avons :
\begin{equation}
    \sum_{\beta}A_{\beta i}\langle e'_{\beta}, e'_{\alpha}\rangle =\sum_j B_{j\alpha}\langle e_i, e_j\rangle ,
\end{equation}
donc
\begin{equation}
    A_{\alpha i}=B_{i \alpha}.
\end{equation}

\end{proof}

\begin{normaltext}
    À cause de l'expression \eqref{EQooUSNVooQtRNGL} pour la matrice de \( A^*\), cette application est souvent appelé \defe{transposé}{transposé} de \( A\) et noté \( A^t\). Nous savons, nous, que la transposée de \( A\) est une application \( A^t\colon F^*\to E^*\) donnée par la définition \ref{DefooZLPAooKTITdd}. Il nous arrivera donc d'écrire des égalités comme \( \langle Ax, y\rangle=\langle x, A^ty\rangle  \).
\end{normaltext}

\begin{proposition}     \label{PROPooSHZMooGwdfBd}
    En ce qui concerne le déterminant,
    \begin{equation}
        \det(A^*)=\det(A)^*
    \end{equation}
    où l'étoile à droite dénote la conjugaison complexe dans \( \eC\).
\end{proposition}

\begin{proof}
    Écrivons l'expression explicite \eqref{EQooOJEXooXUpwfZ} du déterminant. Le tout avec la base canonique :
    \begin{equation}
            \det(A)=\det_{(e_1,\ldots, e_n)}(Ae_1,\ldots, Ae_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne_{\sigma(i)}^*(Ae_i).
    \end{equation}
    Mais nous pouvons développer :
    \begin{equation}
        e^*_{\sigma(i)}(Ae_i)=\langle e_{\sigma(i)}, Ae_i\rangle =\langle A^*e_{\sigma(i)}, e_i\rangle =\langle e_i, A^*e_{\sigma(i)}\rangle^*=e_i^*(A^*e_{\sigma(i)})^*.
    \end{equation}
    Notez que dans la dernière expression, les trois \( {}^*\) ont trois significations différentes. Par conséquent,
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_i^*(A^*e_{\sigma(i)})^*.
    \end{equation}
    Mais \( e_i^*(A^*e_{\sigma(i)})=e_{\sigma(j)}^*(A^*e_{j})\) pour \( j=\sigma(i)\), donc le produit ne change pas si on déplace le \( \sigma\) :
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_{\sigma(i)}^*(A^*e_{i})^*=\det(A^*)^*.
    \end{equation}
    Nous avons donc \( \det(A)=\det(A^*)^*\), c'est-à-dire \( \det(A)^*=\det(A^*)\). Pour information, la dernière étoile est la conjugaison complexe.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooVPSYooRuoEFi}
    Si \( A\colon E_2\to E_3\) et \( B\colon E_1\to E_2\) sont des applications linéaires, alors
    \begin{equation}
        (AB)^*=B^*A^*
    \end{equation}
    où la «multiplication» est la composition.
\end{proposition}

\begin{proof}
    L'existence de \( (AB)^*\), de \( A^*\) et de \( B^*\) ne donne pas lieu à débat parce que la proposition \ref{DEFooROVNooFlTbSK} ne souffre pas de discussions. La propriété que \( (AB)^*\) est unique a avoir est que
    \begin{equation}
        \langle ABx, y\rangle =\langle x, (AB)^*y\rangle 
    \end{equation}
    pour tout \( x\in E_1\) et \( y\in E_3\). Or l'application \( B^*A^*\) possède également cette propriété parce que
    \begin{equation}
        \langle x, B^*A^*y\rangle =\langle Bx, A^*y\rangle =\langle ABx, y\rangle .
    \end{equation}
    La partie unicité de la proposition \ref{DEFooROVNooFlTbSK} nous impose donc d'accepter que les applications \( (AB)^*\) et \( B^*A^*\) sont en réalité les mêmes\footnote{Et ce même si vous croyez les avoir déjà vu ensemble dans la même pièce.}.
\end{proof}

\begin{normaltext}
    Un grand moment d'utilisation de la notion d'adjoint pour un opérateur non carré sera la définition d'une intégrale sur une variété; en particulier dans la proposition \ref{PROPooOAHWooAfxvyv}.
\end{normaltext}

\begin{definition}      \label{DEFooKEBHooWwCKRK}
    Un opérateur \( A\) est \defe{hermitien}{opérateur!hermitien} si \( A^*=A\). On dit aussi \defe{autoadjoint}{opérateur!autoadjoint}.
\end{definition}

\begin{normaltext}
    Le mot «hermitien» est réservé aux opérateurs sur des espaces hermitiens, c'est-à-dire des espaces vectoriels sur \( \eC\). Le mot «autoadjoint» par contre est plutôt utilisé dans le cadre d'opérateurs sur les espaces réels. En conséquence de quoi, ces deux mots sont synonymes, mais il est préférable d'utiliser «hermitien» lorsque l'espace vectoriel est sur \( \eC\) et «autoadjoint» lorsqu'il est sur \( \eR\).

    L'ensemble des opérateurs autoadjoints de \( E\) est noté \( \gS(E)\)\nomenclature[A]{\( \gS(E)\)}{Les opérateurs autoadjoints de $E$}. Cette notation provient du fait que dans \( \eR^n\) muni du produit scalaire usuel, les opérateurs autoadjoints sont les matrices symétriques.
\end{normaltext}

\begin{remark}
    Le fait d'être hermitien n'implique en rien le fait d'être inversible.
\end{remark}

\begin{lemma}
    Si \( E\) est un espace euclidien, un endomorphisme \( f\colon E\to E\) est autoadjoint si et seulement si pour tout \( x,y\in E\) nous avons \( \langle x, f(y)\rangle=\langle f(x), y\rangle  \).
\end{lemma}

\begin{proof}
    Dans le sens direct, nous avons
    \begin{equation}
        \langle f(x), y\rangle =\langle x, f^*(y)\rangle =\langle x, f(y)\rangle .
    \end{equation}
    La première égalité est la définition de \( f^*\) et la seconde est l'hypothèse \( f=f^*\).

    Dans l'autre sens, l'hypothèse est que l'endomorphisme \( f\) vérifie \( \langle x, f(y)\rangle =\langle f(x), y\rangle \). Mais la proposition \ref{DEFooROVNooFlTbSK}\ref{ITEMooRUZWooSZgGnf} spécifie que \( f^*\) est l'unique endomorphisme à satisfaire cette égalité. Donc \( f=f^*\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Opérateur orthogonal, matrice orthogonale}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYKCSooURQDoS}
    Un opérateur est \defe{orthogonal}{orthogonal!opérateur} lorsque \( A^*=A^{-1}\) où \( A^*\) est l'adjoint de \( A\) définit en~\ref{DEFooROVNooFlTbSK}.
\end{definition}

\begin{definition}      \label{DEFooUHANooLVBVID}
    Une matrice \( U\) est \defe{orthogonale}{matrice!orthogonale}\index{orthogonal!matrice} si \( U^t=U^{-1}\). Le \defe{groupe orthogonal}{groupe!orthogonal} noté \( \gO(n)\) est l'ensemble des matrices orthogonales \( n\times n\).
\end{definition}

\begin{lemma}       \label{LEMooSSALooSBFzJb}
    Soit un opérateur \( A\colon \eR^n\to \eR^n\) muni du produit scalaire usuel. Il est orthogonal si et seulement si sa matrice dans la base canonique est orthogonale\footnote{Définition~\ref{DEFooUHANooLVBVID}.}.
\end{lemma}

\begin{proof}
    Soit la base canonique \( \{ e_i \}_{i=1,\ldots, n}\) de \( \eR^n\). Nous avons
    \begin{equation}
        \langle AA^*e_i, e_j\rangle =\langle e_i, e_j\rangle =\delta_{ij},
    \end{equation}
    donc \( \big( (AA^*)e_i \big)_j=\delta_{ij}\), ou encore \( (AA^*)_{ij}=\delta_{ij}\), ce qui signifie que la matrice $AA^*$ est l'identité.
\end{proof}

\begin{proposition}[Thème~\ref{THMooVUCLooCrdbxm}]     \label{PropKBCXooOuEZcS}
    À propos de matrices orthogonales.
    \begin{enumerate}
        \item
            L'ensemble des matrices réelles orthogonales forme un groupe noté \( \gO(n,\eR)\)\nomenclature[B]{\( \gO(n,\eR)\)}{le groupe des matrices orthogonales}.
        \item
            Si \( A\) est une matrice orthogonale, alors \( \det(A)=\pm 1\).
        \item       \label{ITEMooOWMBooHUatNb}
            Le groupe \( \gO(n)\) est le groupe des isométries linéaires\footnote{Au sens où, parmi les applications linéaires, les isométries sont les éléments de \( \gO(n)\). À part ça, il y a aussi les translations, mais c'est une autre histoire qui vous sera contée une autre fois.} de \( \eR^n\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Si \( A\) et \( B\) sont orthogonales, alors
    \begin{equation}
        (AB)(AB)^t=ABB^tA^t=A\mtu A^t=\mtu.
    \end{equation}
    Vu que \( \mtu\) est orthogonale, nous avons bien un groupe.

    En ce qui concerne le déterminant, \( AA^t=\mtu\) donne \( \det(A)\det(A^t)=1\), mais la proposition~\ref{PROPooSHZMooGwdfBd} dit que \( \det(A)=\det(A^t)\), donc \( \det(A)^2=1\). D'où le fait que \( \det(A)=\pm 1\).

    D'autre part si \( A\) est une isométrie de \( \eR^n\) alors pour tout \( x,y\in \eR^n\) nous avons \( \langle Ax, Ay\rangle =\langle x, y\rangle \). En particulier,
    \begin{equation}
        \langle A^tAx, y\rangle =\langle x, y\rangle
    \end{equation}
    pour tout \( x,y\in \eR^n\). En prenant \( y=e_i\) nous trouvons
    \begin{equation}
        (A^tAx)_i=x_i,
    \end{equation}
    ce qui signifie que pour tout \( x\), \( A^tAx=x\), ou encore que \( A^tA\) est l'identité.

    Réciproquement si \( A^tA\) est l'identité nous avons
    \begin{equation}
        \langle x, y\rangle =\langle A^tAx, y\rangle =\langle Ax, Ay\rangle ,
    \end{equation}
    ce qui prouve que \( A\) est une isométrie.
\end{proof}

En ce qui concerne les valeurs propres des matrices de \( \gO(n)\) ainsi que leurs formes canoniques (avec des fonctions trigonométriques) pour \( \gO(3)\) et \( \SO(3)\), ce sera pour la proposition~\ref{PROPooVEJGooWnqtMm} et ce qui s'ensuit.

\begin{definition}      \label{DEFooJLNQooBKTYUy}
    Le sous-groupe des matrices orthogonales de déterminant \( 1\) est le groupe \defe{spécial orthogonal}{groupe!spécial orthogonal} noté \( \SO(n)\).
\end{definition}

