% This is part of Le Frido
% Copyright (c) 2006-2022
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Uniforme continuité}		\label{SecUnifContinue}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
	Une partie $A\subset\eR^m$ est dite \defe{bornée}{bornée!partie de $\eR^m$} si il existe un $M>0$ tel que $A\subset B(0,M)$. Le \defe{diamètre}{diamètre} de la partie $A$ est\nomenclature[T]{$\Diam(A)$}{Diamètre de la partie $A$} le nombre
	\begin{equation}
		\Diam(A)=\sup_{x,y\in A}\| x-y \|\in\mathopen[ 0 , \infty \mathclose].
	\end{equation}
\end{definition}
Lorsque $A$ est borné, il existe un $M$ tel que $\| x \|\leq M$ pour tout $x\in A$.

\begin{lemma}
	Si $A$ est une partie non vide de $\eR^m$, alors $\Diam(A)=\Diam(\bar A)$.
\end{lemma}
Nous n'allons pas donner de démonstrations de ce lemme.


Si $(x_n)$ est une suite et $I$ est un sous-ensemble infini de $\eN$, nous désignons par $x_I$ la suite des éléments $x_n$ tels que $n\in I$. Par exemple la suite $x_{\eN}$ est la suite elle-même, la suite $x_{2\eN}$ est la suite obtenue en ne prenant que les éléments d'indice pair.

Les suites $x_I$ ainsi construites sont dites des \defe{sous-suites}{sous-suite} de la suite $(x_n)$.


Pour une fonction $f\colon D\subset\eR^m\to \eR$, la continuité au point $a$ signifie que pour tout $\varepsilon>0$,
\begin{equation}
	\exists\delta>0\tq 0<\| x-a \|<\delta\Rightarrow | f(x)-f(a) |<\varepsilon.
\end{equation}
Le $\delta$ qu'il faut choisir dépend évidemment de $\varepsilon$, mais il dépend en général aussi du point $a$ où l'on veut tester la continuité. C'est-à-dire que, étant donné un $\varepsilon>0$, nous pouvons trouver un $\delta$ qui fonctionne pour certains points, mais qui ne fonctionne pas pour d'autres points.

Il peut cependant également arriver qu'un même $\delta$ fonctionne pour tous les points du domaine. Dans ce cas, nous disons que la fonction est uniformément continue sur le domaine.

\begin{definition}
	Une fonction $f\colon D\subset\eR^m\to \eR$ est dite \defe{uniformément continue}{continue!uniformément} sur $D$ si
	\begin{equation}	\label{EqConditionUnifCont}
		\forall\varepsilon>0,\,\exists\delta>0\tq\,\forall x,y\in D,\,\| x-y \|\leq\delta \Rightarrow| f(x)-f(a) |<\varepsilon.
	\end{equation}
\end{definition}

Il est intéressant de voir ce que signifie le fait de \emph{ne pas} être uniformément continue sur un domaine $D$. Il s'agit essentiellement de retourner tous les quantificateurs de la condition \eqref{EqConditionUnifCont} :
\begin{equation}	\label{EqConditionPasUnifCont}
	\exists\varepsilon>0\tq\forall\delta>0,\,\exists x,y\in D\tq \| x-y \|<\delta\text{ et }\big| f(x)-f(y) \big|>\varepsilon.
\end{equation}
Dans cette condition, les points $x$ et $y$ peuvent être fonction du $\delta$. L'important est que pour tout $\delta$, on puisse trouver deux points $\delta$-proches dont les images par $f$ ne soient pas $\varepsilon$-proches.

\begin{example}
	Prenons la fonction $f(x)=\frac{1}{ x }$, et demandons nous pour quel $\delta$ nous sommes sûr d'avoir
	\begin{equation}
		| f(a+\delta)-f(a) |=\left| \frac{1}{ a+\delta }-\frac{1}{ a } \right| <\varepsilon.
	\end{equation}
	Pour simplifier, nous supposons que $a>0$. Nous calculons
	\begin{equation}
		\begin{aligned}[]
			\frac{ 1 }{ a }-\frac{1}{ a+\delta } & <	\varepsilon                                  \\
			\frac{ \delta }{ a(a+\delta) }       & <\varepsilon                                  \\
			\delta                               & <\varepsilon a^2+\varepsilon a\delta          \\
			\delta(1-\varepsilon a)              & <\varepsilon a^2                              \\
			\delta                               & <\frac{ \varepsilon a^2 }{ 1-\varepsilon a }.
		\end{aligned}
	\end{equation}
	Notons que, à $\varepsilon$ fixé, plus $a$ est petit, plus il faut choisir $\delta$ petit. La fonction $x\mapsto\frac{1}{ x }$ n'est donc pas uniformément continue. Cela correspond au fait que, proche de zéro, la fonction monte très vite. Une fonction uniformément continue sera une fonction qui ne montera jamais très vite.
\end{example}

\begin{proposition}     \label{PROPooVOUTooOtiGLG}
	Quelques propriétés des fonctions uniformément continues.
	\begin{enumerate}
		\item
		      Toute application uniformément continue est continue;
		\item
		      la composée de deux fonctions uniformément continues est uniformément continue;
	\end{enumerate}
\end{proposition}
Nous verrons qu'une application lipschitzienne est uniformément continue (proposition~\ref{PROPooVZSAooUneOQK}).

Une fonction peut être uniformément continue sur un domaine et pas sur un autre. Le théorème suivant donne une importante indication à ce sujet.
\begin{theorem}[Heine]\index{théorème!Heine}\index{Heine (théorème)}		\label{ThoHeineContinueCompact}
	Soit \( K\) un compact de \( \eR^n\). Une fonction continue \( f\colon \eR^n\to \eR^m\) est uniformément continue sur \( K\).
\end{theorem}

La démonstration qui suit est valable pour une fonction \( f\colon \eR^n\to \eR^m\) et utilise le fait que le produit cartésien de compacts est compact. Dans le cas de fonctions sur \( \eR\), nous pouvons modifier la démonstration pour ne pas utiliser ce résultat; voir plus bas.
%TODO : trouver où se trouve la preuve du produit de compacts et la référentier ici.
\begin{proof}
	Nous allons prouver ce théorème par l'absurde. Nous commençons par écrire la condition \eqref{EqConditionPasUnifCont} qui exprime que $f$ n'est pas uniformément continue sur le compact \( K\) :
	\begin{equation}
		\exists\varepsilon>0\tq\forall\delta>0,\,\exists x,y\in K\tqs \| x-y \|<\delta\text{ et }\big| f(x)-f(y) \big|>\varepsilon.
	\end{equation}
	En particulier (en prenant $\delta=\frac{1}{ n }$ pour tout $n$), pour chaque $n$ nous pouvons trouver $x_n$ et $y_n$ dans $K$ qui vérifient simultanément les deux conditions suivantes :
	\begin{subequations}
		\begin{numcases}{}
			\| x_n-y_n \|<\frac{1}{ n }\\
			\big| f(x_n)-f(y_n) \big|>\varepsilon.	\label{EqCond3107fxfyepsppt}
		\end{numcases}
	\end{subequations}
	Nous insistons que c'est le même $\varepsilon$ pour chaque $n$. L'ensemble $K$ étant compact, l'ensemble \( K\times K \) est compact (théorème~\ref{THOIYmxXuu}) et nous pouvons trouver une sous-suite convergente \emph{du couple} \( (x_n,y_n)\) dans \( K\times K\). Quitte à passer à ces sous-suites, nous  nous supposons que \( (x_n,y_n)\) converge dans \( K\times K\) et en particulier que les suites $(x_n)$ et $(y_n)$ sont convergentes. Étant donné que pour chaque $n$ elles vérifient $\| x_n-y_n \|<\frac{1}{ n }$, les limites sont égales :
	\begin{equation}
		\lim x_n=\lim y_n=x.
	\end{equation}
	L'ensemble $K$ étant fermé, la limite $x$ est dans $K$. Par continuité de $f$, nous avons finalement
	\begin{equation}
		\lim f(x_n)=\lim f(y_n)=f(x),
	\end{equation}
	mais alors
	\begin{equation}
		\lim_{n\to\infty}\big| f(x_n)-f(y_n) \big|=0,
	\end{equation}
	ce qui est en contradiction avec le choix \eqref{EqCond3107fxfyepsppt}.

	Tout ceci prouve que $f(K)$ est bornée supérieurement et que $f$ atteint son supremum (qui est donc un maximum). Le fait que $f(K)$ soit borné inférieurement se prouve en considérant la fonction $-f$ au lieu de $f$.

\end{proof}

\begin{remark}
	Nous pouvons ne pas utiliser le fait que le produit de compacts est compact. Cela est particulièrement commode lorsqu'on considère des fonctions de \( \eR\) dans \( \eR\) parce que dans ce cadre nous ne pouvons pas supposer connue la notion de produit d'espace topologiques.

	Pour choisir les sous-suites \( (x_n)\) et \( (y_n)\), il suffit de prendre une sous-suite convergente de \( (x_n)\) et d'invoquer le fait que \( \| x_n-y_n \|\leq \frac{1}{ n }\). Les suites \( (x_n)\) et \( (y_n)\) étant adjacentes\footnote{Définition \ref{DEFooDMZLooDtNPmu}.}, la convergence de \( (x_n)\) implique la convergence de \( (y_n)\) vers la même limite.

	Il est donc un peu superflus de parler de la convergence du couple \( (x_n,y_n)\).
\end{remark}

\begin{proposition}[Heine\cite{ooNDDIooKLdIWH}]     \label{PROPooBWUFooYhMlDp}
	Toute application continue d'un espace métrique compact dans un espace métrique quelconque est uniformément continue\footnote{Uniforme continuité, définition \ref{DEFooYIPXooQTscbG}.}.
\end{proposition}

\begin{proof}
	Soient un espace métrique compact \( X\) et un métrique quelconque \( E\). Nous considérons une application continue \( f\colon X\to Y\).

	\begin{subproof}
		\item[Un ensemble]
		Soit \( \epsilon>0\). Nous considérons l'ensemble
		\begin{equation}
			K=\{ (x,y)\in X\times X\tq d\big( f(x), f(y) \big)\geq \epsilon \}.
		\end{equation}
		\item[Il est compact]
		L'espace \( X\) étant compact, \( X\times X\) est également compact par le théorème \ref{THOIYmxXuu}. Les fonction \( f\) et \( d\) étant continues, l'application
		\begin{equation}
			\begin{aligned}
				\varphi\colon X\times X & \to \eR^+                       \\
				(x,y)                   & \mapsto d\big( f(x), f(y) \big)
			\end{aligned}
		\end{equation}
		est continue, de telle sorte que la partie \( \varphi\geq \epsilon\) est fermée. Un fermé dans un compact est compact par le lemme \ref{LemnAeACf}.
		\item[Une borne atteinte]
		Nous considérons l'application distance \( d\colon K\to \eR^+\). C'est une application continue sur le compact \( K\); donc elle atteint ses bornes (théorème des bornes atteintes, \ref{ThoMKKooAbHaro}). Elle a un minimum que nous notons \( \delta\).

		Vu que \( (x,x)\notin K\), nous avons \( d(x,y)>0\) pour tout \( (x,y)\in K\). Et donc \( \delta>0\).
		\item[Conclusion]
		Si \( x,y\in X\) sont tels que \( d(x,y)<\delta\), alors \( (x,y)\notin K\). De ce fait nous avons
		\begin{equation}
			d\big( f(x), f(y) \big)<\epsilon.
		\end{equation}
		D'où l'uniforme continuité de \( f\) sur \( X\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]            \label{LEMooIVAKooUiEENr}
	Soit une fonction continue \( f\colon \mathopen[ a , b \mathclose]\times \mathopen[ c , d \mathclose]\to \eR\). Pour chaque \( x\in \mathopen[ a , b \mathclose]\) nous définissons
	\begin{equation}
		\begin{aligned}
			f_x\colon \mathopen[ c , d \mathclose] & \to \eR         \\
			y                                      & \mapsto f(x,y).
		\end{aligned}
	\end{equation}
	Alors l'application
	\begin{equation}
		\begin{aligned}
			g\colon \mathopen[ a , b \mathclose] & \to \eR                    \\
			x                                    & \mapsto \| f_x \|_{\infty}
		\end{aligned}
	\end{equation}
	est continue.
\end{lemma}

\begin{proof}
	Soit \( \alpha\in\mathopen[ a , b \mathclose]\), et prouvons la continuité de \( g\) en \( \alpha\).
	\begin{subproof}
		\item[Le décor]
		Nous considérons l'espace vectoriel normé \( \big( C^0(\mathopen[ a , b \mathclose]),\| . \|_{\infty} \big)\) des fonctions continues sur \( \mathopen[ a , b \mathclose]\) muni de la norme uniforme. Prouvons que si \( x_k\stackrel{\mathopen[ a , b \mathclose]}{\longrightarrow}\alpha\), alors \( f_{x_k}\stackrel{unif}{\longrightarrow}f_{\alpha}\).

		\item[Module de continuité]
		Pour cela nous avons le calcul suivant, avec justifications juste en-dessous :
		\begin{subequations}
			\begin{align}
				\| f_{x_k}-f_{\alpha} \|_{\infty} & =\sup_{y\in\mathopen[ c , d \mathclose]}\| f(x_k,y)-f(\alpha,y) \|                                                                 \\
				                                  & \leq\sup_{y\in\mathopen[ c , d \mathclose]}\big| \omega_f\big( \| (x_k,y)-(\alpha,y) \| \big) \big|    \label{SUBEQooCOJQooWlvHUa} \\
				                                  & =\sup_{y\in\mathopen[ c , d \mathclose]}| \omega_f(| x_k-\alpha |) |       \label{SUBEQooGLYMooEZKRKm}                             \\
				                                  & =\omega_f(| x_k-\alpha |).
			\end{align}
		\end{subequations}
		Justifications.
		\begin{itemize}
			\item Pour \eqref{SUBEQooCOJQooWlvHUa}. Utilisation du module de continuité, définition \ref{DEFooYARJooYyzMMP}.
			\item Pour \eqref{SUBEQooGLYMooEZKRKm}. La norme dans \( \eR^2\) de \( (x_k,y)-(\alpha,y)\).
		\end{itemize}
		\item[Uniforme continuité]
		La fonction \( f\) est continue sur le compact \( \mathopen[ a , b \mathclose]\times \mathopen[ c , d \mathclose]\). Elle est donc uniformément continue par le théorème de Heine \ref{PROPooBWUFooYhMlDp}, et donc son module de conttinuité vérifie \( \lim_{h\to 0} \omega_f(h)=0\) par \ref{LemeERapq}.

		Nous avons donc
		\begin{equation}
			\| f_{x_k}-f_{\alpha} \|\leq \omega_f(| x_k-\alpha |)\stackrel{\eR}{\longrightarrow}0.
		\end{equation}
		Nous avons donc prouvé que si \( x_k\stackrel{\eR}{\longrightarrow}\alpha\), alors \( f_{x_k}\stackrel{\| . \|_{\infty}}{\longrightarrow}f_{\alpha}\).

		\item[Conclusion]
		La norme étant une application continue, nous en déduisons que si \( x_k\to \alpha\), alors \( \| f_{x_k} \|_{\infty}\to \| f_{\alpha} \|_{\infty}\).

		Cela est la continuité séquentielle de la fonction \( g\), et donc la continuité tout court.
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Fonctions sur un compact}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Par le théorème des valeurs intermédiaires \ref{ThoValInter}, l'image d'un intervalle par une fonction continue est un intervalle, et nous avons l'importante propriété suivante des fonctions continues sur un compact.

Le théorème suivant est un cas particulier du théorème~\ref{ThoMKKooAbHaro}.
\begin{theorem}
	Si $f$ est une fonction continue sur l'intervalle compact $[a,b]$. Alors $f$ est bornée sur $[a,b]$ et elle atteint ses bornes.
\end{theorem}

\begin{proof}
	Étant donné que $[a,b]$ est un intervalle compact, son image est également un intervalle compact, et donc est de la forme $[m,M]$. Ceci découle du théorème~\ref{ThoImCompCotComp} et le corolaire~\ref{CorImInterInter}. Le maximum de $f$ sur $[a,b]$ est la borne $M$ qui est bien dans l'image (parce que $[m,M]$ est fermé). Idem pour le minimum $m$.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Polynômes, théorème de d'Alembert}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

L'algèbre des polynômes sur un anneau est définie en \ref{DEFooFYZRooMikwEL}. Si \( P\in A[X]\) et si \( \alpha\in A\) nous avons également défini l'évaluation de \( P\) en \( \alpha\); c'est la définition \ref{DEFooNXKUooLrGeuh}. Dans le cadre de l'analyse, lorsque nous considérons des polynômes, nous allons complètement confondre le polynôme avec la fonction qu'il définit.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Polynômes sur les réels}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooJKYJooFqbQMr}
	Tout polynôme à coefficients réels de degré impair possède une racine réelle.
\end{proposition}

\begin{proof}
	Nous mettons le plus haut degré en facteur :
	\begin{equation}
		P(x)=\sum_{k=0}^na_kx^k=x^n\sum_{k=0}^n\frac{ a_k }{ x^{n-k} }.
	\end{equation}
	Le terme \( k=0\) vaut \( a_nx^n\) tandis que les autres sont de la forme (à coefficient près) \( \frac{1}{ x^l }\) pour un \( l\geq 1\). Lorsque \( x\to \infty\), chacun de ces termes s'annule (lemme \ref{LEMooFCIXooJuHFqk}). Nous avons donc
	\begin{equation}
		\lim_{x\to \infty} P(x)=+\infty,
	\end{equation}
	et de même, \( n\) étant impair, \( \lim_{x\to -\infty} P(x)=-\infty\). Le théorème des valeurs intermédiaires \ref{ThoValInter} nous donne alors l'existence d'un réel sur lequel \( P\) s'annule.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Polynômes sur les complexes}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons parler de comportement asymptotique de polynômes définis sur \( \eC\). La topologique que nous considérons est celle de la compactification en un point décrite en \ref{PROPooHNOZooPSzKIN}.

Le lemme suivant donne une caractérisation de la limite en l'infini dans le compactifié \( \hat \eC\). Dans beaucoup de cas, cette caractérisation est prise comme la définition de la limite. Hélas, dans le Frido nous sommes des extrémistes et nous ne parvenons pas à dire le mot «limite» si il n'y a pas une topologie.
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooERABooQjLBzW}
	Nous considérons la compatification en un point d'Alexandrov\footnote{Définition \ref{PROPooHNOZooPSzKIN}.}. Soit une fonction \( f\colon \eC\to \eC\). Nous avons \( \lim_{z\to \infty} f(z)=\infty\) si et seulement si pour tout \( M>0\), il existe \( R>0\) tel que \( | z |>R\) implique \( | f(z) |>M\).
\end{lemma}

\begin{proof}
	Souvenons-nous que, en général\footnote{Définition \ref{DefYNVoWBx}.}, nous avons
	\begin{equation}
		\lim_{x\to a} f(x)=b
	\end{equation}
	si pour tout voisinage \( V\) de \( b\), il existe un voisinage \( W\) de \( a\) tel que \( z\in W\setminus\{ a \}\) implique \( f(z)\in V\).

	Précisons encore un point de notation. Si \( K\) est une partie de \( \eC\), nous notons \( K^c\) son complémentaire dans \( \eC\), pas dans \( \hat  \eC\).

	Ceci étant dit, nous passons à la preuve.
	\begin{subproof}
		\item[Sens direct]
		Nous supposons que \( \lim_{z\to \infty} f(z)=\infty\). Soit \( M>0\); nous considérons le voisinage \( V=\overline{ B(0,M) }^c\cup\{ \infty \}\). Par définition de la limite, il existe un voisinage \( W\) de \( \infty\) tel que \( z\in W\Rightarrow f(z)\in V\setminus\{ \infty \}=\overline{ B(0,M) }^c\). Ce voisinage est de la forme \( K^c\cup\{ \infty \}\). Vu que \( K\) est compact, il est borné et il existe \( R>0\) tel que \( K\subset B(0,R)\).

		Avec tout cela nous avons la chaine suivante d'implications :
		\begin{equation}
			| z |>R\Rightarrow z\in K^c\Rightarrow z\in W\Rightarrow f(z)\in V\setminus\{ \infty \}=\overline{ B(0,M) }^c\Rightarrow | f(z) |>M.
		\end{equation}
		C'est bien la propriété que nous voulions.
		\item[Sens réciproque]
		Soit un voisinage \( V\) de \( \infty\). Nous avons \( V=K^c\cup\{ \infty \}\) où \( K\) est compact dans \( \eC\). Il existe \( M>0\) tel que \( K\subset B(0,M)\).

		Par hypothèse, il existe \( R\) tel que \( | z |>R\Rightarrow | f(z) |>M\). Soit \( W=\overline{ B(0,R) }^c\cup\{ \infty \}\). Nous avons la chaine
		\begin{equation}
			z\in W\Rightarrow| z |>R\Rightarrow| f(z) |>M\Rightarrow f(z)\in K^c\Rightarrow f(z)\in V.
		\end{equation}
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooPWVWooGuftxZ}
	Soit le polynôme
	\begin{equation}
		\begin{aligned}
			P\colon \eC & \to \eC                    \\
			z           & \mapsto \sum_{i=0}^na_iz^i
		\end{aligned}
	\end{equation}
	où nous sous-entendons que \( a_n\neq 0\). La fonction \( z\mapsto | P(z) |\) est équivalente\footnote{Définition \ref{DEFooWDSAooKXZsZY}.} en l'infini à la fonction
	\begin{equation}
		\begin{aligned}
			w\colon \eC & \to \eR^+           \\
			z           & \mapsto | a_nz^n |.
		\end{aligned}
	\end{equation}
\end{proposition}

\begin{proof}
	Nous voudrions prouver qu'il existe une fonction \( \alpha\colon \eC\to \eR\) telle que
	\begin{subequations}     \label{EQooGXWZooDJZNzE}
		\begin{numcases}{}
			| \sum_{i=0}^na_iz^i |=\big( 1+\alpha(z) \big)| a_nz^n |.
			\lim_{z\to \infty} \alpha(z)=0.
		\end{numcases}
	\end{subequations}
	Nous trouvons un candidat pour être une telle fonction en isolant simplement \( \alpha(z)\) de cette égalité. Nous trouvons
	\begin{equation}
		\alpha(z)=\big| \sum_{i=0}^n\frac{ a_i }{ a_n }z^{i-n} \big|-1.
	\end{equation}
	Elle vérifie immédiatement \eqref{EQooGXWZooDJZNzE}. Le point qui fait intervenir la topologie de  est de vérifier que \( \lim_{z\to \infty} \alpha(z)=0\). Le terme \( i=0\) de la somme vaut \( 1\). Il suffit donc de montrer que pour \( i\neq 0\) nous avons
	\begin{equation}
		\lim_{z\to \infty} \frac{1}{ z^{n-i} }=0.
	\end{equation}
	Soit \( \epsilon>0\). Nous devons prouver qu'il existe un voisinage \( V\) de \( \infty\) dans \( \hat \eC\) tel que
	\begin{equation}
		| \frac{1}{ z^{n-i} }-0 |\leq \epsilon
	\end{equation}
	pour tout \( z\in V\).

	En utilisant la proposition \ref{PROPooXLARooYSDCsF} nous avons déjà
	\begin{equation}
		| \frac{1}{ z^{n-i} } |=\frac{1}{ | z^{n-i} | }=\frac{1}{ | z |^{n-i} }.
	\end{equation}
	Soit \( R>0\) tel que \( \frac{1}{ R }<\epsilon\). Nous considérons le voisinage \( \{ | z |>R \}\cup \{ \infty \}\) de \( \infty\). Dans ce voisinage, nous avons
	\begin{equation}
		\frac{1}{ | z |^{n-i} }\leq \frac{1}{ | z | }\leq \frac{1}{ R }<\epsilon.
	\end{equation}
	Et voilà.
\end{proof}

Le lemme suivant parle de polynôme sur \( \eC\). Vous pouvez l'adapter à \( \hat \eR\) et \( \bar \eR\).
\begin{lemma}       \label{LEMooYZVGooXZvBAc}
	Si \( P\colon \eC\to \eC\) est un polynôme, alors \( | P |\) atteint une borne inférieure globale.
\end{lemma}

\begin{proof}
	Nous savons, par l'équivalence de fonctions prouvée dans la proposition \ref{PROPooPWVWooGuftxZ} que \( \lim_{z\to \infty} P(z)=\infty\). Soit \( a>0\) dans \( \eR\). Par le lemme \ref{LEMooERABooQjLBzW} il existe un \( R>a\) tel que \( | z |>R\Rightarrow | f(z) |>| f(a) |\).

	La fonction \( | P |\) est continue sur le compact \( \overline{ B(0,R) }\). Soit \( z_0\) le point de minimum\footnote{Théorème de Weierstrass \ref{ThoWeirstrassRn}.} de \( | P |\) sur \( \overline{ B(0,R) }\).

	Nous devons prouver que \( z_0\) donne même un minimum global. Vu que \( a\in\overline{ B(0,R) }\) nous avons
	\begin{equation}
		| f(z_0) |\leq | f(a) |.
	\end{equation}
	Si \( z\in \overline{ B(0,R) }^c\), nous avons
	\begin{equation}
		| f(z) |>| f(a) |\geq | f(z_0) |.
	\end{equation}
	Donc ce \( z_0\) est un minimum sur \( B(0,R)\) et sur \( \overline{ B(0,R) }^c\). Bref, un minimum global.
\end{proof}

\begin{lemma}       \label{LEMooTTOYooXaukuH}
	Soit le polynôme
	\begin{equation}
		\begin{aligned}
			P\colon \eC & \to \eC                     \\
			z           & \mapsto \sum_{i=0}^na_iz^i.
		\end{aligned}
	\end{equation}
	La fonction \( P\) est équivalente à \( a_0+a_1z\) en \( z=0\).
\end{lemma}

\begin{proof}
	En posant \( g(z)=a_0+a_1z\), nous devons trouver une fonction \( \alpha\) telle que
	\begin{equation}        \label{EQooZFJBooVAYVBv}
		P(z)=\big( 1+\alpha(z) \big)g(z).
	\end{equation}
	Si \( a_0\neq 0\), il existe un voisinage de \( z=0\) sur lequel la fonction
	\begin{equation}        \label{EQooVCOVooAKWJxF}
		\alpha(z)=\frac{ z^2\sum_{i=2}^na_iz^{i-2} }{ a_0+a_1z }
	\end{equation}
	existe. Il n'y a aucun problème à ce que \( \alpha(z)\to 0\) pour \( z\to 0\)\footnote{En remarquant toutefois que c'est une limite à deux dimensions. Sachez la définir.}, et un simple calcul\footnote{En fait, la formule \eqref{EQooVCOVooAKWJxF} est obtenue en isolant \( \alpha(z)\) dans \eqref{EQooZFJBooVAYVBv}.} donne \eqref{EQooVCOVooAKWJxF}.

	Si par contre \( a_0=0\), nous faisons le calcul intermédiaire suivant :
	\begin{equation}
		\alpha(z)g(z)=P(z)-g(z)=z^2\sum_{i=2}^na_iz^{i-2},
	\end{equation}
	et donc, en isolant \( \alpha(z)\) et en simplifiant par \( z\), nous voyons que la fonction \( \alpha\) définie par
	\begin{equation}
		\alpha(z)=\frac{z}{ a_1 }\sum_{i=2}^na_iz^{i-2}
	\end{equation}
	fonctionne.
\end{proof}

\begin{proposition}[\cite{ooRIPVooMlBiAH,MonCerveau}]       \label{PROPooLBBLooQwEiHr}
	Soient \( a,b\in \eR\).
	\begin{enumerate}
		\item       \label{ITEMooSPSWooKLtqzZ}
		      L'équation \( z^2=a+bi\) a une solution dans \( \eC\).
		\item       \label{ITEMooQOJDooWjfGXv}
		      Pour tout \( l\), l'équation \( z^{2^l}=a+bi\) a une solution dans \( \eC\).
	\end{enumerate}
	Nous ne disons pas que ces solutions sont uniques\footnote{Comme vous en conviendrez en pensant à \( z^2=1\) qui a déjà les solutions \( 1\) et \( -1\).}.
\end{proposition}

\begin{proof}
	Pour prouver \ref{ITEMooSPSWooKLtqzZ}, l'équation \( z^2=a+bi\) a pour solution \( \pm\xi\) où\footnote{Si vous vous demandez où sont définies les racines carrés, c'est \ref{DEFooGQTYooORuvQb}.}
	\begin{equation}
		\xi=\sqrt{ \frac{ 1 }{2}a+\frac{ 1 }{2}\sqrt{ a^2+b^2 } }+i\signe(b)\sqrt{ -\frac{ 1 }{2}a+\frac{ 1 }{2}\sqrt{ a^2+b^2 } }.
	\end{equation}
	Nous n'avons en fait pas besoin de montrer que \( \pm\xi\) sont toutes deux des solutions, ni que ce sont les seules. Un calcul direct montre que \( \xi^2=a+bi\) et nous sommes content.

	Pour \ref{ITEMooQOJDooWjfGXv}, nous faisons une récurrence sur \( l\). Nous savons que
	\begin{equation}
		z^{2^{k+1}}=(z^{2^k})^2.
	\end{equation}
	Soit \( \xi\in \eC\) tel que \( \xi^{2^k}=a+bi\); un tel \( \xi\) existe par hypothèse de récurrence. Alors si \( z\) est tel que \( z^2=\xi\), nous avons
	\begin{equation}
		z^{2^{k+1}}=a+bi.
	\end{equation}
\end{proof}

Le théorème de d'Alembert possède de nombreuses démonstrations. En voici une qui à ma connaissance est celle demandant le moins d'analyse; une démonstration à base de théorie de Galois peut être trouvée dans \cite{rqrNyg,ooPSLMooAVODjn}. Si vous lisez ces lignes pour savoir qu'un polynôme de degré \( n\) possède au \emph{maximum} \( n\) racines, ce n'est pas ici qu'il faut regarder, mais le corolaire \ref{CORooUGJGooBofWLr}.
\begin{theorem}[d'Alembert\cite{ooRIPVooMlBiAH}]   \label{THOooIRJYooBiHRyW}
	Le corps \( \eC\) est algébriquement clos : tout polynôme non constant à coefficients complexes admet au moins une racine complexe\footnote{C'est la définition \ref{DEFooYZOYooAesmnP} d'être algébriquement clos.}.
\end{theorem}

\begin{proof}
	Nous effectuons une preuve tout à la fois par l'absurde et par récurrence en supposant que le polynôme
	\begin{equation}
		\begin{aligned}
			f\colon \eC & \to \eC                           \\
			z           & \mapsto z^n+a_1z^{n-1}+\ldots+a_n
		\end{aligned}
	\end{equation}
	n'a pas de racines dans \( \eC\), et que \( n\) soit le plus petit entier pour lequel un tel polynôme existe. Nous notons
	\begin{equation}
		n=2^km
	\end{equation}
	où \( m\) est impair.

	Le lemme \ref{LEMooYZVGooXZvBAc} donne un point \( z_0\) qui réalise le minimum global de \( | f |\) sur $\eC$. Nous posons \( g(z)=f(z_0+z)\) et nous définissons ses coefficients \( A_i\) par
	\begin{equation}
		g(z)=\sum_{i=0}^nA_iz^i.
	\end{equation}
	Nous avons \( A_n=1\) et \( | A_0 |=| f(z_0) |\). Soit \( A_r\) le premier à être non nul parmi les \( A_1\), \( A_2\), \ldots.
	\begin{subproof}
		\item[Si \( r<n\)]
		Par hypothèse de récurrence, il existe \( \xi\in \eC\) tel que \( \xi^r=-A_1/A_r\). Nous avons
		\begin{equation}
			g(t\xi)=A_0+\frac{ -A_rt^rA_0 }{ A_r }+t^{r+1}\sum_{i=r+1}^nA_i\xi^it^{i-r-1}.
		\end{equation}
		En notant \( P(t)\) le dernier polynôme, nous pouvons écrire cela sous forme compacte :
		\begin{equation}
			g(t\xi)=A_0-t^rA_0+t^{r+1}P(t).
		\end{equation}
		Vu que
		\begin{equation}
			\lim_{t\to 0} \frac{ t^{r+1}P(t) }{ t^r| A_0 | }=\lim_{t\to 0} tP(t)=0,
		\end{equation}
		il existe \( t_0>0\) tel que
		\begin{equation}
			| t_0^{r+1}P(t_0) |<| A_0t_0r |.
		\end{equation}
		Nous choisissons de plus \( t_0<1\), de telle sorte que \( 1-t^r>0\). Avec cela nous avons
		\begin{equation}
			| g(t\xi) |\leq | A_0 |(1-t^r)+| t^{r+1}P(t) |=| A_0 |\underbrace{-t^r| A_0 |+| t^{r+1}P(t) |}_{<0}<| A_0 |.
		\end{equation}
		Or \( | A_0 |\) était un minimum global de \( | g |\). Contradiction.

		\item[Si \( r=n\)]

		Dans ce cas,
		\begin{equation}
			g(z)=f(z_0+z)=A_0+z^n,
		\end{equation}
		et nous rappelons que \( n=2^km\) où \( m\) est impair. Nous allons trouver une contradiction dans les quatre cas \( \real{A_0}>0\), \( \real(A_0)<0\), \( \imag(A_0)>0\) et \( \imag(A_0)<0\). Bien entendu ces cas se recouvrent largement, mais en toute généralité, nous avons besoin des quatre.
		\begin{subproof}
			\item[Si \( \real(A_0)>0\)]
			La proposition \ref{PROPooLBBLooQwEiHr} nous permet de considérer \( v\in \eC\) tel que \( v^{2^k}=-1\). Nous avons alors
			\begin{equation}
				g(tv)=A_0+(tv)^n=A_0+t^n(v^{2^k})^m=A_0+t^n(-1)^m=A_0-t^n
			\end{equation}
			parce que \( m\) est impair. Nous avons \( \imag\big( g(tv) \big)=\imag(A_0)\). Si \( t\) est assez petit pour que \( t^n<| \real(A_0) |\) nous avons aussi \( |\real\big( g(tv) \big)|<| \real(A_0) |\). Donc
			\begin{equation}
				| g(tv) |^2=| \real\big( g(tv) \big) |^2+| \real\big( g(tv) \big) |^2<| \real(A_0) |^2+| \imag(A_0) |^2=| A_0 |^2.
			\end{equation}
			Donc \( | g(tv) |<| A_0 |\). Contradiction.
			\item[Si \( \real(A_0)<0\)]
			Nous prenons \( v=1\), et même histoire.
			\item[Si \( \imag(A_0)<0\)]
			Nous prenons \( w\in \eC\) tel que
			\begin{equation}
				w^{2^k}=i(-1)^{\frac{ 1 }{2}(m-1)}.
			\end{equation}
			Là, il y a un peu d'arrachage de cheveux pour bien voir les cas. La difficulté est que les puissances de \( i\) alternent entre \( 1\), \( -1\), \( i\) et \( -i\). Vu que \( m\) est impair, nous avons un \( l\) tel que \( m=2l+1\). Nous subdivisons les cas \( l\) pair et \( l\) impair.
			\begin{subproof}
				\item[Si \( l\) est pair]
				Alors d'une part \( \frac{ 1 }{2}(m-1)=l\) est pair et donc
				\begin{equation}
					(-1)^{\frac{ 1 }{2}(m-1)}=1.
				\end{equation}
				Et d'autre part, \( i^{2l+1}=(-1)^li=i\). En tout,
				\begin{equation}
					i^m(-1)^{\frac{ 1 }{2}(m-1)}=i.
				\end{equation}
				\item[Si \( l\) est impair]
				Alors \( \frac{ 1 }{2}(m-1)=l\) et \( (-1)^{\frac{ 1 }{2}(m-1)}=-1\). Mais en même temps, \( i^{2l+1}=-i\), ce qui donne encore une fois
				\begin{equation}
					i^m(-1)^{\frac{ 1 }{2}(m-1)}=i.
				\end{equation}
			\end{subproof}
			Bref, que \( l\) soit pair ou impair, nous avons \( i^m(-1)^{\frac{ 1 }{2}(m-1)}=i\).
		\end{subproof}
		Nous avons donc \( \real\big( g(tw) \big)=\real(A_0)\) et \( \imag\big( g(tw) \big)<\imag(A_0)\). Encore contradiction.
		\item[Si \( \imag(A_0)=0\)]
		Même chose que ce que nous venons de faire, mais avec
		\begin{equation}
			w^{2^k}=-i(-1)^{\frac{ 1 }{2}(m-1)}.
		\end{equation}
	\end{subproof}
\end{proof}


\begin{corollary}[\cite{MonCerveau}]       \label{CORooKKNWooWEQukb}
	Tout polynôme de degré \( 3\) à coefficients réels possède au moins une racine réelle.
\end{corollary}

\begin{proof}
	Soient les racines \( \lambda_1\), \( \lambda_2\) et \( \lambda_3\) du polynôme en question. Toutes trois sont dans \( \eC\). Supposons que \( \lambda_1\) ne soit pas réelle. Alors \( \lambda_2\) ou \( \lambda_3\) doit être égale à \( \bar\lambda_1\). Disons \( \lambda_2\). Nous avons donc les racines \( \lambda_1\), \( \bar\lambda_1\) et \( \lambda_3\). Le polynôme se factorise alors en
	\begin{equation}        \label{EQooELMMooNbpBgg}
		a(X-\lambda_1)(X-\bar\lambda_1)(X-\lambda_3).
	\end{equation}
	Le coefficient \( a\) doit être réel parce qu'il est le coefficient du terme en \( X^3\) (réel par hypothèse). Si \( \lambda_3\) n'est pas réel, alors ce polynôme ne peut pas avoir des coefficients réels. Entre autres parce que terme indépendant est \( a| \lambda_1 |^2\lambda_3\), qui est réel si et seulement si \( \lambda_3\) est réel\footnote{Notez l'utilisation du lemme~\ref{LEMooONLNooXLNbtB}.}.
\end{proof}
Tant que vous y êtes, vous pouvez voir que le polynôme \eqref{EQooELMMooNbpBgg} est à coefficient réels si et seulement si \( a\in \eR\) et \( \lambda_3\in \eR\).

\begin{example}     \label{EXooIPLOooSNfiWg}
	Toute application linéaire \( \eR^3\to \eR^3\) a un vecteur propre. En effet si \( R\colon \eR^3\to \eR^3\) est linéaire, son polynôme caractéristique \( \chi_R\) est de degré \( 3\). Le corolaire \ref{CORooKKNWooWEQukb} indique qu'un tel polynôme possède au moins une racine réelle.
	Une telle racine est une valeur propre de \( R\) par le théorème \ref{ThoWDGooQUGSTL}.
\end{example}

\begin{definition}
	Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!valeur propre!algébrique} de la valeur propre \( \lambda\) de \( u\). À ne pas confondre avec la \defe{multiplicité géométrique}{multiplicité!valeur propre!géométrique} qui sera la dimension de l'espace propre.
\end{definition}

\begin{proposition}
	Un polynôme irréductible à coefficients réels est soit de degré un soit de degré \( 2\) avec un discriminant négatif.
\end{proposition}

\begin{proof}
	Soit un polynôme \( P\) à coefficients réels de degré plus grand que \( 1\). Alors le théorème de d'Alembert-Gauss (théorème~\ref{THOooIRJYooBiHRyW}) implique l'existence d'une racine \( \alpha \in \eC \). Si $\alpha$ est un réel, $P$ est réductible. Si \( \alpha\) n'est pas réel, alors conjugué complexe \( \bar \alpha\) est également une racine. Par conséquent les polynômes \( (X-\alpha)\) et \( (X-\bar \alpha)\) divisent \( P\) dans \( \eC[X]. \).

	Ces deux polynômes sont premiers entre eux parce que
	\begin{equation}
		a(X-\alpha)+b(X-\bar \alpha)=0
	\end{equation}
	implique \( a=b=0\). Par conséquent le produit
	\begin{equation}
		X^2-(\alpha+\bar \alpha)X+\alpha\bar\alpha
	\end{equation}
	divise également \( P\). Ce dernier est un polynôme à coefficients réels de degré \( 2\). Donc tout polynôme de degré \( 3\) ou plus est réductible.
\end{proof}

\begin{proposition}     \label{PROPooLXGSooXmVcVG}
	Si \( E\) est un espace vectoriel sur \( \eC\), tout endomorphisme possède au moins une valeur propre.
\end{proposition}

\begin{proof}
	Soit un endomorphisme \( u\) sur \( E\). Le théorème \ref{ThoWDGooQUGSTL} dit que \( \lambda\in \eC\) est une valeur propre si et seulement si \( \lambda\) est une racine du polynôme caractéristique \( \chi_u\). Or ce polynôme possède au moins une racine dans \( \eC\) par le théorème de d'Alembert \ref{THOooIRJYooBiHRyW}.
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Trigonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : généralités}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooMCOGooEoQCsz}

\begin{definition}[\cite{MQMKooPBfnZN}]
	Une matrice dans \( \eM(n,\eK)\) est \defe{trigonalisable}{matrice!trigonalisable} lorsqu'elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice triangulaire supérieure.
\end{definition}

\begin{proposition}[Trigonalisation et polynôme caractéristique scindé] \label{PropKNVFooQflQsJ}
	Soit \( u\) un endomorphisme d'un espace vectoriel \( E\) sur le corps \( \eK\). Les faits suivants sont équivalents.
	\begin{enumerate}
		\item   \label{ItemZKDMooOrTHkwi}
		      L'endomorphisme \( u\) est trigonalisable (auquel cas les valeurs propres sont sur la diagonale).
		\item   \label{ItemZKDMooOrTHkwii}
		      Le polynôme caractéristique de \( u\) est scindé\footnote{Définition~\ref{DefCPLSooQaHJKQ}.}.
	\end{enumerate}
\end{proposition}
\index{trigonalisation!et polynôme caractéristique}

\begin{proof}
	\begin{subproof}
		\item[\ref{ItemZKDMooOrTHkwii}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwi}]
		Nous avons par hypothèse que
		\begin{equation}
			\chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i}
		\end{equation}
		où les \( \lambda_i\) sont les valeurs propres de \( u\). Le théorème de Cayley-Hamilton~\ref{ThoCalYWLbJQ} dit que \( \chi_u(u)=0\), ce qui permet d'utiliser le théorème de décomposition des noyaux~\ref{ThoDecompNoyayzzMWod} :
		\begin{equation}
			E=\ker(X-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(X-\lambda_r)^{\alpha_r}.
		\end{equation}
		Les espaces \( F_{\lambda_i}(u)=\ker(X-\lambda_i)^{\alpha_i}\) sont les espaces caractéristiques de \( u\), ce qui fait que \( u-\lambda_i\mtu\) est nilpotent sur \( F_{\lambda_i}(u)\). L'endomorphisme \( u-\lambda_i\mtu\) est donc strictement trigonalisable supérieur sur son bloc\footnote{Proposition~\ref{PropMWWJooVIXdJp}.}. Cela signifie que \( u\) est triangulaire supérieure avec les valeurs propres sur la diagonale.

		\item[\ref{ItemZKDMooOrTHkwi}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwii}]

		C'est immédiat parce que le déterminant d'une matrice triangulaire est le produit des éléments de sa diagonale.
	\end{subproof}
\end{proof}

\begin{remark}
	La méthode des pivots de Gauss\footnote{Le lemme~\ref{LemZMxxnfM}.} certes permet de trigonaliser n'importe quelle matrice, mais elle ne correspond pas à un changement de base. Autrement dit, les pivots de Gauss ne sont pas des similitudes.

	C'est là qu'il faut bien avoir en tête la différence entre \emph{équivalence} et \emph{similarité}\footnote{Définition \ref{DefBLELooTvlHoB}.}. Lorsqu'on parle de changement de base, de matrice trigonalisable ou diagonalisable, nous parlons de similarité et non d'équivalence.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

La proposition~\ref{PropKNVFooQflQsJ} dit déjà que tous les endomorphismes sont trigonalisables sur \( \eC\). Nous allons aller plus loin et montrer que la trigonalisation peut être effectuée à l'aide d'une matrice unitaire.

Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque~\ref{RemXFZTooXkGzQg} utilisant la proposition~\ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe, trigonalisation\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
	Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure\footnote{«triangulaire supérieure» ne signifie pas «strictement triangulaire supérieure». Ici, il est possible que la diagonale soit non nulle; non seulement possible, mais même très probable en pratique.}.
\end{lemma}
\index{lemme!Schur complexe}
%TODOooSCDRooMXUMbb : Le lemme de Schur est souvent énoncé en disant que si p est une représentation irréductible, alors les seuls endomorphismes de V commutant avec tous les p(g) sont les multiples de l'identité. Quel est le lien avec ceci ?

\begin{proof}
	Étant donné que \( \eC\) est algébriquement clos\footnote{Algébriquement clos, définition \ref{DEFooYZOYooAesmnP}. Le fait que \( \eC\) le soit est le théorème de d'Alembert \ref{THOooIRJYooBiHRyW}.}, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v_1,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
	\begin{equation}
		Q=\begin{pmatrix}
			\uparrow   & \uparrow   &        & \uparrow   \\
			v_1        & u_2        & \cdots & u_n        \\
			\downarrow & \downarrow &        & \downarrow
		\end{pmatrix}.
	\end{equation}
	Nous avons \( Q^{-1}AQe_1=Q^{-1} Av_1=\lambda Q^{-1} v=\lambda_1 e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
	\begin{equation}
		Q^{-1}AQ=\begin{pmatrix}
			\lambda_1 & *   \\
			0         & A_1
		\end{pmatrix}
	\end{equation}
	où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale\footnote{Proposition \ref{PropKBCXooOuEZcS}\ref{ITEMooHSTAooIbVrwa}.}).
\end{proof}

\begin{definition}  \label{DefWQNooKEeJzv}
	Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} si il commute avec son adjoint.
\end{definition}

Les opérateurs normaux comprennent évidemment les opérateurs hermitiens, mais également les anti-hermitiens, et ça c'est bien parce que c'est le cas de l'algèbre associée à \( \SU(2)\).

\begin{theorem}[Théorème spectral pour les matrices normales\footnote{Définition~\ref{DefWQNooKEeJzv}}\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
	Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
	\begin{enumerate}
		\item   \label{ItemJZhFPSi}
		      \( A\) est normale,
		\item   \label{ItemJZhFPSii}
		      \( A\) se diagonalise par une matrice unitaire,
		\item
		      \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
		\item
		      il existe une base orthonormale de vecteurs propres de \( A\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	Nous allons nous contenter de prouver~\ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
	%TODO : le reste.

	Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme~\ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
	\begin{equation}
		QTT^*Q^{-1}=QT^*TQ^{-1},
	\end{equation}
	ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
	\begin{equation}
		(TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
	\end{equation}
	Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
	\begin{equation}
		| T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\cdots+| T_{1n} |^2,
	\end{equation}
	ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

	Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
	\begin{equation}
		DD^*=UAA^*U^*
	\end{equation}
	et
	\begin{equation}
		D^*D=UA^*AU^*,
	\end{equation}
	qui ce prouve que \( A\) est normale.
\end{proof}

Tant que nous en sommes à parler de spectre de matrices hermitiennes\ldots Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} et le théorème~\ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque~\ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{lemma}[\cite{ooLMMRooUXhOdx}]   \label{LEMooHUGEooVYhZdZ}
	Si \( A\) est une matrice carrée et inversible,
	\begin{equation}
		\Spec(A^*A)=\Spec(AA^*)
	\end{equation}
\end{lemma}

\begin{proof}
	Nous allons montrer l'égalité des polynômes caractéristiques. D'abord une simple multiplication montre que
	\begin{equation}
		(A^*A-\lambda\mtu)A^{-1}=A^{-1}(AA^*-\lambda\mtu).
	\end{equation}
	Nous prenons le déterminant de cette égalité en utilisant les propriétés~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} et~\ref{ITEMooZMVXooLGjvCy} :
	\begin{equation}
		\det(A^*A-\lambda\mtu)\det(A^{-1})=\det(A^{-1})\det(AA^*-\lambda\mtu).
	\end{equation}
	En simplifiant par \( \det(A^{-1})\) (qui est non nul parce que \( A\) est inversible) nous obtenons l'égalité des polynômes caractéristiques et donc l'égalité des spectres.
\end{proof}


En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{lemma}       \label{LEMooRCFGooPPXiKi}
	Soit \( A\in \eM(n,\eC)\) et une matrice unitaire \( U\) telle que \( A=UTU^{-1}\) où \( T\) est triangulaire.
	\begin{enumerate}
		\item
		      En ce qui concerne les polynômes caractéristiques, \( \chi_A=\chi_T\).
		\item
		      Pour les spectres, \( \Spec(A)=\Spec(T)\).
		\item
		      Les valeurs propres de \( A\) sont les éléments diagonaux de \( T\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Vu que \( U\) commute évidemment avec \( \mtu\) nous avons
	\begin{equation}
		\chi_A(\lambda)=\det(A-\lambda \mtu)=\det(UTU^{-1}-\lambda\mtu)=\det\big( U(T-\lambda\mtu)U^{-1} \big).
	\end{equation}
	À ce niveau nous utilisons le fait que le déterminant soit multiplicatif~\ref{PropYQNMooZjlYlA} pour conclure :
	\begin{equation}
		\chi_A(\lambda)=\det\big( U(T-\lambda\mtu)U^{-1} \big)=\det(U)\det(T-\lambda\mtu)\det(U^{-1})=\det(T-\lambda\mtu)=\chi_T(\lambda).
	\end{equation}

	Pour les spectres, l'égalité des polynômes caractéristiques implique l'égalité des spectres parce que les valeurs propres sont les racines du polynôme caractéristique par le théorème~\ref{ThoWDGooQUGSTL}.

	Les valeurs propres d'une matrice triangulaire sont les valeurs sur la diagonale.
\end{proof}

\begin{lemma}[Trigonalisation simultanée]   \label{LemSLGPooIghEPI}
	Une famille de matrices de \( \GL(n,\eC)\) commutant deux à deux est simultanément trigonalisable.
\end{lemma}
\index{trigonalisation!simultanée}

\begin{proof}
	Commençons par enfoncer une porte ouverte par la proposition~\ref{PropKNVFooQflQsJ} : toutes les matrices de \( \GL(n,\eC)\) sont trigonalisables parce que tous les polynômes sont scindés.

	Nous effectuons la démonstration par récurrence sur la dimension. Si \( n=1\) alors toutes les matrices sont triangulaires et nous ne nous posons pas de questions. Nous supposons donc \( n>1\).

	Soit la famille \( (A_i)_{i\in I}\) dans \( \GL(n,\eC)\) et \( A_0\) un de ses éléments. Nous nommons \( \lambda_1,\ldots, \lambda_r\) les valeurs propres distinctes de \( A_0\). Le théorème de décomposition primaire~\ref{ThoSpectraluRMLok} nous donne la somme directe d'espaces caractéristiques\footnote{Définition~\ref{DefFBNIooCGbIix}.}
	\begin{equation}
		E=F_{\lambda_1}(A_0)\oplus\ldots\oplus F_{\lambda_r}(A_0).
	\end{equation}
	Nous pouvons supposer que cette somme n'est pas réduite à un seul terme. En effet si tel était le cas, \( A_0\) serait un multiple de l'identité parce que \( A_0\) n'aurait qu'une seule valeur propre et les sommes dans la décomposition de Dunford~\ref{ThoRURcpW}\ref{ItemThoRURcpWiii} se réduisent à un seul terme (et \( p_i=\id\)). En particulier les dimensions des espaces \( F_{\lambda}(A_0)\) sont strictement plus petites que \( n\).

	Vu que tous les \( A_i\) commutent avec \( A_0\), les espaces \( F_{\lambda}(A_0)\) sont stables par les \( A_i\) et nous pouvons trigonaliser les \( A_i\) simultanément sur chacun des \( F_{\lambda}(A_0)\) en utilisant l'hypothèse de récurrence.
\end{proof}

\begin{theorem}[Lie-Kolchin\cite{PAXrsMn}]  \label{ThoUWQBooCvutTO}
	Tout sous-groupe connexe et résoluble de \( \GL(n,\eC)\) est conjugué à un groupe de matrices triangulaires.
\end{theorem}
\index{trigonalisation!simultanée}
\index{théorème!Lie-Kolchin}

\begin{proof}
	Soit \( G\) un sous-groupe connexe et résoluble de \( \GL(n,\eC)\).

	\begin{subproof}
		\item[Si sous-espace non trivial stable par \( G\)]

		Nous commençons par voir ce qu'il se passe si il existe un sous-espace vectoriel non trivial \( V\) de \( \eC^n\) stabilisé par \( G\). Pour cela nous considérons une base de \( \eC^n\) dont les premiers éléments forment une base de \( V\) (base incomplète, théorème~\ref{ThonmnWKs}). Les éléments de \( G\) s'écrivent, dans cette base,
		\begin{equation}    \label{EqGOKTooEaGACG}
			\begin{pmatrix}
				g_1 & *   \\
				0   & g_2
			\end{pmatrix}.
		\end{equation}
		Les matrices \( g_1\) et \( g_2\) sont carrés. Nous considérons alors l'application \( \psi\) définie par
		\begin{equation}
			\begin{aligned}
				\psi\colon G & \to \GL(V)   \\
				g            & \mapsto g_1.
			\end{aligned}
		\end{equation}
		Cela est un morphisme de groupes parce que
		\begin{equation}
			\begin{pmatrix}
				g_1 & *   \\
				0   & g_2
			\end{pmatrix}\begin{pmatrix}
				h_1 & *   \\
				0   & h_2
			\end{pmatrix}=
			\begin{pmatrix}
				g_1h_1 & *      \\
				0      & g_2h_2
			\end{pmatrix},
		\end{equation}
		de telle sorte que \( \psi(gh)=\psi(g)\psi(h)\).

		Le groupe \( \psi(G)\) est connexe et résoluble. En effet \( \psi(G)\) est connexe en tant qu'image d'un connexe par une application continue (proposition~\ref{PropGWMVzqb}). Et il est résoluble en tant qu'image d'un groupe résoluble par un homomorphisme par la proposition~\ref{PropBNEZooJMDFIB}. Vu que \( \psi(G)\) est un sous-groupe résoluble et connexe de \( \GL(V)\) et que la dimension de \( V\) est strictement plis petite que celle de \( \eC^n\), une récurrence sur la dimension indique que \( \psi(G)\) est conjugué à un groupe de matrices triangulaires. C'est-à-dire qu'il existe une base de \( V\) dans laquelle toutes les matrices \( g_1\) (avec \( g\in G\)) sont triangulaires supérieures.

		On fait de même avec l'application \( g\mapsto g_2\), ce qui donne une base du supplémentaire de \( V\) dans laquelle les matrices \( g_2\) sont triangulaires.

		En couplant ces deux bases, nous obtenons une base de \( \eC^n\) dans laquelle toutes les matrices \eqref{EqGOKTooEaGACG} (c'est-à-dire toutes les matrices de \( G\)) sont triangulaires supérieures.

		\item[Sinon]

		Nous supposons à présent que \( \eC^n\) n'a pas de sous-espaces non triviaux stables sous \( G\). Nous posons \( m=\min\{ k\tq D^k(G)=\{ e \} \}\), qui existe parce que \( G\) et résoluble et que sa suite dérivée termine sur \( {e}\) (proposition~\ref{PropRWYZooTarnmm}).

		\item[Si \( m=1\)]

		Si \( m=1\) alors \( G\) est abélien et il existe une base de \( G\) dans laquelle toutes les matrices de \( G\) sont triangulaires (lemme~\ref{LemSLGPooIghEPI}). Le premier vecteur d'une telle base serait stable par \( G\), mais comme nous avons supposé qu'il n'y avait pas de sous-espaces non triviaux stabilisés par \( G\), il faut déduire que ce vecteur stable est à lui tout seul non trivial, c'est-à-dire que \( n=1\). Dans ce cas, le théorème est démontré.

		\item[Si \( m>1\)]

		Nous devons maintenant traiter le cas où \( m>1\). Nous posons \( H=D^{m-1}(G)\); cela est un sous-groupe normal et abélien de \( G\). Encore une fois le résultat de trigonalisation simultanée~\ref{LemSLGPooIghEPI} donne une base dans laquelle tous les éléments de \( H\) sont triangulaires. En particulier le premier élément de cette base est un vecteur propre commun à toutes les matrices de \( H\).

		Soit \( V\) le sous-espace engendré par tous les vecteurs propres communs de \( H\). Nous venons de voir que \( V\) n'est pas vide. Nous allons montrer que \( V\) est stable par \( G\). Soient \( h\in H\), \( v\in V\) et \( g\in G\) :
		\begin{equation}    \label{EqPMOBooVLIhrJ}
			h\big( g(v) \big)=g\underbrace{g^{-1}hg}_{\in H}(v)=g(\lambda v)=\lambda g(v)
		\end{equation}
		parce que \( v\) est vecteur propre de \( g^{-1} hg\). Ce que le calcul \eqref{EqPMOBooVLIhrJ} montre est que \( g(v)\) est vecteur propre de \( h\) pour la valeur propre \( \lambda\). Donc \( g(v)\in V\) et \( V\) est stabilisé par \( G\). Mais comme il n'existe pas d'espaces non triviaux stabilisés par \( G\), nous en déduisons que \( V=\eC^n\). Donc tous les vecteurs de \( \eC^n\) sont vecteurs propres communs de \( H\). Autrement dit on a une base de diagonalisation simultanée de \( H\).

		\item[\( H\) est dans le centre de \( G\)]

		Montrons à présent que \( H\) est dans le centre de \( G\), c'est-à-dire que pour tout \( g\in G\) et \( h\in H\) il faut \( ghg^{-1}=h\). D'abord \( ghg^{-1}\) est une matrice diagonale (parce que elle est dans \( H\)) ayant les mêmes valeurs propres que \( h\). En effet si \( \lambda\) est valeur propre de \( ghg^{-1}\) pour le vecteur propre \( v\), alors
		\begin{subequations}
			\begin{align}
				(ghg^{-1})(v)         & =\lambda v                    \\
				h\big( g^{-1} v \big) & =\lambda \big( g^{-1}v \big),
			\end{align}
		\end{subequations}
		c'est-à-dire que \( \lambda\) est également valeur propre de \( h\), pour le vecteur propre \( g^{-1} v\). Mais comme \( h\) a un nombre fini de valeurs propres, il n'y a qu'un nombre fini de matrices diagonales ayant les mêmes valeurs propres que \( h\). L'ensemble \( \AD(G)h\) est donc un ensemble fini. D'autre part, l'application \( g\mapsto g^{-1}hg\) est continue, et \( G\) est connexe, donc l'ensemble \( \AD(G)h\) est connexe. Un ensemble fini et connexe dans \( \GL(n,\eC)\) est nécessairement réduit à un seul point. Cela prouve que \( ghg^{-1}=h\) pour tout \( g\in G\) et \( h\in H\).

		\item[Espaces propres stables pour tout \( G\)]

		Soit \( h\in H\) et \( W\) un espace propre de \( h\) (ça existe non vide parce que \( H\) est triangularisé, voir plus haut). Alors nous allons prouver que \( W\) est stable pour tous les éléments de \( G\). En effet si \( w\in W\) avec \( h(w)=\lambda w\) alors en permutant \( g\) et \( h\),
		\begin{equation}
			hg(w)=g(hw)=\lambda g(w),
		\end{equation}
		donc \( g(w)\) est aussi vecteur propre de \( h\) pour la valeurs propre \( \lambda\), c'est-à-dire que \( g(w)\in W\). Vu que nous supposons que \( \eC^n\) n'a pas d'espaces invariants non triviaux, nous devons conclure que \( W=\eC^n\), c'est-à-dire que \( H\) est composé d'homothéties. C'est-à-dire que pour tout \( h\in H\) nous avons \( h=\lambda_h\mtu\).

		\item[Contradiction sur la minimalité de \( m\)]

		Les éléments d'un groupe dérivé sont de déterminant \( 1\) parce que \( \det(g_1g_2g_1^{-1}g_2^{-1})=1\). Par conséquent pour tout \( h\), le nombre \( \lambda_h\) est une racine \( n\)\ieme de l'unité. Vu qu'il n'y a qu'une quantité finie de racines \( n\)\ieme de l'unité, le groupe \( H\) est fini et connexe et donc une fois de plus réduit à un élément, c'est-à-dire \( H=\{ e \}\). Cela contredit la minimalité de \( m\) et donc produit une contradiction. Nous devons donc avoir \( m=1\).

		\item[Conclusion]

		Nous avons vu que si \( \eC^n\) avait un sous-espace non trivial fixé par \( G\) alors le théorème était démontré. Par ailleurs si \( \eC^n\) n'a pas un tel sous-espace, soit \( m=1\) (et alors le théorème est également prouvé), soit \( m>1\) et alors on a une contradiction.

		Bref, le théorème est prouvé sous peine de contradiction.
	\end{subproof}
\end{proof}


\begin{remark}
	Le lemme mentionne le fait que les valeurs propres de \( A\) sont les éléments diagonaux de \( T\). Mais attention : ceci ne dit rien au niveau des multiplicités géométriques. Un nombre peut être cinq fois sur la diagonale de \( T\) alors que l'espace propre correspondant pour \( A\) n'est que de dimension \( 1\). Exemple : la matrice
	\begin{equation}
		A=\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\end{equation}
	a deux \( 1\) sur la diagonale. Le nombre \( 1\) est bien une valeur propre de \( A\), mais le système
	\begin{equation}
		A\begin{pmatrix}
			x \\
			y
		\end{pmatrix}=\begin{pmatrix}
			x \\
			y
		\end{pmatrix}
	\end{equation}
	donne \( y=0\) et donc un espace propre de dimension seulement \( 1\).
\end{remark}

\begin{remark}  \label{RemXFZTooXkGzQg}
	Si \( \eK\) est algébriquement clos (comme \( \eC\) par exemple), alors tous les polynômes sont scindés et toutes les matrices sont trigonalisables\footnote{La proposition~\ref{PropKNVFooQflQsJ} montre cela, et le lemme de Schur complexe~\ref{LemSchurComplHAftTq} va un peu plus loin et précise que la trigonalisation peut être faite par une matrice unitaire.}. Un exemple un peu simple de cela est la matrice
	\begin{equation}
		u=\begin{pmatrix}
			0 & -1 \\
			1 & 0
		\end{pmatrix}.
	\end{equation}
	Le polynôme caractéristique est \( \chi_u(X)=X^2+1\) et les valeurs propres sont \( \pm i\). Il est vite vu que dans la base
	\begin{equation}
		\{ \begin{pmatrix}
			i \\
			1
		\end{pmatrix}, \begin{pmatrix}
			1 \\
			i
		\end{pmatrix}\}
	\end{equation}
	de \( \eC^2\), la matrice \( u\) se note \( \begin{pmatrix}
		i & 0  \\
		0 & -i
	\end{pmatrix}\).
\end{remark}

\begin{remark}  \label{RemREOSooGEDJWX}
	Cela nous donne une autre façon de prouver qu'une matrice nilpotente de \( \eM(n,\eC)\) ou \( \eM(n,\eR)\) est trigonalisable\cite{KDUFooVxwqlC}. D'abord dans \( \eM(n,\eC)\), toutes les matrices sont trigonalisables\footnote{Parce que le polynôme caractéristique est scindé, voir la proposition~\ref{PropKNVFooQflQsJ}..}, et les valeurs propres arrivent sur la diagonale. Mais comme les valeurs propres d'une matrice nilpotente sont zéro, elle est triangulaire stricte. Par ailleurs son polynôme caractéristique est alors \( X^n\).

	Ensuite si \( u\in \eM(n,\eR)\) nous pouvons voir \( u\) comme une matrice dans \( \eM(n,\eC)\) et y calculer son polynôme caractéristique qui sera tout de même \( X^n\). Ce polynôme étant scindé, la proposition~\ref{PropKNVFooQflQsJ} nous assure que \( u\) est trigonalisable. Une fois de plus, les valeurs propres étant sur la diagonale, elle est triangulaire supérieure stricte.
\end{remark}

\begin{corollary}   \label{CorUNZooAZULXT}
	Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
	\begin{equation}
		\chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
	\end{equation}
	où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
	Le lemme~\ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons
	\begin{equation}
		\chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
			X-\lambda_1 & *      & *           \\
			0           & \ddots & *           \\
			0           & 0      & X-\lambda_r
		\end{pmatrix},
	\end{equation}
	qui vaut bien le produit annoncé.
\end{proof}

\begin{corollary}       \label{CORooTPDHooXazTuZ}
	Si \( A\in \eM(n,\eC)\) et \( k\in \eN\) alors
	\begin{equation}
		\Spec(A^k)=\{ \lambda^k\tq \lambda\in \Spec(A) \}.
	\end{equation}
\end{corollary}

\begin{proof}
	Par le lemme~\ref{LemSchurComplHAftTq} nous avons une matrice unitaire \( U\) et une triangulaire \( T\) telles que \( A=UTU^{-1}\). En passant à la puissance \( k\) nous avons aussi
	\begin{equation}
		A^k=UT^kU^{-1}.
	\end{equation}
	Donc le spectre de \( A^k\) est celui de \( T^k\) (lemme~\ref{LEMooRCFGooPPXiKi} et le fait qu'une puissance d'une matrice triangulaire est encore triangulaire). Or les éléments diagonaux de \( T^k\) sont les puissances \( k\)\ieme\ des éléments diagonaux de \( T\), qui sont les valeurs propres de \( A\).
\end{proof}
Pour le cas complexe, c'est le lemme \ref{LEMooVCEOooIXnTpp} et le théorème \ref{ThogammwA}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Matrices, spectre et norme}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La lien entre la norme opérateur d'une matrice et son spectre sera entre autres utilisé pour étudier le conditionnement de problèmes numériques. Voir la définition \ref{DEFooBKQWooJuoCGX} et par exemple son lien avec la résolution numérique de systèmes linéaires dans la proposition \ref{PROPooGIXFooAhJkIs}.

\begin{proposition}[\cite{ooYLHAooCzQvoa}]      \label{PROPooKLFKooSVnDzr}
	Soit une matrice \( A\in \eM(n,\eC)\) de rayon spectral \( \rho(A)\). Soit une norme \( \| . \|\) sur \( \eC^n\) et la norme opérateur correspondante. Alors
	\begin{equation}
		\rho(A)\leq \| A^k \|^{1/k}
	\end{equation}
	pour tout \( k\in \eN\).
\end{proposition}

\begin{proof}
	Soit \( v\in \eC^n\) et \( \lambda\in \eC\) un couple vecteur-valeur propre. Nous avons \( \| Av \|=| \lambda |\| v \|\) et aussi
	\begin{equation}
		| \lambda |^k\| v \|=\| \lambda^kv \|=\| A^kv \|\leq \| A^k \|\| v \|.
	\end{equation}
	La dernière inégalité est due au fait que nous avons choisi sur \( \eM(n,\eC)\) la norme subordonnée à celle choisie sur \( \eC^n\), via le lemme~\ref{LEMooIBLEooLJczmu}. Nous simplifions par \( \| v \|\) et obtenons \( | \lambda |\leq \| A^k \|^{1/k}\). Étant donné que \( \rho(A)\) est la maximum de tous les \( \lambda\) possibles, la majoration passe au maximum :
	\begin{equation}
		\rho(A)\leq \| A^k \|^{1/k}.
	\end{equation}
\end{proof}


\begin{proposition}     \label{PROPooJGNFooEwtNmJ}
	Soient deux espaces vectoriels normés \( E\) et \( V\). Soient des applications continues \( f,g\colon E\to \End(V)\). Alors l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon E & \to \End(V)            \\
			x            & \mapsto f(x)\circ g(x)
		\end{aligned}
	\end{equation}
	est continue.
\end{proposition}

\begin{proof}
	Soit une suite \( x_k\stackrel{E}{\longrightarrow}x\). Nous devons montrer que \( \psi(x_k)\stackrel{\End(V)}{\longrightarrow}\psi(x)\). Pour cela nous utilisons le lemme \ref{LEMooFITMooBBBWGI} qui indique que la norme opérateur est une norme d'algèbre. Nous avons :
	\begin{subequations}
		\begin{align}
			\| \psi(x_k)-\psi(x) \| & =\| f(x_k)\circ g(x_k)-f(x)\circ g(x) \|                                              \\
			                        & \leq \| f(x_k)\circ g(x_k)-f(x_k)\circ g(x) \|+\| f(x_k)\circ g(x)-f(x)\circ g(x) \|  \\
			                        & =\| f(x_k)\circ \big( g(x_k)\circ g(x) \big) \|+\| \big(f(x_k)-f(x)\big)\circ g(x) \| \\
			                        & \leq \| f(x_k) \|\| g(x_k)-g(x) \|+\| f(x_k)-f(x) \|\| g(x) \|.
		\end{align}
	\end{subequations}
	Pour \( k\to \infty\) nous avons \( \| f(x_k)\to \| f(x) \| \|\), \( \| f(x_k)-f(x) \|\to 0\) (parce que \( f\) est continue) et similaire avec \( g\). Donc le tout tend vers zéro.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rayon spectral}
%---------------------------------------------------------------------------------------------------------------------------

La chose impressionnante dans la proposition suivante est que \( \rho(A)\) est définit indépendamment du choix de la norme sur \( \eM(n,\eK)\) ou sur \( \eK\). Lorsque nous écrivons \( \| A \|\), nous disons implicitement qu'une norme a été choisie sur \( \eK\) et que nous avons pris la norme subordonnée sur \( \eM(n,\eK)\).
\begin{proposition}[\cite{ooETMNooSrtWet}]      \label{PROPooWZJBooTPLSZp}
	Soit \( A\) une matrice de \( \eM(n,\eR)\) ou \( \eM(n,\eC)\). Alors
	\begin{equation}
		\rho(A)\leq \| A \|.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous devons séparer les cas suivant que le corps de base soit \( \eR\) ou \( \eC\).

	\begin{subproof}
		\item[Pour \( A\in \eM(n,\eC)\)]
		Soit \( \lambda\) une valeur propre de \( A\) telle que \( | \lambda |\) soit la plus grande. Nous avons donc \( \rho(A)=| \lambda |\). Soit un vecteur propre \( u\in \eC^n\) pour la valeur propre \( \lambda\). En prenant la norme sur l'égalité \( Au=\lambda u\), et en utilisant le lemme~\ref{LEMooIBLEooLJczmu},
		\begin{equation}
			| \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
		\end{equation}
		Donc \( | \lambda |\leq \| A \|\) et \( \rho(A)\leq\| A \|\).

		\item[Pour \( A\in \eM(n,\eR)\)]

		L'endroit qui coince dans le raisonnement fait pour \( \eM(n,\eC)\) est que certes \( A\in \eM(n,\eR)\) possède une plus grande valeur propre en module et qu'un vecteur propre lui est associé. Mais ce vecteur propre est à priori dans \( \eC^n\), et non dans \( \eR^n\). Nous pouvons donc écrire \( Au=\lambda u\), mais pas \( \| Au \|=| \lambda |\| u \|\) parce que nous ne savons pas quelle norme prendre sur \( \eC^n\).

		Il n'est pas certain que nous ayons une norme sur \( \eC^n\) qui se réduit sur \( \eR^n\) à celle choisie implicitement dans l'énoncé. Nous allons donc ruser un peu.

		Soit une norme \( N\) sur \( \eC^n\)\footnote{Il y en a plein, par exemple celle du produit scalaire \( \langle x, y\rangle =\sum_kx_k\bar y_k\).}. Nous nommons également \( N\) la norme subordonnée sur \( \eM(n,\eC)\) et la norme restreinte sur \( \eM(n,\eR)\). Vu que \( N\) est une norme sur \( \eM(n,\eR)\) et que ce dernier est de dimension finie, le théorème~\ref{ThoNormesEquiv} nous indique que \( N\) est équivalente à \( \| . \|\). Il existe donc \( C>0\) tel que
		\begin{equation}        \label{EQooBNWMooNgnMxC}
			N(B)\leq C\| B \|
		\end{equation}
		pour tout \( B\in \eM(n,\eR)\). Nous avons maintenant
		\begin{equation}
			\rho(A)^m\leq N(A^m)\leq C\| A^m \|\leq C\| A \|^m.
		\end{equation}
		Justifications
		\begin{itemize}
			\item Par la proposition~\ref{PROPooKLFKooSVnDzr}.
			\item Parce que \( A^m\in \eM(n,\eR)\) et la relation \eqref{EQooBNWMooNgnMxC}.
			\item Par itération du lemme~\ref{LEMooFITMooBBBWGI}.
		\end{itemize}

		Nous avons donc \( \rho(A)\leq C^{1/m}\| A \|\) pour tout \( m\in\eN\). En prenant \( m\to \infty\) et en tenant compte de \( C^{1/m}\to 1\) nous trouvons \( \rho(A)\leq \| A \|\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{ooETMNooSrtWet}]        \label{LEMooGBLJooCPvxNl}
	Soit \( A\in \eM(n,\eK)\) avec \( \eK=\eR\) ou \( \eC\). Soit \( \epsilon>0\). Il existe une norme algébrique sur \( \eM(n,\eK)\) telle que
	\begin{equation}
		N(A)\leq \rho(A)+\epsilon.
	\end{equation}
\end{lemma}

\begin{proof}
	Soit par le lemme~\ref{LemSchurComplHAftTq} une matrice inversible \( U\) telle que \( T=UAU^{-1}\) soit triangulaire supérieure, avec les valeurs propres sur la diagonale. Notons que même si \( A\in \eM(n,\eR)\), les matrices \( U\) et \( T\) sont à priori complexes.

	Soit \( s\in \eR\) ainsi que les matrices
	\begin{equation}
		D_s=\diag(1,s^{-1},s^{-2},\ldots, s^{1-n})
	\end{equation}
	et \( T_s=D_sTD_s^{-1}\). Nous fixerons un choix de \( s\) plus tard.

	La norme que nous considérons est :
	\begin{equation}
		N(B)=\| (D_sU)B(D_sU)^{-1} \|_{\infty}
	\end{equation}
	où \( \| . \|_{\infty}\) est la norme sur \( \eM(,n\eK)\) subordonnée à la norme \( \| . \|_{\infty}\) sur \( \eK^n\) dont nous avons déjà parlé dans l'exemple~\ref{EXooXPXAooYyBwMX}. Cela est bien une norme parce que
	\begin{itemize}
		\item Nous avons \( \| B \|_{\infty}=0\) si et seulement si \( B=0\), et vu que \( (D_sU)\) est inversible nous avons \( (D_sU)B(D_sU)^{-1}=0\) si et seulement si \( B=0\).
		\item \( N(\lambda B)=| \lambda |N(B)\).
		\item Pour l'inégalité triangulaire :
		      \begin{subequations}
			      \begin{align}
				      N(B+C) & =\| (D_sU)B(D_sU)^{-1}+(D_sU)C(D_sU)^{-1} \|_{\infty}                     \\
				             & \leq  \| (D_sU)B(D_sU)^{-1}\|_{\infty} +\| (D_sU)C(D_sU)^{-1} \|_{\infty} \\
				             & =N(B)+N(C).
			      \end{align}
		      \end{subequations}
	\end{itemize}

	En ce qui concerne la matrice \( A\) elle-même, nous avons
	\begin{equation}
		N(A)=\| (D_sU)A(D_sU)^{-1} \|_{\infty}=\| T_s \|_{\infty}.
	\end{equation}
	C'est le moment de se demander comment se présente la matrice \( T_s\). En tenant compte du fait que \( (D_s)_{ik}=\delta_{ik}s^{1-i}\) nous avons
	\begin{equation}
		(T_s)_{ij}=\sum_{kl}(D_s)_{ik}T_{kl}(D^{-1}_s)_{lj}=T_{ij}s^{j-i}.
	\end{equation}
	La matrice \( T\) est encore triangulaire supérieure avec les valeurs propres de \( A\) sur la diagonale. Les éléments au-dessus de la diagonale sont tous multipliés par au moins \( s\). Il est donc possible de choisir \( s\) suffisamment petit pour avoir\quext{Il me semble qu'il manque un module dans \cite{ooETMNooSrtWet}.}
	\begin{equation}        \label{EQooSIEIooTWAXQD}
		\sum_{j=i+1}^n| (T_s)_{ij} |<\epsilon
	\end{equation}
	Avec ce choix, la formule~\ref{EQooPLCIooVghasD} donne
	\begin{equation}
		N(T_s)\leq\max_i\sum_k| (T_s)_{ik} |\leq \epsilon+\rho(A).
	\end{equation}
	En effet le \( \epsilon\) vient de la somme sur toute la ligne sauf la diagonale (c'est-à-dire la partie \( k\neq i\)) et du choix \eqref{EQooSIEIooTWAXQD} pour \( s\). Le \( \rho(A)\) provient du dernier terme de la somme (le terme sur la diagonale) qui est une valeur propre de \( A\), donc majorable par \( \rho(A)\).

	Nous devons encore prouver que \( N\) est une norme algébrique. Pour cela nous allons montrer qu'elle est subordonnée à la norme
	\begin{equation}
		\begin{aligned}
			n\colon \eK^n & \to \eR^+                       \\
			v             & \mapsto \| (UD_s)v \|_{\infty}.
		\end{aligned}
	\end{equation}
	Cela sera suffisant pour avoir une norme algébrique par le lemme~\ref{LEMooFITMooBBBWGI}. La norme \( n\) sur \( \eK^n\) produit la norme suivante sur \( \eM(n,\eK)\) :
	\begin{equation}
		n(B)=\sup_{v\neq 0}\frac{ n(B) }{ n(v) }=\sup_{v\neq 0}\frac{ \| (UD_s)Bv \|_{\infty} }{ \| UD_sv \|_{\infty} }.
	\end{equation}
	Vu que \( UD_s\) est inversible nous pouvons effectuer le changement de variables \( v\mapsto (UD_s)^{-1} v\) pour écrire
	\begin{equation}
		n(B)=\sup_{v\neq 0}  \frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| (UD_s)(UD_s)^{-1}v \|_{\infty} }=\sup_{v\neq 0}\frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| v \|_{\infty} }=\| (UD_s)B(UD_s)^{-1} \|_{\infty}=N(B).
	\end{equation}
\end{proof}

\begin{proposition}     \label{PROPooYPLGooWKLbPA}
	Si \( A\in \eM(n,\eR)\) alors \( \rho(A)^m=\rho(A^m)\) pour tout \( m\in \eN\).
\end{proposition}

\begin{proof}
	La matrice \( A\) peut être vue dans \( \eM(n,\eC)\) et nous pouvons lui appliquer le corolaire~\ref{CORooTPDHooXazTuZ} :
	\begin{equation}        \label{EQooJJIYooDBacjn}
		\Spec(A^k)=\{ \lambda^k\tq \lambda\in\Spec(A) \}.
	\end{equation}
	À noter qu'il n'y a pas de magie : le spectre de la matrice réelle \( A\) est déjà défini en voyant \( A\) comme matrice complexe. Le spectre dont il est question dans \eqref{EQooJJIYooDBacjn} est bien celui dont on parle dans la définition du rayon spectral.

	Nous avons ensuite :
	\begin{subequations}
		\begin{align}
			\rho(A^k) & =\max\{ | \lambda |\tq \lambda\in\Spec(A^k) \} \\
			          & =\max\{ | \lambda^k |\tq \lambda\in\Spec(A) \} \\
			          & =\max\{ | \lambda |^k\tq\lambda\in\Spec(A) \}  \\
			          & =\rho(A)^k.
		\end{align}
	\end{subequations}
\end{proof}



\begin{proposition}     \label{PROPooXEQLooHvzVVm}
	Soient des espaces vectoriels normés \( V\) de dimension \( n\) et \( W\) de dimension \( m\) sur \( \eK\) (corps normé). Nous considérons une base \( \{ e_s \}_{s=1,\ldots, n}\) de \( V\) et \( \{ f_{\alpha} \}_{\alpha=1,\ldots, m}\) de \( W\).

	Alors l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon \eM(n\times m,\eK) & \to \aL(V,W)                            \\
			\psi(A)v                      & =\sum_{s\alpha}A_{s\alpha}v_sf_{\alpha}
		\end{aligned}
	\end{equation}
	est un isomorphisme d'espaces topologiques.

	Pour rappel, la topologie sur \( \eM(n,\eK)\) est donnée par la définition \ref{DEFooCQHDooYpUAhG}.
\end{proposition}

\begin{proof}
	Nous savons déjà que \( \psi\) est une bijection. De plus, elle est linéaire et donc continue par la proposition \ref{PROPooADPDooOtukQP}. En ce qui concerne son inverse, c'est également une application linéaire (lemme \ref{LEMooLGEHooVEEoiU}); elle est alors également continue.
\end{proof}


\begin{definition}[\cite{ooAISYooXtUafT}]      \label{DEFooTLQUooJvknvi}
	Soient \( E\) et \( F\) deux espaces vectoriels normés.
	\begin{itemize}
		\item
		      L'ensemble des applications linéaires \( E\to F\) est noté \( \aL(E,F)\).
		\item Un \defe{morphisme}{morphisme!espace vectoriel normé} est une application linéaire \( E\to F\) continue pour la topologie de la norme opérateur. Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que la continuité était équivalente à être bornée. L'ensemble des morphismes est noté \( \cL(E,F)\)\nomenclature[B]{\( \cL(E,F)\)}{applications linéaires bornées (continues)}.
		\item
		      Un \defe{isomorphisme}{isomorphisme!espace vectoriel normé} est un morphisme continu inversible dont l'inverse est continu. Nous notons \( \GL(E,F)\) l'ensemble des isomorphismes entre \( E\) et \( F\).
	\end{itemize}
\end{definition}

\begin{proposition}     \label{PROPooDRHMooYzXbkl}
	Soit un espace vectoriel normé \( V\) de dimension finie. Soit une suite d'opérateurs \( T_n\in \End(V)\). Si \( \{ e_i \}\) est une base de \( V\) et si \( T_n(e_i)\stackrel{V}{\longrightarrow}e_i\) pour tout \( i\), alors \( T_n\stackrel{\End(V)}{\longrightarrow}\id\).
\end{proposition}

\begin{proof}
	Nous utilisons l'application \( \psi\colon \eM(n,\eK)\to \End(V)\) définie en \ref{DEFooJVOAooUgGKme}. Elle nous permet d'écrire
	\begin{equation}
		T_n(x)=\sum_{kl}\psi^{-1}(T_i)_{kl}x_le_k,
	\end{equation}
	que nous allons particulariser à \( x=e_j\). Nous avons
	\begin{subequations}
		\begin{align}
			e_j & =\lim_{n\to \infty} T_n(e_j)                                   \\
			    & =\lim_{n\to \infty} \sum_{kl}\psi^{-1}(T_n)_{kl}\delta_{jl}e_k \\
			    & =\sum_{k}\big( \lim_{n\to \infty} \psi^{-1}(T_n)_{kj} \big)e_k
		\end{align}
	\end{subequations}
	En identifiant les coefficients de \( e_j\), on trouve
	\begin{equation}
		\lim_{n\to \infty} \psi^{-1}(T_n)_{kj}=\delta_{kj}.
	\end{equation}
	Pour chaque \( k\) et \( l\), à gauche nous avons une limite dans \( \eK\). Vue la topologie sur \( \eM(n,\eK)\)\footnote{Toutes les normes sur un espace vectoriel de dimension finie sont équivalentes (théorème \ref{ThoNormesEquiv}). Sur \( \eM(n,\eK)\), nous avons convenu dans la définition \ref{DEFooCQHDooYpUAhG} de considérer la norme maximum.}, nous pouvons écrire cela comme une limite dans \( \eM(n,\eK)\) :
	\begin{equation}
		\lim_{n\to \infty} \psi^{-1}(T_n)=\mtu.
	\end{equation}
	Nous savons que \( \psi^{-1}\) est continue (proposition \ref{PROPooXEQLooHvzVVm}) de telle sorte que nous pouvons la commuter avec la limite :
	\begin{equation}
		\mtu= \lim_{n\to \infty} \psi^{-1}(T_n)=\psi^{-1}\big( \lim_{n\to \infty T_n}  \big).
	\end{equation}
	Appliquant maintenant \( \psi\) des deux côtés, \( \psi(\mtu)=\id\) et
	\begin{equation}
		\id=\lim_{n\to \infty} T_n.
	\end{equation}
\end{proof}


Le point important de la définition~\ref{DEFooTLQUooJvknvi} est la continuité. En dimension infine, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

Si \( V\) est un espace vectoriel normé, nous avons déjà défini son dual topologique \( V'\) comme étant l'ensemble des applications linéaires continues \( V\to \eC\) ou \( V\to \eR\) selon le corps de base de \( V\). C'est la définition \ref{DefJPGSHpn}.

\begin{proposition}
	Soient un espace vectoriel normé \( V\) et un élément \( v\in V\) vérifiant \( \| v \|=1\). Il existe une forme \( \varphi\in V'\) telle que \( \| \varphi \|=1\) et \( \varphi(v)=1\).
\end{proposition}

\begin{proof}
	Nous allons utiliser le théorème de la base incomplète \ref{THOooOQLQooHqEeDK}. Pour cela nous considérons \( I=V\) et la partie clairement génératrice \( G=\{ e_i=i \}_{i\in I}\) (si vous avez bien suivi, \( G=V\) en fait; rien de bien profond). Nous considérons ensuite \( I_0=\{ v \}\). Le théorème de la base incomplète nous donne l'existence de \( I_1\) tel que \( I_0\subset I_I\subset I\) et tel que \( B=\{ e_i \}_{i\in I_1}\) est une base.

	Tout cela pour dire que \( B=\{ e_i \}_{i\in I_1}\) est une base contenant \( v\). Nous allons aussi éventuellement redéfinir la norme de \( e_i\) pour avoir \( \| e_i \|=1\). Cette renormalisation n'affecte pas le fait que \( v\in B\).

	Nous passons maintenant à la définition de \( \varphi\colon V\to \eK\). Pour \( x\in V\) nous commençons par écrire
	\begin{equation}
		x=\sum_{j\in J}x_je_j
	\end{equation}
	et nous posons
	\begin{equation}
		\varphi(x)=\begin{cases}
			x_v & \text{si } v\in J \\
			0   & \text{sinon. }
		\end{cases}
	\end{equation}
	Cette définition a un sens par la partie unicité de la proposition \ref{PROPooEIQIooXfWDDV} de décomposition d'un élément dans une base.

	Nous devons calculer la norme de \( \varphi\). Par la proposition \ref{DefNFYUooBZCPTr}\ref{ITEMooUQPRooYQGZzu} nous avons
	\begin{equation}        \label{EQooEFLLooOWPSev}
		\| \varphi \|=\sup_{\| x \|=1}| \varphi(x) |.
	\end{equation}
	Avec \( x=v\) nous avons \( \varphi(x)=1\) et donc \( \| \varphi \|\geq 1\).

	Nous devons encore montrer que \( \| \varphi \|\leq 1\). Un élément \( x\in V\) s'écrit toujours sous la forme
	\begin{equation}
		x=\sum_{i\in J}x_je_j
	\end{equation}
	pour un certain \( J\) fini dans \( I_1\) et pour certains \( x_j\in \eK\). Pour un tel \( x\) nous avons \( \varphi(x)=x_v\). Si \( |\varphi(x)|\geq 1\), alors \( | x_v |\geq 1\), mais alors
	\begin{equation}
		\| x \|\leq \sum_{j\in J}| x_j |\| e_j \|=\sum_{j\in J}| x_j |\geq | x_v |>1,
	\end{equation}
	ce qui fait que ce \( x\) ne participe pas au supremum \eqref{EQooEFLLooOWPSev}.

	Notons que \( \varphi\) est continue (et donc bien dans \( V'\)) parce qu'elle est bornée (proposition \ref{PROPooQZYVooYJVlBd}).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
	La norme $2$ d'une matrice est liée au rayon spectral de la façon suivante :
	\begin{equation}
		\|A\|_2=\sqrt{\rho(A{^t}A)}
	\end{equation}
	ou plus généralement par \( \| A \|_2=\sqrt{\rho(A^*A)}\).
\end{theorem}

\begin{lemma}       \label{LEMooNESTooVvUEOv}
	Soit une matrice \( A\in \eM(n,\eR)\) qui est symétrique, strictement définie positive. Soient \( \lambda_{min}\) et \( \lambda_{max}\) les plus petites et plus grandes valeurs propres. Alors
	\begin{subequations}
		\begin{align}
			\| A \|_2=\lambda_{max} &  & \text{ et } &  & \|A^{-1}  \|_2=\frac{1}{ \lambda_{min} }.
		\end{align}
	\end{subequations}
\end{lemma}

\begin{proof}
	Soient les vecteurs \( v_1,\ldots, v_n\) formant une base orthonormée de vecteurs propres\footnote{Possible par le théorème spectral~\ref{ThoeTMXla}.} de \( A\). Nous notons \( v_{max}\) celui de \( \lambda_{max}\). Nous avons :
	\begin{equation}
		\| A \|_2\geq \| Av_{max} \|=| \lambda_{max} |\| v_{max} \|=| \lambda_{max} |=\lambda_{max}.
	\end{equation}
	Voilà l'inégalité dans un sens. Montrons l'inégalité dans l'autre sens. Soit \( x=\sum_ix_iv_i\) avec \( \| x \|_2=1\). Alors
	\begin{equation}
		\| Ax \|=\| \sum_ix_i\lambda_iv_i \|\leq\sqrt{ \sum_ix_i^2\lambda_i^2 }\leq \lambda_{max}\sqrt{ \sum_ix_i^2}=\lambda_{max}.
	\end{equation}

	En ce qui concerne l'affirmation pour la norme de \( A^{-1}\), il suffit de remarquer que ses valeurs propres sont les inverses des valeurs propres de \( A\).
\end{proof}

\begin{lemma}[\cite{MonCerveau, BIBooFRBHooZYBjQD}]        \label{LEMooCSBVooZzqxqg}
	Soit une matrice diagonale \( D\in \eM(n,\eC)\) dont nous notons \( \lambda_i\in \eC\) les éléments diagonaux. Alors la norme opérateur\footnote{Norme opérateur, définition \ref{DefNFYUooBZCPTr}. La notation \( \| D \|_2\) signifie la norme opérateur de \( D\colon \eC^n\to \eC^n\) où l'on a mis la norme euclidienne sur \( \eC^n\), c'est à dire la norme de la définition \ref{DEFooGUXNooXwCsrq}.} de \( D\) est donnée par
	\begin{equation}
		\| D \|_2=\max_i\{ | \lambda_i | \}.
	\end{equation}
\end{lemma}

\begin{proof}
	En plusieurs points.
	\begin{subproof}
		%TODOooMNIYooHIHdFG
		\item[Le compact]
		Vu que la partie \( \{ x\in \eC^n\tq \| x \|_2=1 \}\) est compacte, nous pouvons utiliser un maximum au lieu d'un supremum dans la définition de la norme opérateur (théorème de Weierstrass \ref{ThoWeirstrassRn}.).
		\item[Notations pour \( \eC^n\)]
		Pour se mettre d'accord sur les notations, si \( x\in \eC^n\), alors \( x=\sum_ix_ie_i\) où \( e_1\in \eC^n\) est le vecteur \( (1, 0,\ldots, 0)\). C'est un vecteur de base de \( \eC^n\) comme espace vectoriel sur \( \eC\). Et d'ailleurs \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( \eC^n\).

		\item[Norme dans \( \eC^n\)]
		Lorsque \( A\) est un opérateur sur \( \eC^n\), nous avons
		\begin{equation}
			\| Ax \|_2=\left( \sum_i| (Ax)_i |^2 \right)^{1/2}
			=\left( \sum_i| \sum_jA_{ij}x_j |^2 \right)^{1/2}.
		\end{equation}
		Nous avons utilisé les conventions \eqref{EQooAXRJooUwHbjB}.
		\item[Le calcul]
		Si c'est bon pour vous, je me lance dans le calcul :
		\begin{subequations}
			\begin{align}
				\| D \|_2 & =\max_{\| x \|_2=1}\| Dx \|_2                                                                              \\
				          & =\max_{\| x \|_2=1}   \left( \sum_{i=1}^n\lambda_ix_i \right)                                              \\
				          & \leq \max_{\| x \|_2=1}\max_i| \lambda_i |\underbrace{\left( \sum_{i=1}^nx_i^2 \right)^{1/2}}_{=\| x \|_2} \\
				          & =\max_i| \lambda_i |.
			\end{align}
		\end{subequations}
	\end{subproof}
	L'inégalité \( \| D \|_2\leq \max_i| \lambda_i |\) est prouvée. Nous démontrons à présent l'inégalité dans l'autre sens. Appliquons \( D\) au vecteur de base \( e_i\) : \( De_i=\lambda_ie_i\). Donc
	\begin{equation}
		\| D \|_2\geq \| De_i \|_2=| \lambda_ie_i |=| \lambda_i |.
	\end{equation}
	Cela étant valable pour tout \( i\), nous avons \( \| D \|_2\geq\max_i| \lambda_i |\).
\end{proof}

\begin{proposition} \label{PropMAQoKAg}
	La fonction
	\begin{equation}
		\begin{aligned}
			f\colon \eM(n,\eR)\times \eM(n,\eR) & \to \eR           \\
			(X,Y)                               & \mapsto \tr(X^tY)
		\end{aligned}
	\end{equation}
	est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
	Il faut vérifier la définition~\ref{DefVJIeTFj}.
	\begin{itemize}
		\item La bilinéarité est la linéarité de la trace.
		\item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
		\item L'application \( f\) est définie positive parce que si \( X\in \eM\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
	\end{itemize}
\end{proof}

\begin{example}
	Soit $m=n$, un point $\lambda$ dans $\eR$ et $T_{\lambda}$ l'application linéaire définie par $T_{\lambda}(x)=\lambda x$. La norme de $T_{\lambda}$ est alors
	\[
		\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
	\]
	Notez que $T_{\lambda}$ n'est rien d'autre que l'homothétie de rapport $\lambda$ dans $\eR^m$.
\end{example}

\begin{example}
	Toutes les isométries ont norme \( 1\). En effet si \( T\) est une isométrie, $\| Tx \|=\| x \|$. En ce qui concerne la norme de $T$ nous avons alors
	\begin{equation}
		\| T \|=\sup_{x\in\eR^2}\frac{ \| T(x) \| }{ \| x \| }=\sup_{x\in\eR^2}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
\end{example}

\begin{example}
	Soit $m=n$, un point $b$ dans $\eR^m$ et $T_b$ l'application linéaire définie par $T_b(x)=b\cdot x$ (petit exercice : vérifiez qu'il s'agit vraiment d'une application linéaire).  La norme de $T_b$ satisfait les inégalités suivantes
	\[
		\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^n}\|x\cdot x\|_{\eR^n}\leq\|b \|_{\eR^n},
	\]
	\[
		\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left\|b\cdot \frac{b}{\|b \|_{\eR^n}}\right\|_{\eR^n}=\|b \|_{\eR^n},
	\]
	donc $\|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}$.
\end{example}

\begin{proposition}
	Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
	Soit $x$ un point dans $\eR^m$. Nous devons vérifier l'égalité
	\begin{equation}
		\lim_{h\to 0_m}T(x+h)=T(x).
	\end{equation}
	Cela revient à prouver que $\lim_{h\to 0_m}T(h)=0$, parce que $T(x+h)=T(x)+T(h)$. Nous pouvons toujours majorer $\|T(h)\|_n$ par $\|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}$ (lemme~\ref{LEMooIBLEooLJczmu}). Quand $h$ s'approche de $ 0_m $ sa norme $\|h\|_m$ tend vers $0$, ce que nous permet de conclure parce que nous savons que de toutes façons, $\| T \|_{\aL}$ est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition~\ref{PROPooQZYVooYJVlBd}.

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooHXJAooGaDtme}
	Soit \( A\in\GL(n,\eC)\). La suite\footnote{Oui, c'est avec \( n\in \eZ\). Vu que \( A\) est dans \( \GL\), elle est inversible, donc pas de soucis à considérer \( A^{-1}\).} \( (A^n)_{n\in \eZ}\) est bornée\footnote{Nous conisdérons sur \( \GL(n,\eC)\) la norme opérateur dérivant de la norme euclidiene sur \( \eC^n\) donnée par la formule \eqref{EQooZIXRooMGcsXY}.} si et seulement si \( A\) est diagonalisable\footnote{Définition \ref{DefCNJqsmo}.} et \( \Spec(A)\subset \gS^1\).
\end{proposition}

\begin{proof}
	Nous commençons par supposer que \( A\) est diagonalisable. Nous nommons \( \lambda_i\) ses valeurs propres. La proposition \ref{PROPooDEETooSOMiGO} permet de considérer une matrice inversible \( Q\) telle que \( A=Q^{-1}DQ\) où \( D\) est la matrice diagonale \( D_{ii}=\lambda_i\). Nous avons donc aussi
	\begin{equation}
		A^n=Q^{-1}D^nQ.
	\end{equation}
	La matrice \( D^n\) est diagonale et \( D^n_{ii}=\lambda_i^n\). Par la la proposition \ref{PROPooUMVGooIrhZZg}\ref{ITEMooFXKYooUOXbwH}, nous avons \( | \lambda_i |^n=1\).

	\begin{subproof}
		\item[Pour \( n\geq 0\)]
		La norme matricielle étant une norme d'algèbre\footnote{Lemme \ref{LEMooFITMooBBBWGI}.},
		\begin{equation}
			\| A^n \|_2=\| Q^{-1}D^nQ \|_2\leq \| Q^{-1} \|_2\| D^n \|_2\| Q \|_2.
		\end{equation}
		En ce qui concerne la norme \( \| D \|_2\), nous avons le lemme \ref{LEMooCSBVooZzqxqg} qui nous annonce que \( \| D \|_2=\max_i| \lambda_i |\). Donc notre cas nous avons donc \( \| D \|_2=1\) et donc
		\begin{equation}
			\| A^n \|_2\leq \| Q^{-1} \|_2\| Q \|_2.
		\end{equation}
		Autrement dit, la suite \( (A^n)_{n\in \eN}\) est majorée en norme par le nombre \( \| Q^{-1} \|_2\| Q \|_2\).
		\item[Pour \( n\leq 0\)]
		Alors il suffit de poser \( B=A^{-1}\). La matrice \( B\) est autant diagonaisable que \( A\) et le même raisonnement s'applique : il existe une matrice inversible \( P\) telle que
		\begin{equation}
			\| B^n \|_2\leq \| P^{-1} \|_2\| P \|_2.
		\end{equation}
		\item[Pour tous les \( n\)]
		La suite \( (A^n)_{n\in \eZ}\) est donc majorée par le maximum entre \( \| P^{-1} \|_2\| P \|\) et \( \| Q^{-1} \|_2\| Q \|\).
	\end{subproof}

	\begin{center}
		Dans l'autre sens, maintenant.
	\end{center}

	Vu que nous travaillons sur \( \eC\), le polynôme caractéristique de \( A\) est scindé et la réduction de Jordan\footnote{Théorème \ref{ThoGGMYooPzMVpe}.} s'applique. Nous considérons une matrice inversible \( Q\) telle que \( A=Q^{-1} MQ\) avec
	\begin{equation}
		M=\begin{pmatrix}
			\lambda_1\mtu+N_1 &        &                   \\
			                  & \ddots &                   \\
			                  &        & \lambda_s\mtu+N_s
		\end{pmatrix},
	\end{equation}
	où les \( N_i\) sont nilpotents. Notez qu'ici les «\( \mtu\)» sont de différentes tailles.

	\begin{subproof}
		\item[Juste un bloc]
		Nous considérons un bloc de Jordan \( \lambda\mtu+N\). Nous supposons que la suite \( (\lambda\mtu+N)^n\) est bornée, et nous allons montrer que \( | \lambda |=1\) et \( N=0\).

		En utilisant la formule du binôme, en nommant \( s\) la borne, et en nommant \( r\) le plus petit entier tel que \( N^r=0\), pour \( n>r\) nous avons
		\begin{equation}
			(\lambda\mtu+N)^n=\sum_{k=0}^{r-1}\binom{ n }{ k }\lambda^{n-k}N^k<s.
		\end{equation}
		Le lemme \ref{LEMooKPWKooOacXju} fait que la partie \( \{ N^k \}\) est libre. En utilisant le lemme \ref{LEMooGCJEooOAynZW}, nous en déduisons que
		\begin{equation}    \label{EQooZWGCooGEsobY}
			\| \binom{ n }{ k }\lambda^{n-k}N^k \|<s
		\end{equation}
		pour tout \( n>r\) et pour tout \( k\leq n\).

		En particulier, pour tout \( n\) nous pouvons considérer le terme \( k=0\). Cela donne
		\begin{equation}
			| \lambda |^n<s
		\end{equation}
		qui implique \( | \lambda |\leq 1\).

		Nous avons donc deux possibilités : \( | \lambda |<1\) et \( | \lambda |=1\). Supposons \( | \lambda |=1\), et considérons l'inéquation \eqref{EQooZWGCooGEsobY} avec \( k=1\) : \( \| n\lambda^{n-1}N \|<s\). Cela implique que
		\begin{equation}
			n\| N \|<s
		\end{equation}
		pour tout \( n\). Cela n'est possible que si \( \| N \|=0\) parce que \( \eR\) est archimédien (théorème \ref{ThoooKJTTooCaxEny}). Nous restons donc avec les deux possibilités
		\begin{itemize}
			\item \( | \lambda |<1\)
			\item \( | \lambda |=1\) et \( N=0\).
		\end{itemize}

		Nous nous tournons maintenant sur la contrainte que \( (\lambda\mtu+N)^n\) doive rester borné pour \( n<0\). Nous avons
		\begin{equation}
			\lambda\mtu+N=\lambda(\mtu+\lambda^{-1}N),
		\end{equation}
		et nous pouvons appliquer la proposition~\ref{PROPooWTFWooXHlmhp} à l'opérateur nilpotent \( -\lambda^{-1} N\) pour avoir
		\begin{subequations}
			\begin{align}
				\big( \mtu-(-\lambda^{-1}N) \big)^{-1} & =\sum_{k=0}^{\infty}(-\lambda)^{-k}N^k      \\
				                                       & =\mtu+\sum_{k=1}^{\infty}(-\lambda)^{-k}N^k \\
				                                       & =\mtu+\sum_{k=1}^{r-1}(-\lambda)^{-k}N^k.
			\end{align}
		\end{subequations}
		Ceci pour dire que \( (\lambda\mtu+N)^{-1}=\lambda^{-1}(\mtu+\lambda^{-1}N')\) pour une autre matrice nilpotente\footnote{Notez que la somme part de \( k=1\), sinon ce serait raté pour la nipolence de \( N'\).} \( N'=\sum_{k=1}^{r-1}(-\lambda)^{-k}N^k\). Le travail déjà fait, appliqué à \( \lambda^{-1}\) et \( N'\), nous donne deux possibilités :
		\begin{itemize}
			\item \( | \lambda^{-1} |<1\)
			\item \( | \lambda^{-1} |=1\) et \( N'=0\).
		\end{itemize}
		La possibilité \( | \lambda^{-1} |<1\) est exclue parce qu'elle impliquerait \( | \lambda |>1\) qui avait déjà été exclu. Il ne reste donc que la possibilité \( | \lambda |=1\) et \( N=N'=0\).
		\item[Pour la matrice \( M\)]
		Nous supposons que \( \{ M^k \}\) est borné : \( \| M^k \|\leq s\) pour tout \( s\). En utilisant le lemme \ref{LEMooHGCKooBzfAtg} et la proposition \ref{PROPooJUYCooHnlFef}, pour tout \( n\) et pour tout \( i\) nous avons :
		\begin{equation}
			\| (\mtu+\lambda_iN_i)^k \|<s.
		\end{equation}
		Nous appliquons ce que nous venons de faire pour les blocs et nous obtenons \( | \lambda_i |=1\) et \( N_i=0\).
		\item[La matrice \( A\)]
		Nous pouvons enfin parler de la matrice \( A=Q^{-1}MQ\). Nous avons \( A^n=Q^{-1}M^nQ\), et donc aussi
		\begin{equation}
			M^n=QA^nQ^{-1}.
		\end{equation}
		En ce qui concerne la norme, si la suite \( (A^n)\) est bornée par le réel \( s\), alors
		\begin{equation}
			\| M^n \|=\| QA^nA^{-1} \|\leq \| Q \|\| Q^{-1} \|\| A^n \|\leq s\| Q \|\| Q^{-1} \|.
		\end{equation}
		Donc la suite \( (M^n)\) est bornée et nous pouvons appliquer ce la partie déjà faite sur \( M\). Nous avons donc
		\begin{equation}
			A=Q^{-1}MQ=Q^{-1}\begin{pmatrix}
				\lambda_1 &        &           \\
				          & \ddots &           \\
				          &        & \lambda_s
			\end{pmatrix}Q
		\end{equation}
		avec \( | \lambda |_i=1\). Nous avons prouvé que \( A\) est diagonalisable et que \( \Spec(A)\subset S^1\).
	\end{subproof}
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Géométrie dans l'espace}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{normaltext}
	Les notions de droites, plans et parallélisme sont des notions vectorielles qui auraient pu être traitées beaucoup plus haut. La chose qui rend la géométrie un peu piquante est la notion de perpendicularité. Cette notion demande un produit scalaire et fait intervenir ici et là des polynômes du second degré. Travailler avec le second degré demande la connaissance des racines carrés\footnote{Définition \ref{DEFooGQTYooORuvQb}.} et donc d'un peu de topologie réelle et de continuité. La résulution dans \( \eR\) du polynôme du seconde degré est la propsition \ref{PROPooEZIKooKjJroH}.
\end{normaltext}

\begin{definition}      \label{DEFooVTXWooVXfUnc}
	Soient deux espaces vectoriels \( E\) et \( V\). Une application \( f\colon E\to V\) est \defe{affine}{application affine} si il existe une application linéaire \( u\colon E \to V\) et un élément \( v\in V\) tel que
	\begin{equation}
		f(x)=u(x)+v
	\end{equation}
	pour tout \( x\in E\).
\end{definition}

\begin{definition}      \label{DEFooTQIFooKcloeY}
	Soit un espace vectoriel \( E\).
	\begin{enumerate}
		\item
		      Une \defe{droite vectorielle}{droite vectorielle} dans \( E\) est un sous-espace vectoriel de dimension \( 1\).
		\item
		      Une \defe{droite affine}{droite affine} est une partie de \( E\) de la forme \( a+V\) où \( a\in E\) et \( V\) est un sous-espace vectoriel de dimension \( 1\) de \( E\).
		\item
		      Un \defe{plan vectoriel}{plan vectoriel} est un sous-espace vectoriel de dimension \( 2\).
		\item
		      Une partie \( P\) est un \defe{plan affine}{plan affine} si il existe un \( v\in E\) tel que \( P-v\) est un plan vectoriel.
	\end{enumerate}
	Le plus souvent, nous parlerons de «droite» et «plan» sans préciser «vectoriel» ou «affine». Dans ces cas, le plus souvent, ce sera «affine».
\end{definition}

\begin{definition}[Perpendiculaires et parallèles]
	Deux notions importantes.
	\begin{enumerate}
		\item
		      Nous disons que les droites \( a+V\) et \( b+W\) sont \defe{parallèles}{droites parallèles} lorsque \( V=W\).
		\item
		      Nous disons que les droites \( a+V\) et \( b+W\) sont \defe{perpendiculaires}{droites perpendiculaires} si pour tout \( v\in V\) et \( w\in W\) nous avons \( v\cdot w=0\).
	\end{enumerate}
	Vous noterez que le parallélisme est une notion vectorielle alors que la perpendicularité dépend du produit scalaire; c'est une notion comme qui dirait «métrique».
\end{definition}

\begin{proposition}     \label{PROPooADJNooMyXUxG}
	Les propriétés usuelles.
	\begin{enumerate}
		\item
		      Deux droites parallèles ayant une intersection sont confondues.
		\item
		      Le parallélisme est une relation d'équivalence sur l'ensemble des droites de \( E\).
		\item
		      Si la droite \( d_1\) est parallèle à la droite \( d_2\), alors une droite est perpendiculaire à \( d_1\) si et seulement si elle est perpendiculaire à \( d_2\).
	\end{enumerate}
\end{proposition}

\begin{lemma}       \label{LEMooRLFQooJADark}
	Deux droites perpendiculaires ont un unique point d'intersection.
\end{lemma}

\begin{proposition}     \label{PROPooPWNWooYuyrOc}
	Soient une droite \( d\) et un point \( p\).
	\begin{enumerate}
		\item
		      Il existe une unique droite parallèle à \( d\) contenant \( p\).
		\item
		      Il existe une unique droite perpendiculaire à \( d\) contenant \( p\).
	\end{enumerate}
\end{proposition}

\begin{lemma}       \label{LEMooQQFFooEZYeck}
	Si \( D\) est une droite et si \( a,b\in D\), alors \( D-a=D-b\) et \( D-a\) est une droite vectorielle.
\end{lemma}

\begin{proof}
	Vu que \( D\) est une droite, il existe \( v\in V\) tel que \( D-v\) soit une droite vectorielle que nous notons \( L\). Nous allons montrer que \( D-a=D-v\). Vu que \( a\) est arbitraire, cela suffit.

	\begin{subproof}
		\item[\( D-a\subset D-v\)]
		Un élément de \( D-a\) est de la forme \( x-a\) avec \( x\in D\). Nous écrivons \( x-a\) sous la forme \( y-v\) et nous espérons que \( y\in D\). Allons-y : d'abord nous isolons \( y\) dans \( x-a=y-v\) :
		\begin{subequations}
			\begin{align}
				y=x-a+v=(x-v)-(a-v)+v.
			\end{align}
		\end{subequations}
		Vu que \( x-v\) et \( a-v\) sont des éléments de \( L\), la somme est dans \( L\) et donc \( y=l+v\) pour un certain élément de \( l\in L\). Nous avons donc prouvé que \( y\in D\) et donc que \( x-a=y-v\in D-v\).
		\item[\( D-v\subset D-a\)]
		Nous notons \( x-v\) un élément générique que \( D-v\) (\( x\in D\)). En posant \( y-a=x-v\), nous trouvons
		\begin{equation}
			y=x-v+a=\underbrace{x-v}_{\in L}+\underbrace{(a-v)}_{\in L}+v
		\end{equation}
		Donc \( y\in D\) et \( x-v=y-a\in D-a\).
	\end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooNTHVooWWyafJ}
	L'image d'une droite par une application affine\footnote{Définition \ref{DEFooVTXWooVXfUnc}.} est une droite.
\end{proposition}

\begin{lemma}
	À propos de droites.
	\begin{enumerate}
		\item       \label{ITEMooYQCIooOrhRwj}
		      Si \( L\) est une droite vectorielle, alors pour tout \( a\neq 0\) dans \( L\), nous avons \( L=\Image(f)\) où \( f\) est l'application linéaire donnée par
		      \begin{equation}
			      \begin{aligned}
				      f\colon \eK & \to V              \\
				      \lambda     & \mapsto \lambda a.
			      \end{aligned}
		      \end{equation}
		\item       \label{ITEMooZIGMooGruFMP}
		      Si \( D\) est une droite affine, alors pour tout \( a\neq b\) sur \( D\) nous avons \( D=\Image(f)\) où \( f\) est l'application affine donnée par
		      \begin{equation}
			      \begin{aligned}
				      g\colon \eK & \to V                   \\
				      \lambda     & \mapsto a+\lambda(b-a).
			      \end{aligned}
		      \end{equation}
	\end{enumerate}
\end{lemma}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\item[Pour \ref{ITEMooYQCIooOrhRwj}]
		Vu que \( L\) est un sous-espace de dimension \( 1\), il possède une base contenant un unique élément, disons \( \{ b \}\). En particulier \( a=\mu b\) pour un certain \( \mu\in \eK\). Si \( x\in L\) nous avons \( x=\lambda_x b\) pour un certain \( \lambda_x\), et donc
		\begin{equation}
			x=\frac{ \lambda_x }{ \mu }a.
		\end{equation}
		Donc \( x=f(\lambda_x/\mu)\). Cela prouve que \( L\subset\Image(f)\).

		L'inclusion inverse est simplement le fait que \( \lambda a\in L\) dès que \( a\in L\) parce que \( L\) est vectoriel.
		\item[Pour \ref{ITEMooZIGMooGruFMP}]
		Le lemme \ref{LEMooQQFFooEZYeck} nous indique qu'il existe une droite vectorielle \( L\) telle que \( D-x=L\) pour tout \( x\in D\).
		\begin{subproof}
			\item[\( D\subset\Image(g)\)]
			Nous nommons \( f\colon \eK\to V\) l'application linéaire qui donne \( L\). Vu que \( b-a\in L\) nous avons
			\begin{equation}
				f(\lambda)=\lambda(b-a),
			\end{equation}
			et tout élément de \( L\) est de la forme \( f(\lambda)\). Nous avons aussi \( D=L+a\); donc un élément de \( D\) est de la forme \( f(\lambda)+a\) et donc de la forme \( \lambda(b-a)+a=g(\lambda)\).
			\item[\( \Image(g)\subset D\)]
			Un élément de \( \Image(g)\) est de la forme \( a+\lambda(b-a)\) avec \( \lambda\in \eK\). Mais \( b-a\in L\), donc \( \lambda(b-a)\in L\) et
			\begin{equation}
				g(\lambda)=a+\lambda(b-a)\in a+L=D.
			\end{equation}
		\end{subproof}
	\end{subproof}
\end{proof}

\begin{example}
	Les exemples les plus courants d'applications affines sont les droites et les plans ne passant pas par l'origine.
	\begin{description}
		\item[Les droites] Une droite dans $\eR^2$ (ou $\eR^3$) qui ne passe pas par l'origine est l'image d'une fonction de la forme $s(t) =u t +v$, avec $t \in \eR$, et $u$ et $v$ dans $\eR^2$ ou $\eR^3$ selon le cas.

		      En choisissant des coordonnées adéquates, les droites peuvent être aussi vues comme graphes de fonctions affines. Dans le cas de $\eR^2$, on retrouve la fonction de l'exemple~\ref{ex_affine}, pour \( n = m = 1 \).

		\item[Les plans]
		      De la même façon nous savons que tout plan qui ne passe pas par l'origine dans $\eR^3$ est le graphe d'une application affine, $P(x,y)= (a,b)^T\cdot(x,y)^T+(c,d)^T$, lorsque les coordonnées sont bien choisies.
	\end{description}
\end{example}

\begin{lemma}[Équation de droite\index{équation de droite}]       \label{LEMooYIHXooEwmlPo}
	Si \( D\) est une droite dans \( \eR^2\), alors \( D\) est d'une des deux formes suivantes :
	\begin{itemize}
		\item Soit il existe \( a\in \eR\) tel que
		      \begin{equation}
			      D=\{ (x,y)\in \eR^2\tq x=a \},
		      \end{equation}
		\item soit il existe \( a,b\in \eR\) tels que
		      \begin{equation}
			      D=\{ (x,y)\in \eR^2\tq y=ax+b \}.
		      \end{equation}
	\end{itemize}
	Le premier cas correspond aux droites verticales.
\end{lemma}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection orthogonale}
%---------------------------------------------------------------------------------------------------------------------------

Le théorème suivant n'est pas indispensablissime parce qu'il est le même que le théorème de la projection sur les espaces de Hilbert\footnote{Théorème~\ref{ThoProjOrthuzcYkz}}. Cependant la partie existence est plus simple en se limitant au cas de dimension finie.
\begin{theoremDef}[Théorème de la projection]  \label{ThoWKwosrH}
	Soit \( E\) un espace vectoriel réel ou complexe de dimension finie, \( x\in E\), et \( C\) un sous-ensemble fermé convexe de \(E\).
	\begin{enumerate}
		\item
		      Les deux conditions suivantes sur \( y\in E\) sont équivalentes:
		      \begin{enumerate}
			      \item   \label{zzETsfYCSItemi}
			            \( \| x-y \|=\inf\{ \| x-z \|\tq z\in C \}\),
			      \item\label{zzETsfYCSItemii}
			            pour tout \( z\in C\), \( \real\langle x-y, z-y\rangle \leq 0\).
		      \end{enumerate}
		\item
		      Il existe un unique \( y\in E\), noté \( y=\pr_C(x)\) vérifiant ces conditions.
	\end{enumerate}
\end{theoremDef}

\begin{proof}
	Nous commençons par prouver l'existence et l'unicité d'un élément dans \( C\) vérifiant la première condition. Ensuite nous verrons l'équivalence.

	\begin{subproof}
		\item[Existence]

		Soit \( z_0\in C\) et \( r=\| x-z_0 \|\). La boule fermée \( \overline{ B(x,r) }\) est compacte\footnote{C'est ceci qui ne marche plus en dimension infinie.} et intersecte \( C\). Vu que \( C\) est fermé, l'ensemble \( C'=C\cap\overline{ B(x,r) }\) est compacte. Tous les points qui minimisent la distance entre \( x\) et \( C\) sont dans \( C'\); la fonction
		\begin{equation}
			\begin{aligned}
				C' & \to \eR        \\
				z  & \mapsto d(x,z)
			\end{aligned}
		\end{equation}
		est continue sur un compact et donc a un minimum qu'elle atteint\footnote{Théorème~\ref{ThoMKKooAbHaro}.}. Un point \( P\) réalisant ce minimum prouve l'existence d'un point vérifiant la première condition.

		\item[Unicité]
		Soient \( y_1\) et \( y_2\), deux éléments de \( C\) minimisant la distance avec \( x\), et soit \( d\) ce minimum. Nous avons par l'identité du parallélogramme \eqref{EqYCLtWfJ} que
		\begin{equation}
			\| y_1-y_2 \|^2=-4\left\| \frac{ y_1+y_2-x }{2} \right\|^2+2\| y_1-x \|^2+2\| y_2-x \|^2\leq -4d+2d+2d=0.
		\end{equation}
		Par conséquent \( y_1=y_2\).

		\item[\ref{zzETsfYCSItemi}\( \Rightarrow\)~\ref{zzETsfYCSItemii}]

		Soit \( z\in C\) et \( t\in \mathopen] 0 , 1 \mathclose[\); nous notons \( P=\pr_Cx\). Par convexité le point \( z=ty+(1-t)P\) est dans \( C\), et par conséquent,
		\begin{equation}
			\| x-P \|^2\leq\| x-tz-(1-t)P \|^2=\| (x-P)-t(z-P) \|^2.
		\end{equation}
		Nous sommes dans un cas \( \| a \|^2\leq | a-b |^2\), qui implique \( 2\real\langle a, b\rangle \leq \| b \|^2\). Dans notre cas,
		\begin{equation}
			2\real\langle x-P , t(z-P)\rangle \leq t^2\| z-P \|^2.
		\end{equation}
		En divisant par \( t\) et en faisant \( t\to 0\) nous trouvons l'inégalité demandée\footnote{Ici nous utilisons la proposition \ref{PROPooKPOXooEHIXJs}, et c'est une des choses qui font que cette partie sur la «géométrie élémentaire» demande en réalité d'être placée après déjà une partie de l'analyse réelle.} :
		\begin{equation}
			2\real\langle x-P, z-P\rangle \leq 0.
		\end{equation}

		\item[\ref{zzETsfYCSItemii}\( \Rightarrow\)~\ref{zzETsfYCSItemi}]

		Soit un point \( P\in C\) vérifiant
		\begin{equation}
			\real\langle x-P, z-P\rangle \leq 0
		\end{equation}
		pour tout \( z\in C\). Alors en notant \( a=x-P\) et \( b=P-z\),
		\begin{equation}
			\begin{aligned}[]
				\| x-z \|^2=\| x-P+P-z \|^2 & =\| a+b \|^2                                       \\
				                            & =\| a \|^2+\| b \|^2+2\real\langle a, b\rangle     \\
				                            & =\| a \|^2+\| b \|^2-2\real\langle x-P, z-P\rangle \\
				                            & \geq \| b \|^2,
			\end{aligned}
		\end{equation}
		ce qu'il fallait.
	\end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooHGGIooIssaTA}
	Soient une droite \( d\) dans \( \eR^3\) ainsi qu'un point \( p\). La projection\footnote{Définition \ref{ThoWKwosrH}.} \( \pr_d(p)\) est le point d'intersection\footnote{Lemme \ref{LEMooRLFQooJADark}.} entre \( d\) et la perpendiculaire à \( d\) passant par \( p\).
\end{proposition}

\begin{proof}
	Nous considérons la droite \( d=\{ a+\lambda v \}_{\lambda\in \eR}\) et un point \( p\in \eR^3\). Nous notons \( x(\lambda)=a+\lambda v\) le point courant dans \( d\). Conformément à la définition \ref{ThoWKwosrH} de la projection orthogonale, nous allons minimiser la distance \( \| p-x(\lambda) \|\) par rapport à \( \lambda\).

	Vu que \( \| p-x(\lambda) \|\) est toujours positif, nous pouvons chercher à minimiser le carré :
	\begin{equation}
		\| p- x(\lambda) \|=\| p \|^2-2p\cdot a-2\lambda p\cdot v+\| a \|^2+| \lambda |^2\| v \|^2+2\lambda a\cdot v.
	\end{equation}
	Quitte à minimiser ça par rapport à \( \lambda\), nous pouvons oublier les termes ne contenant pas \( \lambda\). Nous posons donc
	\begin{equation}
		f(\lambda)=\| v \|^2\lambda^2+ 2(a-p)\cdot v\lambda
	\end{equation}

	Vu que le coefficient de \( \lambda^2\) est positif, la proposition \ref{PROPooEZIKooKjJroH} nous dit que cette fonction aura un minimum (et non un maximum). La valeur \( \lambda_0\) pour laquelle \( f\) est minimal se découvre grâce à \ref{PROPooEZIKooKjJroH}\ref{ITEMooHQTBooZuaPAs} :
	\begin{equation}
		\lambda_0=\frac{ -2(a-p)\cdot v }{ 2\| v \|^2 }.
	\end{equation}
	Cela est la valeur de \( \lambda\) pour laquelle
	\begin{equation}
		\pr_d(p)=x(\lambda_0);
	\end{equation}
	nous avons donc
	\begin{equation}
		x(\lambda_0)=a-\frac{ (a-p)\cdot v }{ \| v \|^2 }v.
	\end{equation}
	Nous devons voir maintenant que \( \big( p-x(\lambda_0) \big)\cdot v=0\). Il suffit d'un peu déballer :
	\begin{equation}
		\big( p-x(\lambda_0) \big)\cdot v=p\cdot v-a\cdot v+\frac{ (a-p)\cdot v }{ \| v \|^2 }\| v \|^2=p\cdot v-a\cdot v+(a-p)\cdot v=0.
	\end{equation}
\end{proof}

\begin{lemma}       \label{LEMooGUVMooPXtXnV}
	Soit \( v_1\in \eR^3\). Il existe des vecteurs \( v_2\) et \( v_3\) tels que les \( v_i\) sont deux à deux perpendiculaires.
\end{lemma}

\begin{proof}
	Nous considérons \( w\neq v\) dans \( \eR^3\) et nous profitons de la proposition \ref{PROPooTUVKooOQXKKl} pour poser \( v_2=v_1\times w\). Enfin nous définissons \( v_3=v_1\times v_2\).
\end{proof}

\begin{lemma}       \label{LEMooGXGCooDfgbqG}
	Soient trois éléments \( v_1,v_2,v_3\in \eR^3\) deux à deux perpendiculaires. Si \( x\perp v_1\), alors \( x\in \Span\{ v_2,v_3 \}\).
\end{lemma}

\begin{proof}
	Il faut se rappeler de la proposition \ref{PropVectsOrthLibres} qui fait de \( \{ v_1,v_2,v_3 \}\) une partie libre. Elle est donc une base par la proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooSGGCooOUsuBs}.

	Soit \( x\perp v_1\). Nous le décomposons dans la base \( \{ v_1,v_2,v_3 \}\) : \( x=\lambda_1 v_1+\lambda_2 v_2+\lambda_3v_3\). En prenant le produit scalaire par \( v_1\), et en tenant compte du fait que \( v_1\cdot v_2=v_2\cdot v_3=0\) nous trouvons \( 0=v_1\cdot x=\lambda_1\| v_1 \|^2\). Donc \( \lambda_1=0\) et \( x\in \Span\{ v_2,v_3 \}\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Plan médiateur}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[plan médiateur\cite{MonCerveau}]
	Soient un espace euclidien \( V\) ainsi que deux points distincts \( a,b\in V\). Nous avons
	\begin{equation}
		\{ x\in V\tq x-m\perp b-a \}=\{ x\in V\tq \| x-a \|=\| x-b \| \}.
	\end{equation}
	Dans le cas de \( V=\eR^3\), alors cet ensemble est un plan\footnote{Définition \ref{DEFooTQIFooKcloeY}.}.

	Ce plan est le \defe{plan médiateur}{plan médiateur} du segment \( [a,b]\).
\end{proposition}

\begin{proof}
	Nous notons
	\begin{subequations}
		\begin{align}
			M & =\{ x\in V\tq x-m\perp b-a \},        \\
			N & =\{ x\in V\tq \| x-a \|=\| x-b \| \}.
		\end{align}
	\end{subequations}
	\begin{subproof}
		\item[\( M\subset N\)]
		Soit \( x\in M\). Nous avons \( (x-m)\cdot (b-a)=0\), et nous pouvons utiliser Pythagore \ref{THOooHXHWooCpcDan} dans les triangles \( xbm\) et \( xma\):
		\begin{subequations}        \label{SUBEQSooVEPCooKnyPoq}
			\begin{align}
				\| x-a \|^2 & =\| x-m \|^2+\| a-m \|^2  \\
				\| x-b \|^2 & =\| x-m \|^2+\| m-b \|^2.
			\end{align}
		\end{subequations}
		Vu que \( m\) est le milieu, nous avons \( a-m=m-b\) et donc \( \| a-m \|=\| m-b \|\). Nous voyons donc que les membres de droites des deux équations \eqref{SUBEQSooVEPCooKnyPoq} sont égaux. Donc \( \| x-a \|^2=\| x-b \|^2\). Comme une norme est toujours positive, les carrés peuvent être simplifiés : \( \| x-a \|=\| x-b \|\).

		Donc \( x\in N\).
		\item[\( N\subset M\)]
		Soit \( x\in N\). Nous posons \( h=\pr_{(ab)}(x)\), la projection de \( x\) sur la droite \( (ab)\). La proposition \ref{PROPooHGGIooIssaTA} nous dit que \( h\) est l'unique point de \( (ab)\) tel que \( x-h\perp b-a\).

		Le théorème de Pythagore \ref{THOooHXHWooCpcDan} dans le triangle \( ahx\) donne
		\begin{equation}
			\| x-a \|^2=\| a-h \|^2+\| x-h \|^2
		\end{equation}
		et dans le triangle \( bhx\) il donne :
		\begin{equation}
			\| b-x \|^2=\| b-h \|^2+\| h-x \|^2.
		\end{equation}
		Par hypothèse nous avons \( \| x-a \|^2=\| x-b \|^2\) et donc
		\begin{equation}
			\| a-h \|=\| b-h \|.
		\end{equation}
		Nous cherchons à présent quel(s) point(s) \( h\) de la droite \( (ab)\) vérifie(nt) cette condition, et nous espérons que ce sera \( (a+b)/2\).

		Nous cherchons \( h\) sous la forme \( h=a+\lambda(b-a)\). D'une part nous avons \( \| a-h \|^2=\| \lambda(b-a) \|^2=\lambda^2\| b-a \|^2\), et d'autre part
		\begin{equation}
			\| b-h \|^2=\| b-a-\lambda(b-a) \|^2=| 1-\lambda |^2\| b-a \|^2
		\end{equation}
		Nous en déduisons que \( | \lambda |=| 1-\lambda |\). Cela laisse deux possibilités : la première est \( \lambda=1-\lambda\) qui donne \( \lambda=1/2\) et la seconde est \( \lambda=-(1-\lambda)\) qui est impossible. Donc \( \lambda=1/2\) et
		\begin{equation}
			h=a+\frac{ b-a }{ 2 }=\frac{ a+b }{ 2 }.
		\end{equation}

		Donc en posant \( m=(a+b)/2\) nous avons bien \( b-a\perp x-m\).
		\item[C'est un plan]
		Nous nous mettons maintenant dans le cas où \( V\) est l'espace \( \eR^3\) muni de sa norme usuelle. Posons \( f_1=b-a\) et considérons deux vecteurs \( f_2,f_3\) tels que les \( f_i\) soient deux à deux perpendiculaires (lemme \ref{LEMooGUVMooPXtXnV}).

		Nous allons prouver que \( M=\Span\{ f_2,f_3 \}+m\).

		\begin{subproof}
			\item[Une inclusion]
			Si \( x\in \Span(f_2,f_3)+m\), alors \( x=\alpha f_2+\beta f_3+m\) et nous avons bien \( x-m\perp b-a\).
			\item[L'autre inclusion]
			Soit \( x\in M\). Donc \( x-m\perp b-a\). Le lemme \ref{LEMooGXGCooDfgbqG} nous indique alors que \( x-m\in\Span\{ f_2,f_3 \}\), ce qu'il fallait.
		\end{subproof}
	\end{subproof}
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Tétraèdre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{MonCerveau}]   \label{DEFooMUUMooFVxKyb}
	Un \defe{tétraèdre régulier}{tétraèdre régulier} est un ensemble de \( 4\) points \( A\), \( B\), \( C\) et \( D\) de \( \eR^3\) deux à deux équidistants.

	Nous allons nommer \( \{ a_i \}\) les segments entre les points, \( \{ d_i \}\) les droites sur ces segments, et \( \{ s_i \}\) les sommets.
\end{definition}

\begin{lemma}
	Un tétraèdre régulier existe.
\end{lemma}

\begin{proof}
	Prenez un triangle équilatérale \( ABC\) dans le plan \( (.,.,0)\), et prenez ensuite un point \( D\) à la verticale du centre, placé à la bonne hauteur pour que les longueurs \( \| AD \|\), \( \| BD \|\) et \( \| CD \|\) soient égales à \( \| AB \|\).
\end{proof}

\begin{lemma}       \label{LEMooNWELooZeSEMN}
	Si \( T\) est un tétraèdre régulier, nous avons \( d_i\cap T=a_i\).
\end{lemma}

\begin{lemma}       \label{LEMooUSKVooQJiBuz}
	Les droites \( \{ d_i \}_{i=1,\ldots, 6}\) ne sont pas confondues ni parallèles.
\end{lemma}

\begin{proof}
	Si trois points \( A\), \( B\), \( C\) sont alignés, il n'est pas possible d'avoir \( \| AB \|=\| AC \|=\| BC \|\). Donc il n'y a pas deux droites parmi les \( \{ d_i \}\) qui sont confondues.

	Supposons que deux des droites \( AB\) et \( CD\) sont parallèles. En particulier, les points \( A\), \( B\), \( C\) et \( D\) sont dans un même plan : le plan \( A+\Span\{ B-A, C-A \}\). Il n'est pas possible d'avoir \( 4\) points dans un plan, tous équidistants deux à deux.
	%Pas 4 points équidistants: TODOooVXINooTNdhAG
\end{proof}

Dans la suite, quand nous parlerons du «tétraèdre», nous parlerons de ses six points et six segments les joignant. L'ensemble \( T\subset \eR^3\) ne contient pas les surfaces et les volumes.

\begin{lemma}   \label{LEMooJCMKooOjMqtw}
	Soit un tétraèdre régulier \( T\). Un point de \( \eR^3\) est un sommet si et seulement si il est l'intersection de deux des droites \( \{ d_i \}\) différentes.
\end{lemma}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\item[Sens direct]

		Par définition les sommets sont les points \( A \), \( B\), \( C\), \( D\); et les droites \( d_i\) sont les droites \( (AB)\), \( (AC)\), \( (AD)\), \( (BC)\), \( (DB)\) et \( (CD)\). Donc oui, les sommets sont à des intersections de ces droites.

		\item[Sens inverse]
		Soit un point \( X\in \eR^3\) à l'intersection entre deux des \( d_i\). Nous avons déjà vu dans le lemme \ref{LEMooUSKVooQJiBuz} que ces droites ne sont ni parallèles ni confondues. Donc elles ont au plus un point d'intesection. Voyons les couples possibles de droites.
		%Position relative de droites dans R^3: TODOooUKNZooPsYyDy

		On a une série de possibilités comme \( (AB)\cap(AC)\). Dans ce cas, l'intersection entre ces deux droits est \( A\) qui est un des sommets. Ensuite nous avons une série de possibilités comme \( (AB)\cap (CD)\). Ces deux droites n'ont pas d'intersection parce que si elles en avaient, les points \( A\), \( B\), \( C\) et \( D\) seraient dans le même plan, ce qui est impossible.
		% Deux droites de R^3 qui ont une intersection sont dans un même plan : TODOooFHYSooJZyuaf
		Donc deux droites \( d_i\) ont soit pas d'intersection soit une intersection qui est un sommet.
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Géométrie dans le plan}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{theorem}[Théorème de Thalès\cite{BIBooJRAPooHayrWy}]     \label{THOooFMMLooLmAnAd}
	Soient trois points \( A\), \( B\), \( C\) non alignés dans \( \eR^2\). Soient \( D\in (AB)\) et \( E\in (AC)\). Nous supposons que \( (DE)\) est parallèle à \( BC\).

	Alors
	\begin{enumerate}
		\item
		      \begin{equation}
			      \frac{ \| D-A \| }{ \| B-A \| }=\frac{ \| E-A \| }{ \| C-A \| }=\frac{ \| E-D \| }{ \| C-B \| }
		      \end{equation}
		\item
		      Il existe une homothétie \( \phi\colon \eR^2\to \eR^2\) centrée en \( A\) telle que \( \phi(B)=D\) et \( \phi(C)=E\).
		      %TODOooJYSDooGHxVgl définir homothétie centrée en un point.
	\end{enumerate}
\end{theorem}

\begin{theorem}[Théorème de Thalès dans le cercle\cite{BIBooODOZooLXsOQk}]      \label{THOooGFTWooACQLFJ}
	Soient des points \( O\), \( A\), \( B\), \( C\) dans \( \eR^2\) tels que
	\begin{enumerate}
		\item
		      \( \| A-O \|=\| B-O \|=\| C-O \|\).
		\item
		      \( A\), \( O\) et \( B\) sont alignés.
	\end{enumerate}
	Alors le triangle \( ABC\) est rectangle en \( C\).
\end{theorem}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dérivée : exemples introductifs}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{La vitesse}
%---------------------------------------------------------------------------------------------------------------------------

Lorsqu'un mobile se déplace à une vitesse variable, nous obtenons la \emph{vitesse instantanée} en calculant une vitesse moyenne sur des intervalles de plus en plus petits. Si le mobile a un mouvement donné par $x(t)$, la vitesse moyenne entre $t=2$ et $t=5$ sera
\[
	v_{\text{moy}}(2\to 5)=\frac{ x(5)-x(2) }{ 5-2 }.
\]
Plus généralement, la vitesse moyenne entre $2$ et $2+\Delta t$ est donnée par
\[
	v_{\text{moy}}(2\to 2+\Delta t)=\frac{ x(2+\Delta t)-x(2) }{ \Delta t }.
\]
Cela est une fonction de $\Delta t$. Oui, mais je te rappelle qu'on a dans l'idée de calculer une vitesse instantanée, c'est-à-dire de voir ce que vaut la vitesse moyenne sur un intervalle très {\small très} {\footnotesize très} {\scriptsize très} {\tiny petit}. La notion de limite semble toute indiquée pour décrire mathématiquement l'idée physique de vitesse instantanée.

Nous allons dire que la vitesse instantanée d'un mobile est la limite quand $\Delta t$ tend vers zéro de sa vitesse moyenne sur l'intervalle de temps $\Delta t$, ou en formule :
\begin{equation}		\label{Eqvinstlimite}
	v(t_0)=\lim_{\Delta t\to 0}\frac{ x(t_0)-x(t_0+\Delta t) }{ \Delta t }.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{La tangente à une courbe}
%---------------------------------------------------------------------------------------------------------------------------

Passons maintenant à tout autre chose, mais toujours dans l'utilisation de la notion de limite pour résoudre des problèmes intéressants. Comment trouver l'équation de la tangente à la courbe $y=f(x)$ au point $(x_0,f(x_0))$ ?

Essayons de trouver la tangente au point $P$ donné de la courbe donnée à la figure~\ref{LabelFigTangenteQuestion}.

\newcommand{\CaptionFigTangenteQuestion}{Comment trouver la tangente à la courbe au point $P$ ?}
\input{auto/pictures_tex/Fig_TangenteQuestion.pstricks}

La tangente est la droite qui touche la courbe en un seul point sans la traverser. Afin de la construire, nous allons dessiner des droites qui touchent la courbe en $P$ et un autre point $Q$, et nous allons voir ce qu'il se passe quand $Q$ est très proche de $P$. Cela donnera une droite qui, certes, touchera la courbe en deux points, mais en deux points \emph{tellement proches que c'est comme si c'étaient les mêmes}. Tu sens que la notion de limite va encore venir.

%Pour rappel cette figure TangenteDetail est générée par phystricksRechercheTangente.py
\newcommand{\CaptionFigTangenteDetail}{Traçons d'abord une corde entre le point $P$ et un point $Q$ un peu plus loin.}
\input{auto/pictures_tex/Fig_TangenteDetail.pstricks}

Nous avons placé le point, sur la figure~\ref{LabelFigTangenteDetail}, le point $P$ en $a$ et le point $Q$ un peu plus loin $x$. En d'autres termes leurs coordonnées sont
\begin{align}
	P=\big(a,f(a)\big) &  & Q=\big(x,f(x)\big).
\end{align}
En regardant par exemple la figure~\ref{LabelFigTangenteDetail}, le coefficient directeur de la droite qui passe par ces deux points est donné par
\begin{equation}
	\frac{ f(x)-f(a) }{ x-a },
\end{equation}
et bang ! Encore le même rapport que celui qu'on avait trouvé à l'équation \eqref{Eqvinstlimite} en parlant de vitesses. Si tu regardes la figure~\ref{LabelFigLesSubFigures}, tu verras que réellement en faisant tendre $x$ vers $a$ on obtient la tangente.

\newcommand{\CaptionFigLesSubFigures}{Recherche de la tangente par approximations successives.}
\input{auto/pictures_tex/Fig_LesSubFigures.pstricks}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubZ}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubO}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubT}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubTh}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubF}
%See also the subfigure~\ref{LabelFigLesSubFiguressssubFi}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{L'aire en dessous d'une courbe}		\label{SubSecAirePrimInto}
%---------------------------------------------------------------------------------------------------------------------------

Encore un exemple. Nous voudrions bien pouvoir calculer l'aire en dessous d'une courbe. Nous notons $S_f(x)$ l'aire en dessous de la fonction $f$ entre l'abscisse $0$ et $x$, c'est-à-dire l'aire bleue de la figure~\ref{LabelFigNOCGooYRHLCn}. % From file NOCGooYRHLCn
\newcommand{\CaptionFigNOCGooYRHLCn}{L'aire en dessous d'une courbe. Le rectangle rouge d'aire $f(x)\Delta x$ approxime l'augmentation de l'aire lorsqu'on passe de $x$ à $x+\Delta x$.}
\input{auto/pictures_tex/Fig_NOCGooYRHLCn.pstricks}

Si la fonction $f$ est continue et que $\Delta x$ est assez petit, la fonction ne varie pas beaucoup entre $x$ et $x+\Delta x$. L'augmentation de surface entre $x$ et $x+\Delta x$ peut donc être approximée par le rectangle de surface $f(x)\Delta x$. Ce que nous avons donc, c'est que quand $\Delta x$ est très petit,
\begin{equation}
	S_f(x+\Delta x)-S_f(x)=f(x)\Delta x,
\end{equation}
c'est-à-dire
\begin{equation}
	f(x)=\lim_{\Delta x\to 0}\frac{  S_f(x+\Delta x)-S_f(x)}{ \Delta x }.
\end{equation}
Donc, la fonction $f$ est la dérivée de la fonction qui représente l'aire en dessous de $f$. Calculer des surfaces revient donc au travail inverse de calculer des dérivées.

Nous avons déjà vu que calculer la dérivée d'une fonction n'est pas très compliqué. Aussi étonnant que cela puisse paraitre, il se fait que le processus inverse est très compliqué : il est en général extrêmement difficile (et même souvent impossible) de trouver une fonction dont la dérivée est une fonction donnée.

Une fonction dont la dérivée est la fonction $f$ s'appelle une \defe{primitive}{primitive} de $f$, et la fonction qui donne l'aire en dessous de la fonction $f$ entre l'abscisse $0$ et $x$ est notée
\begin{equation}
	S_f(x)=\int_0^xf(t)dt.
\end{equation}
Nous pouvons nous demander si, pour une fonction $f$ donnée, il existe une ou plusieurs primitives, c'est-à-dire si il existe une ou plusieurs fonctions $F$ telles que $F'=f$. La réponse viendra\ldots
%TODO : faire la référence

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dérivation de fonctions réelles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{seccontetderiv}

On considère dans la suite une fonction $f : A \to \eR$, où $a \in A \subset \eR$ ; cependant, les notions de continuité et de dérivabilité se généralisent immédiatement au cas de fonctions à valeurs vectorielles ; la notion de continuité se généralise au cas des fonctions à plusieurs variables (la notion de dérivabilité est remplacée par celle de différentiabilité dans ce cadre).

\begin{definition}      \label{DEFooOYFZooFWmcAB}
	La fonction $f$ est \defe{dérivable}{dérivable} en \( a\) si $a \in
		\operatorname{int} A$ et si
	\begin{equation*}
		\lim_{x\to a} \frac{f(x)-f(a)}{x-a}
	\end{equation*}
	existe. On note alors cette quantité $f'(a)$, c'est le nombre
	dérivé de $f$ en $a$. La \defe{fonction dérivée}{fonction dérivée} de $f$ est
	\begin{equation}
		\begin{aligned}
			f'\colon A' & \to \eR      \\
			a           & \mapsto f(a)
		\end{aligned}
	\end{equation}
	définie sur l'ensemble noté $A'$ des points $a$ où $f$ est dérivable.
\end{definition}

\begin{example}
	Montrons que la fonction $f : \eR \to \eR : x\mapsto x$ est continue et dérivable. Exceptionnellement (bien qu'on sache que la dérivabilité implique la continuité), montrons ces deux assertions séparément.
	\begin{description}
		\item[Continuité] Pour prouver la continuité au point $a \in \eR$ nous devons montrer que
		      \begin{equation}
			      \limite x a x = a
		      \end{equation}
		      c'est-à-dire
		      \begin{equation}
			      \forall \epsilon > 0, \exists \delta > 0 :  \forall x \in \eR \abs{x-a} <
			      \delta \Rightarrow \abs{x-a} < \epsilon
		      \end{equation}
		      ce qui est clair en prenant $\delta = \epsilon$.

		\item[Dérivabilité] Soit $a \in \eR$. Calculons la limite du quotient différentiel
		      \begin{equation}
			      \limite[x\neq a]{x}{a} \frac{x-a}{x-a} = \limite[x\neq a]x a 1 = 1
		      \end{equation}
		      ce qui prouve que $f$ est dérivable et que sa dérivée vaut $1$ en
		      tout point $a$ de $\eR$.
	\end{description}

	On a donc montré que la fonction $x \mapsto x$ est continue, dérivable, et que sa dérivée vaut $1$ en tout point $a$ de son domaine.

\end{example}

\begin{proposition} \label{PropSFyxOWF}
	Une fonction dérivable sur un intervalle est continue sur cet intervalle.
\end{proposition}

\begin{proof}
	Soit \( I\) un intervalle sur lequel la fonction \( f\) est dérivable, et soit \( x_0\in I\). Nous allons prouver la continuité de \( f\) en \( x_0\). Le fait que la limite
	\begin{equation}
		f'(x_0)=\lim_{h\to 0} \frac{ f(x_0+h)-f(x_0) }{ h }
	\end{equation}
	existe implique a fortiori que
	\begin{equation}
		\lim_{h\to 0} f(x_0+h)-f(x_0)=0.
	\end{equation}
	Cela signifie la continuité de \( f\) en vertu du critère~\ref{ThoLimCont}.
\end{proof}

\begin{theorem} \label{THOooFFOZooCYGets}
	Toute fonction dérivable en un point est continue en ce point.
\end{theorem}

\begin{proof}
	Soient \( f\colon \eR\to \eR\) et \( a\in \eR\). Nous supposons que \( f\) n'est pas continue en \( a\) et nous allons en déduire qu'elle n'est pas non plus dérivable en \( a\). Pour cela nous considérons le lien entre limite et continuité donné dans le théorème \ref{ThoLimCont}. Nier que \( f\) est continue en \( a\) revient à dire qu'il existe un voisinage \( V\) de \( f(a)\) tel que
	\begin{equation}
		\forall r>0,\,\exists \epsilon<r \tq f(a+\epsilon)\notin V.
	\end{equation}
	Si \( B\big( f(a),R \big)\subset V\)\footnote{Existence par la définition de la topologie métrique \ref{ThoORdLYUu}.}, et si \( r=1/n\), nous construisons une suite \( \epsilon_n\to 0\) telle que
	\begin{equation}
		| f(a+\epsilon_n)-f(a) |>R.
	\end{equation}
	Avec cela nous avons
	\begin{equation}
		\frac{ | f(a+\epsilon_n)-f(a) | }{ \epsilon_n }>\frac{ R }{ \epsilon_n }\to \infty.
	\end{equation}
	Donc la fonction \( f\) ne peut pas être dérivable en \( a\).
\end{proof}

\begin{remark}
	La réciproque du théorème précédent n'est pas vraie : il existent bien des fonctions qui sont continues à un point $x_0$ mais qui ne sont pas dérivables en $x_0$. La fonction valeur absolue, $x\mapsto |x|$, par exemple est continue sur tout $\eR$ mais elle n'est pas dérivable en $0$.
\end{remark}

Si \( f\) est une fonction dérivable, il peut arriver que la fonction dérivée \( f'\) soit elle-même dérivable. Dans ce cas nous notons \( f''\) ou \( f^{(2)}\) la dérivée de la fonction \( f'\). Cette fonction $f''$ est la \defe{dérivée seconde}{dérivée!seconde} de \( f\). Elle peut encore être dérivable; dans ce cas nous notons \( f^{(3)}\) sa dérivée, et ainsi de suite. Nous définissons \( f^{(n)}=(f^{(n-1)})'\) la dérivée \( n\)\ieme de \( f\). Nous posons évidemment $f^{(0)}=f$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Exemples}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
	Commençons par la fonction $f(x)=x$. Dans ce cas nous avons
	\begin{equation}
		\frac{ f(x)-f(a) }{ x-a }=\frac{ x-a }{ x-a }=1.
	\end{equation}
	La dérivée est donc $1$.
\end{example}

\begin{proposition}
	La dérivé de la fonction $x\mapsto x$ vaut $1$, en notations compactes : $(x)'=1$.
\end{proposition}

\begin{proof}
	D'après la définition de la dérivée, si $f(x)=x$, nous avons
	\begin{equation}
		f(x)=\lim_{\epsilon\to 0}\frac{ (x+\epsilon) -x }{\epsilon} =\lim_{\epsilon\to 0}\frac{ \epsilon }{\epsilon} =1,
	\end{equation}
	et c'est déjà fini.
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{La fonction carré}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Prenons ensuite $f(x)=x^2$. En utilisant le produit remarquable $(x^2-a^2)=(x-a)(x+a)$ nous trouvons
\begin{equation}
	\frac{ f(x)-f(a) }{ x-a }=x+a.
\end{equation}
Lorsque $x\to a$, cela devient $2a$. Nous avons par conséquent
\begin{equation}
	f'(x)=2x.
\end{equation}

\begin{lemma}           \label{LemDeccCarr}
	Si $f(x)=x^2$, alors $f'(x)=2x$.
\end{lemma}

\begin{proof}
	Utilisons la définition, et remplaçons $f$ par sa valeur :
	\begin{subequations}
		\begin{align}
			f'(x) & =\lim_{\epsilon\to 0}\frac{ f(x+\epsilon)-f(x) }{ \epsilon }            \\
			      & =\lim_{\epsilon\to 0}\frac{ (x+\epsilon)^2-x^2 }{ \epsilon }            \\
			      & =\lim_{\epsilon\to 0}\frac{ x^2+2x\epsilon+\epsilon^2-x^2 }{ \epsilon } \\
			      & =\lim_{\epsilon\to 0}\frac{\epsilon(2x+\epsilon)}{ \epsilon }           \\
			      & =\lim_{\epsilon\to 0}(2x+\epsilon)                                      \\
			      & =2x,
		\end{align}
	\end{subequations}
	ce qu'il fallait prouver.
\end{proof}


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{La fonction racine carré}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Considérons maintenant la fonction $f(x)=\sqrt{x}$. Nous avons
\begin{equation}
	\begin{aligned}[]
		\frac{ f(x)-f(a) }{ x-a } & =\frac{ \sqrt{x}-\sqrt{a} }{ x-a }                                           \\
		                          & =\frac{ (\sqrt{x}-\sqrt{a})(\sqrt{x}+\sqrt{x}) }{ (x-a)(\sqrt{x}+\sqrt{x}) } \\
		                          & =\frac{1}{ \sqrt{x}+\sqrt{x} }.
	\end{aligned}
\end{equation}
Lorsque $x\to 0$, nous obtenons
\begin{equation}
	f'(a)=\frac{1}{ 2\sqrt{a} }.
\end{equation}
Notons que la dérivée de $f(x)=\sqrt{x}$ n'existe pas en $x=0$. En effet elle serait donnée par le quotient
\begin{equation}
	f'(0)=\lim_{x\to 0} \frac{ \sqrt{x}-\sqrt{0} }{ x }=\lim_{x\to 0} \frac{ \sqrt{x} }{ x }=\lim_{x\to 0} \frac{1}{ \sqrt{x} }.
\end{equation}
Mais si $x$ devient très petit, la dernière fraction tend vers l'infini.

%--------------------------------------------------------------------------------------------------------------------------
\subsection[Interprétation géométrique : tangente]{Interprétation géométrique de la dérivée : tangente}
%--------------------------------------------------------------------------------------------------------------------------

Considérons le \defe{graphe}{graphe} de la fonction $f$ sur $I$, c'est-à-dire l'ensemble
\begin{equation}
	\big\{ \big( x,f(x) \big)\tq x\in I \big\}.
\end{equation}
Le nombre
\begin{equation}
	\frac{ f(x)-f(a) }{ x-a }
\end{equation}
est la pente de la droite qui joint les points $\big( x,f(x) \big)$ et $\big( a,f(a) \big)$, voir la figure ~\ref{LabelFigGWOYooRxHKSm}. % From file GWOYooRxHKSm
\newcommand{\CaptionFigGWOYooRxHKSm}{Le coefficient directeur de la corde entre $a$ et $x$.}
\input{auto/pictures_tex/Fig_GWOYooRxHKSm.pstricks}

Étant donné que $f'(a)$ est le coefficient directeur de la tangente au point $\big( a,f(a) \big)$, l'équation de la tangente est
\begin{equation}		\label{EqTgfaen}
	y-f(a)=f'(a)(x-a).
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------
\subsection[Interprétation géométrique : approximation affine]{Interprétation géométrique de la dérivée : approximation affine}
%--------------------------------------------------------------------------------------------------------------------------

Le fait que la fonction $f$ soit dérivable au point $a\in I$ signifie que
\begin{equation}
	\lim_{x\to a} \frac{ f(x)-f(a) }{ x-a }=\ell
\end{equation}
pour un certain nombre $\ell$. Cela peut être récrit sous la forme
\begin{equation}
	\lim_{x\to a} \frac{ f(x)-f(a) }{ x-a }-\ell=0,
\end{equation}
ou encore
\begin{equation}
	\lim_{x\to a} \frac{ f(x)-f(a)-\ell(x-a) }{ x-a }=0.
\end{equation}
Introduisons la fonction
\begin{equation}
	\alpha(t)=\frac{ f(a+t)-f(a)-t\ell }{ t }.
\end{equation}
Cette fonction est faite exprès pour que
\begin{equation}		\label{EqIntermsaxaama}
	\alpha(x-a)=\frac{ f(x)-f(a)-\ell(x-a) }{ x-a };
\end{equation}
par conséquent $\lim_{x\to a} \alpha(x-a)=0$. Nous récrivons l'équation \eqref{EqIntermsaxaama} sous la forme
\begin{equation}        \label{EqCodeDerviffxam}
	f(x)-f(a)-\ell(x-a)=(x-a)\alpha(x-a).
\end{equation}
Le second membre tend vers zéro lorsque $x$ tend vers $a$ avec une «vitesse au carré» : c'est le produit de deux facteurs tous deux tendant vers zéro. Si $x$ n'est pas très loin de $a$, il n'est donc pas une mauvaise approximation de dire
\begin{equation}
	f(x)-f(a)-\ell(x-a)\simeq 0,
\end{equation}
c'est-à-dire
\begin{equation}		\label{Eqfxsimesfa}
	f(x)\simeq f(a)+f'(a)(x-a).
\end{equation}
Nous avons retrouvé l'équation \eqref{EqTgfaen}. La manipulation que nous venons de faire revient donc à dire que la fonction $f$, au voisinage de $a$, est bien approximée par sa tangente.

L'équation \eqref{Eqfxsimesfa} peut être aussi écrite sous la forme
\begin{equation}		\label{EqfxdxSimeqfxfpx}
	f(x+\Delta x)\simeq f(x)+f'(x)\Delta x
\end{equation}
qui est une approximation d'autant meilleure que $\Delta x$ est petit.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Développement limité au premier ordre}
%---------------------------------------------------------------------------------------------------------------------------

Si une fonction est dérivable en \( a\) alors elle peut être approximée «au premier ordre» par une formule simple qui sera généralisé pour des dérivées d'ordre supérieurs avec les séries de Taylor, théorème~\ref{ThoTaylor}. Pour trouver des versions avec des dérivations partielles, voir le thème \ref{INTERNooXFNTooNNaOzP}.
\begin{proposition}[Développement limité au premier ordre]  \label{PropUTenzfQ}
	Si \( f\colon \eR\to \eR\) est une fonction dérivable, alors is existe une fonction \( \alpha\colon \eR\to \eR\) telle que
	\begin{equation}        \label{EQooHBDHooPrVjJD}
		f(a+h)=f(a)+hf'(a)+\alpha(h)
	\end{equation}
	et
	\begin{equation}
		\lim_{h\to 0} \frac{ \alpha(h) }{ h }=0.
	\end{equation}
	Il existe aussi une fonction \( \beta\colon \eR\to \eR\) telle que
	\begin{equation}        \label{EQooPWIZooVuhjmt}
		f(a+h)=f(a)+ hf'(a)+h\beta(h)
	\end{equation}
	telle que \( \lim_{h\to 0}\beta(h)=0\).
\end{proposition}
\index{développement!limité!premier ordre}

\begin{proof}
	La fonction \( f\) étant dérivable en \( a\) nous avons l'existence de la limite suivante :
	\begin{equation}
		f'(a)=\lim_{h\to 0} \frac{ f(a+h)-f(a) }{ h },
	\end{equation}
	ce qui revient à dire qu'en définissant la fonction \( \beta\) par
	\begin{equation}
		f'(a)=\frac{ f(a+h)-f(a) }{ h }+\beta(h)
	\end{equation}
	alors \( \beta(h)\to 0\) lorsque \( h\to 0\). En multipliant par \( h\), nous avons la fonction \( \beta\) de la formule \eqref{EQooPWIZooVuhjmt}.

	En nommant \( \alpha(h)=h\beta(h)\) nous trouvons la fonction \( \alpha\) de la formule \eqref{EQooHBDHooPrVjJD} :
	\begin{equation}
		f(a+h)=f(a)+hf'(a)+\alpha(h)
	\end{equation}
	avec
	\begin{equation}
		\lim_{h\to 0} \frac{ \alpha(h) }{ h }=\lim_{h\to 0} \beta(h)=0.
	\end{equation}
\end{proof}
