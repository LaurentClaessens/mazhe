% This is part of Le Frido
% Copyright (c) 2006-2025
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Uniforme continuité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooZSZMooBYSDFO}
\label{SecUnifContinue}

L'uniforme continuité est la définition \ref{DEFooYIPXooQTscbG}.

\begin{definition}
	Une partie \( A\subset\eR^m\) est dite \defe{bornée}{bornée!partie de \( \eR^m\)} si il existe un \( M>0\) tel que \( A\subset B(0,M)\). Le \defe{diamètre}{diamètre} de la partie \( A\) est\nomenclature[T]{\( \Diam(A)\)}{Diamètre de la partie \( A\)} le nombre
	\begin{equation}
		\Diam(A)=\sup_{x,y\in A}\| x-y \|\in\mathopen[ 0 , \infty \mathclose].
	\end{equation}
\end{definition}
Lorsque \( A\) est bornée, il existe un \( M\) tel que \( \| x \|\leq M\) pour tout \( x\in A\).

\begin{lemma}       \label{LEMooFABOooZYFjhv}
	Si \( A\) est une partie non vide et bornée de \( \eR^m\), alors \( \Diam(A)=\Diam(\bar A)\).
\end{lemma}

\begin{proof}
	Vu que \( A\subset \bar A\), nous avons l'inégalité \( \Diam(A)\leq \Diam(\bar A)\). Nous nommons \( d=\Diam(\bar A)\). Soit \( \epsilon>0\). Soient \( x,y\in\bar A\) tels que \( \| x-y \|>d-\epsilon\). Nous prenons \( a\in B(x,\epsilon)\cap A\) et \( b\in B(y,\epsilon)\cap A\).

	Nous avons
	\begin{equation}
		\Diam(A)\geq \| a-b \|\geq \| x-y \|-2\epsilon\geq d-3\epsilon.
	\end{equation}
	Nous avons donc \( \Diam(A)\geq \Diam(\bar A)-3\epsilon\) pour tout \( \epsilon\). Donc \( \Diam(A)\geq \Diam(\bar A)\).
\end{proof}

\begin{example}
	Prenons la fonction \( f(x)=\frac{1}{ x }\), et demandons nous pour quel \( \delta\) nous sommes sûr d'avoir
	\begin{equation}
		| f(a+\delta)-f(a) |=\left| \frac{1}{ a+\delta }-\frac{1}{ a } \right| <\varepsilon.
	\end{equation}
	Pour simplifier, nous supposons que \( a>0\). Nous calculons
	\begin{equation}
		\begin{aligned}[]
			\frac{ 1 }{ a }-\frac{1}{ a+\delta } & <\varepsilon                                  \\
			\frac{ \delta }{ a(a+\delta) }       & <\varepsilon                                  \\
			\delta                               & <\varepsilon a^2+\varepsilon a\delta          \\
			\delta(1-\varepsilon a)              & <\varepsilon a^2                              \\
			\delta                               & <\frac{ \varepsilon a^2 }{ 1-\varepsilon a }.
		\end{aligned}
	\end{equation}
	Notons que, à \( \varepsilon\) fixé, plus \( a\) est petit, plus il faut choisir \( \delta\) petit. La fonction \( x\mapsto\frac{1}{ x }\) n'est donc pas uniformément continue. Cela correspond au fait que, proche de zéro, la fonction monte très vite. Une fonction uniformément continue sera une fonction qui ne montera jamais très vite.
\end{example}

\begin{proposition}     \label{PROPooVOUTooOtiGLG}
	Quelques propriétés des fonctions uniformément continues.
	\begin{enumerate}
		\item
		      Toute application uniformément continue est continue;
		\item
		      la composée de deux fonctions uniformément continues est uniformément continue;
	\end{enumerate}
	%TODOooLPUHooCldwAL. Prouver ça.
\end{proposition}
Nous verrons qu'une application lipschitzienne est uniformément continue (proposition~\ref{PROPooVZSAooUneOQK}).

Une fonction peut être uniformément continue sur un domaine et pas sur un autre. Le théorème suivant donne une importante indication à ce sujet.
\begin{theorem}[Heine]\index{théorème!Heine}\index{Heine (théorème)}		\label{ThoHeineContinueCompact}
	Soit \( K\) un compact de \( \eR^n\). Une fonction continue \( f\colon \eR^n\to \eR^m\) est uniformément continue sur \( K\).
\end{theorem}

\begin{proof}
	Nous allons prouver ce théorème par l'absurde. Nous commençons par écrire la condition qui exprime que \( f\) n'est pas uniformément continue sur le compact \( K\) :
	\begin{equation}
		\exists\varepsilon>0\tq\forall\delta>0,\,\exists x,y\in K\tqs \| x-y \|<\delta\text{ et }\big| f(x)-f(y) \big|>\varepsilon.
	\end{equation}
	En particulier (en prenant \( \delta=\frac{1}{ n }\) pour tout \( n\)), pour chaque \( n\) nous pouvons trouver \( x_n\) et \( y_n\) dans \( K\) qui vérifient simultanément les deux conditions suivantes :
	\begin{subequations}
		\begin{numcases}{}
			\| x_n-y_n \|<\frac{1}{ n }\\
			\big| f(x_n)-f(y_n) \big|>\varepsilon.	\label{EqCond3107fxfyepsppt}
		\end{numcases}
	\end{subequations}
	Nous insistons que c'est le même \( \varepsilon\) pour chaque \( n\). L'ensemble \( K\) étant compact, l'ensemble \( K\times K \) est compact (théorème~\ref{THOIYmxXuu}) et nous pouvons trouver une sous-suite convergente \emph{du couple} \( (x_n,y_n)\) dans \( K\times K\). Quitte à passer à ces sous-suites, nous supposons que \( (x_n,y_n)\) converge dans \( K\times K\) et en particulier, que les suites \( (x_n)\) et \( (y_n)\) sont convergentes. Étant donné que pour chaque \( n\) elles vérifient \( \| x_n-y_n \|<\frac{1}{ n }\), les limites sont égales :
	\begin{equation}
		\lim x_n=\lim y_n=x.
	\end{equation}
	L'ensemble \( K\) étant fermé, la limite \( x\) est dans \( K\). Par continuité de \( f\), nous avons finalement
	\begin{equation}
		\lim f(x_n)=\lim f(y_n)=f(x),
	\end{equation}
	mais alors
	\begin{equation}
		\lim_{n\to\infty}\big| f(x_n)-f(y_n) \big|=0,
	\end{equation}
	ce qui est en contradiction avec le choix \eqref{EqCond3107fxfyepsppt}.

	Tout ceci prouve que \( f(K)\) est bornée supérieurement et que \( f\) atteint son supremum (qui est donc un maximum). Le fait que \( f(K)\) soit bornée inférieurement se prouve en considérant la fonction \( -f\) au lieu de \( f\).
\end{proof}

\begin{remark}
	Nous pouvons ne pas utiliser le fait que le produit de compacts est compact.

	Pour choisir les sous-suites \( (x_n)\) et \( (y_n)\), il suffit de prendre une sous-suite convergente de \( (x_n)\) et d'invoquer le fait que \( \| x_n-y_n \|\leq \frac{1}{ n }\). Les suites \( (x_n)\) et \( (y_n)\) étant adjacentes\footnote{Définition \ref{DEFooDMZLooDtNPmu}.}, la convergence de \( (x_n)\) implique la convergence de \( (y_n)\) vers la même limite.

	Il est donc un peu superflu de parler de la convergence du couple \( (x_n,y_n)\).
\end{remark}

\begin{proposition}[Heine\cite{ooNDDIooKLdIWH}]     \label{PROPooBWUFooYhMlDp}
	Toute application continue d'un espace métrique compact dans un espace métrique quelconque est uniformément continue\footnote{Uniforme continuité, définition \ref{DEFooYIPXooQTscbG}.}.
\end{proposition}

\begin{proof}
	Soient un espace métrique compact \( X\) et un espace métrique quelconque \( E\). Nous considérons une application continue \( f\colon X\to E\).

	\begin{subproof}
		\spitem[Un ensemble]
		Soit \( \epsilon>0\). Nous considérons l'ensemble
		\begin{equation}
			K=\{ (x,y)\in X\times X\tq d\big( f(x), f(y) \big)\geq \epsilon \}.
		\end{equation}

		\spitem[Il est compact]
		L'espace \( X\) étant compact, \( X\times X\) est également compact par le théorème \ref{THOIYmxXuu}. Les fonctions \( f\) et \( d\) étant continues, l'application
		\begin{equation}
			\begin{aligned}
				\varphi\colon X\times X & \to \eR^+                       \\
				(x,y)                   & \mapsto d\big( f(x), f(y) \big)
			\end{aligned}
		\end{equation}
		est continue, de telle sorte que la partie \( \varphi\geq \epsilon\) est fermée. Un fermé dans un compact est compact par le lemme \ref{LemnAeACf}.

		\spitem[Une borne atteinte]
		Nous considérons l'application distance \( d\colon K\to \eR^+\). C'est une application continue sur le compact \( K\); donc elle atteint ses bornes (théorème des bornes atteintes, \ref{ThoMKKooAbHaro}). Elle a un minimum que nous notons \( \delta\).

		Comme \( (x,x)\notin K\), nous avons \( d(x,y)>0\) pour tout \( (x,y)\in K\). Et donc \( \delta>0\).

		\spitem[Conclusion]
		Si \( x,y\in X\) sont tels que \( d(x,y)<\delta\), alors \( (x,y)\notin K\). De ce fait nous avons
		\begin{equation}
			d\big( f(x), f(y) \big)<\epsilon.
		\end{equation}
		D'où l'uniforme continuité de \( f\) sur \( X\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]            \label{LEMooIVAKooUiEENr}
	Soit une fonction continue \( f\colon \mathopen[ a , b \mathclose]\times \mathopen[ c , d \mathclose]\to \eR\). Pour chaque \( x\in \mathopen[ a , b \mathclose]\) nous définissons
	\begin{equation}
		\begin{aligned}
			f_x\colon \mathopen[ c , d \mathclose] & \to \eR         \\
			y                                      & \mapsto f(x,y).
		\end{aligned}
	\end{equation}
	Alors l'application
	\begin{equation}
		\begin{aligned}
			g\colon \mathopen[ a , b \mathclose] & \to \eR                    \\
			x                                    & \mapsto \| f_x \|_{\infty}
		\end{aligned}
	\end{equation}
	est continue.
\end{lemma}

\begin{proof}
	Soit \( \alpha\in\mathopen[ a , b \mathclose]\), et prouvons la continuité de \( g\) en \( \alpha\).
	\begin{subproof}
		\spitem[Le décor]
		Nous considérons l'espace vectoriel normé \( \big( C^0(\mathopen[ a , b \mathclose]),\| . \|_{\infty} \big)\) des fonctions continues sur \( \mathopen[ a , b \mathclose]\) muni de la norme uniforme. Prouvons que si \( x_k\stackrel{\mathopen[ a , b \mathclose]}{\longrightarrow}\alpha\), alors \( f_{x_k}\stackrel{unif}{\longrightarrow}f_{\alpha}\).

		\spitem[Module de continuité]
		Pour cela nous avons le calcul suivant, avec justifications juste en-dessous :
		\begin{subequations}
			\begin{align}
				\| f_{x_k}-f_{\alpha} \|_{\infty} & =\sup_{y\in\mathopen[ c , d \mathclose]}\| f(x_k,y)-f(\alpha,y) \|                                                                 \\
				                                  & \leq\sup_{y\in\mathopen[ c , d \mathclose]}\big| \omega_f\big( \| (x_k,y)-(\alpha,y) \| \big) \big|    \label{SUBEQooCOJQooWlvHUa} \\
				                                  & =\sup_{y\in\mathopen[ c , d \mathclose]}| \omega_f(| x_k-\alpha |) |       \label{SUBEQooGLYMooEZKRKm}                             \\
				                                  & =\omega_f(| x_k-\alpha |).
			\end{align}
		\end{subequations}
		Justifications.
		\begin{itemize}
			\item Pour \eqref{SUBEQooCOJQooWlvHUa}. Utilisation du module de continuité, définition \ref{DEFooYARJooYyzMMP}.
			\item Pour \eqref{SUBEQooGLYMooEZKRKm}. La norme dans \( \eR^2\) de \( (x_k,y)-(\alpha,y)\).
		\end{itemize}
		\spitem[Uniforme continuité]
		La fonction \( f\) est continue sur le compact \( \mathopen[ a , b \mathclose]\times \mathopen[ c , d \mathclose]\). Elle est donc uniformément continue par le théorème de Heine \ref{PROPooBWUFooYhMlDp}, et donc son module de continuité vérifie \( \lim_{h\to 0} \omega_f(h)=0\) par \ref{LemeERapq}.

		Nous avons donc
		\begin{equation}
			\| f_{x_k}-f_{\alpha} \|_{\infty} \leq \omega_f(| x_k-\alpha |)\stackrel{\eR}{\longrightarrow}0.
		\end{equation}
		Nous avons donc prouvé que si \( x_k\stackrel{\eR}{\longrightarrow}\alpha\), alors \( f_{x_k}\stackrel{\| . \|_{\infty}}{\longrightarrow}f_{\alpha}\).

		\spitem[Conclusion]
		La norme étant une application continue, nous en déduisons que si \( x_k\to \alpha\), alors \( \| f_{x_k} \|_{\infty}\to \| f_{\alpha} \|_{\infty}\).

		Ceci est la continuité séquentielle de la fonction \( g\), et donc la continuité tout court.
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Fonctions sur un compact}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Par le théorème des valeurs intermédiaires \ref{ThoValInter}, l'image d'un intervalle par une fonction continue est un intervalle, et nous avons l'importante propriété suivante des fonctions continues sur un compact.

Le théorème suivant est un cas particulier du théorème~\ref{ThoMKKooAbHaro}.
\begin{theorem}
	Si \( f\) est une fonction continue sur l'intervalle compact \( [a,b]\). Alors \( f\) est bornée sur \( [a,b]\) et elle atteint ses bornes.
\end{theorem}

\begin{proof}
	Étant donné que \( [a,b]\) est un intervalle compact, son image est également un intervalle compact, et donc est de la forme \( [m,M]\). Ceci découle du théorème~\ref{ThoImCompCotComp} et le corolaire~\ref{CorImInterInter}. Le maximum de \( f\) sur \( [a,b]\) est la borne \( M\) qui est bien dans l'image (parce que \( [m,M]\) est fermé). Idem pour le minimum \( m\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynômes, théorème de d'Alembert}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

L'algèbre des polynômes sur un anneau est définie en \ref{DEFooFYZRooMikwEL}. Si \( P\in A[X]\) et si \( \alpha\in A\) nous avons également défini l'évaluation de \( P\) en \( \alpha\); c'est la définition \ref{DEFooNXKUooLrGeuh}. Dans le cadre de l'analyse, lorsque nous considérons des polynômes, nous allons complètement confondre le polynôme avec la fonction qu'il définit.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes sur les réels}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooJKYJooFqbQMr}
	Tout polynôme à coefficients réels de degré impair possède au moins une racine réelle.
\end{proposition}

\begin{proof}
	Nous mettons le plus haut degré en facteur :
	\begin{equation}
		P(x)=\sum_{k=0}^na_kx^k=x^n\sum_{k=0}^n\frac{ a_k }{ x^{n-k} }.
	\end{equation}
	Le terme en \( k=n\) vaut \( a_nx^n\) tandis que les autres sont de la forme (à un coefficient près) \( \frac{1}{ x^l }\) pour un \( l\geq 1\). Lorsque \( x\to \infty\), chacun de ces termes s'annule (lemme \ref{LEMooFCIXooJuHFqk}). Nous avons donc
	\begin{equation}
		\lim_{x\to \infty} P(x)=\infty,
	\end{equation}
	et de même, \( n\) étant impair, \( \lim_{x\to -\infty} P(x)=-\infty\). Le théorème des valeurs intermédiaires \ref{ThoValInter} nous donne alors l'existence d'un réel sur lequel \( P\) s'annule.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooARLAooMNnNsz}
	À propos de réductibilité
	\begin{enumerate}
		\item
		      Si \( z\in \eC\) est une racine d'un polynôme à coefficients réels, alors le conjugué complexe \( \bar z\) est également une racine.
		\item
		      Tout polynôme à coefficients réels de degré au moins \( 3\) est réductible\footnote{Définition \ref{DeirredBDhQfA}.} sur \( \eR\).
	\end{enumerate}
	%TODOooRHLTooWIAqwz, prouver ça.
\end{proposition}


La proposition suivante donne une utilisation amusante de la notion de polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.}.
\begin{proposition}[\cite{ooNGUJooPphdsT}]      \label{PROPooKJWOooOjSFaA}
	Soit un espace vectoriel \( E\) de dimension finie pour lequel il existe un endomorphisme \( f\colon E\to E\) tel que \( (f\circ f)(x)=-x\) pour tout \( x\in E\). Alors la dimension de \( E\) est paire.
\end{proposition}

\begin{proof}
	Cherchons les valeurs propres de \( f\) en résolvant l'équation \( f(x)=\lambda x\). Nous appliquons \( f\) à cette égalité :
	\begin{equation}
		-x=\lambda f(x)=\lambda^2x.
	\end{equation}
	Donc \( \lambda\) ne peut pas être réel. Nous avons montré que \( f\) n'a pas de valeur propre réelle. Or le polynôme caractéristique de \( f\) est de degré égal à la dimension de l'espace \( E\) par le lemme \ref{LemooWCZMooZqyaHd}\ref{ITEMooAALOooGlQYOs}.

	Si la dimension de \( E\) est impaire, le polynôme caractéristique est de degré impair, et possède donc une racine réelle\footnote{Proposition \ref{PROPooJKYJooFqbQMr}.}. Autrement dit, l'absence de racines réelles au polynôme caractéristique indique une dimension paire.
\end{proof}

\begin{normaltext}
	Une autre preuve possible pour \ref{PROPooKJWOooOjSFaA} est d'utiliser le déterminant : si la dimension de \( E\) est \( n\) nous avons :
	\begin{equation}
		\det(f^2)=\det(-\id)=(-1)^n.
	\end{equation}
	Donc \( (-1)^n\) est positif, ce qui montre que \( n\) est pair.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes sur les complexes}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons parler de comportement asymptotique de polynômes définis sur \( \eC\). La topologie que nous considérons est celle de la compactification en un point, décrite en \ref{PROPooHNOZooPSzKIN}.

Le lemme suivant donne une caractérisation de la limite en l'infini dans le compactifié \( \hat \eC\). Dans beaucoup de cas, cette caractérisation est prise comme la définition de la limite. Hélas, dans le Frido nous sommes des extrémistes et nous ne parvenons pas à dire le mot «limite» si il n'y a pas une topologie.
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooERABooQjLBzW}
	Nous considérons la compactification en un point d'Alexandrov\footnote{Définition \ref{PROPooHNOZooPSzKIN}.}. Soit une fonction \( f\colon \eC\to \eC\). Nous avons \( \lim_{z\to \infty} f(z)=\infty\) si et seulement si pour tout \( M>0\), il existe \( R>0\) tel que \( | z |>R\) implique \( | f(z) |>M\).
\end{lemma}

\begin{proof}
	Souvenons-nous que, en général\footnote{Définition \ref{DefYNVoWBx}.}, nous avons
	\begin{equation}
		\lim_{x\to a} f(x)=b
	\end{equation}
	si pour tout voisinage \( V\) de \( b\), il existe un voisinage \( W\) de \( a\) tel que \( z\in W\setminus\{ a \}\) implique \( f(z)\in V\).

	Précisons encore un point de notation. Si \( K\) est une partie de \( \eC\), nous notons \( K^c\) son complémentaire dans \( \eC\), pas dans \( \hat  \eC\).

	Ceci étant dit, nous passons à la preuve.
	\begin{subproof}
		\spitem[Sens direct]
		Nous supposons que \( \lim_{z\to \infty} f(z)=\infty\). Soit \( M>0\); nous considérons le voisinage \( V=\overline{ B(0,M) }^c\cup\{ \infty \}\). Par définition de la limite, il existe un voisinage \( W\) de \( \infty\) tel que \( z\in W\Rightarrow f(z)\in V\setminus\{ \infty \}=\overline{ B(0,M) }^c\). Ce voisinage est de la forme \( K^c\cup\{ \infty \}\). Puisque \( K\) est compact, il est borné, et il existe \( R>0\) tel que \( K\subset B(0,R)\).

		Avec tout cela nous avons la chaine suivante d'implications :
		\begin{equation}
			| z |>R\Rightarrow z\in K^c\Rightarrow z\in W\Rightarrow f(z)\in V\setminus\{ \infty \}=\overline{ B(0,M) }^c\Rightarrow | f(z) |>M.
		\end{equation}
		C'est bien la propriété que nous voulions.
		\spitem[Sens réciproque]
		Soit un voisinage \( V\) de \( \infty\). Nous avons \( V=K^c\cup\{ \infty \}\) où \( K\) est compact dans \( \eC\). Il existe \( M>0\) tel que \( K\subset B(0,M)\).

		Par hypothèse, il existe \( R\) tel que \( | z |>R\Rightarrow | f(z) |>M\). Soit \( W=\overline{ B(0,R) }^c\cup\{ \infty \}\). Nous avons la chaine
		\begin{equation}
			z\in W\Rightarrow| z |>R\Rightarrow| f(z) |>M\Rightarrow f(z)\in K^c\Rightarrow f(z)\in V.
		\end{equation}
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]  \label{PROPooPWVWooGuftxZ}
	Soit le polynôme
	\begin{equation}
		\begin{aligned}
			P\colon \eC & \to \eC                     \\
			z           & \mapsto \sum_{i=0}^n a_iz^i
		\end{aligned}
	\end{equation}
	où nous sous-entendons que \( a_n\neq 0\). La fonction \( z\mapsto | P(z) |\) est équivalente\footnote{Définition \ref{DEFooWDSAooKXZsZY}.} en l'infini à la fonction
	\begin{equation}
		\begin{aligned}
			w\colon \eC & \to \eR^+           \\
			z           & \mapsto | a_nz^n |.
		\end{aligned}
	\end{equation}
\end{proposition}

\begin{proof}
	Nous voudrions prouver qu'il existe une fonction \( \alpha\colon \eC\to \eR\) telle que
	\begin{subequations}     \label{EQooGXWZooDJZNzE}
		\begin{numcases}{}
			| \sum_{i=0}^n a_iz^i |=\big( 1+\alpha(z) \big)| a_nz^n |.         \\
			\lim_{z\to \infty} \alpha(z)=0.
		\end{numcases}
	\end{subequations}
	Nous trouvons un candidat pour être une telle fonction en isolant simplement \( \alpha(z)\) de cette égalité. Nous trouvons
	\begin{equation}
		\alpha(z)=\left| \sum_{i=0}^n\frac{ a_i }{ a_n }z^{i-n} \right|-1.
	\end{equation}
	Elle vérifie immédiatement \eqref{EQooGXWZooDJZNzE}. Le point qui fait intervenir la topologie de \( \hat \eC\) est de vérifier que \( \lim_{z\to \infty} \alpha(z)=0\). Le terme \( i=n\) de la somme vaut \( 1\). Il suffit donc de montrer que pour \( i\neq n\) nous avons
	\begin{equation}
		\lim_{z\to \infty} \frac{1}{ z^{n-i} }=0.
	\end{equation}
	Soit \( \epsilon>0\). Nous devons prouver qu'il existe un voisinage \( V\) de \( \infty\) dans \( \hat \eC\) tel que
	\begin{equation}
		\left| \frac{1}{ z^{n-i} }-0 \right|\leq \epsilon
	\end{equation}
	pour tout \( z\in V\).

	En utilisant la proposition \ref{PROPooXLARooYSDCsF} nous avons déjà
	\begin{equation}
		\left| \frac{1}{ z^{n-i} } \right|=\frac{1}{ | z^{n-i} | }=\frac{1}{ | z |^{n-i} }.
	\end{equation}
	Soit \( R>0\) tel que \( \frac{1}{ R }<\epsilon\). Nous considérons le voisinage \( \{ | z |>R \}\cup \{ \infty \}\) de \( \infty\). Dans ce voisinage, nous avons
	\begin{equation}
		\frac{1}{ | z |^{n-i} }\leq \frac{1}{ | z | }\leq \frac{1}{ R }<\epsilon.
	\end{equation}
	Et voilà.
\end{proof}

Le lemme suivant parle de polynôme sur \( \eC\). Vous pouvez l'adapter à \( \hat \eR\) et \( \bar \eR\).
\begin{lemma}       \label{LEMooYZVGooXZvBAc}
	Si \( P\colon \eC\to \eC\) est un polynôme, alors \( | P |\) atteint une borne inférieure globale.
\end{lemma}

\begin{proof}
	Nous savons, par l'équivalence de fonctions prouvée dans la proposition \ref{PROPooPWVWooGuftxZ} que \( \lim_{z\to \infty} P(z)=\infty\). Soit \( a>0\) dans \( \eR\). Par le lemme \ref{LEMooERABooQjLBzW} il existe un \( R>a\) tel que \( | z |>R\Rightarrow | f(z) |>| f(a) |\).

	La fonction \( | P |\) est continue sur le compact \( \overline{ B(0,R) }\). Soit \( z_0\) le point de minimum\footnote{Théorème de Weierstrass \ref{ThoWeirstrassRn}.} de \( | P |\) sur \( \overline{ B(0,R) }\).

	Nous devons prouver que \( z_0\) donne même un minimum global. Comme \( a\in\overline{ B(0,R) }\) nous avons
	\begin{equation}
		| P(z_0) |\leq | P(a) |.
	\end{equation}
	Si \( z\in \overline{ B(0,R) }^c\), nous avons
	\begin{equation}
		| P(z) |>| P(a) |\geq | P(z_0) |.
	\end{equation}
	Donc ce \( z_0\) est un minimum sur \( B(0,R)\) et sur \( \overline{ B(0,R) }^c\). Bref, un minimum global.
\end{proof}

\begin{lemma}       \label{LEMooTTOYooXaukuH}
	Soit le polynôme
	\begin{equation}
		\begin{aligned}
			P\colon \eC & \to \eC                     \\
			z           & \mapsto \sum_{i=0}^na_iz^i.
		\end{aligned}
	\end{equation}
	À part si \( a_0=a_1=0\), la fonction \( P\) est équivalente à \( a_0+a_1z\) en \( z=0\).
\end{lemma}

Note: si \( a_0=a_1=0\), alors évidemment \( P\) devrait être équivalente à \( 0\), ce qui est évidemment faux : la fonction identiquement nulle tend vers zéro plus vite que n'importe quoi.

\begin{proof}
	En posant \( g(z)=a_0+a_1z\), nous devons trouver une fonction \( \alpha\) telle que
	\begin{equation}        \label{EQooZFJBooVAYVBv}
		P(z)=\big( 1+\alpha(z) \big)g(z).
	\end{equation}
	Si \( a_0\neq 0\), il existe un voisinage épointé de \( z=0\) sur lequel la fonction
	\begin{equation}        \label{EQooVCOVooAKWJxF}
		\alpha(z)=\frac{ z^2\sum_{i=2}^na_iz^{i-2} }{ a_0+a_1z }
	\end{equation}
	existe. Il n'y a aucun problème à ce que \( \alpha(z)\to 0\) pour \( z\to 0\)\footnote{En remarquant toutefois que c'est une limite à deux dimensions. Sachez la définir.}, et un simple calcul\footnote{En fait, la formule \eqref{EQooVCOVooAKWJxF} est obtenue en isolant \( \alpha(z)\) dans \eqref{EQooZFJBooVAYVBv}.} donne \eqref{EQooVCOVooAKWJxF}.

	Si par contre \( a_0=0\), nous faisons le calcul intermédiaire suivant :
	\begin{equation}
		\alpha(z)g(z)=P(z)-g(z)=z^2\sum_{i=2}^na_iz^{i-2},
	\end{equation}
	et donc, en isolant \( \alpha(z)\) et en simplifiant par \( z\), nous voyons que la fonction \( \alpha\) définie par
	\begin{equation}
		\alpha(z)=\frac{z}{ a_1 }\sum_{i=2}^na_iz^{i-2}
	\end{equation}
	convient.
\end{proof}

\begin{proposition}[\cite{ooRIPVooMlBiAH,MonCerveau}]       \label{PROPooLBBLooQwEiHr}
	Soient \( a,b\in \eR\).
	\begin{enumerate}
		\item       \label{ITEMooSPSWooKLtqzZ}
		      L'équation \( z^2=a+bi\) a une solution dans \( \eC\).
		\item       \label{ITEMooQOJDooWjfGXv}
		      Pour tout \( l\), l'équation \( z^{2^l}=a+bi\) a une solution dans \( \eC\).
	\end{enumerate}
	Nous ne disons pas que ces solutions sont uniques\footnote{Comme vous en conviendrez en pensant à \( z^2=1\) qui a déjà les solutions \( 1\) et \( -1\).}.
\end{proposition}

\begin{proof}
	Pour prouver \ref{ITEMooSPSWooKLtqzZ}, l'équation \( z^2=a+bi\) a pour solution \( \pm\xi\) où\footnote{Si vous vous demandez où sont définies les racines carrés, c'est \ref{DEFooGQTYooORuvQb}.}
	\begin{equation}
		\xi=\sqrt{ \frac{ 1 }{2}a+\frac{ 1 }{2}\sqrt{ a^2+b^2 } }+i\signe(b)\sqrt{ -\frac{ 1 }{2}a+\frac{ 1 }{2}\sqrt{ a^2+b^2 } }.
	\end{equation}
	Nous n'avons en fait pas besoin de montrer que \( \pm\xi\) sont toutes deux des solutions, ni que ce sont les seules. Un calcul direct montre que \( \xi^2=a+bi\) et nous sommes contents.

	Pour \ref{ITEMooQOJDooWjfGXv}, nous faisons une récurrence sur \( l\). Nous savons que
	\begin{equation}
		z^{2^{k+1}}=(z^{2^k})^2.
	\end{equation}
	Soit \( \xi\in \eC\) tel que \( \xi^{2^k}=a+bi\); un tel \( \xi\) existe par hypothèse de récurrence. Alors si \( z\) est tel que \( z^2=\xi\), nous avons
	\begin{equation}
		z^{2^{k+1}}=a+bi.
	\end{equation}
\end{proof}

Le théorème de d'Alembert possède de nombreuses démonstrations. En voici une qui à ma connaissance est celle demandant le moins d'analyse; une démonstration à base de théorie de Galois peut être trouvée dans \cite{rqrNyg,ooPSLMooAVODjn}. Si vous lisez ces lignes pour savoir qu'un polynôme de degré \( n\) possède au \emph{maximum} \( n\) racines, ce n'est pas ici qu'il faut regarder, mais le corolaire \ref{CORooUGJGooBofWLr}.

Une très jolie démonstration sur numberphile :\\ \url{https://www.youtube.com/watch?v=shEk8sz1oOw}.

\begin{theorem}[d'Alembert\cite{ooRIPVooMlBiAH}]   \label{THOooIRJYooBiHRyW}
	Le corps \( \eC\) est algébriquement clos\footnote{Définition \ref{DEFooYZOYooAesmnP}.} : tout polynôme non constant à coefficients complexes admet au moins une racine complexe\footnote{C'est la définition \ref{DEFooYZOYooAesmnP} d'être algébriquement clos.}.
\end{theorem}

\begin{proof}
	Nous effectuons une preuve tout à la fois par l'absurde et par récurrence en supposant que le polynôme
	\begin{equation}
		\begin{aligned}
			f\colon \eC & \to \eC                           \\
			z           & \mapsto z^n+a_1z^{n-1}+\ldots+a_n
		\end{aligned}
	\end{equation}
	n'a pas de racine dans \( \eC\), et que \( n\) soit le plus petit entier pour lequel un tel polynôme existe. Nous notons
	\begin{equation}
		n=2^k m
	\end{equation}
	où \( m\) est impair.

	Le lemme \ref{LEMooYZVGooXZvBAc} donne un point \( z_0\) qui réalise le minimum global de \( | f |\) sur \( \eC\). Nous posons \( g(z)=f(z_0+z)\) et nous définissons ses coefficients \( A_i\) par
	\begin{equation}
		g(z)=\sum_{i=0}^n A_iz^i.
	\end{equation}
	Nous avons \( A_n=1\) et \( | A_0 |=| f(z_0) |\). Soit \( A_r\) le premier à être non nul parmi les \( A_1\), \( A_2\), \ldots.
	\begin{subproof}
		\spitem[Si \( r<n\)]
		Par hypothèse de récurrence, il existe \( \xi\in \eC\) tel que \( \xi^r=-A_1/A_r\). Nous avons
		\begin{equation}
			g(t\xi)=A_0+\frac{ -A_rt^rA_0 }{ A_r }+t^{r+1}\sum_{i=r+1}^nA_i\xi^it^{i-r-1}.
		\end{equation}
		En notant \( P(t)\) le dernier polynôme, nous pouvons écrire cela sous forme compacte :
		\begin{equation}
			g(t\xi)=A_0-t^rA_0+t^{r+1}P(t).
		\end{equation}
		Puisque
		\begin{equation}
			\lim_{t\to 0} \frac{ t^{r+1}P(t) }{ t^r| A_0 | }=\lim_{t\to 0} tP(t)=0,
		\end{equation}
		il existe \( t_0>0\) tel que
		\begin{equation}
			| t_0^{r+1}P(t_0) |<| A_0t_0^r |.
		\end{equation}
		Nous choisissons de plus \( t_0<1\), de telle sorte que \( 1-t^r>0\). Avec cela nous avons
		\begin{equation}
			| g(t\xi) |\leq | A_0 |(1-t^r)+| t^{r+1}P(t) |=| A_0 |\underbrace{-t^r| A_0 |+| t^{r+1}P(t) |}_{<0}<| A_0 |.
		\end{equation}
		Or \( | A_0 |\) était un minimum global de \( | g |\). Contradiction.

		\spitem[Si \( r=n\)]

		Dans ce cas,
		\begin{equation}
			g(z)=f(z_0+z)=A_0+z^n,
		\end{equation}
		et nous rappelons que \( n=2^km\) où \( m\) est impair. Nous allons trouver une contradiction dans les quatre cas \( \real(A_0)>0\), \( \real(A_0)<0\), \( \imag(A_0)>0\) et \( \imag(A_0)<0\). Bien entendu ces cas se recouvrent largement, mais en toute généralité, nous avons besoin des quatre.
		\begin{subproof}
			\spitem[Si \( \real(A_0)>0\)]
			La proposition \ref{PROPooLBBLooQwEiHr} nous permet de considérer \( v\in \eC\) tel que \( v^{2^k}=-1\). Nous avons alors
			\begin{equation}
				g(tv)=A_0+(tv)^n=A_0+t^n(v^{2^k})^m=A_0+t^n(-1)^m=A_0-t^n
			\end{equation}
			parce que \( m\) est impair. Nous avons \( \imag\big( g(tv) \big)=\imag(A_0)\). Si \( t\) est assez petit pour que \( t^n<| \real(A_0) |\) nous avons aussi \( |\real\big( g(tv) \big)|<| \real(A_0) |\). Donc
			\begin{equation}
				| g(tv) |^2=| \real\big( g(tv) \big) |^2+| \imag\big( g(tv) \big) |^2 < | \real(A_0) |^2+| \imag(A_0) |^2=| A_0 |^2.
			\end{equation}
			Donc \( | g(tv) |<| A_0 |\). Contradiction.
			\spitem[Si \( \real(A_0)<0\)]
			Nous prenons \( v=1\), et même histoire.
			\spitem[Si \( \imag(A_0)<0\)]
			Nous prenons \( w\in \eC\) tel que
			\begin{equation}
				w^{2^k}=i(-1)^{\frac{ 1 }{2}(m-1)}.
			\end{equation}
			Là, il y a un peu d'arrachage de cheveux pour bien voir les cas. La difficulté est que les puissances de \( i\) alternent entre \( 1\), \( -1\), \( i\) et \( -i\). Puisque \( m\) est impair, nous avons un \( l\) tel que \( m=2l+1\). Nous subdivisons les cas \( l\) pair et \( l\) impair.
			\begin{subproof}
				\spitem[Si \( l\) est pair]
				Alors d'une part \( \frac{ 1 }{2}(m-1)=l\) est pair et donc
				\begin{equation}
					(-1)^{\frac{ 1 }{2}(m-1)}=1.
				\end{equation}
				Et d'autre part, \( i^{2l+1}=(-1)^li=i\). En tout,
				\begin{equation}
					i^m(-1)^{\frac{ 1 }{2}(m-1)}=i.
				\end{equation}
				\spitem[Si \( l\) est impair]
				Alors \( \frac{ 1 }{2}(m-1)=l\) et \( (-1)^{\frac{ 1 }{2}(m-1)}=-1\). Mais en même temps, \( i^{2l+1}=-i\), ce qui donne encore une fois
				\begin{equation}
					i^m(-1)^{\frac{ 1 }{2}(m-1)}=i.
				\end{equation}
			\end{subproof}
			Bref, que \( l\) soit pair ou impair, nous avons \( i^m(-1)^{\frac{ 1 }{2}(m-1)}=i\).
		\end{subproof}
		Nous avons donc \( \real\big( g(tw) \big)=\real(A_0)\) et \( \imag\big( g(tw) \big)<\imag(A_0)\). Encore contradiction.
		\spitem[Si \( \imag(A_0)>0\)]
		Même chose que ce que nous venons de faire, mais avec
		\begin{equation}
			w^{2^k}=-i(-1)^{\frac{ 1 }{2}(m-1)}.
		\end{equation}
	\end{subproof}
\end{proof}


\begin{corollary}[\cite{MonCerveau}]       \label{CORooKKNWooWEQukb}
	Tout polynôme de degré \( 3\) à coefficients réels possède au moins une racine réelle.
\end{corollary}

\begin{proof}
	Soient les racines \( \lambda_1\), \( \lambda_2\) et \( \lambda_3\) du polynôme en question. Toutes trois sont dans \( \eC\). Supposons que \( \lambda_1\) ne soit pas réelle. Alors \( \lambda_2\) ou \( \lambda_3\) doit être égale à \( \bar\lambda_1\). Disons \( \lambda_2\). Nous avons donc les racines \( \lambda_1\), \( \bar\lambda_1\) et \( \lambda_3\). Le polynôme se factorise alors en
	\begin{equation}        \label{EQooELMMooNbpBgg}
		a(X-\lambda_1)(X-\bar\lambda_1)(X-\lambda_3).
	\end{equation}
	Le coefficient \( a\) doit être réel parce qu'il est le coefficient du terme en \( X^3\) (réel par hypothèse). Si \( \lambda_3\) n'est pas réel, alors ce polynôme ne peut pas avoir des coefficients réels. Entre autres parce que le terme indépendant est \( a| \lambda_1 |^2\lambda_3\), qui est réel si et seulement si \( \lambda_3\) est réel\footnote{Notez l'utilisation de la proposition \ref{PROPooUMVGooIrhZZg}\ref{ITEMooYBJVooGXiDSd}.}.
\end{proof}
Tant que vous y êtes, vous pouvez voir que le polynôme \eqref{EQooELMMooNbpBgg} est à coefficients réels si et seulement si \( a\in \eR\) et \( \lambda_3\in \eR\).

\begin{example}     \label{EXooIPLOooSNfiWg}
	Toute application linéaire \( \eR^3\to \eR^3\) a un vecteur propre. En effet si \( R\colon \eR^3\to \eR^3\) est linéaire, son polynôme caractéristique \( \chi_R\) est de degré \( 3\). Le corolaire \ref{CORooKKNWooWEQukb} indique qu'un tel polynôme possède au moins une racine réelle.
	Une telle racine est une valeur propre de \( R\) par le théorème \ref{ThoWDGooQUGSTL}.
\end{example}

\begin{definition}      \label{DEFooVFTYooJyDePn}
	Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!valeur propre!algébrique} de la valeur propre \( \lambda\) de \( u\). À ne pas confondre avec la \defe{multiplicité géométrique}{multiplicité!valeur propre!géométrique} qui sera la dimension de l'espace propre.
\end{definition}

\begin{lemma}       \label{LEMooPJKEooQwuWMb}
	Si \( \alpha\in \eC\) n'est pas réel, alors les polynômes \( X-\alpha\) et \( X-\bar\alpha\) sont premiers entre eux dans \( \eC[X]\).
\end{lemma}

\begin{proof}
	Un polynôme diviseur commun de \( X-\alpha\) et \( X-\bar \alpha\) doit être de la forme \( aX+b\) parce qu'un diviseur ne peut pas être de degré plus haut. Si nous voulons que le diviseur commun soit non inversible, il faut \( a\neq 0\).

	Dire que \( aX+b\) divise \( X-\alpha\) signifie qu'il existe \( s\in \eC\) tel que \( (aX+b)s=X-\alpha\). Cela donne \( s=1/a\) et \( b=-\alpha/s\). En substituant la première dans la seconde, nous trouvons \( b=-\alpha a\).

	En recommençant avec \( X-\bar\alpha\), nous trouvons également la relation \( b=-\bar \alpha a\). En égalisant,
	\begin{equation}
		-\alpha a=-\bar \alpha a.
	\end{equation}
	En simplifiant, \( \alpha=\bar \alpha\). Cela contredit l'hypothèse que \( \alpha\) n'est pas réel.
\end{proof}

\begin{proposition}     \label{PROPooUMDQooVmfDYU}
	Un polynôme irréductible à coefficients réels est, soit de degré un, soit de degré \( 2\) avec un discriminant négatif.
\end{proposition}

\begin{proof}
	Soit un polynôme \( P\) à coefficients réels de degré plus grand que \( 1\). Alors le théorème de d'Alembert-Gauss (théorème~\ref{THOooIRJYooBiHRyW}) implique l'existence d'une racine \( \alpha \in \eC \). Si \( \alpha\) est un réel, \( P\) est réductible. Si \( \alpha\) n'est pas réel, alors son conjugué complexe \( \bar \alpha\) est également une racine. Par conséquent les polynômes \( (X-\alpha)\) et \( (X-\bar \alpha)\) divisent \( P\) dans \( \eC[X]. \).

	Ces deux polynômes sont premiers entre eux par le lemme \ref{LEMooPJKEooQwuWMb}. Par conséquent le produit
	\begin{equation}
		X^2-(\alpha+\bar \alpha)X+\alpha\bar\alpha
	\end{equation}
	divise également \( P\). Ce dernier est un polynôme à coefficients réels de degré \( 2\). Donc tout polynôme de degré \( 3\) ou plus est réductible.
\end{proof}

\begin{proposition}     \label{PROPooLXGSooXmVcVG}
	Si \( E\) est un espace vectoriel sur \( \eC\), tout endomorphisme possède au moins une valeur propre.
\end{proposition}

\begin{proof}
	Soit un endomorphisme \( u\) sur \( E\). Le théorème \ref{ThoWDGooQUGSTL} dit que \( \lambda\in \eC\) est une valeur propre si et seulement si \( \lambda\) est une racine du polynôme caractéristique \( \chi_u\). Or ce polynôme possède au moins une racine dans \( \eC\) par le théorème de d'Alembert \ref{THOooIRJYooBiHRyW}.
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Trigonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : généralités}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooMCOGooEoQCsz}

\begin{definition}[\cite{MQMKooPBfnZN}]
	Une matrice dans \( \eM(n,\eK)\) est \defe{trigonalisable}{matrice!trigonalisable} lorsqu'elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice triangulaire supérieure.
\end{definition}

\begin{proposition}[Trigonalisation et polynôme caractéristique scindé] \label{PropKNVFooQflQsJ}
	Soit \( u\) un endomorphisme d'un espace vectoriel \( E\) sur le corps \( \eK\). Les points suivants sont équivalents.
	\begin{enumerate}
		\item   \label{ItemZKDMooOrTHkwi}
		      L'endomorphisme \( u\) est trigonalisable (auquel cas les valeurs propres sont sur la diagonale).
		\item   \label{ItemZKDMooOrTHkwii}
		      Le polynôme caractéristique de \( u\) est scindé\footnote{Définition~\ref{DefCPLSooQaHJKQ}.}.
	\end{enumerate}
\end{proposition}
\index{trigonalisation!et polynôme caractéristique}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\ref{ItemZKDMooOrTHkwii}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwi}]
		Nous avons par hypothèse
		\begin{equation}
			\chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i}
		\end{equation}
		où les \( \lambda_i\) sont les valeurs propres de \( u\). Le théorème de Cayley-Hamilton~\ref{ThoCalYWLbJQ} dit que \( \chi_u(u)=0\), ce qui permet d'utiliser le théorème de décomposition des noyaux~\ref{ThoDecompNoyayzzMWod} :
		\begin{equation}
			E=\ker(X-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(X-\lambda_r)^{\alpha_r}.
		\end{equation}
		Les espaces \( F_{\lambda_i}(u)=\ker(u-\lambda_i)^{\alpha_i}\) sont les espaces caractéristiques de \( u\), ce qui fait que \( u-\lambda_i\mtu\) est nilpotent sur \( F_{\lambda_i}(u)\). L'endomorphisme \( u-\lambda_i\mtu\) est donc strictement trigonalisable supérieur sur son bloc\footnote{Proposition~\ref{PropMWWJooVIXdJp}.}. Cela signifie que \( u\) est triangulaire supérieure avec les valeurs propres sur la diagonale.

		\spitem[\ref{ItemZKDMooOrTHkwi}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwii}]

		C'est immédiat parce que le déterminant d'une matrice triangulaire est le produit des éléments de sa diagonale.
	\end{subproof}
\end{proof}

\begin{remark}
	La méthode des pivots de Gauss\footnote{Le lemme~\ref{LemZMxxnfM}.} certes permet de trigonaliser n'importe quelle matrice, mais elle ne correspond pas à un changement de base. Autrement dit, les pivots de Gauss ne sont pas des similitudes.

	C'est là qu'il faut bien avoir en tête la différence entre \emph{équivalence} et \emph{similarité}\footnote{Définition \ref{DefBLELooTvlHoB}.}. Lorsqu'on parle de changement de base, de matrice trigonalisable ou diagonalisable, nous parlons de similarité et non d'équivalence.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

La proposition~\ref{PropKNVFooQflQsJ} dit déjà que tous les endomorphismes sont trigonalisables sur \( \eC\). Nous allons aller plus loin et montrer que la trigonalisation peut être effectuée à l'aide d'une matrice unitaire.

Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque~\ref{RemXFZTooXkGzQg} utilisant la proposition~\ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe, trigonalisation\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
	Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire\footnote{Définition \ref{DEFooKEBHooWwCKRK}.} \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure\footnote{«triangulaire supérieure» ne signifie pas «strictement triangulaire supérieure». Ici, il est possible que la diagonale soit non nulle; non seulement possible, mais même très probable en pratique.}. La diagonale de la matrice triangulaire contient alors les valeurs propres de \( A\).
	%TODOooREVSooDYCRpp : prouver la partie sur la diagonale. C'est sur ma liste.
\end{lemma}
\index{lemme!Schur complexe}

\begin{proof}
	Étant donné que \( \eC\) est algébriquement clos\footnote{Algébriquement clos, définition \ref{DEFooYZOYooAesmnP}. Le fait que \( \eC\) le soit est le théorème de d'Alembert \ref{THOooIRJYooBiHRyW}.}, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v_1,u_2,\ldots, u_n \}\) de \( \eC^n\), et la matrice (unitaire)
	\begin{equation}
		Q=\begin{pmatrix}
			\uparrow   & \uparrow   &        & \uparrow   \\
			v_1        & u_2        & \cdots & u_n        \\
			\downarrow & \downarrow &        & \downarrow
		\end{pmatrix}.
	\end{equation}
	Nous avons \( Q^{-1}AQe_1=Q^{-1} Av_1=\lambda Q^{-1} v=\lambda_1 e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
	\begin{equation}
		Q^{-1}AQ=\begin{pmatrix}
			\lambda_1 & *   \\
			0         & A_1
		\end{pmatrix}
	\end{equation}
	où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale\footnote{Proposition \ref{PropKBCXooOuEZcS}\ref{ITEMooHSTAooIbVrwa}.}).
\end{proof}

\begin{definition}  \label{DefWQNooKEeJzv}
	Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} si il commute avec son conjugué hermitien\footnote{Définition \ref{PROPooRPRRooRYEMCB}.}.
\end{definition}

Les opérateurs normaux comprennent évidemment les opérateurs hermitiens, mais également les anti-hermitiens, et ça c'est bien parce que c'est le cas de l'algèbre associée à \( \SU(2)\).

\begin{theorem}[Théorème spectral pour les matrices normales\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
	Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
	\begin{enumerate}
		\item   \label{ItemJZhFPSi}
		      \( A\) est normale\footnote{Définition \ref{DefWQNooKEeJzv}.},
		\item   \label{ItemJZhFPSii}
		      \( A\) se diagonalise par une matrice unitaire,
		\item   \label{ITEMooIIQTooQORrXP}
		      \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
		\item   \label{ITEMooJRKSooNfsQJb}
		      il existe une base orthonormale de vecteurs propres de \( A\).
	\end{enumerate}
\end{theorem}

\begin{proof}

	En plusieurs parties.
	\begin{subproof}
		\spitem[\ref{ItemJZhFPSi} \( \Rightarrow\) \ref{ItemJZhFPSii}]
		% -------------------------------------------------------------------------------------------- 
		Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme~\ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
		\begin{equation}
			QTT^*Q^{-1}=QT^*TQ^{-1},
		\end{equation}
		ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
		\begin{equation}
			(TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
		\end{equation}
		Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
		\begin{equation}
			| T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\cdots+| T_{1n} |^2,
		\end{equation}
		ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

		\spitem[\ref{ItemJZhFPSii} \( \Rightarrow\) \ref{ItemJZhFPSi}]
		% -------------------------------------------------------------------------------------------- 
		Si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
		\begin{equation}
			DD^*=UAA^*U^*
		\end{equation}
		et
		\begin{equation}
			D^*D=UA^*AU^*,
		\end{equation}
		qui ce prouve que \( A\) est normale.
		\spitem[\ref{ItemJZhFPSii} \( \Rightarrow\) \ref{ITEMooIIQTooQORrXP}]
		Il existe une matrice unitaire \( U\) et une diagonale \( D\) telles que \( D=U^{\dag}AU\). La proposition \ref{PROPooDEETooSOMiGO} nous dit que la diagonale de \( D\) contient les valeurs propres de \( A\), c'est-à-dire \( D_{kl}=\delta_{kl}\lambda_k\).

		Nous avons $D^{\dag}=U^{\dag}A^{\dag}U$, et donc
		\begin{equation}
			D^{\dag}D=U^{\dag}A^{\dag}UU^{\dag}AU=U^{\dag}A^{\dag}AU,
		\end{equation}
		c'est-à-dire que \( D^{\dag}D\) est semblable à \( A^{\dag}A\). La proposition \ref{PROPooRMYQooWkEpJJ} implique que leurs traces sont donc égales. Petit calcul en utilisant le lemme \ref{LEMooBOXMooSDyCfm} (\( A^{\dag}_{ij}=A_{ji}^*\)) :
		\begin{equation}    \label{EQooAGPLooYXVqGy}
			\tr(A^{\dag}A)=\sum_k(A^{\dag}A)_{kk}
			=\sum_{kl}A^*_{lk}A_{lk}
			=\sum_{kl}| A_{lk} |^2.
		\end{equation}
		Deuxième calcul :
		\begin{equation}        \label{EQooTUWNooIigwmx}
			\tr(D^{\dag}D)=\sum_{kl}D^{\dag}_{kl}D_{lk}=\sum_{kl}\delta_{lk}\lambda_k^*\delta_{lk}\lambda_k=\sum_{k}| \lambda_k |^2.
		\end{equation}
		Nous avons le résultat en égalisant \eqref{EQooTUWNooIigwmx} avec \eqref{EQooAGPLooYXVqGy}.

		\spitem[\ref{ITEMooIIQTooQORrXP} \( \Rightarrow\) \ref{ItemJZhFPSii}]
		% -------------------------------------------------------------------------------------------- 
		Le lemme de Schur complexe \ref{LemSchurComplHAftTq} nous indique qu'il existe une matrice unitaire \( U\) et une matrice triangulaire supérieure \( T\) telles que \( T=UAU^{\dag}\) et telle que \( T_{ii}=\lambda_i\). Le lemme \ref{LEMooQXFQooLGPcIt} nous indique que \( \sum_{ij}| T_{ij} |^2=\sum_{ij}| A_{ij} |^2\). Donc
		\begin{equation}
			\sum_{ij}| A_{ij} |^2=\sum_{ij}| T_{ij} |^2=\sum_j| \lambda_j |^2+\sum_{j>i}| T_{ij} |^2.
		\end{equation}
		Mais l'hypothèse nous dit que \( \sum_{ij}| A_{ij} |^2=\sum_j| \lambda_j |^2\). Nous en déduisons que \( \sum_{j>i}| T_{ij} |^2=0\), et donc que \( T\) est diagonale.

		\spitem[\ref{ItemJZhFPSii}\( \Rightarrow\) \ref{ITEMooJRKSooNfsQJb}]
		% -------------------------------------------------------------------------------------------- 
		Soit une base \( \{ e_i \}\) de \( \eC^n\). Soit \( A=UDU^{\dag}\). Il suffit de poser \( f_i=Ue_i\) pour avoir
		\begin{equation}
			Af_i=UDU^{\dag}Ue_i=UDe_i=\lambda_i Ue_i=\lambda_if_i.
		\end{equation}
		Donc \( f_i\) est un vecteur propre de \( A\). Le fait que \( \{ f_i \}\) soit une base de \( \eC^n\) est dû au fait que \( U\) est inversible.

		\spitem[\ref{ITEMooJRKSooNfsQJb} \( \Rightarrow\) \ref{ItemJZhFPSii}]
		% -------------------------------------------------------------------------------------------- 
		Soit la base canonique \( \{ e_i \}\) de \( \eC^n\). Si \( \{ f_i \}\) est une base de vecteurs propres normalisés de \( A\), alors il existe une matrice unitaire \( U\) telle que \( f_i=Ue_i\). Alors la matrice \( UAU^{\dag}\) est diagonale.
	\end{subproof}
\end{proof}

Tant que nous en sommes à parler de spectre de matrices hermitiennes\ldots Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} et le théorème~\ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque~\ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{lemma}[\cite{ooLMMRooUXhOdx}]   \label{LEMooHUGEooVYhZdZ}
	Si \( A\) est une matrice carrée et inversible,
	\begin{equation}
		\Spec(A^*A)=\Spec(AA^*)
	\end{equation}
\end{lemma}

\begin{proof}
	Nous allons montrer l'égalité des polynômes caractéristiques. D'abord une simple multiplication montre que
	\begin{equation}
		(A^*A-\lambda\mtu)A^{-1}=A^{-1}(AA^*-\lambda\mtu).
	\end{equation}
	Nous prenons le déterminant de cette égalité en utilisant les propriétés~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} et~\ref{ITEMooZMVXooLGjvCy} :
	\begin{equation}
		\det(A^*A-\lambda\mtu)\det(A^{-1})=\det(A^{-1})\det(AA^*-\lambda\mtu).
	\end{equation}
	En simplifiant par \( \det(A^{-1})\) (qui est non nul parce que \( A\) est inversible) nous obtenons l'égalité des polynômes caractéristiques et donc l'égalité des spectres.
\end{proof}


En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{lemma}       \label{LEMooRCFGooPPXiKi}
	Soient \( A\in \eM(n,\eC)\) et une matrice unitaire \( U\) telle que \( A=UTU^{-1}\) où \( T\) est triangulaire.
	\begin{enumerate}
		\item
		      En ce qui concerne les polynômes caractéristiques, \( \chi_A=\chi_T\).
		\item
		      Pour les spectres, \( \Spec(A)=\Spec(T)\).
		\item
		      Les valeurs propres de \( A\) sont les éléments diagonaux de \( T\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Puisque \( U\) commute évidemment avec \( \mtu\), nous avons
	\begin{equation}
		\chi_A(\lambda)=\det(A-\lambda \mtu)=\det(UTU^{-1}-\lambda\mtu)=\det\big( U(T-\lambda\mtu)U^{-1} \big).
	\end{equation}
	À ce niveau nous utilisons le fait que le déterminant soit multiplicatif~\ref{PropYQNMooZjlYlA} pour conclure :
	\begin{equation}
		\chi_A(\lambda)=\det\big( U(T-\lambda\mtu)U^{-1} \big)=\det(U)\det(T-\lambda\mtu)\det(U^{-1})=\det(T-\lambda\mtu)=\chi_T(\lambda).
	\end{equation}

	Pour les spectres, l'égalité des polynômes caractéristiques implique l'égalité des spectres parce que les valeurs propres sont les racines du polynôme caractéristique par le théorème~\ref{ThoWDGooQUGSTL}.

	Les valeurs propres d'une matrice triangulaire sont les valeurs sur la diagonale.
\end{proof}

\begin{lemma}[Trigonalisation simultanée]   \label{LemSLGPooIghEPI}
	Une famille de matrices de \( \GL(n,\eC)\) commutant deux à deux est simultanément trigonalisable.
\end{lemma}
\index{trigonalisation!simultanée}

\begin{proof}
	Commençons par enfoncer une porte ouverte par la proposition~\ref{PropKNVFooQflQsJ} : toutes les matrices de \( \GL(n,\eC)\) sont trigonalisables parce que tous les polynômes sont scindés.

	Nous effectuons la démonstration par récurrence sur la dimension. Si \( n=1\) alors toutes les matrices sont triangulaires et nous ne nous posons pas de questions. Nous supposons donc \( n>1\).

	Soit la famille \( (A_i)_{i\in I}\) dans \( \GL(n,\eC)\) et \( A_0\) un de ses éléments. Nous nommons \( \lambda_1,\ldots, \lambda_r\) les valeurs propres distinctes de \( A_0\). Le théorème de décomposition primaire~\ref{ThoSpectraluRMLok} nous donne la somme directe d'espaces caractéristiques\footnote{Définition~\ref{DefFBNIooCGbIix}.}
	\begin{equation}
		E=F_{\lambda_1}(A_0)\oplus\ldots\oplus F_{\lambda_r}(A_0).
	\end{equation}
	Nous pouvons supposer que cette somme n'est pas réduite à un seul terme. En effet si tel était le cas, \( A_0\) serait un multiple de l'identité parce que \( A_0\) n'aurait qu'une seule valeur propre et les sommes dans la décomposition de Dunford~\ref{ThoRURcpW}\ref{ItemThoRURcpWiii} se réduisent à un seul terme (et \( p_i=\id\)). En particulier les dimensions des espaces \( F_{\lambda}(A_0)\) sont strictement plus petites que \( n\).

	Puisque tous les \( A_i\) commutent avec \( A_0\), les espaces \( F_{\lambda}(A_0)\) sont stables par les \( A_i\) et nous pouvons trigonaliser les \( A_i\) simultanément sur chacun des \( F_{\lambda}(A_0)\) en utilisant l'hypothèse de récurrence.
\end{proof}

\begin{theorem}[Lie-Kolchin\cite{PAXrsMn,BIBooGGEGooVKdLbj}]  \label{ThoUWQBooCvutTO}
	Tout sous-groupe connexe et résoluble\footnote{Définition \ref{DefOSYNooTROIKs}.} de \( \GL(n,\eC)\) est conjugué à un groupe de matrices triangulaires.
\end{theorem}
\index{trigonalisation!simultanée}
\index{théorème!Lie-Kolchin}

\begin{proof}
	Soit \( G\) un sous-groupe connexe et résoluble de \( \GL(n,\eC)\). Nous faisons une récurrence sur \( n\).

	\begin{subproof}
		\spitem[Si sous-espace non trivial stable par \( G\)]
		Nous commençons par voir ce qu'il se passe si il existe un sous-espace vectoriel non trivial \( V\) de \( \eC^n\) stabilisé par \( G\). Pour cela nous considérons une base de \( \eC^n\) dont les premiers éléments forment une base de \( V\) (base incomplète, théorème~\ref{ThonmnWKs}). Les éléments de \( G\) s'écrivent, dans cette base,
		\begin{equation}    \label{EqGOKTooEaGACG}
			\begin{pmatrix}
				g_1 & *   \\
				0   & g_2
			\end{pmatrix}.
		\end{equation}
		Les matrices \( g_1\) et \( g_2\) sont carrées. Nous considérons alors l'application \( \psi\) définie par
		\begin{equation}
			\begin{aligned}
				\psi\colon G & \to \GL(V)   \\
				g            & \mapsto g_1.
			\end{aligned}
		\end{equation}
		Cela est un morphisme de groupes parce que
		\begin{equation}
			\begin{pmatrix}
				g_1 & *   \\
				0   & g_2
			\end{pmatrix}\begin{pmatrix}
				h_1 & *   \\
				0   & h_2
			\end{pmatrix}=
			\begin{pmatrix}
				g_1h_1 & *      \\
				0      & g_2h_2
			\end{pmatrix},
		\end{equation}
		de telle sorte que \( \psi(gh)=\psi(g)\psi(h)\).

		Le groupe \( \psi(G)\) est connexe et résoluble. En effet \( \psi(G)\) est connexe en tant qu'image d'un connexe par une application continue (lemme \ref{LemConncontconn}). Et il est résoluble en tant qu'image d'un groupe résoluble par un morphisme par la proposition~\ref{PropBNEZooJMDFIB}.

		Nous avons prouvé que \( \psi(G)\) est un sous-groupe résoluble et connexe de \( \GL(V)\). Vu que \( V\) est de dimension strictement plus petite que \( \eC^n\), nous utilisons l'hypothèse de récurrence et nous déduisons que \( \psi(G)\) est conjugué à un groupe de matrices triangulaire, c'est-à-dire qu'il existe une base de \( V\) dans laquelle toutes les matrices \( g_1\) (avec \( g\in G\)) sont triangulaires supérieures.

		On fait de même avec l'application \( g\mapsto g_2\), ce qui donne une base du supplémentaire de \( V\) dans laquelle les matrices \( g_2\) sont triangulaires supérieures.

		En couplant ces deux bases, nous obtenons une base de \( \eC^n\) dans laquelle toutes les matrices \eqref{EqGOKTooEaGACG} (c'est-à-dire toutes les matrices de \( G\)) sont triangulaires supérieures.

		\spitem[Sinon]
		Nous supposons à présent que \( \eC^n\) n'a pas de sous-espaces non triviaux stables sous \( G\). Nous posons \( m=\min\{ k\tq D^k(G)=\{ e \} \}\), qui existe parce que \( G\) et résoluble et que sa suite dérivée termine sur \( {e}\) (proposition~\ref{PropRWYZooTarnmm}).

		\spitem[Si \( m=1\)]
		Si \( m=1\) alors \( G\) est abélien et il existe une base de \( G\) dans laquelle toutes les matrices de \( G\) sont triangulaires (lemme~\ref{LemSLGPooIghEPI}). Le premier vecteur d'une telle base serait stable par \( G\), mais comme nous avons supposé qu'il n'y avait pas de sous-espaces non triviaux stabilisés par \( G\), ce vecteur tout seul forme un sous-espace trivial, c'est-à-dire que \( n=1\). Dans ce cas, le théorème est démontré.

		\spitem[Si \( m>1\)]
		Nous devons maintenant traiter le cas où \( m>1\). Nous posons \( H=D^{m-1}(G)\); cela est un sous-groupe normal et abélien de \( G\). Encore une fois le résultat de trigonalisation simultanée~\ref{LemSLGPooIghEPI} donne une base dans laquelle tous les éléments de \( H\) sont triangulaires. En particulier le premier élément de cette base est un vecteur propre commun à toutes les matrices de \( H\).

		Soit \( V\) le sous-espace engendré par tous les vecteurs propres communs de \( H\). Nous venons de voir que \( V\) n'est pas vide. Nous allons montrer que \( V\) est stable par \( G\). Soient \( h\in H\), \( v\in V\) et \( g\in G\) :
		\begin{equation}    \label{EqPMOBooVLIhrJ}
			h\big( g(v) \big)=g\underbrace{g^{-1}hg}_{\in H}(v)=g(\lambda v)=\lambda g(v)
		\end{equation}
		parce que \( v\) est vecteur propre de \( g^{-1} hg\). Ce que le calcul \eqref{EqPMOBooVLIhrJ} montre est que \( g(v)\) est vecteur propre de \( h\) pour la valeur propre \( \lambda\). Donc \( g(v)\in V\) et \( V\) est stabilisé par \( G\). Mais comme il n'existe pas d'espaces non triviaux stabilisés par \( G\), nous en déduisons que \( V=\eC^n\). Donc tous les vecteurs de \( \eC^n\) sont vecteurs propres communs de \( H\). Autrement dit on a une base de diagonalisation simultanée de \( H\).

		\spitem[\( H\) est dans le centre de \( G\)]

		Montrons à présent que \( H\) est dans le centre de \( G\), c'est-à-dire que pour tout \( g\in G\) et \( h\in H\) il faut \( ghg^{-1}=h\). D'abord \( ghg^{-1}\) est une matrice diagonale (parce qu'elle est dans \( H\)) ayant les mêmes valeurs propres que \( h\). En effet si \( \lambda\) est valeur propre de \( ghg^{-1}\) pour le vecteur propre \( v\), alors
		\begin{subequations}
			\begin{align}
				(ghg^{-1})(v)         & =\lambda v                    \\
				h\big( g^{-1} v \big) & =\lambda \big( g^{-1}v \big),
			\end{align}
		\end{subequations}
		c'est-à-dire que \( \lambda\) est également valeur propre de \( h\), pour le vecteur propre \( g^{-1} v\). Mais comme \( h\) a un nombre fini de valeurs propres, il n'y a qu'un nombre fini de matrices diagonales ayant les mêmes valeurs propres que \( h\). L'ensemble \( \AD(G)h\) est donc un ensemble fini. D'autre part, l'application \( g\mapsto g^{-1}hg\) est continue, et \( G\) est connexe, donc l'ensemble \( \AD(G)h\) est connexe. Un ensemble fini et connexe dans \( \GL(n,\eC)\) est nécessairement réduit à un seul point. Cela prouve que \( ghg^{-1}=h\) pour tout \( g\in G\) et \( h\in H\).

		\spitem[Espaces propres stables pour tout \( G\)]

		Soit \( h\in H\) et \( W\) un espace propre de \( h\) (ça existe non vide, parce que \( H\) est triangularisé, voir plus haut). Alors nous allons prouver que \( W\) est stable pour tous les éléments de \( G\). En effet si \( w\in W\) avec \( h(w)=\lambda w\) alors en permutant \( g\) et \( h\),
		\begin{equation}
			hg(w)=g(hw)=\lambda g(w),
		\end{equation}
		donc \( g(w)\) est aussi vecteur propre de \( h\) pour la valeurs propre \( \lambda\), c'est-à-dire que \( g(w)\in W\). Comme nous supposons que \( \eC^n\) n'a pas d'espaces invariants non triviaux, nous devons conclure que \( W=\eC^n\), c'est-à-dire que \( H\) est composé d'homothéties. C'est-à-dire que pour tout \( h\in H\) nous avons \( h=\lambda_h\mtu\).

		\spitem[Contradiction sur la minimalité de \( m\)]

		Les éléments d'un groupe dérivé sont de déterminant \( 1\) parce que \( \det(g_1g_2g_1^{-1}g_2^{-1})=1\). Par conséquent pour tout \( h\), le nombre \( \lambda_h\) est une racine \( n\)\ieme de l'unité. Vu qu'il n'y a qu'une quantité finie de racines \( n\)\ieme\ de l'unité, le groupe \( H\) est fini et connexe et donc une fois de plus, réduit à un élément, c'est-à-dire \( H=\{ e \}\). Cela contredit la minimalité de \( m\) et donc produit une contradiction. Nous devons donc avoir \( m=1\).

		\spitem[Conclusion]
		Nous avons vu que si \( \eC^n\) avait un sous-espace non trivial fixé par \( G\) alors le théorème était démontré. Par ailleurs si \( \eC^n\) n'a pas un tel sous-espace, soit \( m=1\) (et alors le théorème est également prouvé), soit \( m>1\) et alors on a une contradiction.

		Bref, le théorème est prouvé sous peine de contradiction.
	\end{subproof}
\end{proof}

\begin{remark}
	Le lemme mentionne le fait que les valeurs propres de \( A\) sont les éléments diagonaux de \( T\). Mais attention : ceci ne dit rien au niveau des multiplicités géométriques. Un nombre peut être cinq fois sur la diagonale de \( T\) alors que l'espace propre correspondant pour \( A\) n'est que de dimension \( 1\). Exemple : la matrice
	\begin{equation}
		A=\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\end{equation}
	a deux \( 1\) sur la diagonale. Le nombre \( 1\) est bien une valeur propre de \( A\), mais le système
	\begin{equation}
		A\begin{pmatrix}
			x \\
			y
		\end{pmatrix}=\begin{pmatrix}
			x \\
			y
		\end{pmatrix}
	\end{equation}
	donne \( y=0\) et donc un espace propre de dimension seulement \( 1\).
\end{remark}

\begin{remark}  \label{RemXFZTooXkGzQg}
	Si \( \eK\) est algébriquement clos (comme \( \eC\) par exemple), alors tous les polynômes sont scindés et toutes les matrices sont trigonalisables\footnote{La proposition~\ref{PropKNVFooQflQsJ} montre cela, et le lemme de Schur complexe~\ref{LemSchurComplHAftTq} va un peu plus loin, et précise que la trigonalisation peut être obtenue par une matrice unitaire.}. Un exemple un peu simple de cela est la matrice
	\begin{equation}
		u=\begin{pmatrix}
			0 & -1 \\
			1 & 0
		\end{pmatrix}.
	\end{equation}
	Le polynôme caractéristique est \( \chi_u(X)=X^2+1\) et les valeurs propres sont \( \pm i\). Il est vite vu que dans la base
	\begin{equation}
		\{ \begin{pmatrix}
			i \\
			1
		\end{pmatrix}, \begin{pmatrix}
			1 \\
			i
		\end{pmatrix}\}
	\end{equation}
	de \( \eC^2\), la matrice \( u\) se note \( \begin{pmatrix}
		i & 0  \\
		0 & -i
	\end{pmatrix}\).
\end{remark}

\begin{remark}  \label{RemREOSooGEDJWX}
	Cela nous donne une autre façon de prouver qu'une matrice nilpotente de \( \eM(n,\eC)\) ou \( \eM(n,\eR)\) est trigonalisable\cite{KDUFooVxwqlC}. D'abord dans \( \eM(n,\eC)\), toutes les matrices sont trigonalisables\footnote{Parce que le polynôme caractéristique est scindé, voir la proposition~\ref{PropKNVFooQflQsJ}.}, et les valeurs propres arrivent sur la diagonale. Mais comme les valeurs propres d'une matrice nilpotente valent zéro, elle est triangulaire stricte. Par ailleurs, son polynôme caractéristique est alors \( X^n\).

	Ensuite si \( u\in \eM(n,\eR)\) nous pouvons voir \( u\) comme une matrice dans \( \eM(n,\eC)\) et y calculer son polynôme caractéristique qui sera tout de même \( X^n\). Ce polynôme étant scindé, la proposition~\ref{PropKNVFooQflQsJ} nous assure que \( u\) est trigonalisable. Une fois de plus, les valeurs propres étant sur la diagonale, elle est triangulaire supérieure stricte.
\end{remark}

\begin{corollary}   \label{CorUNZooAZULXT}
	Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
	\begin{equation}
		\chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
	\end{equation}
	où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
	Le lemme~\ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons
	\begin{equation}
		\chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
			X-\lambda_1 & *      & *           \\
			0           & \ddots & *           \\
			0           & 0      & X-\lambda_r
		\end{pmatrix},
	\end{equation}
	qui vaut bien le produit annoncé.
\end{proof}

\begin{corollary}       \label{CORooTPDHooXazTuZ}
	Si \( A\in \eM(n,\eC)\) et \( k\in \eN\) alors
	\begin{equation}
		\Spec(A^k)=\{ \lambda^k\tq \lambda\in \Spec(A) \}.
	\end{equation}
\end{corollary}

\begin{proof}
	Par le lemme~\ref{LemSchurComplHAftTq} nous avons une matrice unitaire \( U\) et une triangulaire \( T\) telles que \( A=UTU^{-1}\). En passant à la puissance \( k\) nous avons aussi
	\begin{equation}
		A^k=UT^kU^{-1}.
	\end{equation}
	Donc le spectre de \( A^k\) est celui de \( T^k\) (lemme~\ref{LEMooRCFGooPPXiKi} et le fait qu'une puissance d'une matrice triangulaire est encore triangulaire). Or les éléments diagonaux de \( T^k\) sont les puissances \( k\)\ieme\ des éléments diagonaux de \( T\), qui sont les valeurs propres de \( A\).
\end{proof}
Pour le cas complexe, c'est le lemme \ref{LEMooVCEOooIXnTpp} et le théorème \ref{ThogammwA}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices, spectre et norme}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Le lien entre la norme opérateur d'une matrice et son spectre sera entre autres utilisé pour étudier le conditionnement de problèmes numériques. Voir la définition \ref{DEFooBKQWooJuoCGX} et par exemple son lien avec la résolution numérique de systèmes linéaires dans la proposition \ref{PROPooGIXFooAhJkIs}.

\begin{proposition}[\cite{ooYLHAooCzQvoa}]      \label{PROPooKLFKooSVnDzr}
	Soit une matrice \( A\in \eM(n,\eC)\) de rayon spectral \( \rho(A)\). Soit une norme \( \| . \|\) sur \( \eC^n\) et la norme opérateur correspondante. Alors
	\begin{equation}
		\rho(A)\leq \| A^k \|^{1/k}
	\end{equation}
	pour tout \( k\in \eN\).
\end{proposition}

\begin{proof}
	Soit \( v\in \eC^n\) et \( \lambda\in \eC\) un couple vecteur-valeur propre. Nous avons \( \| Av \|=| \lambda |\| v \|\) et aussi
	\begin{equation}
		| \lambda |^k\| v \|=\| \lambda^kv \|=\| A^kv \|\leq \| A^k \|\| v \|.
	\end{equation}
	La dernière inégalité est due au fait que nous avons choisi sur \( \eM(n,\eC)\) la norme subordonnée à celle choisie sur \( \eC^n\), via le lemme~\ref{LEMooIBLEooLJczmu}. Nous simplifions par \( \| v \|\) et obtenons \( | \lambda |\leq \| A^k \|^{1/k}\). Étant donné que \( \rho(A)\) est la maximum de tous les \( \lambda\) possibles, la majoration passe au maximum :
	\begin{equation}
		\rho(A)\leq \| A^k \|^{1/k}.
	\end{equation}
\end{proof}

\begin{proposition}     \label{PROPooJGNFooEwtNmJ}
	Soient deux espaces vectoriels normés \( E\) et \( V\). Soient des applications continues \( f,g\colon E\to \End(V)\). Alors l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon E & \to \End(V)            \\
			x            & \mapsto f(x)\circ g(x)
		\end{aligned}
	\end{equation}
	est continue.
\end{proposition}

\begin{proof}
	Soit une suite \( x_k\stackrel{E}{\longrightarrow}x\). Nous devons montrer que \( \psi(x_k)\stackrel{\End(V)}{\longrightarrow}\psi(x)\). Pour cela nous utilisons le lemme \ref{LEMooFITMooBBBWGI} qui indique que la norme opérateur est une norme d'algèbre. Nous avons :
	\begin{subequations}
		\begin{align}
			\| \psi(x_k)-\psi(x) \| & =\| f(x_k)\circ g(x_k)-f(x)\circ g(x) \|                                             \\
			                        & \leq \| f(x_k)\circ g(x_k)-f(x_k)\circ g(x) \|+\| f(x_k)\circ g(x)-f(x)\circ g(x) \| \\
			                        & =\| f(x_k)\circ \big( g(x_k)-g(x) \big) \|+\| \big(f(x_k)-f(x)\big)\circ g(x) \|     \\
			                        & \leq \| f(x_k) \|\| g(x_k)-g(x) \|+\| f(x_k)-f(x) \|\| g(x) \|.
		\end{align}
	\end{subequations}
	Pour \( k\to \infty\) nous avons \( \| f(x_k) \| \to \| f(x) \| \), \( \| f(x_k)-f(x) \|\to 0\) (parce que \( f\) est continue) et similaire avec \( g\). Donc le tout tend vers zéro.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Rayon spectral}
%---------------------------------------------------------------------------------------------------------------------------

La chose impressionnante dans la proposition suivante est que \( \rho(A)\) est défini indépendamment du choix de la norme sur \( \eM(n,\eK)\) ou sur \( \eK\). Lorsque nous écrivons \( \| A \|\), nous disons implicitement qu'une norme a été choisie sur \( \eK\), et que nous avons pris la norme subordonnée sur \( \eM(n,\eK)\).
\begin{proposition}[\cite{ooETMNooSrtWet}]        \label{PROPooWZJBooTPLSZp}
	Soit \( A\) une matrice de \( \eM(n,\eR)\) ou \( \eM(n,\eC)\). Alors
	\begin{equation}
		\rho(A)\leq \| A \|.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous devons séparer les cas, suivant que le corps de base soit \( \eR\) ou \( \eC\).

	\begin{subproof}
		\spitem[Pour \( A\in \eM(n,\eC)\)]
		Soit \( \lambda\) une valeur propre de \( A\) telle que \( | \lambda |\) soit la plus grande. Nous avons donc \( \rho(A)=| \lambda |\). Soit un vecteur propre \( u\in \eC^n\) pour la valeur propre \( \lambda\). En prenant la norme sur l'égalité \( Au=\lambda u\), et en utilisant le lemme~\ref{LEMooIBLEooLJczmu},
		\begin{equation}
			| \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
		\end{equation}
		Donc \( | \lambda |\leq \| A \|\) et \( \rho(A)\leq\| A \|\).

		\spitem[Pour \( A\in \eM(n,\eR)\)]

		L'endroit qui coince dans le raisonnement effectué pour \( \eM(n,\eC)\) est que, certes \( A\in \eM(n,\eR)\) possède une plus grande valeur propre en module et qu'un vecteur propre lui est associé. Mais ce vecteur propre est, à priori, dans \( \eC^n\), et non dans \( \eR^n\). Nous pouvons donc écrire \( Au=\lambda u\), mais pas \( \| Au \|=| \lambda |\| u \|\) parce que nous ne savons pas quelle norme prendre sur \( \eC^n\).

		Il n'est pas certain que nous ayons une norme sur \( \eC^n\) qui se réduit sur \( \eR^n\) à celle choisie implicitement dans l'énoncé. Nous allons donc ruser un peu.

		Soit une norme \( N\) sur \( \eC^n\)\footnote{Il y en a plein, par exemple celle du produit scalaire \( \langle x, y\rangle =\sum_kx_k\bar y_k\).}. Nous nommons également \( N\) la norme subordonnée sur \( \eM(n,\eC)\) et la norme restreinte sur \( \eM(n,\eR)\). Vu que \( N\) est une norme sur \( \eM(n,\eR)\) et que ce dernier est de dimension finie, le théorème~\ref{ThoNormesEquiv} nous indique que \( N\) est équivalente à \( \| . \|\). Il existe donc \( C>0\) tel que
		\begin{equation}        \label{EQooBNWMooNgnMxC}
			N(B)\leq C\| B \|
		\end{equation}
		pour tout \( B\in \eM(n,\eR)\). Nous avons maintenant
		\begin{equation}
			\rho(A)^m\leq N(A^m)\leq C\| A^m \|\leq C\| A \|^m.
		\end{equation}
		Justifications
		\begin{itemize}
			\item Par la proposition~\ref{PROPooKLFKooSVnDzr}.
			\item Parce que \( A^m\in \eM(n,\eR)\) et la relation \eqref{EQooBNWMooNgnMxC}.
			\item Par itération du lemme~\ref{LEMooFITMooBBBWGI}.
		\end{itemize}

		Nous avons donc \( \rho(A)\leq C^{1/m}\| A \|\) pour tout \( m\in\eN\). En prenant \( m\to \infty\) et en tenant compte de \( C^{1/m}\to 1\) nous trouvons \( \rho(A)\leq \| A \|\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{ooETMNooSrtWet}]        \label{LEMooGBLJooCPvxNl}
	Soit \( A\in \eM(n,\eK)\) avec \( \eK=\eR\) ou \( \eC\). Soit \( \epsilon>0\). Il existe une norme algébrique sur \( \eM(n,\eK)\) telle que
	\begin{equation}
		N(A)\leq \rho(A)+\epsilon.
	\end{equation}
\end{lemma}

\begin{proof}
	Soit par le lemme~\ref{LemSchurComplHAftTq} une matrice inversible \( U\) telle que \( T=UAU^{-1}\) soit triangulaire supérieure, avec les valeurs propres sur la diagonale. Notons que même si \( A\in \eM(n,\eR)\), les matrices \( U\) et \( T\) sont, à priori, complexes.

	Soit \( s\in \eR\) ainsi que les matrices
	\begin{equation}
		D_s=\diag(1,s^{-1},s^{-2},\ldots, s^{1-n})
	\end{equation}
	et \( T_s=D_sTD_s^{-1}\). Nous fixerons un choix de \( s\) plus tard.

	La norme que nous considérons est :
	\begin{equation}
		N(B)=\| (D_sU)B(D_sU)^{-1} \|_{\infty}
	\end{equation}
	où \( \| . \|_{\infty}\) est la norme sur \( \eM(n,\eK)\) subordonnée à la norme \( \| . \|_{\infty}\) sur \( \eK^n\) dont nous avons déjà parlé dans l'exemple~\ref{EXooXPXAooYyBwMX}. Cela est bien une norme parce que
	\begin{itemize}
		\item Nous avons \( \| B \|_{\infty}=0\) si et seulement si \( B=0\), et comme \( (D_sU)\) est inversible, nous avons \( (D_sU)B(D_sU)^{-1}=0\) si et seulement si \( B=0\).
		\item \( N(\lambda B)=| \lambda |N(B)\).
		\item Pour l'inégalité triangulaire :
		      \begin{subequations}
			      \begin{align}
				      N(B+C) & =\| (D_sU)B(D_sU)^{-1}+(D_sU)C(D_sU)^{-1} \|_{\infty}                     \\
				             & \leq  \| (D_sU)B(D_sU)^{-1}\|_{\infty} +\| (D_sU)C(D_sU)^{-1} \|_{\infty} \\
				             & =N(B)+N(C).
			      \end{align}
		      \end{subequations}
	\end{itemize}

	En ce qui concerne la matrice \( A\) elle-même, nous avons
	\begin{equation}
		N(A)=\| (D_sU)A(D_sU)^{-1} \|_{\infty}=\| T_s \|_{\infty}.
	\end{equation}
	C'est le moment de se demander comment se présente la matrice \( T_s\). En tenant compte du fait que \( (D_s)_{ik}=\delta_{ik}s^{1-i}\) nous avons
	\begin{equation}
		(T_s)_{ij}=\sum_{kl}(D_s)_{ik}T_{kl}(D^{-1}_s)_{lj}=T_{ij}s^{j-i}.
	\end{equation}
	La matrice \( T\) est encore triangulaire supérieure avec les valeurs propres de \( A\) sur la diagonale. Les éléments au-dessus de la diagonale sont tous multipliés par au moins \( s\). Il est donc possible de choisir \( s\) suffisamment petit pour avoir\quext{Il me semble qu'il manque un module dans \cite{ooETMNooSrtWet}.}
	\begin{equation}        \label{EQooSIEIooTWAXQD}
		\sum_{j=i+1}^n| (T_s)_{ij} |<\epsilon
	\end{equation}
	Avec ce choix, la formule~\ref{EQooPLCIooVghasD} donne
	\begin{equation}
		N(T_s)\leq\max_i\sum_k| (T_s)_{ik} |\leq \epsilon+\rho(A).
	\end{equation}
	En effet le \( \epsilon\) vient de la somme sur toute la ligne sauf la diagonale (c'est-à-dire la partie \( k\neq i\)) et du choix \eqref{EQooSIEIooTWAXQD} pour \( s\). Le \( \rho(A)\) provient du dernier terme de la somme (le terme sur la diagonale) qui est une valeur propre de \( A\), donc majorable par \( \rho(A)\).

	Nous devons encore prouver que \( N\) est une norme algébrique. Pour cela nous allons montrer qu'elle est subordonnée à la norme
	\begin{equation}
		\begin{aligned}
			n\colon \eK^n & \to \eR^+                       \\
			v             & \mapsto \| (UD_s)v \|_{\infty}.
		\end{aligned}
	\end{equation}
	Cela sera suffisant pour avoir une norme algébrique par le lemme~\ref{LEMooFITMooBBBWGI}. La norme \( n\) sur \( \eK^n\) produit la norme suivante sur \( \eM(n,\eK)\) :
	\begin{equation}
		n(B)=\sup_{v\neq 0}\frac{ n(B) }{ n(v) }=\sup_{v\neq 0}\frac{ \| (UD_s)Bv \|_{\infty} }{ \| UD_sv \|_{\infty} }.
	\end{equation}
	Puisque \( UD_s\) est inversible, nous pouvons effectuer le changement de variables \( v\mapsto (UD_s)^{-1} v\) pour écrire
	\begin{equation}
		n(B)=\sup_{v\neq 0}  \frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| (UD_s)(UD_s)^{-1}v \|_{\infty} }=\sup_{v\neq 0}\frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| v \|_{\infty} }=\| (UD_s)B(UD_s)^{-1} \|_{\infty}=N(B).
	\end{equation}
\end{proof}

\begin{proposition}     \label{PROPooYPLGooWKLbPA}
	Si \( A\in \eM(n,\eR)\) alors \( \rho(A)^m=\rho(A^m)\) pour tout \( m\in \eN\).
\end{proposition}

\begin{proof}
	La matrice \( A\) peut être vue dans \( \eM(n,\eC)\) et nous pouvons lui appliquer le corolaire~\ref{CORooTPDHooXazTuZ} :
	\begin{equation}        \label{EQooJJIYooDBacjn}
		\Spec(A^k)=\{ \lambda^k\tq \lambda\in\Spec(A) \}.
	\end{equation}
	À noter qu'il n'y a pas de magie : le spectre de la matrice réelle \( A\) est déjà défini en voyant \( A\) comme matrice complexe. Le spectre dont il est question dans \eqref{EQooJJIYooDBacjn} est bien celui dont on parle dans la définition du rayon spectral.

	Nous avons ensuite :
	\begin{subequations}
		\begin{align}
			\rho(A^k) & =\max\{ | \lambda |\tq \lambda\in\Spec(A^k) \} \\
			          & =\max\{ | \lambda^k |\tq \lambda\in\Spec(A) \} \\
			          & =\max\{ | \lambda |^k\tq\lambda\in\Spec(A) \}  \\
			          & =\rho(A)^k.
		\end{align}
	\end{subequations}
\end{proof}

\begin{proposition}     \label{PROPooXEQLooHvzVVm}
	Soient des espaces vectoriels normés \( V\) de dimension \( n\) et \( W\) de dimension \( m\) sur \( \eK\) (corps normé). Nous considérons une base \( \{ e_s \}_{s=1,\ldots, n}\) de \( V\) et \( \{ f_{\alpha} \}_{\alpha=1,\ldots, m}\) de \( W\).

	Alors l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon \eM(n\times m,\eK) & \to \aL(V,W)                            \\
			\psi(A)v                      & =\sum_{s\alpha}A_{s\alpha}v_sf_{\alpha}
		\end{aligned}
	\end{equation}
	est un isomorphisme d'espaces topologiques.

	Pour rappel, la topologie sur \( \eM(n,\eK)\) est donnée par la définition \ref{DEFooCQHDooYpUAhG}.
\end{proposition}

\begin{proof}
	Nous savons déjà que \( \psi\) est une bijection. De plus, elle est linéaire et donc continue par la proposition \ref{PROPooADPDooOtukQP}. En ce qui concerne son inverse, c'est également une application linéaire (lemme \ref{LEMooLGEHooVEEoiU}); elle est alors également continue.
\end{proof}
\begin{proposition}     \label{PROPooDRHMooYzXbkl}
	Soit un espace vectoriel normé \( V\) de dimension finie. Soit une suite d'opérateurs \( T_n\in \End(V)\). Si \( \{ e_i \}\) est une base de \( V\) et si \( T_n(e_i)\stackrel{V}{\longrightarrow}e_i\) pour tout \( i\), alors \( T_n\stackrel{\End(V)}{\longrightarrow}\id\).
\end{proposition}

\begin{proof}
	Nous utilisons l'application \( \psi\colon \eM(n,\eK)\to \End(V)\) définie en \ref{DEFooJVOAooUgGKme}. Elle nous permet d'écrire
	\begin{equation}
		T_n(x)=\sum_{kl}\psi^{-1}(T_i)_{kl}x_le_k,
	\end{equation}
	que nous allons particulariser à \( x=e_j\). Nous avons
	\begin{subequations}
		\begin{align}
			e_j & =\lim_{n\to \infty} T_n(e_j)                                   \\
			    & =\lim_{n\to \infty} \sum_{kl}\psi^{-1}(T_n)_{kl}\delta_{jl}e_k \\
			    & =\sum_{k}\big( \lim_{n\to \infty} \psi^{-1}(T_n)_{kj} \big)e_k
		\end{align}
	\end{subequations}
	En identifiant les coefficients de \( e_j\), on trouve
	\begin{equation}
		\lim_{n\to \infty} \psi^{-1}(T_n)_{kj}=\delta_{kj}.
	\end{equation}
	Pour chaque \( k\) et \( l\), à gauche nous avons une limite dans \( \eK\). Vue la topologie sur \( \eM(n,\eK)\)\footnote{Toutes les normes sur un espace vectoriel de dimension finie sont équivalentes (théorème \ref{ThoNormesEquiv}). Sur \( \eM(n,\eK)\), nous avons convenu dans la définition \ref{DEFooCQHDooYpUAhG} de considérer la norme maximum.}, nous pouvons écrire cela comme une limite dans \( \eM(n,\eK)\) :
	\begin{equation}
		\lim_{n\to \infty} \psi^{-1}(T_n)=\mtu.
	\end{equation}
	Nous savons que \( \psi^{-1}\) est continue (proposition \ref{PROPooXEQLooHvzVVm}) de telle sorte que nous pouvons la commuter avec la limite :
	\begin{equation}
		\mtu= \lim_{n\to \infty} \psi^{-1}(T_n)=\psi^{-1}\big( \lim_{n\to \infty} T_n \big).
	\end{equation}
	Appliquant maintenant \( \psi\) des deux côtés, \( \psi(\mtu)=\id\) et
	\begin{equation}
		\id=\lim_{n\to \infty} T_n.
	\end{equation}
\end{proof}


Le point important de la définition~\ref{DEFooTLQUooJvknvi} est la continuité. En dimension infinie, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

Si \( V\) est un espace vectoriel normé, nous avons déjà défini son dual topologique \( V'\) comme étant l'ensemble des applications linéaires continues \( V\to \eC\) ou \( V\to \eR\) selon le corps de base de \( V\). C'est la définition \ref{DefJPGSHpn}.

\begin{proposition}
	Soient un espace vectoriel normé \( V\) et un élément \( v\in V\) vérifiant \( \| v \|=1\). Il existe une forme \( \varphi\in V'\) telle que \( \| \varphi \|=1\) et \( \varphi(v)=1\).
\end{proposition}

\begin{proof}
	Nous allons utiliser le théorème de la base incomplète \ref{THOooOQLQooHqEeDK}. Pour cela nous considérons \( I=V\) et la partie clairement génératrice \( G=\{ e_i=i \}_{i\in I}\) (si vous avez bien suivi, \( G=V\) en fait; rien de bien profond). Nous considérons ensuite \( I_0=\{ v \}\). Le théorème de la base incomplète nous donne l'existence de \( I_1\) tel que \( I_0\subset I_I\subset I\) et tel que \( B=\{ e_i \}_{i\in I_1}\) est une base.

	Tout cela pour dire que \( B=\{ e_i \}_{i\in I_1}\) est une base contenant \( v\). Nous allons aussi éventuellement redéfinir la norme de \( e_i\) pour avoir \( \| e_i \|=1\). Cette renormalisation n'affecte pas le fait que \( v\in B\).

	Nous passons maintenant à la définition de \( \varphi\colon V\to \eK\). Pour \( x\in V\) nous commençons par écrire
	\begin{equation}
		x=\sum_{j\in J}x_je_j
	\end{equation}
	et nous posons
	\begin{equation}
		\varphi(x)=\begin{cases}
			x_v & \text{si } v\in J \\
			0   & \text{sinon. }
		\end{cases}
	\end{equation}
	Cette définition a un sens par la partie unicité de la proposition \ref{PROPooEIQIooXfWDDV} de décomposition d'un élément dans une base.

	Nous devons calculer la norme de \( \varphi\). Par la proposition \ref{PROPooWHJCooXxCSWp} nous avons
	\begin{equation}        \label{EQooEFLLooOWPSev}
		\| \varphi \|=\sup_{\| x \|=1}| \varphi(x) |.
	\end{equation}
	Avec \( x=v\) nous avons \( \varphi(x)=1\) et donc \( \| \varphi \|\geq 1\).

	Nous devons encore montrer que \( \| \varphi \|\leq 1\). Un élément \( x\in V\) s'écrit toujours sous la forme
	\begin{equation}
		x=\sum_{i\in J}x_je_j
	\end{equation}
	pour un certain \( J\) fini dans \( I_1\) et pour certains \( x_j\in \eK\). Pour un tel \( x\) nous avons \( \varphi(x)=x_v\). Si \( |\varphi(x)|\geq 1\), alors \( | x_v |\geq 1\), mais alors
	\begin{equation}
		\| x \|\leq \sum_{j\in J}| x_j |\| e_j \|=\sum_{j\in J}| x_j |\geq | x_v |>1,
	\end{equation}
	ce qui fait que ce \( x\) ne participe pas au supremum \eqref{EQooEFLLooOWPSev}.

	Notons que \( \varphi\) est continue (et donc bien dans \( V'\)) parce qu'elle est bornée (proposition \ref{PROPooQZYVooYJVlBd}).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
	La norme \( 2\) d'une matrice réelle est liée au rayon spectral de la façon suivante :
	\begin{equation}
		\|A\|_2=\sqrt{\rho(A{^t}A)}.
	\end{equation}
\end{theorem}

\begin{proof}
	La matrice \( A^tA\) est réelle et symétrique; nous pouvons lui appliquer le théorème spectral \ref{ThoeTMXla}. Il existe une base orthonormée \( \{ e_i \}\) de vecteurs propres de \( A^tA\). Nous notons \( \lambda_i\) la valeur propre de \( e_i\). La définition de la norme opérateur donne
	\begin{equation}
		\| A \|^2=\sum_{\| x \|=1}\| Ax \|=\sum_{\| x \|=1}(Ax)\cdot (Ax)=\sum_{\| x \|=1}x\cdot A^tA.
	\end{equation}
	Par ailleurs, soit \( x\in \eR^n\); nous le décomposons dans la base \( \{ e_i \}\) : \( x=\sum_ix_ie_i\). Nous avons
	\begin{equation}
		A^tAx=\sum_ix_i(A^tAe_i)=\sum_ix_i\lambda_ie_i,
	\end{equation}
	et donc
	\begin{subequations}
		\begin{align}
			Ax\cdot Ax & =(\sum_jx_ie_j)\cdot(\sum_ix_i\lambda_ie_i) \\
			           & =\sum_ix_i^2\lambda_i                       \\
			           & \leq (\max_i\lambda_i)\| x \|^2             \\
			           & =\rho(A^tA)\| x \|^2.
		\end{align}
	\end{subequations}
	Donc \( \| A \|^2=\sum_{\| x \|=1}Ax\cdot Ax\leq \rho(A^tA)\).

	Pour avoir l'inégalité dans l'autre sens, nous considérons \( u\), le vecteur parmi les \( e_i\) qui a la plus grande valeur propre. Autrement dit \( Au=(\max_i\lambda_i)u=\rho(A^tA)u\). Nous avons \( \| u \|=1\) et
	\begin{equation}
		\| Au \|^2=A^tAu\cdot u=\lambda=\rho(A^tA).
	\end{equation}
	Donc \( \| A \|^2=\sup_{\| x \|=1}\| Ax \|\geq \| Au \|^2=\rho(A^tA)\).
\end{proof}

\begin{lemma}       \label{LEMooNESTooVvUEOv}
	Soit une matrice \( A\in \eM(n,\eR)\) qui est symétrique, strictement définie positive. Soient \( \lambda_{min}\) et \( \lambda_{max}\) les plus petites et plus grandes valeurs propres. Alors
	\begin{subequations}
		\begin{align}
			\| A \|_2=\lambda_{max} &  & \text{ et } &  & \|A^{-1}  \|_2=\frac{1}{ \lambda_{min} }.
		\end{align}
	\end{subequations}
\end{lemma}

\begin{proof}
	Soient les vecteurs \( v_1,\ldots, v_n\) formant une base orthonormée de vecteurs propres\footnote{Possible par le théorème spectral~\ref{ThoeTMXla}.} de \( A\). Nous notons \( v_{max}\) celui de \( \lambda_{max}\). Nous avons :
	\begin{equation}
		\| A \|_2\geq \| Av_{max} \|=| \lambda_{max} |\| v_{max} \|=| \lambda_{max} |=\lambda_{max}.
	\end{equation}
	Voilà l'inégalité dans un sens. Montrons l'inégalité dans l'autre sens. Soit \( x=\sum_ix_iv_i\) avec \( \| x \|_2=1\). Alors
	\begin{equation}
		\| Ax \|=\left| \sum_ix_i\lambda_iv_i \right|\leq\sqrt{ \sum_ix_i^2\lambda_i^2 }\leq \lambda_{max}\sqrt{ \sum_ix_i^2}=\lambda_{max}.
	\end{equation}

	En ce qui concerne l'affirmation pour la norme de \( A^{-1}\), il suffit de remarquer que ses valeurs propres sont les inverses des valeurs propres de \( A\).
\end{proof}

\begin{lemma}[\cite{MonCerveau, BIBooFRBHooZYBjQD}]        \label{LEMooCSBVooZzqxqg}
	Soit une matrice diagonale \( D\in \eM(n,\eC)\) dont nous notons \( \lambda_i\in \eC\) les éléments diagonaux. Alors la norme opérateur\footnote{Norme opérateur, définition \ref{DefNFYUooBZCPTr}. La notation \( \| D \|_2\) signifie la norme opérateur de \( D\colon \eC^n\to \eC^n\) où l'on a mis la norme euclidienne sur \( \eC^n\), c'est-à-dire la norme de la définition \ref{DEFooGUXNooXwCsrq}.} de \( D\) est donnée par
	\begin{equation}
		\| D \|_2=\max_i\{ | \lambda_i | \}.
	\end{equation}
\end{lemma}

\begin{proof}
	En plusieurs points.
	\begin{subproof}
		\spitem[Le compact]
		Puisque la partie \( \{ x\in \eC^n\tq \| x \|_2=1 \}\) est compacte, nous pouvons utiliser un maximum au lieu d'un supremum dans la définition de la norme opérateur (théorème de Weierstrass \ref{ThoWeirstrassRn}.).
		\spitem[Notations pour \( \eC^n\)]
		Pour se mettre d'accord sur les notations, si \( x\in \eC^n\), alors \( x=\sum_ix_ie_i\) où \( e_1\in \eC^n\) est le vecteur \( (1, 0,\ldots, 0)\). C'est un vecteur de base de \( \eC^n\) comme espace vectoriel sur \( \eC\). Et d'ailleurs \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( \eC^n\).

		\spitem[Norme dans \( \eC^n\)]
		Lorsque \( A\) est un opérateur sur \( \eC^n\), nous avons
		\begin{equation}
			\| Ax \|_2=\left( \sum_i| (Ax)_i |^2 \right)^{1/2}
			=\left( \sum_i \left| \sum_jA_{ij}x_j \right|^2 \right)^{1/2}.
		\end{equation}
		Nous avons utilisé les conventions \eqref{EQooAXRJooUwHbjB}.
		\spitem[Le calcul]
		Si c'est bon pour vous, je me lance dans le calcul :
		\begin{subequations}
			\begin{align}
				\| D \|_2 & =\max_{\| x \|_2=1}\| Dx \|_2                                                                              \\
				          & =\max_{\| x \|_2=1}   \left( \sum_{i=1}^n\lambda_ix_i \right)                                              \\
				          & \leq \max_{\| x \|_2=1}\max_i| \lambda_i |\underbrace{\left( \sum_{i=1}^nx_i^2 \right)^{1/2}}_{=\| x \|_2}
				=\max_i| \lambda_i |.
			\end{align}
		\end{subequations}
	\end{subproof}
	L'inégalité \( \| D \|_2\leq \max_i| \lambda_i |\) est prouvée. Nous démontrons à présent l'inégalité dans l'autre sens. Appliquons \( D\) au vecteur de base \( e_i\) : \( De_i=\lambda_ie_i\). Donc
	\begin{equation}
		\| D \|_2\geq \| De_i \|_2=| \lambda_ie_i |=| \lambda_i |.
	\end{equation}
	Cela étant valable pour tout \( i\), nous avons \( \| D \|_2\geq\max_i| \lambda_i |\).
\end{proof}

\begin{proposition} \label{PropMAQoKAg}
	La fonction
	\begin{equation}
		\begin{aligned}
			f\colon \eM(n,\eR)\times \eM(n,\eR) & \to \eR           \\
			(X,Y)                               & \mapsto \tr(X^tY)
		\end{aligned}
	\end{equation}
	est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
	Il faut vérifier la définition~\ref{DefVJIeTFj}.
	\begin{itemize}
		\item La bilinéarité est la linéarité de la trace.
		\item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
		\item L'application \( f\) est définie positive parce que si \( X\in \eM(n,\eR)\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
	\end{itemize}
\end{proof}

\begin{example}     \label{EXooHDCWooHragNA}
	Soient \( m=n\), un point \( \lambda\) dans \( \eR\) et \( T_{\lambda}\) l'application linéaire définie par \( T_{\lambda}(x)=\lambda x\). La norme de \( T_{\lambda}\) est alors
	\[
		\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
	\]
	Notez que \( T_{\lambda}\) n'est rien d'autre que l'homothétie de rapport \( \lambda\) dans \( \eR^m\).
\end{example}

\begin{example}     \label{EXooVXENooZbtBNi}
	Toutes les isométries de \( \eR^n\) ont norme \( 1\). En effet si \( T\) est une isométrie, \( \| Tx \|=\| x \|\). En ce qui concerne la norme de \( T\) nous avons alors
	\begin{equation}
		\| T \|=\sup_{x\in\eR^n}\frac{ \| T(x) \| }{ \| x \| }=\sup_{x\in\eR^n}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
\end{example}


\begin{lemma}       \label{LEMooNWQWooBrpXgn}
	Soit \( b\in \eR^m\). Nous considérons l'application
	\begin{equation}
		\begin{aligned}
			T_b\colon \eR^m & \to \eR           \\
			x               & \mapsto b\cdot x.
		\end{aligned}
	\end{equation}
	\begin{enumerate}
		\item
		      L'application \( T_b\) est linéaire.
		\item
		      Sa norme est \( \| T_b \|_{\aL}=\| b \|_{\eR^m}\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	La norme de \( T_b\) satisfait les inégalités suivantes
	\[
		\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}|b\cdot x|\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^m}\|x\|_{\eR^m}\leq\|b \|_{\eR^m},
	\]
	\[
		\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left|b\cdot \frac{b}{\|b |}\right|=\frac{ \| b \|^2 }{ \| b \| }=\|b \|_{\eR^n},
	\]
	donc \( \|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}\).
\end{proof}

\begin{proposition}
	Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
	Soit \( x\) un point dans \( \eR^m\). Nous devons vérifier l'égalité
	\begin{equation}
		\lim_{h\to 0_m}T(x+h)=T(x).
	\end{equation}
	Cela revient à prouver que \( \lim_{h\to 0_m}T(h)=0\), parce que \( T(x+h)=T(x)+T(h)\). Nous pouvons toujours majorer \( \|T(h)\|_n\) par \( \|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}\) (lemme~\ref{LEMooIBLEooLJczmu}). Quand \( h\) s'approche de \(  0_m \) sa norme \( \|h\|_m\) tend vers \( 0\), ce que nous permet de conclure parce que nous savons que de toutes façons, \( \| T \|_{\aL}\) est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition~\ref{PROPooQZYVooYJVlBd}.

\begin{proposition}[\cite{MonCerveau}]            \label{PROPooHXJAooGaDtme}
	Soit \( A\in\GL(n,\eC)\). La suite\footnote{Oui, c'est avec \( k\in \eZ\). Vu que \( A\) est dans \( \GL\), elle est inversible, donc pas de soucis à considérer \( A^{-1}\).} \( (A^k)_{k\in \eZ}\) est bornée\footnote{Nous considérons sur \( \GL(n,\eC)\) la norme opérateur dérivant de la norme hermitienne sur \( \eC^n\) donnée par la proposition \ref{PROPooSSYJooHAXAnC}.} si et seulement si \( A\) est diagonalisable\footnote{Définition \ref{DefCNJqsmo}.} et \( \Spec(A)\subset \gS^1\).
\end{proposition}

\begin{proof}
	Nous commençons par supposer que \( A\) est diagonalisable et que \( \Spec(A)\subset \gS^1\). Nous nommons \( \lambda_i\) ses valeurs propres. Par hypothèse \( \lambda_i\in \gS^1\). Nous avons donc \( | \lambda_i |=1\) et, par la proposition \ref{PROPooUMVGooIrhZZg}\ref{ITEMooFXKYooUOXbwH}, nous avons \( | \lambda_i |^k=1\).

	La proposition \ref{PROPooDEETooSOMiGO} permet de considérer une matrice inversible \( Q\) telle que \( A=Q^{-1}DQ\) où \( D\) est la matrice diagonale \( D_{ii}=\lambda_i\). Nous avons donc aussi
	\begin{equation}
		A^k=Q^{-1}D^kQ.
	\end{equation}
	La matrice \( D^k\) est diagonale et \( D^k_{ii}=\lambda_i^k\).

	\begin{subproof}
		\spitem[Pour \( k\geq 0\)]
		La norme matricielle étant une norme d'algèbre\footnote{Lemme \ref{LEMooFITMooBBBWGI}.},
		\begin{equation}
			\| A^k \|_2=\| Q^{-1}D^kQ \|_2\leq \| Q^{-1} \|_2\| D^k \|_2\| Q \|_2.
		\end{equation}
		En ce qui concerne la norme \( \| D \|_2\), nous avons le lemme \ref{LEMooCSBVooZzqxqg} qui nous annonce que \( \| D \|_2=\max_i \{| \lambda_i |\}=1 \). Dans notre cas, nous avons donc \( \| D \|_2=1\) et
		\begin{equation}
			\| A^k \|_2\leq \| Q^{-1} \|_2\| Q \|_2.
		\end{equation}
		Autrement dit, la suite \( (A^k)_{k\in \eN}\) est majorée en norme par le nombre \( \| Q^{-1} \|_2\| Q \|_2\).

		\spitem[Pour \( k\leq 0\)]
		Alors il suffit de poser \( B=A^{-1}\). La matrice \( B\) est autant diagonalisable que \( A\) et le même raisonnement s'applique : il existe une matrice inversible \( P\) telle que
		\begin{equation}
			\| B^k \|_2\leq \| P^{-1} \|_2\| P \|_2.
		\end{equation}

		\spitem[Pour tous les \( k\)]
		La suite \( (A^k)_{k\in \eZ}\) est donc majorée par le maximum entre \( \| P^{-1} \|_2\| P \|\) et \( \| Q^{-1} \|_2\| Q \|\).
	\end{subproof}

	\begin{center}
		Dans l'autre sens, maintenant.
	\end{center}

	Puisque nous travaillons sur \( \eC\), le polynôme caractéristique de \( A\) est scindé et la réduction de Jordan\footnote{Théorème \ref{ThoGGMYooPzMVpe}.} s'applique. Nous considérons une matrice inversible \( Q\) telle que \( A=Q^{-1} MQ\) avec
	\begin{equation}
		M=\begin{pmatrix}
			\lambda_1\mtu+N_1 &        &                   \\
			                  & \ddots &                   \\
			                  &        & \lambda_s\mtu+N_s
		\end{pmatrix},
	\end{equation}
	où les \( N_i\) sont nilpotents. Notez qu'ici les «\( \mtu\)» sont de différentes tailles.

	\begin{subproof}
		\spitem[Juste un bloc]
		Nous considérons un bloc de Jordan \( \lambda\mtu+N\). Nous supposons que la suite \( (\lambda\mtu+N)^k\) est bornée, et nous allons montrer que \( | \lambda |=1\) et \( N=0\).

		Pour que \( k\mapsto (\lambda \mtu+N)^k\) soit bornée, il faut que \( \| \lambda\mtu+N \|\leq 1\). Vu que \( N\) est nilpotente, il existe un vecteur \( u\) tel que \( Nu=0\). Nous avons
		\begin{equation}
			\| \lambda\mtu+N \|=\sup_{\| x \|=1}\| (\lambda\mtu+N)x \|\geq \| (\lambda\mtu+N)u \|=| \lambda |.
		\end{equation}
		Donc \( | \lambda |\leq 1\) est obligatoire pour que \( \| \lambda\mtu+N \|^k\)reste bornée. Nous avons donc deux possibilités : \( | \lambda |<1\) et \( | \lambda |=1\).

		Supposons que \( | \lambda |=1\). Si \( N\) n'est pas nulle\footnote{Méthode alternative pour prouver que \( N=0\) dans \cite{BIBooMJJZooWEKogX}.}, nous pouvons considérer un vecteur \( u\) tel que \( Nu\neq 0\) et \( N^2u=0\). En utilisant la formule du binôme,
		\begin{subequations}
			\begin{align}
				(\lambda+N)^nu & =\sum_{k=0}^n\binom{ n }{ k }\lambda^{n-k}N^ku              \\
				               & =\binom{ n }{ 0 }\lambda^nu+\binom{ n }{ 1 }\lambda^{n-1}Nu \\
				               & =\lambda^nu+n\lambda^{n-1}Nu.
			\end{align}
		\end{subequations}
		Grâce à la proposition \ref{PropNmNNm}, nous avons alors
		\begin{equation}
			\| (\lambda+N)^n \|\geq \| (\lambda+N)^nu \|\geq \big| 1-n\| Nu \| \big|.
		\end{equation}
		Cette dernière expression n'est pas bornée quand \( n\to \infty\). Nous devons donc avoir \( N=0\).

		Nous avons donc deux possibilités :
		\begin{itemize}
			\item \( | \lambda |<1\)
			\item \( | \lambda |=1\) et \( N=0\).
		\end{itemize}

		Nous nous tournons maintenant sur la contrainte que \( (\lambda\mtu+N)^k\) doive rester borné pour \( k<0\). Nous avons
		\begin{equation}
			\lambda\mtu+N=\lambda(\mtu+\lambda^{-1}N),
		\end{equation}
		et nous pouvons appliquer la proposition~\ref{PROPooWTFWooXHlmhp} à l'opérateur nilpotent \( -\lambda^{-1} N\) pour avoir
		\begin{subequations}
			\begin{align}
				\big( \mtu-(-\lambda^{-1}N) \big)^{-1} & =\sum_{l=0}^{\infty}(-\lambda)^{-l}N^l      \\
				                                       & =\mtu+\sum_{l=1}^{\infty}(-\lambda)^{-l}N^l \\
				                                       & =\mtu+\sum_{l=1}^{r-1}(-\lambda)^{-l}N^l.
			\end{align}
		\end{subequations}
		Ceci pour dire que \( (\lambda\mtu+N)^{-1}=\lambda^{-1}(\mtu+\lambda^{-1}N')\) pour une autre matrice nilpotente\footnote{Notez que la somme part de \( l=1\), sinon ce serait raté pour la nilpotence de \( N'\).} \( N'=\sum_{l=1}^{r-1}(-\lambda)^{-l}N^l\). Le travail déjà fait, appliqué à \( \lambda^{-1}\) et \( N'\), nous donne deux possibilités :
		\begin{itemize}
			\item \( | \lambda^{-1} |<1\)
			\item \( | \lambda^{-1} |=1\) et \( N'=0\).
		\end{itemize}
		La possibilité \( | \lambda^{-1} |<1\) est exclue parce qu'elle impliquerait \( | \lambda |>1\) qui avait déjà été exclu. Il ne reste donc que la possibilité \( | \lambda |=1\) et \( N=N'=0\).

		\spitem[Pour la matrice \( M\)]
		% -------------------------------------------------------------------------------------------- 
		Nous supposons que \( \{ M^k \}\) est borné : \( \| M^k \|\leq s\) pour tout \( s\). En utilisant le lemme \ref{LEMooHGCKooBzfAtg} et la proposition \ref{PROPooJUYCooHnlFef}, pour tout \( n\) et pour tout \( i\) nous avons :
		\begin{equation}
			\| (\mtu+\lambda_iN_i)^k \|<s.
		\end{equation}
		Nous appliquons ce que nous venons de montrer pour les blocs et nous obtenons \( | \lambda_i |=1\) et \( N_i=0\).

		\spitem[La matrice \( A\)]
		% -------------------------------------------------------------------------------------------- 
		Nous pouvons enfin parler de la matrice \( A=Q^{-1}MQ\). Nous avons \( A^k=Q^{-1}M^kQ\), et donc aussi
		\begin{equation}
			M^k=QA^kQ^{-1}.
		\end{equation}
		En ce qui concerne la norme, si la suite \( (A^k)\) est bornée par le réel \( s\), alors
		\begin{equation}
			\| M^k \|=\| QA^kQ^{-1} \|\leq \| Q \|\| Q^{-1} \|\| A^k \|\leq s\| Q \|\| Q^{-1} \|.
		\end{equation}
		Donc la suite \( (M^k)\) est bornée et nous pouvons appliquer à \( M^k\) ce que nous avons fait sur \( M\). Nous avons donc
		\begin{equation}
			A=Q^{-1}MQ=Q^{-1}\begin{pmatrix}
				\lambda_1 &        &           \\
				          & \ddots &           \\
				          &        & \lambda_s
			\end{pmatrix}Q
		\end{equation}
		avec \( | \lambda_i |=1\). Nous avons prouvé que \( A\) est diagonalisable et que \( \Spec(A)\subset S^1\).
	\end{subproof}
\end{proof}
