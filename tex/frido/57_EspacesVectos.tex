% This is part of Mes notes de mathématique
% Copyright (c) 2008-2020
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Valeur propre et vecteur propre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Nous savons qu'une application \emph{linéaire} $A\colon \eR^3\to \eR^3$ est complètement définie par la donnée de son action sur les trois vecteurs de base, c'est-à-dire par la donnée de
\begin{equation}
	\begin{aligned}[]
		Ae_1,&&Ae_2&&\text{et}&&Ae_3.
	\end{aligned}
\end{equation}
Nous allons former la matrice de $A$ en mettant simplement les vecteurs $Ae_1$, $Ae_2$ et $Ae_3$ en colonne. Donc la matrice
\begin{equation}		\label{EqExempleALin}
	A=\begin{pmatrix}
		3	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	1	&	0
	\end{pmatrix}
\end{equation}
signifie que l'application linéaire $A$ envoie le vecteur $e_1$ sur $\begin{pmatrix}
	3	\\
	0	\\
	0
\end{pmatrix}$, le vecteur $e_2$ sur $\begin{pmatrix}
	0	\\
	0	\\
	1
\end{pmatrix}$ et le vecteur $e_3$ sur $\begin{pmatrix}
	0	\\
	1	\\
	0
\end{pmatrix}$.
Pour savoir comment $A$ agit sur n'importe quel vecteur, on applique la règle de produit vecteur$\times$matrice :
\begin{equation}
	\begin{pmatrix}
		1	&	2	&	3	\\
		4	&	5	&	6	\\
		7	&	8	&	9
	\end{pmatrix}\begin{pmatrix}
		x	\\
		y	\\
		z
	\end{pmatrix}=
	\begin{pmatrix}
		x+2y+3z	\\
		4x+5y+6z	\\
		7x+8y+9z
	\end{pmatrix}.
\end{equation}

Une chose intéressante est de savoir quelles sont les directions invariantes de la transformation linéaire. Par exemple, on peut lire sur la matrice \eqref{EqExempleALin} que la direction $\begin{pmatrix}
	1	\\
	0	\\
	0
\end{pmatrix}$ est invariante : elle est simplement multipliée par $3$. Dans cette direction, la transformation est juste une dilatation. Afin de savoir si $v$ est un vecteur d'une direction conservée, il faut voir s'il existe un nombre $\lambda$ tel que $Av=\lambda v$, c'est-à-dire voir si $v$ est simplement dilaté.

L'équation $Av=\lambda v$ se récrit $(A-\lambda\mtu)v=0$, c'est-à-dire qu'il faut résoudre l'équation
\begin{equation}
	(A-\lambda\mtu)\begin{pmatrix}
		x	\\
		y	\\
		z
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\
		0	\\
		0
	\end{pmatrix}.
\end{equation}
Nous savons qu'une telle équation ne peut avoir de solutions que si $\det(A-\lambda\mtu)=0$. La première étape est donc de trouver les $\lambda$ qui vérifient cette condition.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dans le vif du sujet}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooMMKZooVcskCc}
    Soit un \( \eK\)-espace vectoriel \( E\) et un endomorphisme \( A\colon V\to V\). Un \defe{vecteur propre}{vecteur!propre} de \( A\) est un vecteur \( v \neq 0\) tel que \( Av=\lambda v\) pour un certain \( \lambda\in \eK\). Dans ce cas, \( \lambda\) est la \defe{valeur propre}{valeur!propre} de \( v\).

    L'\defe{espace propre}{espace!propre} de \( A\) pour la valeur \( \lambda\)\footnote{Nous laissons au lecteur le soin de vérifier que c'est bien un sous-espace vectoriel de \( E\).} est l'ensemble des vecteurs propres de \( A\) pour la valeur propre \( \lambda\) et zéro.
\end{definition}

\begin{definition}
    L'ensemble de valeurs propres de l'endomorphisme \( u\) est son \defe{spectre}{spectre d'un endomorphisme} et est noté \( \Spec(u)\).
\end{definition}

\begin{remark}
    Le nombre zéro peut être une valeur propre; c'est le vecteur zéro qui ne peut pas être vecteur propre. La matrice nulle est une matrice diagonalisable.
\end{remark}

\begin{lemma}       \label{LemjcztYH}
    Soit \( u\) un endomorphisme et \( E_{\lambda}(u)\)\nomenclature[A]{\( E_{\lambda}(u)\)}{Espace propre de \( u\)} ses espaces propres. La somme des \( V_{\lambda}\) est directe.
\end{lemma}

\begin{proof}
    Soit \( v_i\in V_{\lambda_i}\) un choix de vecteurs propres de \( u\). Si la somme n'est pas directe, nous pouvons considérer une combinaison linéaire des \( v_i\) qui soit nulle :
    \begin{equation}
        v_1+\cdots+v_p=0.
    \end{equation}
    Appliquons \( (A-\lambda_1\mtu)\) à cette égalité :
    \begin{equation}
        (\lambda_2-\lambda_1)v_1+\cdots+(\lambda_p-\lambda_1)v_p=0.
    \end{equation}
    En appliquant encore successivement les opérateurs \( (A-\lambda_i\mtu)\) nous réduisons le nombre de termes jusqu'à obtenir \( v_p=0\).
\end{proof}

\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels propres\footnote{Définition~\ref{DefooMMKZooVcskCc}.} de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).

    Autrement dit, l'union finie de sous-espaces propres ne peut être égal à l'espace complet.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynômes d'endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooUEQVooLBrRiE}

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\in\End(E)\) où \( E\) est un \( \eK\)-espace vectoriel. Nous considérons l'application
\begin{equation}    \label{EqOVKooeMJuv}
    \begin{aligned}
        \varphi_u\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(u).
    \end{aligned}
\end{equation}
L'image de \( \varphi_u\) est un sous-espace vectoriel. En effet si \( A=\varphi_u(P)\) et \( B=\varphi_u(Q)\), alors \( A+B=\varphi_u(P+Q)\) et \( \lambda A=(\lambda P)(u)\). En particulier c'est un espace fermé.

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant que endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers\footnote{Définition~\ref{DefDSFooZVbNAX}.}. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}
\index{lemme!des noyaux}
Ce résultat est utilisé pour prouver que toute représentation est décomposable en représentations irréductibles, proposition~\ref{PropHeyoAN} ainsi que pour le théorème~\ref{ThoDigLEQEXR} qui dit que si le polynôme minimal d'un endomorphisme est scindé à racine simple alors il est diagonalisable.

\begin{proof}
    Nous posons
    \begin{equation}
        Q_i=\prod_{j\neq i}P_i.
    \end{equation}
    Par le lemme~\ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bézout (théorème~\ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}
        R_1Q_1+\cdots+R_nQ_n=1.
    \end{equation}
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(P_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. Si \( i\neq j\), alors \( Q_iQ_j\) est multiple de \( P\) et nous avons, en utilisant le lemme~\ref{LemQWvhYb},
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=S_{ij}(u)\circ P(u)=0
    \end{equation}
    où \( S_{ij}\) est un polynôme.

    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème~\ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}

    Afin de terminer la preuve, nous devons montrer que \( R_iQ_i(u)E=\ker P_i(u)\). D'abord nous avons
    \begin{equation}
        P_iR_iQ_i(u)=(R_iP)(u)=R_i(u)\circ P(u)=0,
    \end{equation}
    par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). Pour obtenir l'inclusion inverse, nous reprenons l'équation \eqref{EqqVcpUy} avec \( x\in\ker P_i(u)\). Elle se réduit à
    \begin{equation}
        (R_iQ_i)(u)x=x.
    \end{equation}
    Par conséquent \( x\in\Image\big( R_iQ_i(u) \big)\).
\end{proof}

\begin{corollary}   \label{CorKiSCkC}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme minimal \( \mu_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \mu_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \mu_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r.
    \end{equation}
    Nous pouvons décomposer \( x\in F\) en termes de cette somme :
    \begin{equation}     \label{EqbBbrdi}
        x=x_1+\cdots +x_r
    \end{equation}
    avec \( x_i\in E_i\). Toujours selon le lemme des noyaux, les projections sur les espaces \( E_i\) sont des polynômes en \( f\). Par conséquent \( F\) est stable sous toutes ces projections \( \pr_i\colon E\to E_i\), et en appliquant \( \pr_i\) à \eqref{EqbBbrdi}, \( \pr_i(x)=x_i\). Vu que \( x\in F\), le membre de gauche est encore dans \( F\) et \( x_i\in E_i\cap F\). Nous avons donc
    \begin{equation}
        F\subset\bigoplus_{i=1}^rF_i.
    \end{equation}
    L'inclusion inverse est immédiate parce que \( F_i\subset F\) pour chaque \( i\).
\end{proof}

\begin{lemma}   \label{LemVISooHxMdbr}
    Si \( x\) est un vecteur propre de valeur propre \( \lambda\) pour l'endomorphisme \( u\) et si \( P\) est un polynôme, alors \( x\) est vecteur propre de \( u\) pour la valeur propre \( P(\lambda)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul de \( P(u)x\) en ayant noté \( P(X)=\sum_{k=0}^nc_kX^n\) :
    \begin{equation}
        P(u)x=\sum_{k=0}^nc_ku^k(x)=\sum_{k=0}^nc_k\lambda^ku=P(\lambda)x.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme minimal et minimal ponctuel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DefooOHUXooNkPWaB}
    Soit un endomorphisme \( f\colon E\to E\) d'un \( \eK\)-espace vectoriel de dimension finie. Il existe un unique polynôme annulateur normalisé de degré minimum.

    Il est nommé le \defe{polynôme minimal}{polynôme!minimal} de \( f\) et il est noté \( \mu_f\) ou simplement \( \mu\) lorsque la dépendance en \( f\) est claire.
\end{lemmaDef}

\begin{proof}
    Pour l'unicité, soient \( P\) et \( Q\) deux polynômes annulateur de \( f\) de même degré \( N\) et ayant tous deux \( 1\) comme coefficient de \( x^N\). Alors \( P-Q\) est de degré \( N-1\) tout en étant encore annulateur.

    Pour l'existence, les endomorphismes \( \id\), \( f\), \( f^2\), \ldots ne peuvent pas être tous linéairement indépendants parce que la dimension de \( \End(E)\) est finie. Il existe donc un nombre \( N\) et des coefficients \( a_k\) tels que \( \sum_{k=0}^Na_kf^k=0\). Le polynôme \( P(X)=\sum_{k=0}^Na_kX^k\) est donc annulateur de \( f\).

    Une autre façon de le dire est que l'application linéaire \( \varphi\colon \eK[X]\to \End(E)\) donnée par \( \varphi(P)=P(f)\) est un endomorphisme d'un espace vectoriel de dimension infinie vers un espace vectoriel de dimension finie. Il ne peut donc pas être injectif et possède donc un noyau non réduit à zéro.
\end{proof}

\begin{remark}
    La preuve donnée ci-dessus montre que \( \deg(\mu)\leq \dim(E)^2\). Comme conséquence du théorème de Caley-Hamilton~\ref{ThoCalYWLbJQ} nous verrons qu'en réalité le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{remark}

\begin{example}[Pas en dimension infinie]
    L'endomorphisme de dérivation
\end{example}


Dans la suite, l'endomorphisme \( f\) du \( \eK\)-espace vectoriel \( E\) de dimension \( n\) est fixé. Pour \( x\in E\) nous notons
\begin{equation}            \label{EqooOAYDooEpZELo}
    E_x=\{ P(f)x\tq P\in \eK[X] \}.
\end{equation}
Nous considérons le morphisme d'algèbres
\begin{equation}
    \begin{aligned}
        \varphi\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(f)
    \end{aligned}
\end{equation}
et si \( x\in E\) est donné nous considérons le morphisme de \( \eK\)-espaces vectoriels
\begin{equation}
    \begin{aligned}
        \varphi_x\colon \eK[X]&\to E \\
        P&\mapsto P(f)x.
    \end{aligned}
\end{equation}
Les noyaux de ces applications sont des idéaux, entre autres par le lemme~\ref{LemQWvhYb}. Ils ont donc un unique générateur unitaire (chacun) par le théorème~\ref{ThoCCHkoU}. En termes de vocabulaire, l'ensemble
\begin{equation}
    \ker(\phi)=\{  Q\in\eK[X]\tq Q(f)=0  \}
\end{equation}
est l'\defe{idéal annulateur}{polynôme!annulateur} de \( f\) et un polynôme \( Q\) tel que \( Q(f)=0\) est une polynôme annulateur de \( f\).

\begin{definition}      \label{DEFooUICRooBGYhqQ}
    Le générateur unitaire de \( \ker(\varphi_x)\) est le \defe{polynôme minimal ponctuel}{polynôme!minimal!ponctuel} de \( f\) en \( x\). Il sera noté \( \mu_{f,x}\) ou \( \mu_x\) lorsque la dépendance en \( f\) est claire dans le contexte.
\end{definition}
Nous notons \( \mu\) le générateur unitaire du noyau de \( \varphi\) et \( \mu_x\) celui de \( \varphi_x\). Vu que \( \mu\in\ker(\varphi_x)\) pour tout \( x\) nous avons \( \mu_x\divides \mu\) pour tout \( x\).

\begin{example}[Pas en dimension infinie]       \label{ExooDTUJooIMqSKn}
    En dimension infinie, il n'y a pas toujours de polynôme annulateur. Si \( E\) est un espace vectoriel de dimension infine ayant une base dénombrable \( \{ e_i \}_{i\in \eN}\) alors l'opérateur donné par \( f(e_i)=e_{i+1}\) n'a pas de polynôme annulateur. Même pas ponctuel en quel que point que ce soir.

    De même l'opérateur donné par \( g(e_1)=0\) et \( g(e_i)=e_{i-1}\) si \( i\neq 1\) n'a pas de polynôme annulateur, mais il a un polynôme annulateur ponctuel évident en \( x=e_1\). L'exemple~\ref{ExooLRHCooMYLQTU} donnera un habillage à peine subtil à cet exemple.
\end{example}

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(f)=0\), alors le polynôme minimal \( \mu_f\) divise \( P\). Autrement dit, le polynôme minimal engendre l'idéal des polynômes annulateurs.
\end{proposition}

\begin{proof}
    L'ensemble \( \ker(\varphi)=\{ Q\in \eK[X]\tq Q(u)=0 \} \) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( u\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_u)\) par le théorème~\ref{ThoCCHkoU}. Nous concluons que \( \mu_u\) divise tous les éléments de \( I\).
\end{proof}

La proposition suivante permet de caractériser le polynôme minimal.
\begin{proposition}[\cite{ooEPEFooQiPESf}]      \label{PROPooVUJPooMzxzjE}
    Soit une application linéaire \( f\) sur un \( \eK\)-espace vectoriel. Il existe un unique polynôme unitaire\quext{À mon avis, «unitaire» manque dans \cite{ooEPEFooQiPESf}.} \( P\in \eK[X]\) tel que
    \begin{enumerate}
        \item
            \( P(f)=0\);
        \item
            l'application
            \begin{equation}        \label{EQooIBMDooVTaEhf}
                \begin{aligned}
                    \varphi\colon \frac{ \eK[X] }{ (P) }&\to \End(E) \\
                    \bar Q&\mapsto Q(f)
                \end{aligned}
            \end{equation}
            est injective.
    \end{enumerate}
\end{proposition}

\begin{proof}
    En ce qui concerne l'existence, il existe le polynôme minimal de \( f\) qui satisfait les conditions. Pour l'unicité nous travaillons maintenant.

    Supposons que l'application \eqref{EQooIBMDooVTaEhf} soit injective. Alors pour tout \( Q\in \eK[X]\) tel que \( Q(f)=0\) nous avons \( \bar Q=0\), c'est-à-dire \( Q=PR\) pour un certain \( R\in \eK[X]\). Autrement dit : \( P\) est un générateur unitaire de l'idéal annulateur de \( f\). Le théorème~\ref{ThoCCHkoU}\ref{ITEMooASHKooZqkiCH} nous dit alors que \( P=\mu\) parce que \( \mu\) est également générateur unitaire.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]\label{LemSYsJJj}
    Soit \( f\colon E\to E\) un endomorphisme de l'espace vectoriel \( E\). Il existe un élément \( x\in E\) tel que \( \mu_{f,x}=\mu_f\).
\end{lemma}

\begin{proof}
    Soit une décomposition en irréductibles du polynôme minimal \( \mu=P_1^{\alpha_1}\ldots P_r^{\alpha_r}\). Nous notons \( E_i=\ker\big( P_i^{\alpha_i}(f) \big)\). Les polynômes \( P_i\) sont étrangers deux à deux (un diviseur commun aurait a fortiori été un diviseur et aurait contredit l'irréductibilité). Le lemme des noyaux~\ref{ThoDecompNoyayzzMWod} nous donne la somme directe
    \begin{equation}
        E=\bigoplus_{i=1}^r\ker\big( P_i^{\alpha_i}(f) \big).
    \end{equation}
    Si \( x_i\in E_i\) alors \( \mu_{x_i}\) est une puissance de \( P_i\). En effet \( \mu_{x_i}\divides \mu\) et est donc un produit des puissances des \( P_j\). Or si \( (QP_j)(f)x_i=0\) alors \( (P_jQ)(f)x_i=0\), ce qui donne \( Q(f)x_i\in E_j\cap E_i=\{ 0 \}\). Donc \( \mu_{x_i}\) n'est pas de la forme \( QP_j\) pour \( j\neq i\). Nous en déduisons que \( \mu_{x_i}\) est une puissance de \( P_i\) dès que \( x_i\in E_i\). Nous choisissons \( x_i\in E_i\) tel que \( \mu_{x_i}=P_i^{\alpha_i}\).

    Nous posons enfin \( a=x_1+\cdots +x_r\); par définition du polynôme annulateur \( \mu_a\), nous avons
    \begin{equation}        \label{EqooVIGGooSfuvwB}
        0=\mu_a(f)a=\mu_a(f)x_1+\cdots +\mu_a(f)x_r.
    \end{equation}
    Mais \( m_a(f)x_j\in E_i\), et la somme des \( E_j\) est directe, donc l'annulation de la somme \eqref{EqooVIGGooSfuvwB} implique l'annulation de chacun des termes : \( \mu_a(f)x_i=0\) pour tout \( i\). Cela prouve que \( \mu_{x_i}\divides \mu_a\). Mais comme les \( \mu_{x_i}\) sont premiers deux à deux (parce que ce sont les \( P_i^{\alpha_i}\)), nous avons que le produit divise encore \( \mu_a\) :
    \begin{equation}
        \prod_{i=1}^r\mu_{x_i}\divides \mu_a,
    \end{equation}
    c'est-à-dire \( \mu\divides \mu_a\). Comme nous avons aussi \( \mu_a\divides \mu\), nous déduisons \( \mu_a=\mu\).
\end{proof}

\begin{definition}[Matrices, endomorphismes et vecteurs cycliques]      \label{DEFooFEIFooNSGhQE}
    Une matrice est \defe{cyclique}{cyclique!matrice}\index{matrice!cyclique} si elle est semblable à une matrice compagnon. Un endomorphisme \( f\colon E\to E\) est \defe{cyclique}{cyclique!endomorphisme}\index{endomorphisme!cyclique} s'il existe un vecteur \( x\in E\) tel que \( \{ f^k(x) \}_{k=0,\ldots, n-1} \) est une base de \( E\). Un vecteur ayant cette propriété est un \defe{vecteur cyclique}{vecteur!cyclique} pour \( f\).
\end{definition}

\begin{lemma}   \label{LemAGZNNa}
    Soit \( E\) un espace vectoriel de dimension finie et un endomorphisme cyclique\footnote{Voir la définition~\ref{DEFooFEIFooNSGhQE}.} \( f\) de \( E\). Soit un vecteur cyclique \( v\) de \( f\), alors le polynôme minimal de \( f\) est égal au polynôme minimal de \( f\) au point \( v\) : \( \mu_{f}=\mu_{f,v}\).
\end{lemma}

\begin{proof}
    Montrons que \( \mu_{f,v}\) est un polynôme annulateur de \( f\), ce qui prouvera que \( \mu_f\) divise \( \mu_{f,v}\) par la proposition~\ref{PropAnnncEcCxj}. Étant donné que \( v\) est cyclique, tout élément de \( E\) s'écrit sous la forme \( x=Q(f)v\). Prenons un polynôme \( P\) annulateur de \( f\) en \( v\) : \( P(f)v=0\). Nous montrons que \( P\) est alors un polynôme annulateur de \( f\). En effet, nous avons
    \begin{equation}
        P(f)x=\big( P(f)\circ Q(f) \big)v=\big( Q(f)\circ P(f) \big)v=0
    \end{equation}
    où nous avons utilisé le lemme~\ref{LemQWvhYb}.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]
    Soit \( a\in E\) tel que \( \mu_a=\mu\). Alors \( E_a\) est un sous-espace stable pour \( f\) pour lequel il existe un supplémentaire stable.
\end{lemma}

\begin{proof}
    Soit \( l=\deg(\mu)=\deg(\mu_a)\). L'espace \( E_a\) étant engendré par les \( f^k(a)\) nous savons que \( e_1=a\), \( e_2=f(a)\),\ldots, \( e_l=k^{l-1}(a)\) forment une base de \( E_a\). Nous pouvons la compléter en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Et nous posons\footnote{ici, comme presque partout, \( e^*_{l}\) est le dual de \( e_l\), c'est-à-dire l'application linéaire sur \( E\) donnée par \( e^*_l(e_i)=\delta_{li}\). }
    \begin{subequations}
        \begin{align}
            G&=\{ x\in E\tq e^*_l\big( f^k(x) \big)=0\,\forall k\geq 0 \}\\
            &=\bigcap_{k\geq 0}\ker\{ e^*_l\circ f^k \}\\
            &=\bigcap_{k=0}^{l-1}\ker(  e^*_l\circ f^k ).
        \end{align}
    \end{subequations}
    La dernière égalité est due au fait que \( l\) soit le degré de \( \mu\). Du coup \( f^l\) est une combinaison linéaire des \( f^i\) avec \( i\leq l-1\).

    Nous avons \( f(G)\subset G\) et de plus \( E_a\cap G=\{ 0 \}\) parce qu'un élément de \( E_a\) est une combinaison linéaire d'éléments de la forme \( f^j(a)\) (\( j\leq l\)). Après application de \( f^{l-j}\), ces éléments obtiennent une composante \( f^l(a)=e_l\). De plus \( G\) est un sous-espace vectoriel du fait que \( e^*_l\circ f^i\) est une application linéaire.

    Montrons enfin que \( \dim(G)=n-l\). Pour cela nous remarquons que \( G\) est une intersection d'hyperplans, et nous montrons que les équations définissant ces hyperplans sont linéairement indépendantes. Soit donc
    \begin{equation}        \label{EqooOHESooRtBUfc}
        \sum_{j=0}^{l-1}\lambda_j\big( e^*_l\circ f^j \big)=0
    \end{equation}
    et montrons que \( \lambda_j=0\) pour tout $j$ est l'unique solution. Soit \( x\in E\) et appliquons l'opération \eqref{EqooOHESooRtBUfc} au vecteur \( f^i(x)\); le résultat est zéro :
    \begin{equation}
        0=\sum_{j=0}^{l-1}\lambda_j(e^*_l\circ f^i\circ f^j)=(e^*_l\circ f^i)P(u)
    \end{equation}
    où nous avons posé \( P(X)=\sum_{j=0}^{l-1}\lambda_jX^j\). Appliquons cela à \( a\) : pour tout \( i\) nous avons
    \begin{equation}
        (e^*_l\circ f^i)\big( P(f)a \big)=0.
    \end{equation}
    Mais par définition de \( E_a\), l'élément \(P(f)a \) est dans \( E_a\). Nous en déduisons que
    \begin{equation}
        P(f)a\in G\cap E_a=\{ 0 \},
    \end{equation}
    c'est-à-dire que \( P\) est un polynôme annulateur de \( a\). Mais \( P\) est de degré \( l-1\) alors que le polynôme minimal de \( a\) est de degré \( l\). Par conséquent \( P=0\) et \( \lambda_j=0\) pour tout \( j\).
\end{proof}

\begin{definition}  \label{DEFooBOHVooSOopJN}
    Un endomorphisme d'un espace vectoriel est \defe{semi-simple}{semi-simple!endomorphisme} si tout sous-espace stable par \( u\) possède un supplémentaire stable.
\end{definition}

\begin{lemma}   \label{LemrFINYT}
    Si le polynôme minimal d'un endomorphisme est irréductible, alors il est semi-simple\footnote{Définition~\ref{DEFooBOHVooSOopJN}.}.
\end{lemma}

\begin{proof}
    Soit \( f\), un endomorphisme dont le polynôme minimal est irréductible et \( F\), un sous-espace stable par \( f\). Nous devons en trouver un supplémentaire stable. Si \( F=E\), il n'y a pas de problèmes. Sinon nous considérons \( u_1\in E\setminus F\) et
    \begin{equation}
        E_{u_1}=\{ P(f)u_1\tq P\in \eK[X] \},
    \end{equation}
    qui est un espace stable par \( f\).

    Montrons que \( E_{u_1}\cap F=\{ 0 \}\). Pour cela nous regardons l'idéal
    \begin{equation}
        I_{u_1}=\{ P\in \eK[X]\tq P(f)u_1=0 \}.
    \end{equation}
    Cela est un idéal non réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) par exemple est dans \( I_{u_1}\). Soit \( P_{u_1}\) un générateur unitaire de \( I_{u_1}\). Étant donné que \( \mu_f\in I_{u_1}\), nous avons que \( P_{u_1}\) divise \( \mu_f\) et donc \( P_{u_1}=\mu_f\) parce que \( \mu_f\) est irréductible par hypothèse.

    Soit \( y\in E_{u_1}\cap F\). Par définition il existe \( P\in\eK[X]\) tel que \( y=P(f)u_1\) et si \( y\neq 0\), ce la signifie que \( P\notin I_{u_1}\), c'est-à-dire que \( P_{u_1} \) ne divise pas \( P\). Étant donné que \( P_{u_1}\) est irréductible cela implique que \( P_{u_1}\) et \( P\) sont premiers entre eux (ils n'ont pas d'autre \( \pgcd\) que \( 1\)).

    Nous utilisons maintenant Bézout (théorème~\ref{ThoBezoutOuGmLB}) qui nous donne \( A,B\in \eK[X]\) tels que
    \begin{equation}
        AP+BP_{u_1}=1.
    \end{equation}
    Nous appliquons cette égalité à \( f\) et puis à \( u_1\):
    \begin{equation}
        u_1=A(f)\circ \underbrace{P(f)u_1}_{=y}+B(f)\circ \underbrace{P_{u_1}(u_1)}_{=0}=A(f)y.
    \end{equation}
    Mais \( y\in F\), donc \( A(f)y\in F\). Nous aurions donc \( u_1\in F\), ce qui est impossible par choix. Nous avons maintenant que l'espace \( E_{u_1}\oplus F\) est stable sous \( f\). Si cet espace est \( E\) alors nous arrêtons. Sinon nous reprenons le raisonnement avec \( E_{u_1}\oplus F\) en guise de \( F\) et en prenant \( u_2\in E\setminus(E_{u_1}\oplus F)\). Étant donné que \( E\) est de dimension finie, ce procédé s'arrête à un certain moment et nous aurons
    \begin{equation}
        E=F\oplus E_{u_1}\oplus\ldots\oplus E_{u_k}
    \end{equation}
    où chacun des \( E_{u_i}\) sont stables.
\end{proof}

\begin{theorem} \label{ThoFgsxCE}
    Un endomorphisme est semi-simple si et seulement si son polynôme minimal est produit de polynômes irréductibles distincts deux à deux.
\end{theorem}
\index{anneau!principal}

\begin{proof}

    Supposons que \( f\) soit semi-simple et que son polynôme minimal soit donné par \( \mu_f=M_1^{\alpha_1}\ldots M_r^{\alpha_r}\) où les \( M_i\) sont des polynômes irréductibles deux à deux distincts. Nous devons montrer que \( \alpha_i=1\) pour tout \( i\). Soit \( i\) tel que \( \alpha_i\geq 1\) et \( N\in \eK[X]\) tel que \( \mu_f=M^2N\) où l'on a noté \( M=M_i\). Nous étudions l'espace
    \begin{equation}
        F=\ker M(f)
    \end{equation}
    qui est stable par \( f\), et qui possède donc un supplémentaire \( S\) également stable par \( f\). Nous allons montrer que \( MN\) est un polynôme annulateur de \( f\).

    D'abord nous prenons \( x\in S\). Étant donné que \( F\) est le noyau de \( M(f)\),
    \begin{equation}
        M(f)\big( MN(f)x \big)=\mu_f(f)x=0,
    \end{equation}
    ce qui signifie que \( MN(f)x\in F\). Mais vu que \( S\) est stable par \( f\) nous avons aussi que \( MN(f)x\in S\). Finalement \( MN(f)x\in F\cap S=\{ 0 \}\). Autrement dit, \( MN(f)\) s'annule sur \( S\).

    Prenons maintenant \( y\in F\). Nous avons
    \begin{equation}
        MN(f)=N(f)\big( M(f)y \big)=0
    \end{equation}
    parce que \( y\in F=\ker M(f)\).

    Nous avons prouvé que \( MN(f)\) s'annule partout et donc que \( MN(f)\) est un polynôme annulateur de \( f\), ce qui contredit la minimalité de \( \mu_f=M^2N\).

    Nous passons au sens inverse. Soit \( m_f=M_1\ldots M_r\) une décomposition du polynôme minimal de l'endomorphisme \( f\) en irréductibles distincts deux à deux. Soit \( F\) un sous-espace vectoriel stable par \( f\). Nous notons
    \begin{equation}
        E_i=\ker(M_i(f))
    \end{equation}
    et \( f_i=f|_{E_i}\). Par le lemme~\ref{CorKiSCkC} nous avons
    \begin{equation}
        F=\bigoplus_{i=1}^r(F\cap E_i).
    \end{equation}
    Les espaces \( E_i\) sont stables par \( f\) et étant donné que \( M_i\) est irréductible, il est le polynôme minimal de \( f_i\). En effet, \( M_i\) est annulateur de \( f_i\), ce qui montre que le minimal de \( f_i\) divise \( M_i\). Mais \( M_i\) étant irréductible, \( M_i\) est le polynôme minimal. Étant donné que \( \mu_{f_i}=M_i\), l'endomorphisme \( f_i\) est semi-simple par le lemme~\ref{LemrFINYT}.

    L'espace \( F\cap E_i\) étant stable par l'endomorphisme semi-simple \( f_i\), il possède un supplémentaire stable que nous notons \( S_i\)~:
    \begin{equation}
        E_i=S_i\oplus(F\cap E_i).
    \end{equation}
    Étant donné que sur chaque \( S_i\) nous avons \( f|_{S_i}=f_i\), l'espace \( S=S_1\oplus\ldots\oplus S_r\) est stable par \( f\). Du coup nous avons
    \begin{subequations}
        \begin{align}
            E&=E_1\oplus\ldots\oplus E_r\\
            &=\big( S_1\oplus(F\cap E_1) \big)\oplus\ldots\oplus\big( S_r\oplus(F\cap E_r) \big)\\
            &=\big( \bigoplus_{i=1}^rS_i \big)\oplus\big( \bigoplus_{i=1}^rF\cap E_i \big)\\
            &=S\oplus F,
        \end{align}
    \end{subequations}
    ce qui montre que \( F\) a bien un supplémentaire stable par \( f\) et donc que \( f\) est semi-simple.
\end{proof}

\begin{example}[L'espace engendré par \( \mtu\), \( A\), \( A^2\),\ldots]
    Soit \( A\) une matrice, et
    \begin{equation}
        V=\Span\{A^k\tq k\in \eN \}.
    \end{equation}
    Nous montrons que \( \dim(V)\) est le degré du polynôme minimal de \( A\).

    D'abord l'idéal annulateur de \( A\) est engendré par le polynôme minimal\footnote{Proposition~\ref{PropAnnncEcCxj}.} que nous notons
        $\mu=\sum_{k=0}^pa_kX^k$.
    La partie \( \{ \mtu,\ldots, A^{p-1} \}\) est libre parce qu'une combinaison linéaire nulle de cela serait un polynôme annulateur en \( A\) de degré plus petit que \( p\). Donc \( \dim(V)\geq p\).

    La partie \( \{ \mtu,A,\ldots, A^p \}\) est liée à cause du polynôme minimal. Isoler \( A^p\) dans \( \mu(A)=0\) donne un polynôme \( f\) de degré \( p-1\) tel que \( A^p=f(A)\).

    Nous allons montrer à présent que la famille \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice (alors \( \dim(V)\leq p\)). Soit un entier \( q\geq p\)et de division euclidienne\footnote{Théorème~\ref{ThoDivisEuclide}.} \( np+r=q\) avec \( r<p\). Nous avons \( A^q=A^{np}A^r\). D'une part
    \begin{equation}
        A^{np}=(A^p)^n=f(A)^n
    \end{equation}
    est de degré \( n(p-1)\). Par conséquent
    \begin{equation}
        A^q=f(A)^nA^r
    \end{equation}
    qui est de degré \( n(p-1)+r=q-n\). Autrement dit il existe un polynôme \( g_1\) de degré \( q-n\) tel que \( A^q=g_1(A)\). Si \( q-n>p-1\) alors nous pouvons recommencer et obtenir un polynôme \( g_2\) de degré strictement inférieur à celui de \( g_1\) tel que \( A^q=g_2(A)\). Au bout du compte, il existe un polynôme \( g\) de degré au maximum \( p-1\) tel que \( A^q=g(A)\). Cela prouve que la partie \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice de \( V\).

    La dimension de \( V\) est donc \( p\), le degré du polynôme minimal.
\end{example}

\begin{proposition}     \label{PropooCFZDooROVlaA}
    Soit \( f\) un endomorphisme d'un espace vectoriel de dimension finie. Nous avons l'isomorphisme d'espace vectoriel
    \begin{equation}
        \eK[f]\simeq\frac{ \eK[X] }{ (\mu_f) }
    \end{equation}
    La dimension en est \( \deg(\mu_f)\).
\end{proposition}

\begin{proof}
    Notons avant de commencer que \( (\mu)\) est l'idéal engendré par \( \mu\). Les classes dont il est question dans le quotient \( \eK[X]/(\mu)\) sont
    \begin{equation}
        \bar P=\{ P+S\mu \}_{S\in \eK[X]}.
    \end{equation}
    Nous allons montrer que l'application suivante fournit l'isomorphisme :
    \begin{equation}
        \begin{aligned}
            \psi\colon \frac{ \eK[X] }{ (\mu) }&\to \eK[f] \\
            \bar P&\mapsto P(f).
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[\( \psi\) est bien définie]
            Si \( Q\in \bar P\) alors \( Q=P+S\mu\) pour un certain \( S\in \eK[X]\). Du coup nous avons
            \begin{equation}
                \psi(\bar Q)=P(f)+(S\mu)(f).
            \end{equation}
            Mais \( \mu(f)=0\) donc le deuxième terme est nul. Donc \( \psi(\bar P)\) est bien définit.
        \item[Injectif]
            Si \( \psi(\bar P)=0\) nous avons \( P(f)=0\), ce qui signifie que \( P=S\mu\) pour un polynôme \( S\). Par conséquent \( P\in (\mu)\) et donc \( \bar P=0\).
        \item[Surjectif]
            Soit \( P\in \eK[X]\). L'élément \( P(f) \) de \( \eK[f]\) est dans l'image de \( \psi\) parce que c'est \( \psi(\bar P)\).
    \end{subproof}
    En ce qui concerne la dimension, le corolaire~\ref{CorsLGiEN} en parle déjà : une base est donné par les projections de \( 1,X,\ldots, X^{\deg(\mu_a)-1}\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefOWQooXbybYD}
    Soit un anneau commutatif \( A\). Si \( u\in\eM(n,A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
    \begin{equation}    \label{Eqkxbdfu}
        \chi_u(X)=\det(u-X\mtu_n).
    \end{equation}
    Nous définissons de même le polynôme caractéristique d'un endomorphisme \( u\colon E\to E\).
\end{definition}

\begin{remark}
    Quelques remarques à propos du signe\quext{Attention : je crois qu'il y a des incohérences dans le Frido à propos de ce choix}.
    \begin{itemize}
        \item
            Certains auteurs définissent le polynôme caractéristique par \( \det(X-u)\) au lieu de \( \det(u-X)\).
        \item
            Wikipédia francophone prend la définition \( \det(X-u)\) (donc inverse de la notre). Allez lire la page de discussion.
        \item
            Sur les wikipédias d'autre langues, ça varie.
        \item
            Un avantage de \( \det(u-X)\) est que \( \det(u)=\chi_u(0)\).
        \item
            Un avantage de \( \det(X-u)\) est qu'il est unitaire.
    \end{itemize}
\end{remark}

\begin{lemma}       \label{LemooWCZMooZqyaHd}
    Le polynôme caractéristique \( \chi_u\) est unitaire en dimension paire et a pour degré la dimension de l'espace vectoriel \( E\)..
\end{lemma}

\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}

\begin{theorem} \label{ThoWDGooQUGSTL}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\)~\ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul par la proposition~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\).

    \ref{ItemeXHXhHii} \( \Leftrightarrow\)~\ref{ItemeXHXhHiii}. Cela est une application directe du théorème~\ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}

\begin{example} \label{ExICOJcFp}
    Sur \( \eR^2\), nous considérons la matrice \( A=\begin{pmatrix}
        1    &   0    \\
        1    &   1
    \end{pmatrix}\) qui a pour polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} le polynôme \( \chi_A=(X-1)^2\). Le nombre \( \lambda=1\) est une racine double de ce polynôme, et pourtant il n'y a qu'une seule dimension d'espace propre :
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\
            1    &   1
        \end{pmatrix}\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}
    \end{equation}
    entraine \( x=0\).

    Ici la multiplicité algébrique est différente de la multiplicité géométrique.
\end{example}

La proposition suivante donne une utilisation amusante de la notion de polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.}.
\begin{proposition}[\cite{ooNGUJooPphdsT}]
    Soit un espace vectoriel \( V\) de dimension finie pour lequel il existe un endomorphisme \( f\colon V\to V\) tel que \( (f\circ f)(v)=-v\) pour tout \( v\in V\). Alors la dimension de \( V\) est paire.
\end{proposition}

\begin{proof}
    Cherchons les valeurs propres de \( f\) en résolvant l'équation \( f(v)=\lambda v\). Nous appliquons \( f\) à cette égalité :
    \begin{equation}
        -v=\lambda f(v)=\lambda^2v.
    \end{equation}
    Donc \( \lambda\) ne peut pas être réel. Nous avons montré que \( f\) n'a pas de valeurs propres réelles. Or le polynôme caractéristique de \( f\) est de degré égal à la dimension. Si la dimension est impaire, le polynôme caractéristique est de degré impair, et possède donc une racine réelle. Autrement dit, l'absence de racines réelles au polynôme caractéristique indique une dimension paire.
\end{proof}

Une autre preuve possible est d'utiliser le déterminant : si la dimension de \( V\) est \( n\) nous avons :
\begin{equation}
    \det(f^2)=\det(-\id)=(-1)^n.
\end{equation}
Donc \( (-1)^n\) est positif, ce qui montre que \( n\) est pair.

\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point~\ref{ItemHMviZw} est un une application du point~\ref{ItemKHNExH}. Les deux gros morceaux sont donc les points~\ref{ItemfzKOCo} et~\ref{ItemKHNExH}.

    Étant donné que \( \mu_{f,x}\) est de degré minimal dans \( I_{f,x}\), l'ensemble
    \begin{equation}
        B=\{ f^k(x)\tq 0\leq k\leq p_{f,x}-1 \}
    \end{equation}
    est libre. En effet une combinaison nulle des vecteurs de \( B\) donnerait un polynôme en \( f\) de degré inférieur à \( p_{f,x}\) annulant \( x\). Nous écrivons
    \begin{equation}
        \mu_{f,x}(X)=X^{p_{f,x}}-\sum_{i=0}^{p_{f,x}-1}a_iX^k.
    \end{equation}
    Étant donné que \( \mu_{f,x}(f)x=0\) et que la somme du membre de droite est dans \( \Span(B)\), nous avons \( f^{p_{f,x}}(x)\in\Span(B)\). Nous prouvons par récurrence que \( f^{p_{f,x}+k}(x)\in\Span(B)\). En effet en appliquant \( f^k\) à l'égalité
    \begin{equation}
        0=f^{p_{f,x}}(x)-\sum_{i=0}^{p_{f,x}-1}a_if^i(x)
    \end{equation}
    nous trouvons
    \begin{equation}
        f^{p_{f,x}+k}(x)=\sum_{i=0}^{p_{f,x}-1}a_if^{i+k}(x),
    \end{equation}
    alors que par hypothèse de récurrence le membre de droite est dans \( \Span(B)\). L'ensemble \( B\) est alors générateur de \( E_{f,x}\) et donc une base d'icelui. Nous avons donc bien \( \dim(E_{f,x})=p_{f,x}\).

    Nous montrons maintenant que \( \mu_{f,x}\) est annulateur de \( f\) au point \( x\). Nous savons que
    \begin{equation}
        \mu_{f,x}(f)x=0.
    \end{equation}
    En y appliquant \( f^k\) et en profitant de la commutativité des polynômes sur les endomorphismes (proposition~\ref{LemQWvhYb}), nous avons
    \begin{equation}
        0=f^k\big( \mu_{f,x}(f)x \big)=\mu_{f,x}(f)f^k(x),
    \end{equation}
    de telle sorte que \( \mu_{f,x}(f)\) est nul sur \( B\) et donc est nul sur \( E_{f,x}\). Autrement dit,
    \begin{equation}
        \mu_{f,x}\big( f|_{E_{f,x}} \big)=0.
    \end{equation}
    Montrons que \( \mu_{f,x}\) est même minimal pour \( f|_{E_{f,x}}\). Sot \( Q\), un polynôme non nul de degré \( p_{f,x}-1\) annulant \( f|_{E_{f,x}}\). En particulier \( Q(f)x=0\), alors qu'une telle relation signifierait que \( B\) est un système lié, alors que nous avons montré que c'était un système libre. Nous concluons que \( \mu_{f,x}\) est le polynôme minimal de \( f|_{E_{f,x}}\).
\end{proof}

Cette histoire de densité permet de donner une démonstration alternative du théorème de Cayley-Hamilton.
\begin{theorem}[Cayley-Hamlilton]   \label{ThoCalYWLbJQ}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une démonstration plus simple via la densité des diagonalisables est donnée en théorème~\ref{ThoHZTooWDjTYI}.
\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{j,x}}\) divise \( \chi_f\), c'est-à-dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition~\ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

\begin{corollary}
    Le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{corollary}

\begin{proof}
    Le polynôme minimal divise le polynôme caractéristique parce qu'il engendre l'idéal des polynômes annulateurs par la proposition \ref{PropAnnncEcCxj}. Or le degré du polynôme caractéristique est la dimension de l'espace par le lemme~\ref{LemooWCZMooZqyaHd}.
\end{proof}

\begin{example}[Calcul de l'inverse d'un endomorphisme]
    Le polynôme de Cayley-Hamilton donne un moyen de calculer l'inverse d'un endomorphisme inversible pourvu que l'on sache son polynôme caractéristique. En effet, supposons que
    \begin{equation}
        \chi_f(X)=\sum_{k=0}^na_kX^k.
    \end{equation}
    Nous aurons alors
    \begin{equation}
        0=\chi_f(f)=\sum_{k=0}^na_kf^k.
    \end{equation}
    Nous appliquons \( f^{-1}\) à cette dernière égalité en sachant que \( f^{-1}(0)=0\) :
    \begin{equation}
        0=a_0f^{-1}+\sum_{k=1}^na_kf^{k-1},
    \end{equation}
    et donc
    \begin{equation}
        u^{-1}=-\frac{1}{ \det(f) }\sum_{k=1}^na_kf^{k-1}
    \end{equation}
    où nous avons utilisé le fait que \( a_0=\chi_f(0)=\det(f)\).
\end{example}

\begin{proposition}\label{PropooBYZCooBmYLSc}
    Si \( (X-z)^l\) (\( l\geq 1\)) est la plus grande puissance de \( (X-z)\) dans le polynôme caractéristique d'un endomorphisme \( u\) alors
    \begin{equation}
        1\leq \dim(E_e)\leq l.
    \end{equation}
    C'est-à-dire que nous avons au moins un vecteur propre pour chaque racine du polynôme caractéristique.
\end{proposition}

\begin{proof}
    Si $(X-z)$ divise \( \chi_u\) alors en posant \( \chi_u=(X-z)P(X)\) nous avons
    \begin{equation}
        \det(u-X\mtu)=(X-z)P(X),
    \end{equation}
    ce qui, évalué en \( X=z\), donne \( \det(u-z\mtu)=0\). L'annulation du déterminant étant équivalente à l'existence d'un noyau non trivial, nous avons \( v\neq 0\) dans \( E\) tel que \( (u-z\mtu)v=0\). Cela donne \( u(v)=zv\) et donc que \( v\) est vecteur propre de \( u\) pour la valeur propre \( z\). Donc aussi \( \dim(E_z)\geq 1\).

    Si \( \dim(E_z)=k\) alors le théorème de la base incomplète~\ref{ThonmnWKs} nous permet d'écrire une base de \( E\) dont les \( k\) premiers vecteurs forment une base de \( E_z\). Dans cette base, la matrice de \( u\) est de la forme
    \begin{equation}
        \begin{pmatrix}
             z   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   z    &   *    \\
                &       &       &   *
         \end{pmatrix}
    \end{equation}
    où les étoiles représentent des blocs à priori non nuls. En tout cas il est vu sous cette forme que \( (X-z\mtu)^k\) divise \( \chi_u\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation et trigonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[matrices semblables] \label{DefCQNFooSDhDpB}
    Sur l'ensemble \( \eM_n(\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\) nous introduisons la relation d'équivalence \( A\sim B\) si et seulement s'il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). Deux matrices équivalentes en ce sens sont dites \defe{semblables}{semblables!matrices}.
\end{definition}

Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} est un invariant sous les similitudes. En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{PAP^{-1}}&=\det(PAP^{-1}-\lambda X)\\
        &=\det\big( P^{-1}(PAP^{-1}-\lambda X)P^{-1} \big)\\
        &=\det(A-\lambda X).
    \end{align}
\end{subequations}

La permutation de lignes ou de colonnes ne sont pas de similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\
            3    &   4
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\
            4    &   3
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=x^2-5x-2\) tandis que \( \chi_B=x^2-5x+2\) alors que le polynôme caractéristique est un invariant de similitude.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes nilpotents}
%---------------------------------------------------------------------------------------------------------------------------

La \defe{trace}{trace!matrice} d'une matrice \( A\in \eM(n,\eK)\) est la somme de ses éléments diagonaux :
\begin{equation}
    \tr(A)=\sum_{i=1}^nA_{ii}.
\end{equation}
Une propriété importante est son invariance cyclique.

\begin{lemma}   \label{LemhbZTay}
    Quelques propriétés de la trace.
    \begin{enumerate}
        \item
    Si \( A\) et \( B\) sont des matrices carrées, alors \( \tr(AB)=\tr(BA)\).
\item
    La trace est un invariant de similitude.
    \end{enumerate}
\end{lemma}

\begin{proof}
    C'est un simple calcul :
    \begin{equation}
            \tr(AB)=\sum_{ik}A_{ik}B_{ki}
            =\sum_{ik}A_{ki}B_{ik}
            =\sum_{ik}B_{ik}A_{ki}
            =\sum_i(BA)_{ii}
            =\tr(BA)
    \end{equation}
    où nous avons simplement renommé les indices \( i\leftrightarrow k\).

    En particulier, la trace est un invariant de similitude parce que \( \tr(ABA^{-1})=\tr(A^{-1} AB)=\tr(B)\) par l'invariance cyclique démontrée en \ref{LEMooUXDRooWZbMVN}\ref{ITEMooXDYQooAlnArd}.
\end{proof}
La trace étant un invariant de similitude, nous pouvons donc définir la \defe{trace}{trace!endomorphisme} comme étant la trace de sa matrice dans une base quelconque. Si la matrice est diagonalisable, alors la trace est la somme des valeurs propres.

\begin{lemma}[\cite{fJhCTE}]   \label{LemzgNOjY}
    L'endomorphisme \( u\in\End(\eC^n)\) est nilpotent si et seulement si \( \tr(u^p)=0\) pour tout \( p\).
\end{lemma}

\begin{proof}
    Supposons que \( u\) est nilpotent. Alors ses valeurs propres sont toutes nulles et celles de \( u^p\) le sont également. La trace étant la somme des valeurs propres, nous avons alors tout de suite \( \tr(u^p)=0\).

    Supposons maintenant que \( \tr(u^p)=0\) pour tout \( p\). Le polynôme caractéristique \eqref{Eqkxbdfu} est
    \begin{equation}    \label{EqfnCqWq}
        \chi_u=(-1)^nX^{\alpha}(X-\lambda_1)^{\alpha_1}\ldots (X-\alpha_r)^{\alpha_r}.
    \end{equation}
    où les \( \lambda_i\) (\( i=1,\ldots, r\)) sont les valeurs propres non nulles distinctes de \( u\).

    Il est vite vu que le coefficient de \( X^{n-1}\) dans \( \chi_u\) est \( -\tr(u)\) parce que le coefficient de \( X^{n-1}\) se calcule en prenant tous les $X$ sauf une fois \( -\lambda_i\). D'autre part le polynôme caractéristique de \( u^p \) est le même que celui de \( u\), en remplaçant \( \lambda_i\) par \( \lambda_i^p\); cela est dû au fait que si \( v\) est vecteur propre de valeur propre \( \lambda\), alors \( u^pv=\lambda^pv\).

    Par l'équation \eqref{EqfnCqWq}, nous voyons que le coefficient du terme \( X^{n-1}\) dans les polynôme caractéristique est
    \begin{equation}        \label{eqSoDSKH}
        0=\tr(u^p)=\alpha_1\lambda_1^p+\cdots +\alpha_r\lambda_r^p.
    \end{equation}
    Donc les nombres \( (\alpha_1,\ldots, \alpha_r)\) est une solution non triviale\footnote{Si \( \alpha_1=\ldots=\alpha_r=0\), alors les valeurs propres sont toutes nulles et la matrice est en réalité nulle dès le départ.} du système
    \begin{subequations}    \label{EqDpvTnu}
        \begin{numcases}{}
            \alpha_1X_1+\cdots +\lambda_rX_r=0\\
            \qquad\vdots\\
            \lambda^r_1X_1+\cdots +\lambda_r^rX_r=0.
        \end{numcases}
    \end{subequations}
    Cela sont les équations \eqref{eqSoDSKH} écrites avec \( p=1,\ldots, r\). Le déterminant de ce système est
    \begin{equation}
        \lambda_1\ldots\lambda_r\det\begin{pmatrix}
             1   &   \ldots    &   1    \\
             \lambda_1   &   \ldots    &   \lambda_1    \\
             \vdots   &       &   \vdots    \\
             \lambda_1^{r-1}   &   \ldots    &   \lambda_r^{r-1}
         \end{pmatrix}\neq 0,
    \end{equation}
    qui est un déterminant de Vandermonde (proposition~\ref{PropnuUvtj}) valant
    \begin{equation}
        0=\lambda_1\ldots\lambda_r\prod_{1\leq i\leq j\leq r}(\lambda_i-\lambda_j).
    \end{equation}
    Étant donné que les \( \lambda_i\) sont distincts et non nuls, nous avons une contradiction et nous devons conclure que \( (\alpha_1,\ldots, \alpha_r)\) était une solution triviale du système \eqref{EqDpvTnu}.
\end{proof}

\begin{proposition}[\cite{SVSFooIOYShq}]    \label{PropMWWJooVIXdJp}
    Soit un \( \eK\)-espace vectoriel \( E\). Un endomorphisme \( u\in\End(E)\) est nilpotent si et seulement s'il existe une base de \( E\) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.
\end{proposition}

\begin{proof}
    \begin{subproof}
       \item[\( \Rightarrow\)]
           Nous faisons la démonstration par récurrence sur la dimension de \( E\). Lorsque \( n=1\) nous avons \( u=(a)\) avec \( a\in \eK\). Vu que \( a^k=0\) pour un certain \( k\) nous avons \( a=0\) parce qu'un corps est toujours un anneau intègre\footnote{Lemme~\ref{LemAnnCorpsnonInterdivzer}.}.

           Lorsque \( \dim(E)=n\) nous savons que \( u\) a un noyau non réduit au vecteur nul (parce qu'il est nilpotent). Soit donc un vecteur non nul \( x\in\ker(u)\) et une base
           \begin{equation}
               \{ x,e_2,\ldots, e_n \}
           \end{equation}
           donnée par le théorème de la base incomplète~\ref{ThonmnWKs}. La matrice de \( u\) dans cette base s'écrit
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix}
                               * &   *    &   *
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\
                               0 \\
                               0
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A    &       \\
                                &       &
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Un tout petit peu de calcul de produit de matrice montre que la matrice de \( u^k\) est de la forme
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix}
                               * &   *    &   *
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\
                               0 \\
                               0
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A^k    &       \\
                                &       &
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Étant donné que \( u\) est nilpotente, la matrice \( A\) l'est aussi. L'hypothèse de récurrence dit alors que \( A\) est strictement triangulaire supérieure (ou en tout cas peut le devenir par un changement de base adéquat).

       \item[\( \Leftarrow\)]

            Lorsqu'une matrice est triangulaire supérieure stricte, elle applique
            \begin{equation}
                \Span\{ e_1,\ldots, e_k \}\to\Span\{ e_1,\ldots, e_{k-1} \}.
            \end{equation}
            Donc tout vecteur finit sur zéro si on lui applique \( u\) assez souvent.
    \end{subproof}
\end{proof}

\begin{proposition}[Thème~\ref{THEMEooPQKDooTAVKFH}]     \label{PROPooWTFWooXHlmhp}
    Soit \( E\) un espace de Banach (espace vectoriel normé complet). Si \( A\in\aL(E,E)\) est nilpotente, alors \( (\mtu-A)\) est inversible et son inverse est donné par
    \begin{equation}
        (\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k,
    \end{equation}
    où l'infini peut évidemment être remplacé par l'ordre de nilpotence de \( A\).
\end{proposition}

\begin{proof}
    En ce qui concerne la convergence de la somme, elle ne fait pas de doutes parce que \( A\) étant nilpotente, la somme contient seulement une quantité finie de termes non nuls.

    Montrons à présent que la somme est l'inverse de \( \mtu-A\) en multipliant terme à terme :
    \begin{equation}
        \sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
    \end{equation}
    Par conséquent
    \begin{equation}
        \| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\to 0.
    \end{equation}
    La dernière limite est en réalité une égalité pour \( n\) assez grand.
\end{proof}

\begin{proposition}
    Soit \( A\in\GL(n,\eC)\). La suite \( (A^k)_{k\in \eZ}\) est bornée si et seulement si \( A\) est diagonalisable et \( \Spec(A)\subset \gS^1\).
\end{proposition}

\begin{proof}
    Si \( A\) est diagonalisable avec les valeurs propres \( \lambda_i\) de norme \( 1\) dans \( \eC\), alors \( A^k\) est la matrice diagonale avec les \( \lambda_i^k\) sur la diagonale. Cela reste borné pour toute valeur entière de \( k\).

    En ce qui concerne l'autre sens, nous supposons encore que
    \begin{equation}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix},
    \end{equation}
    et nous regardons un des blocs. Nous voulons prouver que \( N=0\) et que \( | \lambda |=1\).

    Nous commençons par regarder ce qu'implique le fait que \( (\lambda \mtu+N)^n\) reste borné pour \( n>0\). En notant \( r\) l'ordre de nilpotence de \( N\), nous avons le développement
    \begin{equation}
        (\lambda\mtu+N)^n=\sum_{k=0}^{r-1}\binom{ n }{ k }N^k\lambda^{n-k}.
    \end{equation}
    Par la proposition~\ref{PropMWWJooVIXdJp}, une matrice nilpotente s'écrit dans une base sous la forme
    \begin{equation}
        N=\begin{pmatrix}
             0   &   1    &       &       \\
                &   0    &   1    &       \\
                & &   \ddots   &   \ddots    &      \\
                &&       &   0    &   1     \\
                &&       &      &   0
         \end{pmatrix}
    \end{equation}
    et effectuer \( A^k\) revient à décaler la diagonale de \( 1\). Donc la famille
    \begin{equation}
        \{ \mtu,N,\ldots, N^{r-1} \}
    \end{equation}
    est libre. Par conséquent la suite \( (\lambda\mtu+N)^n\) restera bornée si et seulement si chacun des termes
    \begin{equation}    \label{EqXRDVDCM}
        \binom{ n }{ k }N^k\lambda^{n-k}
    \end{equation}
    reste borné. Le premier terme étant \( \lambda^n\mtu\), nous avons obligatoirement \( | \lambda |\leq 1\). Si \( | \lambda |<1\), alors le coefficient \( \binom{ n }{ k }\lambda^{n-k}\) tend vers zéro. Si \( | \lambda |=1\) par contre ce coefficient tend vers l'infini et la seule façon pour que \eqref{EqXRDVDCM} reste borné est que \( N=0\). Nous avons donc deux possibilités :
    \begin{itemize}
        \item \( | \lambda |<1\)
        \item \( | \lambda |=1\) et \( N=0\).
    \end{itemize}

    Nous nous tournons maintenant sur la contrainte que \( (\lambda\mtu+N)^n\) doive rester borné pour \( n<0\). Nous avons
    \begin{equation}
        \lambda\mtu+N=\lambda(\mtu+\lambda^{-1}N),
    \end{equation}
    et nous pouvons appliquer la proposition~\ref{PROPooWTFWooXHlmhp} à l'opérateur nilpotent \( -\lambda^{-1} N\) pour avoir
    \begin{equation}
        (\mtu+\lambda^{-1}N)^{-1}=\mtu+\sum_{k=1}^{\infty}(-\lambda)^{-1}N^k.
    \end{equation}
    Ceci pour dire que \( (\lambda\mtu+N)^{-1}=\lambda^{-1}(\mtu+\lambda^{-1}N')\) pour une autre matrice nilpotente \( N'\). Le travail déjà fait, appliqué à \( \lambda^{-1}\) et \( N'\), nous donne deux possibilités :
    \begin{itemize}
        \item \( | \lambda^{-1} |<1\)
        \item \( | \lambda^{-1} |=1\) et \( N'=0\).
    \end{itemize}
    La possibilité \( | \lambda^{-1} |<1\) est exclue parce qu'elle impliquerait \( | \lambda |>1\) qui avait déjà été exclu. Il ne reste donc que la possibilité \( | \lambda |=1\) et \( N=N'=0\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefCNJqsmo}
    Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice diagonale.
\end{definition}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est-à-dire si elle est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item\label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\) dont toutes les racines sont simples tel que \( P(u)=0\).
        \item\label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
        \end{equation}
        mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
        \item\label{ItemThoDigLEQEXRiii}
            Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
        \item       \label{ITEMooZNJFooEiqDYp}
            Dans une base adaptée, la matrice de \( u\) est diagonale et les éléments diagonaux sont ses valeurs propres.
    \end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
    Plein d'implications à prouver.
    \begin{subproof}
    \item[\ref{ItemThoDigLEQEXRi} implique~\ref{ItemThoDigLEQEXRii}] Étant donné que \( P(u)=0\), il est dans l'idéal des polynômes annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynômes annulateurs est généré par \( \mu_u\) par le théorème~\ref{ThoCCHkoU}.

    \item[\ref{ItemThoDigLEQEXRii} implique~\ref{ItemThoDigLEQEXRiv}] Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est-à-dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème~\ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

\item[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRiii}] Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) un \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooCJQGooXwjsfm}, nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est invariant par \( u\).

\item[\ref{ItemThoDigLEQEXRiii} implique~\ref{ItemThoDigLEQEXRiv}] En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

\item[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRi}] Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\),
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme~\ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur une base.

\item[\ref{ITEMooZNJFooEiqDYp} implique~\ref{ItemThoDigLEQEXRi}]
    Si la matrice \( A\) est diagonale alors le polynôme \( P=\prod_{i=1}^n(A-A_{ii}\mtu)\) est annulateur de \( A\).
        \item[\ref{ItemThoDigLEQEXRii} implique~\ref{ITEMooZNJFooEiqDYp}]
            le polynôme minimal de \( u\) s'écrit
            \begin{equation}
                \mu=(X-\lambda_1)\ldots(X-\lambda_r),
            \end{equation}
            et les espaces $E_i$ du lemme~\ref{LemgnaEOk} sont les espaces propres \( E_i=\ker(u-\lambda_i)\). Nous avons donc une somme directe
            \begin{equation}
                E=E_1\oplus\ldots\oplus E_r.
            \end{equation}
            Dans chacun des espaces propres, $u$ a une matrice diagonale avec la valeur propre correspondante sur la diagonale. Une base de \( E\) constituée d'une base de chacun des espaces propres est donc une base comme nous en cherchons.
    \end{subproof}
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
    Si \( u\) est diagonalisable et si \( F\) est une sous-espace stable par \( u\), alors
    \begin{equation}
        F=\bigoplus_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème~\ref{ThoDigLEQEXR}, le polynôme \( \mu_u\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme~\ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soit \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme~\ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n\) est \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme~\ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u)\big)\subset E_{\lambda}(u)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_ix=\lambda x\). Nous montrons que \( u_jx\in E_{\lambda}(u)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point~\ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
    Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).

    Tant que \( 1\neq -1\), \( X^1-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

    Par exemple si le corps est de caractéristique \( 2\), nous avons
    \begin{subequations}
        \begin{align}
            A&=\begin{pmatrix}
                1    &   1    \\
                0    &   1
            \end{pmatrix}\\
            A^1&=\begin{pmatrix}
                1    &   2    \\
                0    &   1
            \end{pmatrix}=\begin{pmatrix}
                1    &   0    \\
                0    &   1
            \end{pmatrix}.
        \end{align}
    \end{subequations}
    Ce \( A\) est donc une involution mais n'est pas diagonalisable.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, pas toujours}
%---------------------------------------------------------------------------------------------------------------------------

Il n'est pas vrai qu'une matrice de \( \eM(n,\eC)\) soit toujours diagonalisable. En effet le théorème~\ref{ThoDigLEQEXR}\ref{ItemThoDigLEQEXRii} dit qu'une matrice est diagonalisable si et seulement si son polynôme minimal est scindé à racines simples. Certes sur \( \eC\) le polynôme minimal sera scindé, mais il ne sera pas spécialement à racines simples.

\begin{example}
    La matrice
    \begin{equation}
        A=\begin{pmatrix}
            0    &   1    \\
            0    &   0
        \end{pmatrix}
    \end{equation}
    a pour polynôme caractéristique \( \chi_A(X)=X^2\). Cela est également son polynôme minimal, et ce n'est pas à racine simple.

    Il est par ailleurs facile de voir que le seul espace propre de \( A\) est \( \Span\{ (1,0) \}\) (ici le span est sur \( \eC\)). Donc l'espace \( \eC^2\) ne possède pas de base de vecteurs propres de \( A\).
\end{example}

Ce qui est vrai, c'est que le polynôme caractéristique a des racines, et que ces racines correspondent à des vecteurs propres. Mais il n'y a pas toujours autant de vecteurs propres que la multiplicité des racines.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : généralités}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{MQMKooPBfnZN}]
    Une matrice dans \( \eM(n,\eK)\) est \defe{trigonalisable}{matrice!trigonalisable} lorsqu'elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice triangulaire supérieure.
\end{definition}

\begin{proposition}[Trigonalisation et polynôme caractéristique scindé] \label{PropKNVFooQflQsJ}
    Soit \( u\) un endomorphisme d'un espace vectoriel \( E\) sur le corps \( \eK\). Les faits suivants sont équivalents.
    \begin{enumerate}
        \item   \label{ItemZKDMooOrTHkwi}
            L'endomorphisme \( u\) est trigonalisable (auquel cas les valeurs propres sont sur la diagonale).
        \item   \label{ItemZKDMooOrTHkwii}
            Le polynôme caractéristique de \( u\) est scindé\footnote{Définition~\ref{DefCPLSooQaHJKQ}.}.
    \end{enumerate}
\end{proposition}
\index{trigonalisation!et polynôme caractéristique}

\begin{proof}
    \begin{subproof}
        \item[\ref{ItemZKDMooOrTHkwii}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwi}]
            Nous avons par hypothèse que
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i}
            \end{equation}
            où les \( \lambda_i\) sont les valeurs propres de \( u\). Le théorème de Cayley-Hamilton~\ref{ThoCalYWLbJQ} dit que \( \chi_u(u)=0\), ce qui permet d'utiliser le théorème de décomposition des noyaux~\ref{ThoDecompNoyayzzMWod} :
            \begin{equation}
                E=\ker(X-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(X-\lambda_r)^{\alpha_r}.
            \end{equation}
            Les espaces \( F_{\lambda_i}(u)=\ker(X-\lambda_i)^{\alpha_i}\) sont les espaces caractéristiques de \( u\), ce qui fait que \( u-\lambda_i\mtu\) est nilpotent sur \( F_{\lambda_i}(u)\). L'endomorphisme \( u-\lambda_i\mtu\) est donc strictement trigonalisable supérieur sur son bloc\footnote{Proposition~\ref{PropMWWJooVIXdJp}.}. Cela signifie que \( u\) est triangulaire supérieure avec les valeurs propres sur la diagonale.

        \item[\ref{ItemZKDMooOrTHkwi}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwii}]

            C'est immédiat parce que le déterminant d'une matrice triangulaire est le produit des éléments de sa diagonale.
    \end{subproof}
\end{proof}

\begin{remark}
    La méthode des pivots de Gauss\footnote{Le lemme~\ref{LemZMxxnfM}.} certes permet de trigonaliser n'importe quoi, mais elle ne correspond pas à un changement de base. Autrement dit, les pivots de Gauss ne sont pas de similitudes.

    C'est là qu'il faut bien avoir en tête la différence entre \emph{équivalence} et \emph{similarité}\footnote{Définition \ref{DefBLELooTvlHoB}.}. Lorsqu'on parle de changement de base, de matrice trigonalisable ou diagonalisable, nous parlons de similarité et non d'équivalence.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trigonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

La proposition~\ref{PropKNVFooQflQsJ} dit déjà que tous les endomorphismes sont trigonalisables sur \( \eC\). Nous allons aller plus loin et montrer que la trigonalisation peut être effectuée à l'aide d'une matrice unitaire.

Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque~\ref{RemXFZTooXkGzQg} utilisant la proposition~\ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe, trigonisation\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
    Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure\footnote{«triangulaire supérieure» ne signifie pas «strictement triangulaire supérieure». Ici, il est possible que la diagonale soit non nulle; non seulement possible, mais même très probable en pratique.}.
\end{lemma}
\index{lemme!Schur complexe}
%TODO : Le lemme de Schur est souvent énoncé en disant que si p est une représentation irréductible, alors les seuls endomorphismes de V commutant avec tous les p(g) sont les multiples de l'identité. Quel est le lien avec ceci ?

\begin{proof}
    Étant donné que \( \eC\) est algébriquement clos, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
    \begin{equation}
        Q=\begin{pmatrix}
             \uparrow   &   \uparrow    &       &   \uparrow    \\
             v   &   u_2    &   \cdots    &   u_n    \\
             \downarrow   &   \downarrow    &       &   \downarrow
         \end{pmatrix}.
    \end{equation}
    Nous avons \( Q^{-1}AQe_1=Q^{-1} Av=\lambda Q^{-1} v=\lambda e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
    \begin{equation}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    \\
            0    &   A_1
        \end{pmatrix}
    \end{equation}
    où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale).
\end{proof}
En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{lemma}       \label{LEMooRCFGooPPXiKi}
    Soit \( A\in \eM(n,\eC)\) et une matrice unitaire \( U\) telle que \( A=UTU^{-1}\) où \( T\) est triangulaire.
    \begin{enumerate}
        \item
            En ce qui concerne les polynômes caractéristiques, \( \chi_A=\chi_T\).
        \item
            Pour les spectres, \( \Spec(A)=\Spec(T)\).
        \item
            Les valeurs propres de \( A\) sont les éléments diagonaux de \( T\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que \( U\) commute évidemment avec \( \mtu\) nous avons
    \begin{equation}
        \chi_A(\lambda)=\det(A-\lambda \mtu)=\det(UTU^{-1}-\lambda\mtu)=\det\big( U(T-\lambda\mtu)U^{-1} \big).
    \end{equation}
    À ce niveau nous utilisons le fait que le déterminant soit multiplicatif~\ref{PropYQNMooZjlYlA} pour conclure :
    \begin{equation}
        \chi_A(\lambda)=\det\big( U(T-\lambda\mtu)U^{-1} \big)=\det(U)\det(T-\lambda\mtu)\det(U^{-1})=\det(T-\lambda\mtu)=\chi_T(\lambda).
    \end{equation}

    Pour les spectres, l'égalité des polynômes caractéristique implique l'égalité des spectres parce que les valeurs propres sont les racines du polynôme caractéristique par le théorème~\ref{ThoWDGooQUGSTL}.

    Les valeurs propres d'une matrice triangulaire sont les valeurs sur la diagonale.
\end{proof}

\begin{remark}
    Le lemme mentionne le fait que les valeurs propres de \( A\) sont les éléments diagonaux de \( T\). Mais attention : ceci ne dit rien au niveau des multiplicités géométriques. Un nombre peut être cinq fois sur la diagonale de \( T\) alors que l'espace propre correspondant pour \( A\) n'est que de dimension \( 1\). Exemple : la matrice
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\
            0    &   1
        \end{pmatrix}
    \end{equation}
    a deux \( 1\) sur la diagonale. Le nombre \( 1\) est bien une valeur propre de \( A\), mais le système
    \begin{equation}
        A\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}
    \end{equation}
    donne \( y=0\) et donc un espace propre de dimension seulement \( 1\).
\end{remark}

\begin{remark}  \label{RemXFZTooXkGzQg}
    Si \( \eK\) est algébriquement clos (comme \( \eC\) par exemple), alors tous les polynômes sont scindés et toutes les matrices sont trigonalisables\footnote{La proposition~\ref{PropKNVFooQflQsJ} montre cela, et le lemme de Schur complexe~\ref{LemSchurComplHAftTq} va un peu plus loin et précise que la trigonalisation peut être faite par une matrice unitaire.}. Un exemple un peu simple de cela est la matrice
    \begin{equation}
        u=\begin{pmatrix}
            0    &   -1    \\
            1    &   0
        \end{pmatrix}.
    \end{equation}
    Le polynôme caractéristique est \( \chi_u(X)=X^2+1\) et les valeurs propres sont \( \pm i\). Il est vite vu que dans la base
    \begin{equation}
        \{ \begin{pmatrix}
        i    \\
    1
\end{pmatrix}, \begin{pmatrix}
1    \\
i
\end{pmatrix}\}
    \end{equation}
    de \( \eC^2\), la matrice \( u\) se note \( \begin{pmatrix}
        i    &   0    \\
        0    &   -i
    \end{pmatrix}\).
\end{remark}

\begin{remark}  \label{RemREOSooGEDJWX}
    Cela nous donne une autre façon de prouver qu'une matrice nilpotente de \( \eM(n,\eC)\) ou \( \eM(n,\eR)\) est trigonalisable\cite{KDUFooVxwqlC}. D'abord dans \( \eM(n,\eC)\), toutes les matrices sont trigonalisables\footnote{Parce que le polynôme caractéristique est scindé, voir la proposition~\ref{PropKNVFooQflQsJ}..}, et les valeurs propres arrivent sur la diagonale. Mais comme les valeurs propres d'une matrice nilpotente sont zéro, elle est triangulaire stricte. Par ailleurs son polynôme caractéristique est alors \( X^n\).

    Ensuite si \( u\in \eM(n,\eR)\) nous pouvons voir \( u\) comme une matrice dans \( \eM(n,\eC)\) et y calculer son polynôme caractéristique qui sera tout de même \( X^n\). Ce polynôme étant scindé, la proposition~\ref{PropKNVFooQflQsJ} nous assure que \( u\) est trigonalisable. Une fois de plus, les valeurs propres étant sur la diagonale, elle est triangulaire supérieure stricte.
\end{remark}

\begin{corollary}   \label{CorUNZooAZULXT}
    Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
    \begin{equation}
        \chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
    Le lemme~\ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons
    \begin{equation}
        \chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
            X-\lambda_1    &   *    &   *    \\
            0    &   \ddots    &   *    \\
            0    &   0    &   X-\lambda_r
        \end{pmatrix},
    \end{equation}
    qui vaut bien le produit annoncé.
\end{proof}

\begin{corollary}       \label{CORooTPDHooXazTuZ}
    Si \( A\in \eM(n,\eC)\) et \( k\in \eN\) alors
    \begin{equation}
        \Spec(A^k)=\{ \lambda^k\tq \lambda\in \Spec(A) \}.
    \end{equation}
\end{corollary}

\begin{proof}
    Par le lemme~\ref{LemSchurComplHAftTq} nous avons une matrice unitaire \( U\) et une triangulaire \( T\) telles que \( A=UTU^{-1}\). En passant à a puissance \( k\) nous avons aussi
    \begin{equation}
        A^k=UT^kU^{-1}.
    \end{equation}
    Donc le spectre de \( A^k\) est celui de \( T^k\) (lemme~\ref{LEMooRCFGooPPXiKi} et le fait qu'une puissance d'une matrice triangulaire est encore triangulaire). Or les éléments diagonaux de \( T^k\) sont les puissances \( k\)\ieme des éléments diagonaux de \( T\), qui sont les valeurs propres de \( A\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, ce qu'on a}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Théorème spectral hermitien]      \label{LEMooVCEOooIXnTpp}
    Pour un opérateur hermitien\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.},
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres pour des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part
    \begin{equation}
        \langle Av, v\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

\begin{remark}      \label{REMooMLBCooTuKFmz}
    Un opérateur de la forme \( A^*A\) est évidemment hermitien. De plus ses valeurs propres sont toutes positives parce que si \( A^*Ax=\lambda v\) alors
    \begin{equation}
        0\leq \langle Av, Av\rangle =\langle A^*Av, v\rangle =\lambda\langle v, v\rangle .
    \end{equation}
    Donc \( \lambda\geq 0\).
\end{remark}

\begin{definition}  \label{DefWQNooKEeJzv}
    Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} s'il commute avec son adjoint.
\end{definition}

Les opérateurs normaux comprennent évidemment les opérateurs hermitiens, mais également les anti-hermitiens, et ça c'est bien parce que c'est le cas de l'algèbre associée à \( \SU(2)\).

\begin{theorem}[Théorème spectral pour les matrices normales\footnote{Définition~\ref{DefWQNooKEeJzv}}\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver~\ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
    %TODO : le reste.

    Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme~\ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\cdots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

Tant que nous en sommes à parler de spectre de matrices hermitiennes\ldots Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} et le théorème~\ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque~\ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{lemma}[\cite{ooLMMRooUXhOdx}]   \label{LEMooHUGEooVYhZdZ}
    Si \( A\) est une matrice carrée et inversible,
    \begin{equation}
        \Spec(A^*A)=\Spec(AA^*)
    \end{equation}
\end{lemma}

\begin{proof}
    Nous allons montrer l'égalité des polynômes caractéristiques. D'abord une simple multiplication montre que
    \begin{equation}
        (A^*A-\lambda\mtu)A^{-1}=A^{-1}(AA^*-\lambda\mtu).
    \end{equation}
    Nous prenons le déterminant de cette égalité en utilisant les propriétés~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} et~\ref{ITEMooZMVXooLGjvCy} :
    \begin{equation}
        \det(A^*A-\lambda\mtu)\det(A^{-1})=\det(A^{-1})\det(AA^*-\lambda\mtu).
    \end{equation}
    En simplifiant par \( \det(A^{-1})\) (qui est non nul parce que \( A\) est inversible) nous obtenons l'égalité des polynômes caractéristiques et donc l'égalité des spectres.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        QAQ^{-1}=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\
            0    &   0    &   \lambda_r    &   *    &   *\\
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\
                c_1    &   d_1
            \end{pmatrix}&   *\\
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\
                c_s    &   d_s
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}
\index{lemme!Schur réel}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\
                \cdot    &   \cdot
            \end{pmatrix}&   C_1    \\
            0    &   A_1
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjuguées.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( QAQ^{-1}\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
    Une matrice symétrique réelle,
    \begin{enumerate}
        \item       \label{ITEMooJWHLooSfhNSW}
            a un spectre contenu dans \( \eR\)
        \item       \label{ITEMooMWWRooXxGONW}
            est diagonalisable par une matrice orthogonale.
    \end{enumerate}
    Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\).

    Le lemme de Schur réel~\ref{LemSchureRelnrqfiy} donne une matrice orthogonale qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( QAQ^{-1}\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( QAQ^{-1}\) est symétrique :
    \begin{equation}
        (QAQ^{-1})^t=(Q^{-1})^tA^tQ^t=QA^tQ^{-1}=QAQ^{-1}
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

    En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale e telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce que
    \begin{equation}
        \langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{proof}
Le théorème spectral pour les opérateurs autoadjoints sera traité plus bas parce qu'il a besoin de choses sur les formes bilinéaires, théorème~\ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchainement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
    Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

    En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
    \begin{equation}
        \begin{aligned}[]
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \lambda_2    &       \\
                &       &   \lambda_3
            \end{pmatrix}
            \begin{pmatrix}
                a    &   b    &   c    \\
                *    &   *    &   *    \\
                *    &   *    &   *
            \end{pmatrix}&=
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1a    &   \lambda_1b    &   \lambda_1c    \\
            *    &   *    &   *    \\
            *    &   *    &   *
        \end{pmatrix}\\
        &=\begin{pmatrix}
            \lambda_1 a^2+*   &   \lambda_1ab+*    &   \lambda_1ac  +*  \\
            \ldots    &   \ldots    &   \ldots    \\
            \ldots    &   \ldots    &   \ldots
        \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}

\begin{definition}[Matrice définie positive, opérateur définit positif]    \label{DefAWAooCMPuVM}
    Un opérateur sur un espace vectoriel sur \( \eC\) ou \( \eR\) est \defe{définit positif}{opérateur!définit positif} si toutes ses valeurs propres sont réelles et strictement positives.  Il est \defe{semi-définie positive}{semi-définie positive} si ses valeurs propres sont réelles positives ou nulles.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} définie positive pour positive.

\begin{normaltext}      \label{NORMooAJLHooQhwpvr}
    Nous nommons \( S^+(n,\eR)\) l'ensemble des matrices réelles symétriques \( n\times n\) et \( S^{++}(n,\eR)\) le sous-ensemble de \( S^+(n,\eR)\) des matrices strictement définies positives.
\end{normaltext}
    \nomenclature[B]{$ S^+(n,\eR)$}{matrices symétriques définies positives}
    \nomenclature[B]{$ S^{++}(n,\eR)$}{matrices symétriques strictement définies positives}

\begin{remark}
    Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

\begin{proposition}     \label{PropcnJyXZ}
    Soit $M$, une matrice symétrique. Nous avons
    \begin{enumerate}
        \item       \label{ITEMooTJVQooYmRkas}
            $\det M>0$ et $\tr(M)>0$ implique $M$ définie positive\footnote{Défintion~\ref{DefAWAooCMPuVM}.},
        \item
        $\det M>0$ et $\tr(M)<0$ implique $M$ définie négative,
    \item   \label{ItemluuFPN}
        $\det M<0$ implique ni semi-définie positive, ni définie négative
        \item
        $\det M=0$ implique $M$ semi-définie positive ou semi-définie négative.
    \end{enumerate}
\end{proposition}

\begin{proposition}     \label{PROPooUAAFooEGVDRC}
    Une application linéaire est définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.} si et seulement si sa matrice associée l'est.
\end{proposition}

Lorsqu'un énoncé parle d'une matrice symétrique, le premier réflexe est de la diagonaliser : considérer une matrice orthogonale \( T\) telle que \( T^tMT=D\) avec \( D\) diagonale. Et les valeurs propres sur la diagonale : \( D_{kl}=\delta_{kl}\lambda_k\). Les matrices symétriques définies positives ont cependant des propriétés même en dehors de leur base de diagonalisation.

\begin{lemma}   \label{LemWZFSooYvksjw}
    Soit une matrice symétrique \( M\).
    \begin{enumerate}
        \item       \label{ITEMooSKRAooOgHbGA}
           Elle est strictement définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\) non nul dans \( \eR^n\).
        \item       \label{ITEMooMOZYooWcrewZ}
           Elle est semi-définie positive si et seulement si \( \langle x, Mx\rangle \geq 0\) pour tout \( x\) non nul dans \( \eR^n\).
       \item        \label{ITEMooRRMFooHSOHxZ}
           Si elle est seulement définie positive, alors \( \langle x, Mx\rangle \geq \lambda\| x \|^2\) dès que \( \lambda\geq 0\) minore toutes les valeurs propres.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Démonstration en trois parties.
    \begin{subproof}
    \item[\ref{ITEMooSKRAooOgHbGA}]
    Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral~\ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
    \begin{equation}
        \langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( M\). Cela est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont strictement positifs.
\item[\ref{ITEMooMOZYooWcrewZ}]

    Nous avons encore
    \begin{equation}
        \langle x, Mx\rangle =\sum_{i}\lambda_ix_i^2.
    \end{equation}
    Cela est plus grand ou égal à zéro si et seulement si tous les \( \lambda_i\) sont plus grands ou égaux à zéro.

\item[\ref{ITEMooRRMFooHSOHxZ}]

        Soit une matrice orthogonale \( T\) diagonalisant \( M\), c'est-à-dire telle que \( T^tMT=D\) avec \( D\) diagonale. Nous allons vérifier que
        \begin{equation}
            \langle Tx, Mtx\rangle \geq \lambda\| Tx \|^2
        \end{equation}
        pour tout \( x\). Vu que \( T\) est une bijection \footnote{Une matrice orthogonale a un déterminant $\pm 1$.}, cela impliquera le résultat pour tout \( x\). Si nous considérons la base de diagonalisation \( \{ e_k \}\) pour les valeurs propres \( \lambda_k\), nous avons le calcul
       \begin{subequations}
            \begin{align}
                \langle Tx, MTx\rangle &=\langle x, T^tMTx\rangle \\
                &=\langle x, Dx\rangle \\
                &=\sum_k\langle x, x_kDe_k\rangle \\
                &=\sum_k\lambda_kx_k \underbrace{\langle x, e_k\rangle }_{=x_k}\\
                &\geq \sum_k\lambda| x_k |^2\\
                &=\lambda\| x \|^2\\
                &=\lambda\| Tx \|^2.
            \end{align}
        \end{subequations}
        Au dernier passage nous avons utilisé le fait que \( T\) est une isométrie (proposition~\ref{PropKBCXooOuEZcS}).
    \end{subproof}
\end{proof}

Les personnes qui aiment les vecteurs lignes et colonnes écriront des inégalités comme
\begin{equation}
    x^tMx\geq x^tx.
\end{equation}
Tout à l'autre bout du spectre des personnes névrosées des notations, on trouvera des inégalités comme
\begin{equation}
    M(x\otimes x)\geq x\cdot x.
\end{equation}
Le penchant personnel de l'auteur de ces lignes est la notation avec le produit tensoriel. Si vous aimez ça, vous pouvez lire la section \ref{SECooUKRYooZjagcX} et en particulier ce qui suit \eqref{EQooUNRYooKBrXyK}.

La notation adoptée ici avec le produit scalaire \( \langle x, Mx\rangle \) est entre les deux. Elle a l'avantage de n'être pas technologique comme le produit tensoriel (si vous y mettez les pieds, vous devez savoir ce que vous faites), tout en évitant de se casser la tête à savoir qui est un vecteur ligne ou un vecteur colonne.

\begin{corollary}
    Une matrice symétrique strictement définie positive est inversible.
\end{corollary}

\begin{proof}
    Si \( Ax=0\) alors \( \langle Ax, x\rangle =0\). Mais dans le cas d'une matrice strictement définie positive, cela implique \( x=0\) par le lemme~\ref{LemWZFSooYvksjw}.
\end{proof}

\begin{lemma}
    Pour une base quelconque, les éléments diagonaux d'une matrice symétrique semi-définie positive sont positifs. Si la matrice est strictement définie positive, alors les éléments diagonaux sont strictement positifs.
\end{lemma}

\begin{proof}
    Il s'agit d'une application du lemme~\ref{LemWZFSooYvksjw}. Si \( A\) est définie positive et que \( \{ e_i \}\) est une base, alors
    \begin{equation}
        A_{ii}=\langle Ae_i, e_i\rangle \geq \lambda\| e_i \|^2=\lambda\geq 0.
    \end{equation}
    Si \( A\) est strictement définie positive, alors \( \lambda\) peut être choisi strictement positif.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{BIBooWVWZooZqliJt}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme!quadratique} sur \( E\) est une application \( q\colon E\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon E\times E\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in E\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{proposition} \label{PROPooZLXVooOsXCcB}
    Soit une forme bilinéaire \( b\) et la forme quadratique associée \( q\). Alors nous avons l'\defe{identité de polarisation}{identité de polarisation} :
    \begin{equation}    \label{EqMrbsop}
        b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de substituer dans le membre de droite \( q(x)=b(x,y)\) et d'utiliser la bilinéarité :
    \begin{subequations}
        \begin{align}
            q(x)+q(y)-q(x-y)&=b(x,x)+b(y,y)-b(x-y,x-y)\\
            &=b(x,x)+b(y,y)-b(x)+b(x,y)+b(y,x)-b(y,y)\\
            &=2b(x,y)
        \end{align}
    \end{subequations}
    où nous avons utilisé le fait que \( b\) est symétrique : \( b(x,y)=b(y,x)\).
\end{proof}

\begin{lemma}       \label{LEMooLKNTooSfLSHt}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'identité de polarisation de la proposition \ref{PROPooZLXVooOsXCcB}. Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

\begin{definition}      \label{DEFooGECOooCCGVXG}
    Soit une forme quadratique \( q\) sur \( E\). Nous disons que \( v,w\in E\) sont \defe{\( q\)-orthogonaux}{\( q\)-orthogonal} si \( b(v,w)=0\) la forme bilinéaire \( b\) associée à \( q\) par le lemme \ref{LEMooLKNTooSfLSHt}.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice associée à une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooAOGPooXWXUcN}
    Soit une forme bilinéaire \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous définissons les nombres
    \begin{equation}    \label{EQooCUGFooRlKUtu}
        B_{\alpha\beta}=b(f_{\alpha},f_{\beta}),
    \end{equation}
    qui forment une matrice symétrique dans \( \eM(n,\eK)\). Cette matrice est la \defe{matrice associée}{matrice d'une forme bilinéaire} à la forme bilinéaire \( b\).

    La matrice d'une forme quadratique est celle associée à sa forme bilinéaire associée.
\end{definition}

\begin{lemma}       \label{LEMooDCIOooTlVZMR}
    Soit une forme bilinéaire \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous notons \( B\) la matrice de \( b\) (definition \ref{DEFooAOGPooXWXUcN}) et \( q\) la forme quadratique associée.

    Alors nous avons
\begin{equation}        \label{EQooQFMWooVKVLMx}
    b(x,y)=\sum_{\alpha\beta}B_{\alpha\beta}x_{\alpha}y_{\beta}.
\end{equation}
et
\begin{equation}
    b(x,y)=x\cdot By.
\end{equation}
où le point est le produit scalaire usuel (composante par composante).
\end{lemma}

\begin{proof}
    Si \( x=\sum_{\alpha}x_{\alpha}f_{\alpha}\) et \( y=\sum_{\beta}y_{\beta}f_{\beta}\) :

    En utilisant la convention \eqref{EQooAXRJooUwHbjB} et les choses autour (voir aussi \ref{SECooBTTTooZZABWA}),
    \begin{equation}
        b(x,y)=\sum_{\alpha}x_{\alpha}\sum_{\beta}B_{\alpha\beta}y_{\beta}=\sum_{\alpha}x_{\alpha}(By)_{\alpha}=x\cdot By.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooCIEUooODqfwm}
    Soit une forme quadratique \( q\colon E\to \eK\) et sa matrice\footnote{Matrice associée à une forme quadratique, définition \ref{DEFooAOGPooXWXUcN}.} \( (q_{ij})\in \eM(n,\eK)\). Nous avons
    \begin{subequations}        \label{SUBEQSooEHVXooJjKLqyiB}
        \begin{align}
            q(x)&=\sum_{i=1}^n\sum_{j=1}^nq_{ij}x_ix_j\\
            &=\sum_{i=1}^nq_{ii}x_i^2+2\sum_{1\leq i <j\leq n}q_{ij}x_ix_j.
        \end{align}
    \end{subequations}
\end{proposition}

\begin{normaltext}
    De nombreux auteurs préfèrent écrire des choses comme \( x^tBy\) ou \( xB^ty\) ou \( xBy^t\) et se poser de longues questions sur qui est un «vecteur colonne» et qui est un «vecteur ligne», et si la matrice \( B\) soit être transposée ou non. Toutes ces notations servent(?) à cacher un bête produit scalaire.
\end{normaltext}

\begin{normaltext}
    Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Voir la section \ref{SECooBTTTooZZABWA}]     \label{PROPooLBIOooUpzxXA}
    Soit une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}} \( b\colon V\times V\to \eK\) dont la matrice\footnote{Définition~\ref{EQooCUGFooRlKUtu}.} dans la base \( \{ e_i \}\) est \( A\) et celle dans la base \( \{ f_{\alpha} \}\) est \( B\). Nous supposons que les bases sont liées par \( f_{\alpha}=\sum_{i}Q_{i\alpha}e_i\). Alors
\begin{equation}        \label{EQooZUVTooKjqnJj}
    B=Q^tAQ.
\end{equation}
\end{proposition}

\begin{proof}
    Soit \( x,x'\in V\) de coordonnées \( (x_i)\) et \( (x'_i)\) dans la base \( \{ e_i \}\) et \( (y_{\alpha})\), \( (y'_{\alpha})\) dans la base \( \{ f_{\alpha} \}\). Par définition de la matrice associée à une forme bilinéaire,
    \begin{equation}
        b(x,x')=\sum_{ij}A_{ij}x_ix'_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
    \end{equation}
    En remplaçant les \( x_i\) et \( x'_i\) par leurs valeurs en fonction de \( y_{\alpha}\) et \( y'_{\beta}\) données par la proposition \ref{PROPooNYYOooHqHryX}, nous trouvons
    \begin{subequations}
        \begin{align}
            b(x,x')&=\sum_{ij\alpha\beta}A_{ij}Q_{i\alpha}y_{\alpha}Q_{j\beta}y'_{\beta}\\
            &=\sum_{\alpha\beta}(Q^tAQ)_{\alpha\beta}y_{\alpha}y'_{\beta}
        \end{align}
    \end{subequations}
    où \( Q^t\) désigne la transposée de la matrice \( Q\) :  \( Q^t_{ij}=Q_{ji}\). Vu que les nombres \( y_{\alpha}\) et \( y'_{\beta}\) sont arbitraires nous déduisons\footnote{Lemme~\ref{LEMooLXAHooPRyHaF}.} que \( B=Q^tAQ\).
\end{proof}

\begin{remark}      \label{REMooNEJLooSqgeih}
    Notons que cette «loi de transformation» n'est pas la même que celle pour une application linéaire\footnote{Proposition \ref{PROPooNZBEooWyCXTw}.}. Ici nous avons \( Q^t\) alors que pour les applications linéaires nous avions \( Q^{-1}\).

    Pour cette raison, tant que nous travaillons avec des bases orthonormées, c'est-à-dire tant que \( Q\) est orthogonale\footnote{Définition~\ref{DefMatriceOrthogonale}.}, nous pouvons confondre une application linéaire avec une application bilinéaire en passant par la matrice. Mais cette identification n'est pas du tout canonique : elle repose sur le fait que les bases soient orthonormées.

    Il en découle que la réduction des endomorphismes et la réduction des formes bilinéaires ne sont pas tout à fait les mêmes théories. Par exemple la pseudo-diagonalisation simultanée (corolaire~\ref{CorNHKnLVA}) est un résultat de réduction de forme bilinéaire et non d'endomorphismes.
\end{remark}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Réduction de Gauss\cite{BIBooNUUEooJUjLpy,BIBooUULNooUtlrar}]     \label{THOooOMMFooKxqICS}
    Soit une forme quadratique non nulle \( q\) sur l'espace vectoriel \( E\) sur le corps \( \eK\). Il existe une base  \(\{ l_i \}_{i=1,\ldots, n}\) de \( E^*\) et des coefficients \( \alpha_i\in \eK\) tels que 
        \begin{equation}
            q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
        \end{equation}
\end{theorem}

\begin{proof}
    Notre point de départ sont les formules \eqref{SUBEQSooEHVXooJjKLqyiB} pour la forme quadratique. Nous allons faire la preuve par récurrence sur la dimension de l'espace. Si \( n=1\), alors nous avons seulement
    \begin{equation}
        q(x)=\alpha x^2
    \end{equation}
    et donc le théorème est fait avec \( l(x)=x\).

    Nous supposons que le théorème est prouvé pour tout espace de dimension \( n\). Une forme quadratique pour un espace de dimension \( n+1\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^{n+1}m_{ii}x_i^2+2\sum_{1\leq i < j\leq n+1}m_{ij}x_ix_j.
    \end{equation}
    Vu que \( q\) est non nulle, un des \( m_{ij}\) est non nul. Nous allons diviser en plusieurs cas.
    \begin{itemize}
        \item
            \( m_{11}\neq 0\)
        \item
            \( m_{kk}\neq 0\) avec \( k\neq 1\)
        \item
            \( m_{12}\neq 0\) et \( m_{ii}=0\) pour tout \( i\).
        \item
            \( m_{kl}\neq 0\) avec \( (k,l)\neq (1,2)\) et \( m_{ii}=0\) pour tout \( i\).
    \end{itemize}
    Ces cas ne sont pas exclusifs, mais ils couvrent toutes les possibilités.

    \begin{subproof}
        \item[Si \( m_{11}\neq 0\)]
            Nous écrivons \( q\) sous la forme
            \begin{subequations}
                \begin{align}
                    q(x)&=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big)\\
                    &=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{j=2}^{n+1}m_{1j}x_1x_j+2\sum_{i=2}^n\sum_{j=i+1}^{n+1}(m_{ij}x_ix_j)\\
                    &=m_{11}x_1^2+2x_1\sum_{j=2}^{n+1}m_{1j}x_k+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\left( x_1^2+2x_1\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j \right)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1^2+2x_1f(x_2,\ldots, x_{n+1}) \big)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1+f(x_2,\ldots, x_{n+1}) \big)^2-f(x_2,\ldots, x_{n+1})+R(x_2,\ldots, x_{n+1})
                \end{align}
            \end{subequations}
            où 
            \begin{itemize}
                \item \( R\) est une forme quadratique de \( n-1\) variables;
                \item nous avons noté \( f(x_2,\ldots, x_{n+1})=\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j\).
            \end{itemize}
            Maintenant, toute la partie \( -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})\) est une forme quadratique de \( n\) variables. Par hypothèse de récurrence, il existe des coefficients \( \alpha_i\) et des formes linéairement indépendantes sur \( \eK^n\) \( l_i'(x_2,\ldots, x_{n+1})\) telles que
            \begin{equation}
                -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})=\sum_{i=2}^{n+1}\alpha_il_i'(x_2,\ldots, x_{n+1})^2.
            \end{equation}
            En posant ensuite \( l_j(x_1,\ldots, x_{n+1})=l'_j(x_2,\ldots, x_{n+1})\), ainsi que \( l_1(x_1,\ldots, x_{n+1})=x_1+f(x_2,\ldots, x_{n+1})\), nous avons
            \begin{equation}
                q(x)=m_{11}l_1(x)^2+\sum_{j=2}^{n+1}\alpha_jl_j(x)^2.
            \end{equation}
            
        \item[Si \( m_{kk}\neq 0\) avec \( k\neq 1\)]

            Nous nommons \( k\) le plus petit entier pour lequel \( m_{kk}\neq 0\), et nous supposons que \( k\neq 1\), parce que nous avons déjà couvert ce cas. Dans ce cas, nous avons
            \begin{equation}
                q(x)=m_{kk}x_k^2+\sum_{j=k+1}^{n+1}m_{jj}x_j^2  +2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big),
            \end{equation}
            et tout tourne comme dans le premier cas.
        \item[\( m_{ii}=0\) pour tout \( i\) et \( m_{12}\neq 0\)]
            Nous écrivons \( q\) en séparant les termes \( m_{1k}\) :
            \begin{subequations}
                \begin{align}
                    q(x)&=2\sum_{1\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2\sum_{2\leq j\leq n+1}m_{1j}x_1x_j+2\sum_{2\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2x_1\sum_{2\leq j\leq n+1}m_{1j}x_j+2\sum_{3\leq j\leq n+1}m_{2j}x_2x_j+2\sum_{3\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+x_1f(x_2,\ldots, x_{n+1})+x_2g(x_3,\ldots, x_{n+1})+T(x_3,\ldots, x_{n+1})      \label{SUBEQooLBXBooXoLyuw}
                \end{align}
            \end{subequations}
            où \( f\) et \( g\) sont linéaires et \( T\) est multilinéaire.

            À ce moment, nous tentons de factoriser toute la partie concernant \( x_1\) et \( x_2\). L'idée est d'utiliser ceci :
            \begin{equation}
                (x_1+g)(x_2+f)=x_1x_2+x_1f+x_2g+fg,
            \end{equation}
            mais en mettant les bons coefficients pour reproduire ce que nous avons dans \eqref{SUBEQooLBXBooXoLyuw} : 
            \begin{equation}
                (2m_{12}+2g)(x_1+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }=2m_{12}x_1x_2+2x_1f+2x_2g.
            \end{equation}
            Cela pour dire que
            \begin{equation}
                q(x)=2(m_{12}x_1+g)(x_2+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }+T
            \end{equation}
            où \(-2fg/m_{12}+T\) est une forme quadratique de \( x_3,\ldots, x_{n+1}\), c'est à dire de \( n-1\) variables.

            L'hypothèse de récurrence nous donne des formes linéaires \( (l_i)_{i=3,\ldots, n+1}\) telles que
            \begin{equation}
                \frac{ 2fg }{ m_{12} }+T=\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
            Nous pouvons donc déjà écrire
            \begin{equation}
                q(x)=2l'_1(x)l'_2(x)+\sum_{i=3}^{n+1}\alpha_il_i(x)^2
            \end{equation}
            où
            \begin{itemize}
                \item Les forme \( l_i\) avec \( i\geq 3\) ne dépendent pas de \( x_1\) et \( x_2\), et sont donc indépendantes de \( l_1\) et \( l_2\).
                \item La forme \( l'_1\) ne dépend pas de \( x_2\),
                \item La forme \( l'_2\) ne dépend pas de \( x_1\).
            \end{itemize}
            Ce sont donc \( n+1\) formes linéaires indépendantes. Le seul problème résiduel est que les formes \( l'_1\) et \( l'_2\) arrivent en produit l'une de l'autre. Nous en définissons donc deux de plus :
            \begin{equation}
                \begin{aligned}[]
                    l_1(x)=\frac{ 1 }{2}(l'_1+l'_2)\\
                    l_2(x)=\frac{ 1 }{2}(l'_1-l'_2),
                \end{aligned}
            \end{equation}
            qui sont linéairement indépendantes l'une de l'autre et indépendantes des \( l_i\) (\( i\geq 3\)). Au final,
            \begin{equation}
                q(x)=l_1(x)^2+l_2(x)^2+\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
        \item[Si \( m_{ii}=0\) et \( m_{12}=0\) et \( m_{kl}\neq 0\) avec \( k<l\)]
            Nous considérons la permutation
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \} \\
                    i&\mapsto \begin{cases}
                         1   &   \text{si } i=k\\
                         2   &   \text{si } i=l\\
                         k   &   \text{si } i=1\\
                         l   &   \text{si } i=2\\
                        i    &    \text{sinon,}
                    \end{cases}
                \end{aligned}
            \end{equation}
            c'est à dire que \( \sigma\) permute \( 1\) et \( k\) ainsi que \( 2\) et \( l\). Ensuite nous posons
            \begin{equation}
                \begin{aligned}
                    s\colon \eR^{n+1}&\to \eR^{n+1} \\
                    e_i&\mapsto e_{\sigma(i)}. 
                \end{aligned}
            \end{equation}
            Nous allons un peu considérer \( q\circ s\), pour changer : 
            \begin{equation}        \label{EQooLVAWooAirEzP}
                (q\circ s)(x)=\sum_{i,j}m_{ij}s(x)_is(x)_j=\sum_{ij}x_{\sigma(i)}x_{\sigma(j)}.
            \end{equation}
            parce que \( s(x)_i=x_{\sigma(i)}\). 

            Utilisons un petit abus de notation pour considérer
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \} \\
                    (i,j)&\mapsto \big(\sigma(i), \sigma(j)\big). 
                \end{aligned}
            \end{equation}
            Cela est une bijection; nous pouvons utiliser le lemme \ref{DEFooLNEXooYMQjRo} pour permuter les termes dans \eqref{EQooLVAWooAirEzP} :      
            \begin{subequations}
                \begin{align}
                    (q\circ s)(x)&=\sum_{ij}m_{\sigma(i)\sigma(j)}x_{\sigma\sigma(i)}x_{\sigma\sigma(j)}\\
                    &=\sum_{ij}a_{ij}x_ix_j     \label{EQooPCTCooFnMWat}
                \end{align}
            \end{subequations}
            où nous avons posé \( a_{ij}=m_{\sigma(i)\sigma(j)}\) et utilisé le fait que \( \sigma=\sigma^{-1}\). Le point intéressant de l'histoire est que dans \eqref{EQooPCTCooFnMWat}, \( a_{12}=m_{kl}\neq 0\). La forme \( q\circ s\) est donc dans le cas déjà traité et il existe des formes linéaires \( l'_i\) telles que
            \begin{equation}
                (q\circ s)(x)=\sum_{i=1}^{n+1}\alpha_il'_i(x)^2.
            \end{equation}
            En évaluant cela en \( s(x)\), et en tenant compte de \( s=s^{-1}\), nous trouvons
            \begin{equation}
                q(x)=\sum_i\alpha_i(l_i\circ s)(x)^2,
            \end{equation}
            de telle sorte que \( l_i=l'_i\circ s\) soit la réponse à notre théorème.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooUULNooUtlrar}]       \label{PROPooYXMMooYIuGRd}
    Soient un espace vectoriel \( (E,\eK)\) et une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\). Une base de \( E\) est \( q\)-orthogonale\footnote{Définition \ref{DEFooGECOooCCGVXG}.} si et seulement si la matrice de \( q\) dans cette base est diagonale.
\end{proposition}

\begin{proof}
    La matrice de \( q\) est donnée par \( Q_{ij}=b(e_i,e_j)\). Donc oui, cette matrice est diagonale si et seulement si les \( e_i\) sont orthogonaux.
\end{proof}

\begin{proposition}
    Soit une forme quadratique \( q\). Si une base \( (e_i )\) de \( E\) est \( q\)-orthogonale, alors \( \mB=\{ e_i\tq q(e_i)=0 \}\) est une base de \( \ker(q)\).
\end{proposition}

\begin{proof}
    Nous considérons un vecteur de base \( e_j\), et nous montrons que \( q(e_j)=0\) si et seulement si \( e_j\in\ker(q)\). Nous savons par la proposition \ref{PROPooYXMMooYIuGRd} que la matrice de \( q\) dans la base \( (e_i)\) est diagonale et que les éléments diagonaux sont les \( q(e_i)\). Soit \( K=\{ i\tq q_(e_i)=0 \}\).
    \begin{subproof}
    \item[\( \Span\{ e_i \}_{i\in K}\subset\ker(q)\)]
        Si \( x=\sum_{i\in K}x_ie_i\), alors 
        \begin{equation}
            q(x)=b(x,x)=\sum_{i,j\in K}| x_i |^2b(e_i,e_j)=\sum_{i,j\in K}| x_i |^2\delta_{ij}q(e_i)=0
        \end{equation}
        parce que \( q(e_i)=0\) dès que \( i\in K\).
        \item{\( \ker(q)\subset\Span\{ e_i \}_{i\in K}\)}
            Soit \( x\in \ker(q)\) et écrivons-le sous la forme \( x=\sum_{i=1}^nx_ie_i\). Nous avons
            \begin{equation}
                0=q(x)=\sum_i| x_i |^2q(e_i).
            \end{equation}
            Mais \(    | x_i |^2\geq 0 \) et \( q(e_i)\geq 0\), donc si \( q(e_i)\neq 0\), alors \( x_i=0\). Donc les seules composantes non nulles de \( x\) sont celles sur lesquelles \( q\) s'annule. En d'autres termes \( x=\sum_ix_ie_i\in \Span\{ e_i \}_{i\in K}\).
    \end{subproof}
\end{proof}

\begin{theorem}[\cite{BIBooUULNooUtlrar,MonCerveau}]       \label{THOooIDMPooIMwkqB}
    Toute forme quadratique sur un espace vectoriel de dimension finie admet une base formée de vecteurs \( 2\) à \( 2\) orthogonaux (pour la forme considérée).
\end{theorem}

\begin{proof}
    Nous considérons la base \(  \{ l_i \}    \) de \( E^*\) donnée par la réduction de Gauss (théorème \ref{THOooOMMFooKxqICS}). La forme quadratique \( q\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
    \end{equation}
    La base préduale\footnote{Définition, existence, unicité dans la proposition \ref{PROPooDBPGooPagbEB}.} \( \{ e_i \}\) de \( \{ l_i \}\) répond aux conditions. Pour le vérifier, nous considérons la forme bilinéaire associée à \( q\) par l'identité de polarisation \ref{PROPooZLXVooOsXCcB} :
    \begin{equation}
        b(e_i,e_j)=\frac{ 1 }{2}\big( q(e_i)+q(e_j)-q(e_i-e_j) \big).
    \end{equation}
    Vu que \( l_k(e_i)=\delta_{ki}\), nous avons
    \begin{equation}
        q(e_i)=\sum_{k=1}^n\alpha_kl_k(e_i)^2=\alpha_i.
    \end{equation}
    En utilisant la linéarité,
    \begin{subequations}
        \begin{align}
            q(e_i-e_j)&=\sum_k\alpha_kl_k(e_i-e_j)^2\\
            &=\sum_k\alpha_k(\delta_{ki}-\delta_{kj})^2\\
            &=\sum_k\alpha_k(\delta_{ki}+\delta_{kj}-2\delta_{ki}\delta_{ki})\\
            &=\alpha_i+\alpha_j-2\delta_{ij}\alpha_i.
        \end{align}
    \end{subequations}
    Donc 
    \begin{equation}
        b(e_i,e_j)=\delta_{ij}\alpha_i.
    \end{equation}
    Les vecteurs \( \{ e_i \}\) sont donc bien deux à deux \( q\)-orthogonaux.
\end{proof}

Notons qu'en l'absence de notion de racine carrée sur \( \eK\), il n'est pas possible de considérer \( \sqrt{ \alpha_i }\) et donc de base \( q\)-orthonormée.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

Le théorème \ref{THOooIDMPooIMwkqB} a déjà donné une base orthogonale pour toute forme quadratique sur un espace vectoriel \( (E,\eK)\) de dimension finie. Dans le cas de \( \eR^n\), nous pouvons en donner une preuve basée sur le théorème spectral, c'est la proposition \ref{PROPooUKRUooGRIDHt}.

\begin{proposition}     \label{PROPooUKRUooGRIDHt}
    Soit une forme bilinéaire symétrique \( b\) sur un \( \eR^n\). Il existe une matrice orthogonale \( Q\) telle que 
    \begin{enumerate}
        \item
            \( D=Q^tbQ\) est diagonale
        \item
            \( D(x,y)=b(Qx,Qy)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Il existe une base \( (f_i)_{i=1,\ldots, n}\) qui est \( b\)-orthogonale.

    Dans cet énoncé, nous mélangeons sans vergogne les formes et les matrices, en supposant qu'une base soit fixée\footnote{Autrement dit, si vous avez en tête d'utiliser cette proposition pour \( \eR^n\) c'est bon; mais sinon vous devez choisir une base et considérer toutes les matrices dans cette base.}. Par exemple
    \begin{equation}
        D(x,y)=\sum_{ij}D_{ij}x_iy_j.
    \end{equation}
\end{proposition}

\begin{proof}
    Pour la matrice diagonale, c'est le théorème spectral \ref{ThoeTMXla}\ref{ITEMooMWWRooXxGONW} qui joue parce que la matrice d'une forme bilinéaire symétrique est symétrique (c'est vu de la définition \eqref{EQooCUGFooRlKUtu}).

    Pour le reste c'est un calcul :
    \begin{subequations}
        \begin{align}
            D(x,y)&=\sum_{ijkl}Q^t_{ik}b_{kl}Q_{lj}x_iy_j\\
            &=\sum_{ijkl}b_{kl}(Q_{ki}x_i)(Q_{lj}y_j)\\
            &=\sum_{kl}b_{kl}(Qx)_k(Qy)_l\\
            &=b(Qx,Qy).
        \end{align}
    \end{subequations}
    Nous avons utilisé le produit matrice fois vecteur donné par \eqref{EQooQFVTooMFfzol}.

    En ce qui concerne l'existence d'une base \( b\)-orthogonale, vu que \( D\) est diagonale, nous avons, pour \( i\neq j\) que \( D(e_i,e_j)=0\). Donc en posant \( f_i=Qe_i\), nous trouvons
    \begin{equation}
        0=D(e_i,e_j)=b(Qe_i,Qe_j)=b(f_i,f_j).
    \end{equation}
    La base \( (Qe_i)_{i=1,\ldots, n}\) est donc \( b\)-orthogonale.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométrie, forme quadratique et bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    La forme quadratique \( q(x)=x_1^2+x_2^2\) donne la norme euclidienne. La forme bilinéaire associée est \( b(x,y)=x_1y_1+x_2y_2\), qui est le produit scalaire usuel.
\end{example}

Il ne faudrait pas déduire trop vite que la formule \( \| x \|^2=q(x)\) donne une norme dès que \( q\) est non dégénérée. En effet \( q\) peut ne pas être définie positive. La forme \( q(x)=x_1^2-x_2^2\) prend des valeurs positives et négatives. A fortiori \( d(x,y)=q(x-y)\) ne donne pas toujours une distance.

\begin{definition}      \label{DEFooECTUooRxBhHf}
    Une \defe{isométrie}{isométrie!de forme quadratique} pour la forme quadratique \( q\) est une application bijective \( f\colon V\to V\) telle que 
    \begin{equation}
     q(x-y)=q\big( f(x)-f(y) \big).
    \end{equation}
     Dans les cas où \( q\) donne une distance, alors c'est une isométrie au sens usuel.
\end{definition}

\begin{definition}[Thème \ref{THMooVUCLooCrdbxm}]      \label{DEFooIQURooMeQuqX}
    Soit un espace vectoriel \( E\) muni d'une forme bilinéaire \( b\). Une \defe{isométrie}{isométrie (forme bilinéaire)} pour \( b\) est une bijection \( f\colon E\to E\) telle que
    \begin{equation}
        b\big( f(x),f(y) \big)=b(x,y)
    \end{equation}
    pour tout \( x,y\in E\).
\end{definition}

\begin{lemma}   \label{LemewGJmM}
    Soient \( q\) une forme quadratique et \( b\) la forme bilinéaire associée par le lemme~\ref{LEMooLKNTooSfLSHt}. Une application \( f\colon E\to E\) telle que \( f(0)=0\) est une isométrie pour \( b\) si et seulement si elle est une isométrie pour \( q\).
\end{lemma}

\begin{proof}
    Pour une application bijective \( f\colon E\to E\) telle que \( f(0)=0\), nous devons prouver l'équivalence des propriétés suivantes :
    \begin{enumerate}
        \item
            \( b\big( f(x),f(y) \big)=b(x,y)\) pour tout \( x,y\in E\);
        \item
            \( q\big( f(x)-f(y) \big)=q(x-y)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Dans le sens direct, en posant \( x=y\) nous trouvons tout de suite \( q(f(x))=q(x)\); ensuite en utilisant la distributivité de \( b\),
    \begin{subequations}
        \begin{align}
            q\big( f(x)-f(y) \big)&=b\big( f(x)-f(y),f(x)-f(y) \big)\\
            &=q\big( f(x) \big)-2b\big( f(x),f(y) \big)+q\big( f(y) \big)\\
            &=q(x)+q(y)-2b(x,y)\\
            &=q(x-y).
        \end{align}
    \end{subequations}

    Dans l'autre sens, nous commençons par remarquer que l'hypothèse \( f(0)=0\) donne \( q(x)=q\big( f(x) \big)\). Ensuite nous utilisons l'identité de polarisation \eqref{EqMrbsop} :
    \begin{subequations}
        \begin{align}
            b\big( f(x),f(y) \big)&=\frac{ 1 }{2}\big[ q\big( f(x) \big)+q\big( f(y) \big)-q\big( f(x-y) \big) \big]\\
            &=\frac{ 1 }{2}\big[ q(x)+q(y)-q(x-y) \big]\\
            &=b(x,y).
        \end{align}
    \end{subequations}
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Signature, théorème de Sylvester}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Signature\cite{BIBooXOWGooAPWTfT}]       \label{DEFooWDCLooDkRYLK}
    Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( Q\) sur un espace vectoriel \( E\) de dimension finie \( n\). L'\defe{indice d'inertie}{indice d'inertie} de \( q\) est le nombre
    \begin{equation}
        q=\max\{ \dim(F)\tq Q(v)<0\,\forall v\in F\setminus\{ 0 \} \}.
    \end{equation}
    Nous définissons aussi
    \begin{equation}
        p=\max\{ \dim(G)\tq Q(v)>0\,\forall v\in G\setminus\{ 0 \} \}.
    \end{equation}
    Le couple \( (p,q)\) est la \defe{signature}{signature!forme quadratique} de \( Q\).
\end{definition}

\begin{definition}[Rang d'une forme quadratique]        \label{DEFooVITQooQaMaTF}
    Si \( Q\colon E\times E\to \eK\) est une forme quadratique, nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f_Q\colon E&\to E^* \\
            x&\mapsto \big[ y\mapsto B(x,y) \big]. 
        \end{aligned}
    \end{equation}
    Le \defe{rang}{rang d'une forme quadratique} de \( Q\) est le rang de l'application linéaire \( f_Q\).
\end{definition}

\begin{proposition}     \label{PROPooLRZQooSfprff}
    Le rang d'une forme quadratique est le rang de sa matrice dans n'importe quelle base.
\end{proposition}

\begin{proof}
    Nous considérons une forme quadratique \( Q\) sur l'espace vectoriel \( E\). Sa trace est, par définition, la trace de l'application linéaire \( f_Q\) de la définition \ref{DEFooVITQooQaMaTF}. Or cette dernière trace ne dépend pas des bases choisies sur \( E\) et \( E^*\). Nous la calculons donc maintenant.

    Soit une base \( \{ e_i \}\) de \( E\) ainsi que sa base duale \( \{ e_i^* \}\) de \( E^*\). Si \( v=\sum_kv_ke_k\in E\), alors
    \begin{equation}
        f_Q(e_i)v=\sum_kv_kB(e_i,e_k)=\sum_kQ_{ik}v_k
    \end{equation}
    où nous avons noté \( B\) la forme bilinéaire associée à \( Q\) et utilisé la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à la forme quadratique \( Q\). Nous avons donc \( f_Q(ei)=\sum_kQ_{ik}e_k^*\) ou encore
    \begin{equation}
        f_Q(e_i)_k=Q_{ik},
    \end{equation}
    ce qui signifie, par \ref{ITEMooKZYYooZPTkpq} que la matrice associée à \( f_Q\) est la matrice \( Q^t\).

    Le rang de \( f_Q\) est donc celui de \( Q^t\), qui est le même que celui de la matrice \( Q\) (ici, nous avons noté \( Q\) la matrice de la forme quadratique \( Q\)). Le rang de \( f_Q\) est celui de sa matrice par la proposition \ref{PROPooCINLooFGNtwS}.
\end{proof}


\begin{lemma}[\cite{BIBooXOWGooAPWTfT}]     \label{LEMooISHCooVDJEKo}
    Soient une forme quadratique \( Q\) ainsi que deux bases \( Q\)-orthogonales \( \{ e_1,\ldots, e_n \}\) et \( \{ e'_1,\ldots, e'_n \}\). Nous posons
    \begin{subequations}
        \begin{align}
            r&=\Card\{ e_i\tq q(e_i)>0 \}\\
            r'&=\Card\{ e_i\tq q(e'_i)>0 \}\\
            s&=\Card\{ e_i\tq q(e_i)<0 \}\\
            s'&=\Card\{ e_i\tq q(e'_i)<0 \}
        \end{align}
    \end{subequations}
    Alors \( r=r'\) et \( s=s'\).
\end{lemma}

\begin{proof}
    Nous posons    
    \begin{subequations}
        \begin{align}
            I&=\{ i\tq Q(e_i)>0 \}\\
            J&=\{ j\tq Q(e_j)<0 \}
        \end{align}
    \end{subequations}
    Nous commençons par prouver que \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre. Supposons pour cela que
    \begin{equation}
        \sum_{i\in I}x_ie_i+\sum_{j\in J}y_je'_j=0,
    \end{equation}
    et posons \( z=\sum_{i\in I}x_ie_i\). Nous avons
    \begin{equation}        \label{EQooWGKAooElpETd}
        Q(z)=\sum_{i\in I}x_i^2Q(e_i)\geq 0.
    \end{equation}
    Mais nous avons aussi \( z=-\sum_{j\in J}y_je'_j\), donc
    \begin{equation}        \label{EQooJYOCooZPXmTf}
        Q(z)=\sum_{j\in J}y_j^2Q(e'_j)\leq 0.
    \end{equation}
    Donc \( Q(z)=0\). Vu \eqref{EQooWGKAooElpETd}, et le fait que \( Q(e_i)>0\), avoir \( Q(z)=0\) impose \( x_i=0\) pour tout \( i\). La relation \eqref{EQooJYOCooZPXmTf} nous donne aussi immédiatement que les \( y_j\) sont nuls. Donc la partie \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre.

    Le lemme \ref{LemytHnlD} nous indique qu'une partie libre est toujours de cardinal plus petit ou égal à la dimension de l'espace\footnote{Ici nous utilisons l'hypothèse que \( V\) est de dimension finie.}. Tout ça pour dire que
    \begin{equation}
        \underbrace{\Card(I)}_{=r}+\underbrace{\Card(J)}_{=n-r'}\leq n,
    \end{equation}
    et donc \( r\leq r'\). 

    Faisant le même raisonnement en partant de \( I=\{ i\tq Q(e_i)\leq 0 \}\) et \( J=\{ j\tq Q(e'_j)>0 \}\), nous trouvons \( r'\leq r\).

    La preuve de \( s=s'\) est du même tonneau.
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooOQIDooPSOeXL}
    Soit une forme quadratique \( Q\) sur \( E\). Si \( F\) est un sous-espace de \( E\), alors
    \begin{equation}
        \dim(F)+\dim(F^{\perp})\geq n
    \end{equation}
    où \( F^{\perp}\) est l'orthogonal par rapport à \( Q\).
\end{lemma}

\begin{proof}
    Nous posons \( p=\dim(F)\). Nous considérons une base \( \{ f_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \{ f_i \}_{i=1,\ldots, p}\) est une base de \( F\)\footnote{Théorème de la base incomplète, \ref{THOooOQLQooHqEeDK}.}. Nous posons
    \begin{equation}
        \begin{aligned}
            \phi\colon E&\to F \\
            x&\mapsto \sum_{i=1}^pB(x,f_i)f_i 
        \end{aligned}
    \end{equation}
    où \( B\) est la forme bilinéaire associée à \( Q\). Ce \( \phi\) est une application linéaire à qui nous appliquons le théorème du rang \eqref{EQooUEOQooLySRiE} :
    \begin{equation}        \label{EQooCLWLooCFxVDq}
        \dim(E)=\rang(\phi)+\dim\big( \ker(\phi) \big).
    \end{equation}
    Mais vu que l'image de \( \phi\) est dans \( F\), nous avons \( \rang(\phi)\leq \dim(F)\). De plus, \( \ker(\phi)=F^{\perp}\). Donc \eqref{EQooCLWLooCFxVDq} devient
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooUOZOooYvEcji}
    Soit un espace vectoriel \( E\) de dimension finie et un sous-espace \( F\) sur lequel la forme quadratique \( Q\) est strictement définie positive ou négative. Alors
    \begin{equation}
        E=F\oplus F^{\perp}.
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord nous montrons que \( F\cap F^{\perp}=\{ 0 \}\). Si \( v\neq 0\) est dans \( F\), alors \( Q(v)>0\), et donc \( v\) n'est pas dans \( F^{\perp}\). Donc \( F\cap F^{\perp}\subset \{ 0 \}\). L'inclusion inverse est immédiate.

    Nous avons vu dans le lemme \ref{LEMooOQIDooPSOeXL} que
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
    Vu que \( F\) et \( F^{\perp}\) n'ont pas d'intersection autre que \( \{ 0 \}\), nous avons
    \begin{equation}
        \dim(E)\geq\dim(F\oplus F^{\perp}) = \dim(F)+\dim(F^{\perp}) \geq\dim(E).
    \end{equation}
    Toutes ces inégalités sont donc des égalités et \( \dim(E)=\dim(F)+\dim(F^{\perp})\).
\end{proof}

\begin{theorem}[de Sylvester\cite{BIBooXOWGooAPWTfT}]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature\footnote{Définition \ref{DEFooWDCLooDkRYLK}.} \( (p,q)\). Alors pour toute base \( Q\)-orthogonale \( \{ e_i \}\) de \( \eR^{p+q}\) nous avons les propriétés suivantes.
    \begin{enumerate}
        \item       \label{ITEMooCFQHooRWfmpT}
            Les nombres \( p\) et \( q\) sont donnée par 
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}             \label{SUBEQooONWLooNsgmQY}   \\
            q&=\Card\{ i\tq Q(e_i)<0 \}.        \label{SUBEQooFKXMooOVwvKR}
        \end{align}
    \end{subequations}
\item       \label{ITEMooWLPVooSTOOjL}
    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\item       \label{ITEMooGOHCooPrNQwm}
    Le rang de \( Q\) est \( p+q\).
    \end{enumerate}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}

\begin{proof}
    Soit  $F$ un sous-espace de dimension maximale $q$ sur lequel $Q$ est définie négative. Le fait que la dimension de \( F\) soit \( q\) est la définition \ref{DEFooWDCLooDkRYLK} de la signature. Nous notons \( F^{\perp}\) sont \( Q\)-orthogonal, c'est à dire que
    \begin{equation}
        F^{\perp}=\{ v\in E\tq B(v,x)=0\,\forall x\in F \}.
    \end{equation}
    Le lemme \ref{LEMooUOZOooYvEcji} nous assure que \( E=F\oplus F^{\perp}\).

    Le théorème \ref{THOooIDMPooIMwkqB} sur l'existence de bases \( Q\)-orthogonales nous permet de considérer une base \( Q\)-orthogonale de \( F\) et une de \( F^{\perp}\). En réunissant les deux, nous avons une base de \( E\). Nous la notons \( \{ f_1,\ldots, f_n \}\) avec
    \begin{itemize}
        \item La partie \( \{ f_1,\ldots, f_q \}\) est une base de \( F\),
        \item La partie \( \{ f_{q+1},\ldots, f_n \}\) est une base de \( F^{\perp}\),
        \item Remarquez cependant qu'il n'est pas dit que \( n=q+p\).
    \end{itemize}
    Notons que pour \( i>q\), nous avons \( Q(f_i)\geq 0\), sinon la maximalité de \( F\) serait contredite par \( \Span\{ f_1,\ldots, f_q,f_i \}\).

    Cela prouve que 
    \begin{equation}
        \Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    Le lemme \ref{LEMooISHCooVDJEKo} nous dit alors que
    \begin{equation}
        \Card\{ i\tq Q(e_i)>0 \}=\Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    C'est l'égalité \eqref{SUBEQooFKXMooOVwvKR}. L'égalité \eqref{SUBEQooONWLooNsgmQY} se prouve de la même façon, en prenant \( F\) maximal pour la propriété que \( Q\) y est strictement définie positive.

    Le point \ref{ITEMooCFQHooRWfmpT} est prouvé.

    Dans une base \( Q\)-orthogonale, la matrice de \( Q\) est diagonale, et contient sur la diagonale les valeurs de \( Q(e_i)\). Parmi celles-ci, on en a \( p\) strictement positives et \( q\) strictement négatives. Les \( n-p-q\) autres sont nulles. Vu que \( Q\) est à valeur réelle, nous avons une notion de racine carré, et nous pouvons considérer \( e_i/\sqrt{ | Q(e_i) | }\) au lieu de \( e_i\). De cette façon, \( Q(e_i)\) est normalisé. Avec ça, la matrice de \( Q\) est
    \begin{equation}        \label{EQooLQNRooCsgKVF}
        D=\begin{pmatrix}
            \mtu_p    &       &       \\
                &   -\mtu_q    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
    Nous venons de prouver qu'il existe une base \( \{ e_i \}\) dans laquelle la matrice de \( Q\) est \eqref{EQooLQNRooCsgKVF}. Si \( A\) est la matrice de \( Q\) dans une base quelconque \( \{ f_i \}\) et si \( P\) est la matrice de changement de base \( f_j=\sum_iP_{ij}e_i\), la proposition \ref{PROPooLBIOooUpzxXA} donne \(D= P^tAP\).

    Le point \ref{ITEMooWLPVooSTOOjL} est prouvé.

    Pour \ref{ITEMooGOHCooPrNQwm}, la proposition \ref{PROPooLRZQooSfprff} nous permet de calculer le rang de \( Q\) par le rang de sa matrice dans n'importe quelle base. Nous choisissons la base qui donne la matrice \eqref{EQooLQNRooCsgKVF}. Le rang est alors bien \( p+q\).
\end{proof}


\begin{definition}[Équivalence de forme quadratique\cite{BIBooWVWZooZqliJt}]        \label{DEFooOLWYooMwhMJp}
    Deux formes quadratiques $Q$ et $Q'$ sont \defe{équivalentes}{équivalence de forme quadratiques} si il existe une application linéaire inversible \( \phi\) telle que \( Q'=Q\circ \phi\).
\end{definition}

\begin{proposition}[\cite{BIBooXOWGooAPWTfT}]       \label{PROPooBWXMooLsgyKm}
    Deux formes quadratiques sont équivalentes\footnote{Définition \ref{DEFooOLWYooMwhMJp}.} si et seulement si elles ont même signature.
\end{proposition}

\index{matrice!semblables}
\index{forme!quadratique}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Invariance de la trace}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRMYQooWkEpJJ}
    Soit une application linéaire \( f\). Si la matrice de \( f\) dans une base est \( A\) et est \( B\) dans une autre base, alors
    \begin{equation}
        \trace(A)=\trace(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Les matrices \( A\) et \( B\) sont liées par la proposition \ref{PROPooNZBEooWyCXTw} : \( B=Q^{-1}AQ\) où \( Q\) est la matrice qui lie les vecteurs des deux bases. L'invariance cyclique de la trace donnée en le lemme \ref{LEMooUXDRooWZbMVN} implique que
    \begin{equation}
        \trace(B)=\trace(Q^{-1}AQ)=\trace(QQ^{-1}A)=\trace(A).
    \end{equation}
\end{proof}


