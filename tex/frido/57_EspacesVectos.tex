% This is part of Mes notes de mathématique
% Copyright (c) 2008-2022
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Valeur propre et vecteur propre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Nous savons qu'une application \emph{linéaire} $A\colon \eR^3\to \eR^3$ est complètement définie par la donnée de son action sur les trois vecteurs de base, c'est-à-dire par la donnée de
\begin{equation}
	\begin{aligned}[]
		Ae_1,&&Ae_2&&\text{et}&&Ae_3.
	\end{aligned}
\end{equation}
La matrice d'une application $A$ se forme en mettant simplement les vecteurs $Ae_1$, $Ae_2$ et $Ae_3$ en colonne. Donc la matrice
\begin{equation}		\label{EqExempleALin}
	A=\begin{pmatrix}
		3	&	0	&	0	\\
		0	&	0	&	1	\\
		0	&	1	&	0
	\end{pmatrix}
\end{equation}
signifie que l'application linéaire $A$ envoie le vecteur $e_1$ sur $\begin{pmatrix}
	3	\\
	0	\\
	0
\end{pmatrix}$, le vecteur $e_2$ sur $\begin{pmatrix}
	0	\\
	0	\\
	1
\end{pmatrix}$ et le vecteur $e_3$ sur $\begin{pmatrix}
	0	\\
	1	\\
	0
\end{pmatrix}$.
Pour savoir comment $A$ agit sur n'importe quel vecteur, on applique la règle de produit vecteur$\times$matrice :
\begin{equation}
	\begin{pmatrix}
		1	&	2	&	3	\\
		4	&	5	&	6	\\
		7	&	8	&	9
	\end{pmatrix}\begin{pmatrix}
		x	\\
		y	\\
		z
	\end{pmatrix}=
	\begin{pmatrix}
		 x+2y+3z	\\
		4x+5y+6z	\\
		7x+8y+9z
	\end{pmatrix}.
\end{equation}

Une chose intéressante est de savoir quelles sont les directions invariantes de la transformation linéaire. Par exemple, on peut lire sur la matrice \eqref{EqExempleALin} que la direction $\begin{pmatrix}
	1	\\
	0	\\
	0
\end{pmatrix}$ est invariante : elle est simplement multipliée par $3$. Dans cette direction, la transformation est juste une dilatation. Afin de savoir si $v$ est un vecteur d'une direction conservée, il faut voir si il existe un nombre $\lambda$ tel que $Av=\lambda v$, c'est-à-dire voir si $v$ est simplement dilaté.

L'équation $Av=\lambda v$ se récrit $(A-\lambda\mtu)v=0$, c'est-à-dire qu'il faut résoudre l'équation
\begin{equation}
	(A-\lambda\mtu)\begin{pmatrix}
		x	\\
		y	\\
		z
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\
		0	\\
		0
	\end{pmatrix}.
\end{equation}
Nous savons qu'une telle équation ne peut avoir de solutions que si $\det(A-\lambda\mtu)=0$. La première étape est donc de trouver les $\lambda$ qui vérifient cette condition.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dans le vif du sujet}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooMMKZooVcskCc}
    Soit un \( \eK\)-espace vectoriel \( E\) et un endomorphisme \( T\colon E\to E\). Un \defe{vecteur propre}{vecteur!propre} de \( T\) est un vecteur \( v \neq 0\) tel que \( T(v)=\lambda v\) pour un certain \( \lambda\in \eK\). Dans ce cas, \( \lambda\) est la \defe{valeur propre}{valeur!propre} de \( v\).

    L'\defe{espace propre}{espace!propre} de \( T\) pour la valeur \( \lambda\)\footnote{Nous laissons au lecteur le soin de vérifier que c'est bien un sous-espace vectoriel de \( E\).} est l'ensemble des vecteurs propres de \( T\) pour la valeur propre \( \lambda\), et le vecteur nul.
\end{definition}
%TODOooBLVVooJMZCRd: prouver que c'est un sous-espace vectoriel.

\begin{definition}
    L'ensemble des valeurs propres de l'endomorphisme \( T\) est son \defe{spectre}{spectre d'un endomorphisme} et est noté \( \Spec(T)\).
\end{definition}

\begin{remark}
    Le nombre zéro peut être une valeur propre; c'est le vecteur zéro qui ne peut pas être vecteur propre. La matrice nulle est une matrice diagonalisable.
\end{remark}

\begin{lemma}       \label{LemjcztYH}
    Soient un espace vectoriel \( E\), un endomorphisme \( T\in \End(E)\), ainsi que ses sous-espaces propres \( \{ E_{\lambda} \}_{\lambda\in \Spec(T)}  \)\nomenclature[A]{\( E_{\lambda}(T)\)}{Espace propre de \( T\)}. Toute somme finie de la forme
    \begin{equation}
        E_{\lambda_1}+\ldots+E_{\lambda_p}
    \end{equation}
    est directe\footnote{Définition \ref{DEFooIJDNooRUDUYF}.}.
\end{lemma}

\begin{proof}
    Nous utilisons le lemme \ref{LEMooDQMQooInVVDY}. Soient \( v_i\in E_{\lambda_i}\) un choix de vecteurs tels que 
    \begin{equation}        \label{EQooROAXooFpgxxF}
        \sum_{i=1}^pv_i=0. 
    \end{equation}
    Soit un entier \( j_0\) entre \( 1\) et \( p\). Nous allons montrer que \( v_{j_0}=0\). Pour cela nous remarquons d'abord que, pour tout \( i\neq j_0\),
    \begin{equation}        \label{EQooGVPYooXRPEVU}
        \prod_{k\neq j_0}(\lambda_i-\lambda_k)=0.
    \end{equation}
    Nous appliquons l'opérateur \( \prod_{k\neq j_0}(T-\lambda_k\mtu)\) à l'égalité \eqref{EQooROAXooFpgxxF} :
    \begin{subequations}
        \begin{align}
            0 & = \sum_{i=1}^p\prod_{k\neq j_0}(T-\lambda_k)v_i               \\
              & = \sum_{i=1}^p\prod_{k\neq j_0}(\lambda_i-\lambda_k)v_i       \label{SUBEQooHHGJooTvCcDb}\\
              & = \prod_{k\neq j_0}(\lambda_{j_0}-\lambda_k)v_{j_0}.                  \label{SUBEQooSHQYooNLVoVZ}
        \end{align}
    \end{subequations}
    Justifications.
    \begin{itemize}
        \item Pour \eqref{SUBEQooHHGJooTvCcDb}. Pour chaque \( k\) et \( i\) nous avons \( (T-\lambda_k)v_i=Tv_i-\lambda_kv_i=\lambda_iv_i-\lambda_kv_i\) parce que \( v_i\) est un vecteur propre de \( T\) pour la valeur propre \( \lambda_i\).
        \item Pour \eqref{SUBEQooSHQYooNLVoVZ}. Dans la somme, seul le terme \( i=j_0\) est non nul, à cause de \eqref{EQooGVPYooXRPEVU}.
    \end{itemize}
    Donc \( v_{j_0}=0\) parce que le produit \( \prod_{k\neq j_0}(\lambda_{j_0}-\lambda_k)\), lui, est non nul.
\end{proof}

\begin{probleme}
    L'énoncé de la proposition \ref{PropTVKbxU} me semble douteux, et la référence bibliographique est un lien mort.

    Si vous savez un énoncé exact avec une preuve, écrivez-moi.
\end{probleme}


\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynômes d'endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooUEQVooLBrRiE}

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\in\End(E)\) où \( E\) est un \( \eK\)-espace vectoriel. Nous considérons l'application
\begin{equation}    \label{EqOVKooeMJuv}
    \begin{aligned}
        \varphi_u\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(u).
    \end{aligned}
\end{equation}
L'image de \( \varphi_u\) est un sous-espace vectoriel. En effet si \( A=\varphi_u(P)\) et \( B=\varphi_u(Q)\), alors \( A+B=\varphi_u(P+Q)\) et \( \lambda A=(\lambda P)(u)\). En particulier c'est un espace fermé.

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant qu'endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux\cite{BIBooLUNZooKoIbjD}]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace\footnote{Le corps \( \eK\) est commutatif comme tous les corps dans le Frido.} vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers\footnote{Définition~\ref{DefDSFooZVbNAX}.}. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}
\index{lemme!des noyaux}

\begin{proof}
    Dans ce qui suit, nous allons beaucoup utiliser le fait que \( \eK[X]\) soit commutatif (lemme \ref{LEMooWVUXooQlaepO}). Nous posons
    \begin{equation}
        Q_i=\prod_{j\neq i}P_j.
    \end{equation}
    \begin{subproof}
    \item[Utilisation de Bézout]
        
    Par le lemme~\ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bézout (théorème~\ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}        \label{EQooMMCVooRzlXpA}
        R_1Q_1+\cdots+R_nQ_n=1.
    \end{equation}
\item[Une première somme, pas directe]
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(R_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. 
\item[\( Q_iQ_j\) est multiple de \( P\)]
    Si \( i\neq j\), en utilisant la commutativité de \( \eK[X]\),
    \begin{equation}
        Q_iQ_j=\left(\prod_{k\neq i}P_k\right)\left(\prod_{l\neq j}P_l\right)=\Big( \prod_{\substack{k\neq i\\k\neq j}}P_k \Big)P_j\left( \prod_{l\neq j}P_l \right)=\prod_{\substack{k\neq i\\k\neq j}}P_k\prod_kP_l=S_{ij}P,
    \end{equation}
    où \( S_{ij}\) est un polynôme. Nous voyons que \( Q_iQ_j\) est multiple de \( P\).
\item[Une somme directe]
    Toujours avec \( i\neq j\), en utilisant le lemme \ref{LemQWvhYb}, 
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=\big( R_iR_j\underbrace{Q_iQ_j}_{=S_{ij}P} \big)(u)=(R_iR_jS_{ij})(u)\circ P(u)=0
    \end{equation}
    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème~\ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}        \label{EQooJPQLooOZepwZ}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}
\item[\( R_iQ_i(u)E\subset \ker P_i(u)\)]
    Attention : utilisation massive du lemme \ref{LemQWvhYb}. Un élément de \( R_iQ_i(u)E\) est de la forme \( (R_iQ_i)(u)x\) avec \( x\in E\). Nous appliquons l'endomorphisme \( P_i(u)\) à cet élément, et nous vérifions que nous obtenons zéro :
    \begin{subequations}
        \begin{align}
            P_i(u)\big( (R_iQ_i)(u)x \big)&=(P_iR_iQ_i)(u)x\\
            &=(R_i\underbrace{P_iQ_i}_{=P})(u)x\\
            &=(R_iP)(u)x\\
            &=\big( R_i(u)\circ \underbrace{P(u)}_{=0}\big)x\\
            &=0.
        \end{align}
    \end{subequations}
    Par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). 


\item[Et la somme qu'il nous fallait]
    Le fait que la somme \eqref{EQooJPQLooOZepwZ} soit directe n'est en fait pas crucial. En effet, vu que chacun des termes est inclus à \( \ker P_i(u)\), nous avons la somme (pas directe à priori)
    \begin{equation}
        E=\sum_{i=1}^nR_iQ_i(u)E\subset\sum_{i=1}^n\ker P_i(u).
    \end{equation}
    Mais cette fois, nous prouvons qu'elle est directe en utilisant la caractérisation du lemme \ref{LEMooDQMQooInVVDY}\ref{ITEMooPLXGooCOQgen}. Supposons que, pour un certain \( k\),
    \begin{equation}
        x\in\ker P_k(u)\cap\big( \sum_{j\neq k}\ker P_j(u) \big).
    \end{equation}
    Nous allons montrer que \( x=0\).
    \begin{subproof}
    \item[\( Q_i(u)x=0\) si \( i\neq k\)]
        Si \( i\neq k\), nous avons
        \begin{equation}
            Q_i(u)x=\Big( \prod_{\substack{j\neq i\\j\neq k}}P_j \Big)P_k(u)x=0
        \end{equation}
        parce que \( x\in\ker P_k(u)\).
    \item[\( Q_k(u)x=0\)]
        Nous savons qu'il existe \( z_l\in\ker P_l(u)\) tel que \( x=\sum_{l\neq k}z_l\). Nous avons alors
        \begin{equation}
            Q_k(u)x=\Big( \prod_{j\neq k}P_j \Big)\sum_{l\neq k}z_l=\sum_{l\neq k}\Big( \prod_{j\neq k}P_j(u) \Big)z_l=0
        \end{equation}
        parce que parmi les \( P_j(u)\) (\( j\neq k\)), il y a \( P_l(u)\) qui annule \( z_l\).
    \item[Et finalement]
        Nous avons prouvé que \( Q_i(u)x=0\) pour tout \( i\). La formule de Bézout \eqref{EQooMMCVooRzlXpA} donne alors
        \begin{equation}
            \sum_iR_i\subset{Q_i(u)x}_{=0}=x
        \end{equation}
        et donc \( x=0\).
    \end{subproof}
\item[Les projecteurs]
    \end{subproof}
\end{proof}

\begin{normaltext}
    Ce résultat est utilisé pour prouver que toute représentation est décomposable en représentations irréductibles, proposition~\ref{PropHeyoAN} ainsi que pour le théorème~\ref{ThoDigLEQEXR} qui dit que si le polynôme minimal d'un endomorphisme est scindé à racine simple alors il est diagonalisable.
\end{normaltext}

\begin{corollary}   \label{CorKiSCkC}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme minimal \( \mu_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \mu_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \mu_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r.
    \end{equation}
    Nous pouvons décomposer \( x\in F\) en termes de cette somme :
    \begin{equation}     \label{EqbBbrdi}
        x=x_1+\cdots +x_r
    \end{equation}
    avec \( x_i\in E_i\). Toujours selon le lemme des noyaux, les projections sur les espaces \( E_i\) sont des polynômes en \( f\). Par conséquent \( F\) est stable sous toutes ces projections \( \pr_i\colon E\to E_i\), et en appliquant \( \pr_i\) à \eqref{EqbBbrdi}, \( \pr_i(x)=x_i\). Puisque \( x\in F\), le membre de gauche est encore dans \( F\) et \( x_i\in E_i\cap F\). Nous avons donc
    \begin{equation}
        F\subset\bigoplus_{i=1}^rF_i.
    \end{equation}
    L'inclusion inverse est immédiate parce que \( F_i\subset F\) pour chaque \( i\).
\end{proof}

\begin{lemma}   \label{LemVISooHxMdbr}
    Si \( x\) est un vecteur propre de valeur propre \( \lambda\) pour l'endomorphisme \( u\) et si \( P\) est un polynôme, alors \( x\) est vecteur propre de \( P(u)\) pour la valeur propre \( P(\lambda)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul de \( P(u)x\) en ayant noté\footnote{En complète violation de ce qu'on disait dans \ref{NORMooHHIVooSfHlxv}.} \( P(X)=\sum_{k=0}^nc_kX^n\) :
    \begin{equation}
        P(u)x=\sum_{k=0}^nc_ku^k(x)=\sum_{k=0}^nc_k\lambda^kx=P(\lambda)x.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme minimal et minimal ponctuel}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons déjà vu la définition de polynôme minimal en \ref{DefCVMooFGSAgL}. Le lemme suivant permet de parler de polynôme minimal d'endomorphisme.

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooEYPSooLCoPlY}
    Si \( E\) est un \( \eK\)-espace vectoriel, l'ensemble \( \End(E)\) des endomorphismes de \( E\) est une extension du corps \( \eK\).
\end{lemma}

\begin{lemma}        \label{LEMooQJQGooRcAxmJ}
    Soit un endomorphisme \( f\colon E\to E\) d'un \( \eK\)-espace vectoriel de dimension finie. Il existe un unique polynôme annulateur unitaire de degré minimum\footnote{Degré minimum au sens où il existe peut-être d'autres polynômes annulateurs, mais ils seront de degré plus élevé.}.

    Tout endomorphisme de \( \eK\)-espace vectoriel de dimension finie possède un polynôme minimal\footnote{Définition \ref{DefCVMooFGSAgL}.}.
\end{lemma}

\begin{proof}
    Pour l'unicité, soient \( P\) et \( Q\) deux polynômes annulateurs de \( f\) de même degré minimum \( N\) et ayant tous deux \( 1\) comme coefficient de \( x^N\). Alors \( P-Q\) est de degré \( N-1\) tout en étant encore annulateur. Vu que nous avions dit que \( N\) était le degré minimum, le seul polynôme annulateur de degré \( N-1\) est le polynôme nul. Donc \( P-Q=0\).

    Pour l'existence, les endomorphismes \( \id\), \( f\), \( f^2\), \ldots ne peuvent pas être tous linéairement indépendants parce que la dimension de \( \End(E)\) est finie. Il existe donc un nombre \( N\) et des coefficients \( a_k\) tels que \( \sum_{k=0}^Na_kf^k=0\). Le polynôme \( P(X)=\sum_{k=0}^Na_kX^k\) est donc annulateur de \( f\).

    Une autre façon de le dire est que l'application linéaire \( \varphi\colon \eK[X]\to \End(E)\) donnée par \( \varphi(P)=P(f)\) est un endomorphisme d'un espace vectoriel de dimension infinie vers un espace vectoriel de dimension finie. Il ne peut donc pas être injectif et possède donc un noyau non réduit à zéro.

    L'existence d'un polynôme minimal est maintenant seulement dû au fait que, avec les notations de la définition \ref{DefCVMooFGSAgL}, l'idéal \( I_f\) n'est pas réduit à \( \{ 0 \}\).
\end{proof}

\begin{remark}
    La preuve donnée ci-dessus montre que \( \deg(\mu)\leq \dim(E)^2\). Comme conséquence du théorème de Cayley-Hamilton~\ref{ThoCalYWLbJQ} nous verrons qu'en réalité le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{remark}

\begin{proposition}[Exemple en dimension infinie\cite{MonCerveau}]        \label{PROPooZCUSooLUUrxi}
    L'endomorphisme de dérivation sur l'espace des fonctions dérivables \( \eR\to \eR\) n'a pas de polynôme minimal.
\end{proposition}

Dans la suite, l'endomorphisme \( f\) du \( \eK\)-espace vectoriel \( E\) de dimension \( n\) est fixé. Pour \( x\in E\) nous notons
\begin{equation}            \label{EqooOAYDooEpZELo}
    E_x=\{ P(f)x\tq P\in \eK[X] \}.
\end{equation}
Nous considérons le morphisme d'algèbres
\begin{equation}
    \begin{aligned}
        \varphi\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(f)
    \end{aligned}
\end{equation}
et si \( x\in E\) est donné nous considérons le morphisme de \( \eK\)-espaces vectoriels
\begin{equation}
    \begin{aligned}
        \varphi_x\colon \eK[X]&\to E \\
        P&\mapsto P(f)x.
    \end{aligned}
\end{equation}
Les noyaux de ces applications sont des idéaux, entre autres par le lemme~\ref{LemQWvhYb}. Ils ont donc un unique générateur unitaire (chacun) par le théorème~\ref{ThoCCHkoU}. En termes de vocabulaire, l'ensemble
\begin{equation}
    \ker(\varphi)=\{  P\in\eK[X]\tq P(f)=0  \}
\end{equation}
est l'\defe{idéal annulateur}{polynôme!annulateur} de \( f\) et un polynôme \( P\) tel que \( P(f)=0\) est un polynôme annulateur de \( f\).

\begin{propositionDef}      \label{DEFooUICRooBGYhqQ}
    La partie \( \ker(\varphi_x)\) est un idéal de \( \eK[X]\) qui possède un unique générateur unitaire.

    Le générateur unitaire de \( \ker(\varphi_x)\) est le \defe{polynôme minimal ponctuel}{polynôme!minimal!ponctuel} de \( f\) en \( x\). Il sera noté \( \mu_{f,x}\) ou \( \mu_x\) lorsque la dépendance en \( f\) est claire dans le contexte.
\end{propositionDef}
Nous notons \( \mu\) le générateur unitaire du noyau de \( \varphi\) et \( \mu_x\) celui de \( \varphi_x\). Puisque \( \mu\in\ker(\varphi_x)\) pour tout \( x\) nous avons \( \mu_x\divides \mu\) pour tout \( x\).

\begin{example}[Pas en dimension infinie]       \label{ExooDTUJooIMqSKn}
    En dimension infinie, il n'y a pas toujours de polynôme annulateur. Si \( E\) est un espace vectoriel de dimension infinie ayant une base dénombrable \( \{ e_i \}_{i\in \eN}\) alors l'opérateur donné par \( f(e_i)=e_{i+1}\) n'a pas de polynôme annulateur. Même pas ponctuel en quel que point que ce soit.

    De même l'opérateur donné par \( g(e_1)=0\) et \( g(e_i)=e_{i-1}\) si \( i\neq 1\) n'a pas de polynôme annulateur, mais il a un polynôme annulateur ponctuel évident en \( x=e_1\). L'exemple~\ref{ExooLRHCooMYLQTU} donnera un habillage à peine subtil à cet exemple.
\end{example}

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(f)=0\), alors le polynôme minimal \( \mu_f\) divise \( P\). Autrement dit, le polynôme minimal engendre l'idéal des polynômes annulateurs.
\end{proposition}

\begin{proof}
    L'ensemble \( \ker(\varphi)=\{ Q\in \eK[X]\tq Q(f)=0 \} \) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( f\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_f)\) par le théorème~\ref{ThoCCHkoU}. Nous concluons que \( \mu_f\) divise tous les éléments de \( I\).
\end{proof}

La proposition suivante permet de caractériser le polynôme minimal.
\begin{proposition}[\cite{ooEPEFooQiPESf}]      \label{PROPooVUJPooMzxzjE}
    Soit une application linéaire \( f\) sur un \( \eK\)-espace vectoriel. Il existe un unique polynôme unitaire\quext{À mon avis, «unitaire» manque dans \cite{ooEPEFooQiPESf}.} \( P\in \eK[X]\) tel que
    \begin{enumerate}
        \item
            \( P(f)=0\);
        \item
            l'application
            \begin{equation}        \label{EQooIBMDooVTaEhf}
                \begin{aligned}
                    \varphi\colon \frac{ \eK[X] }{ (P) }&\to \End(E) \\
                    \bar Q&\mapsto Q(f)
                \end{aligned}
            \end{equation}
            est injective.
    \end{enumerate}
\end{proposition}

\begin{proof}
    En ce qui concerne l'existence, il existe le polynôme minimal de \( f\) qui satisfait les conditions. Pour l'unicité nous y travaillons maintenant.

    Supposons que l'application \eqref{EQooIBMDooVTaEhf} soit injective. Alors pour tout \( Q\in \eK[X]\) tel que \( Q(f)=0\) nous avons \( \bar Q=0\), c'est-à-dire \( Q=PR\) pour un certain \( R\in \eK[X]\). Autrement dit : \( P\) est un générateur unitaire de l'idéal annulateur de \( f\). Le théorème~\ref{ThoCCHkoU}\ref{ITEMooASHKooZqkiCH} nous dit alors que \( P=\mu\) parce que \( \mu\) est également générateur unitaire.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]\label{LemSYsJJj}
    Soit \( f\colon E\to E\) un endomorphisme de l'espace vectoriel \( E\). Il existe un élément \( x\in E\) tel que \( \mu_{f,x}=\mu_f\).
\end{lemma}

\begin{proof}
    Soit une décomposition en irréductibles du polynôme minimal \( \mu=P_1^{\alpha_1}\ldots P_r^{\alpha_r}\). Nous notons \( E_i=\ker\big( P_i^{\alpha_i}(f) \big)\). Les polynômes \( P_i\) sont étrangers deux à deux (un diviseur commun aurait a fortiori été un diviseur et aurait contredit l'irréductibilité). Le lemme des noyaux~\ref{ThoDecompNoyayzzMWod} nous donne la somme directe
    \begin{equation}
        E=\bigoplus_{i=1}^r\ker\big( P_i^{\alpha_i}(f) \big).
    \end{equation}
    Si \( x_i\in E_i\) alors \( \mu_{x_i}\) est une puissance de \( P_i\). En effet \( \mu_{x_i}\divides \mu\) et est donc un produit des puissances des \( P_j\). Or si \( (QP_j)(f)x_i=0\) alors \( (P_jQ)(f)x_i=0\), ce qui donne \( Q(f)x_i\in E_j\cap E_i=\{ 0 \}\) si \( j\neq i\). Donc \( \mu_{x_i}\) n'est pas de la forme \( QP_j\) pour \( j\neq i\). Nous en déduisons que \( \mu_{x_i}\) est une puissance de \( P_i\) dès que \( x_i\in E_i\). Nous choisissons \( x_i\in E_i\) tel que \( \mu_{x_i}=P_i^{\alpha_i}\).

    Nous posons enfin \( a=x_1+\cdots +x_r\); par définition du polynôme annulateur \( \mu_a\), nous avons
    \begin{equation}        \label{EqooVIGGooSfuvwB}
        0=\mu_a(f)a=\mu_a(f)x_1+\cdots +\mu_a(f)x_r.
    \end{equation}
    Mais \( \mu_a(f)x_i\in E_i\), et la somme des \( E_i\) est directe, donc l'annulation de la somme \eqref{EqooVIGGooSfuvwB} implique l'annulation de chacun des termes : \( \mu_a(f)x_i=0\) pour tout \( i\). Cela prouve que \( \mu_{x_i}\divides \mu_a\). Mais comme les \( \mu_{x_i}\) sont premiers deux à deux (parce que ce sont les \( P_i^{\alpha_i}\)), nous concluons que le produit divise encore \( \mu_a\) :
    \begin{equation}
        \prod_{i=1}^r\mu_{x_i}\divides \mu_a,
    \end{equation}
    c'est-à-dire \( \mu\divides \mu_a\). Comme nous avons aussi \( \mu_a\divides \mu\), nous déduisons \( \mu_a=\mu\).
\end{proof}

\begin{definition}[Matrices, endomorphismes et vecteurs cycliques]      \label{DEFooFEIFooNSGhQE}
    Une matrice est \defe{cyclique}{cyclique!matrice}\index{matrice!cyclique} si elle est semblable à une matrice compagnon. Un endomorphisme \( f\colon E\to E\) est \defe{cyclique}{cyclique!endomorphisme}\index{endomorphisme!cyclique} si il existe un vecteur \( x\in E\) tel que \( \{ f^k(x) \}_{k=0,\ldots, n-1} \) est une base de \( E\). Un vecteur ayant cette propriété est un \defe{vecteur cyclique}{vecteur!cyclique} pour \( f\).
\end{definition}

\begin{lemma}   \label{LemAGZNNa}
    Soit \( E\) un espace vectoriel de dimension finie et un endomorphisme cyclique\footnote{Voir la définition~\ref{DEFooFEIFooNSGhQE}.} \( f\) de \( E\). Soit un vecteur cyclique \( v\) de \( f\), alors le polynôme minimal de \( f\) est égal au polynôme minimal de \( f\) au point \( v\) : \( \mu_{f}=\mu_{f,v}\).
\end{lemma}

\begin{proof}
    Montrons que \( \mu_{f,v}\) est un polynôme annulateur de \( f\), ce qui prouvera que \( \mu_f\) divise \( \mu_{f,v}\) par la proposition~\ref{PropAnnncEcCxj}. Étant donné que \( v\) est cyclique, tout élément de \( E\) s'écrit sous la forme \( x=Q(f)v\). Prenons un polynôme \( P\) annulateur de \( f\) en \( v\) : \( P(f)v=0\). Nous montrons que \( P\) est alors un polynôme annulateur de \( f\). En effet, nous avons
    \begin{equation}
        P(f)x=\big( P(f)\circ Q(f) \big)v=\big( Q(f)\circ P(f) \big)v=0
    \end{equation}
    où nous avons utilisé le lemme~\ref{LemQWvhYb}.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]        \label{LEMooOWDAooWPbPda}
    Soit \( a\in E\) un vecteur cyclique pour \( f\), tel que \( \mu_a=\mu\). Alors \( E_a\) est un sous-espace stable par \( f\) pour lequel il existe un supplémentaire stable.
\end{lemma}

\begin{proof}
    Soit \( l=\deg(\mu)=\deg(\mu_a)\). L'espace \( E_a\) étant engendré par les \( f^k(a)\) nous savons que \( e_1=a\), \( e_2=f(a)\),\ldots, \( e_l=f^{l-1}(a)\) forment une base de \( E_a\). Nous pouvons la compléter en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Et nous posons\footnote{ici, comme presque partout, \( e^*_{l}\) est le dual de \( e_l\), c'est-à-dire l'application linéaire sur \( E\) donnée par \( e^*_l(e_i)=\delta_{li}\), voir la définition \ref{DEFooTMSEooZFtsqa}.}
    \begin{subequations}
        \begin{align}
            G & = \{ x\in E\tq e^*_l\big( f^k(x) \big)=0, \forall k\geq 0 \}\\
              & = \bigcap_{k\geq 0}\ker\{ e^*_l\circ f^k \}                 \\
              & = \bigcap_{k=0}^{l-1}\ker( e^*_l\circ f^k ).
        \end{align}
    \end{subequations}
    La dernière égalité est due au fait que \( l\) soit le degré de \( \mu\). Du coup \( f^l\) est une combinaison linéaire des \( f^i\) avec \( i\leq l-1\).

    Nous avons \( f(G)\subset G\) et de plus \( E_a\cap G=\{ 0 \}\) parce qu'un élément de \( E_a\) est une combinaison linéaire d'éléments de la forme \( f^j(a)\) (\( j\leq l\)). Après application de \( f^{l-j}\), ces éléments obtiennent une composante \( f^l(a)=e_l\). De plus \( G\) est un sous-espace vectoriel du fait que \( e^*_l\circ f^i\) est une application linéaire.

    Montrons enfin que \( \dim(G)=n-l\). Pour cela nous remarquons que \( G\) est une intersection d'hyperplans, et nous montrons que les équations définissant ces hyperplans sont linéairement indépendantes. Soit donc
    \begin{equation}        \label{EqooOHESooRtBUfc}
        \sum_{j=0}^{l-1}\lambda_j\big( e^*_l\circ f^j \big)=0
    \end{equation}
    et montrons que \( \lambda_j=0\) pour tout $j$ est l'unique solution. Soit \( x\in E\) et appliquons l'opération \eqref{EqooOHESooRtBUfc} au vecteur \( f^i(x)\); le résultat est zéro :
    \begin{equation}
        0=\sum_{j=0}^{l-1}\lambda_j(e^*_l\circ f^i\circ f^j)=(e^*_l\circ f^i)P(u)
    \end{equation}
    où nous avons posé \( P(X)=\sum_{j=0}^{l-1}\lambda_jX^j\). Appliquons cela à \( a\) : pour tout \( i\) nous avons
    \begin{equation}
        (e^*_l\circ f^i)\big( P(f)a \big)=0.
    \end{equation}
    Mais par définition de \( E_a\), l'élément \(P(f)a \) est dans \( E_a\). Nous en déduisons que
    \begin{equation}
        P(f)a\in G\cap E_a=\{ 0 \},
    \end{equation}
    c'est-à-dire que \( P\) est un polynôme annulateur de \( a\). Mais \( P\) est de degré \( l-1\) alors que le polynôme minimal de \( a\) est de degré \( l\). Par conséquent \( P=0\) et \( \lambda_j=0\) pour tout \( j\).
\end{proof}

\begin{definition}  \label{DEFooBOHVooSOopJN}
    L'endomorphisme \( f\) d'un espace vectoriel est \defe{semi-simple}{semi-simple!endomorphisme} si tout sous-espace stable par \( f\) possède un supplémentaire stable.
\end{definition}

\begin{lemma}   \label{LemrFINYT}
    Si le polynôme minimal d'un endomorphisme est irréductible, alors cet endomorphisme est semi-simple\footnote{Définition~\ref{DEFooBOHVooSOopJN}.}.
\end{lemma}

\begin{proof}
    Soit \( f\), un endomorphisme dont le polynôme minimal est irréductible et \( F\), un sous-espace stable par \( f\). Nous devons en trouver un supplémentaire stable. Si \( F=E\), il n'y a pas de problème. Sinon nous considérons \( u_1\in E\setminus F\) et
    \begin{equation}
        E_{u_1}=\{ P(f)u_1\tq P\in \eK[X] \},
    \end{equation}
    qui est un espace stable par \( f\).

    Montrons que \( E_{u_1}\cap F=\{ 0 \}\). Pour cela nous étudions l'idéal
    \begin{equation}
        I_{u_1}=\{ P\in \eK[X]\tq P(f)u_1=0 \}.
    \end{equation}
    C'est un idéal non réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) par exemple est dans \( I_{u_1}\). Soit \( P_{u_1}\) un générateur unitaire de \( I_{u_1}\). Étant donné que \( \mu_f\in I_{u_1}\), nous avons \( P_{u_1}\) divise \( \mu_f\) et donc, \( P_{u_1}=\mu_f\), parce que \( \mu_f\) est irréductible par hypothèse.

    Soit \( y\in E_{u_1}\cap F\). Par définition il existe \( P\in\eK[X]\) tel que \( y=P(f)u_1\) et si \( y\neq 0\), cela signifie que \( P\notin I_{u_1}\), c'est-à-dire que \( P_{u_1} \) ne divise pas \( P\). Étant donné que \( P_{u_1}\) est irréductible cela implique que \( P_{u_1}\) et \( P\) sont premiers entre eux (ils n'ont pas d'autre \( \pgcd\) que \( 1\)).

    Nous utilisons maintenant des coefficient de Bézout (théorème~\ref{ThoBezoutOuGmLB}) \( A,B\in \eK[X]\) tels que
    \begin{equation}
        AP+BP_{u_1}=1.
    \end{equation}
    Nous appliquons cette égalité à \( f\) et puis à \( u_1\):
    \begin{equation}
        u_1=A(f)\circ \underbrace{P(f)u_1}_{=y}+B(f)\circ \underbrace{P_{u_1}(u_1)}_{=0}=A(f)y.
    \end{equation}
    Mais \( y\in F\), donc \( A(f)y\in F\). Nous aurions donc \( u_1\in F\), ce qui est impossible par choix. Nous savons maintenant que l'espace \( E_{u_1}\oplus F\) est stable sous \( f\). Si cet espace est \( E\) alors nous arrêtons. Sinon nous reprenons le raisonnement avec \( E_{u_1}\oplus F\) en guise de \( F\) et en prenant \( u_2\in E\setminus(E_{u_1}\oplus F)\). Étant donné que \( E\) est de dimension finie, ce procédé s'arrête à un certain moment et nous aurons
    \begin{equation}
        E=F\oplus E_{u_1}\oplus\ldots\oplus E_{u_k}
    \end{equation}
    où chacun des \( E_{u_i}\) sont stables.
\end{proof}

\begin{theorem} \label{ThoFgsxCE}
    Un endomorphisme est semi-simple si et seulement si son polynôme minimal est produit de polynômes irréductibles distincts deux à deux.
\end{theorem}
\index{anneau!principal}

\begin{proof}

    Supposons que \( f\) soit semi-simple et que son polynôme minimal soit donné par \( \mu_f=M_1^{\alpha_1}\ldots M_r^{\alpha_r}\) où les \( M_i\) sont des polynômes irréductibles deux à deux distincts. Nous devons montrer que \( \alpha_i=1\) pour tout \( i\). Soit \( i\) tel que \( \alpha_i\geq 1\) et \( N\in \eK[X]\) tel que \( \mu_f=M^2N\) où l'on a noté \( M=M_i\). Nous étudions l'espace
    \begin{equation}
        F=\ker M(f)
    \end{equation}
    qui est stable par \( f\), et qui possède donc un supplémentaire \( S\) également stable par \( f\). Nous allons montrer que \( MN\) est un polynôme annulateur de \( f\).

    D'abord nous prenons \( x\in S\). Étant donné que \( F\) est le noyau de \( M(f)\),
    \begin{equation}
        M(f)\big( MN(f)x \big)=\mu_f(f)x=0,
    \end{equation}
    ce qui signifie que \( MN(f)x\in F\). Mais puisque \( S\) est stable par \( f\) nous avons aussi \( MN(f)x\in S\). Finalement \( MN(f)x\in F\cap S=\{ 0 \}\). Autrement dit, \( MN(f)\) s'annule sur \( S\).

    Prenons maintenant \( y\in F\). Nous avons
    \begin{equation}
        MN(f)=N(f)\big( M(f)y \big)=0
    \end{equation}
    parce que \( y\in F=\ker M(f)\).

    Nous avons prouvé que \( MN(f)\) s'annule partout et donc que \( MN(f)\) est un polynôme annulateur de \( f\), ce qui contredit la minimalité de \( \mu_f=M^2N\).

    Nous passons au sens inverse. Soit \( m_f=M_1\ldots M_r\) une décomposition du polynôme minimal de l'endomorphisme \( f\) en irréductibles distincts deux à deux. Soit \( F\) un sous-espace vectoriel stable par \( f\). Nous notons
    \begin{equation}
        E_i=\ker(M_i(f))
    \end{equation}
    et \( f_i=f|_{E_i}\). Par le lemme~\ref{CorKiSCkC} nous avons
    \begin{equation}
        F=\bigoplus_{i=1}^r(F\cap E_i).
    \end{equation}
    Les espaces \( E_i\) sont stables par \( f\) et étant donné que \( M_i\) est irréductible, il est le polynôme minimal de \( f_i\). En effet, \( M_i\) est annulateur de \( f_i\), ce qui montre que le polynôme minimal de \( f_i\) divise \( M_i\). Mais \( M_i\) étant irréductible, \( M_i\) est le polynôme minimal. Étant donné que \( \mu_{f_i}=M_i\), l'endomorphisme \( f_i\) est semi-simple par le lemme~\ref{LemrFINYT}.

    L'espace \( F\cap E_i\) étant stable par l'endomorphisme semi-simple \( f_i\), il possède un supplémentaire stable que nous notons \( S_i\)~:
    \begin{equation}
        E_i=S_i\oplus(F\cap E_i).
    \end{equation}
    Étant donné que sur chaque \( S_i\) nous avons \( f|_{S_i}=f_i\), l'espace \( S=S_1\oplus\ldots\oplus S_r\) est stable par \( f\). Par conséquent, nous avons
    \begin{subequations}
        \begin{align}
            E &= E_1\oplus\ldots\oplus E_r\\
              &= \big( S_1\oplus(F\cap E_1) \big)\oplus\ldots\oplus\big( S_r\oplus(F\cap E_r) \big)\\
              &= \big( \bigoplus_{i=1}^rS_i \big)\oplus\big( \bigoplus_{i=1}^rF\cap E_i \big)\\
              &= S\oplus F,
        \end{align}
    \end{subequations}
    ce qui montre que \( F\) a bien un supplémentaire stable par \( f\) et donc que \( f\) est semi-simple.
\end{proof}

\begin{example}[L'espace engendré par \( \mtu\), \( A\), \( A^2\),\ldots]
    Soit \( A\) une matrice, et
    \begin{equation}
        E=\Span\{A^k\tq k\in \eN \}.
    \end{equation}
    Nous montrons que \( \dim(E)\) est le degré du polynôme minimal de \( A\).

    D'abord l'idéal annulateur de \( A\) est engendré par le polynôme minimal\footnote{Proposition~\ref{PropAnnncEcCxj}.} que nous notons
        $\mu=\sum_{k=0}^pa_kX^k$.
    La partie \( \{ \mtu,\ldots, A^{p-1} \}\) est libre parce qu'une combinaison linéaire nulle de ces éléments serait un polynôme annulateur en \( A\) de degré plus petit que \( p\). Donc \( \dim(E)\geq p\).

    La partie \( \{ \mtu,A,\ldots, A^p \}\) est liée à cause du polynôme minimal. Isoler \( A^p\) dans \( \mu(A)=0\) donne un polynôme \( f\) de degré \( p-1\) tel que \( A^p=f(A)\).

    Nous allons montrer à présent que la famille \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice (alors \( \dim(E)\leq p\)). Soit un entier \( q\geq p\) et de division euclidienne\footnote{Théorème~\ref{ThoDivisEuclide}.} \( np+r=q\) avec \( r<p\). Nous avons \( A^q=A^{np}A^r\). D'une part
    \begin{equation}
        A^{np}=(A^p)^n=f(A)^n
    \end{equation}
    est de degré \( n(p-1)\). Par conséquent
    \begin{equation}
        A^q=f(A)^nA^r
    \end{equation}
    qui est de degré \( n(p-1)+r=q-n\). Autrement dit il existe un polynôme \( g_1\) de degré \( q-n\) tel que \( A^q=g_1(A)\). Si \( q-n>p-1\) alors nous pouvons recommencer et obtenir un polynôme \( g_2\) de degré strictement inférieur à celui de \( g_1\) tel que \( A^q=g_2(A)\). Au bout du compte, il existe un polynôme \( g\) de degré au maximum \( p-1\) tel que \( A^q=g(A)\). Cela prouve que la partie \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice de \( E\).

    La dimension de \( E\) est donc \( p\), le degré du polynôme minimal.
\end{example}

\begin{proposition}     \label{PropooCFZDooROVlaA}
    Soit \( f\) un endomorphisme d'un espace vectoriel de dimension finie. Nous avons l'isomorphisme d'espace vectoriel
    \begin{equation}
        \eK[f]\simeq\frac{ \eK[X] }{ (\mu_f) }
    \end{equation}
    La dimension en est \( \deg(\mu_f)\).
\end{proposition}

\begin{proof}
    Notons avant de commencer que \( (\mu)\) est l'idéal engendré par \( \mu\). Les classes dont il est question dans le quotient \( \eK[X]/(\mu)\) sont
    \begin{equation}
        \bar P=\{ P+S\mu \}_{S\in \eK[X]}.
    \end{equation}
    Nous allons montrer que l'application suivante fournit l'isomorphisme :
    \begin{equation}
        \begin{aligned}
            \psi\colon \frac{ \eK[X] }{ (\mu) }&\to \eK[f] \\
            \bar P&\mapsto P(f).
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[\( \psi\) est bien définie]
            Si \( Q\in \bar P\) alors \( Q=P+S\mu\) pour un certain \( S\in \eK[X]\). Du coup nous avons
            \begin{equation}
                \psi(\bar Q)=P(f)+(S\mu)(f).
            \end{equation}
            Mais \( \mu(f)=0\) donc le deuxième terme est nul. Donc \( \psi(\bar P)\) est bien défini.
        \item[Injectif]
            Si \( \psi(\bar P)=0\) nous avons \( P(f)=0\), ce qui signifie que \( P=S\mu\) pour un polynôme \( S\). Par conséquent \( P\in (\mu)\) et donc \( \bar P=0\).
        \item[Surjectif]
            Soit \( P\in \eK[X]\). L'élément \( P(f) \) de \( \eK[f]\) est dans l'image de \( \psi\) parce que c'est \( \psi(\bar P)\).
    \end{subproof}
    En ce qui concerne la dimension, le corolaire~\ref{CorsLGiEN} en parle déjà : une base est donnée par les projections de \( 1,X,\ldots, X^{\deg(\mu_f)-1}\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefOWQooXbybYD}
    Soit un anneau commutatif \( A\). Si \( u\in\eM(n,A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
    \begin{equation}    \label{Eqkxbdfu}
        \chi_u(X)=\det(u-X\mtu_n).
    \end{equation}
    Nous définissons de même le polynôme caractéristique d'un endomorphisme \( u\colon E\to E\).
\end{definition}

\begin{remark}
    Quelques remarques à propos du signe\quext{Attention : je crois qu'il y a des incohérences dans le Frido à propos de ce choix}.
    \begin{itemize}
        \item
            Certains auteurs définissent le polynôme caractéristique par \( \det(X-u)\) au lieu de \( \det(u-X)\).
        \item
            Wikipédia francophone\cite{BIBooYPTLooZlMfAG} prend la définition \( \det(X-u)\) (donc opposée de la notre). Allez lire la page de discussion.
        \item
            Sur les wikipédias en d'autres langues, ça varie.
        \item
            Un avantage de \( \det(u-X)\) est que \( \det(u)=\chi_u(0)\).
        \item
            Un avantage de \( \det(X-u)\) est qu'il est unitaire.
    \end{itemize}
\end{remark}

\begin{lemma}       \label{LemooWCZMooZqyaHd}
    Le polynôme caractéristique \( \chi_u\) est unitaire en dimension paire et a pour degré la dimension de l'espace vectoriel \( E\).
\end{lemma}

\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}

\begin{theorem} \label{ThoWDGooQUGSTL}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\)~\ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul par la proposition~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\).

    \ref{ItemeXHXhHii} \( \Leftrightarrow\)~\ref{ItemeXHXhHiii}. C'est une application directe du théorème~\ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}

\begin{example} \label{ExICOJcFp}
    Sur \( \eR^2\), nous considérons la matrice \( A=\begin{pmatrix}
        1    &   0    \\
        1    &   1
    \end{pmatrix}\) qui a pour polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} le polynôme \( \chi_A=(X-1)^2\). Le nombre \( \lambda=1\) est une racine double de ce polynôme, et pourtant il n'y a qu'une seule dimension d'espace propre :
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\
            1    &   1
        \end{pmatrix}\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}
    \end{equation}
    entraine \( x=0\).

    Ici la multiplicité algébrique est différente de la multiplicité géométrique.
\end{example}

La proposition suivante donne une utilisation amusante de la notion de polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.}.
\begin{proposition}[\cite{ooNGUJooPphdsT}]      \label{PROPooKJWOooOjSFaA}
    Soit un espace vectoriel \( E\) de dimension finie pour lequel il existe un endomorphisme \( f\colon E\to E\) tel que \( (f\circ f)(x)=-x\) pour tout \( x\in E\). Alors la dimension de \( E\) est paire.
\end{proposition}

\begin{proof}
    Cherchons les valeurs propres de \( f\) en résolvant l'équation \( f(x)=\lambda x\). Nous appliquons \( f\) à cette égalité :
    \begin{equation}
        -x=\lambda f(x)=\lambda^2x.
    \end{equation}
    Donc \( \lambda\) ne peut pas être réel. Nous avons montré que \( f\) n'a pas de valeur propre réelle. Or le polynôme caractéristique de \( f\) est de degré égal à la dimension.
    %TODOooDJPSooTCqwfW égal à quel dimension ? ça demande une précision.
    Si la dimension est impaire, le polynôme caractéristique est de degré impair, et possède donc une racine réelle. Autrement dit, l'absence de racines réelles au polynôme caractéristique indique une dimension paire.
\end{proof}

Une autre preuve possible est d'utiliser le déterminant : si la dimension de \( E\) est \( n\) nous avons :
\begin{equation}
    \det(f^2)=\det(-\id)=(-1)^n.
\end{equation}
Donc \( (-1)^n\) est positif, ce qui montre que \( n\) est pair.

\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point~\ref{ItemHMviZw} est une application du point~\ref{ItemKHNExH}. Les deux gros morceaux sont donc les points~\ref{ItemfzKOCo} et~\ref{ItemKHNExH}.

    Étant donné que \( \mu_{f,x}\) est de degré minimal dans \( I_{f,x}\), l'ensemble
    \begin{equation}
        B=\{ f^k(x)\tq 0\leq k\leq p_{f,x}-1 \}
    \end{equation}
    est libre. En effet une combinaison nulle des vecteurs de \( B\) donnerait un polynôme en \( f\) de degré inférieur à \( p_{f,x}\) annulant \( x\). Nous écrivons
    \begin{equation}
        \mu_{f,x}(X)=X^{p_{f,x}}-\sum_{i=0}^{p_{f,x}-1}a_iX^i.
    \end{equation}
    Étant donné que \( \mu_{f,x}(f)x=0\) et que la somme du membre de droite est dans \( \Span(B)\), nous avons \( f^{p_{f,x}}(x)\in\Span(B)\). Nous prouvons par récurrence que \( f^{p_{f,x}+k}(x)\in\Span(B)\). En effet en appliquant \( f^k\) à l'égalité
    \begin{equation}
        0=f^{p_{f,x}}(x)-\sum_{i=0}^{p_{f,x}-1}a_if^i(x)
    \end{equation}
    nous trouvons
    \begin{equation}
        f^{p_{f,x}+k}(x)=\sum_{i=0}^{p_{f,x}-1}a_if^{i+k}(x),
    \end{equation}
    alors que par hypothèse de récurrence le membre de droite est dans \( \Span(B)\). L'ensemble \( B\) est alors générateur de \( E_{f,x}\) et donc une base d'icelui. Nous avons donc bien \( \dim(E_{f,x})=p_{f,x}\).

    Nous montrons maintenant que \( \mu_{f,x}\) est annulateur de \( f\) au point \( x\). Nous savons que
    \begin{equation}
        \mu_{f,x}(f)x=0.
    \end{equation}
    En y appliquant \( f^k\) et en profitant de la commutativité des polynômes sur les endomorphismes (proposition~\ref{LemQWvhYb}), nous avons
    \begin{equation}
        0=f^k\big( \mu_{f,x}(f)x \big)=\mu_{f,x}(f)f^k(x),
    \end{equation}
    de telle sorte que \( \mu_{f,x}(f)\) est nul sur \( B\) et donc est nul sur \( E_{f,x}\). Autrement dit,
    \begin{equation}
        \mu_{f,x}\big( f|_{E_{f,x}} \big)=0.
    \end{equation}
    Montrons que \( \mu_{f,x}\) est même minimal pour \( f|_{E_{f,x}}\). 

    Supposons avoir \( Q\), un polynôme non nul de degré \( p_{f,x}-1\) annulant \( f|_{E_{f,x}}\). En particulier \( Q(f)x=0\). Cela signifie que \( B\) est un système lié, alors que nous avons montré que c'était un système libre. Contradiction. Nous concluons que \( \mu_{f,x}\) est le polynôme minimal de \( f|_{E_{f,x}}\).
\end{proof}

Cette histoire de densité permet de donner une démonstration alternative du théorème de Cayley-Hamilton.
\begin{theorem}[Cayley-Hamlilton]   \label{ThoCalYWLbJQ}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une démonstration plus simple via la densité des diagonalisables est donnée en théorème~\ref{ThoHZTooWDjTYI}.
\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{f,x}}\) divise \( \chi_f\), c'est-à-dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition~\ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

\begin{corollary}
    Le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{corollary}

\begin{proof}
    Le polynôme minimal divise le polynôme caractéristique parce qu'il engendre l'idéal des polynômes annulateurs par la proposition \ref{PropAnnncEcCxj}. Or le degré du polynôme caractéristique est la dimension de l'espace par le lemme~\ref{LemooWCZMooZqyaHd}.
\end{proof}

\begin{example}[Calcul de l'inverse d'un endomorphisme]
    Le théorème de Cayley-Hamilton donne un moyen de calculer l'inverse d'un endomorphisme inversible pourvu que l'on connaisse son polynôme caractéristique. En effet, supposons que
    \begin{equation}
        \chi_f(X)=\sum_{k=0}^na_kX^k.
    \end{equation}
    Nous aurons alors
    \begin{equation}
        0=\chi_f(f)=\sum_{k=0}^na_kf^k.
    \end{equation}
    Nous appliquons \( f^{-1}\) à cette dernière égalité en sachant que \( f^{-1}(0)=0\) :
    \begin{equation}
        0=a_0f^{-1}+\sum_{k=1}^na_kf^{k-1},
    \end{equation}
    et donc
    \begin{equation}
        f^{-1}=-\frac{1}{ \det(f) }\sum_{k=1}^na_kf^{k-1}
    \end{equation}
    où nous avons utilisé le fait que \( a_0=\chi_f(0)=\det(f)\).
\end{example}

\begin{proposition}\label{PropooBYZCooBmYLSc}
    Si \( (X-z)^l\) (\( l\geq 1\)) est la plus grande puissance de \( (X-z)\) dans le polynôme caractéristique d'un endomorphisme \( f\) alors
    \begin{equation}
        1\leq \dim(E_z)\leq l.
    \end{equation}
    C'est-à-dire que nous avons au moins un vecteur propre pour chaque racine du polynôme caractéristique.
\end{proposition}

\begin{proof}
    Si $(X-z)$ divise \( \chi_f\) alors en posant \( \chi_f=(X-z)P(X)\) nous avons
    \begin{equation}
        \det(f-X\mtu)=(X-z)P(X),
    \end{equation}
    ce qui, évalué en \( X=z\), donne \( \det(f-z\mtu)=0\). L'annulation du déterminant étant équivalente à l'existence d'un noyau non trivial, nous avons \( v\neq 0\) dans \( E\) tel que \( (f-z\mtu)v=0\). Cela donne \( f(v)=zv\) et montre que \( v\) est vecteur propre de \( f\) pour la valeur propre \( z\). Et aussi que \( \dim(E_z)\geq 1\).

    Si \( \dim(E_z)=k\) alors le théorème de la base incomplète~\ref{ThonmnWKs} nous permet d'écrire une base de \( E\) dont les \( k\) premiers vecteurs forment une base de \( E_z\). Dans cette base, la matrice de \( f\) est de la forme
    \begin{equation}
        \begin{pmatrix}
             z  &       &       &   *                 \\
                &   \ddots      &       &   \vdots    \\
                &       &   z   &   *                 \\
                &       &       &   *
         \end{pmatrix}
    \end{equation}
    où les étoiles représentent des blocs à priori non nuls. En tout cas, sous cette forme, il est visible que \( (X-z)^k\) divise \( \chi_f\).
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

\begin{definition}[\cite{ooUQBZooCAKfrE}]      \label{DEFooEEQGooNiPjHz}
    Soient trois espaces vectoriels \( E,F\) et \( V\) sur le même corps commutatif \( \eK\). Une application \( b\colon E\times F\to V\) est \defe{bilinéaire}{application bilinéaire} si elle est séparément linéaire en ses deux variables, c'est-à-dire si
    \begin{enumerate}
        \item 
            \( b(u_1+u_2,v)=b(u_1,v)+b(u_2,v)\),
        \item
            \( b(u,v_1+v_2)=b(u,v_1)+b(u,v_2)\)
        \item
            \( b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,u_1,u_2\in E\), \( v,v_1,v_2\in F\) et pour tout \( \lambda\in \eK\).

    Dans le cas \( E=F\) et \( V=\eK\), nous parlons de \defe{forme bilinéaire}{forme!bilinéaire} sur \( E\).

    Nous parlons de forme bilinéaire \defe{symétrique}{forme bilinéaire symétrique} si de plus \( b(u,v)=b(v,u)\).
\end{definition}

\begin{normaltext}
    Une application bilinéaire \( E\times E\to \eK\) n'est pas une application linéaire; la distinction est importante. La linéarité est
    \begin{equation}
        b(\lambda u,\lambda v)= b\big( \lambda(u,v) \big)=\lambda b(u,v)
    \end{equation}
    et la bilinéarité est
    \begin{equation}
        b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v).
    \end{equation}
    En réalité la seule forme qui soit à la fois linéaire et bilinéaire est la forme identiquement nulle : la condition
    \begin{equation}
        b(\lambda u,\lambda v)=\lambda^2b(u,v)=\lambda b(u,v)
    \end{equation}
    pour tout \( \lambda\in \eK\) implique \( b(u,v)=0\).
\end{normaltext}

\begin{example}[\cite{BIBooJMSXooYUADgm}]
    L'application
    \begin{equation}
        \begin{aligned}
            b\colon \eM(n,\eK)\times \eM(n,\eK)&\to \eK \\
            (A,B)&\mapsto \trace(AB) 
        \end{aligned}
    \end{equation}
    est une forme bilinéaire symétrique.

    La vérification est un calcul :
    \begin{equation}
        \trace(BA)=\sum_{i}(BA)_{ii}=\sum_{ik}B_{ik}A_{ki}=\sum_{ik}A_{ki}A_{ik}=\sum_k(AB)_{kk}=\trace(AB).
    \end{equation}
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dégénérescence d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\), une forme bilinéaire symétrique non dégénérée  sur l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\) où \( \eK\) est un corps de caractéristique différente de \( 2\). Nous notons \( q\) la forme quadratique associée.

\begin{definition}      \label{DEFooNUBFooLfCqaK}
    Une forme bilinéaire est \defe{non dégénérée}{forme!bilinéaire!non dégénérée} \( b(x,z)=0\) pour tout \( z\) implique \( x=0\).
\end{definition}

\begin{lemma}   \label{LemyKJpVP}
    Soit \( b\) une forme bilinéaire non dégénérée. Si \( x\) et \( y\) sont tels que \( b(x,z)=b(y,z)\) pour tout \( z\), alors \( x=y\).
\end{lemma}

\begin{proof}
    C'est immédiat du fait de la linéarité en le premier argument et de la non-dégénérescence : si \( b(x,z)-b(y,z)=0\) alors
    \begin{equation}
        b(x-y,z)=0
    \end{equation}
    pour tout \( z\), ce qui implique \( x-y=0\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Topologie}
%---------------------------------------------------------------------------------------------------------------------------

La topologie considérée sur \( Q(E)\) est celle de la norme
\begin{equation}    \label{EqZYBooZysmVh}
    N(q)=\sup_{\| x \|_E=1}| q(x) |,
\end{equation}
qui du point de vue de \( S(n,\eR)\) est
\begin{equation}    \label{EQooJETQooIjxRWu}
    N(A)=\sup_{\| x \|_E=1}| x^tAx |.
\end{equation}
Notons que à droite, c'est la valeur absolue usuelle sur \( \eR\).

\begin{proposition} \label{PropFSXooRUMzdb}
    Soit \( \{ e_i \}\) une base de \( E\). L'application
    \begin{equation}
        \begin{aligned}
            \phi\colon Q(E)&\to S(n,\eR) \\
            q&\mapsto \big(   b(e_i,e_j)   \big)_{i,j}
        \end{aligned}
    \end{equation}
    où \( b\) est forme bilinéaire associée à \( q\) est une bijection linéaire et continue\footnote{Pour les topologies des normes \eqref{EqZYBooZysmVh} et \eqref{EQooJETQooIjxRWu}.}.
\end{proposition}

\begin{proof}
    Si \( \phi(q)=\phi(q')\); alors
    \begin{equation}
        q(x)=\sum_{i,j}\phi(q)_{ij}x_ix_j=\sum_{i,j}\phi(q')_{ij}x_ix_j=q'(x).
    \end{equation}
    Donc \( q=q'\). L'application \( \phi\) est donc injective

    De plus elle est surjective parce que si \( B\in S(n,\eR)\) alors la forme quadratique
    \begin{equation}
        q(x)=\sum_{i,j}B_{ij}x_ix_j
    \end{equation}
    a évidemment \( B\) comme matrice associée. L'application \( \phi\) est donc surjective.

    Notre application \( \phi\) est de plus linéaire parce que l'association d'une forme quadratique à la forme bilinéaire associée est linéaire.

    En ce qui concerne la continuité, nous la prouvons en zéro en considérant une suite convergente \( q_n\stackrel{Q(E)}{\longrightarrow}0\). C'est-à-dire que
    \begin{equation}
        \sup_{\| x \|=1}| q_n(x) |\to 0.
    \end{equation}
    Nous rappelons l'identité de polarisation :
    \begin{equation}
        b_n(x,y)=\frac{ 1 }{2}\big( q_n(x-y)-q(x)-q(y) \big).
    \end{equation}
    En ce qui concerne deux des trois termes, il n'y a pas de problèmes :
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|=\big| b_n(e_i,e_j) \big|\leq\frac{ 1 }{2}\big| b_n(e_i-e_j) \big|+\frac{ 1 }{2}\big| q_n(e_i) \big|+\frac{ 1 }{2}\big| q_n(e_j) \big|.
    \end{equation}
    Si \( n\) est assez grand, nous avons tout de suite
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\epsilon.
    \end{equation}
    Nous définissons \( e_{ij}\) et \( \alpha_{ij}\) de telle sorte que \( e_i-e_j=\alpha_{ij}e_{ij}\) avec \( \| e_{ij} \|=1\). Si \( \alpha=\max\{ \alpha_{ij},1 \}\) alors nous avons
    \begin{equation}
        q_n(e_i-e_j)=\alpha_{ij}^2q_n(e_{ij})\leq \alpha^2q_n(e_{ij}).
    \end{equation}
    Il suffit maintenant de prendre \( n\) assez grand pour avoir \( \sup_{\| x \|=1}| q_n(x) |\leq \frac{ \epsilon }{ \alpha^2 }\) pour avoir
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ \epsilon }{2}+\frac{ \epsilon }{ \alpha^2 }.
    \end{equation}
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isotropie}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Isotropie]   \label{DefVKMnUEM}
    Un vecteur est \defe{isotrope}{isotrope (vecteur)} pour \( b\) si il est perpendiculaire à lui-même; en d'autres termes, \( x\) est isotrope si et seulement si \( b(x,x)=0\). Un sous-espace \( W\subset E\) est \defe{totalement isotrope}{isotrope!totalement} si pour tout \( x,y\in W\), nous avons \( b(x,y)=0\).

    Le \defe{cône isotrope}{isotrope!cône} de \( b\) est l'ensemble de ses vecteurs isotropes :
    \begin{equation}
        C(b)=\{ x\in E\tq b(x,x)=0 \}.
    \end{equation}
\end{definition}
Nous introduisons quelques notations. D'abord pour \( y\in E\) nous notons
\begin{equation}
    \begin{aligned}
        \Phi_y\colon E&\to \eR \\
        x&\mapsto b(x,y)
    \end{aligned}
\end{equation}
et ensuite
\begin{equation}
    \begin{aligned}
        \Phi\colon E&\to E^* \\
        y&\mapsto \Phi_y.
    \end{aligned}
\end{equation}
\begin{definition}
    Le fait pour une forme bilinéaire \( b\) d'être dégénérée signifie que l'application \( \Phi\) n'est pas injective. Le \defe{noyau}{noyau!d'une forme bilinéaire} de la forme bilinéaire est celui de \( \Phi\), c'est-à-dire
    \begin{equation}
        \ker(b)=\{ z\in E\tq b(z,y)=0\,\forall y\in E \}.
    \end{equation}
    Autrement dit, \( \ker(b)=E^{\perp}\) où le perpendiculaire est pris par rapport à \( b\).
\end{definition}
Notons tout de même que nous utilisons la notation \( \perp\) même si \( b\) est dégénérée et éventuellement pas positive; c'est-à-dire même si la formule \( (x,y)\mapsto b(x,y)\) ne fournit pas un produit scalaire.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{BIBooWVWZooZqliJt}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme quadratique} sur \( E\) est une application \( q\colon E\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon E\times E\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in E\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{definition}[Application bilinéaire définie positive, thème~\ref{THEMEooYEVLooWotqMY}]      \label{DEFooJIAQooZkBtTy}
    Si $b$ est une application bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} sur un espace vectoriel \( E\) nous disons qu'elle est
    \begin{enumerate}
        \item
            \defe{définie positive}{application!définie positive} si $b(x,x)\geq 0$ pour tout $x\in E$ et $b(x,x)=0$ si et seulement si $x=0$.
        \item
            \defe{semi-définie positive}{application!semi-définie positive} si $b(x,x)\geq 0$ pour tout $x\in E$. Nous dirons aussi parfois qu'elle est simplement «positive».
        \end{enumerate}
\end{definition}
Cela est évidemment à lier à la définition~\ref{DefAWAooCMPuVM} et à la proposition \ref{PROPooUAAFooEGVDRC} : une application bilinéaire est définie positive si et seulement si sa matrice symétrique associée l'est.
%TODOooHJECooFCAAHN en faire une proposition à prouver.

\begin{proposition} \label{PROPooZLXVooOsXCcB}
    Soit une forme bilinéaire \( b\) et la forme quadratique associée \( q\). Alors nous avons l'\defe{identité de polarisation}{identité de polarisation} :
    \begin{equation}    \label{EqMrbsop}
        b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de substituer dans le membre de droite \( q(x)=b(x,y)\) et d'utiliser la bilinéarité :
    \begin{subequations}
        \begin{align}
            q(x)+q(y)-q(x-y)&=b(x,x)+b(y,y)-b(x-y,x-y)\\
            &=b(x,x)+b(y,y)-b(x)+b(x,y)+b(y,x)-b(y,y)\\
            &=2b(x,y)
        \end{align}
    \end{subequations}
    où nous avons utilisé le fait que \( b\) est symétrique : \( b(x,y)=b(y,x)\).
\end{proof}

\begin{lemma}       \label{LEMooLKNTooSfLSHt}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'identité de polarisation de la proposition \ref{PROPooZLXVooOsXCcB}. Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

\begin{definition}      \label{DEFooGECOooCCGVXG}
    Soit une forme quadratique \( q\) sur \( E\). Nous disons que \( v,w\in E\) sont \defe{\( q\)-orthogonaux}{\( q\)-orthogonal} si \( b(v,w)=0\) la forme bilinéaire \( b\) associée à \( q\) par le lemme \ref{LEMooLKNTooSfLSHt}.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice associée à une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooAOGPooXWXUcN}
    Soit une forme bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.} \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous définissons les nombres
    \begin{equation}    \label{EQooCUGFooRlKUtu}
        B_{\alpha\beta}=b(f_{\alpha},f_{\beta}),
    \end{equation}
    qui forment une matrice symétrique dans \( \eM(n,\eK)\). Cette matrice est la \defe{matrice associée}{matrice d'une forme bilinéaire} à la forme bilinéaire \( b\).

    La matrice d'une forme quadratique est celle associée à sa forme bilinéaire associée.
\end{definition}

\begin{lemma}       \label{LEMooDCIOooTlVZMR}
    Soit une forme bilinéaire \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous notons \( B\) la matrice de \( b\) (definition \ref{DEFooAOGPooXWXUcN}) et \( q\) la forme quadratique associée.

    Alors nous avons
\begin{equation}        \label{EQooQFMWooVKVLMx}
    b(x,y)=\sum_{\alpha\beta}B_{\alpha\beta}x_{\alpha}y_{\beta}.
\end{equation}
et
\begin{equation}
    b(x,y)=x\cdot By.
\end{equation}
où le point est le produit scalaire usuel (composante par composante).
\end{lemma}

\begin{proof}
    Si \( x=\sum_{\alpha}x_{\alpha}f_{\alpha}\) et \( y=\sum_{\beta}y_{\beta}f_{\beta}\) :

    En utilisant la convention \eqref{EQooAXRJooUwHbjB} et les choses autour (voir aussi \ref{SECooBTTTooZZABWA}),
    \begin{equation}
        b(x,y)=\sum_{\alpha}x_{\alpha}\sum_{\beta}B_{\alpha\beta}y_{\beta}=\sum_{\alpha}x_{\alpha}(By)_{\alpha}=x\cdot By.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Voir la section \ref{SECooBTTTooZZABWA}]     \label{PROPooLBIOooUpzxXA}
    Soit une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}} \( b\colon V\times V\to \eK\) dont la matrice\footnote{Définition~\ref{EQooCUGFooRlKUtu}.} dans la base \( \{ e_i \}\) est \( A\) et celle dans la base \( \{ f_{\alpha} \}\) est \( B\). Nous supposons que les bases sont liées par \( f_{\alpha}=\sum_{i}Q_{i\alpha}e_i\). Alors
\begin{equation}        \label{EQooZUVTooKjqnJj}
    B=Q^tAQ.
\end{equation}
\end{proposition}

\begin{proof}
    Soit \( x,x'\in V\) de coordonnées \( (x_i)\) et \( (x'_i)\) dans la base \( \{ e_i \}\) et \( (y_{\alpha})\), \( (y'_{\alpha})\) dans la base \( \{ f_{\alpha} \}\). Par définition de la matrice associée à une forme bilinéaire,
    \begin{equation}
        b(x,x')=\sum_{ij}A_{ij}x_ix'_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
    \end{equation}
    En remplaçant les \( x_i\) et \( x'_i\) par leurs valeurs en fonction de \( y_{\alpha}\) et \( y'_{\beta}\) données par la proposition \ref{PROPooNYYOooHqHryX}, nous trouvons
    \begin{subequations}
        \begin{align}
            b(x,x')&=\sum_{ij\alpha\beta}A_{ij}Q_{i\alpha}y_{\alpha}Q_{j\beta}y'_{\beta}\\
            &=\sum_{\alpha\beta}(Q^tAQ)_{\alpha\beta}y_{\alpha}y'_{\beta}
        \end{align}
    \end{subequations}
    où \( Q^t\) désigne la transposée de la matrice \( Q\) :  \( Q^t_{ij}=Q_{ji}\). Vu que les nombres \( y_{\alpha}\) et \( y'_{\beta}\) sont arbitraires nous déduisons\footnote{Lemme~\ref{LEMooLXAHooPRyHaF}.} que \( B=Q^tAQ\).
\end{proof}

\begin{remark}      \label{REMooNEJLooSqgeih}
    Notons que cette «loi de transformation» n'est pas la même que celle pour une application linéaire\footnote{Proposition \ref{PROPooNZBEooWyCXTw}.}. Ici nous avons \( Q^t\) alors que pour les applications linéaires nous avions \( Q^{-1}\).

    Pour cette raison, tant que nous travaillons avec des bases orthonormées, c'est-à-dire tant que \( Q\) est orthogonale\footnote{Définition~\ref{DefMatriceOrthogonale}.}, nous pouvons confondre une application linéaire avec une application bilinéaire en passant par la matrice. Mais cette identification n'est pas du tout canonique : elle repose sur le fait que les bases soient orthonormées.

    Il en découle que la réduction des endomorphismes et la réduction des formes bilinéaires ne sont pas tout à fait les mêmes théories. Par exemple la pseudo-diagonalisation simultanée (corolaire~\ref{CorNHKnLVA}) est un résultat de réduction de forme bilinéaire et non d'endomorphismes.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooUULNooUtlrar}]       \label{PROPooYXMMooYIuGRd}
    Soient un espace vectoriel \( (E,\eK)\) et une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\). Une base de \( E\) est \( q\)-orthogonale\footnote{Définition \ref{DEFooGECOooCCGVXG}.} si et seulement si la matrice de \( q\) dans cette base est diagonale.
\end{proposition}

\begin{proof}
    La matrice de \( q\) est donnée par \( Q_{ij}=b(e_i,e_j)\). Donc oui, cette matrice est diagonale si et seulement si les \( e_i\) sont orthogonaux.
\end{proof}

\begin{proposition}
    Soit une forme quadratique \( q\). Si une base \( (e_i )\) de \( E\) est \( q\)-orthogonale, alors \( \mB=\{ e_i\tq q(e_i)=0 \}\) est une base de \( \ker(q)\).
\end{proposition}

\begin{proof}
    Nous considérons un vecteur de base \( e_j\), et nous montrons que \( q(e_j)=0\) si et seulement si \( e_j\in\ker(q)\). Nous savons par la proposition \ref{PROPooYXMMooYIuGRd} que la matrice de \( q\) dans la base \( (e_i)\) est diagonale et que les éléments diagonaux sont les \( q(e_i)\). Soit \( K=\{ i\tq q_(e_i)=0 \}\).
    \begin{subproof}
    \item[\( \Span\{ e_i \}_{i\in K}\subset\ker(q)\)]
        Si \( x=\sum_{i\in K}x_ie_i\), alors 
        \begin{equation}
            q(x)=b(x,x)=\sum_{i,j\in K}| x_i |^2b(e_i,e_j)=\sum_{i,j\in K}| x_i |^2\delta_{ij}q(e_i)=0
        \end{equation}
        parce que \( q(e_i)=0\) dès que \( i\in K\).
        \item{\( \ker(q)\subset\Span\{ e_i \}_{i\in K}\)}
            Soit \( x\in \ker(q)\) et écrivons-le sous la forme \( x=\sum_{i=1}^nx_ie_i\). Nous avons
            \begin{equation}
                0=q(x)=\sum_i| x_i |^2q(e_i).
            \end{equation}
            Mais \(    | x_i |^2\geq 0 \) et \( q(e_i)\geq 0\), donc si \( q(e_i)\neq 0\), alors \( x_i=0\). Donc les seules composantes non nulles de \( x\) sont celles sur lesquelles \( q\) s'annule. En d'autres termes \( x=\sum_ix_ie_i\in \Span\{ e_i \}_{i\in K}\).
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométrie, forme quadratique et bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    La forme quadratique \( q(x)=x_1^2+x_2^2\) donne la norme euclidienne. La forme bilinéaire associée est \( b(x,y)=x_1y_1+x_2y_2\), qui est le produit scalaire usuel.
\end{example}

Il ne faudrait pas déduire trop vite que la formule \( \| x \|^2=q(x)\) donne une norme dès que \( q\) est non dégénérée. En effet \( q\) peut ne pas être définie positive. La forme \( q(x)=x_1^2-x_2^2\) prend des valeurs positives et négatives. A fortiori \( d(x,y)=q(x-y)\) ne donne pas toujours une distance.

\begin{definition}      \label{DEFooECTUooRxBhHf}
    Une \defe{isométrie}{isométrie!de forme quadratique} pour la forme quadratique \( q\) est une application bijective \( f\colon V\to V\) telle que 
    \begin{equation}
     q(x-y)=q\big( f(x)-f(y) \big).
    \end{equation}
     Dans les cas où \( q\) donne une distance, alors c'est une isométrie au sens usuel.
\end{definition}

\begin{definition}[Thème \ref{THMooVUCLooCrdbxm}]      \label{DEFooIQURooMeQuqX}
    Soit un espace vectoriel \( E\) muni d'une forme bilinéaire \( b\). Une \defe{isométrie}{isométrie (forme bilinéaire)} pour \( b\) est une bijection \( f\colon E\to E\) telle que
    \begin{equation}
        b\big( f(x),f(y) \big)=b(x,y)
    \end{equation}
    pour tout \( x,y\in E\).
\end{definition}

\begin{lemma}   \label{LemewGJmM}
    Soient \( q\) une forme quadratique et \( b\) la forme bilinéaire associée par le lemme~\ref{LEMooLKNTooSfLSHt}. Une application \( f\colon E\to E\) telle que \( f(0)=0\) est une isométrie pour \( b\) si et seulement si elle est une isométrie pour \( q\).
\end{lemma}

\begin{proof}
    Pour une application bijective \( f\colon E\to E\) telle que \( f(0)=0\), nous devons prouver l'équivalence des propriétés suivantes :
    \begin{enumerate}
        \item
            \( b\big( f(x),f(y) \big)=b(x,y)\) pour tout \( x,y\in E\);
        \item
            \( q\big( f(x)-f(y) \big)=q(x-y)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Dans le sens direct, en posant \( x=y\) nous trouvons tout de suite \( q(f(x))=q(x)\); ensuite en utilisant la distributivité de \( b\),
    \begin{subequations}
        \begin{align}
            q\big( f(x)-f(y) \big)&=b\big( f(x)-f(y),f(x)-f(y) \big)\\
            &=q\big( f(x) \big)-2b\big( f(x),f(y) \big)+q\big( f(y) \big)\\
            &=q(x)+q(y)-2b(x,y)\\
            &=q(x-y).
        \end{align}
    \end{subequations}

    Dans l'autre sens, nous commençons par remarquer que l'hypothèse \( f(0)=0\) donne \( q(x)=q\big( f(x) \big)\). Ensuite nous utilisons l'identité de polarisation \eqref{EqMrbsop} :
    \begin{subequations}
        \begin{align}
            b\big( f(x),f(y) \big)&=\frac{ 1 }{2}\big[ q\big( f(x) \big)+q\big( f(y) \big)-q\big( f(x-y) \big) \big]\\
            &=\frac{ 1 }{2}\big[ q(x)+q(y)-q(x-y) \big]\\
            &=b(x,y).
        \end{align}
    \end{subequations}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométries}
%---------------------------------------------------------------------------------------------------------------------------

Voici un théorème pas toujours bien énoncé dans les cours de physique qui font de la relativité. Au moment de «prouver» les transformations de Lorentz\footnote{Théorème \ref{THOooYHDWooWxVovH}.}, beaucoup oublient de justifier pourquoi elles devraient être linéaires.
\begin{theorem}[\cite{ooQFKAooFnllQU}]     \label{ThoDsFErq}
    Une isométrie\footnote{Définition \ref{DEFooIQURooMeQuqX}.} d'une forme bilinéaire non dégénérée est linéaire.
\end{theorem}

\begin{proof}
    Soient une forme bilinéaire non-dégénérée \( b\) sur l'espace vectoriel \( E\) ainsi qu'une isométrie $f$ pour icelle. Soit \( z\in E\); étant donné que \( f\) est bijective nous pouvons considérer l'élément \( f^{-1}(z)\in E\) et calculer
    \begin{subequations}
        \begin{align}
            b\big( f(x+y),z \big)&=b\big( f(x+y),f(f^{-1}(z)) \big)\\
            &=b(x+y,f^{-1}(z))\\
            &=b(x,f^{-1}(z))+b(y,f^{-1}(z))\\
            &=b(f(x),z)+b(f(y),z)\\
            &=b\big( f(x)+f(y),z \big),
        \end{align}
    \end{subequations}
    donc \( f(x+y)=f(x)+f(y)\) par le lemme~\ref{LemyKJpVP}.

    De la même façon on trouve \( b\big( f(\lambda x),z \big)=b\big( \lambda f(x),z \big)\) qui prouve que \( f(\lambda x)=\lambda f(x)\) et donc que \( f\) est linéaire.
\end{proof}

\begin{example}
    Une isométrie peut ne pas être linéaire quand la forme bilinéaire est dégénérée. Par exemple pour la forme bilinéaire sur \( \eR^2\) donnée par
    \begin{equation}
        b\big( (a,b),(x,y) \big)=ax,
    \end{equation}
    nous pouvons faire
    \begin{equation}
        f(x,y)=\begin{pmatrix}
            x    \\ 
            \lambda(x,y)    
        \end{pmatrix}
    \end{equation}
    où \( \lambda\) est n'importe quoi.
\end{example}

\ifbool{isGiulietta}{
\begin{remark}
    Des preuves alternatives.
    \begin{enumerate}
        \item
            En utilisant un peut plus d'indices et un peu plus de mots comme «tenseurs», peut être trouvée dans \cite{BIBooMBAGooNCUaMT}. Le fait que la preuve donnée soit tensorielle me fait penser que le résultat peut encore être généralisé.
        \item
            Et encore une autre preuve, utilisant des techniques de groupes de Lie sera la proposition~\ref{PROPooDVIWooAFDNPy}.
    \end{enumerate}
\end{remark}
}
{}

\begin{theorem}
    Soit un espace vectoriel \( E\) muni d'une forme quadratique \( q\). Soit une isométrie \( f\colon E\to E\) pour \( q\). Alors
    \begin{enumerate}
        \item
            si \( f(0)=0\), alors \( f\) est linéaire;
        \item
            si \( f(0)\neq 0\) alors \( f\) est affine\footnote{Définition \ref{DEFooUAWZooXcMKve}.}.
            
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous considérons la forme bilinéaire associée \( b\). Si \( f(0)=0\), nous savons par le lemme~\ref{LemewGJmM} que \( b\big( f(x),f(y) \big)=b(x,y)\). La proposition \ref{ThoDsFErq} nous dit alors que \( f\) est linéaire.


    Si \( f(0)\neq 0\), alors nous posons \( g(x)=f(x)-f(0)\) qui vérifie \( g(0)=0\) et
    \begin{equation}
        q\big( g(x)-g(y) \big)=q\big( f(x)-f(0)-f(y)+f(0) \big)=q(x-y).
    \end{equation}
    Nous pouvons donc appliquer le premier point à \( g\), déduire que \( g\) est linéaire et donc que \( f\) est affine. C'est la caractérisation du lemme \ref{LEMooZZAIooOMiayy} des fonctions affines.
\end{proof}

Nous pouvons maintenant particulariser tout cela au cas de \( \eR^n\) muni du produit scalaire usuel et de la norme associée pour voir quel résultat nous avons à peine prouvé.

\begin{lemma}[\cite{ooYPVPooYGSlNU}]        \label{LEMooJPYZooHETCqt}
    Une isométrie d'un espace vectoriel normé de dimension finie est bijective.
\end{lemma}

\begin{proof}
    Si \( f\colon E\to E\) est une isométrie, elle est linéaire par le théorème~\ref{ThoDsFErq}. Elle vérifie également \( \| f(x) \|=\| x \|\), et donc \( f(x)=0\) si et seulement si \( x=0\), c'est-à-dire que \( f\) est injective. Elle est alors bijective par le corolaire~\ref{CORooCCXHooALmxKk} du théorème du rang.
\end{proof}

Nous notons ici \( T(n)\) le groupe des translations sur \( \eR^n\). Un élément de \( T(n)\) est une translation \( \tau_v\) donnée par un vecteur \( v\) et agissant sur \( \eR^n\) par
\begin{equation}
    \begin{aligned}
        \tau_v\colon \eR^n&\to \eR^{n} \\
        x&\mapsto x+v.
    \end{aligned}
\end{equation}
Ce groupe est isomorphe au groupe abélien \( (\eR^n,+)\), et nous allons souvent identifier \( \tau_v\) à \( v\).

Vous savez par culture générale que les isométries de \( \eR^n\) pour le produit scalaire usuel sont les matrices orthogonales. En voici une petite généralisation (pensez à \( \eta=\mtu\) dans le cas du produit scalaire usuel).
\begin{proposition}     \label{PROPooSYQMooEnZFdp}
    Soit une forme bilinéaire \( b\) sur \( \eR^n\) de matrice symétrique \( \eta\). Si \( A\) est la matrice d'une application linéaire \( \eR^n\to \eR^n\) telle que
    \begin{equation}
        b(Ax,Ay)=b(x,y)
    \end{equation}
    pour tout \( x,y\in\eR^n\), alors
    \begin{equation}
        A^t\eta A=\eta.
    \end{equation}
\end{proposition}

\begin{proof}
    En suivant la formule générale \eqref{EQooQFMWooVKVLMx},
    \begin{equation}
            b(Ax,Ay)=\sum_{ij} \eta_{ij} (Ax)_i(Ay)_j=\sum_{ijkl}\eta_{ij}A_{ik}A_{jl}x_ky_l.
    \end{equation}
    En imposant que ce soit égal à \( \sum_{kl}\eta_{kl}\eta_{kl}x_ky_l\) pour tout \( x,y\) nous avons la contrainte
    \begin{equation}
        \sum_{ij}\eta_{ij}A_{ik}A_{jl}=\eta_{kl}
    \end{equation}
    qui signifie exactement \( A^t\eta A=\eta\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Signature, théorème de Sylvester}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Signature\cite{BIBooXOWGooAPWTfT}]       \label{DEFooWDCLooDkRYLK}
    Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( Q\) sur un espace vectoriel \( E\) de dimension finie \( n\). L'\defe{indice d'inertie}{indice d'inertie} de \( q\) est le nombre
    \begin{equation}
        q=\max\{ \dim(F)\tq Q(v)<0\,\forall v\in F\setminus\{ 0 \} \}.
    \end{equation}
    Nous définissons aussi
    \begin{equation}
        p=\max\{ \dim(G)\tq Q(v)>0\,\forall v\in G\setminus\{ 0 \} \}.
    \end{equation}
    Le couple \( (p,q)\) est la \defe{signature}{signature!forme quadratique} de \( Q\).
\end{definition}

\begin{definition}[Rang d'une forme quadratique]        \label{DEFooVITQooQaMaTF}
    Si \( Q\colon E\times E\to \eK\) est une forme quadratique, nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f_Q\colon E&\to E^* \\
            x&\mapsto \big[ y\mapsto B(x,y) \big]. 
        \end{aligned}
    \end{equation}
    Le \defe{rang}{rang d'une forme quadratique} de \( Q\) est le rang de l'application linéaire \( f_Q\).
\end{definition}

\begin{proposition}     \label{PROPooLRZQooSfprff}
    Le rang d'une forme quadratique est le rang de sa matrice dans n'importe quelle base.
\end{proposition}

\begin{proof}
    Nous considérons une forme quadratique \( Q\) sur l'espace vectoriel \( E\). Sa trace est, par définition, la trace de l'application linéaire \( f_Q\) de la définition \ref{DEFooVITQooQaMaTF}. Or cette dernière trace ne dépend pas des bases choisies sur \( E\) et \( E^*\). Nous la calculons donc maintenant.

    Soit une base \( \{ e_i \}\) de \( E\) ainsi que sa base duale \( \{ e_i^* \}\) de \( E^*\). Si \( v=\sum_kv_ke_k\in E\), alors
    \begin{equation}
        f_Q(e_i)v=\sum_kv_kB(e_i,e_k)=\sum_kQ_{ik}v_k
    \end{equation}
    où nous avons noté \( B\) la forme bilinéaire associée à \( Q\) et utilisé la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à la forme quadratique \( Q\). Nous avons donc \( f_Q(ei)=\sum_kQ_{ik}e_k^*\) ou encore
    \begin{equation}
        f_Q(e_i)_k=Q_{ik},
    \end{equation}
    ce qui signifie, par \ref{ITEMooKZYYooZPTkpq} que la matrice associée à \( f_Q\) est la matrice \( Q^t\).

    Le rang de \( f_Q\) est donc celui de \( Q^t\), qui est le même que celui de la matrice \( Q\) (ici, nous avons noté \( Q\) la matrice de la forme quadratique \( Q\)). Le rang de \( f_Q\) est celui de sa matrice par la proposition \ref{PROPooEGNBooIffJXc}.
\end{proof}


\begin{lemma}[\cite{BIBooXOWGooAPWTfT}]     \label{LEMooISHCooVDJEKo}
    Soient une forme quadratique \( Q\) ainsi que deux bases \( Q\)-orthogonales \( \{ e_1,\ldots, e_n \}\) et \( \{ e'_1,\ldots, e'_n \}\). Nous posons
    \begin{subequations}
        \begin{align}
            r&=\Card\{ e_i\tq q(e_i)>0 \}\\
            r'&=\Card\{ e_i\tq q(e'_i)>0 \}\\
            s&=\Card\{ e_i\tq q(e_i)<0 \}\\
            s'&=\Card\{ e_i\tq q(e'_i)<0 \}
        \end{align}
    \end{subequations}
    Alors \( r=r'\) et \( s=s'\).
\end{lemma}

\begin{proof}
    Nous posons    
    \begin{subequations}
        \begin{align}
            I&=\{ i\tq Q(e_i)>0 \}\\
            J&=\{ j\tq Q(e_j)<0 \}
        \end{align}
    \end{subequations}
    Nous commençons par prouver que \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre. Supposons pour cela que
    \begin{equation}
        \sum_{i\in I}x_ie_i+\sum_{j\in J}y_je'_j=0,
    \end{equation}
    et posons \( z=\sum_{i\in I}x_ie_i\). Nous avons
    \begin{equation}        \label{EQooWGKAooElpETd}
        Q(z)=\sum_{i\in I}x_i^2Q(e_i)\geq 0.
    \end{equation}
    Mais nous avons aussi \( z=-\sum_{j\in J}y_je'_j\), donc
    \begin{equation}        \label{EQooJYOCooZPXmTf}
        Q(z)=\sum_{j\in J}y_j^2Q(e'_j)\leq 0.
    \end{equation}
    Donc \( Q(z)=0\). Vu \eqref{EQooWGKAooElpETd}, et le fait que \( Q(e_i)>0\), avoir \( Q(z)=0\) impose \( x_i=0\) pour tout \( i\). La relation \eqref{EQooJYOCooZPXmTf} nous donne aussi immédiatement que les \( y_j\) sont nuls. Donc la partie \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre.

    Le lemme \ref{LemytHnlD} nous indique qu'une partie libre est toujours de cardinal plus petit ou égal à la dimension de l'espace\footnote{Ici nous utilisons l'hypothèse que \( V\) est de dimension finie.}. Tout ça pour dire que
    \begin{equation}
        \underbrace{\Card(I)}_{=r}+\underbrace{\Card(J)}_{=n-r'}\leq n,
    \end{equation}
    et donc \( r\leq r'\). 

    Le même raisonnement, en partant de \( I=\{ i\tq Q(e_i)\leq 0 \}\) et de \( J=\{ j\tq Q(e'_j)>0 \}\), prouve que \( r'\leq r\).

    La preuve de \( s=s'\) est du même tonneau.
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooOQIDooPSOeXL}
    Soit une forme quadratique \( Q\) sur \( E\). Si \( F\) est un sous-espace de \( E\), alors
    \begin{equation}
        \dim(F)+\dim(F^{\perp})\geq n
    \end{equation}
    où \( F^{\perp}\) est l'orthogonal par rapport à \( Q\).
\end{lemma}

\begin{proof}
    Nous posons \( p=\dim(F)\). Nous considérons une base \( \{ f_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \{ f_i \}_{i=1,\ldots, p}\) est une base de \( F\)\footnote{Théorème de la base incomplète, \ref{THOooOQLQooHqEeDK}.}. Nous posons
    \begin{equation}
        \begin{aligned}
            \phi\colon E&\to F \\
            x&\mapsto \sum_{i=1}^pB(x,f_i)f_i 
        \end{aligned}
    \end{equation}
    où \( B\) est la forme bilinéaire associée à \( Q\). Ce \( \phi\) est une application linéaire à qui nous appliquons le théorème du rang \eqref{EQooUEOQooLySRiE} :
    \begin{equation}        \label{EQooCLWLooCFxVDq}
        \dim(E)=\rang(\phi)+\dim\big( \ker(\phi) \big).
    \end{equation}
    Mais vu que l'image de \( \phi\) est dans \( F\), nous avons \( \rang(\phi)\leq \dim(F)\). De plus, \( \ker(\phi)=F^{\perp}\). Donc \eqref{EQooCLWLooCFxVDq} devient
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooUOZOooYvEcji}
    Soit un espace vectoriel \( E\) de dimension finie et un sous-espace \( F\) sur lequel la forme quadratique \( Q\) est strictement définie positive ou négative. Alors
    \begin{equation}
        E=F\oplus F^{\perp}.
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord nous montrons que \( F\cap F^{\perp}=\{ 0 \}\). Si \( v\neq 0\) est dans \( F\), alors \( Q(v)>0\), et donc \( v\) n'est pas dans \( F^{\perp}\). Donc \( F\cap F^{\perp}\subset \{ 0 \}\). L'inclusion inverse est immédiate.

    Nous avons vu dans le lemme \ref{LEMooOQIDooPSOeXL} que
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
    Vu que \( F\) et \( F^{\perp}\) n'ont pas d'intersection autre que \( \{ 0 \}\), nous avons
    \begin{equation}
        \dim(E)\geq\dim(F\oplus F^{\perp}) = \dim(F)+\dim(F^{\perp}) \geq\dim(E).
    \end{equation}
    Toutes ces inégalités sont donc des égalités et \( \dim(E)=\dim(F)+\dim(F^{\perp})\).
\end{proof}



%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équivalence de formes quadratiques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Équivalence de forme quadratique\cite{BIBooWVWZooZqliJt}]        \label{DEFooOLWYooMwhMJp}
    Deux formes quadratiques $Q$ et $Q'$ sont \defe{équivalentes}{équivalence de forme quadratiques} si il existe une application linéaire inversible \( \phi\) telle que \( Q'=Q\circ \phi\).
\end{definition}

\begin{proposition}[\cite{BIBooXOWGooAPWTfT}]       \label{PROPooBWXMooLsgyKm}
    Deux formes quadratiques sont équivalentes\footnote{Définition \ref{DEFooOLWYooMwhMJp}.} si et seulement si elles ont même signature.
\end{proposition}

\index{matrice!semblables}
\index{forme!quadratique}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Invariance de la trace}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRMYQooWkEpJJ}
    Soit une application linéaire \( f\). Si la matrice de \( f\) dans une base est \( A\) et est \( B\) dans une autre base, alors
    \begin{equation}
        \trace(A)=\trace(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Les matrices \( A\) et \( B\) sont liées par la proposition \ref{PROPooNZBEooWyCXTw} : \( B=Q^{-1}AQ\) où \( Q\) est la matrice qui lie les vecteurs des deux bases. L'invariance cyclique de la trace donnée en le lemme \ref{LEMooUXDRooWZbMVN} implique que
    \begin{equation}
        \trace(B)=\trace(Q^{-1}AQ)=\trace(QQ^{-1}A)=\trace(A).
    \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit scalaire, produit hermitien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}\label{DefVJIeTFj}
    Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel réel est une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition~\ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

La définition suivante est utile pour celles qui veulent faire de la relativité\footnote{Voir le théorème \ref{THOooYHDWooWxVovH} qui établit les transformations de Lorentz.}.
\begin{definition}      \label{DEFooLPBGooXLxubc}
    Un \defe{produit pseudo-scalaire}{produit pseudo-scalaire} sur un espace vectoriel réel est une forme bilinéaire et symétrique.
\end{definition}

\begin{definition}      \label{DEFooZBWTooIqXwRp}
    Nous dirons que deux vecteurs sont \defe{orthogonaux}{orthogonal} lorsque leur produit scalaire\footnote{Définition \ref{DefVJIeTFj}.} est nul. Nous écrivons que \( u\perp v\) lorsque \( \langle u, v\rangle =0\).

    Si \( \{ e_i \}_{i=1,\ldots, n}\) est une base de \( E\), nous disons qu'elle est \defe{orthonormée}{base orthonormée} si 
    \begin{equation}
        \langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{definition}

\begin{lemma}       \label{LEMooLPUFooVCvnwW}
    Un produit scalaire est toujours non dégénéré\footnote{Définition \ref{DEFooNUBFooLfCqaK}.}.
\end{lemma}


Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj} 
    Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel de dimension finie muni d'un produit scalaire (définition~\ref{DefVJIeTFj}).
\end{definition}
Avouez que c'est drôle qu'un espace vectoriel est euclidien lorsqu'il possède une \emph{multiplication} alors qu'un anneau est euclidien lorsqu'il possède une \emph{division} (voir la définition~\ref{DefAXitWRL}). C'est pas très profond, mais si ça peut vous servir de moyen mnémotechnique\ldots


\begin{propositionDef}[Produit scalaire dans \( \eR^n\)]     \label{PROPooSKVRooDGVCYj}
    Si \( x,y\in \eR^n\), nous définissons
    \begin{equation}        \label{EQooFITHooEXDCGd}
        x\cdot y=\sum_{i=1}^n x_iy_i =x_1y_1+x_2y_2+\cdots+x_ny_n.
    \end{equation}
    C'est un produit scalaire\footnote{Définition \ref{DefVJIeTFj}.} qui vérifie
    \begin{description}
        \item[Symétrie] \( x\cdot y=y\cdot x\);
        \item[Linéarité] \( (\lambda x+\mu x')\cdot y=\lambda(x\cdot y)+\mu(x'\cdot y)\) pour tout \( \lambda\) et \( \mu\) dans \( \eR\);
        \item[Défini positif] \( x\cdot x\geq 0$ et $x\cdot x=0\) si et seulement si \( x=0\).
    \end{description}
    Ce produit scalaire est le \defe{produit scalaire}{produit scalaire sur \( \eR^n\)} qui sera toujours considéré. C'est de lui qui découle toujours la norme, et la topologie de \( \eR^n\). Il sera aussi souvent noté \( \langle x, y\rangle \).
\end{propositionDef}


Calculons par exemple le produit scalaire de deux vecteurs de la base canonique : \( \langle e_i, e_j\rangle\). En utilisant la formule de définition et le fait que \( (e_i)_k=\delta_{ik}\), nous avons
\begin{equation}
    \langle e_i, e_j\rangle =\sum_{k=1}^m\delta_{ik}\delta_{jk}.
\end{equation}
Nous pouvons effectuer la somme sur $k$ en remarquant qu'à cause du \( \delta_{ik}\), seul le terme avec \( k=i\) n'est pas nul. Effectuer la somme revient donc à remplacer tous les \( k\) par des \( i\) :
\begin{equation}
    \langle e_i, e_j\rangle =\delta_{ii}\delta_{ji}=\delta_{ji}.
\end{equation}

Une des propriétés intéressantes du produit scalaire est qu'il permet de décomposer un vecteur dans une base, comme nous le montre la proposition suivante.

\begin{proposition}     \label{PropScalCompDec}
    Si nous notons \( v_i\) les composantes du vecteur \( v\), c'est-à-dire si \( v=\sum_{i=1}^m v_ie_i\), alors nous avons \( v_j=\langle v, e_j\rangle\).
\end{proposition}

\begin{proof}
    \begin{equation}    \label{Eqvejscalcomp}
        v\cdot e_j=\sum_{i=1}^m\langle v_ie_i, e_j\rangle =\sum_{i=1}^mv_i\langle e_i, e_j\rangle =\sum_{i=1}^mv_i\delta_{ij}
    \end{equation}
    En effectuant la somme sur \( i\) dans le membre de droite de l'équation \eqref{Eqvejscalcomp}, tous les termes sont nuls sauf celui où \( i=j\); il reste donc
    \begin{equation}
        v\cdot e_j=v_j.
    \end{equation}
\end{proof}

Le produit scalaire ne dépend en réalité pas de la base orthogonale choisie.

\begin{lemma}
    Si \( \{ e_i \}\) est la base canonique, et si \( \{ f_i \}\) est une autre base orthonormale\footnote{Définition \ref{DEFooZBWTooIqXwRp}.}, alors si \( u\) et \( v\) sont deux vecteurs de \( \eR^m\), nous avons
    \begin{equation}
        \sum_i u_iv_j=\sum_iu'_iv'_j
    \end{equation}
    où \( u_i\) sont les composantes de \( u\) dans la base \( \{ e_i \}\) et \( u'_i\) sont celles dans la base \( \{ f_i \}\).
\end{lemma}

\begin{proof}
    La preuve demande un peu d'algèbre linéaire. Étant donné que $\{ f_i \}$ est une base orthonormale, il existe une matrice $A$ orthogonale ($AA^t=\mtu$) telle que $u'_i=\sum_jA_{ij}u_j$ et idem pour $v$. Nous avons alors
    \begin{equation}
      \begin{aligned}[]
        \sum_iu'_iv'_j  & =\sum_i\left( \sum_jA_{ij} u_j\right)\left( \sum_k A_{ik}v_k \right) \\
                        & =\sum_{ijk}A_{ij}A_{ik}u_jv_k                                        \\
                        & =\sum_{jk}\underbrace{\sum_i(A^t)_{ji}A_{ik}}_{=\delta_{jk}}u_jv_k   \\
                        & =\sum_{jk}\delta_{jk}u_jv_k                                          \\
                        & =\sum_ku_jv_k.
      \end{aligned}
    \end{equation}
\end{proof}

Cette proposition nous permet de réellement parler du produit scalaire entre deux vecteurs de façon intrinsèque sans nous soucier de la base dans laquelle nous exprimons les vecteurs.

\begin{definition}[\cite{ooJUXBooVrwvfP}]  \label{DefMZQxmQ}
    Soit \( E\) est un espace vectoriel sur \( \eC\). Une application \( \langle ., .\rangle \colon E\times E\to \eC\) est \defe{sesquilinéaire à droite}{sesquilinéaire} si pour tout \( x,y\in E\) et pour tout \( \lambda\in \eC\),
    \begin{enumerate}
        \item
            \( \langle \lambda x, y\rangle =\lambda\langle x,y, \rangle =\langle x, \bar\lambda y\rangle \),
        \item
            \( \langle x+y, z\rangle =\langle x, y\rangle+\langle y, z\rangle  \),
        \item
            \( \langle x, y+z\rangle =\langle x, y\rangle +\langle x, z\rangle \).
    \end{enumerate}
    Cette forme est \defe{hermitienne}{hermitienne} si de plus
    \begin{equation}
        \langle x, y\rangle =\overline{ \langle y, x\rangle  }.
    \end{equation}
    Un \defe{produit hermitien}{produit hermitien} est une forme hermitienne strictement définie positive, c'est-à-dire telle que \( \langle x, x\rangle \geq 0\) pour tout \( x\in E\) et \( \langle x, x\rangle =0\) si et seulement si \( x=0\).
\end{definition}

\begin{normaltext}
    Les normes associées aux produits scalaires font intervenir une racine carré, et donc devront être données plus tard. Voir la proposition \ref{PROPooMWUCooMbJuaJ} sur \( \eC^n\) et la définition \ref{PROPooSKVRooDGVCYj} sur \( \eR^n\).
\end{normaltext}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooMWUCooMbJuaJ}
    Nous considérons \(\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\).
    \begin{enumerate}
        \item
            La formule, pour \( x,y\in \eC^n\),
    \begin{equation}    \label{EqFormSesqQrjyPH}
        \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
    \end{equation}
    définit une une forme sesquilinéaire sur \( \eC^n\).
\item
    L'ensemble \( \eC^n\) devient un espace vectoriel hermitien.
    \end{enumerate}
\end{proposition}


\begin{proposition}     \label{PROPooZKWXooWmEzoA}
    Soit une application linéaire \( A\colon \eR^n\to \eR^n\). Nous considérons le produit scalaire usuel\footnote{Définition \ref{PROPooSKVRooDGVCYj}.} sur \( \eR^n\). Alors :
    \begin{enumerate}
        \item
            Les éléments de matrice de \( A\) sont donnés par \( A_{ij}=e_i\cdot Ae_j\).
        \item
            Nous avons la formule \( x\cdot Ay=\sum_{kl}A_{kl}x_ky_l \).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Pour \( Ae_j\) nous utilisons la formule \ref{EQooOKOJooYgteNP} avec des notations plus décontractées : \( Ae_j=\sum_kA_{kj}e_k\). Ensuite nous faisons un calcul avec la formule \eqref{EQooFITHooEXDCGd} :
    \begin{equation}
        e_i\cdot Ae_j=e_i\cdot \sum_kA_{kj}e_k=\sum_{k}A_{kj}\delta_{i,k}=A_{ij}.
    \end{equation}
    La seconde formule à prouver est du même tonneau, en utilisant cette fois la formule \eqref{EQooAXRJooUwHbjB} :
    \begin{equation}
        x\cdot Ay=\sum_kx_k(Ay)_k=\sum_{kl}x_kA_{kl}y_l=\sum_{kl}A_{kl}x_ky_l.
    \end{equation}
\end{proof}

La proposition suivante est une version plus «pragmatique» de la proposition \ref{PropXrTDIi}.
\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooNITTooCYcrrT}
    Soient un espace euclidien\footnote{Qui possède un produit scalaire, définition \ref{DefLZMcvfj}.} de dimension finie \( V\) ainsi qu'un sous-espace \( M\). Nous posons
    \begin{equation}
        M^{\perp}=\{ x\in V\tq x\cdot y=0\forall y\in M \}.
    \end{equation}
    Alors \( M\oplus M^{\perp}=V\).
\end{proposition}

\begin{proof}
    D'abord si \( x\in M\cap M^{\perp}\), alors \( x\cdot x=0\) et donc \( x=0\). Donc nous avons déjà \( M\cap M^{\perp}=\{ 0 \}\). Nous considérons une base \( \{b_1,\ldots, b_k\}\) de \( M\), et nous définissons l'application linéaire
    \begin{equation}
        \begin{aligned}
            f\colon V&\to \eR^k \\
            x&\mapsto (x\cdot b_1,\ldots, x\cdot b_k). 
        \end{aligned}
    \end{equation}
    Nous avons que \( M^{\perp}=\ker(f)\). Le théorème du rang \ref{ThoGkkffA} nous indique que
    \begin{equation}
        \dim(V)=\dim\big( \ker(f) \big)+\dim\big( \Image(f) \big)\leq \dim(M^{\perp})+k=\dim(M^{\perp})+\dim(M).
    \end{equation}
    Une justification : vu que \( f\) prend ses valeurs dans \( \eR^k\), la dimension de son image est majorée par \( k\).

    Nous en déduisons que 
    \begin{equation}
        \dim(M)+\dim(M^{\perp})\geq\dim(V),
    \end{equation}
    et la proposition \ref{PROPooCASNooEqisqa} nous permet de conclure que \( M\oplus M^{\perp}=V\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : pas d'approche naïve}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooGPXVooEYwIiJ}

Il est légitime, si \( t\colon E\to E\) est une application linéaire, de dire que sa transposée soit l'application linéaire \( t^t\colon E\to E\) dont la matrice est la matrice transposée de celle de \( t\). Lorsque nous travaillons sur \( \eR^n\) muni de la base canonique, cela ne pose pas de problème et nous pouvons écrire des égalités du type \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).

\begin{proposition}[Matrice transposée et produit scalaire]     \label{PROPooNARVooEuhweD}
    Soit une matrice réelle \( A\). En utilisant l'application linéaire associée\footnote{Définition \ref{DEFooJVOAooUgGKme}. Ici nous considérons la base canonique sur \( \eR^n\)} \( f_A\colon \eR^n\to \eR^n\), nous avons
    \begin{equation}
        x\cdot f_A(y)=f_{A^t}(x)\cdot y.
    \end{equation}
    Cette formule est souvent écrite \( x\cdot Ay=A^tx\cdot y\) ou \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).
\end{proposition}

\begin{proof}
    Il s'agit d'un calcul utilisant la formule \eqref{EQooBVGHooJhFbMs} et le produit scalaire \eqref{EQooFITHooEXDCGd} :
    \begin{equation}
        x\cdot f_A(y)=\sum_ix_i\big( f_A(y) \big)_i=\sum_ix_i\sum_jA_{ij}y_j=\sum_{ij}A^t_{ji}x_iy_j=\sum_jf_{A^t}(x)_jy_j=f_{A^t}(x)\cdot y.
    \end{equation}
\end{proof}

Hélas nous allons voir que cette façon de définir une transposée est mauvaise.

Soit une application linéaire \( t\colon E\to E\) de matrice \( A\) dans la base \( \{ e_i \}_{i=1,\ldots, n}\) et de matrice \( B\) dans la base \( \{ f_{\alpha} \}_{\alpha=1,\ldots, n}\). Nous notons \( Q\) la matrice de passage d'une base à l'autre :
\begin{equation}
    e_i=\sum_{\alpha}Q_{\alpha i}^{-1}f_{\alpha}.
\end{equation}

Nous nommons \( t_1\) l'application linéaire associée à \( A^t\) dans la base \( \{ e_i \}\) et \( t_2\) l'application linéaire associée à la matrice \( B^t\) dans la base \( \{ f_{\alpha} \}\). Définir la transposée d'une application linéaire comme étant l'application linéaire associée à la transposée de sa matrice ne sera une bonne définition que si \( t_1=t_2\).

La première chose facile à voir est
\begin{equation}        \label{EQooAMHPooUQEkJo}
    t_1(e_i)_j=\sum_k(A^t)_{jk}(e_i)_k=A^t_{ji}=A_{ij}.
\end{equation}
Pour calculer \( t_2(e_i)_j\), c'est un peu plus laborieux :
\begin{subequations}
    \begin{align}
        t_2(e_i)  & =\sum_{\alpha}Q_{\alpha i}^{-1} t_2(f_\alpha)=\sum_{\beta\gamma\alpha}Q_{\alpha i}^{-1}B^t_{\gamma\beta}\underbrace{(t_{\alpha})_{\beta}}_{\delta_{\alpha,\beta}}f_{\gamma}=\sum_{\beta\gamma}Q_{\beta i}^{-1}B^t_{\gamma\beta}f_{\gamma}  \\
                  & =(B^tQ^{-1})_{\gamma i}Q_{j\gamma}e_j\\
                  & =\sum_j(QB^tQ^{-1})_{ji}e_j.
    \end{align}
\end{subequations}
Donc \( t_2(e_i)_j=(QB^tQ^{-1})_{ji}\). En tenant compte du fait que \( B=Q^{-1}AQ\) nous avons
\begin{equation}
    t_2(e_i)_j=(QQ^tA^t(Q^{-1})^tQ^{-1})_{ji}.
\end{equation}
Ceci est égal à l'expression \eqref{EQooAMHPooUQEkJo} lorsque \( Q^t=Q^{-1}\). Nous voyons que confondre transposée d'une application linéaire avec transposée de la matrice associée n'est valable que si nous sommes certain de ne considérer que des changements de base par des matrices orthogonales.

C'est la situation typique dans laquelle nous nous trouvons lorsque nous considérons des applications linéaires sur \( \eR^n\) muni de la base canonique, et que nous n'avons aucune intention de changer de base, et encore moins de chercher une base non orthonormale. Cette situation est clairement la situation la plus courante.

\begin{example}[\cite{ooLIOMooBuCPUS}]
    Soit la base canonique \( \{ e_1,e_2 \}\) de \( \eR^2\). Nous considérons l'application linéaire \( t\colon \eR^2\to \eR^2\) définie par
    \begin{subequations}
        \begin{align}
            t(e_1)  & =e_1    \\
            t(e_2)  & =0.
        \end{align}
    \end{subequations}
    La matrice de \( t\) dans cette base est
    \begin{equation}
        A=\begin{pmatrix}
            1    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Elle est symétrique : elle vérifie \( A^t=A\). Si nous comptions sur la transposée de matrice pour définir la transposée de \( t\), nous aurions \( t^t=t\).

    Soit maintenant la base \( f_1=e_1\), \( f_2=e_1+e_2\). Nous avons \( t(f_1)=f_1\) et
    \begin{equation}
        t(f_2)=t(e_1)+t(e_2)=e_1=f_1.
    \end{equation}
    Donc la matrice de \( t\) dans cette base est
    \begin{equation}
        B=\begin{pmatrix}
            1    &   1    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Et là, nous avons \( B^t\neq B\). Donc en comptant sur cette base pour définir la transposée de \( t\) nous aurions \( t^t\neq t\).
\end{example}

\begin{normaltext}      \label{NooMZVRooExWVKJ}
    Autrement dit, la façon «usuelle» de voir la transposée d'une application linéaire, ne fonctionne dans les livres pour enfants uniquement parce qu'on y considère toujours \( \eR^n\) muni de la base canonique ou de bases orthonormées.

    Notons que nous avons tout de même les notions d'opérateur adjoint et autoadjoint pour parler d'application orthogonale sans passer par la transposée, voir~\ref{DEFooYKCSooURQDoS}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : la bonne approche}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooZLPAooKTITdd}
    Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
    \begin{equation}
        f^t(\omega)(x)=\omega\big( f(x) \big).
    \end{equation}
    pour tout \( \omega\in F^*\) et \( x\in E\).
\end{definition}

\begin{lemma}       \label{LEMooEMNNooPquZMg}
    Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).

    Autrement dit, en utilisant l'application \( \psi\colon \eM\to \aL(F^*,E^*)\) de la proposition \ref{PROPooGXDBooHfKRrv},
    \begin{equation}
        \psi(A^t)=f^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Attention aux indices, ça va chauffer\footnote{Et merci à Alain Vigne pour m'avoir fait remarquer qu'il fallait mettre de l'ordre dans les indices.}.

    Nous allons montrer que \( f^t=\psi(A^t)\) sur la base \( \{ g_i^* \}\), et pour cela nous appliquons \( f^t(g_i^*)\) à \( x\in E\) :
    \begin{subequations}
        \begin{align}
            f^t(g_i^*)x & =g_i^*\big( f(x) \big)            & \text{Définition  \ref{DefooZLPAooKTITdd}}      \\
                & =g_i^*\big( \sum_kx_kf(e_k) \big)                                                           \\
                & =g_i^*\big( \sum_{kl}x_kA_{lk}g_l \big)   & \text{eq. \eqref{EQooOKOJooYgteNP}}             \\
                & =\sum_{kl}x_kA_{lk}\underbrace{g_i^*(g_l)}_{=\delta_{i,l}}                                  \\
                & =\sum_kx_kA_{ik}                            \\
                & =\sum_k(A^t)_{ki}x_k                        \\
                & =\sum_k(A^t)_{ki}e^*_k(x).
        \end{align}
    \end{subequations}
    Voila. Donc nous avons
    \begin{equation}
        f^t(g_i^*)=\sum_k(A^t)_{ki}e^*_k=\psi(A^t)g_i^*.
    \end{equation}
\end{proof}

\begin{normaltext}
Intuitivement,  les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carrée de déterminant non nul.

Nous donnons maintenant une vraie preuve de ce résultat.
\end{normaltext}

\begin{lemma}[\cite{BIBooETROooZIGZkN}]   \label{LemSEpTcW}
    Si \( f\colon E\to F\) est une application linéaire, alors
    \begin{equation}
        \rang(f)=\rang(f^t).
    \end{equation}
\end{lemma}

\begin{proof}
    Soient \( n=\dim(E)\) et \( r=\dim(F)\).
    Nous posons \( \dim\ker(f)=p\) et donc \( \rang(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
    \begin{equation}
        g_i=f(e_{p+i})
    \end{equation}
    pour \( i=1,\ldots, n-p\). C'est-à-dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
    \begin{equation}
        \sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
    \end{equation}
    alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), ce qui signifierait que \( \sum_ka_ke_{p+k}\) se trouve dans le noyau de \( f\), ce qui est impossible par construction de la base \( \{ e_i \}_{i=1,\ldots, n}\). Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous pouvons les compléter en une base de \( F\) :
    \begin{equation}
        \{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}.
    \end{equation}

    Nous prouvons maintenant que \( \rang(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) forment une partie libre (et donc l'espace image de \( f^t\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
    \begin{equation}
        f^t(g^*_i)(e_k)=g_i^*(f(e_k)),
    \end{equation}
    Si \( k=1,\ldots, p\), alors \( f(e_k)=0\) et donc \( g_i^*(f(e_k))=0\); si \( k=p+l\) alors
    \begin{equation}
        f^t(g_i^*)(e_k)=g_i^*(f(e_{k+l}))=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
    \end{equation}
    Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
    \begin{equation}
        \rang(f^t)\geq n-p=\rang(f).
    \end{equation}
    En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
    \begin{equation}
        \rang\big( (f^t)^t \big)\geq \rang(f^t)
    \end{equation}
    et donc, sachant que \( (f^t)^t=f\), nous obtenons \( \rang(f)=\rang(f^t)\).

\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]        \label{PropWOPIooBHFDdP}
    Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
    \begin{equation}
        \Image(f^t)=\ker(f)^{\perp}.
    \end{equation}
\end{proposition}

\begin{proof}
    Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est-à-dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est-à-dire que \( \omega\in (\ker f)^{\perp}\).

    Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
    \begin{subequations}
        \begin{align}
            \dim\big( \Image(f^t) \big)&=\rang(f^t)\\
            &=\rang(f)  &\text{lemme~\ref{LemSEpTcW}}\\
            &=\dim(E)-\dim\ker(f)   &\text{théorème~\ref{ThoGkkffA}}\\
            &=\dim\big( (\ker f)^{\perp} \big)  &\text{proposition~\ref{PropXrTDIi}}.
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}[\cite{ooEPEFooQiPESf}]
    Soit \( \eK\) un corps, \( E\) et \( F\) deux \( \eK\)-espaces vectoriels de dimension finie et une application linéaire \( f\colon E\to F\). L'application \( f\) est injective si et seulement si sa transposée\footnote{Définition~\ref{DefooZLPAooKTITdd}.} \( f^t\) est surjective.
\end{lemma}

\begin{proof}
    Supposons que \( f\) soit injective. Alors par le lemme~\ref{LEMooDAACooElDsYb}, il existe \( g\colon F\to E\) tel que \( g\circ f=\id|_E\). Nous avons alors aussi \( (g\circ f)^t=\id|_{E^*}\), mais \( (g\circ f)^t=f^t\circ g^t\), donc \( f^t\) est surjective.

    Inversement, nous supposons que \( f^t\colon F^*\to E^*\) est surjective. Alors en nous souvenant que \( E\) et \( F\) sont de dimension finie et en faisant jouer les identifications \( (f^t)^t=f\) et \( (E^*)^*=E\) nous savons qu'il existe \( s\colon E^*\to F^*\) tel que \( f^t\circ s=\id|_{E^*}\). En passant à la transposée,
    \begin{equation}
        s^t\circ f=\id|_{E},
    \end{equation}
    qui implique que \( f\) est injective.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}
    Soit \( E=\Poly_n(\eR)\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in \Poly_n(\eR)^*\),
\begin{equation}
    f_i(P)=P(a_i).
\end{equation}
    La partie \( \{ f_1,\ldots, f_n \}\) est une base de \( \Poly_n(\eR)^*\).

    Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme de Lagrange} aux points \( (a_i)\) sont les polynômes de la base préduale de la base \( \{ f_i \}\).
\end{lemmaDef}

\begin{proof}
    Nous prouvons que l'orthogonal est réduit au singleton nul :
    \begin{equation}
        \Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}.
    \end{equation}
    La proposition~\ref{PropXrTDIi} conclura. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

\begin{proposition}
    Les polynômes de Lagrange aux points \( (a_i)_{i=1,\ldots, n}\) sont donnés par
    \begin{equation}
        P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de vérifier que \( f_j(P_i)=\delta_{i,j}\). Nous avons
    \begin{equation}
        f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
    \end{equation}
    Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dual de \texorpdfstring{$ \eM(n,\eK)$}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
    Soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
    \begin{equation}
        \begin{aligned}
            f_A\colon \eM(n,\eK)&\to \eK \\
            M&\mapsto \tr(AM).
        \end{aligned}
    \end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}


\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eK)&\to \eM(n,\eK)^* \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM(n,\eK)\) et \( \left(\eM(n,\eK)\right)^*\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{i,k}\delta_{j,l}\) :
    \begin{equation}
          0 = f_A(E_{ij})
            = \sum_{k}(AE_{ij})_{kk}
            = \sum_{kl}A_{kl}(E_{ij})_{lk}
            = \sum_{kl}A_{kl}\delta_{i,l}\delta_{j,k}
            = A_{ij}.
    \end{equation}
    Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soient un corps \( \eK\) ainsi qu'une application \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( M,N\in \eM(n,\eK)\) on ait
    \begin{equation}
        \phi(MN)=\phi(NM).
    \end{equation}
    Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
    La proposition~\ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(MN)=f_A(NM)\), c'est-à-dire
    \begin{equation}
        \Tr(AMN)=\Tr(ANM)
    \end{equation}
    pour toutes matrices \( M, N\in \eM(n,\eK)\). L'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.} appliqué au membre de droite nous donne \( \Tr(AMN)=\Tr(MAN)\), ce qui signifie que
    \begin{equation}
        \Tr\big( (AM-MA)N \big)=0
    \end{equation}
    ou encore que \( f_{AM-MA}=0\), et ce, pour toute matrice \( M\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
    \begin{equation}
        AM=MA
    \end{equation}
    pour tout \( M\in\eM(n,\eK)\). En particulier, en prenant pour \( M \) les fameuses matrices \( E_{ij}\) et en calculant un peu,
    \begin{equation}
        A_{li}\delta_{j,m}=\delta_{i,l}A_{jm}
    \end{equation}
    pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
    \begin{equation}
        \phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
    \end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]       \label{CorICUOooPsZQrg}
    Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
    Soit \( \mH\) un hyperplan de \( \eM(n,\eK)\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition~\ref{PropHOjJpCa} nous donne \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme~\ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r  &   0     \\
            0       &   0
        \end{pmatrix}.
    \end{equation}
    Pour tout \( M\in \eM(n,\eK)\) nous avons
    \begin{equation}
        \phi(M)=\Tr(AM)=\Tr(PJ_rQM)=\Tr(J_rQMP),
    \end{equation}
    la dernière égalité découlant de l'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.}. Ce que nous cherchons est \( M\in \GL(n,\eK)\) telle que \( \phi(M)=0\). Nous commençons par trouver \( N\in\GL(n,\eK)\) telle que \( \Tr(J_rN)=0\). Celle-là est facile : c'est
    \begin{equation}
        N=\begin{pmatrix}
            0           &   1 \\
            \mtu_{n-1}  &   0
        \end{pmatrix}.
    \end{equation}
    Les éléments diagonaux de \( J_rN\) sont tous nuls. Par conséquent en posant \( M=Q^{-1}NP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation et trigonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[matrices semblables\cite{MonCerveau}] \label{DefCQNFooSDhDpB}
    Nous définissons, sur l'ensemble \( \eM(n,\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\), la relation  \( A\sim B\) si et seulement si il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). 

    Cette relation est une relation d'équivalence.

    Deux matrices équivalentes en ce sens sont dites \defe{semblables}{matrices semblables}.
\end{propositionDef}

\begin{propositionDef}      \label{PROPooIXFSooZsFWHm}
    Soit un espace vectoriel \( E\). Nous définissons sur \( \End(E)\) la relation \( u\sim v\) si et seulement si il existe une application inversible \( A\colon E\to E\) telle que \( v=A^{-1}\circ u\circ A\).

    Cette relation est une relation d'équivalence.

    Deux endomorphismes équivalents en ce sens sont dits \defe{semblables}{applications linéaires semblables}.
\end{propositionDef}

\begin{proposition}     \label{PROPooBGJBooXlDYEv}
    Deux applications linéaires sont semblables si et seulement si leurs matrices sont semblables dans toute base.
\end{proposition}

\begin{lemma}
Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} est un invariant sous les similitudes. 
\end{lemma}

\begin{proof}
En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{P^{-1}AP} & = \det(P^{-1}AP-\lambda X)                     \\
                        & = \det\big( P(P^{-1}AP-\lambda X)P^{-1} \big)  \\
                        & = \det(A-\lambda X)
                        & = \chi_A.
    \end{align}
\end{subequations}
\end{proof}

La permutation de lignes ou de colonnes ne sont pas des similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\
            3    &   4
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\
            4    &   3
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=X^2-5X-2\) tandis que \( \chi_B=X^2-5X+2\) alors que le polynôme caractéristique est un invariant de similitude.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes nilpotents}
%---------------------------------------------------------------------------------------------------------------------------

La \defe{trace}{trace!matrice} d'une matrice \( A\in \eM(n,\eK)\) est la somme de ses éléments diagonaux :
\begin{equation}
    \tr(A)=\sum_{i=1}^nA_{ii}.
\end{equation}
Une propriété importante est son invariance cyclique.

\begin{lemma}   \label{LemhbZTay}
    Quelques propriétés de la trace.
    \begin{enumerate}
        \item
    Si \( A\) et \( B\) sont des matrices carrées, alors \( \tr(AB)=\tr(BA)\).
\item
    La trace est un invariant de similitude.
    \end{enumerate}
\end{lemma}

\begin{proof}
    C'est un simple calcul :
    \begin{equation}
            \tr(AB)=\sum_{ik}A_{ik}B_{ki}
            =\sum_{ik}A_{ki}B_{ik}
            =\sum_{ik}B_{ik}A_{ki}
            =\sum_i(BA)_{ii}
            =\tr(BA)
    \end{equation}
    où nous avons simplement renommé les indices \( i\leftrightarrow k\).

    En particulier, la trace est un invariant de similitude parce que \( \tr(ABA^{-1})=\tr(A^{-1} AB)=\tr(B)\) par l'invariance cyclique démontrée en \ref{LEMooUXDRooWZbMVN}\ref{ITEMooXDYQooAlnArd}.
\end{proof}
%TODOooQGISooCmwqdR Mettre la définition de la trace dans une proposition-définition.
La trace étant un invariant de similitude, nous pouvons donc définir la \defe{trace}{trace!endomorphisme} comme étant la trace de sa matrice dans une base quelconque. Si la matrice est diagonalisable, alors la trace est la somme des valeurs propres.

\begin{lemma}[\cite{fJhCTE}]   \label{LemzgNOjY}
    L'endomorphisme \( u\in\End(\eC^n)\) est nilpotent si et seulement si \( \tr(u^p)=0\) pour tout \( p\).
\end{lemma}

\begin{proof}
    Supposons que \( u\) est nilpotent. Alors ses valeurs propres sont toutes nulles et celles de \( u^p\) le sont également. La trace étant la somme des valeurs propres, nous avons alors tout de suite \( \tr(u^p)=0\).

    Supposons maintenant que \( \tr(u^p)=0\) pour tout \( p\). Le polynôme caractéristique \eqref{Eqkxbdfu} est
    \begin{equation}    \label{EqfnCqWq}
        \chi_u=(-1)^nX^{\alpha}(X-\lambda_1)^{\alpha_1}\ldots (X-\lambda_r)^{\alpha_r}.
    \end{equation}
    où les \( \lambda_i\) (\( i=1,\ldots, r\)) sont les valeurs propres non nulles distinctes de \( u\).

    Il est vite vu que le coefficient de \( X^{n-1}\) dans \( \chi_u\) est \( -\tr(u)\) parce que le coefficient de \( X^{n-1}\) se calcule en prenant tous les $X$ sauf une fois \( -\lambda_i\). D'autre part le polynôme caractéristique de \( u^p \) est le même que celui de \( u\), en remplaçant \( \lambda_i\) par \( \lambda_i^p\); cela est dû au fait que si \( v\) est vecteur propre de valeur propre \( \lambda\), alors \( u^pv=\lambda^pv\).

    Par l'équation \eqref{EqfnCqWq}, nous voyons que le coefficient du terme \( X^{n-1}\) dans le polynôme caractéristique est
    \begin{equation}        \label{eqSoDSKH}
        0=\tr(u^p)=\alpha_1\lambda_1^p+\cdots +\alpha_r\lambda_r^p.
    \end{equation}
    Donc les nombres \( (\alpha_1,\ldots, \alpha_r)\) sont une solution non triviale\footnote{Si \( \alpha_1=\ldots=\alpha_r=0\), alors les valeurs propres sont toutes nulles et la matrice est en réalité nulle dès le départ.} du système
    \begin{subequations}    \label{EqDpvTnu}
        \begin{numcases}{}
            \lambda_1X_1    +\cdots +\lambda_rX_r=0   \\
            \qquad\vdots                              \\
            \lambda^r_1X_1  +\cdots +\lambda_r^rX_r=0.
        \end{numcases}
    \end{subequations}
    Ce sont les équations \eqref{eqSoDSKH} écrites pour \( p=1,\ldots, r\). Le déterminant de ce système est
    \begin{equation}
        \lambda_1\ldots\lambda_r\det\begin{pmatrix}
             1                &   \ldots    &   1            \\
             \lambda_1        &   \ldots    &   \lambda_r    \\
             \vdots           &             &   \vdots       \\
             \lambda_1^{r-1}  &   \ldots    &   \lambda_r^{r-1}
         \end{pmatrix}\neq 0,
    \end{equation}
    qui est un déterminant de Vandermonde (proposition~\ref{PropnuUvtj}) valant
    \begin{equation}
        0=\lambda_1\ldots\lambda_r\prod_{1\leq i\leq j\leq r}(\lambda_i-\lambda_j).
    \end{equation}
    Étant donné que les \( \lambda_i\) sont distincts et non nuls, nous avons une contradiction et nous devons conclure que \( (\alpha_1,\ldots, \alpha_r)\) était une solution triviale du système \eqref{EqDpvTnu}.
\end{proof}

\begin{proposition}[\cite{SVSFooIOYShq}]    \label{PropMWWJooVIXdJp}
    Soit un \( \eK\)-espace vectoriel \( E\). Un endomorphisme \( u\in\End(E)\) est nilpotent si et seulement si il existe une base de \( E\) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.
\end{proposition}

\begin{proof}
    \begin{subproof}
       \item[\( \Rightarrow\)]
           Nous faisons la démonstration par récurrence sur la dimension de \( E\). Lorsque \( n=1\) nous avons \( u=(a)\) avec \( a\in \eK\). Puisque \( a^k=0\) pour un certain \( k\) nous avons \( a=0\) parce qu'un corps est toujours un anneau intègre\footnote{Lemme~\ref{LemAnnCorpsnonInterdivzer}.}.

           Lorsque \( \dim(E)=n\) nous savons que \( u\) a un noyau non réduit au vecteur nul (parce qu'il est nilpotent). Soit donc un vecteur non nul \( x\in\ker(u)\) et une base
           \begin{equation}
               \{ x,e_2,\ldots, e_n \}
           \end{equation}
           donnée par le théorème de la base incomplète~\ref{ThonmnWKs}. La matrice de \( u\) dans cette base s'écrit
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix}
                               * &   *    &   *
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\
                               0 \\
                               0
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A   &       \\
                                &       &
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Un tout petit peu de calcul de produit de matrice montre que la matrice de \( u^k\) est de la forme
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix}
                               * &   *    &   *
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\
                               0 \\
                               0
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A^k &       \\
                                &       &
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Étant donné que l'endomorphisme \( u\) est nilpotent, la matrice \( A\) l'est aussi. L'hypothèse de récurrence dit alors que \( A\) est strictement triangulaire supérieure (ou en tout cas peut le devenir par un changement de base adéquat).

       \item[\( \Leftarrow\)]
           Soit une base \( \{ e_1,\ldots, e_n \}  \) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.

           Alors \( u(e_1)=0\) et plus généralement, \( u(e_k)\in \Span\{ e_1,\ldots, e_{k-1} \}\). Voyez par récurrence que \( u^l(e_k)\in \Span\{ e_1,\ldots, e_{k-l} \}\). Donc \( u^l(e_k)=0\) dès que \( l\geq k\).
    \end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooKPWKooOacXju}
    Si \( N\in \eM(n,\eC)\) est une matrice nilpotente d'ordre de nilpotence \( r\), alors \( \{ N^k \}_{k=0,\ldots, r-1}\) est libre dans \( \eM(n,\eC)\).
\end{lemma}

\begin{proposition}[Thème~\ref{THEMEooPQKDooTAVKFH}]     \label{PROPooWTFWooXHlmhp}
%TODO: Définition "espace de Banach" avant ? Si oui, reference ?
    Soit \( E\) un espace de Banach (espace vectoriel normé complet\footnote{Définition \ref{DefVKuyYpQ}.}). Si \( A\in\aL(E,E)\) est nilpotente, alors \( (\mtu-A)\) est inversible et son inverse est donné par
    \begin{equation}
        (\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k,
    \end{equation}
    où l'infini peut évidemment être remplacé par l'ordre de nilpotence de \( A\).
\end{proposition}

\begin{proof}
    En ce qui concerne la convergence de la somme, elle ne fait pas de doute parce que \( A\) étant nilpotente, la somme contient seulement une quantité finie de termes non nuls.

    Montrons à présent que la somme est l'inverse de \( \mtu-A\) en multipliant terme à terme :
    \begin{equation}
        \sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
    \end{equation}
    Par conséquent
    \begin{equation}
        \| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\to 0.
    \end{equation}
    La dernière limite est en réalité une égalité pour \( n\) assez grand.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefCNJqsmo}
    Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice diagonale.

    Une application linéaire est diagonalisable si elle est semblable\footnote{Définition \ref{PROPooIXFSooZsFWHm}.} à une application linéaire diagonale.
\end{definition}

La proposition \ref{PROPooBGJBooXlDYEv} nous assure que la notion de diagonalisabilité pour les matrices et pour les applications sont les mêmes.

\begin{proposition}     \label{PROPooDEETooSOMiGO}
    Si \( A\) est un opérateur diagonalisable dont les valeurs propres sont \( \lambda_i\), alors il existe un opérateur inversible \( Q\) tel que
    \begin{equation}
        A=Q^{-1} DQ
    \end{equation}
    où \( D \) est l'opérateur diagonal contenant les \( \lambda_i\) sur sa diagonale.
\end{proposition}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est-à-dire si c'est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item       \label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\), dont toutes les racines sont simples, tel que \( P(u)=0\).
        \item       \label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
            \end{equation}
            mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
        \item       \label{ItemThoDigLEQEXRiii}
            Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
        \item       \label{ITEMooZNJFooEiqDYp}
            Dans une base adaptée, la matrice de \( u\) est diagonale et les éléments diagonaux sont ses valeurs propres.
    \end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
    Plein d'implications à prouver.
    \begin{subproof}
    \item[\ref{ItemThoDigLEQEXRi} implique~\ref{ItemThoDigLEQEXRii}] Étant donné que \( P(u)=0\), il est dans l'idéal des polynômes annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynômes annulateurs est généré par \( \mu_u\) par le théorème~\ref{ThoCCHkoU}.

    \item[\ref{ItemThoDigLEQEXRii} implique~\ref{ItemThoDigLEQEXRiv}] Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est-à-dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème~\ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

    \item[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRiii}] Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) et \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooCJQGooXwjsfm}, nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est stable par \( u\).

    \item[\ref{ItemThoDigLEQEXRiii} implique~\ref{ItemThoDigLEQEXRiv}] En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

    \item[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRi}] Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\),
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme~\ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur la base \( \{ e_i \}\).

    \item[\ref{ITEMooZNJFooEiqDYp} implique~\ref{ItemThoDigLEQEXRi}]
    Si la matrice \( A\) est diagonale alors le polynôme \( P=\prod_{i=1}^n(X-A_{ii})\) est annulateur de \( A\). En effet,
    \begin{equation}
        P(A)e_k=\prod_{i=1}^n(A-A_{ii})x=\prod_{i=1}^n\big( u(e_k)-A_{ii}e_k \big)=\prod_{i=1}^n\big( A_{kk}e_k-A_{ii}e_k \big)=0
    \end{equation}
    parce que le facteur \( i=k\) est nul.
    \item[\ref{ItemThoDigLEQEXRii} implique~\ref{ITEMooZNJFooEiqDYp}]
        le polynôme minimal de \( u\) s'écrit
        \begin{equation}
            \mu=(X-\lambda_1)\ldots(X-\lambda_r),
        \end{equation}
        et les espaces $E_i$ du lemme~\ref{LemgnaEOk} sont les espaces propres \( E_i=\ker(u-\lambda_i)\). Nous avons donc une somme directe
        \begin{equation}
            E=E_1\oplus\ldots\oplus E_r.
        \end{equation}
        Dans chacun des espaces propres, $u$ a une matrice diagonale avec la valeur propre correspondante sur la diagonale. Une base de \( E\) constituée d'une base de chacun des espaces propres est donc une base comme nous en cherchons.
    \end{subproof}
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
    Si \( u\) est diagonalisable et si \( F\) est un sous-espace stable par \( u\), alors
    \begin{equation}
        F=\bigoplus_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème~\ref{ThoDigLEQEXR}, le polynôme \( \mu_u\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme~\ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soient \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme~\ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n=\dim(E)\) et \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme~\ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u_i)\big)\subset E_{\lambda}(u_i)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_i(x)=\lambda x\). Nous montrons que \( u_j(x)\in E_{\lambda}(u_i)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point~\ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
    Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).

    Tant que \( 1\neq -1\), \( X^2-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

    Par exemple si le corps est de caractéristique \( 2\), nous avons
    \begin{subequations}
        \begin{align}
            A   &=\begin{pmatrix}
                1    &   1    \\
                0    &   1
            \end{pmatrix}\\
            A^2 &=\begin{pmatrix}
                1    &   2    \\
                0    &   1
            \end{pmatrix}=\begin{pmatrix}
                1    &   0    \\
                0    &   1
            \end{pmatrix}.
        \end{align}
    \end{subequations}
    Cette matrice \( A\) représente donc une involution, mais n'est pas diagonalisable.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, pas toujours}
%---------------------------------------------------------------------------------------------------------------------------

Il n'est pas vrai qu'une matrice de \( \eM(n,\eC)\) soit toujours diagonalisable. En effet le théorème~\ref{ThoDigLEQEXR}\ref{ItemThoDigLEQEXRii} dit qu'une matrice est diagonalisable si et seulement si son polynôme minimal est scindé à racines simples. Certes sur \( \eC\) le polynôme minimal sera scindé, mais il ne sera pas spécialement à racines simples.

\begin{example}
    La matrice
    \begin{equation}
        A=\begin{pmatrix}
            0    &   1    \\
            0    &   0
        \end{pmatrix}
    \end{equation}
    a pour polynôme caractéristique \( \chi_A(X)=X^2\). C'est également son polynôme minimal, qui n'est pas à racine simple.

    Il est par ailleurs facile de voir que le seul espace propre de \( A\) est \( \Span\{ (1,0) \}\) (ici le span est sur \( \eC\)). Donc l'espace \( \eC^2\) ne possède pas de base de vecteurs propres de \( A\).
\end{example}

Ce qui est vrai, c'est que le polynôme caractéristique a des racines, et que ces racines correspondent à des vecteurs propres. Mais il n'y a pas toujours autant de vecteurs propres que la multiplicité des racines.

\begin{normaltext}
    Lorsque la diagonalisation n'est pas possible, il est souvent possible de trigonaliser. Les matrices triangulaires ne sont pas aussi faciles à manipuler que les matrices diagonales, mais c'est toujours ça de pris.

    Nous étudierons ça plus tard, en \ref{SUBSECooMCOGooEoQCsz} parce que ça va nécéssiter le théorème de d'Alembert.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\
            0    &   0    &   \lambda_r    &   *    &   *\\
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\
                c_1    &   d_1
            \end{pmatrix}&   *\\
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\
                c_s    &   d_s
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}
\index{lemme!Schur réel}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\
                \cdot    &   \cdot
            \end{pmatrix}&   C_1    \\
            0    &   A_1
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjugués.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( Q^{-1}AQ\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
    Une matrice symétrique réelle,
    \begin{enumerate}
        \item       \label{ITEMooJWHLooSfhNSW}
            a un spectre contenu dans \( \eR\)
        \item       \label{ITEMooMWWRooXxGONW}
            est diagonalisable par une matrice orthogonale.
    \end{enumerate}
    Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Elle agit sur l'espace \( \eC^n\) par la définition \ref{DEFooJVOAooUgGKme}, et en particulier la formule \ref{EQooQFVTooMFfzol}. Nous munissons de plus \( \eC^n\) de la forme sesquilinéaire définie en la proposition \ref{PROPooMWUCooMbJuaJ}. 

    Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\), et \( \lambda\) est réelle.

    Le lemme de Schur réel~\ref{LemSchureRelnrqfiy} donne une matrice orthogonale \( Q\) qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( Q^{-1}AQ\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( Q^{-1}AQ\) est symétrique :
    \begin{equation}
        (Q^{-1}AQ)^t=Q^tA^t(Q^{-1})^t=Q^{-1}A^tQ=Q^{-1}AQ
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

    En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce qu'en utilisant la proposition \ref{PROPooNARVooEuhweD},
    \begin{equation}
        \langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{proof}
Le théorème spectral pour les opérateurs autoadjoints sera traité plus bas parce qu'il a besoin de notions sur les formes bilinéaires, théorème~\ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchainement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
    Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

    En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
    \begin{equation}
        \begin{aligned}[]
        \begin{pmatrix}
            a   &   *    &   *    \\
            b   &   *    &   *    \\
            c   &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1    &   0   &   0  \\
            0   &   \lambda_2    &   0  \\
            0   &   0    &  \lambda_3 & 0
            \end{pmatrix}
            \begin{pmatrix}
                a    &  b     &   c     \\
                *    &  *     &   *     \\
                *    &  *     &   *
            \end{pmatrix}&=
        \begin{pmatrix}
            a   &   *    &   *    \\
            b   &   *    &   *    \\
            c   &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1a    &   \lambda_1b    &   \lambda_1c    \\
            *   &   *    &   *    \\
            *   &   *    &   *
        \end{pmatrix}\\
        &=\begin{pmatrix}
            \lambda_1 a^2+*   &   \lambda_1ab+*    &   \lambda_1ac  +*  \\
            \ldots    &   \ldots    &   \ldots    \\
            \ldots    &   \ldots    &   \ldots
        \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}



\begin{proposition}     \label{PROPooQHHPooSqpgcb}
    Une forme bilinéaire est non-dénénérée\footnote{Définition \ref{DEFooNUBFooLfCqaK}.} si et seulement si sa matrice associée est inversible.
\end{proposition}

\begin{proof}
    Nous savons que la matrice associée est symétrique et qu'elle peut donc être diagonalisée (théorème~\ref{ThoeTMXla}). En nous plaçant dans une base de diagonalisation, nous devons prouver que la forme est non-dégénérée si et seulement si les éléments diagonaux de la matrice sont tous non nuls.

    Écrivons \( b(x,z)\) en choisissant pour \( z\) le vecteur de base \( e_k\) de composantes \( (e_k)_j=\delta_{kj}\) :
    \begin{equation}
            b(x,e_k)=\sum_{ij}x_i(e_k)_j
            =\sum_i b_{ik}x_i
            =b_{kk}x_k.
    \end{equation}
    Si \( b\) est dégénérée et si \( x\) est un vecteur non nul (disons que la composante \( x_i\) est non nulle) de \( E\) tel que \( b(x,z)=0\) pour tout \( z\in E\), alors \( b_{ii}=0\), ce qui montre que la matrice de \( b\) n'est pas inversible.

    Réciproquement si la matrice de \( b\) est inversible, alors tous les \( b_{kk}\) sont différents de zéro, et le seul vecteur \( x\) tel que \( b_{kk}x_k=0\) pour tout \( k\) est le vecteur nul.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice définie positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Matrice définie positive, opérateur défini positif]    \label{DefAWAooCMPuVM}
    Un opérateur sur un espace vectoriel sur \( \eC\) ou \( \eR\) est \defe{défini positif}{opérateur!défini positif} si toutes ses valeurs propres sont réelles et strictement positives.  Il est \defe{semi-défini positif}{semi-défini positif} si ses valeurs propres sont réelles positives ou nulles.

    Mêmes définitions pour une matrice.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} défini positif pour positif.

\begin{normaltext}      \label{NORMooAJLHooQhwpvr}
    Nous nommons \( S^+(n,\eR)\) l'ensemble des matrices réelles symétriques \( n\times n\) et \( S^{++}(n,\eR)\) le sous-ensemble de \( S^+(n,\eR)\) des matrices strictement définies positives.
\end{normaltext}
    \nomenclature[B]{$ S^+(n,\eR)$}{matrices symétriques définies positives}
    \nomenclature[B]{$ S^{++}(n,\eR)$}{matrices symétriques strictement définies positives}

\begin{remark}
    Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

\begin{proposition}     \label{PropcnJyXZ}
    Soit \( M\), une matrice symétrique. Nous avons
    \begin{enumerate}
        \item       \label{ITEMooTJVQooYmRkas}
            \( \det(M)>0\) et \( \tr(M)>0\) implique \( M\) définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.},
        \item
            \( \det(M)>0\) et \( \tr(M)<0\) implique \( M\) définie négative,
        \item       \label{ItemluuFPN}
            \( \det(M)<0\) implique ni semi-définie positive, ni définie négative
        \item
            \( \det(M)=0\) implique \( M\) semi-définie positive ou semi-définie négative.
    \end{enumerate}
\end{proposition}

Lorsqu'un énoncé parle d'une matrice symétrique, le premier réflexe est de la diagonaliser : considérer une matrice orthogonale \( Q\) telle que \( Q^tMQ=D\) avec \( D\) diagonale. Et les valeurs propres sur la diagonale : \( D_{kk}=\lambda_k\). Les matrices symétriques définies positives ont cependant des propriétés même en dehors de leur base de diagonalisation.

Pour rappel, \( \langle x, y\rangle \) est le produit scalaire dans \( \eR^n\) défini par la proposition \ref{PROPooSKVRooDGVCYj}.

\begin{lemma}   \label{LemWZFSooYvksjw}
    Soit une matrice symétrique \( M\).
    \begin{enumerate}
        \item       \label{ITEMooSKRAooOgHbGA}
           Elle est strictement définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\) non nul dans \( \eR^n\).
        \item       \label{ITEMooMOZYooWcrewZ}
           Elle est semi-définie positive si et seulement si \( \langle x, Mx\rangle \geq 0\) pour tout \( x\) non nul dans \( \eR^n\).
       \item        \label{ITEMooRRMFooHSOHxZ}
           Si elle est seulement définie positive, alors \( \langle x, Mx\rangle \geq \lambda\| x \|^2\) dès que \( \lambda\geq 0\) minore toutes les valeurs propres.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Démonstration en trois parties.
    \begin{subproof}
    \item[\ref{ITEMooSKRAooOgHbGA}]
    Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral~\ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
    \begin{equation}
        \langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( M\).  Le produit\( \langle x,Mx \rangle\) est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont strictement positifs.

    \item[\ref{ITEMooMOZYooWcrewZ}]
    Nous avons encore
    \begin{equation}
        \langle x, Mx\rangle =\sum_{i}\lambda_ix_i^2
    \end{equation}
    qui est plus grand ou égal à zéro si et seulement si tous les \( \lambda_i\) sont plus grands ou égaux à zéro.

    \item[\ref{ITEMooRRMFooHSOHxZ}]

        Soit une matrice orthogonale \( T\) diagonalisant \( M\), c'est-à-dire telle que \( T^tMT=D\) avec \( D\) diagonale. Nous allons vérifier que si \( \lambda\leq\min\{ \lambda_i \}\), alors
        \begin{equation}        \label{EQooOSFEooCoPuuG}
            \langle Tx, MTx\rangle \geq \lambda\| Tx \|^2
        \end{equation}
        pour tout \( x\). Si nous considérons la base de diagonalisation \( \{ e_k \}\) pour les valeurs propres \( \lambda_k\), nous avons le calcul
        \begin{subequations}     \label{SUBEQSooHBIYooSpkYAl}
            \begin{align}
                \langle Tx, MTx\rangle  & = \langle x, T^tMTx\rangle    \\
                                        & = \langle x, Dx\rangle        \\
                                        & = \sum_k\langle x, x_kDe_k\rangle                               \\
                                        & = \sum_k\lambda_kx_k \underbrace{\langle x, e_k\rangle }_{=x_k} \\
                                        & \geq \sum_k\lambda| x_k |^2 & \text{en posant} \lambda=\min\{ \lambda_i \}   
            \end{align}
        \end{subequations}
        Nous avons donc 
        \begin{equation}
                \langle Tx, MTx\rangle   \geq \sum_k\lambda| x_k |^2 = \lambda\| x \|^2 = \lambda\| Tx \|^2.
        \end{equation}
        Au dernier passage nous avons utilisé le fait que \( T\) est une isométrie (proposition~\ref{PropKBCXooOuEZcS}). L'inéquation \eqref{EQooOSFEooCoPuuG} est démontrée.

        Comme \( T\) est une bijection \footnote{Une matrice orthogonale a un déterminant qui vaut $\pm 1$.}, cela implique le résultat pour tout \( x\). 
    \end{subproof}
\end{proof}

Les personnes qui aiment les vecteurs lignes et colonnes écriront des inégalités comme
\begin{equation}
    x^tMx\geq x^tx.
\end{equation}
Tout à l'autre bout du spectre des personnes névrosées des notations, on trouvera des inégalités comme
\begin{equation}
    M(x\otimes x)\geq x\cdot x.
\end{equation}
Le penchant personnel de l'auteur de ces lignes est la notation avec le produit tensoriel. Si vous aimez ça, vous pouvez lire la section \ref{SECooUKRYooZjagcX} et en particulier ce qui suit \eqref{EQooUNRYooKBrXyK}.

La notation adoptée ici avec le produit scalaire \( \langle x, Mx\rangle \), qui peut aussi être écrite \( x\cdot Mx\) est entre les deux. Elle a l'avantage de n'être pas technologique comme le produit tensoriel (si vous y mettez les pieds, vous devez savoir ce que vous faites), tout en évitant de se casser la tête à savoir qui est un vecteur ligne ou un vecteur colonne.

\begin{proposition}     \label{PROPooUAAFooEGVDRC} \label{PROPooNQSXooVMFAtU}
    Une application bilinéaire est définie positive\footnote{Définition \ref{}.} si et seulement si sa matrice symétrique associée l'est.
\end{proposition}

\begin{proof}
    La définition \ref{DEFooJIAQooZkBtTy} dit que \( b\) est strictement définie positive lorsque \( b(x,x)\geq 0\) et \( b(x,x)=0\) si et seulement si \( x=0\).

    D'autre part, le lemme \ref{LemWZFSooYvksjw} dit que la matrice \( B\) est strictement définie positive lorsque \( x\cdot Bx\geq 0\) et \( x\cdot Bx=0\) si et seulement si \( x=0\).

    Le lien entre les deux est que le lemme \ref{LEMooDCIOooTlVZMR} nous enseigne que pour tout \( x\) et \( y\),
    \begin{equation}
        b(x,y)=x\cdot By
    \end{equation}
    où \( B\) est la matrice de \( b\).
\end{proof}

\begin{proposition}     \label{PROPooCIEUooODqfwm}
    Soit une forme quadratique \( q\colon E\to \eK\) et sa matrice\footnote{Matrice associée à une forme quadratique, définition \ref{DEFooAOGPooXWXUcN}.} \( (q_{ij})\in \eM(n,\eK)\). Nous avons
    \begin{subequations}        \label{SUBEQSooEHVXooJjKLqyiB}
        \begin{align}
            q(x)&=\sum_{i=1}^n\sum_{j=1}^nq_{ij}x_ix_j\\
            &=\sum_{i=1}^nq_{ii}x_i^2+2\sum_{1\leq i <j\leq n}q_{ij}x_ix_j.
        \end{align}
    \end{subequations}
\end{proposition}

\begin{normaltext}
    De nombreux auteurs préfèrent écrire des choses comme \( x^tBy\) ou \( xB^ty\) ou \( xBy^t\) et se poser de longues questions sur qui est un «vecteur colonne» et qui est un «vecteur ligne», et si la matrice \( B\) soit être transposée ou non. Toutes ces notations servent(?) à cacher un bête produit scalaire.
\end{normaltext}

\begin{normaltext}
    Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire.
\end{normaltext}

\begin{corollary}
    Une matrice symétrique strictement définie positive est inversible.
\end{corollary}

\begin{proof}
    Si \( Ax=0\) alors \( \langle Ax, x\rangle =0\). Mais dans le cas d'une matrice strictement définie positive, cela implique \( x=0\) par le lemme~\ref{LemWZFSooYvksjw}.
\end{proof}

\begin{lemma}
    Pour une base quelconque, les éléments diagonaux d'une matrice symétrique semi-définie positive sont positifs. Si la matrice est strictement définie positive, alors les éléments diagonaux sont strictement positifs.
\end{lemma}

\begin{proof}
    Il s'agit d'une application du lemme~\ref{LemWZFSooYvksjw}. Si \( A\) est définie positive et que \( \{ e_i \}\) est une base, alors
    \begin{equation}
        A_{ii}=\langle Ae_i, e_i\rangle \geq \lambda\| e_i \|^2=\lambda\geq 0.
    \end{equation}
    Si \( A\) est strictement définie positive, alors \( \lambda\) peut être choisi strictement positif.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Réduction de Gauss\cite{BIBooNUUEooJUjLpy,BIBooUULNooUtlrar}]     \label{THOooOMMFooKxqICS}
    Soit une forme quadratique non nulle \( q\) sur l'espace vectoriel \( E\) sur le corps \( \eK\). Il existe une base  \(\{ l_i \}_{i=1,\ldots, n}\) de \( E^*\) et des coefficients \( \alpha_i\in \eK\) tels que 
        \begin{equation}
            q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
        \end{equation}
\end{theorem}

\begin{proof}
    Notre point de départ sont les formules \eqref{SUBEQSooEHVXooJjKLqyiB} pour la forme quadratique. Nous allons faire la preuve par récurrence sur la dimension de l'espace. Si \( n=1\), alors nous avons seulement
    \begin{equation}
        q(x)=\alpha x^2
    \end{equation}
    et donc le théorème est fait avec \( l(x)=x\).

    Nous supposons que le théorème est prouvé pour tout espace de dimension \( n\). Une forme quadratique pour un espace de dimension \( n+1\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^{n+1}m_{ii}x_i^2+2\sum_{1\leq i < j\leq n+1}m_{ij}x_ix_j.
    \end{equation}
    Vu que \( q\) est non nulle, un des \( m_{ij}\) est non nul. Nous allons diviser en plusieurs cas.
    \begin{itemize}
        \item
            \( m_{11}\neq 0\)
        \item
            \( m_{kk}\neq 0\) avec \( k\neq 1\)
        \item
            \( m_{12}\neq 0\) et \( m_{ii}=0\) pour tout \( i\).
        \item
            \( m_{kl}\neq 0\) avec \( (k,l)\neq (1,2)\) et \( m_{ii}=0\) pour tout \( i\).
    \end{itemize}
    Ces cas ne sont pas exclusifs, mais ils couvrent toutes les possibilités.

    \begin{subproof}
        \item[Si \( m_{11}\neq 0\)]
            Nous écrivons \( q\) sous la forme
            \begin{subequations}
                \begin{align}
                    q(x)&=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big)\\
                    &=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{j=2}^{n+1}m_{1j}x_1x_j+2\sum_{i=2}^n\sum_{j=i+1}^{n+1}(m_{ij}x_ix_j)\\
                    &=m_{11}x_1^2+2x_1\sum_{j=2}^{n+1}m_{1j}x_k+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\left( x_1^2+2x_1\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j \right)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1^2+2x_1f(x_2,\ldots, x_{n+1}) \big)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1+f(x_2,\ldots, x_{n+1}) \big)^2-f(x_2,\ldots, x_{n+1})+R(x_2,\ldots, x_{n+1})
                \end{align}
            \end{subequations}
            où 
            \begin{itemize}
                \item \( R\) est une forme quadratique de \( n-1\) variables;
                \item nous avons noté \( f(x_2,\ldots, x_{n+1})=\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j\).
            \end{itemize}
            Maintenant, toute la partie \( -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})\) est une forme quadratique de \( n\) variables. Par hypothèse de récurrence, il existe des coefficients \( \alpha_i\) et des formes linéairement indépendantes sur \( \eK^n\) \( l_i'(x_2,\ldots, x_{n+1})\) telles que
            \begin{equation}
                -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})=\sum_{i=2}^{n+1}\alpha_il_i'(x_2,\ldots, x_{n+1})^2.
            \end{equation}
            En posant ensuite \( l_j(x_1,\ldots, x_{n+1})=l'_j(x_2,\ldots, x_{n+1})\), ainsi que \( l_1(x_1,\ldots, x_{n+1})=x_1+f(x_2,\ldots, x_{n+1})\), nous avons
            \begin{equation}
                q(x)=m_{11}l_1(x)^2+\sum_{j=2}^{n+1}\alpha_jl_j(x)^2.
            \end{equation}
            
        \item[Si \( m_{kk}\neq 0\) avec \( k\neq 1\)]

            Nous nommons \( k\) le plus petit entier pour lequel \( m_{kk}\neq 0\), et nous supposons que \( k\neq 1\), parce que nous avons déjà couvert ce cas. Dans ce cas, nous avons
            \begin{equation}
                q(x)=m_{kk}x_k^2+\sum_{j=k+1}^{n+1}m_{jj}x_j^2  +2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big),
            \end{equation}
            et tout tourne comme dans le premier cas.
        \item[\( m_{ii}=0\) pour tout \( i\) et \( m_{12}\neq 0\)]
            Nous écrivons \( q\) en séparant les termes \( m_{1k}\) :
            \begin{subequations}
                \begin{align}
                    q(x)&=2\sum_{1\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2\sum_{2\leq j\leq n+1}m_{1j}x_1x_j+2\sum_{2\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2x_1\sum_{2\leq j\leq n+1}m_{1j}x_j+2\sum_{3\leq j\leq n+1}m_{2j}x_2x_j+2\sum_{3\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+x_1f(x_2,\ldots, x_{n+1})+x_2g(x_3,\ldots, x_{n+1})+T(x_3,\ldots, x_{n+1})      \label{SUBEQooLBXBooXoLyuw}
                \end{align}
            \end{subequations}
            où \( f\) et \( g\) sont linéaires et \( T\) est multilinéaire.

            À ce moment, nous tentons de factoriser toute la partie concernant \( x_1\) et \( x_2\). L'idée est d'utiliser ceci :
            \begin{equation}
                (x_1+g)(x_2+f)=x_1x_2+x_1f+x_2g+fg,
            \end{equation}
            mais en mettant les bons coefficients pour reproduire ce que nous avons dans \eqref{SUBEQooLBXBooXoLyuw} : 
            \begin{equation}
                (2m_{12}+2g)(x_1+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }=2m_{12}x_1x_2+2x_1f+2x_2g.
            \end{equation}
            Cela pour dire que
            \begin{equation}
                q(x)=2(m_{12}x_1+g)(x_2+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }+T
            \end{equation}
            où \(-2fg/m_{12}+T\) est une forme quadratique de \( x_3,\ldots, x_{n+1}\), c'est-à-dire de \( n-1\) variables.

            L'hypothèse de récurrence nous donne des formes linéaires \( (l_i)_{i=3,\ldots, n+1}\) telles que
            \begin{equation}
                \frac{ 2fg }{ m_{12} }+T=\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
            Nous pouvons donc déjà écrire
            \begin{equation}
                q(x)=2l'_1(x)l'_2(x)+\sum_{i=3}^{n+1}\alpha_il_i(x)^2
            \end{equation}
            où
            \begin{itemize}
                \item Les forme \( l_i\) avec \( i\geq 3\) ne dépendent pas de \( x_1\) et \( x_2\), et sont donc indépendantes de \( l_1\) et \( l_2\).
                \item La forme \( l'_1\) ne dépend pas de \( x_2\),
                \item La forme \( l'_2\) ne dépend pas de \( x_1\).
            \end{itemize}
            Ce sont donc \( n+1\) formes linéaires indépendantes. Le seul problème résiduel est que les formes \( l'_1\) et \( l'_2\) arrivent en produit l'une de l'autre. Nous en définissons donc deux de plus :
            \begin{equation}
                \begin{aligned}[]
                    l_1(x)=\frac{ 1 }{2}(l'_1+l'_2)\\
                    l_2(x)=\frac{ 1 }{2}(l'_1-l'_2),
                \end{aligned}
            \end{equation}
            qui sont linéairement indépendantes l'une de l'autre et indépendantes des \( l_i\) (\( i\geq 3\)). Au final,
            \begin{equation}
                q(x)=l_1(x)^2+l_2(x)^2+\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
        \item[Si \( m_{ii}=0\) et \( m_{12}=0\) et \( m_{kl}\neq 0\) avec \( k<l\)]
            Nous considérons la permutation
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \} \\
                    i&\mapsto \begin{cases}
                         1   &   \text{si } i=k\\
                         2   &   \text{si } i=l\\
                         k   &   \text{si } i=1\\
                         l   &   \text{si } i=2\\
                        i    &    \text{sinon,}
                    \end{cases}
                \end{aligned}
            \end{equation}
            c'est-à-dire que \( \sigma\) permute \( 1\) et \( k\) ainsi que \( 2\) et \( l\). Ensuite nous posons
            \begin{equation}
                \begin{aligned}
                    s\colon \eR^{n+1}&\to \eR^{n+1} \\
                    e_i&\mapsto e_{\sigma(i)}. 
                \end{aligned}
            \end{equation}
            Nous allons un peu considérer \( q\circ s\), pour changer : 
            \begin{equation}        \label{EQooLVAWooAirEzP}
                (q\circ s)(x)=\sum_{i,j}m_{ij}s(x)_is(x)_j=\sum_{ij}x_{\sigma(i)}x_{\sigma(j)}.
            \end{equation}
            parce que \( s(x)_i=x_{\sigma(i)}\). 

            Utilisons un petit abus de notation pour considérer
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \} \\
                    (i,j)&\mapsto \big(\sigma(i), \sigma(j)\big). 
                \end{aligned}
            \end{equation}
            Cela est une bijection; nous pouvons utiliser le lemme \ref{DEFooLNEXooYMQjRo} pour permuter les termes dans \eqref{EQooLVAWooAirEzP} :      
            \begin{subequations}
                \begin{align}
                    (q\circ s)(x)&=\sum_{ij}m_{\sigma(i)\sigma(j)}x_{\sigma\sigma(i)}x_{\sigma\sigma(j)}\\
                    &=\sum_{ij}a_{ij}x_ix_j     \label{EQooPCTCooFnMWat}
                \end{align}
            \end{subequations}
            où nous avons posé \( a_{ij}=m_{\sigma(i)\sigma(j)}\) et utilisé le fait que \( \sigma=\sigma^{-1}\). Le point intéressant de l'histoire est que dans \eqref{EQooPCTCooFnMWat}, \( a_{12}=m_{kl}\neq 0\). La forme \( q\circ s\) est donc dans le cas déjà traité et il existe des formes linéaires \( l'_i\) telles que
            \begin{equation}
                (q\circ s)(x)=\sum_{i=1}^{n+1}\alpha_il'_i(x)^2.
            \end{equation}
            En évaluant cela en \( s(x)\), et en tenant compte de \( s=s^{-1}\), nous trouvons
            \begin{equation}
                q(x)=\sum_i\alpha_i(l_i\circ s)(x)^2,
            \end{equation}
            de telle sorte que \( l_i=l'_i\circ s\) soit la réponse à notre théorème.
    \end{subproof}
\end{proof}

\begin{theorem}[\cite{BIBooUULNooUtlrar,MonCerveau}]       \label{THOooIDMPooIMwkqB}
    Toute forme quadratique sur un espace vectoriel de dimension finie admet une base formée de vecteurs \( 2\) à \( 2\) orthogonaux (pour la forme considérée).
\end{theorem}

\begin{proof}
    Nous considérons la base \(  \{ l_i \}    \) de \( E^*\) donnée par la réduction de Gauss (théorème \ref{THOooOMMFooKxqICS}). La forme quadratique \( q\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
    \end{equation}
    La base préduale\footnote{Définition, existence, unicité dans la proposition \ref{PROPooDBPGooPagbEB}.} \( \{ e_i \}\) de \( \{ l_i \}\) répond aux conditions. Pour le vérifier, nous considérons la forme bilinéaire associée à \( q\) par l'identité de polarisation \ref{PROPooZLXVooOsXCcB} :
    \begin{equation}
        b(e_i,e_j)=\frac{ 1 }{2}\big( q(e_i)+q(e_j)-q(e_i-e_j) \big).
    \end{equation}
    Vu que \( l_k(e_i)=\delta_{ki}\), nous avons
    \begin{equation}
        q(e_i)=\sum_{k=1}^n\alpha_kl_k(e_i)^2=\alpha_i.
    \end{equation}
    En utilisant la linéarité,
    \begin{subequations}
        \begin{align}
            q(e_i-e_j)&=\sum_k\alpha_kl_k(e_i-e_j)^2\\
            &=\sum_k\alpha_k(\delta_{ki}-\delta_{kj})^2\\
            &=\sum_k\alpha_k(\delta_{ki}+\delta_{kj}-2\delta_{ki}\delta_{ki})\\
            &=\alpha_i+\alpha_j-2\delta_{ij}\alpha_i.
        \end{align}
    \end{subequations}
    Donc 
    \begin{equation}
        b(e_i,e_j)=\delta_{ij}\alpha_i.
    \end{equation}
    Les vecteurs \( \{ e_i \}\) sont donc bien deux à deux \( q\)-orthogonaux.
\end{proof}

Notons qu'en l'absence de notion de racine carrée sur \( \eK\), il n'est pas possible de considérer \( \sqrt{ \alpha_i }\) et donc de base \( q\)-orthonormée.


\begin{proposition} \label{PROPooPMYCooAAtHsB}
    Si \( A\in \eM(n,\eK)\) est telle que \( \det(A)=0\), alors il existe des matrices de manipulation de lignes et de colonnes \( G_1,\ldots, G_N\) telles que \( G_1\ldots G_NA\) ait une colonne de zéros.
\end{proposition}

\begin{proof}
    Si la matrice \( A\) elle-même n'a pas de colonnes de zéros, alors nous pouvons faire un pas de réduction de Gauss\footnote{La réduction de Gauss est le théorème \ref{THOooOMMFooKxqICS}, mais le lien avec ce que nous disons ici n'est peut-être pas directement clair.}
% TODOooITYKooFiRuau Il faut clarifier l'utilisation de la réduction de Gauss et montrer ce qu'est un pas de réduction.
    et obtenir des matrices \( G_1,\ldots,  G_{N_1}\) telles que
    \begin{equation}
        G_1\ldots G_{N_1}A=
      \begin{pmatrix}
            1    &   \begin{matrix}
                0    &   \ldots    &   0
            \end{matrix}\\
            \begin{matrix}
                0    \\
                \vdots    \\
                0
            \end{matrix}& A^{(1)}
        \end{pmatrix}.
    \end{equation}
    Si \( A^{(1)}\) ne possède pas de colonnes de zéros, nous pouvons continuer.

    Si nous parvenons à faire \( n\) pas de la sorte, alors nous aurions
    \begin{equation}
        G_1\ldots G_NA=\delta,
    \end{equation}
    et donc \( \det(G_1\ldots G_N)\det(A)=1\), ce qui est impossible lorsque \( \det(A)=0\). Nous en concluons que le processus doit s'arrêter et qu'une des matrices \( A^{(k)}\) doit avoir une colonne de zéros\footnote{En réalité, le processus tel que nous l'avons décrit ne s'arrête que lorsque la première colonne est remplie de zéros.}.
\end{proof}

\begin{proposition}     \label{PROPooVUDJooLWjmSI}
    Une matrice dont le déterminant est nul n'est pas inversible.
\end{proposition}

\begin{proof}
    Par la proposition \ref{PROPooPMYCooAAtHsB}, il existe des matrices de manipulation de lignes et de colonnes \( G_1,\ldots, G_N\) telles que la matrice \( G_1\ldots G_NA\) ait une colonne de zéros. De là, la proposition \ref{PROPooEOKBooKUROFg} implique que la matrice
    \begin{equation}        \label{EQooQGXBooXxFOtb}
        G_1\ldots G_NA
    \end{equation}
    n'est pas inversible. Vu les déterminants des matrices \( G_i\),  la proposition \ref{PROPooAVIXooMtVCet} implique que \( G_1\ldots G_N\) est inversible. Si \( A\) était inversible, nous aurions
    \begin{equation}
        G_1\dots G_NAA^{-1}(G_1\ldots G_N)^{-1}=\delta,
    \end{equation}
    c'est-à-dire que \( A^{-1}(G_1\ldots G_N)^{-1}\) serait un inverse de la matrice \eqref{EQooQGXBooXxFOtb}. Cette dernière n'ayant pas d'inverse, nous concluons que \( A\) n'en a pas non plus.
\end{proof}

\begin{theorem}     \label{THOooSNXWooSRjleb}
    Une matrice sur un corps commutatif est inversible si et seulement si son déterminant est non nul.
\end{theorem}

\begin{proof}
    Dans un sens c'est la proposition \ref{PROPooAVIXooMtVCet} et dans l'autre sens c'est la proposition \ref{PROPooVUDJooLWjmSI}.
\end{proof}


\begin{proposition}     \label{PROPooHQNPooIfPEDH}
    Soient des matrices \( A\) et \( B\) sur un corps commutatif. Alors
    \begin{equation}
        \det(AB)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Les propositions \ref{PROPooUCZVooPkloQp} et \ref{PROPooWVJFooTmqoec} ont déjà fait une grosse partie du travail. Il ne reste que le cas où \( \det(A)=\det(B)=0\).

    Dans ce cas, les matrices \( A\) et \( B\) ne sont pas inversibles (proposition \ref{THOooSNXWooSRjleb}). Le produit \( AB\) n'est alors pas inversible non plus\footnote{Citez le lemme \ref{LEMooZDNVooArIXzC} si vous voulez justifier ça.}. La proposition \ref{THOooSNXWooSRjleb}, utilisée dans le sens inverse, nous dit alors que \( \det(AB)=0\).

    Au final dans le cas \( \det(A)=\det(B)=0\) nous avons \( 0=\det(AB)=\det(A)\det(B)=0\).
\end{proof}

Faisons maintenant le cas général des manipulations de lignes et colonnes.

\begin{proposition}     \label{PROPooSLLGooSZjQrv}
    Soit une matrice carrée \( A\in \eM(n,\eK)\). La matrice \( B\) obtenue par la substitution simultanée
    \begin{equation}
        C_j\to \sum_ka_{kj}C_k
    \end{equation}
    a pour déterminant
    \begin{equation}
        \det(B)=\det(a)\det(A).
    \end{equation}
\end{proposition}

\begin{proof}
    L'élément \( B_{ij}\) de la matrice \( B\) est une combinaison linéaire de tous les éléments de sa ligne :
    \begin{equation}
        B_{ij}=\sum_ka_{kj}A_{ik}=(Aa)_{ij}.
    \end{equation}
    Donc \( B=Aa\). La proposition \ref{PROPooHQNPooIfPEDH} nous dit alors que \( \det(B)=\det(a)\det(A)\).
\end{proof}

\begin{theorem}[de Sylvester\cite{BIBooXOWGooAPWTfT}]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature\footnote{Définition \ref{DEFooWDCLooDkRYLK}.} \( (p,q)\). Alors pour toute base \( Q\)-orthogonale \( \{ e_i \}\) de \( \eR^{p+q}\) nous avons les propriétés suivantes.
    \begin{enumerate}
        \item       \label{ITEMooCFQHooRWfmpT}
            Les nombres \( p\) et \( q\) sont donnée par 
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}             \label{SUBEQooONWLooNsgmQY}   \\
            q&=\Card\{ i\tq Q(e_i)<0 \}.        \label{SUBEQooFKXMooOVwvKR}
        \end{align}
    \end{subequations}
\item       \label{ITEMooWLPVooSTOOjL}
    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\item       \label{ITEMooGOHCooPrNQwm}
    Le rang de \( Q\) est \( p+q\).
    \end{enumerate}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}

\begin{proof}
    Soit  $F$ un sous-espace de dimension maximale $q$ sur lequel $Q$ est définie négative. Le fait que la dimension de \( F\) soit \( q\) est la définition \ref{DEFooWDCLooDkRYLK} de la signature. Nous notons \( F^{\perp}\) sont \( Q\)-orthogonal, c'est-à-dire que
    \begin{equation}
        F^{\perp}=\{ v\in E\tq B(v,x)=0\,\forall x\in F \}.
    \end{equation}
    Le lemme \ref{LEMooUOZOooYvEcji} nous assure que \( E=F\oplus F^{\perp}\).

    Le théorème \ref{THOooIDMPooIMwkqB} sur l'existence de bases \( Q\)-orthogonales nous permet de considérer une base \( Q\)-orthogonale de \( F\) et une de \( F^{\perp}\). En réunissant les deux, nous avons une base de \( E\). Nous la notons \( \{ f_1,\ldots, f_n \}\) avec
    \begin{itemize}
        \item La partie \( \{ f_1,\ldots, f_q \}\) est une base de \( F\),
        \item La partie \( \{ f_{q+1},\ldots, f_n \}\) est une base de \( F^{\perp}\),
        \item Remarquez cependant qu'il n'est pas dit que \( n=q+p\).
    \end{itemize}
    Notons que pour \( i>q\), nous avons \( Q(f_i)\geq 0\), sinon la maximalité de \( F\) serait contredite par \( \Span\{ f_1,\ldots, f_q,f_i \}\).

    Cela prouve que 
    \begin{equation}
        \Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    Le lemme \ref{LEMooISHCooVDJEKo} nous dit alors que
    \begin{equation}
        \Card\{ i\tq Q(e_i)>0 \}=\Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    C'est l'égalité \eqref{SUBEQooFKXMooOVwvKR}. L'égalité \eqref{SUBEQooONWLooNsgmQY} se prouve de la même façon, en prenant \( F\) maximal pour la propriété que \( Q\) y est strictement définie positive.

    Le point \ref{ITEMooCFQHooRWfmpT} est prouvé.

    Dans une base \( Q\)-orthogonale, la matrice de \( Q\) est diagonale, et contient sur la diagonale les valeurs de \( Q(e_i)\). Parmi celles-ci, on en a \( p\) strictement positives et \( q\) strictement négatives. Les \( n-p-q\) autres sont nulles. Vu que \( Q\) est à valeur réelle, nous avons une notion de racine carré, et nous pouvons considérer \( e_i/\sqrt{ | Q(e_i) | }\) au lieu de \( e_i\). De cette façon, \( Q(e_i)\) est normalisé. Avec ça, la matrice de \( Q\) est
    \begin{equation}        \label{EQooLQNRooCsgKVF}
        D=\begin{pmatrix}
            \mtu_p    &       &       \\
                &   -\mtu_q    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
    Nous venons de prouver qu'il existe une base \( \{ e_i \}\) dans laquelle la matrice de \( Q\) est \eqref{EQooLQNRooCsgKVF}. Si \( A\) est la matrice de \( Q\) dans une base quelconque \( \{ f_i \}\) et si \( P\) est la matrice de changement de base \( f_j=\sum_iP_{ij}e_i\), la proposition \ref{PROPooLBIOooUpzxXA} donne \(D= P^tAP\).

    Le point \ref{ITEMooWLPVooSTOOjL} est prouvé.

    Pour \ref{ITEMooGOHCooPrNQwm}, la proposition \ref{PROPooLRZQooSfprff} nous permet de calculer le rang de \( Q\) par le rang de sa matrice dans n'importe quelle base. Nous choisissons la base qui donne la matrice \eqref{EQooLQNRooCsgKVF}. Le rang est alors bien \( p+q\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DEFooGVGGooWQEIET}
    Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\) sur l'espace vectoriel \( V\) sur \( \eK\). Soit \( A\) la matrice de \( q\) dans la base \( \{ e_i \}\) et \( B\) sa matrice dans la base \( \{f_{\alpha}  \}\). Nous supposons que le changement de base est orthogonal.

    Alors les valeurs propres de \( A\) et \( B\) sont les mêmes.

    Ces valeurs sont les \defe{valeurs propres}{valeur propre!forme quadratique} de \( q\).
\end{lemmaDef}

\begin{proof}
    Nous nous rappelons de la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à \( Q\), et à la proposition \ref{PROPooLBIOooUpzxXA} qui parle de changement de base : \( B=Q^tAQ\) où \( Q\) est orthogonale.

    Soit un vecteur propre \( v\) de \(A \), de valeur propre \( \lambda\). Alors nous prouvons que \( Q^tv\) est un vecteur propre pour \( B\), de même valeur propre \( \lambda\). En effet,
    \begin{equation}
        BQ^tv=Q^tAQQ^tv=Q^tAv=\lambda Q^tv
    \end{equation}
    où nous avons utilisé \( QQ^t=\mtu\) et \( Av=\lambda v\).
\end{proof}

\begin{proposition}\label{PropFWYooQXfcVY}
    Dans la base de diagonalisation de sa matrice associée, une forme quadratique a la forme
    \begin{equation}
        q(x)=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de la matrice associée à \( q\).
\end{proposition}

\begin{proof}
    Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée. Si \( \{ f_i \}\) est une base de diagonalisation\footnote{Qui existe parce que la matrice est symétrique, théorème~\ref{ThoeTMXla}.} de la matrice de \( b\) alors dans cette base nous avons
\begin{equation}
    q(x)=b(x,x)=\sum_{ij}x_ix_jb(f_i,f_j)=\sum_i\lambda_ix_i^2
\end{equation}
où les \( \lambda_i\) sont les valeurs propres de la matrice de \( b\).
\end{proof}

Notons que si nous choisissons une autre base de diagonalisation, les \( \lambda_i\) ne changement pas (à part l'ordre éventuellement). 

Cela justifie la définition pour dire que nous nous permettrons de parler des \defe{valeurs propres}{valeur propre!d'une forme quadratique} d'une forme quadratique comme étant les valeurs propres de la matrice associée.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

Le théorème \ref{THOooIDMPooIMwkqB} a déjà donné une base orthogonale pour toute forme quadratique sur un espace vectoriel \( (E,\eK)\) de dimension finie. Dans le cas de \( \eR^n\), nous pouvons en donner une preuve basée sur le théorème spectral, c'est la proposition \ref{PROPooUKRUooGRIDHt}.

\begin{proposition}     \label{PROPooUKRUooGRIDHt}
    Soit une forme bilinéaire symétrique \( b\) sur un \( \eR^n\). Il existe une matrice orthogonale \( Q\) telle que 
    \begin{enumerate}
        \item
            \( D=Q^tbQ\) est diagonale
        \item
            \( D(x,y)=b(Qx,Qy)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Il existe une base \( (f_i)_{i=1,\ldots, n}\) qui est \( b\)-orthogonale.

    Dans cet énoncé, nous mélangeons sans vergogne les formes et les matrices, en supposant qu'une base soit fixée\footnote{Autrement dit, si vous avez en tête d'utiliser cette proposition pour \( \eR^n\) c'est bon; mais sinon vous devez choisir une base et considérer toutes les matrices dans cette base.}. Par exemple
    \begin{equation}
        D(x,y)=\sum_{ij}D_{ij}x_iy_j.
    \end{equation}
\end{proposition}

\begin{proof}
    Pour la matrice diagonale, c'est le théorème spectral \ref{ThoeTMXla}\ref{ITEMooMWWRooXxGONW} qui joue parce que la matrice d'une forme bilinéaire symétrique est symétrique (c'est vu de la définition \eqref{EQooCUGFooRlKUtu}).

    Pour le reste c'est un calcul :
    \begin{subequations}
        \begin{align}
            D(x,y)&=\sum_{ijkl}Q^t_{ik}b_{kl}Q_{lj}x_iy_j\\
            &=\sum_{ijkl}b_{kl}(Q_{ki}x_i)(Q_{lj}y_j)\\
            &=\sum_{kl}b_{kl}(Qx)_k(Qy)_l\\
            &=b(Qx,Qy).
        \end{align}
    \end{subequations}
    Nous avons utilisé le produit matrice fois vecteur donné par \eqref{EQooQFVTooMFfzol}.

    En ce qui concerne l'existence d'une base \( b\)-orthogonale, vu que \( D\) est diagonale, nous avons, pour \( i\neq j\) que \( D(e_i,e_j)=0\). Donc en posant \( f_i=Qe_i\), nous trouvons
    \begin{equation}
        0=D(e_i,e_j)=b(Qe_i,Qe_j)=b(f_i,f_j).
    \end{equation}
    La base \( (Qe_i)_{i=1,\ldots, n}\) est donc \( b\)-orthogonale.
\end{proof}
