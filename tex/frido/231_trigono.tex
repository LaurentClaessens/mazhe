% This is part of (everything) I know in mathematics
% Copyright (c) 2011-2017, 2019, 2021-2022
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Transformations de Lorentz}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous considérons dans cette section un nombre réel \( c>0\) ainsi que l'espace \( \eR^2\) muni du produit pseudo-scalaire\footnote{Définition \ref{DEFooLPBGooXLxubc}.} donné par la matrice
\begin{equation}
	\eta=\begin{pmatrix}
		c^2 & 0  \\
		0   & -1
	\end{pmatrix}.
\end{equation}
Et pour faire plus vrai, nous notons \( (x_0,x_1)\) les coordonnées sur \( \eR^2\). Ainsi
\begin{equation}
	x\cdot y=c^2x_0y_0-x_1y_1.
\end{equation}
Nous insistons sur le fait que cela n'est pas un produit scalaire.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooPZPZooVAdPVj}
	Soit \( c>0\). L'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon \mathopen] -c , c \mathclose[ & \to \eR                                                 \\
			v                                           & \mapsto \frac{ -v/c }{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } }
		\end{aligned}
	\end{equation}
	est une bijection.
\end{lemma}

\begin{proof}
	Nous commençons par mentionner le fait que \( \varphi\) est continue du fait que le dénominateur ne s'annule pas. Une petite étude de fonction montre que
	\begin{equation}
		\lim_{v\to -c} \varphi(v)=+\infty,
	\end{equation}
	et
	\begin{equation}
		\lim_{v\to c} \varphi(v)=-\infty,
	\end{equation}
	et
	\begin{equation}
		\varphi'(v)=-\frac{1}{ c\sqrt{ 1-\frac{ v^2 }{ c^2 } } }-\frac{ v^2/c^3 }{ \left( 1-\frac{ v^2 }{ c^2 } \right)^{3/2} }<0.
	\end{equation}
	Tout cela fait que \( \varphi\) est bijective (entre autres par le théorème des valeurs intermédiaires \ref{ThoValInter} et le théorème dérivée et croissance \ref{PropGFkZMwD}).
\end{proof}

\begin{lemma}       \label{LEMooUZFKooSIjery}
	La forme bilinéaire
	\begin{equation}
		\begin{aligned}
			b\colon \eR^2\times \eR^2 & \to \eR          \\
			x,y                       & \mapsto x\cdot y
		\end{aligned}
	\end{equation}
	est non dégénérée\footnote{Définition \ref{DEFooNUBFooLfCqaK}.}.
\end{lemma}

\begin{proof}
	Soit \( (x_0,x_1)\in \eR^2\) tel que
	\begin{equation}
		b\big( (x_0,x_1), (y_0,y_1) \big)=0
	\end{equation}
	pour tout \( (y_0,y_1)\in \eR^2\). Nous avons
	\begin{equation}
		c^2x_0y_0-x_1y_1=0.
	\end{equation}
	En écrivant cela avec \( (y_0,y_1)=(1,0)\) puis \( (0,1)\) nous obtenons immédiatement que \( (x_0,x_1)=(0,0)\).
\end{proof}

\begin{theorem}     \label{THOooYHDWooWxVovH}
	Soit une bijection\quext{À mon avis, il y a moyen d'affaiblir cette hypothèse. Écrivez-moi si vous avez une idée.} \( f\colon \eR^2\to \eR^2\) telle que
	\begin{equation}
		f(x)\cdot f(y)=x\cdot y
	\end{equation}
	pour tout \( x,y\in \eR^2\). Alors :
	\begin{enumerate}
		\item
		      \( f\) est linéaire.
		\item
		      Il existe un unique choix de \( (x,\sigma_1,\sigma_2)\in \eR\times \{ \pm1 \}\times \{ \pm1 \}\) tel que la matrice de \( f\) ait la forme
		      \begin{equation}
			      f=\begin{pmatrix}
				      \sigma_1\cosh(\xi) & \frac{ \sigma_1\sigma_2 }{ c }\sinh(\xi) \\
				      c\sinh(\xi)        & \sigma_2\cosh(\xi)
			      \end{pmatrix}.
		      \end{equation}
		\item
		      Il existe un unique \( v\in\mathopen] -c , c \mathclose[\) tel que la matrice de \( f\) ait la forme
		      \begin{equation}
			      f=\begin{pmatrix}
				      \frac{ \sigma_1 }{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } } & -\frac{ \sigma_1\sigma_2 }{ c^2 }\frac{ v }{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } } \\
				      \frac{ -v }{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } }       & \frac{ \sigma_2 }{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } }
			      \end{pmatrix}.
		      \end{equation}
	\end{enumerate}
\end{theorem}

\begin{proof}
	Puisque notre produit pseudo-scalaire est non dégénéré (lemme \ref{LEMooUZFKooSIjery}), le fait que \( f\) soit linéaire est la proposition \ref{ThoDsFErq}. Nous posons
	\begin{equation}
		A=\begin{pmatrix}
			\alpha & \beta  \\
			\gamma & \delta
		\end{pmatrix}
	\end{equation}
	et, conformément à la proposition \ref{PROPooSYQMooEnZFdp}, nous imposons \( A^t\eta A=\eta\). Après un petit produit matriciel nous obtenons :
	\begin{equation}
		\begin{pmatrix}
			c^2\alpha^2-\gamma^2        & c^2\alpha\beta-\gamma\delta \\
			c^2\alpha\beta-\gamma\delta & c^2\beta^2-\delta^2
		\end{pmatrix}=\begin{pmatrix}
			c^2 & 0  \\
			0   & -1
		\end{pmatrix}.
	\end{equation}
	Voilà quatre équations à résoudre pour les quatre inconnues \( \alpha, \beta, \gamma, \delta\). Déjà les équations des termes anti-diagonaux sont les mêmes. Nous recopions le reste :
	\begin{subequations}
		\begin{numcases}{}
			c^2\alpha^2-\gamma^2=c^2            \label{SUBEQooXZUGooITKZnH}\\
			c^2\alpha\beta-\gamma\delta=0       \label{SUBEQooDWQRooBeDaPw}\\
			c^2\beta^2-\delta^2=1.              \label{SUBEQooJAFLooGxmbaO}
		\end{numcases}
	\end{subequations}
	C'est le moment d'utiliser la proposition \ref{PROPooWEHGooOBqSHY}. La relation \eqref{SUBEQooXZUGooITKZnH} donne
	\begin{equation}
		\alpha^2-\left( \frac{ \gamma }{ c } \right)^2=1,
	\end{equation}
	ce qui implique l'existence d'un unique\footnote{Par \ref{PROPooWEHGooOBqSHY}.} \( \xi_1\in \eR\) et \( \sigma_1\in \{ \pm 1 \}\) tels que
	\begin{subequations}        \label{SUBEQSooQUSIooRZRYSW}
		\begin{align}
			\gamma & =c\sinh(\xi_1)         \\
			\alpha & =\sigma_1\cosh(\xi_1).
		\end{align}
	\end{subequations}
	La relation \eqref{SUBEQooJAFLooGxmbaO} implique quant à elle l'existence de \( \xi_2\in \eR\) et \( \sigma_2\in\{ \pm 1 \}\) tels que
	\begin{subequations}        \label{SUBEQSooLFHCooXVetmK}
		\begin{align}
			\delta & =\sigma_2\cosh(\xi_2)       \\
			\beta  & =\frac{1}{ c }\sinh(\xi_2).
		\end{align}
	\end{subequations}

	Nous substituons maintenant toutes les valeurs \eqref{SUBEQSooQUSIooRZRYSW} et \eqref{SUBEQSooLFHCooXVetmK} dans \eqref{SUBEQooDWQRooBeDaPw}. Cela donne
	\begin{equation}            \label{EQooHTMSooVYzJUS}
		\sigma_1\cosh(\xi_1)\sinh(\xi_2)=\sigma_2\sinh(\xi_1)\cosh(\xi_2).
	\end{equation}
	Nous mettons cette relation au carré et nous substituons \( \cosh(\xi_1)^2=1+\sinh^2(\xi_1)\). Ce que nous trouvons est
	\begin{equation}
		\sinh(\xi_1)^2=\sinh(\xi_2)^2,
	\end{equation}
	qui implique que \( \xi_1=\pm\xi_2\). Nous posons donc \( \xi_2=\sigma_3\xi_1\) pour un certain \( \sigma_3\in \{ \pm 1 \}\). Cela nous permet d'alléger la notation et d'écrire \( \xi\) au lieu de \( \xi_1\).

	Nous remettons la valeur \( \xi=\xi_1=\sigma_3\xi_2\) dans l'équation \eqref{EQooHTMSooVYzJUS} en tenant compte du fait que \( \sinh\) est impaire et \( \cosh\) est paire :
	\begin{equation}
		\sigma_1\sigma_3\cosh(\xi)\sinh(\xi)=\sigma_2\sinh(\xi)\cosh(\xi).
	\end{equation}
	Et cela nous enseigne que \( \sigma_3=\sigma_1\sigma_2\).

	Jusqu'à présent nous avons prouvé qu'il existe un unique \( \xi\in \eR\) et \( \sigma_1,\sigma_2\in \{ \pm 1 \}\) tels que
	\begin{equation}        \label{EQooYZIVooCTdmSh}
		A=\begin{pmatrix}
			\sigma_1\cosh(\xi) & \frac{ \sigma_1\sigma_2 }{ c }\sinh(\xi) \\
			c\sinh(\xi)        & \sigma_2\cosh(\xi)
		\end{pmatrix}.
	\end{equation}

	Nous utilisons à présent la bijection du lemme \ref{LEMooPZPZooVAdPVj}. Il existe un unique \( v\in \mathopen] -c , c \mathclose[\) tel que \( \sinh(\xi)=\varphi(v)\). En utilisant \( \cosh(\xi)^2=1+\varphi(v)^2\), nous trouvons
	\begin{equation}
		\cosh(\xi)^2=\frac{1}{ 1-\frac{ v^2 }{ c^2 } }.
	\end{equation}
	Mais comme le cosinus hyperbolique est toujours strictement positif, nous pouvons prendre la racine carrée des deux côtés :
	\begin{equation}
		\cosh(\xi)=\frac{1}{ \sqrt{ 1-\frac{ v^2 }{ c^2 } } }.
	\end{equation}
	En substituant dans \eqref{EQooYZIVooCTdmSh}, nous trouvons le résultat annoncé.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Sous-groupe fini d'isométries du plan}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{BIBooULRWooPsjtBE}]       \label{THOooKDMUooUxQqbB}
	Soit un groupe fini \( G\) d'isométries de \( (\eR^2,d)\) contenant \( n \) éléments.
	\begin{enumerate}
		\item       \label{ITEMooYEONooCOMpeb}
		      Il existe un point \( C\in \eR^2\) fixé par tous les éléments de \( G\).
		\item       \label{ITEMooGELWooFFAqkc}
		      Si \( G\) ne contient pas de réflexion, alors il est cyclique\footnote{Définition \ref{DefHFJWooFxkzCF}.} et engendré par la rotation d'angle \( 2\pi/n\) autour de \( C\).
		\item       \label{ITEMooDHKEooFpCfmX}
		      Si \( G\) contient au moins une réflexion, et si \( C\) est un point fixe de \( G\), alors
		      \begin{enumerate}
			      \item       \label{ITEMooGQZTooJIPPLtyf}
			            toutes les réflexions ont un axe qui passe par \( C\),
			      \item       \label{ITEMooKPQRooLquSiQ}
			            \( n\) est pair,
			      \item       \label{ITEMooCHSWooHpDGHf}
			            Si \( \sigma\) est une réflexion dans \( G\), alors nous avons \( G=\gr\big(\sigma,R_C(4\pi/n)\big)\) où \( R_C(\theta)  \) est la rotation d'angle \( \theta\) autour de \( C\),
			      \item       \label{ITEMooROUYooRghvMv}
			            \( G\) est isomorphe au groupe diédral \( D_{n/2}\).
		      \end{enumerate}
	\end{enumerate}
\end{theorem}

\begin{proof}
	Soit un groupe fini \( G\) constitué d'isométries de \( (\eR^2,d)\). Nous prouvons le théorème point par point.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooYEONooCOMpeb}]
		C'est la proposition \ref{PROPooLAEBooWdcBoe}.
		\spitem[Questions de réflexions]
		Le théorème \ref{THOooRORQooTDWFdv} nous dit que les éléments de \( G\) sont des compositions d'au maximum \( 3\) réflexions.
		\spitem[Exclure trois réflexions]
		Il n'est pas possible que \( G\) contienne un élément composé de trois réflexions. En effet, les composées de trois réflexions, par le théorème \ref{THOooVRNOooAgaVRN} sont des réflexions glissées\footnote{Définition \ref{DEFooJEOYooNwYtuQ}.}, c'est-à-dire des transformations de la forme \( g=\tau_v\circ \sigma_{\ell}\) où \( v\) est un vecteur parallèle à la droite \( \ell\). Si \( x\in \ell\), alors
		\begin{equation}
			g(x)=\tau_v(x)=x+v,
		\end{equation}
		de telle sorte que \( g^k(x)=x+kv\), qui signifie que tous les \( g^k\) sont différents. Le groupe \( G\) ne peut pas être fini si il contient une réflexion glissée.

		\spitem[\( G^+\) et \( G^-\)]
		Pour la même raison que celle qui exclut les réflexions glissées, \( G\) ne peut pas contenir de translation. Le théorème \ref{THOooVRNOooAgaVRN} nous donne la liste des possibilités. Après exclusion des translations et des réflexions glissées, il reste :
		\begin{itemize}
			\item l'identité,
			\item les rotations,
			\item les réflexions.
		\end{itemize}
		Nous notons \( G^+\) la partie de \( G\) contenant l'identité et les rotations et \( G^-\) celle contenant les réflexions. Notons que \( G^+\) n'est pas vide parce qu'il contient au moins l'identité, tandis que \( G^-\) peut être vide, mais n'est certainement pas un groupe.
		\spitem[Même nombre d'éléments]
		Nous prouvons à présent que si \( G^-\) est non vide, alors il a le même nombre d'éléments que \( G\). Un élément de \( G^-\) est une réflexion. Soit \( \sigma\in G^-\). Nous prouvons que
		\begin{equation}        \label{EQooWRVVooBQCtPg}
			\begin{aligned}
				\varphi\colon G^+ & \to G^-               \\
				f                 & \mapsto \sigma\circ f
			\end{aligned}
		\end{equation}
		est une bijection.

		\begin{subproof}
			\spitem[Surjective]
			Soit \( s\in G^-\). Posons \( f=\sigma^{-1}\circ s\). Puisque \( \sigma^{-1}\) et \( s\) sont des réflexions, \( f\) est une rotation. Donc \( f\in G^+\) et \( \varphi(f)=s\).
			\spitem[Injective]
			La condition \( \varphi(f)=\varphi(g)\) dit que \( \sigma\circ f=\sigma\circ g\). En composant par \( \sigma^{-1}\) nous obtenons \( f=g\).
		\end{subproof}
		\spitem[\( G=\gr\big(R_C(2\pi /p)\big)\)]
		Nous nommons \( p\) le nombre d'éléments de \( G^+\). Si \( G^-\) est vide, \( p=n\), et sinon \( p=n/2\). Dans les deux cas, \( G^+\) est un groupe de rotations à \( p\) éléments.

		Le groupe \( G^+\) contient seulement des rotations; or le centre d'une rotation est l'unique point fixe. Donc tous les éléments de \( G^+\) sont des rotations autour de \( C\).

		Le corolaire \ref{CorpZItFX} au théorème de Lagrange nous indique que tous les éléments de \( G^+\) vérifient \( g^p=\id\). Seules les rotations d'angle \( 2k\pi/p\) autour de \( C\) satisfont la condition \( g^p=\id\). Or il n'y a que \( p\) telles rotations. Donc elles sont toutes dans \( G^+\). Nous en déduisons que
		\begin{equation}        \label{EQooUWTVooEMqkVH}
			G^+=\gr\big( R_C(2\pi/p) \big).
		\end{equation}
		\spitem[Pour \ref{ITEMooGELWooFFAqkc}]
		Dans le cas où \( G\) ne contient pas de réflexion, \( G^-\) est vide et \( G\) contient \( n\) éléments. La relation \eqref{EQooUWTVooEMqkVH} devient
		\begin{equation}
			G=G^+=\gr\big( R_C(2\pi/n) \big).
		\end{equation}
		\spitem[Pour \ref{ITEMooDHKEooFpCfmX}]
		Nous supposons maintenant que \( G\) contienne au moins une réflexion. De la sorte \( G^-\neq \emptyset\).
		\begin{subproof}
			\spitem[Pour \ref{ITEMooGQZTooJIPPLtyf}]
			Les seuls points fixes d'une réflexion sont ceux de l'axe. Donc \( C\) doit être sur tous les axes des réflexions contenues dans \( G^-\).

			Notons au passage que deux réflexions d'axes qui se coupent forment une rotation. Donc \( G^-\) ne forme pas un groupe, mais même pas en rêve, quoi.
			\spitem[Pour \ref{ITEMooKPQRooLquSiQ}]
			Vu que l'union \( G=G^+\cup G^-\) est disjointe et que \( G^+\) et \( G^-\) ont le même nombre d'éléments par la bijection \ref{EQooWRVVooBQCtPg}, si \( G^-\) est non vide, \( G\) possède un nombre pair d'éléments.
			\spitem[Pour \ref{ITEMooCHSWooHpDGHf}]
			Si \( \sigma\in G\) est une réflexion, nous savons que \( n\) est pair, que \( G^+\) possède \( p=n/2\) éléments et que
			\begin{equation}
				G^+=\{  R_C(2k\pi/p)  \}=\{  R_C(4k\pi/n) \}_{k=1,\ldots, n/2}.
			\end{equation}
			L'élément \( \sigma\in G^- \) étant fixé, la bijection \eqref{EQooWRVVooBQCtPg} nous indique que tous les éléments de \( G^-\) sont de la forme \( \sigma\circ f\) avec \( f\in G^+\). Donc
			\begin{equation}
				G^{-}\subset \gr\big( \sigma, R_C(4\pi/n)  \big).
			\end{equation}
			Nous avons aussi
			\begin{equation}
				G^+\subset \gr\big( \sigma,  R_C(4\pi/n)  \big).
			\end{equation}
			Et comme \( \sigma\) et \(  R_C(4\pi/n)  \) sont dans \( G\) nous avons \( \gr\big( \sigma ,  R_C(4\pi/n)  \big)\subset G\). Tout cela pour dire que
			\begin{equation}
				G=\gr\big( \sigma,  R_C(4\pi/n) \big).
			\end{equation}

			\spitem[\( R\sigma = \sigma R^{-1}\)]
			Nous restons dans le cas où \( G^-\) n'est pas vide. Nous considérons \( R\), la rotation d'angle \( \theta\) autour de \( C\). Si \( R_0\) est la rotation d'angle \( \theta\) autour de \( (0,0)\), nous avons
			\begin{equation}
				R=\tau_C\circ R_0\circ \tau_C^{-1},
			\end{equation}
			et si \( \sigma_0\) est la symétrie d'axe parallèle à l'axe de \( \sigma\), mais passant par \( (0,0)\) nous avons :
			\begin{equation}
				\sigma=\tau_C\circ\sigma_0\circ\tau_C^{-1}.
			\end{equation}
			Si \( v\) est le vecteur directeur de la réflexion \( \sigma_0\), nous considérons enfin \( \alpha\), la rotation qui réalise \( \alpha(v)=(1,0)\). Nous avons alors
			\begin{equation}
				\sigma_0=\alpha^{-1}\circ s\circ \alpha
			\end{equation}
			où \( s\) est la symétrie autour de l'axe horizontal. En n'ayant pas peur d'identifier \( \eR^2\) à \( \eC\), l'application \( s\) est la conjugaison complexe. Avec tout ça nous avons
			\begin{equation}
				R\sigma=\tau_CR_0\tau_C^{-1}\tau_C\sigma_0\tau_C^{-1}=\tau_CR_0\alpha^{-1}s\alpha\tau_C^{-1}=\tau_C\alpha^{-1}R_0s\alpha\tau_C^{-1}
			\end{equation}
			où nous avons utilisé le fait que les rotations autour de \( (0,0)\) forment un groupe abélien pour commuter \( \alpha^{-1}\) avec \( R_0\). Nous utilisons à présent le lemme \ref{LEMooBNJFooAbhsUa} pour commuter \( R\) avec \( s\) :
			\begin{subequations}
				\begin{align}
					R\sigma & =\tau_C\alpha^{-1}sR_0^{-1}\alpha\tau_C^{-1}                          \\
					        & =\tau_C\underbrace{\alpha^{-1}s\alpha}_{\sigma_0} R_0^{-1}\tau_C^{-1} \\
					        & =\tau_C\sigma_0\tau_C^{-1}\tau_CR_0^{-1}\tau_C^{-1}                   \\
					        & =\sigma R^{-1}.
				\end{align}
			\end{subequations}
			Nous avons utilisé le fait que \( \tau_CR_0^{-1}\tau_C^{-1}=R^{-1}\) comme on peut s'en convaincre en calculant le produit.

			\spitem[Table de multiplication]

			Nous considérons une réflexion \( \sigma\in G\). Les éléments de \( G^+ \) sont des rotations autour de \( C\) et ceux de \( G^-\) de la forme \( \sigma R\) où \( R\) est une rotation autour de \( C\). Pour connaître la table de multiplication de \( G\), nous devons écrire
			\begin{equation}
				(\sigma^{\epsilon_1}R^k)(\sigma^{\epsilon_2}R^l)=\sigma^{\epsilon}R^m
			\end{equation}
			où \( \epsilon_1,\epsilon_2\in \{ 0,1 \}\), \( R\) est la rotation d'angle \( 4\pi/n\) autour de \( C\) et \( \epsilon\) et \( m\) sont des constantes à exprimer en fonction de \( \epsilon_1\), \( \epsilon_2\), \( k\) et \( l\).

			Tous les éléments de \( G\) pouvant être écrits soit sous la forme \( R^m\), soit sous la forme \( \sigma R^m\), nous avons les possibilités suivantes :
			\begin{enumerate}
				\item
				      \( R^mR^l=R^{m+l}\)
				\item
				      \( (R^m)(\sigma R^l)=\sigma R^{-m}R^l=\sigma R^{l-m}\)
				\item
				      \( (\sigma R^m)R^l=\sigma R^{m+l}\)
				\item
				      \( (\sigma R^m)(\sigma R^l)=\sigma\sigma R^{l-m}=R^{l-m}\).
			\end{enumerate}

			\spitem[Pour \ref{ITEMooROUYooRghvMv}]
			Récoltons quelque acquis.
			\begin{itemize}
				\item
				      Nous venons de prouver que \( R\sigma=\sigma R^{-1}\).
				\item
				      Tout élément de \( G\) peut s'écrire soit sous la forme \( R^m\), soit sous la forme \( \sigma R^m\), selon que l'élément est dans \( G^+\) ou \( G^-\).
				\item
				      Tout élément du groupe diédral \( D_n\) s'écrit soit sous la forme \( r^m\), soit sous la forme \( sr^m\) (proposition \ref{PropLDIPoZ}\ref{ITEMooOEBHooULRmZk}).
			\end{itemize}
			L'application \( \varphi\colon G\to D_n\) suivante est donc une bijection :
			\begin{subequations}
				\begin{numcases}{}
					\varphi(R^m)=r^m\\
					\varphi(\sigma R^m)=sr^m.
				\end{numcases}
			\end{subequations}
			Il nous reste à prouver que c'est un morphisme. Cela se fait en utilisant la table de multiplication du groupe diédral donnée dans la proposition \ref{PROPooPYDLooLgiUjk} et celle du groupe \( G\) que nous venons d'écrire.
		\end{subproof}
	\end{subproof}
\end{proof}

\begin{definition}[Groupe de symétrie d'une partie de \( \eR^n\)\cite{ooZYLAooXwWjLa}]
	Si \( Y\) est une partie de \( \eR^n\), nous définissons le \defe{groupe des symétries}{groupe!des symétries} de \( Y\) par
	\begin{equation}
		\Sym(Y)=\{ f\in\Isom(\eR^n)\tq f(Y)=Y \}.
	\end{equation}
	Nous définissons aussi le groupe des symétries propres de \( Y\) par
	\begin{equation}
		\Sym^+(Y)=\{ f\in\Isom^+(\eR^n)\tq f(Y)=Y \}
	\end{equation}
    où \( \Isom^+(\eR^n)\) est le groupe des isométries positives de \( \eR^n\), définition \ref{DEFooOKGSooUhDIfu}.
\end{definition}

\begin{theorem}[\cite{ooZYLAooXwWjLa}]      \label{THOooAYZVooPmCiWI}
	Soit \( Y\subset \eR^2\) tel que le groupe \( \Sym^+(Y)\) soit fini d'ordre \( n\). Alors c'est un groupe cyclique d'ordre \( n\).

	Si \( \Sym^+(Y)\) est fini, alors \( \Sym(Y)\) est soit cyclique\footnote{Définition \ref{DefHFJWooFxkzCF}.} d'ordre \( n\), soit isomorphe au groupe diédral\footnote{Définition \ref{DEFooIWZGooAinSOh}.} d'ordre \( 2n\).
\end{theorem}

\begin{proof}
	Nous savons déjà par la proposition~\ref{PROPooEUFIooDUIYzi} que \( \Sym^+(Y)\) est isomorphe à un sous-groupe \( H^+\) d'ordre \( n\) de \( \SO(2)\). Vérifions que ce groupe est cyclique. Si \( n=1\), c'est évident. Si \( n\geq 2\) alors nous savons que \( H^+\) est constitué de rotations d'angles dans \( \mathopen[ 0 , 2\pi \mathclose[\) et vu que c'est un ensemble fini, il possède une rotation d'angle minimal (à part zéro). Notons \( \alpha_0\) cet angle.

	Nous montrons que \( H^+\) est engendré par la rotation d'angle \( \alpha_0\). Soit une rotation d'angle \( \alpha\). Étant donné que \( \alpha_0<\alpha\) nous pouvons effectuer la division euclidienne\footnote{Théorème~\ref{ThoDivisEuclide}.} de \( \alpha\) par \( \alpha_0\) et obtenir
	\begin{equation}
		\alpha=k\alpha_0+\beta
	\end{equation}
	avec \( \beta<\alpha_0\). Mézalors \( R(\beta)=R(\alpha)R(\alpha_0)^{-k}\) est également un élément du groupe. Cela contredit la minimalité dès que \( \beta\neq 0\). Avoir \( \beta=0\) revient à dire que \( \alpha\) est un multiple de \( \alpha_0\), ce qui signifie que le groupe \( H^+\) est cyclique engendré par \( \alpha_0\).

	Notons au passage que nous avons automatiquement \( \alpha_0=\frac{ 2\pi }{ n }\) parce qu'il faut \( R(\alpha_0)^n=\id\). Nous avons prouvé que \( \Sym^+(Y) \) est cyclique d'ordre \( n\).

	Nous étudions maintenant le groupe \( \Sym(Y)\). Par la proposition~\ref{PROPooEUFIooDUIYzi} nous avons un morphisme injectif
	\begin{equation}
		\phi\colon \Sym(Y)\to \gO(2),
	\end{equation}
	et en posant \( H=\phi\big( \Sym(Y) \big)\) nous avons un isomorphisme de groupes \( \phi\colon \Sym(Y)\to H\). Nous savons aussi que ce \( \phi\) se restreint en
	\begin{equation}
		\phi\colon   \Sym^+(Y) \to H^+\subset\SO(2)
	\end{equation}
	où \( H^+=\phi\big( \Sym^+(Y) \big)=H\cap\SO(2)\). Le groupe \( H^+\) est cyclique et est engendré par la rotation \( R(2\pi/n)\).

	Supposons un instant que \( H\subset \SO(2)\). Alors nous avons \( H=H^+\) et \( \phi\) est un isomorphisme entre \( \Sym(Y)\) et le groupe cyclique engendré par \( R(2\pi/n)\).

	Nous supposons à présent que \( H\) n'est pas un sous-ensemble de \( \SO(2)\). Quelles sont les isométries de \( \eR^2\) qui ne sont pas de déterminant \( 1\) ? Il faut regarder dans le théorème~\ref{THOooVRNOooAgaVRN} quelles sont les isométries contenant un nombre impair de réflexions. Ce sont les réflexions et les réflexions glissées. Or il ne peut pas y avoir de réflexion glissée dans un groupe fini, parce que si \( f\) est une réflexion glissée, tous les \( f^k\) sont différents.

	Nous en déduisons que si \( H\) n'est pas inclus dans \( \SO(2)\), il contient une réflexion que nous nommons \( \sigma\). Nous allons en déduire que \( H\simeq H^+\times_{\AD}C_2\) où \( C_2=\{ \id,\sigma \}\). Si \( h\in H\) nous pouvons écrire
	\begin{equation}
		h=(h\sigma^{\epsilon})\sigma^{\epsilon}
	\end{equation}
	pour n'importe quelle valeur de \( \epsilon\), et en particulier pour \( \epsilon=\pm 1\).

	Si \( h\in \SO(2)\) alors nous écrivons \( h=h\epsilon^{0}\) et si \( h\notin\SO(2)\) nous écrivons \( h=(h\sigma)\sigma\). Vu que \( h\sigma\in\SO(2)\), cette dernière écriture est encore de la forme \( \SO(2)\times C_2\). Quoi qu'il en soit, tout élément de \( H\) s'écrit comme un produit
	\begin{equation}
		H=H^+C_2.
	\end{equation}
	Cette décomposition est unique parce que si \( h_1c_1=h_2c_2\) alors \( h_2^{-1}h_1=c_2c_1^{-1}\), et comme \( h_2^{-1}h_1\in H^+\) nous avons \( c_2c_1^{-1}\in H^+\) et donc \( c_1=c_2\). Partant, nous avons aussi \( h_1=h_2\). Pour avoir le produit semi-direct, il faut encore montrer que \( \AD(C_2)H^+\subset H^+\). Le seul cas à vérifier est \( \AD(\sigma)H^+\subset H^+\). Comme les éléments de \( H^+\) sont caractérisés par le fait d'avoir un déterminant positif, nous avons
	\begin{equation}
		\AD(\sigma)R(\alpha)=\sigma R(\alpha)\sigma^{-1}\in H^+.
	\end{equation}
\end{proof}

\begin{remark}
	Tout ceci est cohérent avec le théorème de Burnside~\ref{ThooJLTit} parce que le sous-groupe fini de \( \SO(n)\) engendré par la rotation \( R(2\pi/n)\) est un groupe d'exposant fini, à savoir que si \( h\) est dans ce groupe, \( h^n=\id\).
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Relations trigonométriques dans un triangle rectangle}
%---------------------------------------------------------------------------------------------------------------------------

Nous donnons maintenant quelques relations trigonométriques classiques dans un triangle rectangle. Le théorème de Pythagore est déjà le théorème \ref{THOooHXHWooCpcDan}; nous nous concentrons ici sur les angles.

\begin{proposition}     \label{PROPooCDZVooKOQzct}
	Soient \( A,B,S\in \eR^2\) des points distincts et non alignés formant un triangle rectangle en \( A\) :
	\begin{equation}
		(A-S)\cdot (B-A)=0.
	\end{equation}
	En posant \( \theta=\reallywidehat{ASB}\) nous avons
	\begin{equation}
		\cos(\theta)=\frac{ \| A-S \| }{ \| B-S \| }
	\end{equation}
	et
	\begin{equation}        \label{EQooEKZEooFeNImX}
		\sin(\theta)=\pm\frac{ \| B-A \| }{ \| B-S \| }.
	\end{equation}

\end{proposition}

\begin{proof}
	Nous posons \( C=A-S\) et \( D=B-S\). Comme \( C\neq 0\), il existe une rotation \( R_{\alpha}\) telle que
	\begin{subequations}
		\begin{numcases}{}
			(R_{\alpha}C)_x>0\\
			(R_{\alpha}C)_y=0.
		\end{numcases}
	\end{subequations}
	Nous posons \( X=R_{\alpha}C\) et \( Y=R_{\alpha}D\).

	Le triangle formé de \( O\), \( X\) et \( Y\) est «posé» sur l'axe des abscisses et est rectangle en \( X\), c'est-à-dire
	\begin{equation}
		X\cdot (Y-X)=0.
	\end{equation}
	De ce fait, le point \( Y\) satisfait à \( Y_x=X_x\). Et enfin, grâce aux propositions \ref{PROPooKVSHooRODGWE} et \ref{PROPooYWKJooRjybUJ} nous avons \( \widehat{ASB}=\widehat{XOY}\).

	Nous écrivons les relations qui définissent l'angle \( \widehat{XOY}\). Pour cela nous posons \( X'=X/\| X \|\) et \( Y'=Y/\| Y \|\) et nous avons
	\begin{subequations}        \label{SUBEQooVHNDooPOfbjC}
		\begin{numcases}{}
			\cos(\theta)=X'_xY'_x\\
			\sin(\theta)=X'_yY'_y.
		\end{numcases}
	\end{subequations}
	Vu que \( X=(X_x,0)\), nous avons \( X'_x=1\). De plus
	\begin{equation}
		\| Y \|=\| R_{\alpha}(D) \|=\| D \|=\| B-S \|.
	\end{equation}
	En substituant les valeurs dans \eqref{SUBEQooVHNDooPOfbjC},
	\begin{equation}
		\cos(\theta)=Y'_x=\frac{ Y_x }{ \| Y \| }=\frac{ X_x }{ \| B-S \| }=\frac{ \| C \| }{ \| B-S \| }=\frac{ \| A-S \| }{ \| B-S \| }.
	\end{equation}
	Voilà déjà une chose de prouvée.

	Pour la seconde, nous avons \( \sin(\theta)=Y'_y\). Selon le signe de \( Y_y\) nous avons \( Y_y=\pm\| Y-X \|\) et donc
	\begin{equation}
		\sin(\theta)=Y'_y=\frac{ Y_y }{ \| Y \| }=\frac{ \pm\| Y-X \| }{ \| Y \| }=\pm\frac{ \| D-C \| }{ \| Y \| }=\pm\frac{ \| B-A \| }{ \| B-S \| }.
	\end{equation}
\end{proof}

Le signe sur la formule du sinus revient au fait que la définition de l'angle \( \widehat{AOB}\) est de considérer la rotation qui fait aller \( A\) vers \( B\). Donc suivant la position relative de \( A\), \( O\) et \( B\), il se peut que l'angle mesuré soit l'angle \emph{extérieur} au triangle.

La proposition suivante est parfois prise comme définition de l'angle.
\begin{proposition}
	Soient trois points non alignés \( A,S,B\in \eR^2\). Nous avons
	\begin{equation}        \label{EQooOWULooVQntyE}
		\cos\big( \widehat{ASB} \big)=\frac{ (A-S)\cdot(B-S) }{ \| A-S \|\| B-S \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous posons \( C=A-S\), \( D=B-S\), \( X=C/\| C \|\) et \( Y=D/\| D \|\). Avec cela, la définition \ref{DEFooUPUUooKAPFrh} donne l'équation
	\begin{equation}
		\begin{pmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{pmatrix}\begin{pmatrix}
			X_x \\
			X_y
		\end{pmatrix}=\begin{pmatrix}
			Y_x \\
			Y_y
		\end{pmatrix}
	\end{equation}
	que nous écrivons comme le système
	\begin{subequations}
		\begin{numcases}{}
			X_x\cos(\theta)-X_y\sin(\theta)=Y_x\\
			X_y\cos(\theta)+X_x\sin(\theta)=Y_y.
		\end{numcases}
	\end{subequations}
	Nous considérons maintenant cela comme un système pour \( \big( \cos(\theta), \sin(\theta) \big)\) :
	\begin{equation}
		\begin{pmatrix}
			X_x & -X_y \\
			X_y & X_x
		\end{pmatrix}\begin{pmatrix}
			\cos(\theta) \\
			\sin(\theta)
		\end{pmatrix}=\begin{pmatrix}
			Y_x \\
			Y_y
		\end{pmatrix}.
	\end{equation}
	Le déterminant de la dernière matrice est \( X_x+X_y^2=\| X \|^2=1\) parce que \( X\) est unitaire. Cette matrice est donc inversible et son inverse est vite calculée. Nous avons
	\begin{equation}
		\begin{pmatrix}
			\cos(\theta) \\
			\sin(\theta)
		\end{pmatrix}=\begin{pmatrix}
			X_x  & X_y \\
			-X_y & X_x
		\end{pmatrix}\begin{pmatrix}
			Y_x \\
			Y_y
		\end{pmatrix}.
	\end{equation}
	Cela donne ce que nous voulions :
	\begin{equation}
		\cos(\theta)=X\cdot Y=\frac{ C\cdot D }{ \| C \|\| D \| }=\frac{ (A-S)\cdot(B-S) }{ \| A-S \|\| B-S \| }.
	\end{equation}
\end{proof}

\begin{remark}
	Prendre la formule \eqref{EQooOWULooVQntyE} 
	\begin{equation}     
		\cos\big( \widehat{ASB} \big)=\frac{ (A-S)\cdot(B-S) }{ \| A-S \|\| B-S \| }.
	\end{equation}
    comme définition de l'angle \( \widehat{ASB}\) est cependant trompeur parce que ça ne permet de définir les angles que sur une partie de \( \mathopen[ 0 , 2\pi \mathclose[\) sur laquelle le cosinus est injectif. Pour réellement définir tous les angles, il faut alors un peu bricoler.

        Sans vouloir être méchant, je crois que ceux qui procèdent de cette manière sont ceux qui donnent un cours sur le produit scalaire sans avoir l'intention de lier la définition d'une rotation comme composée de réflexions aux matrices de \( \SO(2)\) et aux fonctions trigonométriques.
\end{remark}
