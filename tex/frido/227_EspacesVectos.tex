% This is part of Le Frido
% Copyright (c) 2008-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Changement de base}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit un espace vectoriel \( E\) muni de deux bases \( (e_i)_{i=1,\ldots, n}\) et \( (f_{\alpha})_{\alpha=1,\ldots, n}\). Les deux bases sont liées entre elles par
\begin{equation}        \label{EQooFRQRooSMsQQB}
	f_{\alpha}=\sum_i Q_{i,\alpha}e_i.
\end{equation}
Ici \( Q\) n'est pas une application linéaire \( E\to E\) : \( Q\) est seulement un tableau de nombres, donnant les coordonnées des vecteurs \( f_{\alpha}\) dans la base de \( e_i\). Éventuellement \( Q\) peut être vu comme une application linéaire \( \eK^n\to \eK^n\).

Dans la suite nous nommerons \( Q^{-1}\) la matrice inverse de \( Q\). Inverse au sens des bêtes tableaux de nombres, sans interprétation en tant qu'application linéaire. De même pour \( Q^t\) qui est la transposée de \( Q\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : vecteurs de base}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooIHZGooOZoYZd}
	Soit un espace vectoriel \( E\) sur \( \eK\) ainsi que deux bases \( (e_i)_{i=1,\ldots, n}\), \( (f_{\alpha})_{\alpha=1,\ldots, n}\) de \( E\) liées par \( f_{\alpha}=\sum_i Q_{i,\alpha}e_i\). Alors
	\begin{equation}    \label{EQooZQPAooAbKAdg}
		e_i=\sum_{\alpha}Q^{-1}_{\alpha, i}f_{\alpha}.
	\end{equation}
\end{lemma}

\begin{proof}
	Nous multiplions l'égalité \( f_{\alpha}=\sum_i Q_{i,\alpha}e_i\) par le nombre\footnote{Attention à la bonne interprétation de ce nombre : on fait bien référence à l'élément situé en \( (\alpha, j) \) de la matrice \( Q^{-1} \), et pas autre chose.} \( Q^{-1}_{\alpha, j}\in \eK\)  et nous sommons sur \( \alpha\) :
	\begin{equation}
		\sum_{\alpha}Q^{-1}_{\alpha, j}f_{\alpha}=\sum_{i\alpha}(A_{i,\alpha}Q^{-1}_{\alpha, j})e_i=e_j.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : coordonnées}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooNYYOooHqHryX}
	Soit un espace vectoriel \( E\) sur \( \eK\). Soient deux bases \( (e_i)_{i=1,\ldots, n}\) et \( (f_{\alpha})_{\alpha=1,\ldots, n}\) liées par \( f_{\alpha}=\sum_i Q_{i,\alpha}e_i\). Nous considérons un même vecteur dans les deux bases : \( \sum_i x_ie_i=\sum_{\alpha}y_{\alpha}f_{\alpha}\). Alors
	\begin{enumerate}
		\item       \label{ITEMooIBAEooNaUnPD}
		      \( y_{\alpha}=\sum_i Q^{-1}_{\alpha, i}x_i\)
		\item       \label{ITEMooKPWTooMwdbPu}
		      \( x_i=\sum_{\alpha} Q_{i,\alpha}y_{\alpha}\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Soit un vecteur \( x\in E\). Il peut être écrit dans les deux bases :
	\begin{equation}    \label{EQooGSJMooGQstMx}
		x=\sum_i x_ie_i=\sum_{\alpha}y_{\alpha}f_{\alpha}.
	\end{equation}
	En remplaçant \( e_i\) par sa valeur \eqref{EQooZQPAooAbKAdg} nous avons l'égalité
	\begin{equation}
		\sum_{i\alpha}x_iQ^{-1}_{\alpha, i}f_{\alpha}=\sum_{\alpha}y_{\alpha}f_{\alpha}.
	\end{equation}
	Puisque les \( f_{\alpha}\) sont linéairement indépendants, l'égalité des sommes donne l'égalité de chacun des termes :
	\begin{equation}
		y_{\alpha}=\sum_i x_iQ^{-1}_{\alpha, i}.
	\end{equation}
	En identifiant \( x\in E\) au vecteur dans \( \eK^n\) de ses coordonnées dans la base \( \{ e_i \}\) nous pouvons écrire
	\begin{equation}
		y_{\alpha}=(Q^{-1}x)_{\alpha},
	\end{equation}
	Le point \ref{ITEMooIBAEooNaUnPD} est prouvé.

	En ce qui concerne le point \ref{ITEMooKPWTooMwdbPu}, nous repartons encore de \eqref{EQooGSJMooGQstMx}, mais nous y substituons la définition des \( f_{\alpha}\) :
	\begin{equation}
		\sum_{i}x_ie_i=\sum_{\alpha i}y_{\alpha}Q_{i,\alpha}e_i.
	\end{equation}
	Vous voulez des détails ? Allez, une étape de plus que le strict nécessaire : nous écrivons
	\begin{equation}
		\sum_i\big( x_i-\sum_{\alpha}y_{\alpha}Q_{i,\alpha} \big)e_i=0.
	\end{equation}
	Par linéaire indépendance des \( e_i\), nous avons annulation de tous les coefficients, c'est-à-dire
	\begin{equation}
		x_i=\sum_{\alpha}Q_{i,\alpha}y_{\alpha},
	\end{equation}
	comme annoncé.
\end{proof}

\begin{normaltext}
	Attention à l'ordre des indices dans la dernière égalité : la matrice \( Q\) vient avec les indices dans l'ordre \( i,\alpha\), tandis que la matrice \( Q^{-1}\) vient avec les indices dans l'ordre opposé : \( \alpha, i\). C'est pour cela qu'il est intéressant de noter avec des lettres latines les indices se rapportant à la première base, et avec des lettres grecques ceux se rapportant à la seconde base.
\end{normaltext}

\begin{normaltext}      \label{NORMooNWKZooPMwYTO}
	Les formules de changement de coordonnées de la proposition \ref{PROPooNYYOooHqHryX} s'écrivent souvent de la façon suivante :
	\begin{enumerate}
		\item       \label{ITEMooLHQCooBRvSlp}
		      \( y_{\alpha}=(Q^{-1}x)_{\alpha}\)
		\item       \label{ITEMooNXUGooJIeoBf}
		      \( y=Q^{-1}x\).
		\item       \label{ITEMooEFILooNENamW}
		      \( x_i=(Qy)_i\)
		\item       \label{ITEMooMOKHooFEJvIW}
		      \( x=Qy\)
	\end{enumerate}
	Ces égalités reposent sur un petit paquet d'abus de notations qu'il convient de bien comprendre. Ici, \( x\) et \( y\) sont les éléments de \( \eK^n\) donnés par les composantes de \( x\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\), et \( Q\) est vu comme une matrice, un opérateur linéaire sur \( \eK^n\). Autrement dit, le choix des bases permet d'identifier \( E\) avec \( \eK^n\) et la matrice \( Q\) avec l'application linéaire \( f_Q\) de la proposition \ref{PROPooGXDBooHfKRrv}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une application linéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooNZBEooWyCXTw}
	Soit une application linéaire \( t\colon E\to E\) de matrices \( A\) et \( B\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\). Si les bases sont liées par
	\begin{equation}
		f_{\alpha}=\sum_i Q_{i,\alpha}e_i,
	\end{equation}
	alors les matrices \( A\) et \( B\) sont liées par
	\begin{equation}
		B=Q^{-1}AQ.
	\end{equation}
\end{proposition}

\begin{proof}
	L'hypothèse sur le fait que \( A\) et \( B\) sont les matrices de \( t\) signifie que pour tout \( x\in E\),
	\begin{equation}
		t(x)=\sum_{ij}A_{j,i}x_ie_j=\sum_{\alpha\beta}B_{\alpha,\beta}y_{\beta}f_{\alpha}.
	\end{equation}
	En remplaçant \( e_j\) par son expression \eqref{EQooZQPAooAbKAdg} en termes des \( f_{\alpha}\) et \( x_i\) par son expression \( x_i=(Qy)_i\) (proposition \ref{PROPooNYYOooHqHryX}), nous avons
	\begin{subequations}
		\begin{align}
			(By)_{\alpha} & =\sum_{ij\alpha}A_{j,i}(Qy)_iQ^{-1}_{\alpha, j}f_{\alpha} \\
			              & =\sum_{i \alpha}(Q^{-1}A)_{\alpha, i}(Qy)_i f_{\alpha}    \\
			              & =\sum_{\alpha}(Q^{-1} AQy)_{\alpha}f_{\alpha}.
		\end{align}
	\end{subequations}
	Puisque les \( f_{\alpha}\) forment une base, nous en déduisons \( Q^{-1}AQy=By\). Et comme \( y\) est un élément quelconque de \( \eK^n\), nous en déduisons l'égalité de matrices
	\begin{equation}    \label{ooWKTYooOJfclT}
		B=Q^{-1}AQ.
	\end{equation}
\end{proof}
Il s'agit bien d'une égalité de matrices ou, à la limite, d'applications linéaires sur \( \eK^n\), et non d'une égalité d'application linéaire sur \( E\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Vandermonde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]  \label{PropnuUvtj}
	Le \defe{déterminant de Vandermonde}{déterminant!Vandermonde}\index{Vandermonde (déterminant)} est le polynôme en \( n\) variables donné par
	\begin{equation}
		V(T_1,\ldots, T_n)=\det\begin{pmatrix}
			1         & 1         & \ldots & 1         \\
			T_1       & T_2       & \ldots & T_n       \\
			\vdots    & \ddots    & \ddots & \vdots    \\
			T_1^{n-1} & T_2^{n-1} & \ldots & T_n^{n-1}
		\end{pmatrix}=\prod_{1\leq i<j\leq n}(T_j-T_i).
	\end{equation}
	Notez que l'inégalité du milieu est stricte (sinon d'ailleurs l'expression serait nulle).
\end{proposition}

\begin{proof}
	Nous considérons le polynôme
	\begin{equation}
		f(X)=V(T_1,\ldots, T_{n-1},X)\in \big( \eK[T_1,\ldots, T_{n-1}] \big)[X].
	\end{equation}
	C'est un polynôme de degré au plus \( n-1\) en \( X\) et il s'annule aux points \( T_1,\ldots, T_{n-1}\). Par conséquent\footnote{Proposition \ref{PropHSQooASRbeA}.} nous pouvons factoriser les \( X-T_i\), c'est-à-dire qu'il existe \( \alpha\in \eK[T_1,\ldots, T_{n-1}]\) tel que
	\begin{equation}    \label{EqeVxRwO}
		f=\alpha\prod_{i=1}^{n-1}(X-T_i).
	\end{equation}
	Nous trouvons \( \alpha\) en écrivant \( f(0)\). D'une part la formule \eqref{EqeVxRwO} nous donne
	\begin{equation}    \label{EqblwWMj}
		f(0)=\alpha(-1)^{n-1}T_1\ldots T_{n-1}.
	\end{equation}
	D'autre part la définition donne
	\begin{subequations}
		\begin{align}
			f(0) & =\det\begin{pmatrix}
				            1         & \cdots & 1             & 1      \\
				            T_1       &        & T_{n-1}       & 0      \\
				            \vdots    &        & \vdots        & \vdots \\
				            T_1^{n-1} & \cdots & T_{n-1}^{n-1} & 0
			            \end{pmatrix}                   \\
			     & =(-1)^{n-1}\det\begin{pmatrix}
				                      T_1       & \ldots & T_{n-1}       \\
				                      \vdots    & \ddots & \vdots        \\
				                      T_1^{n-1} & \ldots & T_{n-1}^{n-1}
			                      \end{pmatrix}                  \\
			     & =(-1)^{n-1}T_1\ldots T_{n-1}\det\begin{pmatrix}
				                                       1         & \cdots & 1             \\
				                                       \vdots    & \ddots & \vdots        \\
				                                       T_1^{n-1} & \cdots & T_{n-1}^{n-1}
			                                       \end{pmatrix} \\
			     & =(-1)^{n-1}T_1\ldots T_{n-1}V(T_1,\ldots, T_{n-1})
		\end{align}
	\end{subequations}
	En égalisant avec \eqref{EqblwWMj}, nous trouvons \( \alpha=V(T_1,\ldots, T_{n-1})\), et donc
	\begin{equation}
		f=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(X-T_j)
	\end{equation}
	Enfin, une récurrence montre que
	\begin{subequations}
		\begin{align}
			V(T_1,\ldots, T_n) & =f(T_n)                                           \\
			                   & =V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(T_n-T_j) \\
			                   & =\prod_{k\leq n}\prod_{j\leq k-1}(T_k-T_j)        \\
			                   & =\prod_{1\leq j<k\leq n}(T_i-T_j).
		\end{align}
	\end{subequations}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooNVXIooXJKplZ}
	Soient des \( a_i\in \eR\) tels que
	\begin{equation}
		f(x)=\sum_{k=0}^na_kx^k
	\end{equation}
	ait \( n+1\) racines distinctes dans \( \eR\). Alors
	\begin{enumerate}
		\item
		      \( f=0\)
		\item
		      \( a_k=0\) pour tout \( k\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous nommons \( (r_i)_{i=1,\ldots,n+1}\) les racines de \( f\). L'équation \( f(x)=0\) s'écrit
	\begin{equation}
		(1,x,\ldots,x^n)\cdot(a_0,\ldots,a_n)=0.
	\end{equation}
	Le système \( f(r_i)=0\) (vu comme système d'équation pour les \( a_i\)) pour tout \( i\) s'écrit
	\begin{equation}
		\begin{pmatrix}
			1      & r_1    & \ldots & r_1^n  \\
			1      & r_2    & \ldots & r_2^n  \\
			\vdots & \vdots &        & \vdots \\
			1      & r_n    & \ldots & r_n^n
		\end{pmatrix}
		\begin{pmatrix}
			a_0    \\
			\vdots \\
			a_n
		\end{pmatrix}=0.
	\end{equation}
	La proposition \ref{PropnuUvtj} nous dit que le déterminant de ce système est non nul. Donc il existe une solution unique. Cette unique solution est manifestement \( (a_0,\ldots,a_n)=(0,\ldots,0)\).
\end{proof}


%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de polynômes}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecEspacePolynomes}

Attention : les polynômes en-soi, font l'objet de la définition~\ref{DEFooFYZRooMikwEL}.

Pour chaque \( k>0\) donné, nous définissons
\begin{equation}
	\mathcal{P}_\eR^k=\{p:\eR\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k, \, a_i\in\eR,\,\forall i=0,\ldots,k\}.
\end{equation}
Il est facile de se convaincre que la somme de deux polynômes de degré inférieur ou égal à \( k\) est encore un polynôme de degré inférieur ou égal à \( k\). En outre il est clair que la multiplication par un scalaire ne peut pas augmenter le degré d'un polynôme. L'ensemble \( \mathcal{P}_\eR^k\) est donc un espace vectoriel muni des opérations héritées de \( \mathcal{P}_{\eR}\).

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooWOMFooLpTKgG}
	Les polynômes \( x\mapsto x^k\) avec \( k=0,\ldots,n\) forment une base de \( \mP_{\eR}^k\).
\end{proposition}

\begin{proof}
	Si nous notons \( e_i(x)=x^i\), et si \( P\) est un polynôme, alors il existe des \( a_i\) tels que \( P(x)=\sum_{i=0}^na_ix^i\). Dans ce cas, \( P=\sum_ia_ie_i\). Cela prouver que \( \{ e_i \}_{i=0,\ldots,n}\) est générateur.

	Pour montrer que c'est libre, nous supposons que \( \sum_{i=0}^na_ie_i=0\). En particulier le polynôme \( P(x)=\sum_ia_ix^i\) a plein de racines distinctes et la proposition \ref{PROPooNVXIooXJKplZ} implique que \( a_i=0\) pour tout \( i\).
\end{proof}


Nous allons maintenant étudier trois applications linéaires de \( \mathcal{P}_\eR^k\) vers d'autres espaces vectoriels.
\begin{description}
	\item[L'isomorphisme canonique  \( \phi:\mathcal{P}_\eR^k \to\eR^{k+1}\)] Nous définissons \( \phi\) par les relations suivantes
	      \[
		      \phi(x^j)=e_{j+1}, \qquad \forall j\in\{0,\dots, k\}.
	      \]
	      Cela veut dire que pour tout \( p\) dans \( \mathcal{P}_\eR^k\), avec \( p(x)=a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k\), l'image de \( p\) par \( \phi\) est
	      \[
		      \phi(p)=\phi\left(\sum_{j=0}^k a_j x^j\right)=\sum_{j=0}^k a_j e_{j+1}.
	      \]
	      \begin{example} Soit \( k=5\) on a
		      \begin{equation}
			      \phi(-8-7x-4x^2+4x^3+2x^5)=
			      \begin{pmatrix}
				      -8 \\
				      -7 \\
				      -4 \\
				      4  \\
				      0  \\
				      2
			      \end{pmatrix}.
		      \end{equation}
	      \end{example}

	      Cette application est clairement bijective et respecte les opérations d'espace vectoriel, donc c'est un isomorphisme d'espaces vectoriels. L'existence d'un isomorphisme entre \( \mathcal{P}_\eR^k\)  et \( \eR^{k+1}\) est un cas particulier du théorème qui dit que  pour chaque \( m\) dans \( \eN_0\) fixée, tous les espaces vectoriels sur \( \eR\) de dimension \( m\) sont isomorphes à \( \eR^m\). Vous connaissez peut être déjà ce théorème depuis votre cours d'algèbre linéaire.
	\item[La dérivation \( d: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k-1}\)] L'application de dérivation \( d\) fait exactement ce qu'on attend d'elle
	      \[
		      d(x^0)=d(1)=0, \qquad d(x^j)=j x^{j-1}, \quad \forall j\in\{1,\dots, k\}.
	      \]
	      Cette application n'est pas injective, parce que l'image de \( p\) ne dépend pas de la valeur de \( a_0\), donc si deux polynômes sont les mêmes à une constante près ils auront la même image par \( d\).

	      \begin{example} Soit \( k=3\) on a
		      \begin{equation}
			      d(-8-12x+4x^3)= -12 (1) + 4 (3x^2) = -12+12 x^2.
		      \end{equation}

		      Noter que \( d(-30-12x+4x^3)=d(-8-12x+4x^3)\). Cela confirme, comme mentionné plus haut, que la dérivée n'est pas injective.
	      \end{example}
	\item[L'intégration \( I: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k+1}\)] Nous pouvons définir une application qui est «à une constante près » l'application réciproque de la dérivation. Cette application est définie sur les éléments de base par
	      \begin{equation}
		      I(x^j)= \frac{x^{j+1}}{j+1}.
	      \end{equation}
	      Bien entendu, la raison d'être et la motivation de cette définition apparaîtront lorsque nous développerons une théorie générale de l'intégration.

	      \begin{example}
		      Soit \( k=4\) on a
		      \begin{equation}
			      I(6+2x+x^2+x^4)= 6x+x^2+\frac{x^3}{3}+\frac{x^5}{5}.
		      \end{equation}
	      \end{example}

	      Remarque: étant donné que dans la définition de \( I\) nous avons décidé d'intégrer entre zéro et \( x\), tous les polynômes dans \( \mathcal{P}_\eR^{k+1}\) qui sont l'image par \( I\) d'un polynôme de \( \mathcal{P}_\eR^{k}\) ont \( a_0=0\). Cela veut dire que nous pouvons générer toute l'image de \( I\) en utilisant un sous-ensemble de la base canonique de \( \mathcal{P}_\eR^{k+1}\), en particulier \( \mathcal{B}_1=\{x\mapsto x^j \,|\, j=1, \ldots, k\}\subset \mathcal{B}\) nous suffira. Cela n'est guère surprenant, parce que l'image par une application linéaire d'un espace vectoriel de dimension finie ne peut pas être un espace de dimension supérieure.
\end{description}

Les applications de dérivation et intégration correspondent évidemment à des applications linéaires de \( \mathcal{P}_\eR\) dans lui-même.

L'espace de tous les polynômes étant de dimension infinie, il peut servir de contre-exemple assez simple. Dans la sous-section~\ref{SubSecPOlynomesCE}, nous verrons que toutes les normes ne sont pas équivalentes sur l'espace des polynômes.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Projection et orthogonalité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}     \label{PropProjScal}
	Si nous écrivons \( \pr_Y\) l'opération de projection sur la droite qui sous-tend \( Y\), alors nous avons
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	Les vecteurs \( X\) et \( Y\) sont des flèches dans l'espace. Nous pouvons choisir un système d'axe orthogonal tel que les coordonnées de \( X\) et \( Y\) soient
	\begin{equation}
		\begin{aligned}[]
			X & =\begin{pmatrix}
				     x \\
				     y \\
				     0
			     \end{pmatrix},
			  & Y                & =\begin{pmatrix}
				                        l \\
				                        0 \\
				                        0
			                        \end{pmatrix}
		\end{aligned}
	\end{equation}
	où \( l\) est la longueur du vecteur \( Y\). Pour ce faire, il suffit de mettre le premier axe le long de \( Y\), le second dans le plan qui contient \( X\) et \( Y\), et enfin le troisième axe dans le plan perpendiculaire aux deux premiers.

	Un simple calcul montre que \( X\cdot Y=xl+y\cdot 0+0\cdot 0=xl\). Par ailleurs, nous avons \( \| \pr_YX \|=x\). Par conséquent,
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ l }=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proof}

\begin{corollary}
	Si la norme de \( Y\) est \( 1\), alors le nombre \( X\cdot Y\) est la longueur de la projection de \( X\) sur \( Y\).
\end{corollary}

\begin{proof}
	Poser \( \| Y \|=1\) dans la proposition~\ref{PropProjScal}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropEJBZooTNFPRj}
	Si \( A\) est la matrice d'une application linéaire, alors le rang de cette application linéaire est égal au rang de \( A \), c'est-à-dire à la taille de la plus grande matrice carrée de déterminant non nul contenue dans \( A\).
\end{proposition}

\begin{definition}  \label{DefJPGSHpn}
	Soit \( E\) un espace vectoriel sur \( \eK\).
	\begin{enumerate}
		\item
		      Une \defe{forme linéaire}{forme linéaire} sur \( E \) est une application linéaire de \( E \) sur son corps de base \( \eK\).
		\item
		      Le \defe{dual algébrique}{dual algébrique} de \( E\), noté \( E^*\), est l'ensemble des formes linéaires sur \( E\). Autrement dit : \( E^* = \aL(E,\eK)\).
	\end{enumerate}
\end{definition}
Nous verrons les formes multilinéaires en la définition \ref{DefFRHooKnPCT}.

Nous verrons plus tard qu'en dimension infinie, les applications linéaires ne sont pas toujours continues. Nous définirons donc aussi un concept de dual topologique. Voir la proposition~\ref{PROPooQZYVooYJVlBd}, la remarque~\ref{RemOAXNooSMTDuN} et la définition~\ref{DEFooKSDFooGIBtrG}.

\begin{lemmaDef}      \label{DEFooTMSEooZFtsqa}
	Si \( E\) est un espace vectoriel et si \( \{ b_i \}\) est une base de \( E\), alors nous définissons
	\begin{equation}
		\begin{aligned}
			b_i^*\colon E & \to \eK              \\
			b_j           & \mapsto \delta_{ij},
		\end{aligned}
	\end{equation}
	et sa prolongation par linéarité. Ces éléments du dual \( E^*\) forment une base appelée \defe{base duale}{base!duale}.

	En particulier nous avons
	\begin{equation}
		\dim(E)=\dim(E^*).
	\end{equation}
\end{lemmaDef}

\begin{proof}
	Notons pour commencer que \( b_i^*\big( \sum_ix_ib_i \big)=x_i\).
	\begin{subproof}
		\spitem[Générateur]
		%-----------------------------------------------------------
		Soit \( \varphi\in E^*\). Nous avons
		\begin{equation}
			\varphi(x)=\sum_ix_i\varphi(b_i)=\sum_i\varphi(b_i)b^*_i(x),
		\end{equation}
		ce qui signifie que \( \varphi=\sum_i\varphi(b_i)b^*_i\).

		\spitem[Libre]
		%-----------------------------------------------------------
		Si \( \sum_ia_ib^*_i=0\), alors en particulier en appliquant à \( b_k\) nous avons
		\begin{equation}
			0=\big( \sum_ia_ib^*_i \big)(b_k)=\sum_ia_i\delta_{ik}=a_k.
		\end{equation}
		Donc tous les coefficients sont nuls et \( \{ b_i^* \}_{i=1,\ldots,n}\) est libre.
	\end{subproof}
\end{proof}

Notons que si \( v\in E\) est un vecteur, ça n'a aucun sens à priori de parler de \( v^*\). Il s'agit bien de définir \emph{toute} la base \( \{ e_i^* \}\) à partir de toute la base \( \{ e_i \}\).

\begin{lemma}[Changement de base duale\cite{MonCerveau}]		\label{LEMooSVRIooFbxfue}
	Soient un espace vectoriel \( V\) de dimension finie, une base \( \{ e_i \}_{i=1,\ldots,n}\) et sa base duale \( \{ \alpha_i \}_{i=1,\ldots,n}\). Soit une bijection linéaire \(A \colon V\to V  \). Nous considérons \( \{ \beta_i \}_{i=1,\ldots,n}\) la base duale de la base \( \{ Ae_i \}\).

	Alors
	\begin{equation}
		\beta_i=\sum_jA^{-1}_{ij}\alpha_j.
	\end{equation}
\end{lemma}

\begin{proof}

	Par définition d'une base duale, il faut \( \beta_i(Ae_k)=\delta_{ik}\). Vu que \( \{ \alpha_i \}\) est une base, nous pouvons décomposer \( \beta_i\) de la façon suivante : \( \beta_i=\sum_jB_{ij}\alpha_j\). Il nous reste à prouver que \( B=A^{-1}\).

	Pour cela nous calculons un peu :
	\begin{equation}
		\delta_{ik}=\beta_i(Ae_k)=\sum_jB_{ij}\alpha_j(Ae_k)=\sum_{lj}B_{ij}A_{lk}\underbrace{\alpha_j(e_l)}_{=\delta_{jl}}=\sum_lB_{il}A_{lk}=(BA)_{ik}.
	\end{equation}
	Nous avons donc \( \delta_{ik}=(BA)_{ik}\), c'est-à-dire \( BA=\mtu\) ou encore \( B=A^{-1}\).
\end{proof}

\begin{lemma}[\cite{BIBooQVDOooFONdrf}]           \label{LEMooQLWNooYUpGdo}
	Soit un espace vectoriel \( E\) de dimension finie.
	\begin{enumerate}
		\item       \label{ITEMooHHTLooNCjgfn}
		      Si \( \alpha\) est une forme linéaire non nulle sur \( E\), alors il existe \( x\in E\) tel que \( \alpha(x)=1\).
		\item       \label{ITEMooBYAAooUWBKDk}
		      Si \( x\neq 0\) dans \( E\), alors il existe une forme linéaire \( \alpha\) sur \( E\) telle que \( \alpha(x)=1\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooHHTLooNCjgfn}]
		Puisque \( \alpha\) est non nulle, nous pouvons considérer \( v\in E\) tel que \( \alpha(v)\neq 0\). Alors en posant \( x=\alpha(v)^{-1}  v\), nous avons le résultat.
		\spitem[Pour \ref{ITEMooBYAAooUWBKDk}]
		Soit un vecteur non nul que nous écrivons sous la forme \( x=\sum_{i=1}^nx_ie_i\) (pour une certaine base \( \{ e_i \}_{i=1,\ldots, n}\) de \( E\)). Supposons que \( x_k\neq 0\). Alors la forme
		\begin{equation}
			\begin{aligned}
				\alpha\colon  E & \to \eK         \\
				y               & \mapsto y_k/x_k
			\end{aligned}
		\end{equation}
		fait l'affaire.
	\end{subproof}
\end{proof}

\begin{lemma}       \label{LEMooKTREooBrnWVz}
	Soit un espace vectoriel de dimension finie \( E\) sur le corps \( \eK\). Si \( \{ \alpha_1,\ldots,\alpha_n \}\) est une base de \( E^*\), alors l'application
	\begin{equation}
		\begin{aligned}
			\Phi\colon E & \to \eK^n                                           \\
			x            & \mapsto \big( \alpha_1(x),\ldots, \alpha_n(x) \big)
		\end{aligned}
	\end{equation}
	est un isomorphisme d'espaces vectoriels.
\end{lemma}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\( \Phi\) est injective]
		Soit \( z\in \ker(\Phi)\). Nous avons \( \alpha_i(z)=0\) pour tout \( i\). Si \( z\neq 0\), alors le lemme \ref{LEMooQLWNooYUpGdo} dit qu'il existe \( \beta\in E^*\) tel que \( \beta(z)\neq 0\).

		Décomposons un tel \( \beta\) dans la base de \( \{\alpha_i\}\) :
		\begin{equation}
			\beta=\sum_{i=1}^n\beta_i\alpha_i.
		\end{equation}
		Alors nous avons
		\begin{equation}
			0\neq \beta(z)=\sum_{i=1}^n\beta_i\underbrace{\alpha_i(z)}_{=0}=0.
		\end{equation}
		Contradiction. Donc \( \ker(\Phi)=\{ 0 \}\) et \( \Phi\) est injective.

		\spitem[\( \Phi\) est surjective]
		Les espaces vectoriels \( E\), \( E^*\) et \( \eK^n\) ont tout trois, une dimension \( n\). Donc \( \Phi\) est une application linéaire injective entre deux espaces de même dimension. Elle est donc surjective par le corolaire \ref{CORooCCXHooALmxKk}.
	\end{subproof}
\end{proof}


\begin{propositionDef}[\cite{BIBooQVDOooFONdrf}]            \label{PROPooDBPGooPagbEB}
	Soit un espace vectoriel \( E\) de dimension finie sur le corps \( \eK\). Toute base du dual \( E^*\) est duale d'une unique base de \( E\). Cette base est dite \defe{préduale}{base préduale}.
\end{propositionDef}

\begin{proof}
	Nous considérons une base \( \mF=\{ \alpha_1,\ldots,\alpha_n \}\) de \( E^*\). Nous devons prouver qu'il existe une unique base de \( E\) dont la base duale est \( \mF\).

	\begin{subproof}
		\spitem[Existence]
		Le lemme \ref{LEMooKTREooBrnWVz} nous indique que l'application
		\begin{equation}
			\begin{aligned}
				\Phi\colon  E & \to \eK^n                                           \\
				x             & \mapsto \big( \alpha_1(x),\ldots, \alpha_n(x) \big)
			\end{aligned}
		\end{equation}
		est un isomorphisme d'espaces vectoriels.

		Soit la base canonique de \( \eK^n\) : \( (\epsilon_1,\ldots, \epsilon_n)\). Puisque \( \Phi\) est un isomorphisme, \( \big( \Phi^{-1}(\epsilon_i) \big)_{i=1,\ldots, n}\) est une base de \( E\). Nous allons montrer qu'elle est préduale de \( (\alpha_i)\). Nous posons \( e_i=\Phi^{-1}(\epsilon_i)\) et nous calculons :
		\begin{subequations}
			\begin{align}
				\alpha_i(e_j) & =\alpha_i\big( \Phi^{-1}(\epsilon_j) \big)                                  \\
				              & =\Phi\big( \Phi^{-1}(\epsilon_j) \big)_i        \label{SUBEQooACVAooVNwzMq} \\
				              & =(\epsilon_i)_j                                                             \\
				              & =\delta_{i,j}                                   \label{SUBEQooOYNHooSVOOyz}
			\end{align}
		\end{subequations}
		Justifications :
		\begin{itemize}
			\item Pour \ref{SUBEQooACVAooVNwzMq}, nous remarquons que \( \alpha_i(x)=\Phi(x)_i\).
			\item Pour \ref{SUBEQooOYNHooSVOOyz}, nous utilisons le fait que les \( \epsilon_j\) forment la base canonique de \( \eK^n\).
		\end{itemize}

		\spitem[Unicité]
		Soit une base préduale \( (e_i)\) de \( (\alpha_i)\). Nous avons, par définition, que \( \alpha_i(e_j)=\delta_{i,j}\). Donc
		\begin{equation}
			\big( \alpha_1(e_j),\ldots, \alpha_n(e_j) \big)=\epsilon_j.
		\end{equation}
		Nous appliquons \( \Phi^{-1}\) à cette dernière équation pour obtenir \( e_j=\Phi^{-1}(\epsilon_j)\). Donc les \( e_j\) sont déterminés de façon unique à partir des \( \alpha_i\).
	\end{subproof}
\end{proof}

Si \( u\) et \( v\) sont des vecteurs de \( \eK^n\), alors nous notons \( uv^*\) l'application linéaire
\begin{equation}
	\begin{aligned}
		uv^*\colon \eK^n & \to \eK^n        \\
		x                & \mapsto uv^*(x).
	\end{aligned}
\end{equation}

\begin{lemma}[Lemme des déterminants\cite{BIBooEWYNooTcCLSQ}]	\label{LEMooJVDHooONXNAh}
	Soit une application linéaire \( A\in\End(\eK^n)\) ainsi que \( u,v\in \eK^n\). Nous avons
	\begin{enumerate}
		\item
		      \( \det(A+uv^*)=(1+v\cdot Au)\det(A)\)
		\item		\label{ITEMooGJOZooVLfZcX}
		      \( \det(\mtu+af)=1+f(a)\).
	\end{enumerate}
	%TODOooBWATooAnEenD. Prouver ça.
\end{lemma}


%-------------------------------------------------------
\subsection{Changement de base duale}
%----------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooDOEYooDpUbnc}
	Soient deux bases \( \{ e_i \}\) et \( \{ e'j \}\) de l'espace vectoriel \( V\). Nous notons \( \{ \alpha_i \}\) et \( \{ \alpha'_i \}\) les bases duales. Si \( e'_j=\sum_iA_{ij}e_i\), alors
	\begin{equation}
		\alpha'_j=\sum_kA_{jk}^{-1}\alpha_k.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous posons \( \alpha'_j=\sum_kB_{kj}\alpha_k\) et nous appliquons ça à \( e'_i\). À gauche nous avons \( \alpha'_j(e'_i)=\delta_{ij}\), et à droite,
	\begin{subequations}
		\begin{align}
			\sum_kB_{kj}\alpha_k(e'_i) & = \sum_kB_{kj}\alpha_k\big( \sum_lA_{li}e_l \big) \\
			                           & = \sum_{kl}B_{kj}A_{li}\delta_{kl}                \\
			                           & = \sum_lB_{lj}A_{li}                              \\
			                           & = (B^tA)_{ji}.
		\end{align}
	\end{subequations}
	Nous avons donc montré que \( B^tA=\mtu\), ou encore que \( B^t=A^{-1}\), ce qu'il fallait.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooEQSMooHVzbfz}
	Soit \( E\), un espace vectoriel, et \( F\) un sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
	\begin{equation}    \label{Eqiiyple}
		F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
	\end{equation}
\end{definition}

Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
	F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) muni du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition~\ref{DEFooEQSMooHVzbfz}, au contraire, est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
	B^o=\{ x\in E\tq \omega(x)=0, \forall\omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual, et on prend une notation précise pour dire qu'on remonte au pré-dual, et non qu'on va au dual du dual.

\begin{proposition} \label{PropXrTDIi}
	Soient un espace vectoriel \( E\) et \( F\), un sous-espace vectoriel de \( E\). Nous avons
	\begin{equation}
		\dim F+\dim F^{\perp}=\dim E.
	\end{equation}
\end{proposition}

\begin{proof}
	Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème~\ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\).

	D'abord, ce sont des éléments de \( F^{\perp}\), parce que si \( i\leq p\) et si \( k\geq 1\), nous avons \( e^*_{p+k}(e_i)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

	Ensuite, en tant que partie d'une base de \( F^*\), c'est une partie libre. Il reste à montrer que c'est générateur.


	Enfin \( F^{\perp}\subset\Span\{ e_{k}^*, k \in \{p+1, \dots, n\}\}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{k=p+1}^n\omega_ke^*_k\).
\end{proof}


La proposition \ref{PROPooNITTooCYcrrT} donnera une version plus terre à terre de la proposition \ref{PropXrTDIi} en disant que si nous avons un produit scalaire, alors \( E=F\oplus F^{\perp}\) où \( F^{\perp}\) est cette fois défini comme l'orthogonal pour le produit scalaire.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Représentation de groupe}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Représentation]      \label{DEFooXVMSooXDIfZV}
	Soit un groupe \( G\). Une \defe{représentation}{représentation} de \( G\) est un couple \( (E,\rho)\) où \( E\) est un espace vectoriel et \( \rho\) est une application \( \rho\colon G\to \GL(E)\) vérifiant
	\begin{equation}
		\rho(g)\circ\rho(h)=\rho(gh).
	\end{equation}
	pour tout \( g,h\in G\).
\end{definition}

\begin{definition}
	Une représentation\footnote{Définition \ref{DEFooXVMSooXDIfZV}.} \( \rho\colon G\to \GL(E)\) est \defe{fidèle}{représentation!fidèle} si elle est injective

	La dimension de \( E\) est le \defe{degré}{degré!d'une représentation} de la représentation \( (E,\rho)\).
\end{definition}

Le fait que la représentation \( \rho\colon G \to \GL(E)\) soit fidèle ne dit pas que chacun des \( \rho(g)\) est injectif.

\begin{proposition}     \label{PROPooHNQOooSzeEFG}
	Soit un corps \( \eK\). Si \( G\) est un groupe dans \( \eM(n,\eK)\) (c'est-à-dire un groupe de matrices \( n\times n\) à coefficients dans \( \eK\)), alors l'application
	\begin{equation}
		\begin{aligned}
			\rho\colon G & \to \GL(\eK^n) \\
			A            & \mapsto f_A
		\end{aligned}
	\end{equation}
	où \( f_A\) est l'application linéaire associée à \( A\), est une représentation de \( G\).
\end{proposition}

\begin{proof}
	La représentation dont nous parons n'est autre que l'application \( \psi\) de la définition \ref{DEFooJVOAooUgGKme}, dont nous connaissons beaucoup de propriétés. La proposition \ref{PROPooFMBFooEVCLKA} dit, entre autres, que \( \psi(AB)=\psi(A)\psi(B)\), c'est-à-dire que \( \rho(AB)=\rho(A)\rho(B)\), et donc que \( \rho\) est une représentation.
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Somme directe d'espaces vectoriels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Si \( V\) et \( W\) sont des espaces vectoriels, ce que nous notons \( V\oplus W\) n'est rien d'autre que l'espace vectoriel de l'ensemble \( V\times W\).

\begin{propositionDef}[\cite{ooXISFooTypogf,BIBoo4034093}]                      \label{DEFooJKAWooKkkkwm}
	Si \( V\) et \( W\) sont des espaces vectoriels sur le même corps \( \eK\), alors les définitions
	\begin{enumerate}
		\item
		      \( (v_1,w_1)+(v_2,w_2)=(v_1+v_2,w_1+w_2)\)
		\item
		      \( \lambda(v,w)=(\lambda v,\lambda w)\)
	\end{enumerate}
	donnent une structure d'espace vectoriel sur \( V\times W\).

	Cet espace sera noté \( V\oplus W\) et est appelé \defe{somme directe}{somme directe} de \( V\) et \( W\).
\end{propositionDef}

\begin{definition}[Sous-espaces en somme directe\cite{BIBooJADFooBZwsEa}]       \label{DEFooIJDNooRUDUYF}
	Soient un espace vectoriel \( E\) sur \(\eK\) ainsi que des sous-espaces vectoriels \( \{ F_i \}_{i\in I}\) (\( I\) est un ensemble fini ou infini). Nous disons que les \( F_i\) sont \defe{en somme directe}{somme directe} si pour tout élément \( u\in\sum_{i\in I}F_i\), il existe un unique ensemble \( \{ u_i \}_{i\in I}\) de vecteurs tel que
	\begin{enumerate}
		\item
		      \( u=\sum_{i\in I}u_i\)
		\item
		      \( u_i\in F_i\) pour tout \( i\),
		\item
		      \( \{ j\in I\tq u_j\neq 0 \}\) est fini.
	\end{enumerate}

	Si l'espace vectoriel \( E\) est un espace vectoriel topologique, nous avons la définition \ref{PropKZDqTR} qui donne des conditions de compatibilité entre les topologies.
\end{definition}


\begin{lemma}[Décomposition suivant une somme directe]   \label{LEMooHWRVooLedAmF}
	Soit un espace vectoriel \( E\) et deux sous-espaces \( F_1\), \( F_2\) en somme directe :  \( E=F_1\oplus F_2\). Alors l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon F_1\times F_2 & \to E       \\
			(x,y)                    & \mapsto x+y
		\end{aligned}
	\end{equation}
	est une bijection.
\end{lemma}

\begin{proof}
	L'application est injective parce que si \( \psi(x_1,x_2)=\psi(y_1,y_2)\), alors \( x_1+x_2=y_1+y_2\). Nommons \(u\) ce vecteur. Par unicité de l'ensemble \(  \{ u_1,u_2 \} \) tel que \( u=u_1+u_2\), nous avons automatiquement \( \{ x_1,x_2 \}=\{ y_1,y_2 \}\) et donc \( x_1=y_1\) et \( x_2=y_2\).

	En ce qui concerne la surjectivité, si \( u\in E\), il existe un ensemble \( \{ u_1,u_2 \}\) avec \( u_i\in F_i\) tel que \( u=u_1+u_2=\psi(u_1,u_2)\).
\end{proof}


\begin{lemma}[\cite{MonCerveau, BIBooJADFooBZwsEa}]         \label{LEMooDQMQooInVVDY}
	Soient un espace vectoriel \( E\) ainsi que des sous-espaces vectoriels \( F_i\). Nous avons équivalence entre les assertions suivantes.
	\begin{enumerate}
		\item		\label{ITEMooHJXIooQKAfBe}
		      Les \( F_i\) sont en somme directe\footnote{Définition \ref{DEFooIJDNooRUDUYF}.}.
		\item		\label{ITEMooJFOFooGDlwUz}
		      Si \( \sum_{i\in I}u_i=0\) avec \( u_i\in F_i\) et si \( \{ j\in I\tq u_j\neq 0 \}\) est fini, alors tous les \( u_i\) sont nuls.
		\item		\label{ITEMooNFWVooNdhhid}
		      Chaque espace \( F_k\) est en somme directe avec la somme des précédents, c'est-à-dire que pour tout \( k\),
		      \begin{equation}
			      \left( \sum_{i=1}^{k-1}F_i \right)\cap F_k=\{ 0 \}.
		      \end{equation}
		\item   \label{ITEMooPLXGooCOQgen}
		      Pour tout \( k\),
		      \begin{equation}
			      F_k\cap\left( \sum_{i\neq k}F_i \right)=\{ 0 \}.
		      \end{equation}
		      Notez qu'ici \( \sum_{i\neq k}F_i\) signifie l'ensemble des \( x\in V\) pouvant être écrit comme une combinaison linéaire finie d'éléments des \( F_i\) avec \( i\neq k\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[\ref{ITEMooHJXIooQKAfBe} implique \ref{ITEMooJFOFooGDlwUz}]
		%-----------------------------------------------------------
		Par définition de la somme directe, le vecteur \( 0\) de \( E\) peut être écrit de façon unique sous la forme \( \sum_iu_i\), et cette façon est avec \( u_i=0\) pour tout \( i\).

		\spitem[\ref{ITEMooJFOFooGDlwUz} implique \ref{ITEMooNFWVooNdhhid}]
		%-----------------------------------------------------------
		Supposons que \( \sum_{i=1}^{k-1}u_i=u_k\) avec \( u_j\in F_j\) pour tout \( j\). Alors en posant \( v_i=u_i\) (i=1,\ldots,k-1) et \( v_{k}=-u_k\) nous avons \( \sum_{i=1}^kv_i=0\), ce qui, en vertu de \ref{ITEMooJFOFooGDlwUz} donne \( v_i=0\) pour tout \( i\).

		\spitem[\ref{ITEMooNFWVooNdhhid} implique \ref{ITEMooPLXGooCOQgen}]
		%-----------------------------------------------------------
		Supposons avoir
		\begin{equation}
			u_k=\sum_{i\in J}u_i
		\end{equation}
		où \( J\) est fini dans \( \eN\setminus\{ k \}\). Donc \( J=\{ 1,\ldots,n \}\setminus\{ k \}\) pour un certain \( n\). En réarrangeant la somme nous avons
		\begin{equation}
			u_n=\sum_{i=1}^{n-1}u_i,
		\end{equation}
		et donc tous les \( u_i\) sont nuls par \ref{ITEMooNFWVooNdhhid}.

		\spitem[\ref{ITEMooPLXGooCOQgen} implique \ref{ITEMooHJXIooQKAfBe}]
		%-----------------------------------------------------------
		Soit un élément \( x\in \sum_{i\in I}F_i\). Nous l'écrivons de deux façons
		\begin{equation}
			x=\sum_{i\in J_1}u_i=\sum_{i\in J_2}v_i
		\end{equation}
		où \( J_1\) et \( J_2\) sont finis. Nous posons \( J=J_1\cup J_2\) et \( u_j=0\) pour \( j\in J\setminus J_1\) et \( v_j=0\) pour \( j\in J\setminus J_2\). Ainsi \( \sum_{i\in J}u_i=\sum_{i\in J}v_j\) et donc
		\begin{equation}
			\sum_{i\in J}(i_i-v_i)=0.
		\end{equation}
		Soit \( k\in J\). Nous avons
		\begin{equation}
			u_k-v_k=\sum_{i\in J\setminus \{ k \}}(u_i-v_i).
		\end{equation}
		En vertu de \ref{ITEMooPLXGooCOQgen}, tout est nul et en particulier \( v_i=u_i\) pour tout \( i\).
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]               \label{PROPooCASNooEqisqa}
	Soient \( E\) un espace vectoriel de dimension finie, et deux sous-espaces \( F_1\) et \( F_2\) satisfaisant
	\begin{enumerate}
		\item
		      \( F_1\cap F_2=\{ 0 \}\),
		\item
		      \( \dim(F_1)+\dim(F_2)\geq \dim(E)\).
	\end{enumerate}
	Alors \( E=F_1\oplus F_2\).
\end{proposition}

\begin{proof}
	Soient une base \( \{ e_i \}_{i\in I}\) de \( F_1\) et \( \{ f_{\alpha} \}\) de \( F_2\). Nous commençons par prouver que la partie \( B=\{ e_i \}\cup \{ f_{\alpha} \}\) est libre.

	Supposons en effet, avoir des coefficients \( a_i\) et \( b_{\alpha}\) tels que
	\begin{equation}
		\sum_i a_ie_i+\sum_{\alpha}b_{\alpha}f_{\alpha}=0.
	\end{equation}
	Cela implique que \( \sum_ia_ie_i=-\sum_{\alpha}b_{\alpha}f_{\alpha}\). Or \( \sum_ia_ie_i\in F_1\) et \( -\sum_{\alpha}b_{\alpha}f_{\alpha}\in F_2\). Donc les éléments \( \sum_i a_ie_i\) et \( \sum_{\alpha}b_{\alpha}f_{\alpha}\) sont dans \( F_1\cap F_2=\{ 0 \}\). Nous avons alors les égalités
	\begin{equation}
		\sum_i a_ie_i=0
	\end{equation}
	et
	\begin{equation}
		\sum_{\alpha}b_{\alpha}f_{\alpha}=0.
	\end{equation}
	La première implique \( a_i=0\) pour tout \( i\) et la seconde implique \( b_{\alpha}=0\) pour tout \( \alpha\).

	Donc \( B\) est une partie libre de \( E\) contenant \( \dim(F_1)+\dim(F_2)\geq \dim(E)\) éléments. La proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooZNLDooBISkJyBS} nous indique alors qu'en réalité \( \dim(F_1)+\dim(F_2)=\dim(E)\). Comme \( B\) est une partie libre contenant \( \dim(E)\) éléments, c'est une base par la proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooSGGCooOUsuBs}.
\end{proof}


\begin{definition}[Projection parallèle]		\label{DEFooYQZHooCDuhDU}
	Si nous avons une somme directe d'espaces vectoriels\footnote{Définition \ref{DEFooIJDNooRUDUYF}.} \( E=\bigoplus_{k=1}^nE_k\) alors le \defe{projecteur sur \( E_i \) parallèlement}{projecteur parallèlement} aux autres \( E_i\) est l'application
	\begin{equation}
		\begin{aligned}
			p_i\colon E    & \to E       \\
			x_1+\ldots+x_n & \mapsto x_i
		\end{aligned}
	\end{equation}
	où \( (x_1,\ldots,x_n)\) est la décomposition\footnote{Lemme \ref{DEFooIJDNooRUDUYF}.} de \( x\) selon les espaces \( E_i\).

	Cette application est un projecteur au sens de la définition \ref{DEFooZACWooWydFDp}.
	%TODOooUBXDooFPeDeO. Prouver ça.
\end{definition}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooCFDYooHTroSi}
	Soit une somme directe d'espaces vectoriels\footnote{Définition \ref{DEFooIJDNooRUDUYF}.} \( E=\bigoplus_{i=1}^nE_i\). Si une application linéaire \(p \colon E\to E  \) vérifie
	\begin{enumerate}
		\item		\label{ITEMooCDFXooKZJvON}
		      Si \( i\neq k\), \( p(E_i)=\{ 0 \}\),
		\item		\label{ITEMooJKLVooLbSljW}
		      \( p(x)=x\) pour tout \( x\in E_k\),
	\end{enumerate}
	alors \( p\) est la projection sur \( E_k\) parallèle\footnote{Définition \ref{DEFooYQZHooCDuhDU}.} aux autres \( E_i\).
\end{lemma}

\begin{proof}
	Soit \( x\in E\) et décomposons-le selon le lemme \ref{DEFooIJDNooRUDUYF} : \( x=x_1+\ldots+x_n\). Alors
	\begin{subequations}
		\begin{align}
			p(x) & =p(x_1)+\ldots+p(x_n) & \text{\( p\) est linéaire}           \\
			     & =p(x_k)               & \text{hyp. \ref{ITEMooCDFXooKZJvON}} \\
			     & =x_k                  & \text{hyp. \ref{ITEMooJKLVooLbSljW}} \\
		\end{align}
	\end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Structure réelle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{enwiki:990936192,BIBoo4034093,MonCerveau}]         \label{DEFooCIFSooVmcNtE}
	Soit un espace vectoriel \( E\) sur \( \eC\). Il existe une application \( \sigma\colon E\to E\) telle que
	\begin{enumerate}
		\item
		      \( \sigma^2=\id_E\)
		\item
		      Pour tout \( \alpha,\beta\in \eR\), \( f(\alpha x+\beta y)=\bar \alpha \sigma(x)+\bar \beta \sigma(y)\).
	\end{enumerate}
	Une telle application est une \defe{structure réelle}{structure réelle} sur \( E\).
\end{propositionDef}

\begin{proof}
	La proposition \ref{PROPooHDCEooMhDjPi} nous permet de considérer une base \( \{ e_i \}_{i\in I}\) de \( E\). Alors, nous définissons
	\begin{equation}
		\sigma\big( \sum_{i\in I}\lambda_ie_i \big)=\sum_{i\in I}\bar\lambda_ie_i.
	\end{equation}
	Notez que la somme est toujours finie.
\end{proof}

\begin{proposition}     \label{PROPooPZHPooNdarzg}
	Soit un espace vectoriel \( E\) sur \( \eC\) et une structure réelle\footnote{Définition \ref{DEFooCIFSooVmcNtE}.} \( \sigma\) sur \( E\). Nous posons
	\begin{equation}
		E_{\eR}=\{ v\in E\tq \sigma(v)=v \}.
	\end{equation}
	Alors
	\begin{enumerate}
		\item
		      La partie \( E_{\eR}\) est un espace vectoriel réel.
		\item
		      Nous avons la décomposition en somme directe\footnote{Définition \ref{DEFooJKAWooKkkkwm}.}
		      \begin{equation}
			      E=E_{\eR}\oplus iE_{\eR}.
		      \end{equation}
	\end{enumerate}
\end{proposition}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Espace vectoriel réel]
		Si \( v,w\in E_{\eR}\), alors
		\begin{equation}
			\sigma(v+w)=\sigma(v)+\sigma(w)=v+w,
		\end{equation}
		et si \( \lambda\in \eR\),
		\begin{equation}
			\sigma(\lambda x)=\lambda\sigma(x)=\lambda x.
		\end{equation}
		Donc \( E_{\eR}\) est un espace vectoriel réel.
		\spitem[Première somme directe]
		Nous définissons
		\begin{subequations}
			\begin{align}
				E^+ & =\{ v\in E\tq \sigma(v)=v   \}, \\
				E^- & =\{ v\in E\tq \sigma(v)=-v  \}
			\end{align}
		\end{subequations}
		Nous prouvons que
		\begin{equation}
			\begin{aligned}
				\psi\colon E^+\times E^- & \to E       \\
				(a,b)                    & \mapsto a+b
			\end{aligned}
		\end{equation}
		est un isomorphisme d'espaces vectoriels.

		Puisque \( \psi\) est linéaire, il suffit de prouver qu'elle est bijective.
		\begin{subproof}
			\spitem[Surjectif]
			Si \( v\in E\), alors en posant \( v_+=\frac{ 1 }{2}\big( v+\sigma(v) \big)\), \( v_-=\frac{ 1 }{2}\big( v-\sigma(v) \big)\), nous avons
			\begin{equation}
				\begin{aligned}[]
					v & =v_+ + v_-, & v_+ & \in E^+, & v_- & \in E^-,
				\end{aligned}
			\end{equation}
			et donc \( v=\psi(v_+,v_-)\).
			\spitem[Injectif]
			Supposons \( \psi(a,b)=\psi(\alpha,\beta)\). Alors \( a+b=\alpha+\beta\) et donc \( a-\alpha=\beta-b\). Comme \( a-\alpha\in E^+\)  et \( \beta-b\in E^- \), nous savons que \( a-\alpha=\beta-b\in E^+\cap E^-\). Étant donné que \( E^+\cap E^-=\{ 0 \}\), nous avons \( a-\alpha=\beta-b=0\).
		\end{subproof}
		Nous avons donc la somme directe \( E=E^+\oplus E^-\).
		\spitem[Conclusion]
		Par définition, \( E^+=E_{\eR}\). Il nous reste à voir que \( E^-=iE^+\). Nous prouvons les inclusions dans les deux sens.
		\begin{subproof}
			\spitem[\( E^-\subset iE^+\)]
			Soit \( v\in E^-\). Nous avons \( iv\in E^+\); en effet
			\begin{equation}
				\sigma(iv)=\bar i\sigma(v)=-i\sigma(v)=iv.
			\end{equation}
			Donc \( iv\in E^+\) pour \( v\in E^-\).
			\spitem[\( iE^+\subset E^-\)]
			Soit \( v\in E^+\), et voyons que \( iv\in E^-\). En effet,
			\begin{equation}
				\sigma(iv)=-i\sigma(v)=-iv.
			\end{equation}
		\end{subproof}
	\end{subproof}
\end{proof}

\begin{normaltext}
	Lorsque nous avons une structure réelle \( \sigma\) sur un espace vectoriel complexe \( E\), nous écrivons \( E=E_{\eR}\oplus iE_{\eR}\) sans préciser dans la notation «\( E_{\eR}\)» que cet ensemble dépend du choix de \( \sigma\). En particulier si \( F\) est un sous-espace vectoriel de \( E\), nous utiliserons la notation \( F_{\eR}\) relativement à la même involution que celle utilisée pour \( E\).
\end{normaltext}

\begin{lemma}
	Soit un espace vectoriel complexe \( E\) muni d'une structure réelle \( \sigma\). Si \( F\) est un sous-espace de \( E\) alors \( F_{\eR}=E_{\eR}\cap F\).
\end{lemma}

\begin{proof}
	Par définition,
	\begin{equation}
		F_{\eR}=\{ v\in F\tq \sigma(v)=v \}.
	\end{equation}
	\begin{subproof}
		\spitem[\( F_{\eR}\subset F\)]
		C'est dans la définition de \( F_{\eR}\) (sous-ensemble de \( F\)).
		\spitem[\( F_{\eR}\subset E_{\eR}\)]
		Si \( v\in F_\eR\), alors \( \sigma(v)=v\). Mais cette égalité est précisément celle qui permet d'être dans \( E_{\eR}\).
	\end{subproof}
\end{proof}

Vous remarquerez que ce lemme ne fonctionne que parce que nous avons choisi la même structure réelle sur \( F\) que sur \( E\).
