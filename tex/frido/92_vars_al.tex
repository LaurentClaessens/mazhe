% This is part of Agregation : modélisation
% Copyright (c) 2011-2016,2018-2019, 2022-2024
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Fonction de répartition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooYAZVooNdxDCx}
	Si \( X\) est une variable aléatoire réelle, nous définissons sa \defe{fonction de répartition}{fonction!de répartition} par
	\begin{equation}
		\begin{aligned}
			F_X\colon \eR & \to \mathopen[ 0 , 1 \mathclose] \\
			F_X(x)        & =P(X\leq x).
		\end{aligned}
	\end{equation}
\end{definition}

\begin{remark}
	La fonction de répartition est discontinue en \( a\) si \( P(X=a)>0\). En particulier nous ne pouvons pas dire
	\begin{equation}
		P(X\geq a)=1-F_X(a).
	\end{equation}
\end{remark}

\begin{lemma}		\label{LEMooMNXJooBdGXWg}
	La fonction de répartition de la loi exponentielle est
	\begin{equation}
		F(x)=1- e^{-\lambda x},
	\end{equation}
\end{lemma}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Fonction caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooEIVXooNtHLQQ}
	La \defe{fonction caractéristique}{fonction!caractéristique!d'une variable aléatoire} de la variable aléatoire \( X\colon \Omega\to \eR\) est la fonction réelle définie par
	\begin{equation}
		\Phi_X(t)=E( e^{itX}).
	\end{equation}
\end{definition}
Une autre façon d'écrire la définition est
\begin{equation}
	\Phi_X(t)=\int_{\eR} e^{itX}dP_X(x),
\end{equation}
ou encore, si \( X\) a une densité \( f_X\),
\begin{equation}        \label{EqFnCaractfncadens}
	\Phi_X(t)=\int_{\eR} e^{itx}f_X(x)dx
\end{equation}
Nous reconnaissons la transformée de Fourier :
\begin{equation}
	\Phi_X(t)=\hat f_X(-t/2\pi).
\end{equation}

\begin{proposition}     \label{PropDerFnCaract}
	Soit \( X\) une variable aléatoire qui accepte un moment\footnote{Définition \ref{DEFooTECPooMzGYgG}.} d'ordre \( r\geq 1\). Alors la fonction caractéristique \( \Phi_X\) est \( r\) fois continument dérivable et
	\begin{equation}		\label{EQooVOZTooFDkWjd}
		\Phi_X^{(r)}(t)=E\big( (iX)^r e^{itX} \big).
	\end{equation}
\end{proposition}
\index{transformée!de Fourier}

\begin{proof}
	Nous étudions la fonction
	\begin{equation}
		\Phi(t)=\int_{\Omega} e^{itX(\omega)}dP(\omega).
	\end{equation}
	Nous prouvons par récurrence que \( \Phi\) est \( r\) fois dérivable et que
	\begin{equation}
		\Phi^{(k)}(t)=E\big( (iX)^ke^{itX} \big)
	\end{equation}
	pour tout \( \leq r\).

	Lorsque \( r=0\) c'est immédiat. Supposons que la récurrence soit correcte pour \( k<r\), et prouvons pour \( k+1\). Par hypothèse de récurrence,
	\begin{equation}
		\Phi^{(k)}(t)=E\big( (iX)^ke^{itX} \big)=\int_{\Omega}\big( iX(\omega) \big)^ke^{itX(\omega)}d\omega.
	\end{equation}
	Nous posons
	\begin{equation}
		\begin{aligned}
			f\colon \eR\times \Omega & \to \eR                                         \\
			(t,\omega)               & \mapsto\big( iX(\omega) \big)^ke^{itX(\omega)},
		\end{aligned}
	\end{equation}
	et nous vérifions que ce \( f\) satisfait aux hypothèses du théorème~\ref{ThoMWpRKYp}.
	\begin{subproof}
		\spitem[\( f_t\in L(\Omega)\)]
		%-----------------------------------------------------------
		Pour chaque \( t\) fixé, nous posons \( f_t(\omega)=f(t,\omega)\). Cette application est dans \( L^1(\Omega)\) parce que
		\begin{equation}
			| f_t(\omega) |=| X(\omega) |^k.
		\end{equation}
		La proposition \ref{PROPooIQGWooZeQJgb} nous enseigne que \( X\) a un moment d'ordre \( k\). Donc l'intégrale de \( | f_t |\) existe et est finie.

		\spitem[Majoration]
		%-----------------------------------------------------------
		En posant \( G(\omega)=| X(\omega) |^{k+1}\) nous avons \( G\in L^1(\Omega)\) parce que \( k+1\leq r\) et que \( X\) a des moments finis à tous les ordres jusqu'à \( r\). Et \( G\) vérifie aussi
		\begin{equation}
			| (\partial_tf)(t,\omega) |=\big| \big( iX(\omega) \big)^{k+1}e^{itX(\omega)} \big|=| X(\omega) |^{k+1}.
		\end{equation}
	\end{subproof}
	Toutes les hypothèses étant vérifiées, nous avons
	\begin{equation}
		\Phi^{(k+1)}(t)=\frac{d}{dt} \left[ f(t,\omega)  \right]_{t=0}=\int_{\Omega}(\partial_tf)(t,\omega)d\omega=E\big( (iX)^{k+1}e^{itX} \big),
	\end{equation}
	ce qu'il fallait.
\end{proof}

\begin{example}
	Sachant la fonction caractéristique de \( X\), nous pouvons calculer les moments. Par exemple en posant \( r=2\) et \( t=0\) dans la formule \eqref{EQooVOZTooFDkWjd} nous avons
	\begin{equation}
		E(X^2)=-\Phi''_X(0).
	\end{equation}
\end{example}

\begin{theorem}     \label{ThonMxtTy}
	Si \( \Phi_X=\Phi_Y\), alors \( P_X=P_Y\).
\end{theorem}
Notons que cela n'implique pas que \( X=Y\). En effet \( X\) et \( Y\) peuvent même être définis sur des espaces probabilisés différents.

Dans le cas d'une variable aléatoire vectorielle, nous définissons \( \Phi_X\colon \eR^d\to \eR\) par
\begin{equation}        \label{EqydvDxg}
	\Phi_X(v)=E\big(  e^{i\langle v, X\rangle } \big)
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Fonction génératrice des moments, transformée de Laplace}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( X\) une variable aléatoire. Sa \defe{transformée de Laplace}{transformée!Laplace} ou \defe{fonction génératrice des moments}{moment!fonction génératrice}\index{fonction!génératrice} est la fonction
\begin{equation}
	M_X(t)=E( e^{tX})
\end{equation}
pour chaque \( t\) tel que cette espérance existe.

\begin{theorem}[\cite{LetacGen}]
	Soit \( X\) une variable aléatoire réelle et
	\begin{equation}
		I_X=\{ t\in \eR\tq  \,E( e^{tX}) \text{ existe}\}.
	\end{equation}
	La fonction
	\begin{equation}
		\begin{aligned}
			M_X\colon I_X & \to \eR            \\
			t             & \mapsto E( e^{tX})
		\end{aligned}
	\end{equation}
	est la \defe{transformée de Laplace}{transformée!Laplace}\index{Laplace!transformée} de \( X\).
	\begin{enumerate}
		\item
		      \( I_X\) est un intervalle contenant \( 0\).
		\item
		      Si \( I_X\) n'est pas réduit à \( \{ 0 \}\) alors \( M_X\) se développe en série entière
		      \begin{equation}
			      M_X(t)=\sum_{n=0}^{\infty}\frac{ E(X^n) }{ n! }t^n.
		      \end{equation}
		\item
		      Si \( X\) et \( Y\) sont des variables aléatoires indépendantes, alors \( I_{X+Y}=I_X\cap I_Y\) et
		      \begin{equation}
			      M_{X+Y}=M_XM_Y
		      \end{equation}
		      sur \( I_{X+Y}\).
	\end{enumerate}
\end{theorem}
\index{transformée!Laplace}
\index{limite!permutation!utilisation}
\index{série!entière!utilisation}

\begin{proof}
	Le fait que \( 0\) soit dans \( I_X\) est évident : \( E(1)=1\). Pour montrer que \( I_X\) est un intervalle nous prenons \( z\in I_X\) et \( 0<s<z\) ou \( z<s<0\), puis nous montrons que \( s\in I_X\). Il faut remarquer que dans tous les cas,
	\begin{equation}
		e^{sX}\leq 1+ e^{zX}.
	\end{equation}
	En effet soit \( sX\) et \( zX\) sont tous deux à gauche de zéro et alors ils sont tous deux plus petit que \( 1\); soit ils sont tous deux à droite de \( 0\) et alors \( e^{zX}> e^{sX}\) par croissance de l'exponentielle. Nous avons donc dans tous les cas que
	\begin{equation}
		E( e^{sX})=\int_{\eR}f_X(x) e^{sX}dx\leq \int_{\eR}f_X(x)(1+ e^{zx})=1+E( e^{zX}).
	\end{equation}

	Soit maintenant \( a>0\) tel que \( \mathopen[ -a , a \mathclose]\subset I_X\). Étant donné que \(  e^{a| X |}< e^{aX} e^{-aX}\), l'espérance \( E( e^{a| X |})\) existe toujours pour \( | t |\). Nous avons
	\begin{subequations}
		\begin{align}
			\left| M_X(t)-\sum_{n=0}^N\frac{ E(X^n) }{ n! }t^n \right| & =\left| E\Big(  e^{tX}-\sum_{n=0}^N\frac{ X^n }{ n! }t^n \Big) \right|  \\
			                                                           & =\left| E\Big( \sum_{n=N+1}^{\infty}\frac{ X^n }{ n! }t^n \Big) \right| \\
			                                                           & \leq E\left( \sum_{n=N+1}^{\infty}\frac{ | tX |^n }{ n! } \right).
		\end{align}
	\end{subequations}
	Maintenant le but est de prendre la limite \( N\to\infty\) en inversant la limite et l'espérance par le théorème de la convergence dominée (\ref{ThoConvDomLebVdhsTf}). L'intégrale à traiter est
	\begin{equation}
		\lim_{N\to \infty} \int_{\Omega}\sum_{n=N+1}^{\infty}\frac{ | tX(\omega) |^n }{ n! }dP(\omega).
	\end{equation}
	L'intégrande est uniformément borné (en \( N\)) par \(  e^{tX(\omega)}\), qui est intégrable par hypothèse (choix de \( t\)). Du coup
	\begin{equation}
		\lim_{N\to \infty} \int_{\Omega}\sum_{n=N+1}^{\infty}\frac{ | tX(\omega) |^n }{ n! }dP(\omega)=E\left( \lim_{N\to \infty} \sum_{n=N+1}^{\infty}\frac{ | tX |^n }{ n! } \right)=0.
	\end{equation}
\end{proof}


%-------------------------------------------------------
\subsection{Théorème de transfert}
%----------------------------------------------------

\begin{lemma}       \label{LEMooHVFJooBDZYnT}
	Soient un espace de probabilité \( (\Omega,\tribA,P)\) ainsi qu'une variable aléatoire \( X\colon \Omega\to \eR^n\). Nous considérons un autre espace probabilisé \( (S,\tribA_S,\mu)\) ainsi que l'application
	\begin{equation}
		\begin{aligned}
			Y\colon \Omega\times S & \to \eR^n          \\
			(\omega,s)             & \mapsto X(\omega).
		\end{aligned}
	\end{equation}
	Alors :
	\begin{enumerate}
		\item   \label{ITEMooDSIKooSqdyJb}
		      L'application \( Y\) est une variable aléatoire sur\footnote{Produit de tribus, définition \ref{DefTribProfGfYTuR}; produit de mesure, définition \ref{ThoWWAjXzi}.} \( \big( \Omega\times S,\tribA\otimes\tribA_S,P\otimes\mu \big)\).
		\item   \label{ITEMooQMMHooPcGKoD}
		      La loi de \( Y\) est la même\footnote{La même; pas seulement la même à abus de notation près. Ce lemme est une partie du bout manquant dont il est question dans \cite{BIBooWPRAooVIOYhF}.} que celle de \( X\).
	\end{enumerate}
\end{lemma}


\begin{proof}
	Tout repose sur le fait que \( Y\) soit la même chose que \( X\), mais «diagonalisé» sur \( S\). Commençons par prouver que si \( B\subset \eR^n\), alors \( Y^{-1}(B)=X^{-1}(B)\times S\).
	\begin{subproof}
		\spitem[Dans un sens]
		Soit \( (\omega,s)\in X^{-1}(B)\times S\). Alors \( Y(\omega,s)=X(\omega)\in B\). Nous avons donc bien \( X^{-1}(B)\times S\subset Y^{-1}(B)\).
		\spitem[Dans l'autre sens]
		% -------------------------------------------------------------------------------------------- 
		Soit \( (\omega,s)\in Y^{-1}(B)\). Nous avons \( X(\omega)= Y(\omega,s)\in B\), donc \( \omega\in X^{-1}(B)\) et donc \( (\omega,s)\in X^{-1}(B)\times S\).
	\end{subproof}
	Passons maintenant à la preuve proprement dite.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooDSIKooSqdyJb}]
		% -------------------------------------------------------------------------------------------- 
		Soit un borélien \( B\subset \eR^n\). Nous avons \( Y^{-1}(B)=X^{-1}(B)\times S\). La partie \( X^{-1}(B)\) est un borélien de \( \Omega\), et \( S\) est un borélien de \( S\). Donc \( X^{-1}(B)\times S\) est un borélien de \( \Omega\times S\). L'application \( Y\) est donc mesurable. Elle est une variable aléatoire.
		\spitem[Pour \ref{ITEMooQMMHooPcGKoD}]
		% -------------------------------------------------------------------------------------------- 
		Soit un borélien \( B\) de \( \eR^n\). Nous avons
		\begin{subequations}
			\begin{align}
				P_Y(B) & =(P\otimes \mu)\big( Y^{-1}(B) \big)         \\
				       & =(P\otimes \mu)\big( X^{-1}(B)\times S \big) \\
				       & =P\big( X^{-1}(B) \big)\mu(S)                \\
				       & =P\big( X^{-1}(B) \big)                      \\
				       & =P_X(B).
			\end{align}
		\end{subequations}
		Nous avons utilisé le fait que \( \mu(S)=1\).
	\end{subproof}
\end{proof}

La proposition suivante permet de calculer en pratique les intégrales qui définissent par exemple l'espérance mathématique d'une variable aléatoire.
\begin{proposition}[Théorème de transfert\cite{ThjoTrnSaada}]\label{PropintdPintdPXeR}
	Si \( X\colon \Omega\to \eR^d\) est une variable aléatoire, alors dès que \( f\colon \eR^d\to \bar\eR\) est telle qu'une des deux intégrales existe,
	\begin{equation}
		E(f\circ X)=\int_{\Omega}f\big( X(\omega) \big)dP(\omega)=\int_{\eR^d}f(x)dP_X(x)
	\end{equation}
	où \( P_X\) est la loi de \( X\)\footnote{Définition \ref{DEFooBKJVooRJdMeA}.}.

	En particulier, ça marche si \( f\) est borélienne.
\end{proposition}
\index{théorème!transfert}


En utilisant cette proposition nous trouvons une formule pratique pour l'espérance d'une variable aléatoire réelle:
\begin{equation}
	E(X)=\int_{\Omega}X(\omega)dP(\omega)=\int_{\eR}xdP_X(x),
\end{equation}
en vertu de la proposition~\ref{PropintdPintdPXeR} appliquée à la fonction \( f(x)=x\).

\begin{proposition}     \label{PROPooNNTNooOmCLvy}
	Une variable aléatoire réelle \( X\) est intégrable si et seulement si \( P(x=\pm\infty)=0\) et
	\begin{equation}
		\int_{\eR}| x |dP_X(x)<\infty.
	\end{equation}
\end{proposition}

Le lien entre la densité \( f_X\) de la variable aléatoire \( X\) et sa loi est
\begin{equation}
	P_X(A)=\int_Af_X(x)dx
\end{equation}
pour tout ensemble mesurable \( A\subset\eR\). Le lien entre la mesure de Lebesgue et celle de la loi de \( X\) est alors donné par
\begin{equation}
	dP_X(x)=f_X(x)dx.
\end{equation}
En particulier l'espérance de \( X\) peut être calculée à partir de sa densité via la formule
\begin{equation}        \label{EqEspDensform}
	E(X)=\int_{\eR}xdP_X(x)=\int_{\eR}x f_X(x)dx.
\end{equation}

\begin{lemma}       \label{LEMooYVJAooXUkRTh}
	Soit une variable aléatoire \( X\colon \Omega \to \eR^n\) de densité \( g_X\).
	\begin{enumerate}
		\item       \label{ITEMooBUWNooJkiRPp}
		      Nous avons
		      \begin{equation}
			      E(X)=\int_{\eR^n}xg_X(x)dx.
		      \end{equation}
		\item       \label{ITEMooKZHAooKQQmno}
		      Si la fonction \( f\colon \eR^n\to \bar\eR\) est borélienne, alors
		      \begin{equation}
			      E(f\circ X)=\int_{\eR^n}f(x)g_X(x)dx.
		      \end{equation}
	\end{enumerate}
\end{lemma}

\begin{probleme}
	%TODOooAAEXooKTwKCD faire la preuve de LEMooYVJAooXUkRTh et supprimer ce problème.
	Je n'ai pas vérifié le lemme \ref{LEMooYVJAooXUkRTh}. Écrivez-moi si vous en connaissez une démonstration, ou si vous croyez qu'il est faux.

	À mon avis le point \ref{ITEMooBUWNooJkiRPp} est une variation autour de \eqref{EqEspDensform} et \ref{ITEMooKZHAooKQQmno} utilise ça et le théorème de transfert \ref{PropintdPintdPXeR}.
\end{probleme}

\begin{lemma}       \label{LEMooAVTFooIQRvop}
	Soient des variables aléatoires \( X_i\colon \Omega\to \eR^n\) (\( i=1,\ldots, n\)). Nous supposons qu'il existe une application \( g\colon \eR^n\to \eR\) telle que
	\begin{equation}
		E\big( f(X_1,\ldots, X_n) \big)=\int_{\eR^n}f(x)g(x)dx
	\end{equation}
	pour tout fonction borélienne \( f\colon \eR^n\to \eR\).

	Alors \( g\) est une densité pour la variable aléatoire \( (X_1,\ldots, X_n)\colon \Omega\to \eR\).
\end{lemma}

\begin{proposition}[\cite{BIBooHSRGooLYOAcX}]       \label{PROPooNYNUooTCJgpl}
	Soient des variables aléatoires \( X_i\colon \Omega\to \eR\) (\( i=1,\ldots, n\)). Nous posons \( Z=(X_1,\ldots, X_n)\) et nous supposons que \( Z\) est une variable aléatoire à densité notée \( f_Z\).

	Alors les \( X_i\) sont à densité et les densités de \( X_i\) sont
	\begin{equation}
		f_i(t)=\int_{\eR^{n-1}}f_Z(x_1,\ldots, t, x_{i+1},\ldots, x_n)dx_1,\ldots, dx_{i-1}dx_{i+1}\ldots dx_n.
	\end{equation}
\end{proposition}

\begin{proposition}     \label{PROPooVUTLooDPdhxK}
	Soient des variables aléatoires à densité \( X_i\colon \Omega\to \eR\) (\( i=1,\ldots, n\)); nous notons \( f_i\) leurs densités. Nous posons \( Z=(X_1,\ldots, X_n)\).
	\begin{enumerate}
		\item
		      La variable aléatoire \( Z\) est à densité.
		\item   \label{ITEMooUQEFooAAFXJY}
		      Les variables aléatoires \( X_i\) sont indépendantes si et seulement si \( f_Z=\prod_{i=1}^n f_i\).
	\end{enumerate}
	\begin{probleme}
		Je n'ai pas vérifié si cette proposition est vraie. Écrivez-moi si vous connaissez une démonstration, ou si vous savez que c'est faux.
	\end{probleme}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de variables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}     \label{THOooDSTYooXMWfNw}
	Soit \( \mO\), un ouvert de \( \eR^n\) et \( \mO'\) un ouvert de \( \eR^m\) ainsi qu'un difféomorphisme \( C^1\) \( \varphi\colon \mO\to \mO'\). Soit \( X\colon \Omega\to \eR^n\) une variable aléatoire prenant presque surement ses valeurs dans \( \mO\). Si nous supposons que \( X\) a la densité \( f_X\), alors la variable aléatoire \( Y=\varphi(X)\) accepte la densité \( f_Y\colon \mO'\to \eR\) donnée par
	\begin{equation}
		f_Y(v)=f_X\big( \varphi^{-1}(v) \big)| J_{\varphi^{-1}}(v) |.
	\end{equation}
\end{theorem}

\begin{proof}
	Nous devons vérifier la relation
	\begin{equation}
		P(Y\in B)=\int_Bf_Y(v)dv
	\end{equation}
	pour tout borélien \( B\subset \mO'\). Nous avons
	\begin{subequations}
		\begin{align}
			P(Y\in B) & =\int_{\eR^m}\mtu_{B}(v)dP_Y(v)                \\
			          & =E(\mtu_B\circ Y)                              \\
			          & =E\big( (\mtu_B\circ \varphi)\circ X\big)      \\
			          & =\int_{\eR^n}(\mtu_B\circ\varphi)(u)dP_X(u)    \\
			          & =\int_{\eR^n}(\mtu_B \circ\varphi)(u)f_X(u)du.
		\end{align}
	\end{subequations}
	À ce niveau, nous utilisons la formule de changement de variables du théorème~\ref{THOooUMIWooZUtUSg}. Nous trouvons alors
	\begin{equation}
		P(Y\in B)=\int_{\eR^m}\mtu_B\big( \varphi^{-1}(v) \big)f_X\big( \varphi^{-1}(v) \big)| J_{\varphi^{-1}}(v) |dv.
	\end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Convergence}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


\begin{definition}[\cite{ooUEDUooVTNhus,BIBvitali2}]      \label{DEFooZKLFooZkKuMC}
	Soit un espace de probabilité \( (\Omega,\tribA,P)\). Soient des variables aléatoires\footnote{Définition \ref{DEFooRUyiBSUooALDDOa} pour la topologie et donc les boréliens sur la droite réelle achevée.} \( X_i\colon \Omega\to \eR\cup\{ \pm\infty \}\).

	Nous avons plein de modes de convergence.
	\begin{description}
		\item[Convergence presque sure]

		      Nous disons que \( X_i\) converge \defe{presque surement}{convergence!presque surement} vers la variable aléatoire \( X\) et nous notons
		      \begin{equation}
			      X_n\stackrel{p.s.}{\longrightarrow}X
		      \end{equation}
		      si
		      \begin{equation}
			      P\big( \{ \omega\in\Omega\tq\,X_n(\omega)\to X(\omega) \} \big)=1
		      \end{equation}
		      où la convergence \( X_n(\omega)\to X(\omega)\) est la convergence usuelle dans \( \eR\cup\{ \pm\infty \}\).
		\item[Convergence en probabilité]
		      Nous disons que les variables aléatoires réelles \( X_n\) convergent \defe{en probabilité}{convergence!en probabilité} vers la variable aléatoire \( X\) si pour tout \( \eta>0\), on a
		      \begin{equation}
			      P\big( | X_n-X |\geq \eta \big)\to 0,
		      \end{equation}
		      et on note
		      \begin{equation}
			      X_n\stackrel{P}{\longrightarrow}X.
		      \end{equation}

		\item[Convergence \( L^p\)]
		      Si \( X_n\) et \( X\) sont de classe \( L^p\), alors nous pouvons également parler de la convergence \( L^p\) :
		      \begin{equation}
			      X_n\stackrel{ L^p}{\longrightarrow} X.
		      \end{equation}
		      En notant les intégrales comme des espérances, cette convergence signifie
		      \begin{equation}
			      E\big( | X_n-X |^r \big)\to 0.
		      \end{equation}

		\item[Convergence en loi]
		      Nous disons que \( X_n\) converge vers \( X\) \defe{en loi}{convergence!en loi} vers la variable aléatoire \( X\) et nous notons
		      \begin{equation}		\label{EQooWSMPooHCyPwk}
			      X_n\stackrel{\hL}{\longrightarrow}X
		      \end{equation}
		      si pour toute fonction continue et bornée \( g\) nous avons
		      \begin{equation}
			      E\big( g(X_n) \big)\to E\big( g(X) \big).
		      \end{equation}
		      La convergence en loi se distingue des autres modes de convergence en cela que les variables aléatoires \( X_n\) et \( X\) peuvent être définies sur des espaces probabilisés \( (\Omega_n,\tribA_n,P_n)\) et \( (\Omega,\tribA,P)\) tous différents.
	\end{description}
\end{definition}

\begin{theorem}[\cite{BIBvitali2}]	\label{THOooMXFMooVHdSFE}
	Soient des variables aléatoires \( X_n\) et \( X\) sur l'espace probabilité \( (\Omega,\tribA,P)\).
	\begin{enumerate}
		\item
		      Si \( X_n\stackrel{ p.s.}{\longrightarrow} X\) alors \( X_n\stackrel{ P}{\longrightarrow} X\).
		\item		\label{ITEMooRPLIooHcFLCR}
		      Si \( X_n\stackrel{ P}{\longrightarrow} X\) alors il existe une sous-suite \( (Y_n) \) de \( X_n\) telle que \( Y_n\stackrel{ p.s.}{\longrightarrow} X\).
		\item
		      Si les variables aléatoires \( X_n\) sont de classe \( L^r\) et si \( X_n\stackrel{ L^r}{\longrightarrow} X\), alors \( X_n\stackrel{ P}{\longrightarrow} X\).
		\item	\label{ITEMooAYRCooCYLpWX}
		      Si \( X_n\stackrel{ P}{\longrightarrow} X\) et si la famille \( \{ | X |^r \}\) est uniformément intégrable\footnote{Voir lemme \ref{LEMooUOQDooIkVyWm}.}, alors \( X_n\stackrel{ L^r}{\longrightarrow} X\).
		\item
		      Si \( X_n\stackrel{ P}{\longrightarrow} X\) et si \( \limsup_{n}E\big( | X_n |^r \big)\leq E(| X |^r)\) alors \( X_n\stackrel{ L^r}{\longrightarrow} X\).
		\item
		      Si \( X_n\stackrel{ L^r}{\longrightarrow} X\), alors \( X_n\stackrel{ L^p}{\longrightarrow} X\) pour tout \( 0<p\leq r\).
		\item		\label{ITEMooPRGRooUqNKjB}
		      Si \( X_n\stackrel{ P}{\longrightarrow} X\) alors \( X_n\stackrel{ \hL}{\longrightarrow} X\).
		\item
		      Si \( X_n\stackrel{ P}{\longrightarrow} X\) alors toute sous-suite \( (Y_n)\) contient une sous-sous-suite \( (Y'_n)\) telle que \( Y'_n\stackrel{ p.s.}{\longrightarrow} X\)
	\end{enumerate}
	Notez les implications
	\begin{equation}
		\text{presque sure }\Rightarrow\text{ en probabilité }\Rightarrow\text{ en loi}.
	\end{equation}
\end{theorem}


\begin{proposition}[\cite{MonCerveau}]	\label{PROPooXRVOooEYQzGr}
	Si \( X_n\) et \(  X \) est sont des variables aléatoires telles que \( X_n\stackrel{ \hL}{\longrightarrow} X\), alors
	\begin{equation}
		| X_n |\stackrel{ \hL}{\longrightarrow} | X |.
	\end{equation}
\end{proposition}


\begin{lemma}       \label{LEMooADHZooGCxVvn}
	Nous avons \( X_n\stackrel{p.s.}{\longrightarrow}X\) si et seulement si il existe un événement \( A\in\tribA\) tel que \( P(A)=1\) et tel que \( X_n(\omega)\to X(\omega)\) pour tout \( \omega\in A\).
\end{lemma}

\begin{lemma}       \label{LEMooNTNIooKbLwgX}
	Si \( X\) est une variable aléatoire à valeurs dans \( \eR\cup\{ \pm\infty \}\), alors
	\begin{enumerate}
		\item
		      Pour tout \( n\in \eN\), l'application
		      \begin{equation}
			      \begin{aligned}
				      \min(X,n)\colon \Omega & \to \eR\cup\{ \pm\infty \}          \\
				      \omega                 & \mapsto \min\big( X(\omega),n \big)
			      \end{aligned}
		      \end{equation}
		      est une variable aléatoire.
		\item
		      Nous avons la convergence presque sure\footnote{Définition \ref{DEFooZKLFooZkKuMC}.}
		      \begin{equation}
			      \min(X,n)\stackrel{p.s.}{\longrightarrow}X
		      \end{equation}
	\end{enumerate}
\end{lemma}

\begin{proof}
	Le fait que \( \min(X,n)\) soit mesurable est le lemme \ref{LEMooAITEooMjHxvh}. D'ailleurs pour la suite, nous la notons \( X_n=\min(X,n)\). Sinon il faudrait écrire des choses comme \( \min(X,n)(\omega)\) pour \( X_n(\omega)\), et ça deviendrait compliqué.

	Nous allons montrer que pour tout \( \omega\in \Omega\), nous avons \( X_n(\omega)\to X(\omega)\). Ainsi le lemme \ref{LEMooADHZooGCxVvn} conclura.

	Soit \( \omega\in\Omega\). Si \( X(\omega)=\infty\), alors nous avons \( X_n(\omega)=n\) pour tout \( n\) et alors \( X_n(\omega)\stackrel{\eR}{\longrightarrow}\infty=X(\omega)\).

	Si par contre \( X(\omega)<\infty\). Alors en choisissant \( N>X(\omega)\) dans \( \eN\) (lemme \ref{LemooMWOUooVWgaEi}), nous avons \( X_n(\omega)=X(\omega)\) pour tout \( n\geq N\). Et donc aussi \( X_n(\omega)\to X(\omega)\).
\end{proof}

\begin{normaltext}
	Dans certains textes, la variable aléatoire \( \min(X,n)\) est notée \( X\wedge n\). Donc le lemme \ref{LEMooNTNIooKbLwgX} s'écrit
	\begin{equation}
		X\wedge n\stackrel{p.s.}{\longrightarrow}X
	\end{equation}
\end{normaltext}

\begin{proposition}[\cite{MonCerveau,BIBvitali2}]	\label{PROPooDWTWooPygbmK}
	Si \( X_n\stackrel{ p.s.}{\longrightarrow} X\) et si \( X\) est intégrable, alors
	\begin{equation}
		E(X)=E\big( \liminf_n(X_n) \big).
	\end{equation}
\end{proposition}

\begin{proof}
	Soit
	\begin{equation}
		A=\{ \omega\in\Omega\tq X_n(\omega)\to X(\omega) \}.
	\end{equation}
	Vu que \( X_n\stackrel{ p.s}{\longrightarrow} X\), nous avons \( P(A)=1\) et donc, en posant \( Z=\Omega\setminus A\), nous avons \( P(Z)=0\). Nous avons
	\begin{subequations}
		\begin{align}
			E\big( \liminf(X_n) \big) & =\int_{\Omega}\liminf_n(X_n)d\omega                          \\
			                          & =\int_{A}\liminf_n(X_n)d\omega+\int_{Z}\liminf_n(X_n)d\omega
		\end{align}
	\end{subequations}
	La seconde intégrale est nulle par le corolaire \ref{CORooZPLOooXgbtDs}; vous noterez que rien ne garanti que \( \liminf_n X_n(\omega)<\infty\) lorsque \( \omega\in Z\) parce que rien ne garantit que \( \liminf_nX_n(\omega)=X'\omega\).

	Pour \( \omega\in A\) nous avons \( X_n(\omega)\to X(\omega)\), et à fortiori \( \liminf_n X_n(\omega)=X(\omega)\). Donc
	\begin{subequations}
		\begin{align}
			E\big( \liminf_n(X_n) \big) & =\int_A\liminf_n\big( X_n(\omega) \big)d\omega  \\
			                            & =\int_AX(\omega)d\omega                         \\
			                            & =\int_AX(\omega)d\omega +\int_ZX(\omega)d\omega \\
			                            & =\int_{\Omega}X(\omega)d\omega                  \\
			                            & =E(X).
		\end{align}
	\end{subequations}
\end{proof}


\begin{theorem}[Théorème de Vitali\cite{BIBvitali2}]	\label{THOooEPYBooHlSuLI}
	Soient des variables aléatoires \( X_n\in L^r(\Omega,\tribA,P)\) avec \( 0<r<\infty\). Nous supposons que\footnote{Convergence en probabilité, définition \ref{DEFooZKLFooZkKuMC}.} \( X_n\stackrel{ P}{\longrightarrow} X\). Nous avons équivalence entre
	\begin{enumerate}
		\item		\label{SPITEMooGKRVooQoDfKm}
		      L'ensemble \( \{ | X_n |^r \}\) est uniformément intégrable\footnote{Définition \ref{DefOZlZnse}.}.
		\item		\label{SPITEMooUYTKooBClYcR}
		      Nous avons \( X\in L^r(\Omega) \) et \( X_n\stackrel{ L^r(\Omega)}{\longrightarrow} X\).
		\item		\label{SPITEMooQYLOooDiBvne}
		      \( E\big( | X_n |^r \big)\to E\big( | X |^r \big)\).
		\item		\label{SPITEMooRVFAooXUNQlf}
		      \( \limsup_n E\big( | X_n |^r \big)\leq E\big( | X |^r \big)\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[\ref{SPITEMooGKRVooQoDfKm} \( \Rightarrow\) \ref{SPITEMooUYTKooBClYcR}]
		%-----------------------------------------------------------

		Par le théorème \ref{THOooMXFMooVHdSFE}\ref{ITEMooRPLIooHcFLCR}, il existe une sous-suite \( (Y_n)\) de \( (X_n)\) telle que \( Y_n\stackrel{ p.s.}{\longrightarrow} X\). Nous avons donc également \( | Y_n |^r\stackrel{ p.s.}{\longrightarrow} | X |^r\). Nous utilisons \ref{PROPooDWTWooPygbmK} :
		\begin{subequations}
			\begin{align}
				E\big( | X |^r \big) & =E\big( \liminf_n| Y_n |^r \big)                 & \text{prop. \ref{PROPooDWTWooPygbmK}} \\
				                     & \leq \liminf_n\Big( E\big( | Y_n |^r \big) \Big) & \text{Fatou \ref{LemFatouUOQqyk}}.
			\end{align}
		\end{subequations}
		Nous nous souvenons que la partie \( \{ | X_n |^r \}\) est uniformément intégrable; à fortiori la partie \( \{ | Y_n |^r \}\) l'est également. Le théorème \ref{THOooGLYJooCbNysA}\ref{ITEMooCAIFooHXOKQb} dit qu'il existe \( M>0\) tel que \( E\big( | Y_n |^r \big)<M\). Nous avons donc
		\begin{equation}
			E\big( | X |^r \big)  \leq \liminf_n\Big( E\big( | Y_n |^r \big) \Big) < M.
		\end{equation}
		Cela prouve que \( | X |^r\in L^1(\Omega)\), c'est-à-dire que \( X\in L^r(\Omega)\). Voilà une bonne chose de faite. Nous devons encore prouver que \( X_n\stackrel{ L^r(\Omega)}{\longrightarrow} X\).


		Soit \( \omega\in\Omega\). La c-r inégalité (proposition \ref{PROPooTIJYooJGxphQ}) donne
		\begin{equation}		\label{EQooZYDGooKhtrqz}
			| X_n(\omega)-X(\omega) |^r\leq c_r\big( | X_n(\omega) |^r+| X(\omega) |^r \big).
		\end{equation}
		Vu que \( \{ | X_n(\omega) | \}_{n\in \eN}\) est uniformément intégrable, les différents points de la proposition \ref{PROPooPZRAooSiqgKT} et l'inégalité \ref{EQooZYDGooKhtrqz} montrent que \( \{ | X_n-X |^r \}_{n\in \eN}\) est uniformément intégrable.

		Nous utilisons la proposition \ref{THOooGLYJooCbNysA}. Soit \( \epsilon>0\). Il existe \( \delta_{\epsilon}>0\) tel que pour tout \( n\) et pour toute partie \( A\in\tribA\) telle que \( P(A)\leq \delta_{\epsilon}\),
		\begin{equation}
			E\big( | X_n-X |^r\mtu_A \big)<\epsilon.
		\end{equation}
		Vu que \( X_n\stackrel{ P}{\longrightarrow} X\), si \( n\) est assez grand\quext{Dans \cite{BIBvitali2}, il choisit \( \epsilon\) petit et non \( n\) grand. Je ne comprends pas pourquoi.}, nous avons \( P\big( | X_n-X |\geq \epsilon \big)<\delta_{\epsilon}\). Pour de tels \( n\), nous avons
		\begin{equation}
			E\Big( | X_n-X |^r\mtu_{\{ | X_n-X |\geq \epsilon \}} \Big)<\epsilon.
		\end{equation}
		Par la \( \sigma\)-additivité de l'intégrale (et le recouvrement en \( \{ | X_n-X |=\epsilon \}\)), nous avons
		\begin{subequations}
			\begin{align}
				E\big( | X_n-X |^r \big) & \leq E\Big( | X_n-X |^r\mtu_{\{ | X_n-X |\geq \epsilon \}} \Big) + E\Big( | X_n-X |^r\mtu_{\{ | X_n-X |\leq \epsilon \}} \Big) \\
				                         & < \epsilon + E\Big( | X_n-X |^r\mtu_{\{ | X_n-X |\leq \epsilon \}} \Big)		\label{SUBEQooHHISooMueQQp}
			\end{align}
		\end{subequations}
		Sur la partie \( \{ | X_n-X |\leq \epsilon \}\), nous pouvons majorer \( | X_n(\omega)-X(\omega) |\leq \epsilon\). Vu que \( P(\Omega)=1\) nous avons
		\begin{subequations}
			\begin{align}
				E\Big( | X_n-X |^r\mtu_{\{ | X_n-X |\leq \epsilon \} }\Big) &
				= \int_{\Omega}| X_n(\omega)-X(\omega) |^r\mtu_{\{ | X_n-X|\leq\epsilon\}}dP(\omega)                             \\
				                                                            & \leq  \int_{\Omega}\epsilon^rdP(\omega)=\epsilon^r
			\end{align}
		\end{subequations}
		Cela nous permet de majorer la seconde partie de \eqref{SUBEQooHHISooMueQQp}. Au final si \( n\) est grand,
		\begin{equation}
			E\big( | X_n-X |^r \big)\leq \epsilon+\epsilon^r.
		\end{equation}
		Autrement dit, \( E\big( | X_n-X |^r \big)\to 0\) ou encore
		\begin{equation}
			X_n\stackrel{ L^r}{\longrightarrow} X.
		\end{equation}

		\spitem[\ref{SPITEMooUYTKooBClYcR} \( \Rightarrow\) \ref{SPITEMooQYLOooDiBvne}]
		%-----------------------------------------------------------

		C'est la proposition \ref{PROPooXXOYooCUtDZf}.

		\spitem[\ref{SPITEMooQYLOooDiBvne} \( \Rightarrow\) \ref{SPITEMooRVFAooXUNQlf}]
		%-----------------------------------------------------------

		Si la limite existe, alors, à fortiori, la limite supérieure existe et est égale.

		\spitem[\ref{SPITEMooRVFAooXUNQlf} \( \Rightarrow\) \ref{SPITEMooGKRVooQoDfKm}]
		%-----------------------------------------------------------

		Nous supposons que \( E\big( | X_n |^r \big)\leq E\big( | X |^r \big)\). Pour chaque \( \lambda>0\), nous considérons la fonction
		\begin{equation}
			\begin{aligned}
				f_{\lambda}\colon \mathopen[ 0,\infty\mathclose[ & \to \eR                                                                  \\
				x                                                & \mapsto \begin{cases}
					                                                           x^r                            & \text{si }  x^r\leq \lambda     \\
					                                                           \lambda^r-\lambda^r(x-\lambda) & \text{si }\lambda<x^r<\lambda+1 \\
					                                                           0                              & \text{si } x^r\geq\lambda+1.
				                                                           \end{cases}
			\end{aligned}
		\end{equation}
		\begin{subproof}
			\spitem[\( f_{\lambda}\) est continue]
			%-----------------------------------------------------------
			Si \( x=\lambda\), la première ligne donne \( f_{\lambda}(\lambda)=\lambda^r\) (parce que \( \lambda>0\)), qui coïncide avec la valeur de la seconde ligne en \( x=\lambda\). Si \( x=\lambda+1\), les seconde et troisième lignes donnent toutes deux \( 0\).

			\spitem[\( f_{\lambda}\) est bornée]
			%-----------------------------------------------------------
			Chacun des trois bouts de formule est borné.

			\spitem[Une majoration avec \( f_{\lambda}\)]
			%-----------------------------------------------------------
			Soit \(g \colon \Omega\to \eR  \). Nous montrons que
			\begin{equation}		\label{EQooVLGYooYEiVoS}
				f_{\lambda}\circ | g |\leq | g |^r\mtu_{\{ | g |^r\leq \lambda+1 \}}.
			\end{equation}
			Il y a trois possibilités en fonction de ce que vaut \( | g(\omega) |^r\).
			\begin{subproof}
				\spitem[Si \( | g(\omega)|\leq \lambda \)]
				%-----------------------------------------------------------
				Alors \( (f_{\lambda}\circ |g|)(\omega)=| g(\omega) |^r\) alors que \( \mtu_{\{ | g |^r\leq \lambda+1 \}}(\omega)=1\).

				\spitem[Si \( \lambda<| g(\omega) |^r<\lambda+1\)]
				%-----------------------------------------------------------
				Nous avons
				\begin{subequations}
					\begin{align}
						(f_{\lambda}\circ g)(\omega) & =\lambda^r-\lambda^r\big( g(\omega)-\lambda \big)                                  \\
						                             & <\lambda^r-\lambda^r(\lambda-\lambda)             & \text{pcq. } g(\omega)<\lambda \\
						                             & =\lambda^r                                                                         \\
						                             & \leq | g(\omega) |^r.
					\end{align}
				\end{subequations}
				\spitem[Si \( | g(\omega) |^r\geq \lambda+1\)]
				%-----------------------------------------------------------
				Nous avons \( (f_{\lambda}\circ g)(\omega)=0\).
			\end{subproof}

			\spitem[Une autre inégalité avec \( f_{\lambda}\)]
			%-----------------------------------------------------------
			Nous prouvons à présent que
			\begin{equation}		\label{EQooNWROooBcZZTo}
				| X |^r-(f_{\lambda}\circ X)\leq | X |^r\mtu_{\{ | X |^r\geq \lambda \}}.
			\end{equation}
			\begin{subproof}
				\spitem[Si \( | X(\omega) |\leq \lambda\)]
				%-----------------------------------------------------------
				Dans ce cas nous avons \( 0\) à gauche de \eqref{EQooNWROooBcZZTo}, et pas besoin de regarder à droite.

				\spitem[Si \( \lambda<| X(\omega) |^r<\lambda+1\)]
				%-----------------------------------------------------------
				À gauche nous avons
				\begin{subequations}
					\begin{align}
						\lambda^r-\lambda^r\big( | X(\omega) |-\lambda \big) & \leq \lambda^r-\lambda^r(\lambda-\lambda) \\
						                                                     & =\lambda^r                                \\
						                                                     & \leq | X(\omega) |^r.
					\end{align}
				\end{subequations}
				Et à droite, nous avons la même chose.

				\spitem[Si \( | X(\omega) |\geq \lambda+1\)]
				%-----------------------------------------------------------
				Alors nous avons \( 0\) à gauche.
			\end{subproof}

			\spitem[Une limite avec l'espérance]
			%-----------------------------------------------------------
			Par hypothèse, \( X_n\stackrel{ P}{\longrightarrow} X\). Par le théorème \ref{THOooMXFMooVHdSFE}\ref{ITEMooPRGRooUqNKjB}, cela implique la convergence en loi \( X_n\stackrel{ \hL}{\longrightarrow} X\). Et donc \( | X_n |\stackrel{ \hL}{\longrightarrow} |X|\) et aussi\footnote{Proposition \ref{PROPooXRVOooEYQzGr}.}
			\begin{equation}		\label{EQooJGSVooBXkEnG}
				E(f_{\lambda}\circ |X_n|)=E(f_{\lambda}\circ |X|)
			\end{equation}
			parce que \( f_{\lambda}\) est continue et bornée.
		\end{subproof}
		Avec tout ça, nous pouvons calculer un petit peu :
		\begin{subequations}
			\begin{align}
				\limsup_n E\Big( | X_n |^r\mtu_{\{ | X_n |^r>\lambda+1 \}} \Big) & =\limsup_n\Big( E(| X_n |^r)-E\big( | X_n |^r\mtu_{\{ | X_n |^r\leq \lambda+1 \}} \big) \Big)                                           \\
				                                                                 & \leq  E(| X |^r)  - \limsup_n\Big(E\big( | X_n |^r\mtu_{\{ | X_n |^r\leq \lambda+1 \}} \big) \Big)                                      \\
				                                                                 & \leq  E(| X |^r)  - \liminf_n\Big(E\big( | X_n |^r\mtu_{\{ | X_n |^r\leq \lambda+1 \}} \big) \Big)                                      \\
				                                                                 & \leq  E(| X |^r)  - \liminf_n E(f_{\lambda}\circ |X_n|)                                            & \text{de \eqref{EQooVLGYooYEiVoS}} \\
				                                                                 & \leq E(| X |^r)-E(f_{\lambda}\circ |X|)                                                            & \text{de \eqref{EQooJGSVooBXkEnG}} \\
				                                                                 & = E\big( | X |^r-(f_{\lambda}\circ| X |) \big)                                                                                          \\
				                                                                 & \leq E\big( | X |^r\mtu_{\{ | X |^r\geq \lambda \}} \big)
			\end{align}
		\end{subequations}
		Nous avons donc démontré que pour tout \( \lambda>0\),
		\begin{equation}
			\limsup_n E\Big( | X_n |^r\mtu_{\{ | X_n |^r>\lambda+1 \}} \Big) \leq E\big( | X |^r\mtu_{\{ | X |^r\geq \lambda \}} \big).
		\end{equation}
		Pour tester l'uniforme intégrabilité, il nous suffit maintenant de prendre la limite \( \lambda\to \infty\). La proposition \ref{PROPooPFZJooLySrgp} dit que \( E\big( | X |^r\mtu_{\{ | X |^r\geq \lambda \}} \big) \stackrel{ \lambda\to\infty}{\longrightarrow} 0\). Nous avons donc bien l'uniforme intégrabilité.
	\end{subproof}
\end{proof}

%-------------------------------------------------------
\subsection{Convergence et fonction caractéristique}
%----------------------------------------------------


\begin{proposition}[Helly-Bray\cite{BIBvitali2}]     \label{PrpopCaractCvL}
	Deux autres caractérisations de la convergence en loi\footnote{Définition \ref{EQooWSMPooHCyPwk}}.
	\begin{enumerate}
		\item
		      Nous avons \( X_n\stackrel{\hL}{\longrightarrow}X\) si et seulement si
		      \begin{equation}
			      \Phi_{X_n}(v)\to\Phi_X(v)
		      \end{equation}
		      pour tout \( v\in\eR^d\). Ici \( \Phi_X\) est la fonction caractéristique de \( X\).
		\item
		      Dans la définition de la convergence en loi nous pouvons indifféremment utiliser les fonctions continues et bornées, les fonctions continues à support compact ou les fonctions bornées uniformément continues.
	\end{enumerate}
\end{proposition}


Dans le cas particulier \( d=1\) nous avons quelques critères supplémentaires.

\begin{proposition}     \label{PropoFnrepCvL}
	Supposons que les variables aléatoires \( X_n\) soient réelles, et notons \( F_n\) la fonction de répartition de \( X_n\). Si \( F_n(x)\to F(x)\) pour tout \( x\) dans l'ensemble des points de continuité de \( F\), alors \( X_n\stackrel{\hL}{\longrightarrow}X\).
\end{proposition}

\begin{proposition}		\label{PROPooNRQJooXoCcbZ}
	Si les \( X_n\) sont des variables aléatoires réelles positives, et si \( X\) est une variable aléatoire positive, alors \( X_n\stackrel{\hL}{\longrightarrow}X\) si les transformées de Laplace des fonctions de répartition convergent ponctuellement, c'est-à-dire si
	\begin{equation}
		E\big(  e^{-\alpha X_n} \big)\to E\big(  e^{-\alpha X} \big)
	\end{equation}
	pour tout \( \alpha\geq 0\).
\end{proposition}

\begin{proposition}		\label{PROPooZMHKooLwCwqs}
	Si les \( X_n\) et \( X\) sont des variables aléatoires réelles discrètes à valeurs dans \( \{ x_0,x_1,\ldots \}\) alors \( X_n\stackrel{\hL}{\longrightarrow} X\) si et seulement si
	\begin{equation}
		P(X_n=x_k)\to P(X=x_k)
	\end{equation}
	pour tout \( k\in\eN\).
\end{proposition}

\begin{proposition}[\cite{CourgGudRennes}]     \label{PropXncvXFXcvFxt}
	Soient \( X_n\) et \( X\) des variables aléatoires réelles. Nous avons
	\begin{equation}
		X_n\stackrel{\hL}{\longrightarrow}X
	\end{equation}
	si et seulement si pour tout \( t\) où \( F_X\) est continue,
	\begin{equation}
		\lim_{n\to \infty} F_{X_n}(t)=F_X(t).
	\end{equation}
\end{proposition}

\begin{proposition}[\cite{CourgGudRennes}]     \label{PropCvLfcvPsicst}
	Soit \( X_n\) une suite de variables aléatoires \( \Omega\to \eR^d\) et \( a\in\eR^d\). Si \( X_n\stackrel{\hL}{\longrightarrow}a\), alors
	\begin{equation}
		X_n\stackrel{P}{\longrightarrow}a.
	\end{equation}
\end{proposition}

\begin{proof}
	Quitte à passer aux composantes, nous pouvons supposer que \( d=1\). Soit \( \eta>0\); nous savons que l'inégalité \( | x |>a\) a pour solution \( x>a\) ou \( x<-a\). Dans notre cas,
	\begin{subequations}
		\begin{align}
			P\big( | X_n-a |>\eta \big) & =P(X_n-a>\eta)+P(X_n-a<-\eta)                        \\
			                            & =P(X_n>\eta+a)+P(X_n<a-\eta)                         \\
			                            & =1-P(X_n\leq \eta+a)+P(X_n\leq a-\eta)-P(X_n=a-\eta) \\
			                            & \leq 1-F_{X_n}(\eta+a)+F_{X_n}(a-\eta)
		\end{align}
	\end{subequations}
	où la majoration est l'oubli du terme \( P(X_n=a-\eta)\), lequel est positif ou nul et \( F_{X_n}\) est la fonction de répartition de \( X_n\), définition~\ref{DefooYAZVooNdxDCx}. Nous allons utiliser la proposition~\ref{PropXncvXFXcvFxt}. La fonction de répartition de la variable aléatoire constante \( X=a\) est donnée par
	\begin{equation}
		F_a(t)=P(a\leq t)=\caract_{\mathopen[ 0 , \infty \mathclose[}(t-a).
	\end{equation}
	Par conséquent, la convergence en loi \( X_n\stackrel{\hL}{\longrightarrow}a\) nous montre que
	\begin{equation}
		F_{X_n}(t)\to \caract_{\mathopen[ 0 , \infty \mathclose[}(t-a)
	\end{equation}
	pour tout \( t\neq a\) parce que \( t=0\) est un point de discontinuité de \( \caract_{\mathopen[ 0 , \infty \mathclose[}\). Nous avons par conséquent
	\begin{equation}
		P\big( | X_n-a |>\eta \big)=1-\caract_{\mathopen[ 0 , \infty \mathclose[}(\eta)+\caract_{\mathopen[ 0 , \infty \mathclose[}(-\eta)=1-1+0=0
	\end{equation}
	parce que \( \eta>0\).
\end{proof}

Le lemme de Slutsky sera utilisé en combinaison avec la proposition~\ref{PropcvLsousfonc} pour calculer des intervalles de confiance, voir par exemple ce qui se passe autour de l'équation \eqref{EqGJxxjqs}.
\begin{lemma}[Slutsky\cite{MPSmaitrise}]  \label{LemgXDlhs}
	Soient \( X_n\) et \( Y_n\) des suites de variables aléatoires réelles telles que
	\begin{equation}
		\begin{aligned}[]
			X_n & \stackrel{\hL}{\longrightarrow} X     \\
			Y_n & \stackrel{P}{\longrightarrow}a\in\eR.
		\end{aligned}
	\end{equation}
	Alors \( (X_n,Y_n)\stackrel{\hL}{\longrightarrow}(X,a)\).
\end{lemma}
\index{lemme!de Slutsky}

\begin{proof}
	Étant donné que \( Y_n\stackrel{\hL}{\longrightarrow} a\), nous avons \( Y_n\stackrel{P}{\longrightarrow} a\) par la proposition~\ref{PropCvLfcvPsicst}. Soit une fonction \(f\colon \eR^2\to \eR^2 \); nous devons prouver que
	\begin{equation}
		E\big( f(X_n,Y_n) \big)\to E\big( f(X,a) \big).
	\end{equation}
	Soit \( \epsilon>0\). Nous avons
	\begin{equation}    \label{EqEparXnYnfXa}
		E\big( \| f(X_n,Y_n)-f(X,a) \| \big)\leq E\big(  \| f(X_n,Y_n)-f(X_n,a) \|  \big)+E\big(   \| f(X_n,a)-f(X,a) \|  \big).
	\end{equation}
	La fonction \( g(t)=f(t,a)\) étant continue et bornée, la convergence en loi \( X_n\stackrel{\hL}{\longrightarrow}X\) donne
	\begin{equation}
		E\big( \| f(X_n,a)-f(X,a) \| \big)\to 0.
	\end{equation}
	Étudions à présent le premier terme du membre de droite de \eqref{EqEparXnYnfXa}. Pour tout \( \eta> 0\) et toute variables aléatoires \( Z\) et \( Z'\) nous avons
	\begin{equation}
		E(Z)=E(Z\caract_{| Z' |<\eta})+E(Z\caract_{| Z' |\geq \eta}).
	\end{equation}
	Nous décomposons donc le premier terme de \eqref{EqEparXnYnfXa} en
	\begin{equation}    \label{EqEXnADecomsecvolta}
		\begin{aligned}[]
			E\big( \| f(X_n,Y_n)-f(X_n,a) \| \big) & =E\big( \| f(X_n,Y_n)-f(X_n,a) \|\caract_{| Y_n-a |<\eta} \big)          \\
			                                       & \quad+E\big( \| f(X_n,Y_n)-f(X_n,a) \|\caract_{| Y_n-a |\geq\eta} \big).
		\end{aligned}
	\end{equation}
	Choisissons maintenant une valeur de \( \eta\) telle que
	\begin{equation}
		| (x,y)-(x',y') |<\eta\Rightarrow| f(x,y)-f(x',y') |\leq \epsilon.
	\end{equation}
	Un tel \( \eta\) existe par l'uniforme continuité de \( f\). Dans le premier terme, \( | Y_n-a |<\eta\), par conséquent
	\begin{equation}
		\| (X_n,Y_n)-(X_n,a) \|=| Y_n-a |<\eta
	\end{equation}
	et donc
	\begin{equation}
		\| f(X_n,Y_n)-f(X_n,a) \|\leq \epsilon.
	\end{equation}
	Le premier terme devient donc
	\begin{equation}
		E\big( \| f(X_n,Y_n)-f(X_n,a) \|\caract_{| Y_n-a |<\eta} \big)\leq \epsilon E(\caract_{| Y_n-a |<\eta})\leq \epsilon
	\end{equation}
	parce que \( E(\caract_A)=P(A)\leq 1\). Pour le second terme de \eqref{EqEXnADecomsecvolta} nous effectuons la majoration
	\begin{equation}
		\| f(X_n,Y_n)-f(X_n,a) \|\leq 2\| f \|_{\infty}
	\end{equation}
	tandis que la convergence \( Y_n\stackrel{P}{\longrightarrow} a\) entraine
	\begin{equation}
		P\big( | Y_n-a |\geq \eta \big)\to 0.
	\end{equation}
\end{proof}

\begin{proposition}[\cite{JPzkRmm}]     \label{PropcvLsousfonc}
	Soient \( X_i\) des variables aléatoires telles que
	\begin{equation}
		X_i\stackrel{\hL}{\longrightarrow} X
	\end{equation}
	et \( h\), une fonction mesurable sur l'espace d'arrivée de \( X_i\). Soit \( C\) l'ensemble des points de continuité de \( h\) au sens
	\begin{equation}
		C=\{ \omega\in\Omega\tq  h\text{ est continue en } X_i(\omega) \}.
	\end{equation}
	Alors si \( P(X\in C)=1\), nous avons
	\begin{equation}
		h(X_i)\stackrel{\hL}{\longrightarrow}h(X).
	\end{equation}
\end{proposition}
%TODO : une preuve; elle a l'air d'être sur Wikipédia.

Une conséquence de cette proposition couplée au lemme de Slutsky est le résultat suivant, qui est donné sous le nom de \wikipedia{en}{Slutsky's_theorem}{théorème de Slutsky} sur Wikipédia.
\begin{corollary}       \label{CorINgTPH}
	En reprenant les notations du lemme de Slutsky, si
	\begin{subequations}
		\begin{align}
			X_n & \stackrel{\hL}{\longrightarrow} X \\
			Y_n & \stackrel{P}{\longrightarrow} a,
		\end{align}
	\end{subequations}
	alors
	\begin{subequations}
		\begin{align}
			X_n+Y_n     & \stackrel{\hL}{\longrightarrow}X+a     \\
			X_nY_n      & \stackrel{\hL}{\longrightarrow}aX      \\
			Y_n^{-1}X_n & \stackrel{\hL}{\longrightarrow}a^{-1}X \\
		\end{align}
	\end{subequations}
	pourvu que \( a\) soit inversible.
\end{corollary}


\begin{lemma}[Borel-Cantelli]\index{lemme!de Borel-Cantelli}		\label{LEMooXWMXooYEwWBn}
	Soit \( (A_n)\) une suite d'événements (avec \( A_n\in\tribA\) pour tout \( n\)).
	\begin{enumerate}
		\item
		      Si \( \sum_{n=0}^{\infty}P(A_n)\) converge, alors
		      \begin{equation}
			      P(A_n\,\text{i.s.})=0.
		      \end{equation}
		\item
		      Si la somme \( \sum_nP(A_n)\) diverge, et si de plus les \( A_i\) sont indépendants, alors
		      \begin{equation}
			      P(A_n\,\text{i.s.})=1.
		      \end{equation}
	\end{enumerate}
\end{lemma}
La notation \( P(A_n\,\text{i.s.})\) signifie «infiniment souvent», c'est-à-dire
\begin{equation}
	P(A_n\,\text{i.s.})=P\big( \bigcap_{N\in\eN}\bigcup_{k\geq N}A_k \big)=P(\limsup A_n)
\end{equation}

Une façon de paraphraser le lemme de Borel-Cantelli est que nous avons l'alternative
\begin{equation}    \label{EqparaphrCantelli}
	P(\limsup A_n)=\begin{cases}
		0 & \text{si }\sum_{n\geq 0}P(A_n)<\infty \\
		1 & \text{sinon}.
	\end{cases}
\end{equation}

\begin{proposition}
	Soit \( X_n\), une suite de variables aléatoires et \( X\) une variable aléatoire. Si
	\begin{equation}
		\sum_nP\big( \| X_n-X \|>\epsilon \big)<\infty
	\end{equation}
	pour tout \( \epsilon\), alors \( X_n\stackrel{p.s.}{\longrightarrow}X\).
\end{proposition}

\begin{proof}
	Fixons \( \epsilon\) et considérons les événements \( A_{n}=\| X_n-X \|>\epsilon\). L'hypothèse dit que
	\begin{equation}
		\sum_nP(A_{n,\epsilon})<\infty
	\end{equation}
	et le lemme de Borel-Cantelli implique que
	\begin{equation}
		P(\limsup\| X_n-X \|>\epsilon)=0.
	\end{equation}
	Un élément \( \omega\) est dans \( \limsup A_n\) si il est contenu dans tous les \( A_n\), par conséquent, pour chaque \( \epsilon\) nous avons l'inclusion
	\begin{equation}
		\{ \omega\in\Omega\tq X_n(\omega)\to X(\omega) \}\subset\complement\limsup A_n.
	\end{equation}
	Nous pouvons aller plus loin et écrire
	\begin{equation}        \label{EqProbomOmXnmXforalleps}
		\{ \omega\in\Omega\tq X_n(\omega)\to X(\omega) \}=\complement\{ \omega\in\Omega\tq \| X_n-X \|>\epsilon,\forall\epsilon>0 \}.
	\end{equation}
	Or la probabilité de l'ensemble
	\begin{equation}
		\{ \omega\in\Omega\tq\| X_n-X \|>\epsilon \}
	\end{equation}
	est \( 0\) pour chaque \( \epsilon\), et par conséquent la probabilité du membre de droite de \eqref{EqProbomOmXnmXforalleps} est \( 1\).
\end{proof}

\begin{example}
	Considérons une suite de \( 0\) et de \( 1\) dans laquelle le \( 1\) arrive avec une probabilité \( p\) et le \( 0\) avec une probabilité \( 1-p\). Une telle suite est modélisée par une suite de variables aléatoires de Bernoulli \( (X_n)_{n\in\eN}\) indépendantes de paramètre \( p\).

	Question : une telle suite contient elle une infinité de \( 1\) ? Considérons les événements indépendants \( A_n=\{ X_n=1 \}\). Nous avons
	\begin{equation}
		\sum_n P(A_n)=\sum_nP(X_n=1)=\sum_np=\infty.
	\end{equation}
	Par Borel-Cantelli et son expression \eqref{EqparaphrCantelli}, nous avons alors
	\begin{equation}
		P(\limsup A_n)=1.
	\end{equation}
	Donc une infinité d'événements \( A_n\) se produisent, et nous avons bien une infinité de \( 1\) dans la suite.

	Remarque : dans ce raisonnement nous pouvons considérer une probabilité non constante \( p_n\) tant que la série \( \sum_np_n\) diverge.
\end{example}

\begin{example}		\label{EXooSRQUooWEpXRi}
	À propos de maximum. La fonction \( h\colon \eR^d\to \eR\) donnée par \( h(x_1,\ldots, x_d)=\max_i\{ x_i \}\) est une fonction continue. Nous voudrions prouver que si on a une famille (finie en \( i=1,\ldots, l\)) de suites (en \( n\)) de variables aléatoires \( X^{(i)}_n\stackrel{p.s.}{\longrightarrow} a\) convergeant toutes vers la même limite \( a\), alors
	\begin{equation}
		M_n=\max_i\{ X^{(i)}_n \}\stackrel{p.s.}{\longrightarrow} a.
	\end{equation}
	D'abord si nous avons \( l\) suites numériques \( (x^{(i)}_n)\), alors la suite
	\begin{equation}
		M_n=\max_ix_n^{(i)}
	\end{equation}
	converge vers la même limite. En effet si \( \epsilon>0\) est donné, il suffit de prendre \( N_i\) l'entier tel que \( | x^{(i)}_n-a |\leq \epsilon\) pour tout \( n>N_i\). Et ensuite on prend \( N>\max\{ N_i \}\).

	Si maintenant au lieu de suites numériques, nous avons des variables aléatoires, le résultat reste valable. Nous cherchons à prouver que
	\begin{equation}\label{EqMZcnwTh}
		P\Big( \{ \omega\in \Omega\tq \max\{ X^{(i)}_n(\omega) \}\to a \} \Big)=1.
	\end{equation}
	Par ce que nous venons de dire sur les suites numériques, un élément \( \omega\) n'est pas dans cet ensemble seulement si il y a un \( i\) pour lequel \( X^{(i)}_n\) ne converge pas vers \( a\). Or cela, pour chaque \( i\) est un événement de probabilité zéro.

	Les \( \omega\) qui ne fonctionneront pas dans l'équation \eqref{EqMZcnwTh} sont ceux de la réunion d'un ensemble fini d'ensembles de probabilité nulle. C'est donc de probabilité nulle.
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Loi des grands nombres, théorème central limite}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Loi des grands nombres}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Inégalité de Markov\cite{SGpYnKH}]
	Soit une variable aléatoire \( X\in L^p\) et \( \epsilon>0\). Nous avons
	\begin{equation}
		P(| X |\geq \epsilon)\leq \frac{1}{ \epsilon^r }E(| X |^r).
	\end{equation}
\end{lemma}
\index{Markov!inégalité}
\index{inégalité!Markov}

\begin{proof}
	Nous avons
	\begin{equation}
		E(| X |^r)\geq\int_{| X |\geq \epsilon}X(\omega)dP(\omega)\geq \epsilon^r\int_{| X |\geq\epsilon}dP=\epsilon^rP(| X |\geq\epsilon).
	\end{equation}
\end{proof}

\begin{corollary}[\cite{SGpYnKH}]   \label{CorEWhIsBB}
	Soit \( \phi\), une fonction croissante et positive ou nulle sur l'intervalle \( I\). Soit aussi une variable aléatoire \( Y\colon \Omega\to \eR\) telle que \( P(Y\in I)=1\). Alors pour tout \( b\in I\) tel que \( \phi(b)>0\) nous avons
	\begin{equation}
		P(Y\geq b)\leq \frac{ E\big[ \phi(Y) \big] }{ \phi(b) }.
	\end{equation}
\end{corollary}

\begin{theorem}[Loi forte des grands nombres]     \label{ThoefQyKZ}
	Soit \( (X_n)\) une suite de variables aléatoires réelles
	\begin{enumerate}
		\item
		      indépendantes et identiquement distribuées,
		\item
		      intégrables (c'est-à-dire dans \( L^1\)),
	\end{enumerate}
	alors
	\begin{equation}
		\frac{1}{ n }\sum_{i=1}^nX_i  \stackrel{p.s.}{\longrightarrow} E(X_1).
	\end{equation}
\end{theorem}
\index{loi!des grands nombres!forte}
Note : étant donné que les variables aléatoires sont identiquement distribuées, nous avons évidemment \( E(X_1)=E(X_2)=\ldots\)

\begin{probleme}
	Est-ce que les variables aléatoires doivent vraiment être réelles ?
\end{probleme}

\begin{corollary}		\label{CORooAYTPooMdAdoN}
	Si les variables aléatoires réelles \( X_n\) sont
	\begin{enumerate}
		\item
		      indépendantes et identiquement distribuées,
		\item
		      dans \( L^2\)
	\end{enumerate}
	alors
	\begin{equation}
		\bar X_n\stackrel{P}{\longrightarrow}E(X_1).
	\end{equation}
\end{corollary}

\begin{proof}
	Nous voulons prouver que pour tout \( \eta>0\),
	\begin{equation}
		P\big( | \bar X_n-E(X_1) |>\eta \big)\to 0.
	\end{equation}
	Remarquons d'abord que les variables aléatoires \( X_n\) étant identiquement distribuées, \( E(\bar X_n)=E(X_1)\) parce que \( E(X_i)=E(X_1)\) pour tout \( i\). L'inégalité de Markov avec \( r=2\) nous donne
	\begin{subequations}
		\begin{align}
			P\big( | \bar X_n-E(\bar X_n) |  >\eta \big) & \leq\frac{1}{ \eta^2 }E\big( | \bar X_n-E(\bar X_n) |^2 \big) \\
			                                             & =\frac{1}{ \eta^2 } \Var(\bar X_n)                            \\
			                                             & = \frac{1}{ n\eta^2 }\Var(X_1)
		\end{align}
	\end{subequations}
	où nous avons utilisé la la proposition~\ref{LemEXYEXEYindep} : \( \Var(\bar X_n)=\Var(X_1)/n\). Au final nous avons prouvé que
	\begin{equation}
		P\big( | \bar X_n-E(\bar X_n) |>\eta\big)\leq\frac{1}{ n\eta^2 }\Var(X_1),
	\end{equation}
	qui tend vers zéro lorsque \( n\to\infty\).
\end{proof}

\begin{proposition}
	Soient \( X_n\) des variables aléatoires indépendantes et identiquement distribuées avec \( X_n\geq 0\). Nous acceptons \( E(X_1)=\infty\), c'est-à-dire que nous relaxons la condition \( X_n\in L^1\) par rapport à la loi des grands nombres.

	Alors
	\begin{equation}
		\bar X_n\stackrel{p.s.}{\longrightarrow} E(X_1)\in\mathopen[ 0 , \infty \mathclose].
	\end{equation}
\end{proposition}

\begin{proof}
	Si \( E(X_1)<\infty\), nous sommes dans le cas de la loi des grands nombres. Pour chaque \( N\in\eN\) nous considérons la suite de variables aléatoires
	\begin{equation}
		X_n^{(N)}=\min(X_n,N).
	\end{equation}
	Nous avons évidemment \( \bar X^{(N)}_n\leq \bar X_n\). Les variables aléatoires \( X^{(N)}_n\) étant bornées par \( N\), elles vérifient la loi des grands nombres pour chaque \( N\) séparément. Par conséquent nous avons pour chaque \( N\) la limite
	\begin{equation}        \label{EqbarXNbtoXnubus}
		\bar X^{(N)}_n\to E(X^{(N)}_1)
	\end{equation}
	Nous supposons que \( E(X_1)=\infty\), par conséquent  \( \lim_{N\to\infty}E(X_1^{(N)})=\infty\). Soit \( \eta>0\) et choisissons \( N\) de telle manière à avoir
	\begin{equation}
		E(X_1^{(N)})>\eta+1.
	\end{equation}
	La limite \eqref{EqbarXNbtoXnubus} nous permet de trouver \( n_0\) tel que pour tout \( n>n_0\) nous ayons \( \bar X^{(N)}_n>\eta\). Au final,
	\begin{equation}
		\eta<\bar X^{(N)}_n\leq \bar X_n,
	\end{equation}
	ce qui montre que \( \bar X_n\to\infty\).
\end{proof}

\begin{example}
	La loi des grands nombres justifie la pratique courante d'approximer une grandeur physique par la moyenne empirique d'un grand nombre de mesures.
\end{example}

%TODO : mettre cette référence vers wikisource en bibliographie.
\begin{example}
	Citons ici le dernier paragraphe de \emph{Le mystère de Marie Roget} par Edgar Allan Poe, traduit par Charles Baudelaire\footnote{Disponible sur \url{https://fr.wikisource.org/wiki/Le_Mystère_de_Marie_Roget}}.

	\begin{quote}
		Rien, par exemple, n’est plus difficile que de convaincre le lecteur non spécialiste que, si un joueur de dés a amené les six deux fois coup sur coup, ce fait est une raison suffisante de parier gros que le troisième coup ne ramènera pas les six. Une opinion de ce genre est généralement rejetée tout d’abord par l’intelligence. On ne comprend pas comment les deux coups déjà joués, et qui sont maintenant complètement enfouis dans le Passé, peuvent avoir de l’influence sur le coup qui n’existe que dans le Futur. La chance pour amener les six semble être précisément ce qu’elle était à n’importe quel moment, c’est-à-dire soumise seulement à l’influence de tous les coups divers que peuvent amener les dés. Et c’est là une réflexion qui semble si parfaitement évidente, que tout effort pour la controverser est plus souvent accueilli par un sourire moqueur que par une condescendance attentive.
	\end{quote}
	Dans le cours de la nouvelle, Edgar Poe cite et utilise la théorie des probabilités avec une justesse inaccoutumée dans la littérature. Mais dans ce paragraphe final, Poe montre de façon la plus formelle qu'il n'a \emph{rien} compris à la loi des grands nombres.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème central limite}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{Lemexpznznsurnton}
	Soit \( z_n\to z\) une suite convergente dans \( \eC\). Alors
	\begin{equation}
		\left( 1+\frac{ z_n }{ n } \right)^n\to e^z.
	\end{equation}
	%TODOooLKKUooZdWHcf. Prouver ça. 
	% Mais c'est probablement déjà fait quelque part.
\end{lemma}

\begin{theorem}     \label{ThoOWodAi}
	Si les variables aléatoires \( X_n\) sont
	\begin{enumerate}
		\item
		      indépendantes et identiquement distribuées de loi parente \( X\),
		\item
		      \( X_1\in L^2(\Omega,\tribA)\),
	\end{enumerate}
	alors nous notons \( \bar X_n=\frac{1}{ n }\sum_{i=1}^{n}X_i\), \( m=E(X_1)\) et \( \sigma^2=\Var(X_1)\) et nous avons
	\begin{equation}
		\frac{ \bar X_n-E(X) }{ \sqrt{\Var(\bar X_n)} }=\frac{ \bar X_n-m }{ \sigma/\sqrt{n} }\stackrel{\hL}{\longrightarrow}\dN(0,1).
	\end{equation}
\end{theorem}
\index{théorème!central limite}

\begin{proof}
	Nous allons écrire la démonstration dans le cas de variables aléatoires réelles. La proposition~\ref{PrpopCaractCvL} dit que la suite \( X_n\) converge en loi vers \( X\) si et seulement si les fonctions caractéristiques convergent ponctuellement. Nous devons donc prouver, pour chaque\footnote{Chuck Norris peut \emph{vraiment} le faire pour \emph{chaque} \( t\in\eR\)} \( t\in\eR\), que
	\begin{equation}        \label{EqPhitophidNtznu}
		\Phi_{\frac{ S_n-nm }{ \sigma\sqrt{n} }}(t)\to\Phi_{\dN(0,1)}(t).
	\end{equation}
	Supposons dans un premier temps que \( E(X_i)=0\) et \( \sigma(X_i)=1\). Dans ce cas nous considérons la fonction
	\begin{subequations}
		\begin{align}
			\Phi_{\frac{ S_n }{ \sqrt{n} }} & =E\big(  e^{i\frac{ t }{ \sqrt{n} }\sum_{k=1}^nX_k} \big)     \\
			                                & =\prod_{k=1}^nE\big(  e^{i\frac{ t }{ \sqrt{n} }X_1} \big)    \\
			                                & =\prod_{k=1}^n\Phi_{X_1}\left( \frac{ t }{ \sqrt{n} } \right) \\
			                                & =\Phi_{X_1}\left( \frac{ t }{ \sqrt{n} } \right)^n.
		\end{align}
	\end{subequations}
	Cette quantité est à priori complexe; nous ne pouvons donc pas immédiatement passer au logarithme. Nous pouvons par contre utiliser un développement en puissances de \( t\) en nous servant de la proposition~\ref{PropDerFnCaract} et de l'hypothèse comme quoi \( X_1\in L^2\) :
	\begin{equation}
		\Phi_{X_1}(t)=\Phi_{X_1}(0)+\Phi_{X_1}'(0)t+\Phi_{X_1}''(0)\frac{ t^2 }{2}+\alpha(t)t^2
	\end{equation}
	où \( \alpha\) est une fonction qui a la propriété \( \lim_{x\to 0} \alpha(x)=0\).

	En utilisant les hypothèses et la formule de dérivation de la fonction caractéristique,
	\begin{subequations}
		\begin{align}
			\Phi_{X_1}(0)   & =1                      \\
			\Phi'_{X_1}(0)  & =E\big( (iX) \big)=0    \\
			\Phi''_{X_1}(0) & =E(-X^2)=-\Var(X_1)=-1.
		\end{align}
	\end{subequations}
	Nous avons donc
	\begin{equation}
		\Phi_{X_1}\left( \frac{ t }{ \sqrt{n} } \right)=\underbrace{1-\frac{ 1 }{2}\frac{ t^2 }{ n }}_{\in\eR}+\underbrace{\frac{ t^2 }{ n }\alpha\left( \frac{ t }{ \sqrt{n} } \right)}_{\in\eC},
	\end{equation}
	de telle sorte que, en considérant une valeur fixée de \( t\),
	\begin{equation}        \label{EqPhifracfacbetanrigh}
		\Phi_{\frac{ S_n }{ \sqrt{n} }}(t)=\left( 1-\frac{ \frac{ t^2 }{ 2 }+\beta_n }{ n } \right)^n
	\end{equation}
	où \( \beta_n=t^2\alpha(t/\sqrt{n})\). Nous avons bien entendu \( \lim_{n\to \infty} \beta_n=0\).

	Nous pouvons appliquer le lemme~\ref{Lemexpznznsurnton} pour obtenir la limite
	\begin{equation}        \label{EqlimninfySnsqrtntdsnd}
		\lim_{n\to \infty} \Phi_{\frac{ S_n }{ \sqrt{n} }}(t)= e^{-t^2/2}.
	\end{equation}
	La convergence \eqref{EqPhitophidNtznu} est par conséquent prouvée dans le cas où \( E(X_i)=0\) et \( \Var(X_i)=1\).

	Considérons maintenant des variables aléatoires avec \( E(X_i)=m\) et \( \Var(X_i)=\sigma^2\). Elles peuvent être écrites sous la forme
	\begin{equation}
		X_i=\sigma X'_i+m
	\end{equation}
	où \( X'_i\) est d'espérance nulle et de variance un. Nous avons alors
	\begin{equation}
		S_n=\sigma\sum_{i=1}^nX_i'+nm,
	\end{equation}
	et
	\begin{equation}
		\frac{ S_n-nm }{ \sigma\sqrt{n} }=\frac{ S'_n }{ \sqrt{n} }
	\end{equation}
	où \( S'_n=\sum_iX'_i\). L'étude de la variable aléatoire
	\begin{equation}
		\frac{ S_n-nm }{ \sigma\sqrt{n} }
	\end{equation}
	revient donc à celle de \( S'_n/\sqrt{n}\) qui vient d'être effectuée.
\end{proof}

\begin{normaltext}
	À propos du théorème central limite~\ref{ThoOWodAi}. Si pour une certaine variable aléatoire \( X\) on a \( E(X)=m\), alors nous n'avons pas forcément \( P(X=m+a)=P(X=m-a)\). Est-ce que le théorème central limite permet cependant d'affirmer que dans un certaine mesure nous avons
	\begin{equation}        \label{EQooNAGLooKYYWpY}
		P(\bar X_n=m+a)=P(\bar X_n=m-a)
	\end{equation}
	lorsque \( n\) est grand ?

	Tel quelle, l'équation \eqref{EQooNAGLooKYYWpY} est en général fausse pour chaque \( n\) parce qu'il existe des distributions non symétriques. Mais bien entendu les deux membres tendent vers zéro pour \( n\to \infty\). Mais cela n'est pas lié à la symétrie de la distribution gaussienne. C'est seulement le fait que la gaussienne n'a pas de masses ponctuelles.

	Par contre, il y a effectivement une assurance de symétrie pour \( \bar X_n\) lorsque \( n\to \infty\). Le fait est que \( X_n\stackrel{\hL}{\longrightarrow}X\) où \( X\) est une gaussienne. La fonction de répartition de \( X\) est continue partout et la proposition~\ref{PropXncvXFXcvFxt} nous dit que pour tout \( x\in \eR\),
	\begin{equation}
		\lim_{n\to \infty} F_{X_n}(x)=F_X(t).
	\end{equation}
	Vu que le nombre \( P\big( \bar X_n\in B(m+a,\delta) \big)\) peut être exprimé avec des sommes et différences \( F_{X_n}\), nous avons
	\begin{equation}
		\lim_{n\to \infty} P\big( \bar X_n\in B(m+a,\delta) \big)=P\big( X\in B(m+a,\delta) \big).
	\end{equation}
	Par symétrie de la gaussienne le membre de droite est égal à \( P\big( X\in B(m-a,\delta) \big)\) et nous avons bien
	\begin{equation}
		\lim_{n\to \infty} P\big( \bar X_n\in B(m+a,\delta) \big)= \lim_{n\to \infty} P\big( \bar X_n\in B(m-a,\delta) \big).
	\end{equation}
\end{normaltext}

\begin{remark}  \label{RemRHFDooGbaPYu}

	Le théorème central limite s'applique quelle que soit la distribution des variables aléatoires \( X_i\) (dans les limites de hypothèses); en particulier il ne dit rien sur la moyenne des \( X_i\). Il dit seulement que l'écart de la moyenne «mesurée»  à la moyenne «théorique» est une variable aléatoire gaussienne si on a mesuré assez de fois.

	Autrement dit, si la durée d'attente à la poste est de \( 5\) minutes, et si j'y vais \( 2000\) fois, alors la probabilité que ma moyenne d'attente soit de \( 4\) minutes est la même que la probabilité qu'elle soit de \( 6\) minutes\footnote{Et en l'occurrence, cette probabilité est nulle parce qu'on est en train de parler de variable aléatoire continue, mais vous voyez l'idée.}.

\end{remark}

\begin{probleme}
	La remarque~\ref{RemRHFDooGbaPYu} est une interprétation personnelle. J'aimerais avoir l'avis de quelqu'un de plus compétent.
\end{probleme}


\begin{remark}
	%TODO : pour ceci, il faudra parler quelque part de la détermination du logarithme dans le plan complexe.
	Nous pouvons obtenir la limite \eqref{EqlimninfySnsqrtntdsnd} d'une façon alternative. Nous considérons la détermination du logarithme complexe sur \( \eC\setminus\eR^-\); cela est une fonction analytique (théorème~\ref{THOooWUXOooYKvLbJ}) vérifiant l'équation
	\begin{equation}
		e^{\ln(z)}=z
	\end{equation}
	pour tout \( z\in\eC\setminus\eR^-\) et le développement
	\begin{equation}
		\ln(1+z)=\sum_{n=1}^{\infty}(-1)^{n+1}\frac{ z^n }{ n }.
	\end{equation}
	En particulier, \( \ln(1+z)=z+z\alpha(z)\) où \( \lim_{z\to 0}\alpha(z)=0\). Nous reprenons à l'équation \eqref{EqPhifracfacbetanrigh} en fixant \( t\). Nous avons
	\begin{subequations}
		\begin{align}
			\Phi_{\frac{ S_n }{ \sqrt{n} }}(t) & =\exp\left[ \ln\big(\Phi_{\frac{ S_n }{ \sqrt{n} }}(t)\big) \right]                                                                            \\
			                                   & =\exp\left[ n\ln\left( 1+\frac{ -\frac{ t^2 }{2}-\beta_n }{ n } \right) \right]                                                                \\
			                                   & =\exp\left[ -\frac{ t^2 }{2}-\beta_n+ \big( -\frac{ t^2 }{2}-\beta_n \big)\alpha\left( \frac{ -\frac{ t^2 }{2}-\beta_n }{ n } \right) \right].
		\end{align}
	\end{subequations}
	À la limite \( n\to 0\) nous tombons sur \(  e^{-t^2/2}\).

\end{remark}

\begin{remark}
	Étant donné que la variable aléatoire
	\begin{equation}
		\frac{ S_n-nm }{ \sigma\sqrt{n} }
	\end{equation}
	converge en loi vers \( \dN(0,1)\), nous avons la convergence des fonctions de répartition partout où la fonction de répartition de la normale est continue\footnote{Proposition~\ref{PropXncvXFXcvFxt}.} (donc sur tout \( \eR\)). En particulier,
	\begin{equation}
		\left| P\left( \frac{ S_n-nm }{ \sigma\sqrt{n} }\leq x \right)-\int_{-\infty}^x\frac{1}{ \sqrt{2\pi} } e^{-y^2/2}dy \right| \to 0.
	\end{equation}
	Nous avons la borne de \defe{Berry-Esséen}{Berry-Esséen (borne)} qui donne une estimation de la vitesse de convergence: si \( X\in L^3\), alors  il existe une constante \( C\), indépendante de \( x\), des \( X_i\) et de \( n\) telle que
	\begin{equation}
		\left| P\left( \frac{ S_n-nm }{ \sigma\sqrt{n} }\leq x \right)-\int_{-\infty}^x\frac{1}{ \sqrt{2\pi} } e^{-y^2/2}dy \right| \leq\frac{ C\mu_3 }{ \sigma^3\sqrt{n} }
	\end{equation}
	où \( \mu_3=E\big( | X_1-m |^3 \big)\) est le moment d'ordre \( 3\) de \( X\). La chose à retenir est que la convergence est à la vitesse de \( 1/\sqrt{n}\).
\end{remark}

En dimension \( d>1\), nous avons encore un théorème central limite.
\begin{theorem}		\label{THOooADSIooFrWawC}
	Si \( d>1\), et si nous avons des variables aléatoires \( X_n\) à valeurs dans \( \eR^d\) avec
	\begin{enumerate}
		\item
		      les \( X_n\) sont indépendantes et identiquement distribuées
		\item
		      les \( X_n\) sont dans \( L^2\).
	\end{enumerate}
	Alors nous notons \( X_1=(X_1^{(1)},\ldots,X_1^{(d)})\). Nous avons
	\begin{equation}
		\frac{ S_n-nm }{ \sqrt{n} }\stackrel{\hL}{\longrightarrow}\dN(0,\Sigma)
	\end{equation}
	où \( \Sigma\) est ma matrice de covariance du vecteur aléatoire \( X_1\) :
	\begin{equation}
		\Sigma=\big( \Cov(X_1^{(1)},\ldots,X_1^{(d)}) \big)_{i,j=1,\ldots,d}.
	\end{equation}
\end{theorem}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Marche aléatoire}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons un mobile qui se déplace sur l'axe \( \eZ\). À chaque pas de temps, nous supposons qu'il va faire un pas à gauche avec une probabilité \( p\) et un pas à droite avec une probabilité \( (1-p)\). Nous nous demandons quel est le mouvement du mobile sur le long terme.

La position \( S_n\) du mobile à l'instant \( n\) est donnée par
\begin{equation}
	S_n=\sum_{i=1}^nX_i
\end{equation}
où \( X_i\) est le pas effectué à l'instant \( i\). Ce sont des variables de Bernoulli indépendantes avec
\begin{equation}
	X_i\stackrel{\hL}{=}p\delta_{-1}+(1-p)\delta_1
\end{equation}
c'est-à-dire
\begin{subequations}
	\begin{align}
		P(X_i=-1) & =p    \\
		P(X_i=1)  & =1-p.
	\end{align}
\end{subequations}
Ces variables vérifient les hypothèses de la loi des grands nombres :
\begin{enumerate}
	\item
	      elles sont indépendantes et identiquement distribuées,
	\item
	      elles sont intégrables.
\end{enumerate}
Pour le second point, le calcul est
\begin{equation}
	\int_{\Omega}| X_i |dP=\int_{\eR}| x |dP_X=\int_{\eR}| x |(p\delta_{-1}+(1-p)\delta_{1})=| 1-p |+| p |=1.
\end{equation}
Nous avons par conséquent
\begin{equation}
	\bar X_n=\frac{1}{ n }\sum_{i=1}^nX_i\stackrel{p.s.}{\longrightarrow} E(X_1)=(1-2p)
\end{equation}
et
\begin{equation}
	\frac{ S_n }{ n }\to(1-2p).
\end{equation}
Si \( p\neq 1/2\) nous pouvons conclure que
\begin{enumerate}
	\item
	      si \( p>1/2\), alors \( S_n\stackrel{p.s.}{\longrightarrow}-\infty\)
	\item
	      si \( p<1/2\), alors \( S_n\stackrel{p.s.}{\longrightarrow}\infty\).
\end{enumerate}
De plus nous connaissons la vitesse de divergence : elle est linéaire. Le mobile suit essentiellement l'équation
\( p(n)=(1-2p)n\).

\begin{remark}
	Cela ne traite pas le cas \( p=1/2\). Dans ce cas, nous pouvons simplement dire que \( S_n=o(n)\).
\end{remark}
