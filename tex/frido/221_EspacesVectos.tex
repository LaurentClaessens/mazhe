% This is part of Mes notes de mathématique
% Copyright (c) 2008-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les matrices et les applications linéaires sont deux choses différentes. Une application linéaire\footnote{Définition \ref{DEFooULVAooXJuRmr}.} est une application d'un espace vectoriel vers un autre, et une matrice est un simple tableau de nombres sur lesquels nous définissons des opérations, de telle sorte à fournir une structure d'espace vectoriel. Le lien entre ces opérations et les opérations correspondantes sur les applications linéaires sera fait plus tard. Voir la définition \ref{DEFooJVOAooUgGKme} et ce qui s'en suit.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

Les notions topologiques sur les espaces de matrices sont pour plus tard, à commencer par la définition \ref{DEFooCQHDooYpUAhG}.

\begin{definition}
	Soit un anneau \( \eA\) ainsi que des entiers \( m\), \( n\) strictement positifs. L'ensemble \( \eM(n\times m,\eA)\) est l'ensemble des applications
	\begin{equation}
		\{ 1,\ldots, n \}\times \{ 1,\ldots, m \}\to \eA,
	\end{equation}
	et est appelé ensemble des \defe{matrices}{matrice} \(n\times m\) sur \( \eA \).
\end{definition}
Si \( A\) est une matrice, nous notons \( A_{i,j}\) au lieu de \( A(i,j)\) l'image de \( (i,j)\) par l'application \( A\).


\begin{definition}
	Quelques ensembles de matrices particuliers.
	\begin{enumerate}
		\item Si \( n=m\), alors:
		      \begin{itemize}
			      \item nous disons que la matrice est \defe{carrée}{carrée!matrice},
			      \item nous notons \( \eM(n,\eA)\) pour \( \eM(n\times n,\eA)\),
			      \item \( n \) est appelée \defe{ordre}{ordre!d'une matrice carrée} de la matrice.
		      \end{itemize}
		\item Si \( n = 1 \), alors la matrice est appelée \defe{matrice-ligne}{matrice-ligne}.
		\item Si \( m = 1 \), alors la matrice est appelée \defe{matrice-colonne}{matrice-colonne}.
	\end{enumerate}
\end{definition}

\begin{normaltext}
	On note les isomorphismes naturels \( \eM(1\times m,\eA) \simeq \eA^m\) et \( \eM(n\times 1,\eA) \simeq \eA^n\).
\end{normaltext}

\begin{lemmaDef}        \label{LEMooYWTEooQyLxKv}
	Nous considérons les opérations suivantes sur \( \eM(n\times m, \eA)\) :
	\begin{description}
		\item[Somme] \( (A+B)_{i,j}=A_{i,j}+B_{i,j}\),
		\item[Produit par un scalaire] \( (\lambda \cdot A)_{i,j}=\lambda A_{i,j}\) pour tout \( A,B\in \eM(n\times m,\eA ) \) et \( \lambda\in \eA \).
	\end{description}
	Alors \( \big( \eM(n\times m, \eA), +,\cdot \big)\) est un \( \eA\)-module\footnote{Définition \ref{DEFooHXITooBFvzrR}}.
\end{lemmaDef}

\begin{proof}
	Plusieurs parties. Il faut vérifier les conditions de la définition \ref{DEFooHXITooBFvzrR}.
	\begin{subproof}
		\spitem[Groupe]
		%-----------------------------------------------------------
		Le neutre est la matrice nulle \( N_{ij}=0\). Nous avons \( ((A+B)+C)_{ij}=(A+B)_{ij}+C_{ij}=A_{ij}+B_{ij}+C_{ij}\). Et l'inverse de \( A\) est \( B_{ij}=-A_{ij}\).
		\spitem[Autres propriétés]
		%-----------------------------------------------------------
		Nous n'allons pas toutes les faire. Par exemple
		\begin{subequations}
			\begin{align}
				\lambda\big( \lambda(A+B) \big)_{ij} & =\lambda(A+B)_{ij}                  \\
				                                     & =\lambda(A_{ij}+B_{ij})             \\
				                                     & =\lambda A_{ij}+\lambda B_{ij}      \\
				                                     & =(\lambda A)_{ij}+(\lambda B)_{ij}.
			\end{align}
		\end{subequations}
		Notez l'utilisation de \( \lambda(x+y)=\lambda x+\lambda y\) lorsque \( x,y\in \eA\).
	\end{subproof}
\end{proof}

\begin{lemmaDef}        \label{LEMooMBZTooKdGvON}
	Avec la multiplication
	\begin{equation}
		\begin{aligned}
			\eM(n\times p,\eA)\times \eM(p\times m,\eA) & \to \eM(n\times m,\eA)                         \\
			(A,B)                                       & \mapsto (AB)_{i,j}=\sum_{k=1}^pA_{i,k}B_{k,j},
		\end{aligned}
	\end{equation}
	l'espace \( \eM(n,\eK)\) est une \( \eK\)-algèbre\footnote{Définition \ref{DefAEbnJqI}.}.
\end{lemmaDef}

\begin{proof}
	Il faut vérifier les trois formules de la définition \ref{DefAEbnJqI}.
	\begin{subproof}
		\spitem[Distribution (1)]
		%-----------------------------------------------------------
		Nous avons
		\begin{equation}
			\big( (x+y)\times z \big)_{ij}=\sum_k(x+y)_{ik}z_k=\sum_k\big( x_{ik}z_kj+y_{ik}z_{kj} \big)=(xz)_{ij}+(yz)_{ij}.
		\end{equation}

		\spitem[Distribution (2)]
		%-----------------------------------------------------------
		Même calcul.
		\spitem[Multiplication par un scalaire]
		%-----------------------------------------------------------
		Encore le même genre de calculs, en partant de
		\begin{equation}
			\big( (\alpha x)\times (\beta y) \big)_{ij}=\sum_k(\alpha x)_{ik}(\beta y)_{kj}.
		\end{equation}
	\end{subproof}
\end{proof}

\begin{definition}
	Pour un élément \( A\in \eM(n\times m, \eA)\) nous définissons encore
	\begin{description}
		\item[La transposée] \( A^t_{i,j}=A_{j,i}\),
		\item[La trace] \( \tr(A)=\sum_iA_{i,i}\).
	\end{description}
\end{definition}


\begin{remark}
	Quelques remarques directes sur les définitions.
	\begin{enumerate}
		\item
		      La motivation de cette définition pour le produit apparaîtra plus loin, mais le Frido n'étant pas un livre d'introduction, j'imagine que le lecteur a déjà une idée.
		\item
		      Nous verrons plus loin en \ref{SUBSECooGPXVooEYwIiJ} que la définition de transposée d'une application linéaire n'est pas tout à fait évidente; elle sera la définition \ref{DefooZLPAooKTITdd}.

		      Ici nous avons bien défini la transposée d'une matrice, pas d'une application linéaire.
	\end{enumerate}
\end{remark}

\begin{remark}
	Quelques remarques à propos de structures supplémentaires.
	\begin{enumerate}
		\item Nous utiliserons (presque) tout le temps des matrices à coefficients dans un corps. Il est clair que, si \( \eK \) est un corps (commutatif), alors \( \eM(n\times m,\eK) \) a une structure d'espace vectoriel sur \( \eK \).
		\item Par ailleurs, sur les matrices carrées d'ordre \( n \) fixé, le produit de deux matrices est bien défini. Ainsi, \( \eM(n,\eA)\) se voit conférer une structure d'anneau, dont le neutre pour la multiplication est la matrice carrée \( \mtu_n\) (notée aussi \( \mtu\) lorsqu'il n'y a pas d'ambiguïté sur la taille), donnée par
		      \begin{equation}
			      \mtu_{i,j}=\begin{cases}
				      1 & \text{si } i=j \\
				      0 & \text{sinon.}
			      \end{cases}
		      \end{equation}
		      Il est vite vu que si \( A\) est une matrice carrée d'ordre \( n \), alors \( A\mtu=\mtu A=A\).
	\end{enumerate}
\end{remark}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooUXDRooWZbMVN}
	Si \( A\), \( B\) et \( C\) sont des matrices, nous avons
	\begin{enumerate}
		\item
		      \( (AB)^t=B^tA^t\),
		\item       \label{ITEMooXDYQooAlnArd}
		      \( \tr(ABC)=\tr(CAB)\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	La première est un simple calcul :
	\begin{equation}
		(AB)^t_{i,j}=(AB)_{j,i}=\sum_kA_{j,k}B_{k,i}=\sum_kA^t_{k,j}B^t_{i,k}=(B^tA^t)_{i,j}.
	\end{equation}
	Pour la seconde :
	\begin{equation}
		\tr(ABC)=\sum_{ikl}A_{i,k}B_{k,l}C_{l,i}=\sum_{ikl}C_{l,i}A_{i,k}B_{k,l}=\sum_l(CAB)_{l,l}=\tr(CAB).
	\end{equation}
\end{proof}

\begin{normaltext}
	La seconde égalité est importante et est nommée \defe{invariance cyclique}{invariance cyclique!trace} de la trace. Elle sert, entre autres nombreuses choses, à prouver que la trace d'une matrice d'une application linéaire ne dépend pas de la base choisie. Ce sera la proposition \ref{PROPooRMYQooWkEpJJ}.
\end{normaltext}

\begin{lemma}       \label{LEMooLXAHooPRyHaF}
	Soient des matrices \( A,B\in \eM(n,\eK)\). Si pour tout \( x,y\in \eK^n\) nous avons
	\begin{equation}
		\sum_{ij}A_{i,j}x_iy_j=\sum_{ij}B_{i,j}x_iy_j
	\end{equation}
	alors \( A=B\).
\end{lemma}

\begin{proof}
	Il suffit de choisir \( x_i=\delta_{i,k}\) et \( y_j=\delta_{j,l}\) et d'effectuer les sommes; par exemple
	\begin{equation}
		\sum_{ij}A_{i,j}\delta_{i,k}y_j=\sum_jA_{k,j}y_j.
	\end{equation}
	Après avoir effectué toutes les sommes, nous nous retrouvons avec \( A_{k,l}=B_{k,l}\), ce qui signifie \( A=B\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Identifier matrices et applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Voir dans l'index thématique, \ref{SUBSECooAFPDooOzXdGz}.

Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases\footnote{C'est le théorème~\ref{ThonmnWKs} qui nous permet de considérer des bases. Et ce théorème ne fonctionne que parce que nous avons supposé une dimension finie.} \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\).

\begin{definition}      \label{DEFooJVOAooUgGKme}
	Nous considérons l'application
	\begin{equation}        \label{EQooVZQWooMyFFeO}
		\begin{aligned}
			\psi\colon \eM(n\times m, \eK) & \to \aL(E,F) \\
			A                              & \mapsto f_A
		\end{aligned}
	\end{equation}
	où \( f_A\) est définie par
	\begin{equation}        \label{EQooBVGHooJhFbMs}
		f_A(x)=\sum_{i\alpha}A_{\alpha, i}x_if_{\alpha}
	\end{equation}
	si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).
\end{definition}

\begin{normaltext}
	Nous allons prouver un certain nombre de résultats montrant que cette application a toutes les propriétés imaginables permettant d'identifier les matrices aux applications linéaires : elle est un isomorphisme pour toutes les structures que vous pouvez raisonnablement imaginer.

	À cette application \( \psi\) il manque cependant une propriété importante : elle n'est pas canonique. Elle dépend des bases choisies. Autrement dit : nous avons à priori autant d'applications \( \psi\) différentes qu'il y a de choix de bases sur \( E\) et \( F\)\quext{Bonne question. Est-ce qu'il y a moyen de construire deux choix de bases donnant la même application \( \psi\) ? Écrivez-moi si vous savez la réponse.}.

	Nous allons prouver maintenant quelques résultats montrant que les matrices et les applications linéaires, dans le cas des espaces vectoriels \( \eK^n\) sont deux présentations de la même chose.

	Le fait que \( \psi\) est continue sera la proposition \ref{PROPooXEQLooHvzVVm}.
\end{normaltext}

\begin{normaltext}
	Lorsque \( A\in \eM(n,\eK)\) est une matrice et \( x\in \eK^n\) un vecteur, nous notons \( Ax\) l'élément de \( \eK^n\) donné par
	\begin{equation}        \label{EQooQFVTooMFfzol}
		(Ax)_i=\sum_jA_{i,j}x_j.
	\end{equation}
	Autrement dit, \( Ax=f_A(x)\).

	Cette convention et de nombreuses autres à propos de matrice sera rappelée dans \ref{SECooBTTTooZZABWA}.
\end{normaltext}

\begin{propositionDef}      \label{PROPooGXDBooHfKRrv}
	Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\).

	Nous considérons l'application
	\begin{equation}
		\begin{aligned}
			\psi\colon \eM(n\times m, \eK) & \to \aL(E,F) \\
			A                              & \mapsto f_A
		\end{aligned}
	\end{equation}
	où \( f_A\) est définie par
	\begin{equation}        \label{EQooZKEKooNYjvhP}
		f_A(x)=\sum_{i\alpha}A_{\alpha, i}x_if_{\alpha}
	\end{equation}
	si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).

	Alors
	\begin{enumerate}
		\item       \label{ITEMooKZYYooZPTkpq}
		      Nous avons
		      \begin{equation}
			      f_A(e_i)_{\alpha}=A_{\alpha, i}.
		      \end{equation}
		\item       \label{ITEMooANXFooGIuxUR}
		      Nous avons
		      \begin{equation}          \label{EQooOKOJooYgteNP}
			      f_A(e_i)=\sum_{\alpha}A_{\alpha, i}f_{\alpha}.
		      \end{equation}
		\item       \label{ITEMooXLLLooKfigfB}
		      Nous avons
		      \begin{equation}          \label{EQooAXRJooUwHbjB}
			      \big( f_A(x) \big)_{\alpha}=\sum_{i}A_{\alpha, i}x_i.
		      \end{equation}
		\item       \label{ITEMooHSMLooRJZref}
		      L'application \( \psi\) est une bijection.
	\end{enumerate}
	Si \( f\) est une application linéaire, alors la matrice \( \psi^{-1}(f)\) est la \defe{matrice associée}{matrice!d'une application linéaire} à \( f\) dans les bases choisies.
\end{propositionDef}

Remarque : les bases ne sont supposées être canoniques en aucun sens du terme. Les dimensions de \( E\) et \( F\) ne sont pas non plus supposées identiques.

\begin{proof}
	En nous rappelant que \( (e_j)_i=\delta_{i,j}\) nous avons
	\begin{equation}        \label{EQooWGZHooIBoygB}
		f_A(e_j)=\sum_{i\alpha}A_{\alpha, i}(e_j)_if_{\alpha}=\sum_{\alpha}A_{\alpha, j}f_{\alpha},
	\end{equation}
	donc \( f_A(e_i)_{\alpha}=A_{\alpha, i}\). Cela prouve la formule du point \ref{ITEMooKZYYooZPTkpq}.

	Le point \ref{ITEMooANXFooGIuxUR} est une simple somme sur \( \alpha\) de \ref{ITEMooKZYYooZPTkpq}.

	La formule du point \ref{ITEMooXLLLooKfigfB} est simplement la composante \( f_{\alpha}\) de la définition \ref{EQooZKEKooNYjvhP}.

	Prouvons que \( \psi\) est injective. Si \( f_A=f_B\), nous avons en particulier \( f_A(e_i)_{\alpha}=f_B(e_i)_{\alpha}\) et donc \( A_{\alpha, i}=B_{\alpha, i}\).

	Prouvons que \( \psi\) est surjective. Pour cela nous considérons \( f\in \aL(E,F)\) et nous posons \( A_{\alpha, i}=f(e_i)_{\alpha}\). Nous avons alors \( f=f_A\) parce que
	\begin{equation}
		f_A(x)=\sum_{i\alpha}A_{\alpha, i}x_if_{\alpha}=\sum_{i\alpha}f(e_i)_{\alpha}x_if_{\alpha}=\sum_{\alpha}f(\sum_ix_ie_i)_{\alpha}f_{\alpha}=\sum_{\alpha}f(x)_{\alpha}f_{\alpha}=f(x).
	\end{equation}
\end{proof}

La proposition suivante montre que le produit matriciel correspond à la composition d'applications linéaires, pourvu que l'on travaille avec les bases canoniques sur \( \eK^n\).
\begin{proposition}[\cite{MonCerveau}]      \label{PROPooIYVQooOiuRhX}
	Soit un corps commutatif \( \eK\). Nous considérons des espaces vectoriels \( E\) et \( F\) munis de bases \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_{\alpha}\}_{\alpha=1,\ldots, m} \).

	L'application déjà définie\footnote{Notez la position du \( n\) et du \( m\). Sachez noter les bornes des sommes écrites dans la démonstration.}
	\begin{equation}
		\psi\colon \eM(m\times n,\eK)\to \aL(E,F)
	\end{equation}
	est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
	Le fait que \( \psi\) soit une bijection est la proposition \ref{PROPooGXDBooHfKRrv}. Nous devons montrer que c'est une application linéaire.

	Pour \( \lambda\in \eK\) nous avons le calcul
	\begin{equation}
		\psi(\lambda A)(e_k)=f_{\lambda A}(e_k)=\sum_{\alpha i}(\lambda A)_{\alpha, i}\underbrace{(e_k)_i}_{=\delta_{k,i}}f_{\alpha}=\lambda\sum_{\alpha}A_{\alpha, k}f_{\alpha}=\lambda f_A(e_k).
	\end{equation}
	Donc \( \psi(\lambda A)=\lambda\psi(A)\).

	Si \( A,B\in \eM(n,\eK)\) nous avons de la même façon, \( f_{A+B}=f_A+f_B\).
\end{proof}

\begin{proposition}     \label{PROPooCSJNooEqcmFm}
	Soient des espaces vectoriels \( E\), \( F\) et \( G\) de dimensions \( n\), \( m\) et \( p\) munis de bases\footnote{Avec trois ensembles, nous renonçons à utiliser des alphabets différents pour numéroter les éléments des bases.} \( \{ e_i \}\), \( \{ f_i \}\) et \( \{ g_i \}\). Nous considérons les applications
	\begin{subequations}
		\begin{align}
			\psi & \colon \eM(m\times n,\eK)\to \aL(E,F)  \\
			\psi & \colon \eM(p\times m,\eK)\to \aL(F,G)  \\
			\psi & \colon \eM(p\times n,\eK)\to \aL(E,G).
		\end{align}
	\end{subequations}
	Nous avons
	\begin{equation}
		\psi(A)\circ \psi(B)=\psi(AB)
	\end{equation}
	pour toutes matrices \( A\in \eM(p\times m,\eK)\) et \( B\in \eM(m\times n,\eK)\).
\end{proposition}

\begin{proof}
	Nous considérons les applications linéaires associées à \( A\) et \( B\) : \( f_A\colon F\to G\) et \( f_B\colon E\to F\) et la composée \( f_A\circ f_B\colon E\to G\). Et puis c'est le calcul :
	\begin{subequations}
		\begin{align}
			(f_A\circ f_B)(e_k) & =f_A\big( \sum_{ij}B_{i,j}(e_k)_j f_i \big) \\
			                    & =\sum_i B_{i,k}f_A(f_i)                     \\
			                    & =\sum_iB_{i,k}\sum_{rs}A_{r,s}(f_i)_s g_r   \\
			                    & =\sum_{ir}B_{i,k}A_{r,i} g_r                \\
			                    & =\sum_r(AB)_{r,k} g_r                       \\
			                    & =f_{AB}(e_k).
		\end{align}
	\end{subequations}
	Donc \( f_A\circ f_B=f_{AB}\) comme il se doit.
\end{proof}

Nous pouvons particulariser au cas où \( E=F=G\).
\begin{proposition}     \label{PROPooFMBFooEVCLKA}
	Si \( E\) est un espace vectoriel muni d'une base \( \{ e_i \}\), alors l'application
	\begin{equation}
		\psi\colon \eM(n,\eK)\to \End(E)
	\end{equation}
	est un isomorphisme d'algèbre\footnote{Définition \ref{DefAEbnJqI}.} et d'anneaux\footnote{Définition \ref{DEFooSPHPooCwjzuz}}.
\end{proposition}

\begin{proof}
	Le fait que \( \psi\) soit un isomorphisme d'algèbre est juste la combinaison entre les propositions \ref{PROPooIYVQooOiuRhX} et \ref{PROPooCSJNooEqcmFm}.

	En ce qui concerne l'isomorphisme d'anneaux, il faut en plus identifier les neutres. Le neutre pour la composition d'applications linéaires est l'application identité et le neutre pour la multiplication de matrices est la matrice identité. Nous devons donc montrer que \( \psi(\delta)=f_{\delta}=\id\). Juste un calcul :
	\begin{equation}
		f_{\delta}(x)=\sum_{ij}\delta_{i,j}x_je_i=\sum_ix_ie_i=x.
	\end{equation}
	Donc oui, \( f_{\delta}\) est l'identité.
\end{proof}

Le fait que \( \psi\) est continue sera la proposition \ref{PROPooXEQLooHvzVVm}.


Voilà. Soyez bien conscient que l'application \( \psi\) dont nous avons beaucoup parlé est surtout intéressante dans le cas des espaces de la forme \( \eK^n\). Dans ce cas, nous avons une identification canonique entre \( \eM(n,\eK)\) et \( \End(\eK^n)\) qui est un isomorphisme d'anneaux et d'algèbres.

Nous verrons que ce \( \psi\) respecte encore les inverses\footnote{Proposition \ref{PROPooNPMCooPmaCwu}.} et les déterminants\footnote{Proposition \ref{PROPooFKDXooKMSolt}.}.

\begin{normaltext}
	Il convient de ne pas confondre matrice et application linéaire (bien que nous le ferons sans vergogne). Une matrice est un bête tableau de nombres, tandis qu'une application linéaire est une application entre deux espaces vectoriels vérifiant certaines propriétés.

	Cependant si les espaces vectoriels \( E\) et \( F\) sont munis de bases, alors il y a une application
	\begin{equation}
		\psi\colon \eM(m\times n,\eK)\to \aL(E,F)
	\end{equation}
	qui a toutes les propriétés imaginables\footnote{Et elle en aura encore plus lorsque nous aurons vu les déterminants.}.

	Cette application dépend des bases choisies. Il n'y a donc pas de trucs comme «la matrice de telle application linéaire» ou comme «voici une matrice, nous considérons l'application linéaire associée».

	Cependant, sur des espaces comme \( \eR^n\) ou plus généralement sur \( \eK^n\), nous avons une base canonique et toute personne raisonnable utilise toujours la base canonique (sauf mention du contraire). Dans ces cas il est sans danger de dire «la matrice associée à telle application linéaire» sans préciser les bases.

	Mais si un jour vous utilisez une base autre que la base canonique sur \( \eR^n\), précisez-le et plutôt deux fois qu'une\footnote{Au passage, non, les coordonnées polaires ne sont pas une base de \( \eR^2\). C'est un système de coordonnées, et ce n'est pas la même chose.}.
\end{normaltext}

\begin{normaltext}
	Tant que nous sommes à parler de matrice et d'applications linéaires, les plus acharnés anti-abus de language\footnote{Dont l'auteur de ces lignes fait partie.} remarqueront qu'il n'est pas vrai que «étant donné une base, une application linéaire a une matrice».

	En effet, une base est une partie libre et génératrice (définition \ref{DEFooNGDSooEDAwTh}). Or une partie d'un ensemble n'est pas muni d'un ordre. Toutes les permutations de colonnes de la matrice sont encore possible d'après l'ordre que l'on met sur les vecteurs de la base.

	Encore une fois, la base canonique n'a pas de problème parce que les \( \{ e_i \}\) de \( \eR^n\) viennent avec un ordre indiscutable. Plus généralement, très souvent, lorsqu'on construit une base, la construction suggère un ordre.
\end{normaltext}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ensemble des applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooRIFBooGOvsfb}
	Soient deux espaces vectoriels de dimension finie \( V\) et \( W\). Soit des bases \( \{ e_i \}_{i \in I}\) de \( V\) et \( \{ e'_j \}_{j\in J}\) de \( W\). Nous considérons les applications
	\begin{equation}	\label{EQooBMKQooCCypzP}
		\begin{aligned}
			f_{ij}\colon V & \to W            \\
			\sum_kx_ke_k   & \mapsto x_ie'_j.
		\end{aligned}
	\end{equation}
	Nous avons :
	\begin{enumerate}
		\item
		      Ces applications sont linéaires.
		\item		\label{ITEMooSEMLooXuxrpk}
		      Elles forment une base de \( \aL(V,W)\).
		\item
		      \( \dim(V,W)=\dim(V)\dim(W)\).
	\end{enumerate}
\end{proposition}

\ssdem

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooPNQNooAZpojm}
	Soient des bases \( \{ e_i \}\) et \( \{ e'_j) \}\) des espaces vectoriels \( V\) et \( W\). Soit une application linéaire \( A\in\End(V,W)\). Alors
	\begin{equation}
		A=\sum_{ij}A_{ji}f_{ij}
	\end{equation}
	où les \( f_{ij}\) sont les éléments de bases définis en \eqref{EQooBMKQooCCypzP} et \( A_{ji}\) sont les éléments de matrices définis par la proposition \ref{PROPooGXDBooHfKRrv}.
\end{proposition}

\begin{proof}
	Il s'agit d'un calcul. En appliquant à \( e_k\), nous avons
	\begin{equation}
		\sum_{ij}A_{ji}f_{ij}(e_k)  = \sum_{ij}A_{ji}\delta_{ik}e'_j =\sum_{j}A_{jk}e'_j.
	\end{equation}
	En comparant avec la formule de la proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooKZYYooZPTkpq}, nous avons le résultat.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYCKRooTrajdP}
	Si \( A\in\eM(n,\eK)\) nous définissons le \defe{déterminant}{déterminant!matrice} de \( A\) par la formule
	\begin{equation}
		\det(A)=\sum_{\sigma\in S_n}(-1)^{\sigma}\prod_{i=1}^nA_{i,\sigma(i)}
	\end{equation}
	où la somme est effectuée sur tous les éléments du groupe symétrique\footnote{Pour le groupe symétrique, c'est la définition \ref{DEFooJNPIooMuzIXd}, le fait que ce soit un groupe fini est le lemme \ref{LEMooSGWKooKFIDyT}, et pour la somme sur un groupe fini c'est la définition \ref{DEFooLNEXooYMQjRo}.} \( S_n\) et où \( (-1)^{\sigma}\) représente la parité de la permutation \( \sigma\).
\end{definition}
En se souvenant que \( | S_n |=n!\), nous sommes frappés de stupeur devant le fait que le nombre de termes dans la somme croît de façon factorielle (c'est plus qu'exponentiel, pour info) en la taille de la matrice. Cette formule est donc sans espoir pour une matrice plus grande que \( 3\times 3\) ou à la rigueur \( 4\times 4\) à la main. À l'ordinateur, il est possible de monter plus haut, mais pas tellement.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant en petite dimension}
%---------------------------------------------------------------------------------------------------------------------------

En dimension deux, le déterminant de la matrice
\( \begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix}\)
est le nombre
\begin{equation}        \label{EQooQRGVooChwRMd}
	\det\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}=\begin{vmatrix}
		a & b \\
		c & d
	\end{vmatrix}=ad-cb.
\end{equation}
Ce nombre détermine entre autres le nombre de solutions que va avoir le système d'équations linéaires associé à la matrice.

Pour une matrice \( 3\times 3\), nous avons le même concept, mais un peu plus compliqué; nous avons la formule
\begin{equation}
	\det
	\begin{pmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33}
	\end{pmatrix}
	=
	\begin{vmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33}
	\end{vmatrix}=
	a_{11}\begin{vmatrix}
		a_{22} & a_{23} \\
		a_{32} & a_{33}
	\end{vmatrix}-
	a_{12}\begin{vmatrix}
		a_{21} & a_{23} \\
		a_{31} & a_{33}
	\end{vmatrix}+
	a_{13}\begin{vmatrix}
		a_{21} & a_{22} \\
		a_{31} & a_{32}
	\end{vmatrix}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Manipulations de lignes et de colonnes}
%---------------------------------------------------------------------------------------------------------------------------

Nous voudrions savoir ce qu'il se passe avec le déterminant d'une matrice lorsque nous substituons à une ligne ou une colonne, une combinaison des autres lignes et colonnes. Lorsqu'une matrice est donnée, nous notons \( C_j\) sa \( j\)\ieme\ colonne.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooRSJTooQEoOtN}
	Si \( A\) et \( B\) sont des matrices, alors
	\begin{equation}
		(AB)^t=B^tA^t.
	\end{equation}
\end{lemma}

\begin{proof}
	Il suffit de calculer les éléments de matrice :
	\begin{equation}
		(AB)^t_{i,j}=(AB)_{j,i}=\sum_k A_{j,k}B_{k,i}=\sum_k B^t_{i,k}A^t_{k,j}=(B^tA^t)_{i,j}.
	\end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau,ooKYTYooJlzZMp}]             \label{LEMooCEQYooYAbctZ}
	Si \( A\) est une matrice, alors \( \det(A)=\det(A^t)\).
\end{lemma}

\begin{proof}
	Nous commençons par écrire la définition du déterminant :
	\begin{equation}
		\det(A^t)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n (A^t)_{i,\sigma(i)}=\sum_{\sigma} \epsilon(\sigma)\prod_i A_{\sigma(i),i}.
	\end{equation}
	Pour chaque \( \sigma\) séparément, en utilisant la proposition \ref{PROPooQMUDooQQVRIe} pour ré-indexer le produit :
	\begin{equation}
		\prod_i A_{\sigma(i),i}=\prod_i A_{i,\sigma^{-1}(i)}.
	\end{equation}
	Nous profitons du fait que l'application \( \varphi\colon S_n\to S_n\) donnée par \( \varphi(\sigma)=\sigma^{-1}\) soit une permutation de \( S_n\) pour appliquer la définition \ref{DEFooLNEXooYMQjRo} et faire la somme sur \( \sigma^{-1}\) :
	\begin{equation}
		\det(A^t)=\sum_{\sigma}\epsilon(\sigma)\prod_iA_{i,\sigma^{-1}(i)}=\sum_{\sigma}\epsilon(\sigma^{-1})\prod_iA_{i,\sigma(i)}=\det(A)
	\end{equation}
	où nous avons utilisé le fait que \(\epsilon(\sigma^{-1})=\epsilon(\sigma)\) (corolaire \ref{CORooZLUKooBOhUPG}).
\end{proof}

Le fait que \( \det(A)=\det(A^t)\) permet, dans toutes les propositions du type «ce qui arrive au déterminant si on change telle ligne ou colonne» de ne donner qu'une preuve pour la partie «ligne» et déduire automatiquement le cas «colonne». Le lemme suivant donne un exemple d'utilisation.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooWMQWooGWFlmC}
	Soit une matrice \( A\). Nous considérons la matrice \( B\) obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\) ainsi que la matrice \( C\) obtenue à partir de \( A^t\) par la permutation de colonnes \( C_k\leftrightarrow C_l\).

	Alors \( C^t=B\).
\end{lemma}

\begin{proof}
	Calculons les éléments de matrice de \( C\) :
	\begin{equation}
		C_{i,j}=\begin{cases}
			(A^t)_{i,j} & \text{si }  j\neq k, j\neq l \\
			(A^t)_{i,k} & \text{si }  j=l              \\
			(A^t)_{i,l} & \text{si }  j=k
		\end{cases}=
		\begin{cases}
			A_{j,i} & \text{si }  j\neq k, j\neq l \\
			A_{k,i} & \text{si }  j=l              \\
			A_{l,i} & \text{si }  j=k.
		\end{cases}
	\end{equation}
	Ensuite nous prouvons que \( C^t=B\) en écrivant les éléments de \( C^t\) :
	\begin{equation}
		(C^t)_{i,j}=C_{j,i}=\begin{cases}
			A_{i,j} & \text{si } i\neq k, i\neq l \\
			A_{k,j} & \text{si } i=l              \\
			A_{l,j} & \text{si } i=k.
		\end{cases}
	\end{equation}
	Cette dernière expression est la matrice \( A\) après permutation des lignes \( L_k\leftrightarrow L_l\), c'est-à-dire la matrice \( B\).
\end{proof}

Pour la suite nous écrivons \( \delta\) la matrice «identité», c'est-à-dire celle dont les entrées sont précisément les \( \delta_{i,k}\).  Nous écrivons également \( E_{i,j}\) la matrice contenant des zéros partout sauf en \( (i,j)\) où elle a un \( 1\), c'est-à-dire
\begin{equation}
	(E_{i,j})_{k,l}=\delta_{i,k}\delta_{j,l}.
\end{equation}

\begin{proposition}[Permuter des lignes ou des colonnes \( L_k\leftrightarrow L_l\)\cite{ooKBOMooSkKHvu,MonCerveau}]    \label{PROPooFQRDooRPfuxk}
	Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\).
	\begin{enumerate}
		\item   \label{ITEMooAIHWooHXzeys}
		      Si \( B\) est la matrice obtenue à partir de \( A\) en permutant deux lignes ou deux colonnes, alors
		      \begin{equation}
			      \det(A)=-\det(B).
		      \end{equation}
		\item   \label{ITEMooDNHWooOMgmxa}
		      Si \( B\) est la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\). Alors
		      \begin{equation}
			      B=SA
		      \end{equation}
		      avec \( S=\delta+E_{k,l}+E_{l,k}-E_{k,k}-E_{l,l}\).

		      Autrement dit : la matrice \( S\) est une matrice de permutations de lignes.
		\item   \label{ITEMooSHRQooQrqVdO}
		      La matrice \( S\) vérifie \( \det(S)=-1\)
		\item   \label{ITEMooQXSEooMWiKbL}
		      Nous avons
		      \begin{equation}
			      \det(SA)=\det(S)\det(A).
		      \end{equation}
	\end{enumerate}
\end{proposition}

\begin{proof}
	Point par point
	\begin{subproof}
		\spitem[\ref{ITEMooAIHWooHXzeys} pour les colonnes]

		Soient \( k\) et \( l\) fixés, et considérons la permutation des colonnes \( C_k\) et \( C_l\). Nous notons \( \alpha\) la permutation \( (k,l)\) dans \( S_n\) (groupe symétrique, définition \ref{DEFooJNPIooMuzIXd}). Nous avons
		\begin{equation}
			B_{i,j}=A_{i,\alpha(j)},
		\end{equation}
		ou encore : \( A_{i,j}=B_{i,\alpha(j)}\). Par définition,
		\begin{equation}
			\det(A)=\sum_{\sigma\in S_{n}}\epsilon(\sigma)\prod_{i=1}^n A_{i,\sigma(i)}
		\end{equation}
		C'est le moment d'utiliser la proposition \ref{PROPooWJQQooFINSEc} à propos de somme sur des groupes avec \( G=S_n\), \( h=\alpha\) et
		\begin{equation}
			f(\sigma)=\epsilon(\sigma)\prod_iA_{i,\sigma(i)}.
		\end{equation}
		Nous savons que \( \epsilon(\alpha)=-1\) et que \( \epsilon\) est un morphisme par la proposition \ref{ProphIuJrC}\ref{ITEMooBQKUooFTkvSu}, donc
		\begin{equation}
			f(\alpha \sigma)=\epsilon(\alpha\sigma)\prod_iA_{i,(\alpha\sigma)(i)}=-\epsilon(\sigma)\prod_iB_{i,\sigma(i)}.
		\end{equation}
		Avec ça, nous concluons :
		\begin{equation}
			\det(A)=\sum_{\sigma\in S_n}f(\sigma)=\sum_{\sigma}f(\alpha \sigma)=-\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n B_{i,\sigma(i)}=-\det(B).
		\end{equation}

		\spitem[\ref{ITEMooAIHWooHXzeys} pour les lignes]
		Que se passe-t-il si nous permutons les lignes \( L_k\) et \( L_{l}\) ? Si nous notons \( B'\) la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\), et \( C\) celle obtenue de \( A^t\) après permutation de colonnes \( C_k\leftrightarrow C_l\) alors nous avons \( C^t=B'\). Le lemme \ref{LEMooWMQWooGWFlmC} nous dit que \( C^t=B'\). En utilisant le lemme \ref{LEMooCEQYooYAbctZ} sur le déterminant de la transposée,
		\begin{equation}
			\det(B')=\det(C^t)=\det(C)=-\det(A^t)=-\det(A).
		\end{equation}
		Voilà qui prouve le résultat pour les permutations de lignes.

		\spitem[\ref{ITEMooDNHWooOMgmxa}]
		Si \( k=l\), il n'y a pas de permutation, et il est vite vu que la matrice \( S\) est l'identité parce qu'il y a quatre fois le terme \( E_{k,k}\). Nous supposons donc que \( k\neq l\); en particulier \( \delta_{k,l}=0\).

		Il s'agit surtout d'un beau calcul :
		\begin{subequations}
			\begin{align}
				(SA)_{i,j}=\sum_{m}S_{i,m}A_{m,j} & =A_{i,j}+\sum_m(\delta_{k,i}\delta_{l,m}+\delta_{l,i}\delta_{l,m}-\delta_{k,i}\delta_{k,m}-\delta_{l,i}\delta_{l,m})A_{m,j} \\
				                                  & =A_{i,j}+\delta_{k,i}A_{l,j}+\delta_{l,i}A_{k,j}-\delta_{k,i}A_{k,j}-\delta_{l,i}A_{l,j}.
			\end{align}
		\end{subequations}
		Si \( i\neq j\) et \( i\neq l\), alors \( (SA)_{i,j}=A_{i,j}\). Si \( i=k\), alors
		\begin{equation}
			(SA)_{k,j}=A_{k,j}+A_{l,j}-A_{k,j}=A_{l,j},
		\end{equation}
		c'est-à-dire que la \( k\)\ieme\ ligne de \( SA\) est la \( l\)\ieme\ ligne de \( A\).

		Avec \( i=l\) nous obtenons la \( k\)\ieme\ ligne de \( A\).

		Tout cela montre que \( SA\) est la matrice \( A\) dans laquelle les lignes \( k\) et \( l\) ont été échangées, c'est-à-dire \( SA=B\).

		\spitem[\ref{ITEMooSHRQooQrqVdO}]
		En utilisant la définition du déterminant,
		\begin{subequations}
			\begin{align}
				\det(S) & =\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n S_{i,\sigma(i)}                                                                                                                                         \\
				        & =\sum_{\sigma}\epsilon(\sigma)\prod_i\big( \delta_{i,\sigma(i)}+\delta_{k,i}\delta_{l,\sigma(i)}+\delta_{l,i}\delta_{k,\sigma(i)}-\delta_{k,i}\delta_{k,\sigma(i)}-\delta_{l,i}\delta_{l,\sigma(i)} \big).
			\end{align}
		\end{subequations}
		Nous utilisons l'associativité et la commutativité du produit pour séparer les facteurs \( i=k\) et \( i=l\) des autres :
		\begin{equation}
			\det(S)=\sum_{\sigma}\epsilon(\sigma)\prod_{\substack{i\neq k \\i\neq l}}\delta_{i,\sigma(i)}(\delta_{k,\sigma(k)}+\delta_{l,\sigma(k)}-\delta_{k,\sigma(k)})(\delta_{l,\sigma(l)}+\delta_{k,\sigma(l)}-\delta_{l,\sigma(l)}).
		\end{equation}
		À cause des facteurs \( i\neq k\) et \( i\neq l\), les \( \sigma\) pour lesquels le tout n'est pas nul doivent vérifier \( \delta_{i,\sigma(i)}=1\) pour tout \( i\) différent de \( k\) et \( l\). Les deux seuls sont donc \( \sigma=\id\) et la permutation \( \sigma=(k,l)\). Pour \( \sigma=\id\), nous avons
		\begin{equation}
			\prod_{\substack{i\neq k \\i\neq l}}\delta_{i,i}(\delta_{k,k}+\delta_{l,k}-\delta_{k,k})(\delta_{l,l}+\delta_{k,l}-\delta_{l,l})=0.
		\end{equation}
		Dernier espoir : \( \sigma=(k,l)\). Pour ce terme nous avons \( \epsilon(\sigma)=-1\) et
		\begin{equation}
			\prod_{\substack{i\neq k \\i\neq l}}\delta_{i,i}(\delta_{k,l}+\delta_{l,l}-\delta_{k,l})(\delta_{l,k}+\delta_{k,k}-\delta_{l,k})=1.
		\end{equation}
		Au final dans \( \det(S)\), il n'y a que le terme \( \sigma=(k,l)\) qui est non nul, et il vaut \( -1\). Donc
		\begin{equation}
			\det(S)=-1.
		\end{equation}

		\spitem[\ref{ITEMooQXSEooMWiKbL}]
		Il s'agit de mettre bout à bout les points déjà prouvés :
		\begin{equation}
			\det(SA)=-\det(A)=\det(S)\det(A).
		\end{equation}
	\end{subproof}
\end{proof}

\begin{corollary}[\cite{ooKBOMooSkKHvu}]        \label{CORooAZFCooSYINvBl}
	Soit une matrice \( A\in \eM(n,\eK)\). Si deux lignes ou deux colonnes de \( A\) sont égales, alors \( \det(A)=0\).
\end{corollary}

\begin{proof}
	Si deux colonnes sont égales, la matrice ne change pas lorsqu'on les permute, alors que le déterminant change de signe. La seule possibilité est que \( \det(A)=-\det(A)\), ce qui signifie que \( \det(A)=0\).
\end{proof}
Notons que si pour \( k\neq l\) nous avons \( C_k=\lambda C_l\), alors nous avons aussi \( \det(A)=0\).

La réciproque n'est pas vraie : il existe des matrices dont le déterminant est nul et dont aucune entrée n'est nulle. Par exemple
\begin{equation}
	\begin{pmatrix}
		1 & 2 \\
		1 & 2
	\end{pmatrix}.
\end{equation}


\begin{proposition}[\cite{ooKBOMooSkKHvu}]      \label{PROPooNGZJooHjtMyn}
	Soient \( A\in \eM(n,\eK)\), et \( v\in \eK^n\). Si \( B\) est la matrice \( A\) avec la substitution \( L_j\to L_j+v\) et \( C\) est la matrice \( A\) avec la substitution \( L_j\to v\), alors
	\begin{equation}
		\det(B)=\det(A)+\det(C).
	\end{equation}
\end{proposition}

\begin{proof}
	En utilisant l'associativité de la multiplication,
	\begin{subequations}
		\begin{align}
			\det(B) & =\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n B_{i,\sigma(i)}                                                                                                \\
			        & =\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}B_{i,\sigma(i)} \big)B_{j,\sigma(j)}                                                                           \\
			        & =\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}A_{i,\sigma(i)} \big)(A_{j,\sigma(j)}+v_{\sigma(j)})                                                           \\
			        & =\sum_{\sigma}\epsilon(\sigma)\prod_i A_{i,\sigma(i)}+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i,\sigma(i)}v_{\sigma(j)}        \label{SUBEQooKATCooVIbEpv} \\
			        & =\det(A)+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i,\sigma(i)}C_{j,\sigma(j)}                                                   \label{SUBEQooCOTDooPPrEYJ} \\
			        & =\det(A)+\det(C).
		\end{align}
	\end{subequations}
	Justifications :
	\begin{itemize}
		\item \ref{SUBEQooKATCooVIbEpv} parce que pour \( i\neq j\) nous avons \( A_{i,\sigma(i)}=C_{i,\sigma(i)}\)
		\item \ref{SUBEQooCOTDooPPrEYJ} parce que \( v_{\sigma(j)}=C_{j,\sigma(j)}\).
	\end{itemize}
\end{proof}

\begin{proposition}[Combinaison de lignes ou colonnes \( L_k\to L_k+\lambda L_l\)\cite{ooKBOMooSkKHvu}]       \label{PROPooPYNHooLbeVhj}
	Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\).
	\begin{enumerate}
		\item       \label{ITEMooJSRDooTggEyO}
		      Si \( B\) est la matrice obtenue à partir de \( A\) par la substitution \( L_k\to L_k+\lambda L_l\) ou \( C_k\to C_k+\lambda C_l\), alors
		      \begin{equation}
			      \det(A)=\det(B).
		      \end{equation}
		\item       \label{ITEMooHKZWooVZDgnf}
		      Si \( B\) est la matrice \( A\) dans laquelle nous avons opéré la substitution \( L_k\to L_k+\lambda L_l\), alors
		      \begin{equation}
			      B=UA
		      \end{equation}
		      avec \( U=\delta+\lambda E_{k,l}\), c'est-à-dire que \( U\) est une matrice de combinaison de lignes.
		\item       \label{ITEMooPGYJooWTTghT}
		      La matrice \( U\) vérifie \( \det(U)=1\).
		\item       \label{ITEMooBBEAooZJVGNV}
		      Nous avons
		      \begin{equation}
			      \det(UA)=\det(U)\det(A).
		      \end{equation}
	\end{enumerate}
\end{proposition}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[\ref{ITEMooJSRDooTggEyO}]
		Soit la matrice \( C\) obtenue à partir de \( A\) par \( L_k\to \lambda L_l\). En considérant le vecteur \( v=\lambda L_l\), nous sommes dans la situation de la proposition \ref{PROPooNGZJooHjtMyn}. Donc
		\begin{equation}
			\det(B)=\det(A)+\det(C).
		\end{equation}
		Mais dans la matrice \( C\), nous avons \( L_k=\lambda L_l\), ce qui implique \( \det(C)=0\) par le corolaire \ref{CORooAZFCooSYINvBl}. Donc \( \det(A)=\det(B)\) comme il se devait.

		\spitem[\ref{ITEMooHKZWooVZDgnf}]
		Encore un calcul :
		\begin{equation}
			(UA)_{i,j}=\sum_m\big( \delta_{i,m}+\lambda(E_{k,l})_{i,m} \big)A_{m,j}=A_{i,j}+\lambda\sum_m\delta_{k,i}\delta_{l,m}A_{m,j}=A_{i,j}+\lambda \delta_{l,i}A_{k,j}.
		\end{equation}
		Cela donne, pour \( i=k\) la ligne
		\begin{equation}
			(UA)_{k,j}=A_{k,j}+\lambda A_{l,j},
		\end{equation}
		ce qui correspond bien à \( L_k\to L_k+\lambda L_l\).

		\spitem[\ref{ITEMooPGYJooWTTghT}]
		Nous calculons le déterminant de \( U=\delta+\lambda E_{k,l}\) avec \( k\neq l\). Nous avons dans un premier temps :
		\begin{equation}
			\det(U)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n(\delta_{i,\sigma(i)}+\lambda \delta_{k,i}\delta_{l,\sigma(i)}).
		\end{equation}
		Puisque nous avons toujours \( \delta_{k,i}\delta_{l,i}=0\), le terme \( \sigma=\id\) donne \( 1\).

		Pour les \( \sigma\neq \id\), le facteur \( \lambda\delta_{k,i}\delta_{l,\sigma(i)}\) ne s'annule pas, uniquement si \( i=k\) et \( \sigma(i)=l\). Donc le seul terme non nul autre que \( \sigma=\id\) peut provenir de \( \sigma=(k,l)\). Pour ce terme, nous isolons les termes \( i=l\) et \( i=k\) :
		\begin{equation}
			(\delta_{k,\sigma(k)}+\lambda\delta_{k,k}\delta_{k,\sigma(k)})(\delta_{l,\sigma(l)}+\lambda\delta_{k,l}\delta_{k,\sigma(l)}).
		\end{equation}
		Le dernier facteur est nul.

		\spitem[\ref{ITEMooBBEAooZJVGNV}]
		En mettant bout à bout les résultats prouvés,
		\begin{equation}
			\det(UA)=\det(A)=\det(U)\det(A).
		\end{equation}
	\end{subproof}
\end{proof}

\begin{proposition}[Multiplication par un scalaire d'une ligne ou colonne \( L_k\to \lambda L_k\)\cite{ooKBOMooSkKHvu}] \label{PROPooXUFKooOaPnna}
	Soient une matrice \( A\in \eM(n,\eK)\), un entier \( k\leq n\). Soit la matrice \( B\) obtenue à partir de \( A\) en multipliant la ligne \( L_k\) par \( \lambda\in \eK\).
	\begin{enumerate}
		\item       \label{ITEMooBKIGooCDQEDt}
		      \( \det(B)=\lambda\det(A)\)
		\item       \label{ITEMooWRRCooFXkRNW}
		      En considérant la matrice \( T=\delta+(\lambda-1)E_{k,k}\), nous avons
		      \begin{equation}
			      B=TA,
		      \end{equation}
		      c'est-à-dire que la matrice \( T\) est une matrice de multiplication de ligne par un scalaire.
		\item       \label{ITEMooOGGDooPVVRzk}
		      Nous avons \( \det(T)=\lambda\).
		\item       \label{ITEMooIFRVooWQYgkK}
		      Et aussi : \( \det(TA)=\det(T)\det(A)\)
	\end{enumerate}
\end{proposition}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[\ref{ITEMooBKIGooCDQEDt}]
		La matrice \( B\) est donnée par les éléments
		\begin{equation}
			B_{i,j}=\begin{cases}
				A_{i,j}         & \text{si } i\neq k \\
				\lambda A_{i,j} & \text{si } i=k
			\end{cases}
		\end{equation}
		c'est-à-dire \( B_{i,j}=\big( 1+(\lambda-1)\delta_{i,k} \big)A_{i,j}\). Nous mettons cela dans la définition du déterminant de \( B\) :
		\begin{equation}        \label{EQooGVMTooPntKew}
			\det(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n B_{i,\sigma(i)}=\sum_{\sigma}\prod_i\big( 1+(\lambda-1)\delta_{\sigma(i),k}A_{i,\sigma(i)} \big).
		\end{equation}
		L'associativité du produit dans \( \eK\) nous permet de séparer le produit de la façon suivante :
		\begin{equation}
			\prod_{i=1}^n\big( 1+(\lambda-1)\delta_{\sigma(i),k} \big)A_{i,\sigma(i)}=\prod_i\big( 1+(\lambda-1)\delta_{\sigma(i),k} \big)\prod_i A_{i,\sigma(i)}=\lambda\prod_i A_{i,\sigma(i)}.
		\end{equation}
		En remettant dans \eqref{EQooGVMTooPntKew}, nous trouvons \( \det(B)=\det(A)\).

		\spitem[\ref{ITEMooWRRCooFXkRNW}]
		C'est un cas particulier de la proposition \ref{PROPooPYNHooLbeVhj}\ref{ITEMooHKZWooVZDgnf} en prenant \( k=l\) et en adaptant le \( \lambda\).

		\spitem[\ref{ITEMooOGGDooPVVRzk}]
		Nous calculons le déterminant de la matrice \( T=\delta+(\lambda-1)E_{k,k}\). La formule du déterminant donne
		\begin{equation}
			\det(T)=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^n\big( \delta_{i,\sigma(i)}+(\lambda-1)\delta_{k,i}\delta_{k,\sigma(i)} \big).
		\end{equation}
		Si \( i\neq \sigma(i)\), alors non seulement \( \delta_{i,\sigma(i)}=0\), mais en plus \( \delta_{k,i}\delta_{k,\sigma(i)}=0\). Donc seul \( \sigma=\id\) reste dans la somme sur \( \sigma\in S_n\). Il reste donc
		\begin{equation}
			\det(T)=\prod_{i=1}^n\big( 1+(\lambda-1)\delta_{k,i} \big)=\left( \prod_{i\neq k}1 \right)(1+(\lambda-1))=\lambda
		\end{equation}
		où nous avons utilisé encore l'associativité pour isoler le facteur \( i=k\).

		\spitem[\ref{ITEMooIFRVooWQYgkK}]
		Il faut mettre bout à bout les résultats déjà établis :
		\begin{equation}
			\det(TA)=\lambda\det(A)=\det(T)\det(A).
		\end{equation}
	\end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}			\label{DEFooKJNNooOcIJuu}
	Nous avons vu les matrices d'opérations élémentaire sur les lignes :
	\begin{itemize}
		\item Permutation de lignes \( L_k\leftrightarrow L_l\)  : \( S(n;k,l)=\delta+E_{k,l}+E_{l,k}-E_{k,k}-E_{l,l}\), proposition \ref{PROPooFQRDooRPfuxk}.
		\item Combinaisons de lignes \( L_k\to L_k+\lambda L_l\) : \( U(n;k,l,\lambda)=\delta+\lambda E_{k,l}\), proposition \ref{PROPooPYNHooLbeVhj}.
		\item Multiplication d'une ligne par un scalaire \( L_k\to \lambda L_k\) : \( T(n;k,\lambda)=\delta+(\lambda-1)E_{k,k}\), proposition \ref{PROPooXUFKooOaPnna}.
	\end{itemize}
	Les matrices \( S(n;k,l)\), \( U(n;k,l)\) et \( T(n;k,\lambda)\) sont les \defe{matrices de manipulation de lignes}{manipulation de lignes}.
\end{definition}


\begin{normaltext}		\label{NORMooDXYZooEdnztA}
	Il existe également des matrices pour faire les mêmes opérations, mais sur les colonnes. Ces matrices doivent toutefois multiplier à droite et non à gauche. Par exemple il existe une matrice \( C(n,k,l)\) telle que \( AC(n,k,l)\) soit la matrice \( A\), avec les colonnes \( k\) et \( l\) inversées.

	%TODOooYHBYooNjOPKJ Il faut donner ces matrices, et les lier dans PROPooPMYCooAAtHsB.
	% c'est dans ma liste.
\end{normaltext}


\begin{lemma}		\label{LEMooDXSZooVpbCRj}
	Si \( G\) est une matrice de manipulation de lignes, alors
	\begin{equation}        \label{EQooLQTVooBYjVYl}
		\det(GA)=\det(G)\det(A)
	\end{equation}
	pour toute matrice \( A\).

	Si \( H\) est une matrice de manipulation de colonnes, alors
	\begin{equation}
		\det(AH)=\det(A)\det(H)
	\end{equation}
	pour toute matrice \( A\).
	%TODOooYHBYooNjOPKJ. Faire la preuve pour les colonnes.
\end{lemma}

\begin{proof}
	En ce qui concerne les manipulations de ligne, nous avons déjà tout fait dans les propositions \ref{PROPooFQRDooRPfuxk}\ref{ITEMooQXSEooMWiKbL}, \ref{PROPooPYNHooLbeVhj}\ref{ITEMooBBEAooZJVGNV} et \ref{PROPooXUFKooOaPnna}\ref{ITEMooIFRVooWQYgkK}.

	%TODOooYHBYooNjOPKJ. Faire la preuve pour les colonnes.
\end{proof}

\begin{proposition}[Réduction de Gauss\cite{MonCerveau}]        \label{PROPooJBTZooNLobpf}
	Soit une matrice \( A\in \eM(n,\eK)\) de déterminant non nul : \( \det(A)\neq 0\). Alors il existe des matrices \( G_1,\ldots, G_N\) toutes de type \( S\), \( U\) ou \( T\) telles que
	\begin{equation}
		G_1\ldots G_NA=\delta.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous procédons par récurrence sur \( n\). D'abord pour \( n=1\), la matrice \( A\) contient un seul élément \( A_{1,1}\) qui est non nul par hypothèse. Nous pouvons multiplier sa ligne par \( 1/A_{1,1}\) pour obtenir le résultat. Plus précisément, nous avons l'égalité
	\begin{equation}
		T(1;1,\frac{1}{ A_{1,1} })A=\delta
	\end{equation}
	dans \( \eM(1, \eK)\). Notons que \( \eK\) est un corps (donc \( A_{1,1}\) est inversible) commutatif, ce qui permet d'écrire \( 1/A_{1,1}\) sans ambiguïté.

	Supposons le résultat prouvé pour \( n\), et voyons ce qu'il se passe pour \( n+1\). Puisque \( \det(A)\neq 0\), aucune de ses colonnes n'est nulle (corolaire \ref{CORooAZFCooSYINvBl}). Il existe donc un \( k\) tel que \( A_{k,1}\neq 0\).

	Par la proposition \ref{PROPooFQRDooRPfuxk}, la matrice
	\begin{equation}
		B^{(1)}=S(n+1;k,1)A
	\end{equation}
	est une matrice telle que \( B^{(1)}_{1,1}=A_{k,1}\neq 0\). Ensuite, par la proposition \ref{PROPooXUFKooOaPnna} la matrice
	\begin{equation}
		B^{(2)}=T(n+1;1,\frac{1}{ A_{k,1} })B^{(1)}
	\end{equation}
	vérifie \( B^{(2)}_{1,1}=1\).

	Puisque la multiplication par la matrice \( U(n+1;k;l;\lambda)\) réalise, par la proposition \ref{PROPooPYNHooLbeVhj}, la substitution \( L_k\to L_{k}+\lambda L_l\), la matrice
	\begin{equation}
		B^{(3)}=\prod_{k=2}^{n+1} U(n+1;k,1,-B^{(1)}_{k,1})B^{(1)}
	\end{equation}
	a toute sa première colonne nulle à l'exception de \( B^{(3)}_{1,1}=1\).

	Nous n'avons pas donné de nom ni démontré de théorèmes à propos de la substitution \( C_k\to C_k+\lambda C_l\). En passant éventuellement par les transposées et en utilisant les lemmes \ref{LEMooRSJTooQEoOtN} et \ref{LEMooCEQYooYAbctZ} nous obtenons une matrice \( U'(n+1;k,l,\lambda)\) ayant la propriété que la matrice
	\begin{equation}
		B^{(4)}=\prod_{k=2}^{n+1} U'(n+1;k,1,-B^{(3)}_{1,k})B^{(3)}
	\end{equation}
	vérifie \( B^{(4)}_{1,j}=B^{(4)}_{j,1}=0\) pour tout \( j\) sauf \( j=1\). En d'autres termes, la matrice \( B^{(4)}\) est de la forme
	\begin{equation}
		B^{(4)}=\begin{pmatrix}
			\begin{matrix} 1 \end{matrix} & \begin{matrix}
				                                0 & \ldots & 0
			                                \end{matrix} \\
			\begin{matrix}
				0      \\
				\vdots \\
				0
			\end{matrix}                & \begin{pmatrix}
				                               &    & \\
				                               & A' & \\
				                               &    &
			                              \end{pmatrix}
		\end{pmatrix}
	\end{equation}
	où \( A'\) est une matrice de taille \( n\).

	Voyons quelques propriétés de \( A'\). Nous savons que
	\begin{equation}
		B^{(4)}=\prod_i G_iA
	\end{equation}
	où les \( G_i\) sont de type \( S\), \( T\) ou \( U\). Puisque \( \det(SA)=\det(S)\det(A)\) (et idem pour \( T\) et \( U\)), nous avons
	\begin{equation}
		\det(B^{(4)})=\prod_i\det(G_i)\det(A),
	\end{equation}
	et comme aucun des \( \det(G_i)\) n'est nul, nous avons encore \( \det(B^{(4)})\neq 0\), ce qui implique \( \det(A')\neq 0\).

	La récurrence peut avoir lieu. Il existe des matrices \( G'_i\) telles que
	\begin{equation}
		G'_1\ldots G'_MA'=\delta
	\end{equation}
	où les \( G'_i\) sont de taille \( n\), ainsi que le \( \delta\). En remarquant que
	\begin{equation}
		S(n+1;k,l) =\begin{pmatrix}
			1              & \begin{matrix}
				                 0 & \ldots & 0
			                 \end{matrix} \\
			\begin{matrix}
				0      \\
				\vdots \\
				0
			\end{matrix} & S(n;k-1,l-1)
		\end{pmatrix},
	\end{equation}
	et pareillement pour les matrices \( T\) et \( U\), nous voyons qu'en prenant
	\begin{equation}
		G_i =\begin{pmatrix}
			1              & \begin{matrix}
				                 0 & \ldots & 0
			                 \end{matrix} \\
			\begin{matrix}
				0      \\
				\vdots \\
				0
			\end{matrix} & G'_i
		\end{pmatrix},
	\end{equation}
	nous avons
	\begin{equation}
		\prod_{i=1}^MG_iB^{(3)}=
		\begin{pmatrix}
			1              & \begin{matrix}
				                 0 & \ldots & 0
			                 \end{matrix}    \\
			\begin{matrix}
				0      \\
				\vdots \\
				0
			\end{matrix} & \prod_{i=1}^MG'_iA'
		\end{pmatrix}=\delta_{n+1}
	\end{equation}
	où nous avons mis un indice sur le dernier \( \delta\) pour être plus explicite.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices inversibles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}      \label{PROPooMLWRooRWfZXE}
	Soit une matrice \( A\in \eM(n,\eK)\). Si les matrices \( B_1\) et \( B_2\) de \( \eM(n,\eK)\) vérifient
	\begin{equation}
		AB_1=B_1A=\delta
	\end{equation}
	et
	\begin{equation}
		AB_2=B_2A=\delta,
	\end{equation}
	alors \( B_1=B_2\). Dans ce cas, nous disons que \( A\) est inversible et nous notons \( A^{-1}\) l'unique matrice telle que \( AA^{-1}=A^{-1}A=\delta\).
\end{propositionDef}

\begin{proof}
	La preuve est réalisée dans le cas général par le lemme \ref{LEMooECDMooCkWxXf}. Mais si vous en voulez une preuve avec les notations d'ici, en voici une.

	Nous avons \( AB_1=AB_2\). En multipliant à gauche par \( B_1\), nous trouvons \( B_1AB_1=B_1AB_2\). En remplaçant \( B_1A\) par \( \delta\) des deux côtés, il reste \( B_1=B_2\).
\end{proof}

\begin{lemma}[\cite{ooKBOMooSkKHvu}]        \label{LEMooGZCTooQigDvC}
	Si \( A\in \eM(n,\eK)\), alors il existe au plus une matrice \( B\in \eM(n,\eK)\) telle que \( AB=\delta\).
\end{lemma}

\begin{proof}
	Soient des matrices \( B,C\in \eM(n,\eK)\) telles que \( AB=AC=\delta\). Nous allons montrer que \( B=C\).

	Pour cela nous considérons les applications linéaires \( f_A, f_B, f_C\in \End(\eK^n)\) associées par la proposition \ref{PROPooGXDBooHfKRrv}. Puisque \( AB=\delta\), par la proposition \ref{PROPooCSJNooEqcmFm}, nous avons \( f_A\circ f_B = f_{AB}=\id\). La proposition \ref{PROPooADESooATJSrH} nous dit alors que \( f_A\) et \( f_B\) sont bijectives.

	En particulier, comme \( \{e_i\}\) est une base, son image par \( f_B\) est une base par la proposition \ref{PROPooZFKZooBGLSex}. La proposition \ref{PROPooHLUYooNsDgbn} dit alors que \( \{f_B(e_i)\}\) est une base. Nous décomposons \( f_B(e_k)-f_C(e_k)\) dans cette base :
	\begin{equation}
		f_B(e_k)-f_C(e_k)=\sum_j\alpha_j f_B(e_j)
	\end{equation}
	où les \( \alpha_j\) dépendent à priori de \( k\). Puisque \( f_A\circ(f_B-f_C)=0\), nous avons
	\begin{equation}
		0=f_A\big( f_B(e_k)-f_C(e_k) \big)=\sum_j(f_A\circ f_B)(e_j)=\sum_j\alpha_j e_j.
	\end{equation}
	Donc les \( \alpha_j\) sont tous nuls.

	Nous en déduisons que \( f_B(e_k)=f_C(e_k)\), et donc \( f_B=f_C\). Cela implique que \( B=C\) par la proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.
\end{proof}

\begin{proposition}[\cite{ooKBOMooSkKHvu}]        \label{PROPooECIIooVMCIwz}
	Si \( A,B\in \eM(n,\eK)\) vérifient \( AB=\delta\), alors \( BA=\delta\).
\end{proposition}

\begin{proof}
	L'astuce est de poser \( C=BA-\delta+B\) et de montrer que \( C=B\). Pour cela, un rapide calcul commence par montrer que
	\begin{equation}
		AC=ABA-A+AB=AB=\delta.
	\end{equation}
	Donc \( C\) est également un inverse à droite de \( A\). Le lemme \ref{LEMooGZCTooQigDvC} donne alors \( C=B\).
\end{proof}

\begin{corollary}       \label{CORooBQLXooTeVfgb}
	Soit \( A\in \eM(n,\eK)\). Si il existe \( B\in \eM(n,\eK)\) tel que \( AB=\delta\), alors \( A\) est inversible et son inverse est \( B\).
\end{corollary}

\begin{proof}
	Il s'agit d'une paraphrase de la proposition \ref{PROPooECIIooVMCIwz} et de la définition \ref{PROPooMLWRooRWfZXE}.
\end{proof}

\begin{lemma}       \label{LEMooZDNVooArIXzC}
	Si une matrice \( A\) n'est pas inversible, alors le produit \( AB\) n'est inversible pour aucune matrice \( B\).
\end{lemma}

\begin{proof}
	Soient une matrice non inversible \( A\), ainsi qu'une matrice quelconque \( B\). Supposons que \( AB\) soit inversible. Alors
	\begin{equation}
		AB(AB)^{-1}=\delta.
	\end{equation}
	Donc la matrice \( B(AB)^{-1}\) est un inverse de \( A\). Contradiction.
\end{proof}

\begin{proposition}     \label{PROPooNPMCooPmaCwu}
	Une matrice est inversible si et seulement si son application linéaire associée est inversible. Dans ce cas, nous avons
	\begin{equation}
		f_A^{-1}=f_{A^{-1}}.
	\end{equation}
\end{proposition}

\begin{proof}
	Dans le sens direct, si \( A\) est inversible nous avons \( AA^{-1}=\delta\). Donc
	\begin{equation}        \label{EQooQQOSooBKVqXh}
		f_A\circ f_{A^{-1}}=f_{AA^{-1}}=f_{\delta}=\id
	\end{equation}
	où nous avons utilisé la proposition \ref{PROPooCSJNooEqcmFm} pour la composition et la proposition \ref{PROPooFMBFooEVCLKA} pour l'identité. L'égalité \eqref{EQooQQOSooBKVqXh} indique que \( f_A\) est inversible et que son inverse est \( f_{A^{-1}}\).

	Dans l'autre sens, l'application \( f_A^{-1}\) existe. Soit \( B\in \eM(n,\eK)\) sa matrice. Alors nous avons
	\begin{equation}
		f_{\delta}=\id=f_A\circ f_B=f_{AB}.
	\end{equation}
	Le fait que l'application \(\psi\colon A\to f_A\) soit une bijection\footnote{Proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.} implique que \( AB=\delta\), c'est-à-dire que \( A\) est inversible et que \( B=A^{-1}\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooMCIDooYBHrbq}
	Soient une matrice inversible \( A\in \eM(n,\eR)\) et \( r<n\). Il existe une permutation \( \sigma\in S_n\) telle que la matrice \( a\in\eM(r,\eR)\) donnée par
	\begin{equation}
		a_{i,j}=A_{i,\sigma(j)}
	\end{equation}
	soit inversible.
	%TODOooOIWMooDEUJOk. Prouver ça.
\end{lemma}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Inversibilité et déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooAVIXooMtVCet}
	Une matrice au déterminant non nul est inversible.
\end{proposition}

\begin{proof}
	Si \( A\) est une matrice telle que \( \det(A)\neq 0\), alors la proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices \( G_1,\ldots, G_N\) telles que
	\begin{equation}
		G_1\ldots G_NA=\delta.
	\end{equation}
	Donc la matrice \( G_1\ldots G_N\) est un inverse de \( A\) par le corolaire \ref{CORooBQLXooTeVfgb}.
\end{proof}

\begin{proposition}     \label{PROPooEOKBooKUROFg}
	Si une matrice \( A\) a une ligne ou une colonne de zéros, alors
	\begin{enumerate}
		\item
		      \( \det(A)=0\),
		\item
		      \( A\) n'est pas inversible.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Par définition, nous avons
	\begin{equation}
		\det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n A_{i,\sigma(i)}.
	\end{equation}
	Si la \( k\)\ieme\ ligne est nulle, alors \( A_{k,\sigma(k)}=0\) pour tout \( \sigma\). Donc tous les produits contiennent un facteur nul. Donc \( \det(A)=0\).

	Pour toute matrice \( B\) nous avons
	\begin{equation}
		(AB)_{k,k}=\sum_l A_{k,l}B_{l,k}.
	\end{equation}
	Si la \( k\)\ieme\ ligne de \( A\) est nulle nous avons \( (AB)_{k,k}=0\) et donc pas \( AB=\delta\). Donc \( A\) n'est pas inversible.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques ensembles de matrices particuliers}
%---------------------------------------------------------------------------------------------------------------------------
Certains ensembles de matrices ont une importance particulière, que nous développerons plus tard.

\begin{definition}[Groupe linéaire de matrices]
	On note \( \GL(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont inversibles. En d'autres termes, \( \GL(n,\eA) = U (\eM (n,\eA) ) \).
\end{definition}

\begin{definition}[Groupe orthogonal de matrices]\label{DefMatriceOrthogonale}
	On dit qu'une matrice \( A \) est \defe{orthogonale}{matrice!orthogonale} si son inverse est sa transposée, c'est-à-dire si \( A^{-1} = A^t \). On note \( \gO(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont orthogonales.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant et combinaisons de lignes et colonnes}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooKMSVooBBHwkH}

\begin{proposition}     \label{PROPooUCZVooPkloQp}
	Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)\neq 0\) et \( \det(B)\neq 0\). Alors
	\begin{equation}
		\det(AB)=\det(A)\det(B).
	\end{equation}
\end{proposition}

\begin{proof}
	La proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices de permutations de lignes et de colonnes \( G_1,\ldots, G_N\) et \( G'_1,\ldots, G'_N\) telles que\footnote{Les plus acharnés préciseront que pour avoir le même \( N\) des deux côtés, il a fallu compléter avec des matrices \( \delta\) là où il y en avait le moins.}
	\begin{subequations}        \label{EQooDNZUooHBhcZj}
		\begin{align}
			G_1\ldots G_NA   & =\delta  \\
			G'_1\ldots G'_NB & =\delta.
		\end{align}
	\end{subequations}
	Nous avons
	\begin{equation}
		(G'_1\ldots G'_N)\underbrace{(G_1\ldots G_N)A}_{=\delta}B=\delta.
	\end{equation}
	En prenant le déterminant des deux côtés et en tenant compte de \eqref{EQooLQTVooBYjVYl},
	\begin{equation}
		1=\det(\delta)=\det\big(  G'_1\ldots G'_NG_1\ldots G_NAB\big)=\det(G_1'\ldots G_N')\det(G_1\ldots G_N)\det(AB).
	\end{equation}
	Mais en même temps, les équations \ref{EQooDNZUooHBhcZj} donnent
	\begin{subequations}
		\begin{align}
			\det(G_1 \ldots G_N)  =\det(A)^{-1} \\
			\det(G_1'\ldots G'_N) =\det(B)^{-1}.
		\end{align}
	\end{subequations}
	Cela pour dire que
	\begin{equation}
		1=\det(A)^{-1}\det(B)^{-1}\det(AB),
	\end{equation}
	et donc ce qu'il nous fallait.
\end{proof}

\begin{proposition}     \label{PROPooWVJFooTmqoec}
	Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)=0\) et \( \det(B)\neq0\). Alors
	\begin{equation}
		\det(AB)=\det(BA)=\det(A)\det(B)=0.
	\end{equation}
\end{proposition}

\begin{proof}
	Il existe des matrices de manipulations de lignes et de colonnes \( G_1,\ldots, G_N\) telles que \( G_1\ldots G_NB=\delta\). Donc
	\begin{equation}
		0=\det(A)=\det(G_1\ldots G_NBA)=\det(G_1\ldots G_N)\det(BA).
	\end{equation}
	Donc \( \det(BA)=0\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transvections}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( E_{i,j}\) la matrice remplie de zéros sauf à la case \( i,j\) qui vaut \( 1\). Autrement dit
\begin{equation}
	(E_{i,j})_{k,l}=\delta_{i,k}\delta_{j,l}.
\end{equation}
\begin{definition}
	Une \defe{matrice de transvection}{transvection (matrice)}\index{matrice!de transvection} est une matrice de la forme
	\begin{equation}
		T_{i,j}(\lambda)=\id+\lambda E_{i,j}
	\end{equation}
	avec \( i\neq j\).

	Une \defe{matrice de dilatation}{matrice!de dilatation}\index{dilatation (matrice)} est une matrice de la forme
	\begin{equation}
		D_i(\lambda)=\id+(\lambda-1)E_{i,i}.
	\end{equation}
	Ici le \( (\lambda-1)\) sert à avoir \( \lambda\) et non \( 1+\lambda\). C'est donc une matrice qui dilate d'un facteur \( \lambda\) la direction \( i\), tout en laissant le reste inchangé.

	Si \( \sigma\) est une permutation (un élément du groupe symétrique \( S_n\)) alors la \defe{matrice de permutation}{matrice!de permutation}\index{permutation!matrice} associée est la matrice d'entrées
	\begin{equation}
		(P_{\sigma})_{i,j}=\delta_{i,\sigma(j)}.
	\end{equation}
\end{definition}

\begin{lemma}   \label{LemyrAXQs}
	La matrice \( T_{i,j}(\lambda)A=(\mtu+\lambda E_{i,j})A\) est la matrice \( A\) à qui on a effectué la substitution
	\begin{equation}
		L_i\to L_i+\lambda L_j.
	\end{equation}
	La matrice \( AT_{i,j}(\lambda)\) est la substitution
	\begin{equation}
		C_j\to C_j+\lambda C_i.
	\end{equation}

	La matrice \( AP_{\sigma}\) est la matrice \( A\) dans laquelle nous avons permuté les colonnes avec \( \sigma\).

	La matrice \( P_{\sigma}A\) est la matrice \( A\) dans laquelle nous avons permuté les lignes avec \( \sigma^{-1}\).
\end{lemma}

\begin{proof}
	Calculons la composante \( k,l\) de la matrice \( E_{i,j}A\) :
	\begin{subequations}
		\begin{align}
			(E_{i,j}A)_{k,l} & =\sum_m(E_{i,j})_{k,m}A_{m,l}          \\
			                 & =\sum_m\delta_{i,k}\delta_{j,m}A_{m,l} \\
			                 & =\delta_{i,k}A_{j,l}.
		\end{align}
	\end{subequations}
	C'est donc la matrice pleine de zéros, sauf la ligne \( i\) qui est donnée par la ligne \( j\) de \( A\). Donc effectivement la matrice
	\begin{equation}
		A+\lambda E_{i,j}A
	\end{equation}
	est la matrice \( A\) à laquelle on a substitué la ligne \( i\) par la ligne \( i\) plus \( \lambda\) fois la ligne \( j\).

	En ce qui concerne l'autre assertion sur les transvections, le calcul est le même et nous obtenons
	\begin{equation}
		(AE_{i,j})=A_{k,i}\delta_{j,l}.
	\end{equation}

	Pour les matrices de permutation, nous avons
	\begin{equation}
		(AP_{\sigma})_{k,l}=A_{k,\sigma(l)}
	\end{equation}
	et
	\begin{equation}
		(P_{\sigma}A)_{k,l}=\sum_m\delta_{k,\sigma(m)}A_{m,l}=\sum_m\delta_{\sigma^{-1}(k),m}A_{m,l}=A_{\sigma^{-1}(k),l}.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Mineur, rang}
%---------------------------------------------------------------------------------------------------------------------------

Pour la définition du rang d'une matrice, nous en donnons une qui est clairement inspirée de l'application linéaire associée.
\begin{definition}[\cite{ooKBOMooSkKHvu}]         \label{DEFooCSGXooFRzLRj}
	Le \defe{rang}{rang d'une matrice} d'une matrice de \( \eM(n,\eK)\) est la dimension de la partie de \( \eK^n\) engendrée par ses colonnes.
\end{definition}

Il est possible d'exprimer le rang d'une matrice de façon plus «intrinsèque» via le concept de mineur.
\begin{definition}[\cite{ooLFZTooJWJUed}]
	Les mineurs d'une matrice sont les déterminants de ses sous-matrices carrées.
\end{definition}
Dans la suite nous désignerons souvent par le mot «mineur» la sous-matrice carrée elle-même au lieu de son déterminant.

Lorsque \( A\) est une matrice, nous notons \( f_A\) l'application linéaire associée à la matrice \( A\) par l'application \eqref{EQooVZQWooMyFFeO}.

\begin{lemma} \label{LEMVecsaRgFixe}
	Soit \( \eK \) un corps commutatif\footnote{Comme toujours.}. Si \( A \) est une matrice carrée d'ordre \( n \) et de rang \( r \) à coefficients dans \( \eK \), alors il existe des vecteurs \( (x_i)_{i=1,\dots,n} \) formant une base de \( \eK^n \) tels que
	\begin{equation}
		f_A(x_i)\neq 0
	\end{equation}
	pour \( i\leq r\) et
	\begin{equation}
		f_A(x_i) = 0
	\end{equation}
	pour \( i > r \).
\end{lemma}

\begin{proof}
	Soit \( V\) le sous-espace de \( \eK^n\) engendré par les colonnes de \( A\). Nous considérons la base canonique \( \{ e_i \}\) de \( \eK^n\), ainsi que \( v_i\) le vecteur créé par la \( i\)\ieme\ colonne de \( A\). Nous avons
	\begin{equation}
		v_i=f_A(e_i).
	\end{equation}
	Les vecteurs \( v_i\) engendrent \( V\), donc nous pouvons en extraire une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooTZUDooFEgymQ}. Soit donc \( \{ v_j \}_{i\in J}\) une base de \( V\) avec \( J\subset\{ 1,\ldots, n \}\).

	La base de \( \eK^n\) que nous cherchons commence par les vecteurs \( \{ e_j \}_{j\in J}\). Ces vecteurs vérifient \( f_A(e_j)=v_j\neq 0\) parce que des vecteurs d'une base ne sont jamais nuls.

	% Note : toute la ligne suivante fait des références qui peuvent être vers le futur parce que ce sont des choses qui ne sont
	%        pas utilisées dans la démonstration.
	Pour la suite de la base, nous pourrions penser au théorème de la base incomplète\footnote{Théorème \ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu}.}, mais les vecteurs ainsi complétant la base ne sont pas garantis de s'annuler par \( f_A\). Voir l'exemple \ref{EXooRKVQooZOGDEf}.

	L'idée est d'utiliser le noyau de \( f_A\) qui est un sous-espace vectoriel par la proposition \ref{PROPooRLLPooKYzsJp}. Soit une base\footnote{Cette base contient \( n-r\) éléments, mais ce n'est pas très important pour la suite.} \(  \{ z_k \}  \) de \( \ker(f)\). Les vecteurs \( \{ e_j \}_{j\in J}\) forment une base de \( \Image(f_A)\). Puisque les \( z_i\) forment une base de \( \ker(f_A)\), le théorème du rang \ref{ThoGkkffA} dit alors que \( \{ e_j \}_{j\in J}\cup \{ z_k \}\) est une base de \( \eK^n\).

	Il y a \( r\) éléments dans \( J\) parce que l'espace engendré par les colonnes de \( A\) est de dimension \( r\) par hypothèse. Donc il y a \( n-r\) éléments dans les \( \{ z_k \}\) pour que le tout ait le bon nombre d'éléments.
\end{proof}

\begin{example}     \label{EXooRKVQooZOGDEf}
	Soit la matrice
	\begin{equation}
		A=\begin{pmatrix}
			1 & 1 \\
			2 & 2
		\end{pmatrix}.
	\end{equation}
	Elle est de rang \( 1\). En suivant l'idée de la démonstration, nous commençons la base de \( \eR^2\) par le vecteur \( e_1\) qui vérifie
	\begin{equation}
		f_A(e_1)=\begin{pmatrix}
			1 \\
			2
		\end{pmatrix}.
	\end{equation}
	L'utilisation du théorème de la base incomplète ne permet pas de trouver un second vecteur de base \( v\) tel que \( f_A(v)=0\). En effet ce théorème donne juste l'existence d'une completion de la base, mais pas de propriétés particulières de la base obtenue. Elle pourrait donner \( v=e_2\) comme second vecteur de base. Mais alors
	\begin{equation}
		f_A(v)=f_A(e_2)=\begin{pmatrix}
			1 \\
			2
		\end{pmatrix}\neq 0.
	\end{equation}

	Au contraire, le noyau de \( f_A\) est donné par le sous-espace engendré par \( \begin{pmatrix}
		1 \\
		-1
	\end{pmatrix}\). Une base convenable est donc \( \{ e_1, e_1-e_2 \}\).
\end{example}

\begin{proposition}     \label{PROPooEGNBooIffJXc}
	Le rang d'une application linéaire\footnote{Définition \ref{DefALUAooSPcmyK}.} est égal au rang de sa matrice\footnote{Définition \ref{DEFooCSGXooFRzLRj}.} dans n'importe quelle base.
\end{proposition}

\begin{proof}
	Soient deux espaces vectoriels \( V\) de dimension \( n\) et \( W\) de dimension \( m\). Nous considérons les bases \( \{ e_i \}_{i=1,\ldots,n}\) de \( V\) et \( \{ f_j \}_{j=1,\ldots,m}\) de \( W\). Soit une application linéaire \(\alpha \colon V\to W  \). La matrice de \( \alpha\) est donnée par \( A_{ji}=\alpha(e_i)j\). Nous considérons les applications linéaires
	\begin{equation}
		\begin{aligned}
			\beta\colon V & \to \eK^m                                               \\
			e_i           & \mapsto \big( \alpha(e_i)_1,\ldots,\alpha(e_i)_m \big).
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\begin{aligned}
			\gamma\colon \eK^m & \to W                       \\
			(x_1,\ldots,x_m)   & \mapsto \sum_{k=1}^mx_kf_k.
		\end{aligned}
	\end{equation}
	L'espace de \( \eK^m\) engendré par les colonnes de \( A\) est
	\begin{equation}
		S=\Span\{ \beta(e_i) \}_{i=1,\ldots,n}.
	\end{equation}
	Nous allons montrer que \( \gamma\) est un isomorphisme entre \( S\) et \( \Image(\alpha)\). Pour cela nous commençons par remarquer que
	\begin{equation}
		\beta(e_i)_k=\alpha(e_i)_k
	\end{equation}
	et donc que
	\begin{subequations}
		\begin{align}
			\alpha(e_i) & =\sum_k\alpha(e_i)_kf_k        \\
			            & =\sum_k\beta(e_i)_kf_k         \\
			            & =\gamma\big( \beta(e_i) \big).
		\end{align}
	\end{subequations}
	Cela montre que \( \gamma(S)=\Image(\alpha)\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooSWDMooRsLOyF}
	Soit une matrice inversible \( A\in \eM(n,\eK)\). Soit \( r\leq n\). Il existe une permutation de colonnes de \( A\) telle que la sous-matrice \( (A_{ij})_{i,j=1,\ldots,r}\) soit inversible.
	%TODOooSADMooZJVSHt. Prouver ça.
\end{lemma}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices équivalentes et semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}  \label{DefBLELooTvlHoB}
	Deux relations d'équivalence\footnote{Définition \ref{DefHoJzMp}.} entre les matrices.
	\begin{enumerate}
		\item   \label{ItemPFXCooOUbSCt}
		      Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) si il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\).
		\item
		      Deux matrices sont \defe{semblables}{matrices!similitude} si il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
	\end{enumerate}
	%TODOooURGGooAfleUw. Prouver ça.
\end{propositionDef}

\begin{lemma}   \label{LemZMxxnfM}
	Une matrice de rang\footnote{Définition~\ref{DEFooCSGXooFRzLRj}.} \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
	\begin{equation}
		J_r=\begin{pmatrix}
			\mtu_r & 0 \\
			0      & 0
		\end{pmatrix}.
	\end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
	Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\), qui existe par le lemme~\ref{LEMVecsaRgFixe}.

	Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\); ses colonnes sont donc précisément les \( f_i \), si bien que
	\begin{equation}
		APe_i=Af_i=\begin{cases}
			0      & \text{si } i>r \\
			\neq 0 & \text{sinon}.
		\end{cases}
	\end{equation}
	La matrice \( AP\) se présente donc sous la forme
	\begin{equation}
		AP=\begin{pmatrix}
			M & 0 \\
			* & 0
		\end{pmatrix}
	\end{equation}
	où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
	\begin{equation}
		QAPe_i=\begin{cases}
			e_i & \text{si } i<r \\
			0   & \text{sinon}.
		\end{cases}.
	\end{equation}
	Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{corollary}[Équivalence et rang]      \label{CorGOUYooErfOIe}
	Deux matrices sont équivalentes\footnote{Définition~\ref{DefBLELooTvlHoB}\ref{ItemPFXCooOUbSCt}.} si et seulement si elles sont de même rang.
\end{corollary}

\begin{proof}
	D'abord il y a des implicites dans l'énoncé. Puisque nous voulons, soit par hypothèse, soit par conclusion, que les matrices \( A\) et \( B\) soient équivalentes, nous supposons qu'elles ont même dimension. Soient donc \( A\) et \( B\) deux matrices carrées d'ordre \( n \).

	Par le lemme~\ref{LemZMxxnfM}, deux matrices de même rang \( r\) sont équivalentes à \( J_r\). Elles sont donc équivalentes entre elles.

	Inversement, supposons que \( A\) et \( B\) soient deux matrices équivalentes\footnote{Définition \ref{DefBLELooTvlHoB}.} : \( A=PBQ^{-1}\) avec \( P\) et \( Q\) inversibles. Alors
	\begin{subequations}
		\begin{align}
			\Image(PBQ^{-1}) & =\{ PBQ^{-1}v\tq v\in \eK^n \}                       \\
			                 & =PB\underbrace{\{ Q^{-1}v\tq v\in \eK^n \}}_{=\eK^n} \\
			                 & =P\big( B(\eK^n) \big).
		\end{align}
	\end{subequations}
	L'ensemble \( B(\eK^n)\) est un sous-espace vectoriel de \( \eK^n\). Comme le rang de \( P\) est maximum, la dimension de \( P\big( B(\eK^n) \big)\) est la même que celle de \( B(\eK^n)\). Par conséquent
	\begin{equation}
		\dim\Big( \Image(PBQ^{-1}) \Big)=\dim\big( B(\eK^n) \big)=\rank(B).
	\end{equation}
	Le membre de gauche de cela n'est autre que \( \rank(A)=\dim\big( \Image(PBQ^{-1}) \big)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithme des facteurs invariants}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Algorithme des facteurs invariants\cite{KXjFWKA}]   \label{PropPDfCqee}
	Soit \( (\eA,\delta)\) un anneau euclidien muni de son stathme  et \( U\in \eM(n \times m,\eA)\). Alors il existe \( d_1,\ldots, d_s\in \eA^*\) et des matrices \( P\in\GL(m,\eA)\), \( Q\in \GL(n,\eA)\) tels que nous ayons
	\begin{equation}
		U=P \begin{pmatrix}
			\begin{matrix}
				d_1 &        &     \\
				    & \ddots &     \\
				    &        & d_s
			\end{matrix} & 0        \\
			0                     & 0
		\end{pmatrix}Q
	\end{equation}
	avec \( d_i\divides d_{i+1}\) pour tout \( i\).
\end{proposition}
\index{anneau!euclidien!facteurs invariants}
\index{algorithme!facteurs invariants}

\begin{proof}
	Nous allons donner la preuve plus ou moins sous forme d'algorithme.

	D'abord si \( U=0\) c'est bon, on a la réponse. Sinon, nous prenons l'élément \( (i_0,j_0)\) dont le stathme est le plus petit et nous l'amenons en \( (1,1)\) par les permutations
	\begin{equation}
		\begin{aligned}[]
			C_1 & \leftrightarrow C_{j_0} \\
			L_1 & \leftrightarrow L_{i_0}
		\end{aligned}
	\end{equation}
	Ensuite nous traitons la première colonne jusqu'à amener des zéros partout en dessous de \( u_{1,1}\) de la façon suivante : pour chaque ligne successivement nous calculons la division euclidienne
	\begin{equation}
		u_{i,1}=qu_{1,1}+r_i,
	\end{equation}
	et nous faisons
	\begin{equation}
		L_i\to L_i-qL_1,
	\end{equation}
	c'est-à-dire que nous enlevons le maximum possible et il reste seulement \( r_i\) en \( u_{i1}\). Vu que le but est de ne laisser que des zéros dans la première colonne, si le reste n'est pas zéro, nous ne sommes pas contents\footnote{Si il est zéro, nous passons à la ligne suivante}. Dans ce cas nous permutons \( L_1\leftrightarrow L_i\), ce qui aura pour effet de strictement diminuer le stathme de \( u_{1,1}\) parce qu'on va mettre en \( u_{1,1}\) le nombre \( r_i\) dont le stathme est strictement plus petit que celui de \( u_{1,1}\).

	En faisant ce jeu de division euclidienne puis échange, on diminue toujours le stathme de \( u_{1,1}\), donc ça finit par s'arrêter, c'est-à-dire qu'à un certain moment, la division euclidienne de \( u_{i,1}\) par \( u_{1,1}\) va donner un reste nul et nous serons contents.

	Une fois la première colonne ramenée à la forme
	\begin{equation}
		C_1=\begin{pmatrix}
			u_{1,1} \\
			0       \\
			\vdots  \\
			0
		\end{pmatrix},
	\end{equation}
	nous faisons tout le même jeu avec la première ligne, en faisant maintenant des sommes divisions et permutations de colonnes. Notons que ce faisant, nous ne changeons plus la première colonne.

	En fin de compte, nous trouvons une matrice\footnote{Nous nommons toujours par la même lettre \( U\) la matrice originale et la matrice modifiée, comme il est d'usage en informatique.}
	\begin{equation}
		U=\begin{pmatrix}
			u_{1,1} & 0 & \ldots & 0 \\
			0       &   &        &   \\
			\vdots  &   & A      &   \\
			0       &   &        &
		\end{pmatrix}
	\end{equation}
	Si l'élément \( u_{1,1}\) ne divise pas un des éléments de \( A\), disons \( a_{i,j}\), alors nous opérons
	\begin{equation}
		C_1\to C_1-C_j.
	\end{equation}
	Cela nous détruit un peu la première colonne, mais ne change pas \( u_{1,1}\). Nous avons maintenant
	\begin{equation}
		U=\begin{pmatrix}
			u_{1,1} & 0 & \ldots & 0 \\
			0       &   &        &   \\
			*       &   &        &   \\
			u_{i,j} &   & A      &   \\
			*       &   &        &   \\
			0       &   &        &
		\end{pmatrix}
	\end{equation}
	Et nous refaisons tout le jeu depuis le début. Cependant, lorsque nous allons nous attaquer à la ligne \( i\), \( u_{1,1}\) ne divisera pas \( u_{i,j}\), ce qui donnera lieu à une division euclidienne et un échange \( L_1\leftrightarrow L_i\). L'échange consistant à mettre \( r_i\) à la place de \( u_{1,1}\) et réciproquement, diminuera encore strictement le stathme. Encore une fois, nous allons travailler jusqu'à avoir la matrice sous la forme
	\begin{equation}    \label{EqADcNVgI}
		U=\begin{pmatrix}
			u_{1,1} & 0 & \ldots & 0 \\
			0       &   &        &   \\
			\vdots  &   & A      &   \\
			0       &   &        &
		\end{pmatrix},
	\end{equation}
	sauf que cette fois, le stathme de \( u_{1,1}\) est strictement plus petit que la fois précédente. Si \( u_{1,1}\) ne divise toujours pas tous les éléments de \( A\), nous recommençons encore et encore. En fin de compte, nous finissons par avoir une matrice de la forme \eqref{EqADcNVgI} avec \( u_{1,1}\) qui divise tous les éléments de \( A\).

	Une fois que cela est fait, il faut continuer en recommençant tout sur la matrice \( A\). Nous avons maintenant
	\begin{equation}
		U=\begin{pmatrix}
			\begin{matrix}
				u_{1,1} &         \\
				        & u_{2,2}
			\end{matrix} & 0        \\
			0                    & B
		\end{pmatrix}.
	\end{equation}
	Sous cette forme nous avons \( u_{1,1}\divides u_{2,2}\) et \( u_{1,1}\) divise tous les éléments de \( B\). En effet \( u_{1,1}\) divisant tous les éléments de \( A\), il divise toutes les combinaisons de ces éléments. Or tout l'algorithme ne consiste qu'à prendre des combinaisons d'éléments.

	Nous finissons donc bien sur une matrice comme annoncée. De plus, n'ayant effectué que des combinaisons de lignes, nous avons seulement multiplié par des matrices inversibles (lemme~\ref{LemyrAXQs}).
\end{proof}
