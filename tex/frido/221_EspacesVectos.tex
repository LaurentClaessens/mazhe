% This is part of Mes notes de mathématique
% Copyright (c) 2008-2022
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les matrices et les applications linéaires sont deux choses différentes. Une application linéaire\footnote{Définition \ref{DEFooULVAooXJuRmr}.} est une application d'un espace vectoriel vers un autre, et une matrice est un simple tableau de nombres sur lesquels nous définissons des opérations, de telle sorte à fournir une structure d'espace vectoriel. Le lien entre ces opérations et les opérations correspondantes sur les applications linéaires sera fait plus tard. Voir la définition \ref{DEFooJVOAooUgGKme} et ce qui s'en suit.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

Les notions topologiques sur les espaces de matrices sont pour plus tard, à commencer par la définition \ref{DEFooCQHDooYpUAhG}.

\begin{definition}
    Soit un anneau \( \eA\) ainsi que des entiers \( m\), \( n\) strictement positifs. L'ensemble \( \eM(n\times m,\eA)\) est l'ensemble des applications
    \begin{equation}
        \{ 1,\ldots, n \}\times \{ 1,\ldots, m \}\to \eA,
    \end{equation}
    et est appelé ensemble des \defe{matrices}{matrice} \(n\times m\) sur \( \eA \).
\end{definition}
Si \( A\) est une matrice, nous notons \( A_{ij}\) au lieu de \( A(i,j)\) l'image de \( (i,j)\) par l'application \( A\).


\begin{definition}
Quelques ensembles de matrices particuliers.
  \begin{enumerate}
  \item Si \( n=m\), alors:
  \begin{itemize}
    \item nous disons que la matrice est \defe{carrée}{carrée!matrice},
    \item nous notons \( \eM(n,\eA)\) pour \( \eM(n\times n,\eA)\),
    \item \( n \) est appelée \defe{ordre}{ordre!d'une matrice carrée} de la matrice.
  \end{itemize}
  \item Si \( n = 1 \), alors la matrice est appelée \defe{matrice-ligne}{matrice-ligne}.
    \item Si \( m = 1 \), alors la matrice est appelée \defe{matrice-colonne}{matrice-colonne}.
  \end{enumerate}
\end{definition}

\begin{normaltext}
    On note les isomorphismes naturels \( \eM(1\times m,\eA) \simeq \eA^m\) et \( \eM(n\times 1,\eA) \simeq \eA^n\).
\end{normaltext}

\begin{lemmaDef}        \label{LEMooYWTEooQyLxKv}
    Nous considérons les opérations suivantes sur \( \eM(n\times m, \eA)\) :
    \begin{description}
        \item[Somme] \( (A+B)_{ij}=A_{ij}+B_{ij}\),
        \item[Produit par un scalaire] \( (\lambda \cdot A)_{ij}=\lambda A_{ij}\) pour tout \( A,B\in \eM(n\times m,\eA ) \) et \( \lambda\in \eA \).
    \end{description}
    Alors \( \big( \eM(n\times m, \eA), +,\cdot \big)\) est un \( \eA\)-module\footnote{Définition \ref{DEFooHXITooBFvzrR}}.
\end{lemmaDef}

\begin{lemmaDef}        \label{LEMooMBZTooKdGvON}
    Avec la multiplication
    \begin{equation}
        \begin{aligned}
             \eM(n\times p,\eA)\times \eM(p\times m,\eA)&\to \eM(n\times m,\eA) \\
             (A,B)&\mapsto (AB)_{ij}=\sum_{k=1}^pA_{ik}B_{kj},
        \end{aligned}
    \end{equation}
    l'espace \( \eM(n,\eK)\) est une \( \eK\)-algèbre\footnote{Définition \ref{DefAEbnJqI}.}.
\end{lemmaDef}

\begin{definition}
    Pour un élément \( A\in \eM(n\times m, \eA)\) nous définissons encore
    \begin{description}
        \item[La transposée] \( A^t_{ij}=A_{ji}\),
        \item[La trace] \( \tr(A)=\sum_iA_{ii}\).
    \end{description}
\end{definition}


\begin{remark}
    Quelques remarques directes sur les définitions.
    \begin{enumerate}
        \item
            La motivation de cette définition pour le produit apparaîtra plus loin, mais le Frido n'étant pas un livre d'introduction, j'imagine que le lecteur a déjà une idée.
        \item
            Nous verrons plus loin en \ref{SUBSECooGPXVooEYwIiJ} que la définition de transposée d'une application linéaire n'est pas tout à fait évidente; elle sera la définition \ref{DefooZLPAooKTITdd}.

            Ici nous avons bien défini la transposée d'une matrice, pas d'une application linéaire.
    \end{enumerate}
\end{remark}

\begin{remark}
    Quelques remarques à propos de structures supplémentaires.
\begin{enumerate}
    \item Nous utiliserons (presque) tout le temps des matrices à coefficients dans un corps. Il est clair que, si \( \eK \) est un corps (commutatif), alors \( \eM(n\times m,\eK) \) a une structure d'espace vectoriel sur \( \eK \).
    \item Par ailleurs, sur les matrices carrées d'ordre \( n \) fixé, le produit de deux matrices est bien défini. Ainsi, \( \eM(n,\eA)\) se voit conférer une structure d'anneau, dont le neutre pour la multiplication est la matrice carrée \( \mtu_n\) (notée aussi \( \mtu\) lorsqu'il n'y a pas d'ambiguïté sur la taille), donnée par
\begin{equation}
    \mtu_{ij}=\begin{cases}
        1    &   \text{si } i=j\\
        0    &    \text{sinon.}
    \end{cases}
\end{equation}
Il est vite vu que si \( A\) est une matrice carrée d'ordre \( n \), alors \( A\mtu=\mtu A=A\).
\end{enumerate}
\end{remark}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooUXDRooWZbMVN}
    Si \( A\), \( B\) et \( C\) sont des matrices nous avons
    \begin{enumerate}
        \item
            \( (AB)^t=B^tA^t\),
        \item       \label{ITEMooXDYQooAlnArd}
            \( \tr(ABC)=\tr(CAB)\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    La première est un simple calcul :
    \begin{equation}
        (AB)^t_{ij}=(AB)_{ji}=\sum_kA_{jk}B_{ki}=\sum_kA^t_{kj}B^t_{ik}=(B^tA^t)_{ij}.
    \end{equation}
    Pour la seconde :
    \begin{equation}
        \tr(ABC)=\sum_{ikl}A_{ik}B_{kl}C_{li}=\sum_{ikl}C_{li}A_{ik}B_{kl}=\sum_l(CAB)_{ll}=\tr(CAB).
    \end{equation}
\end{proof}

\begin{normaltext}
    La seconde égalité est importante et est nommée \defe{invariance cyclique}{invariance cyclique!trace} de la trace. Elle sert entre autres nombreuses choses à prouver que la trace d'une matrice d'une application linéaire ne dépend pas de la base choisie. Ce sera la proposition \ref{PROPooRMYQooWkEpJJ}.
\end{normaltext}

\begin{lemma}       \label{LEMooLXAHooPRyHaF}
    Soient des matrices \( A,B\in \eM(n,\eK)\). Si pour tout \( x,y\in \eK^n\) nous avons
    \begin{equation}
        \sum_{ij}A_{ij}x_iy_j=\sum_{ij}B_{ij}x_iy_j
    \end{equation}
    alors \( A=B\).
\end{lemma}

\begin{proof}
    Il suffit de choisir \( x_i=\delta_{i,k}\) et \( y_j=\delta_{j,l}\) et d'effectuer les sommes; par exemple
    \begin{equation}
        \sum_{ij}A_{ij}\delta_{i,k}y_j=\sum_jA_{kj}y_j.
    \end{equation}
    Après avoir effectué toutes les sommes nous nous retrouvons avec \( A_{kl}=B_{kl}\), ce qui signifie \( A=B\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Identifier matrices et applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Voir dans l'index thématique, \ref{SUBSECooAFPDooOzXdGz}.

Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases\footnote{C'est le théorème~\ref{ThonmnWKs} qui nous permet de considérer des bases. Et ce théorème ne fonctionne que parce que nous avons supposé une dimension finie.} \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\).

\begin{definition}      \label{DEFooJVOAooUgGKme}
    Nous considérons l'application
    \begin{equation}        \label{EQooVZQWooMyFFeO}
        \begin{aligned}
            \psi\colon \eM(n\times m, \eK)&\to \aL(E,F) \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    où \( f_A\) est définie par
    \begin{equation}        \label{EQooBVGHooJhFbMs}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}
    \end{equation}
    si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).
\end{definition}

\begin{normaltext}
    Nous allons prouver un certain nombre de résultats montrant que cette application a toutes les propriétés imaginables permettant d'identifier les matrices aux applications linéaires : elle est un isomorphisme pour toutes les structure que vous pouvez raisonnablement imaginer.

    À cette application \( \psi\) il manque cependant une propriété importante : elle n'est pas canonique. Elle dépend des bases choisies. Autrement dit : nous avons à priori autant d'applications \( \psi\) différentes qu'il y a de choix de bases sur \( E\) et \( F\)\quext{Bonne question. Est-ce qu'il y a moyen de construire deux choix de bases donnant la même application \( \psi\) ? Écrivez-moi si vous savez la réponse.}.

    Nous allons prouver maintenant quelques résultats montrant que les matrices et les applications linéaires, dans le cas des espaces vectoriels \( \eK^n\) sont deux présentations de la même chose.

    Le fait que \( \psi\) est continue sera la proposition \ref{PROPooXEQLooHvzVVm}.
\end{normaltext}

\begin{normaltext}
    Lorsque \( A\in \eM(n,\eK)\) est une matrice et \( x\in \eK^n\) un vecteur, nous notons \( Ax\) l'élément de \( \eK^n\) donné par
    \begin{equation}        \label{EQooQFVTooMFfzol}
        (Ax)_i=\sum_jA_{ij}x_j.
    \end{equation}
    Autrement dit, \( Ax=f_A(x)\).

    Cette convention et de nombreuses autres à propos de matrice sera rappelée dans \ref{SECooBTTTooZZABWA}.
\end{normaltext}

\begin{propositionDef}      \label{PROPooGXDBooHfKRrv}
    Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\).

    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon \eM(n\times m, \eK)&\to \aL(E,F) \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    où \( f_A\) est définie par
    \begin{equation}        \label{EQooZKEKooNYjvhP}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}
    \end{equation}
    si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).

    Alors
    \begin{enumerate}
        \item       \label{ITEMooKZYYooZPTkpq}
            Nous avons
            \begin{equation}
                f_A(e_i)_{\alpha}=A_{\alpha i}.
            \end{equation}
        \item       \label{ITEMooANXFooGIuxUR}
            Nous avons
            \begin{equation}                \label{EQooOKOJooYgteNP}
                f_A(e_i)=\sum_{\alpha}A_{\alpha i}f_{\alpha}.
            \end{equation}
        \item       \label{ITEMooXLLLooKfigfB}
            Nous avons
            \begin{equation}        \label{EQooAXRJooUwHbjB}
                \big( f_A(x) \big)_{\alpha}=\sum_{i}A_{\alpha i}x_i.
            \end{equation}
        \item       \label{ITEMooHSMLooRJZref}
            L'application \( \psi\) est une bijection.
    \end{enumerate}
    Si \( f\) est une application linéaire, alors la matrice \( \psi^{-1}(f)\) est la \defe{matrice associée}{matrice d'une application linéaire} à \( f\) dans les bases choisies.
\end{propositionDef}

Remarque : les bases ne sont supposées être canoniques en aucun sens du terme. Les dimensions de \( E\) et \( F\) ne sont pas non plus supposées identiques.

\begin{proof}
    En nous rappelant que \( (e_j)_i=\delta_{i,j}\) nous avons
    \begin{equation}        \label{EQooWGZHooIBoygB}
        f_A(e_j)=\sum_{i\alpha}A_{\alpha i}(e_j)_if_{\alpha}=\sum_{\alpha}A_{\alpha j}f_{\alpha},
    \end{equation}
    donc \( f_A(e_i)_{\alpha}=A_{\alpha i}\). Cela prouve la formule du point \ref{ITEMooKZYYooZPTkpq}.

    Le point \ref{ITEMooANXFooGIuxUR} est une simple somme sur \( \alpha\) de \ref{ITEMooKZYYooZPTkpq}.

    La formule \eqref{ITEMooXLLLooKfigfB} est simplement la composante \( f_{\alpha}\) de la définition \ref{EQooZKEKooNYjvhP}.

    Prouvons que \( \psi\) est injective. Si \( f_A=f_B\), nous avons en particulier \( f_A(e_i)_{\alpha}=f_B(e_i)_{\alpha}\) et donc \( A_{\alpha i}=B_{\alpha i}\).

    Prouvons que \( \psi\) est surjective. Pour cela nous considérons \( f\in \aL(E,F)\) et nous posons \( A_{\alpha i}=f(e_i)_{\alpha}\). Nous avons alors \( f=f_A\) parce que
    \begin{equation}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}=\sum_{i\alpha}f(e_i)_{\alpha}x_if_{\alpha}=\sum_{\alpha}f(\sum_ix_ie_i)_{\alpha}f_{\alpha}=\sum_{\alpha}f(x)_{\alpha}f_{\alpha}=f(x).
    \end{equation}
\end{proof}

La proposition suivante montre que le produit matriciel correspond à la composition d'applications linéaires, pourvu que l'on travaille avec les bases canoniques sur \( \eK^n\).
\begin{proposition}[\cite{MonCerveau}]      \label{PROPooIYVQooOiuRhX}
    Soit un corps commutatif \( \eK\). Nous considérons des espaces vectoriels \( E\) et \( F\) munis de bases \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_{\alpha}\}_{\alpha\in 1,\ldots, m} \).

    L'application déjà définie\footnote{Notez la position du \( n\) et du \( m\). Sachez noter les bornes des sommes écrites dans la démonstration.}
    \begin{equation}
        \psi\colon \eM(m\times n,\eK)\to \aL(E,F)
    \end{equation}
    est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Le fait que \( \psi\) soit une bijection est la proposition \ref{PROPooGXDBooHfKRrv}. Nous devons montrer que c'est linéaire.

    Pour \( \lambda\in \eK\) nous avons le calcul
    \begin{equation}
        \psi(\lambda A)(e_k)=f_{\lambda A}(e_k)=\sum_{\alpha i}(\lambda A)_{\alpha i}\underbrace{(e_k)_i}_{=\delta_{k,i}}f_{\alpha}=\lambda\sum_{\alpha}A_{\alpha k}f_{\alpha}=\lambda f_A(e_j).
    \end{equation}
    Donc \( \psi(\lambda A)=\lambda\psi(A)\).

    Si \( A,B\in \eM(n,\eK)\) nous avons de la même façon \( f_{A+B}=f_A+f_B\).
\end{proof}

\begin{proposition}     \label{PROPooCSJNooEqcmFm}
    Soient des espaces vectoriels \( E\), \( F\) et \( G\) de dimensions \( n\), \( m\) et \( p\) munis de bases\footnote{Avec trois, nous renonçons à utiliser des alphabets différents pour numéroter les éléments des bases.} \( \{ e_i \}\), \( \{ f_i \}\) et \( \{ g_i \}\). Nous considérons les applications
    \begin{subequations}
        \begin{align}
        \psi&\colon \eM(m\times n,\eK)\to \aL(E,F)\\
        \psi&\colon \eM(p\times m,\eK)\to \aL(F,G)\\
        \psi&\colon \eM(p\times n,\eK)\to \aL(E,G).
        \end{align}
    \end{subequations}
    Nous avons
    \begin{equation}
        \psi(A)\circ \psi(B)=\psi(AB)
    \end{equation}
    pour toutes matrices \( A\in \eM(p\times m,\eK)\) et \( B\in \eM(m\times n,\eK)\).
\end{proposition}

\begin{proof}
    Nous considérons les applications linéaires associées à \( A\) et \( B\) : \( f_A\colon F\to G\) et \( f_B\colon E\to F\) et la composée \( f_A\circ f_B\colon E\to G\). Et puis c'est le calcul :
    \begin{subequations}
        \begin{align}
            (f_A\circ f_B)(e_k)&=f_A\big( \sum_{ij}B_{ij}(e_k)_jf_i \big)\\
            &=\sum_i B_{ik}f_A(f_i)\\
            &=\sum_iB_{ik}\sum_{rs}A_{rs}(f_i)_sg_r\\
            &=\sum_{ir}B_{ik}A_{ri}g_r\\
            &=\sum_r(AB)_{rk}g_r\\
            &=f_{AB}(e_k).
        \end{align}
    \end{subequations}
    Donc \( f_A\circ f_B=f_{AB}\) comme il se doit.
\end{proof}

Nous pouvons particulariser au cas où \( E=F=G\).
\begin{proposition}     \label{PROPooFMBFooEVCLKA}
    Si \( E\) est un espace vectoriel muni d'une base \( \{ e_i \}\), alors l'application
    \begin{equation}
        \psi\colon \eM(n,\eK)\to \End(E)
    \end{equation}
    est un isomorphisme d'algèbre\footnote{Définition \ref{DefAEbnJqI}.} et d'anneaux\footnote{Définition \ref{DEFooSPHPooCwjzuz}}.
\end{proposition}

\begin{proof}
    Le fait que \( \psi\) soit un isomorphisme d'algèbre est juste la combinaison entre les propositions \ref{PROPooIYVQooOiuRhX} et \ref{PROPooCSJNooEqcmFm}.

    En ce qui concerne l'isomorphisme d'anneaux, il faut en plus identifier les neutres. Le neutre pour la composition d'applications linéaires est l'application identité et le neutre pour la multiplication de matrices est la matrice identité. Nous devons donc montrer que \( \psi(\delta)=\id\). Juste un calcul :
    \begin{equation}
        f_{\delta}(x)=\sum_{ij}\delta_{i,j}x_je_i=\sum_ix_ie_i=x.
    \end{equation}
    Donc oui, \( f_{\delta}\) est l'identité.
\end{proof}


Le fait que \( \psi\) est continue sera la proposition \ref{PROPooXEQLooHvzVVm}.


Voilà. Soyez bien conscient que l'application \( \psi\) dont nous avons beaucoup parlé est surtout intéressante dans le cas des espaces de la forme \( \eK^n\). Dans ce cas, nous avons une identification canonique entre \( \eM(n,\eK)\) et \( \End(\eK^n)\) qui est un isomorphisme d'anneaux et d'algèbres.

Nous verrons que ce \( \psi\) respecte encore les inverses\footnote{Proposition \ref{PROPooNPMCooPmaCwu}.} et les déterminants\footnote{Proposition \ref{PROPooFKDXooKMSolt}.}.

\begin{normaltext}
    Il convient de ne pas confondre matrice et application linéaire (bien que nous le ferons sans vergogne). Une matrice est un bête tableau de nombres, tandis qu'une application linéaire est une application entre deux espaces vectoriels vérifiant certaines propriétés.

    Cependant si les espaces vectoriels \( E\) et \( F\) sont munis de bases, alors il y a une application
    \begin{equation}
        \psi\colon \eM(m\times n,\eK)\to \aL(E,F)
    \end{equation}
    qui a toutes les propriétés imaginables\footnote{Et elle en aura encore plus lorsque nous aurons vus les déterminants.}.

    Cette application dépend des bases choisies. Il n'y a donc pas de trucs comme «la matrice de telle application linéaire» ou comme «voici une matrice, nous considérons l'application linéaire associée».

    Cependant, sur des espaces comme \( \eR^n\) ou plus généralement sur \( \eK^n\), nous avons une base canonique et toute personne raisonnable utilise toujours la base canonique (sauf mention du contraire). Dans ces cas il est sans danger de dire «la matrice associée à telle application linéaire» sans préciser les bases.

    Mais si un jour vous utilisez une base autre que la base canonique sur \( \eR^n\), précisez-le et plutôt deux fois qu'une\footnote{Au passage, non, les coordonnées polaires ne sont pas une base de \( \eR^2\). C'est un système de coordonnées, et ce n'est pas la même chose.}.
\end{normaltext}

\begin{normaltext}
    Tant que nous sommes à parler de matrice et d'applications linéaires, les plus acharnés anti-abus de language\footnote{Dont l'auteur de ces lignes fait partie.} remarqueront qu'il n'est pas vrai que «étant donné une base, une application linéaire a une matrice».

    En effet, une base est une partie libre et génératrice (définition \ref{DEFooNGDSooEDAwTh}). Or une partie d'un ensemble n'est pas muni d'un ordre. Toutes les permutations de colonnes de la matrice sont encore possible d'après l'ordre que l'on met sur les vecteurs de la base.

    Encore une fois, la base canonique n'a pas de problèmes parce que les \( \{ e_i \}\) de \( \eR^n\) viennent avec un ordre indiscutable. Plus généralement, très souvent, lorsqu'on construit une base, la construction suggère un ordre.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYCKRooTrajdP}
    Si \( A\in\eM(n,\eK)\) nous définissons le \defe{déterminant}{déterminant!matrice} de \( A\) par la formule
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}(-1)^{\sigma}\prod_{i=1}^nA_{i\sigma(i)}
    \end{equation}
    où la somme est effectuée sur tous les éléments du groupe symétrique\footnote{Pour le groupe symétrique, c'est la définition \ref{DEFooJNPIooMuzIXd}, le fait que ce soit un groupe fini est le lemme \ref{LEMooSGWKooKFIDyT}, et pour la somme sur un groupe fini c'est la définition \ref{DEFooLNEXooYMQjRo}.} \( S_n\) et où \( (-1)^{\sigma}\) représente la parité de la permutation \( \sigma\).
\end{definition}
En se souvenant que \( | S_n |=n!\), nous sommes frappés de stupeur devant le fait que le nombre de termes dans la somme croît de façon factorielle (c'est plus qu'exponentiel, pour info) en la taille de la matrice. Cette formule est donc sans espoir pour une matrice plus grande que \( 3\times 3\) ou à la rigueur \( 4\times 4\) à la main. À l'ordinateur, il est possible de monter plus haut, mais pas tellement.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant en petite dimension}
%---------------------------------------------------------------------------------------------------------------------------

En dimension deux, le déterminant de la matrice
    $\begin{pmatrix}
        a   &   b       \\
        c   &   d
    \end{pmatrix}$
est le nombre
\begin{equation}        \label{EQooQRGVooChwRMd}
     \det\begin{pmatrix}
        a   &   b       \\
        c   &   d
     \end{pmatrix}=\begin{vmatrix}
        a   &   b       \\
        c   &   d
    \end{vmatrix}=ad-cb.
\end{equation}
Ce nombre détermine entre autres le nombre de solutions que va avoir le système d'équations linéaires associé à la matrice.

Pour une matrice \( 3\times 3\), nous avons le même concept, mais un peu plus compliqué; nous avons la formule
\begin{equation}
    \det
    \begin{pmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}
    \end{pmatrix}
    =
    \begin{vmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}
    \end{vmatrix}=
    a_{11}\begin{vmatrix}
        a_{22}    &   a_{23}  \\
        a_{32}    &   a_{33}
    \end{vmatrix}-
    a_{12}\begin{vmatrix}
        a_{21}    &   a_{23}  \\
        a_{31}    &   a_{33}
    \end{vmatrix}+
    a_{13}\begin{vmatrix}
        a_{21}    &   a_{22}  \\
        a_{31}    &   a_{32}
    \end{vmatrix}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Manipulations de lignes et de colonnes}
%---------------------------------------------------------------------------------------------------------------------------

Nous voudrions savoir ce qu'il se passe avec le déterminant d'une matrice lorsque nous substituons à une ligne ou une colonne, une combinaison des autres lignes et colonnes. Lorsqu'une matrice est donnée, nous notons \( C_j\) sa \( j\)\ieme\ colonne.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooRSJTooQEoOtN}
    Si \( A\) et \( B\) sont des matrices, alors
    \begin{equation}
        (AB)^t=B^tA^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Il suffit de calculer les éléments de matrice :
    \begin{equation}
        (AB)^t_{ij}=(AB)_{ji}=\sum_k A_{jk}B_{ki}=\sum_kB^t_{ik}A^t_{kj}=(B^tA^t)_{ij}.
    \end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau,ooKYTYooJlzZMp}]        \label{LEMooCEQYooYAbctZ}
    Si \( A\) est une matrice, alors \( \det(A)=\det(A^t)\).
\end{lemma}

\begin{proof}
    Nous commençons par écrire la définition du déterminant :
    \begin{equation}
        \det(A^t)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n(A^t)_{i,\sigma(i)}=\sum_{\sigma}\epsilon(\epsilon)\prod_iA_{\sigma(i),i}.
    \end{equation}
    Pour chaque \( \sigma\) séparément, en utilisant la proposition \ref{PROPooQMUDooQQVRIe} pour ré-indexer le produit :
    \begin{equation}
        \prod_i A_{\sigma(i),i}=\prod_iA_{i,\sigma^{-1}(i)}.
    \end{equation}
    Nous profitons du fait que l'application \( \varphi\colon S_n\to S_n\) donnée par \( \varphi(\sigma)=\sigma^{-1}\) soit une permutation de \( S_n\) pour appliquer la définition \ref{DEFooLNEXooYMQjRo} et faire la somme sur \( \sigma^{-1}\) :
    \begin{equation}
        \det(A^t)=\sum_{\sigma}\epsilon(\sigma)\prod_iA_{i,\sigma^{-1}(i)}=\sum_{\sigma}\epsilon(\sigma^{-1})\prod_iA_{i,\sigma(i)}=\det(A)
    \end{equation}
    où nous avons utilisé le fait que \(\epsilon(\sigma^{-1})=\epsilon(\sigma)\) (corolaire \ref{CORooZLUKooBOhUPG}).
\end{proof}

Le fait que \( \det(A)=\det(A^t)\) permet, dans toutes les propositions du type «ce qui arrive au déterminant si on change telle ligne ou colonne» de ne donner qu'une preuve pour la partie «ligne» et déduire automatiquement le cas «colonne». Le lemme suivant donne un exemple d'utilisation.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooWMQWooGWFlmC}
    Soit une matrice \( A\). Nous considérons la matrice \( B\) obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\) ainsi que la matrice \( C\) obtenue à partir de \( A^t\) par la permutation de colonnes \( C_k\leftrightarrow C_l\).

    Alors \( C^t=B\).
\end{lemma}

\begin{proof}
    Calculons les éléments de matrice de \( C\) :
    \begin{equation}
        C_{ij}=\begin{cases}
            (A^t)_{ij}    &   \text{si }  j\neq k, j\neq l\\
            (A^t)_{ik}    &   \text{si } j=l\\
            (A^t)_{il}    &    \text{si }j=k
        \end{cases}=
        \begin{cases}
            A_{ji}    &   \text{si }  j\neq k, j\neq l\\
            A_{ki}    &   \text{si } j=l\\
            A_{li}    &    \text{si }j=k.
        \end{cases}
    \end{equation}
    Ensuite nous prouvons que \( C^t=B\) en écrivant les éléments de \( C^t\) :
    \begin{equation}
        (C^t)_{ij}=C_{ji}=\begin{cases}
            A_{ij}    &   \text{si } i\neq k, i\neq l\\
            A_{kj}    &   \text{si } i=l\\
            A_{lj}    &    \text{si }i=k.
        \end{cases}
    \end{equation}
    Cette dernière expression est la matrice \( A\) après permutation des lignes \( L_k\leftrightarrow L_l\), c'est-à-dire la matrice \( B\).
\end{proof}

Pour la suite nous écrivons \( \delta\) la matrice «identité», c'est-à-dire celle dont les entrées sont précisément les \( \delta_{i,k}\).  Nous écrivons également \( E_{ij}\) la matrice contenant des zéros partout sauf en \( (i,j)\) où elle a un \( 1\), c'est-à-dire
\begin{equation}
    (E_{ij})_{kl}=\delta_{i,k}\delta_{j,l}.
\end{equation}

\begin{proposition}[Permuter des lignes ou des colonnes \( L_k\leftrightarrow L_l\)\cite{ooKBOMooSkKHvu,MonCerveau}]    \label{PROPooFQRDooRPfuxk}
    Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\).
    \begin{enumerate}
        \item   \label{ITEMooAIHWooHXzeys}
            Si \( B\) est la matrice obtenue à partir de \( A\) en permutant deux lignes ou deux colonnes, alors
            \begin{equation}
                \det(A)=-\det(B).
            \end{equation}
        \item  \label{ITEMooDNHWooOMgmxa}
            Si \( B\) est la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\). Alors
            \begin{equation}
                B=SA
            \end{equation}
            avec \( S=\delta+E_{kl}+E_{lk}-E_{kk}-E_{ll}\).

            Autrement dit : la matrice \( S\) est une matrice de permutations de lignes.
        \item \label{ITEMooSHRQooQrqVdO}
            La matrice \( S\) vérifie \( \det(S)=-1\)
        \item       \label{ITEMooQXSEooMWiKbL}
            Nous avons
            \begin{equation}
                \det(SA)=\det(S)\det(A).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point
    \begin{subproof}
    \item[\ref{ITEMooAIHWooHXzeys} pour les colonnes]

    Soient \( k\) et \( l\) fixés, et considérons la permutation des colonnes \( C_k\) et \( C_l\). Nous notons \( \alpha\) la permutation \( (kl)\) dans \( S_n\) (groupe symétrique, définition \ref{DEFooJNPIooMuzIXd}). Nous avons
    \begin{equation}
        B_{ij}=A_{i \alpha(j)},
    \end{equation}
    ou encore : \( A_{ij}=B_{i\alpha(j)}\). Par définition,
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_{n}}\epsilon(\sigma)\prod_{i=1}^nA_{i\sigma(i)}
    \end{equation}
    C'est le moment d'utiliser la proposition \ref{PROPooWJQQooFINSEc} à propos de somme sur des groupes avec \( G=S_n\), \( h=\alpha\) et
    \begin{equation}
        f(\sigma)=\epsilon(\sigma)\prod_iA_{i,\sigma(i)}.
    \end{equation}
    Nous savons que \( \epsilon(\alpha)=-1\) et que \( \epsilon\) est un homomorphisme par la proposition \ref{ProphIuJrC}\ref{ITEMooBQKUooFTkvSu}, donc
    \begin{equation}
        f(\alpha \sigma)=\epsilon(\alpha\sigma)\prod_iA_{i,(\alpha\sigma)(i)}=-\epsilon(\sigma)\prod_iB_{i,\sigma(i)}.
    \end{equation}
    Avec ça, nous concluons :
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}f(\sigma)=\sum_{\sigma}f(\alpha \sigma)=-\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}=-\det(B).
    \end{equation}
    \item[\ref{ITEMooAIHWooHXzeys} pour les lignes]

    Que se passe-t-il si nous permutons les lignes \( L_k\) et \( L_{l}\) ? Si nous notons \( B'\) la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\), et \( C\) celle obtenue de \( A^t\) après permutation de colonnes \( C_k\leftrightarrow C_l\) alors nous avons \( C^t=B'\). Le lemme \ref{LEMooWMQWooGWFlmC} nous dit que \( C^t=B'\). En utilisant le lemme \ref{LEMooCEQYooYAbctZ} sur le déterminant de la transposée,
    \begin{equation}
        \det(B')=\det(C^t)=\det(C)=-\det(A^t)=-\det(A).
    \end{equation}
    Voilà qui prouve le résultat pour les permutation de lignes.

\item[\ref{ITEMooDNHWooOMgmxa}]
    Si \( k=l\), il n'y a pas de permutation, et il est vite vu que la matrice \( S\) est l'identité parce qu'il y a quatre fois le terme \( E_{kk}\). Nous supposons donc que \( k\neq l\); en particulier \( \delta_{k,l}=0\).

    Il s'agit surtout d'un beau calcul :
    \begin{subequations}
        \begin{align}
            (SA)_{ij}=\sum_{m}S_{im}A_{mj}  & =A_{ij}+\sum_m(\delta_{k,i}\delta_{l,m}+\delta_{l,i}\delta_{l,m}-\delta_{k,i}\delta_{k,m}-\delta_{l,i}\delta_{l,m})A_{mj}  \\
                                            & =A_{ij}+\delta_{k,i}A_{lj}+\delta_{l,i}A_{kj}-\delta_{k,i}A_{kj}-\delta_{l,i}A_{lj}.
        \end{align}
    \end{subequations}
    Si \( i\neq j\) et \( i\neq l\), alors \( (SA)_{ij}=A_{ij}\). Si \( i=k\), alors
    \begin{equation}
        (SA)_{kj}=A_{kj}+A_{lj}-A_{kj}=A_{lj},
    \end{equation}
    c'est-à-dire que la \( k\)\ieme\ ligne de \( SA\) est la \( l\)\ieme\ ligne de \( A\).

    Avec \( i=l\) nous obtenons la \( k\)\ieme\ ligne de \( A\).

    Tout cela montre que \( SA\) est la matrice \( A\) dans laquelle les lignes \( k\) et \( l\) ont été inversées, c'est-à-dire \( SA=B\).

\item[\ref{ITEMooSHRQooQrqVdO}]
            En utilisant la définition du déterminant,
            \begin{subequations}
                \begin{align}
                    \det(S) & =\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nS_{i\sigma(i)}      \\
                            & =\sum_{\sigma}\epsilon(\sigma)\prod_i\big( \delta_{i,\sigma(i)}+\delta_{k,i}\delta_{l,\sigma(i)}+\delta_{l,i}\delta_{k,\sigma(i)}-\delta_{k,i}\delta_{k,\sigma(i)}-\delta_{l,i}\delta_{l,\sigma(i)} \big).
                \end{align}
            \end{subequations}
            Nous utilisons l'associativité et la commutativité du produit pour séparer les facteurs \( i=k\) et \( i=l\) des autres :
            \begin{equation}
                \det(S)=\sum_{\sigma}\epsilon(\sigma)\prod_{\substack{i\neq k \\i\neq l}}\delta_{i,\sigma(i)}(\delta_{k,\sigma(k)}+\delta_{l,\sigma(k)}-\delta_{k,\sigma(k)})(\delta_{l,\sigma(l)}+\delta_{k,\sigma(l)}-\delta_{l,\sigma(l)}).
            \end{equation}
            À cause des facteurs \( i\neq k\) et \( i\neq l\), les \( \sigma\) pour lesquels le tout n'est pas nul doivent vérifier \( \delta_{i,\sigma(i)}=1\) pour tout \( i\) différent de \( k\) et \( l\). Les deux seuls sont donc \( \sigma=\id\) et la permutation \( \sigma=(k,l)\). Pour \( \sigma=\id\), nous avons
            \begin{equation}
                \prod_{\substack{i\neq k \\i\neq l}}\delta_{i,i}(\delta_{k,k}+\delta_{l,k}-\delta_{k,k})(\delta_{l,l}+\delta_{k,l}-\delta_{l,l})=0.
            \end{equation}
            Dernier espoir : \( \sigma=(k,l)\). Pour ce terme nous avons \( \epsilon(\sigma)=-1\) et
            \begin{equation}
                \prod_{\substack{i\neq k \\i\neq l}}\delta_{i,i}(\delta_{k,l}+\delta_{l,l}-\delta_{k,l})(\delta_{l,k}+\delta_{k,k}-\delta_{l,k})=1.
            \end{equation}
            Au final dans \( \det(S)\) il n'y a que le terme \( \sigma=(k,l)\) qui est non nul, et il vaut \( -1\). Donc
            \begin{equation}
                \det(S)=-1.
            \end{equation}
        \item[\ref{ITEMooQXSEooMWiKbL}]
            Il s'agit de mettre bout à bout les points déjà prouvés :
            \begin{equation}
                \det(SA)=-\det(A)=\det(S)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

\begin{corollary}[\cite{ooKBOMooSkKHvu}]        \label{CORooAZFCooSYINvBl}
    Soit une matrice \( A\in \eM(n,\eK)\). Si deux lignes ou deux colonnes de \( A\) sont égales, alors \( \det(A)=0\).
\end{corollary}

\begin{proof}
    Si deux colonnes sont égales, la matrice ne change pas lorsqu'on les permute, alors que le déterminant change de signe. La seule possibilité est que \( \det(A)=-\det(A)\), ce qui signifie que \( \det(A)=0\).
\end{proof}
Notons que si pour \( k\neq l\) nous avons \( C_k=\lambda C_l\), alors nous avons aussi \( \det(A)=0\).

La réciproque n'est pas vraie : il existe des matrices dont le déterminant est nul et dont aucune entrée n'est nulle. Par exemple
\begin{equation}
    \begin{pmatrix}
        1    &   2    \\
        1    &   2
    \end{pmatrix}.
\end{equation}


\begin{proposition}[\cite{ooKBOMooSkKHvu}]      \label{PROPooNGZJooHjtMyn}
    Soient \( A\in \eM(n,\eK)\), et \( v\in \eK^n\). Si \( B\) est la matrice \( A\) avec la substitution \( L_j\to L_j+v\) et \( C\) est la matrice \( A\) avec la substitution \( L_j\to v\), alors
    \begin{equation}
        \det(B)=\det(A)+\det(C).
    \end{equation}
\end{proposition}

\begin{proof}
    En utilisant l'associativité de la multiplication,
    \begin{subequations}
        \begin{align}
            \det(B)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}\\
            &=\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}B_{i\sigma(i)} \big)B_{j\sigma(j)}\\
            &=\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}A_{i\sigma(i)} \big)(A_{j\sigma(j)}+v_{\sigma(j)})\\
            &=\sum_{\sigma}\epsilon(\sigma)\prod_iA_{i\sigma(i)}+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i\sigma(i)}v_{\sigma(j)}         \label{SUBEQooKATCooVIbEpv}\\
            &=\det(A)+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i\sigma(i)}C_{j\sigma(j)}  \label{SUBEQooCOTDooPPrEYJ}\\
            &=\det(A)+\det(C).
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item \ref{SUBEQooKATCooVIbEpv} parce que pour \( i\neq j\) nous avons \( A_{i\sigma(i)}=C_{i\sigma(i)}\)
        \item \ref{SUBEQooCOTDooPPrEYJ} parce que \( v_{\sigma(j)}=C_{j\sigma(j)}\).
    \end{itemize}
\end{proof}

\begin{proposition}[Combinaison de lignes ou colonnes \( L_k\to L_k+\lambda L_l\)\cite{ooKBOMooSkKHvu}]     \label{PROPooPYNHooLbeVhj}
    Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\).
    \begin{enumerate}
        \item       \label{ITEMooJSRDooTggEyO}
            Si \( B\) est la matrice obtenue à partir de \( A\) par la substitution \( L_k\to L_k+\lambda L_l\) ou \( C_k\to C_k+\lambda C_l\), alors
            \begin{equation}
                \det(A)=\det(B).
            \end{equation}
        \item   \label{ITEMooHKZWooVZDgnf}
            Si \( B\) est la matrice \( A\) dans laquelle nous avons fait la substitution \( L_k\to L_k+\lambda L_l\), alors
            \begin{equation}
                B=UA
            \end{equation}
            avec \( U=\delta+\lambda E_{kl}\), c'est-à-dire que \( U\) est une matrice de combinaison de lignes.
        \item           \label{ITEMooPGYJooWTTghT}
            La matrice \( U\) vérifie \( \det(U)=1\).
        \item       \label{ITEMooBBEAooZJVGNV}
            Nous avons
            \begin{equation}
                \det(UA)=\det(U)\det(A).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
    \item[\ref{ITEMooJSRDooTggEyO}]
        Soit la matrice \( C\) obtenue à partir de \( A\) par \( L_k\to \lambda L_l\). En considérant le vecteur \( v=\lambda L_l\), nous sommes dans la situation de la proposition \ref{PROPooNGZJooHjtMyn}. Donc
        \begin{equation}
            \det(B)=\det(A)+\det(C).
        \end{equation}
        Mais dans la matrice \( C\), nous avons \( L_k=\lambda L_l\), ce qui implique \( \det(C)=0\) par le corolaire \ref{CORooAZFCooSYINvBl}. Donc \( \det(A)=\det(B)\) comme il se devait.
    \item[\ref{ITEMooHKZWooVZDgnf}]
        Encore un calcul :
        \begin{equation}
            (UA)_{ij}=\sum_m\big( \delta_{i,m}+\lambda(E_{kl})_{im} \big)A_{mj}=A_{ij}+\lambda\sum_m\delta_{k,i}\delta_{l,m}A_{mj}=A_{ij}+\lambda \delta_{l,i}A_{kj}.
        \end{equation}
        Cela donne, pour \( i=k\) la ligne
        \begin{equation}
            (UA)_{kj}=A_{kj}+\lambda A_{lj},
        \end{equation}
        ce qui correspond bien à \( L_k\to L_k+\lambda L_l\).

    \item[\ref{ITEMooPGYJooWTTghT}]
            Nous calculons le déterminant de \( U=\delta+\lambda E_{kl}\) avec \( k\neq l\). Nous avons dans un premier temps :
            \begin{equation}
                \det(U)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n(\delta_{i,\sigma(i)}+\lambda \delta_{k,i}\delta_{l,\sigma(i)}).
            \end{equation}
            Puisque nous avons toujours \( \delta_{k,i}\delta_{l,i}=0\), le terme \( \sigma=\id\) donne \( 1\).

            Pour les \( \sigma\neq \id\), le facteur \( \lambda\delta_{k,i}\delta_{l,\sigma(i)}\) ne s'annule pas, uniquement si \( i=k\) et \( \sigma(i)=k\). Donc le seul terme non nul autre que \( \sigma=\id\) peut provenir de \( \sigma=(k,l)\). Pour ce terme, nous isolons les termes \( i=l\) et \( i=k\) :
            \begin{equation}
                (\delta_{k,\sigma(k)}+\lambda\delta_{k,k}\delta_{k,\sigma(k)})(\delta_{l,\sigma(l)}+\lambda\delta_{k,l}\delta_{k,\sigma(l)}).
            \end{equation}
            Le dernier facteur est nul.
        \item[\ref{ITEMooBBEAooZJVGNV}]
            En mettant bout à bout les résultats prouvés,
            \begin{equation}
                \det(UA)=\det(A)=\det(U)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

\begin{proposition}[Multiplication par un scalaire d'une ligne ou colonne \( L_k\to \lambda L_k\)\cite{ooKBOMooSkKHvu}] \label{PROPooXUFKooOaPnna}
    Soient une matrice \( A\in \eM(n,\eK)\), un entier \( k\neq l\) inférieur ou égal à \( n\). Soit la matrice \( B\) obtenue à partir de \( A\) en multipliant la ligne \( L_k\) par \( \lambda\in \eK\).
    \begin{enumerate}
        \item       \label{ITEMooBKIGooCDQEDt}
            \( \det(B)=\lambda\det(A)\)
        \item       \label{ITEMooWRRCooFXkRNW}
            En considérant la matrice \( T=\delta+(\lambda-1)E_{kk}\), nous avons
            \begin{equation}
                B=TA,
            \end{equation}
            c'est-à-dire que la matrice \( T\) est une matrice de multiplication de ligne par un scalaire.
        \item       \label{ITEMooOGGDooPVVRzk}
            Nous avons \( \det(T)=\lambda\).
        \item       \label{ITEMooIFRVooWQYgkK}
            Et aussi : \( \det(TA)=\det(T)\det(A)\)
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[\ref{ITEMooBKIGooCDQEDt}]
            La matrice \( B\) est donnée par les éléments
            \begin{equation}
                B_{ij}=\begin{cases}
                    A_{ij}    &   \text{si } j\neq k\\
                    \alpha A_{ij}    &    \text{si } j=kn
                \end{cases}
            \end{equation}
            c'est-à-dire \( B_{ij}=\big( 1+(\alpha-1)\delta_{j,k} \big)A_{ij}\). Nous mettons cela dans la définition du déterminant de \( B\) :
            \begin{equation}        \label{EQooGVMTooPntKew}
                \det(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}=\sum_{\sigma}\prod_i\big( 1+(\alpha-1)\delta_{\sigma(i),k}A_{i\sigma(i)} \big).
            \end{equation}
            L'associativité du produit dans \( \eK\) nous permet de séparer le produit de la façon suivante :
            \begin{equation}
                \prod_{i=1}^n\big( 1+(\alpha-1)\delta_{\sigma(i),k} \big)A_{i\sigma(i)}=\prod_i\big( 1+(\lambda-1)\delta_{\sigma(i),k} \big)\prod_iA_{i\sigma(i)}=\lambda\prod_iA_{i\sigma(i)}.
            \end{equation}
            En remettant dans \eqref{EQooGVMTooPntKew}, nous trouvons \( \det(B)=\det(A)\).
        \item[\ref{ITEMooWRRCooFXkRNW}]
            C'est un cas particulier de la proposition \ref{PROPooPYNHooLbeVhj}\ref{ITEMooHKZWooVZDgnf} en prenant \( k=l\) et en adaptant le \( \lambda\).
        \item[\ref{ITEMooOGGDooPVVRzk}]
            Nous calculons le déterminant de la matrice \( T=\delta+(\lambda-1)E_{kk}\). La formule du déterminant donne
            \begin{equation}
                \det(T)=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^n\big( \delta_{i,\sigma(i)}+(\lambda-1)\delta_{k,i}\delta_{k,\sigma(i)} \big).
            \end{equation}
            Si \( i\neq \sigma(i)\), alors non seulement \( \delta_{i,\sigma(i)}=0\), mais en plus \( \delta_{k,i}\delta_{k,\sigma(i)}=0\). Donc seul \( \sigma=\id\) reste dans la somme sur \( \sigma\in S_n\). Il reste donc
            \begin{equation}
                \det(T)=\prod_{i=1}^n\big( 1+(\lambda-1)\delta_{k,i} \big)=\left( \prod_{i\neq k}1 \right)(1+(\lambda-1))=\lambda
            \end{equation}
            où nous avons utilisé encore l'associativité pour isoler le facteur \( i=k\).
        \item[\ref{ITEMooIFRVooWQYgkK}]
            Il faut mettre bout à bout les résultats déjà établis :
            \begin{equation}
                \det(TA)=\lambda\det(A)=\det(T)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu les matrices d'opérations élémentaire sur les lignes et colonnes :
\begin{itemize}
    \item Permutation de lignes \( L_k\leftrightarrow L_l\)  : \( S(n;k,l)=\delta+E_{kl}+E_{lk}-E_{kk}-E_{ll}\), proposition \ref{PROPooFQRDooRPfuxk}.
    \item Combinaisons de lignes \( L_k\to L_k+\lambda L_l\) : \( U(n;k,l,\lambda)=\delta+\lambda E_{kl}\), proposition \ref{PROPooPYNHooLbeVhj}.
    \item Multiplication d'une ligne par un scalaire \( L_k\to \lambda L_k\) : \( T=\delta+(\lambda-1)E_{kk}\), proposition \ref{PROPooXUFKooOaPnna}.
\end{itemize}

Ces matrices seront dans la suite notées \( G\). Et elles vérifient l'importante propriété
\begin{equation}        \label{EQooLQTVooBYjVYl}
    \det(GA)=\det(G)\det(A)
\end{equation}
pour toute matrice \( A\).

\begin{proposition}[Réduction de Gauss\cite{MonCerveau}]        \label{PROPooJBTZooNLobpf}
    Soit une matrice \( A\in \eM(n,\eK)\) de déterminant non nul : \( \det(A)\neq 0\). Alors il existe des matrices \( G_1,\ldots, G_N\) toutes de type \( S\), \( U\) ou \( T\) telles que
    \begin{equation}
        G_1\ldots G_NA=\delta.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous faisons une récurrence sur \( n\). D'abord pour \( n=1\), la matrice \( A\) contient un seul élément \( A_{11}\) qui est non nul par hypothèse. Nous pouvons multiplier sa ligne par \( 1/A_{11}\) pour obtenir le résultat. Plus précisément, nous avons l'égalité
    \begin{equation}
        T(1;1,\frac{1}{ A_{11} })A=\delta
    \end{equation}
    dans \( \eM(1, \eK)\). Notons que \( \eK\) est un corps (donc \( A_{11}\) est inversible) commutatif, ce qui permet d'écrire \( 1/A_{11}\) sans ambiguïté.

    Supposons le résultat prouvé pour \( n\), et voyons ce qu'il se passe pour \( n+1\). Puisque \( \det(A)\neq 0\), aucune de ses colonnes n'est nulle (corolaire \ref{CORooAZFCooSYINvBl}). Il existe donc un \( k\) tel que \( A_{k1}\neq 0\).

    Par la proposition \ref{PROPooFQRDooRPfuxk}, la matrice
    \begin{equation}
        B^{(1)}=S(n+1;k,1)A
    \end{equation}
    est une matrice telle que \( B^{(1)}_{11}=A_{k1}\neq 0\). Ensuite, par la proposition \ref{PROPooXUFKooOaPnna} la matrice
    \begin{equation}
        B^{(2)}=T(n+1;1,\frac{1}{ A_{k1} })B^{(1)}
    \end{equation}
    vérifie \( B^{(2)}_{11}=1\).

    Puisque la multiplication par la matrice \( U(n+1;k;l;\lambda)\) réalise, par la proposition \ref{PROPooPYNHooLbeVhj}, la substitution \( L_k\to L_{k}+\lambda L_l\), la matrice
    \begin{equation}
        B^{(3)}=\prod_{k=2}^{n+1}U(n+1;k,1,-B^{(1)}_{k1})B^{(1)}
    \end{equation}
    a toute sa première colonne nulle à l'exception de \( B^{(3)}_{11}=1\).

    Nous n'avons pas donné de nom ni démontré de théorèmes à propos de la substitution \( C_k\to C_k+\lambda C_l\). En passant éventuellement par les transposées et en utilisant les lemmes \ref{LEMooRSJTooQEoOtN} et \ref{LEMooCEQYooYAbctZ} nous obtenons une matrice \( U'(n+1;k,l,\lambda)\) ayant la propriété que la matrice
    \begin{equation}
        B^{(4)}=\prod_{k=2}^{n+1}U'(n+1;k,1,-B^{(3)}_{1k})B^{(3)}
    \end{equation}
    vérifie \( B^{(4)}_{1j}=B^{(4)}_{j1}=0\) pour tout \( j\) sauf \( j=1\). En d'autres termes, la matrice \( B^{(4)}\) est de la forme
    \begin{equation}
        B^{(4)}=\begin{pmatrix}
            1    &   \begin{matrix}
                0    &   \ldots    &   0
            \end{matrix}\\
            \begin{matrix}
                0    \\
                \vdots    \\
                0
            \end{matrix}&   \begin{pmatrix}
                    &       &       \\
                    &   A'    &       \\
                    &       &
            \end{pmatrix}
        \end{pmatrix}
    \end{equation}
    où \( A'\) est une matrice de taille \( n\).

    Voyons quelques propriétés de \( A'\). Nous savons que
    \begin{equation}
        B^{(4)}=\prod_i G_iA
    \end{equation}
    où les \( G_i\) sont de type \( S\), \( T\) ou \( U\). Vu que \( \det(SA)=\det(S)\det(A)\) (et idem pour \( T\) et \( U\)), nous avons
    \begin{equation}
        \det(B^{(4)})=\prod_i\det(G_i)\det(A),
    \end{equation}
    et comme aucun des \( \det(G_i)\) n'est nul, nous avons encore \( \det(B^{(4)})\neq 0\), ce qui implique \( \det(A')\neq 0\).

    La récurrence peut avoir lieu. Il existe des matrices \( G'_i\) telles que
    \begin{equation}
        G'_1\ldots G'_MA'=\delta
    \end{equation}
    où les \( G'_i\) sont de taille \( n\), ainsi que le \( \delta\). En remarquant que
    \begin{equation}
       S(n+1;k,l) =\begin{pmatrix}
            1    &   \begin{matrix}
                0    &   \ldots    &   0
            \end{matrix}\\
            \begin{matrix}
                0    \\
                \vdots    \\
                0
            \end{matrix}& S(n;k-1,l-1)
        \end{pmatrix},
    \end{equation}
    et pareillement pour les matrices \( T\) et \( U\), nous voyons qu'en prenant
    \begin{equation}
       G_i =\begin{pmatrix}
            1    &   \begin{matrix}
                0    &   \ldots    &   0
            \end{matrix}\\
            \begin{matrix}
                0    \\
                \vdots    \\
                0
            \end{matrix}& G'_i
        \end{pmatrix},
    \end{equation}
    nous avons
    \begin{equation}
        \prod_{i=1}^MG_iB^{(3)}=
      \begin{pmatrix}
            1   &   \begin{matrix}
                0   &   \ldots    &   0
            \end{matrix}\\
            \begin{matrix}
                0       \\
                \vdots  \\
                0
            \end{matrix}& \prod_{i=1}^MG'_iA'
        \end{pmatrix}=\delta_{n+1}
    \end{equation}
    où nous avons mis un indice sur le dernier \( \delta\) pour être plus explicite.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices inversibles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}      \label{PROPooMLWRooRWfZXE}
    Soit une matrice \( A\in \eM(n,\eK)\). Si les matrices \( B_1\) et \( B_2\) de \( \eM(n,\eK)\) vérifient
    \begin{equation}
        AB_1=B_1A=\delta
    \end{equation}
    et
    \begin{equation}
        AB_2=B_2A=\delta,
    \end{equation}
    alors \( B_1=B_2\). Dans ce cas, nous disons que \( A\) est inversible et nous notons \( A^{-1}\) l'unique matrice telle que \( AA^{-1}=A^{-1}A=\delta\).
\end{propositionDef}

\begin{proof}
    La preuve est réalisée dans le cas général par le lemme \ref{LEMooECDMooCkWxXf}. Mais si vous en voulez une preuve avec les notations d'ici, en voici une.

    Nous avons \( AB_1=AB_2\). En multipliant à gauche par \( B_1\), nous trouvons \( B_1AB_1=B_1AB_2\). En remplaçant \( B_1A\) par \( \delta\) des deux côtés, il reste \( B_1=B_2\).
\end{proof}

\begin{lemma}[\cite{ooKBOMooSkKHvu}]        \label{LEMooGZCTooQigDvC}
    Si \( A\in \eM(n,\eK)\), alors il existe au plus une matrice \( B\in \eM(n,\eK)\) telle que \( AB=\delta\).
\end{lemma}

\begin{proof}
    Soient des matrices \( B,C\in \eM(n,\eK)\) telles que \( AB=AC=\delta\). Nous allons montrer que \( B=C\).

    Pour cela nous considérons les applications linéaires \( f_A, f_B, f_C\in \End(\eK^n)\) associées par la proposition \ref{PROPooGXDBooHfKRrv}. Vu que \( AB=\delta\), par la proposition \ref{PROPooCSJNooEqcmFm}, nous avons \( f_A\circ f_B = f_{AB}=\id\). La proposition \ref{PROPooADESooATJSrH} nous dit alors que \( f_A\) et \( f_B\) sont bijectives.

    En particulier, vu que \( \{e_i\}\) est une base, son image par \( f_B\) est une base par la proposition \ref{PROPooZFKZooBGLSex}. La proposition \ref{PROPooHLUYooNsDgbn} dit alors que \( \{f_B(e_i)\}\) est une base. Nous décomposons \( f_B(e_k)-f_C(e_k)\) dans cette base :
    \begin{equation}
        f_B(e_k)-f_C(e_k)=\sum_j\alpha_jf_B(e_j)
    \end{equation}
    où les \( \alpha_j\) dépendent à priori de \( k\). Vu que \( f_A\circ(f_B-f_C)=0\), nous avons
    \begin{equation}
        0=f_A\big( f_B(e_k)-f_C(e_k) \big)=\sum_j(f_A\circ f_B)(e_j)=\sum_j\alpha_je_j.
    \end{equation}
    Donc les \( \alpha_j\) sont tous nuls.

    Nous en déduisons que \( f_B(e_k)=f_C(e_k)\), et donc \( f_B=f_C\). Cela implique que \( B=C\) par la proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.
\end{proof}

\begin{proposition}[\cite{ooKBOMooSkKHvu}]      \label{PROPooECIIooVMCIwz}
    Si \( A,B\in \eM(n,\eK)\) vérifient \( AB=\delta\), alors \( BA=\delta\).
\end{proposition}

\begin{proof}
    L'astuce est de poser \( C=BA-\delta+B\) et de montrer que \( C=B\). Pour cela, un rapide calcul commence par montrer que
    \begin{equation}
        AC=ABA-A+AB=AB=\delta.
    \end{equation}
    Donc \( C\) est également un inverse à droite de \( A\). Le lemme \ref{LEMooGZCTooQigDvC} donne alors \( C=B\).
\end{proof}

\begin{corollary}       \label{CORooBQLXooTeVfgb}
    Soit \( A\in \eM(n,\eK)\). Si il existe \( B\in \eM(n,\eK)\) tel que \( AB=\delta\), alors \( A\) est inversible et son inverse est \( B\).
\end{corollary}

\begin{proof}
    Il s'agit d'une paraphrase de la proposition \ref{PROPooECIIooVMCIwz} et de la définition \ref{PROPooMLWRooRWfZXE}.
\end{proof}

\begin{lemma}       \label{LEMooZDNVooArIXzC}
    Si une matrice \( A\) n'est pas inversible, alors le produit \( AB\) n'est inversible pour aucune matrice \( B\).
\end{lemma}

\begin{proof}
    Supposons que \( AB\) soit inversible. Alors
    \begin{equation}
        AB(AB^{-1})=\delta,
    \end{equation}
    ce qui dirait que \( B(AB^{-1})\) serait un inverse de \( A\).
\end{proof}

\begin{proposition}     \label{PROPooNPMCooPmaCwu}
    Une matrice est inversible si et seulement si son application linéaire associée est inversible. Dans ce cas, nous avons
    \begin{equation}
        f_A^{-1}=f_{A^{-1}}.
    \end{equation}
\end{proposition}

\begin{proof}
    Dans le sens direct, si \( A\) est inversible nous avons \( AA^{-1}=\delta\). Donc
    \begin{equation}        \label{EQooQQOSooBKVqXh}
        f_A\circ f_{A^{-1}}=f_{AA^{-1}}=f_{\delta}=\id
    \end{equation}
    où nous avons utilisé la proposition \ref{PROPooCSJNooEqcmFm} pour la composition et la proposition \ref{PROPooFMBFooEVCLKA} pour l'identité. L'égalité \eqref{EQooQQOSooBKVqXh} indique que \( f_A\) est inversible et que son inverse est \( f_{A^{-1}}\).

    Dans l'autre sens, l'application \( f_A^{-1}\) existe. Soit \( B\in \eM(n,\eK)\) sa matrice. Alors nous avons
    \begin{equation}
        f_{\delta}=\id=f_A\circ f_B=f_{AB}.
    \end{equation}
    Le fait que l'application \(\psi\colon A\to f_A\) soit une bijection\footnote{Proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.} implique que \( AB=\delta\), c'est-à-dire que \( A\) est inversible et que \( B=A^{-1}\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Inversibilité et déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooAVIXooMtVCet}
    Une matrice au déterminant non nul est inversible.
\end{proposition}

\begin{proof}
    Si \( A\) est une matrice telle que \( \det(A)\neq 0\), alors la proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices \( G_1,\ldots, G_N\) telles que
    \begin{equation}
        G_1\ldots G_NA=\delta.
    \end{equation}
    Donc la matrice \( G_1\ldots G_N\) est un inverse de \( A\) par le corolaire \ref{CORooBQLXooTeVfgb}.
\end{proof}

\begin{proposition}     \label{PROPooEOKBooKUROFg}
    Si une matrice \( A\) a une ligne ou une colonne de zéros, alors
    \begin{enumerate}
        \item
            \( \det(A)=0\),
        \item
            \( A\) n'est pas inversible.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Par définition nous avons
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nA_{i\sigma(i)}.
    \end{equation}
    Si la \( k\)\ieme\ ligne est nulle, alors \( A_{k\sigma(k)}=0\) pour tout \( \sigma\). Donc tous les produits contiennent un facteur nul. Donc \( \det(A)=0\).

    Pour toute matrice \( B\) nous avons
    \begin{equation}
        (AB)_{kk}=\sum_lA_{kl}B_{lk}.
    \end{equation}
    Si la \( k\)\ieme\ ligne de \( A\) est nulle nous avons \( (AB)_{kk}=0\) et donc pas \( AB=\delta\). Donc \( A\) n'est pas inversible.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques ensembles de matrices particuliers}
%---------------------------------------------------------------------------------------------------------------------------
Certains ensembles de matrices ont une importance particulière, que nous développerons plus tard.

\begin{definition}[Groupe linéaire de matrices]
On note \( \GL(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont inversibles. En d'autres termes, \( \GL(n,\eA) = U (\eM (n,\eA) ) \).
\end{definition}

\begin{definition}[Groupe orthogonal de matrices]\label{DefMatriceOrthogonale}
    On dit qu'une matrice \( A \) est \defe{orthogonale}{matrice!orthogonale} si son inverse est sa transposée, c'est-à-dire si \( A^{-1} = A^t \). On note \( \gO(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont orthogonales.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant et combinaisons de lignes et colonnes}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooKMSVooBBHwkH}

\begin{proposition}     \label{PROPooUCZVooPkloQp}
    Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)\neq 0\neq \det(B)\). Alors
    \begin{equation}
        \det(AB)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    La proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices de permutations de lignes et de colonnes \( G_1,\ldots, G_N\) et \( G'_1,\ldots, G'_N\) telles que\footnote{Les plus acharnés préciseront que pour avoir le même \( N\) des deux côtés, il a fallu compléter avec des matrices \( \delta\) là où il y en avait le moins.}
    \begin{subequations}        \label{EQooDNZUooHBhcZj}
        \begin{align}
            G_1\ldots G_NA&=\delta\\
            G'_1\ldots G'_NB&=\delta.
        \end{align}
    \end{subequations}
    Nous avons
    \begin{equation}
        (G'_1\ldots G'_N)\underbrace{(G_1\ldots G_N)A}_{=\delta}B=\delta.
    \end{equation}
    En prenant le déterminant des deux côtés et en tenant compte de \eqref{EQooLQTVooBYjVYl},
    \begin{equation}
        1=\det(\delta)=\det\big(  G'_1\ldots G'_NG_1\ldots G_NAB\big)=\det(G_1'\ldots G_N')\det(G_1\ldots G_N)\det(AB).
    \end{equation}
    Mais en même temps, les équations \ref{EQooDNZUooHBhcZj} donnent
    \begin{subequations}
        \begin{align}
            \det(G_1\ldots G_N)=\det(A)^{-1}\\
            \det(G_1'\ldots G'_N)=\det(B)^{-1}.
        \end{align}
    \end{subequations}
    Cela pour dire que
    \begin{equation}
        1=\det(A)^{-1}\det(B)^{-1}\det(AB),
    \end{equation}
    et donc ce qu'il nous fallait.
\end{proof}

\begin{proposition}     \label{PROPooWVJFooTmqoec}
    Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)=0\) et \( \det(B)\neq0\). Alors
    \begin{equation}
        \det(AB)=\det(BA)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Il existe des matrices de manipulations de lignes et de colonnes \( G_1,\ldots, G_N\) telles que \( G_1\ldots G_NB=\delta\). Donc
    \begin{equation}
        0=\det(A)=\det(G_1\ldots G_NBA)=\det(G_1\ldots G_N)\det(BA).
    \end{equation}
    Donc \( \det(BA)=0\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transvections}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( E_{ij}\) la matrice remplie de zéros sauf à la case \( ij\) qui vaut \( 1\). Autrement dit
\begin{equation}
    (E_{ij})_{kl}=\delta_{i,k}\delta_{j,l}.
\end{equation}
\begin{definition}
    Une \defe{matrice de transvection}{transvection (matrice)}\index{matrice!de transvection} est une matrice de la forme
    \begin{equation}
        T_{ij}(\lambda)=\id+\lambda E_{ij}
    \end{equation}
    avec \( i\neq j\).

    Une \defe{matrice de dilatation}{matrice!de dilatation}\index{dilatation (matrice)} est une matrice de la forme
    \begin{equation}
        D_i(\lambda)=\id+(\lambda-1)E_{ii}.
    \end{equation}
    Ici le \( (\lambda-1)\) sert à avoir \( \lambda\) et non \( 1+\lambda\). C'est donc une matrice qui dilate d'un facteur \( \lambda\) la direction \( i\) tout en laissant le reste inchangé.

    Si \( \sigma\) est une permutation (un élément du groupe symétrique \( S_n\)) alors la \defe{matrice de permutation}{matrice!de permutation}\index{permutation!matrice} associée est la matrice d'entrées
    \begin{equation}
        (P_{\sigma})_{ij}=\delta_{i,\sigma(j)}.
    \end{equation}
\end{definition}

\begin{lemma}   \label{LemyrAXQs}
    La matrice \( T_{ij}(\lambda)A=(\mtu+\lambda E_{ij})A\) est la matrice \( A\) à qui on a effectué la substitution
    \begin{equation}
        L_i\to L_i+\lambda L_j.
    \end{equation}
    La matrice \( AT_{ij}(\lambda)\) est la substitution
    \begin{equation}
        C_j\to C_j+\lambda C_i.
    \end{equation}

    La matrice \( AP_{\sigma}\) est la matrice \( A\) dans laquelle nous avons permuté les colonnes avec \( \sigma\).

    La matrice \( P_{\sigma}A\) est la matrice \( A\) dans laquelle nous avons permuté les lignes avec \( \sigma^{-1}\).
\end{lemma}

\begin{proof}
    Calculons la composante \( kl\) de la matrice \( E_{ij}A\) :
    \begin{subequations}
        \begin{align}
            (E_{ij}A)_{kl}  & =\sum_m(E_{ij})_{km}A_{ml}              \\
                            & =\sum_m\delta_{i,k}\delta_{j,m}A_{ml}   \\
                            & =\delta_{i,k}A_{jl}.
        \end{align}
    \end{subequations}
    C'est donc la matrice pleine de zéros, sauf la ligne \( i\) qui est donnée par la ligne \( j\) de \( A\). Donc effectivement la matrice
    \begin{equation}
        A+\lambda E_{ij}A
    \end{equation}
    est la matrice \( A\) à laquelle on a substitué la ligne \( i\) par la ligne \( i\) plus \( \lambda\) fois la ligne \( j\).

    En ce qui concerne l'autre assertion sur les transvections, le calcul est le même et nous obtenons
    \begin{equation}
        (AE_{ij})=A_{ki}\delta_{j,l}.
    \end{equation}

    Pour les matrices de permutation, nous avons
    \begin{equation}
        (AP_{\sigma})_{kl}=A_{k\sigma(l)}
    \end{equation}
    et
    \begin{equation}
        (P_{\sigma}A)_{kl}=\sum_m\delta_{k,\sigma(m)}A_{ml}=\sum_m\delta_{\sigma^{-1}(k),m}A_{ml}=A_{\sigma^{-1}(k)l}.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Mineur, rang}
%---------------------------------------------------------------------------------------------------------------------------

Pour la définition du rang d'une matrice, nous en donnons une qui est clairement inspirée de l'application linéaire associée.
\begin{definition}[\cite{ooKBOMooSkKHvu}]       \label{DEFooCSGXooFRzLRj}
    Le \defe{rang}{rang d'une matrice} d'une matrice de \( \eM(n,\eK)\) est la dimension de la partie de \( \eK^n\) engendrée par ses colonnes.
\end{definition}

Il est possible d'exprimer le rang d'une matrice de façon plus «intrinsèque» via le concept de mineur.
\begin{definition}[\cite{ooLFZTooJWJUed}]
    Les mineurs d'une matrice sont les déterminants de ses sous-matrices carrées.
\end{definition}
Dans la suite nous désignerons souvent par le mot «mineur» la sous-matrice carrée elle-même au lieu de son déterminant.

Lorsque \( A\) est une matrice, nous notons \( f_A\) l'application linéaire associée à la matrice \( A\) par l'application \eqref{EQooVZQWooMyFFeO}.

\begin{lemma} \label{LEMVecsaRgFixe}
    Soit \( \eK \) un corps commutatif\footnote{Comme toujours.}. Si \( A \) est une matrice carrée d'ordre \( n \) et de rang \( r \) à coefficients dans \( \eK \), alors il existe des vecteurs \( (x_i)_{i=1,\dots,n} \) formant une base de \( \eK^n \) tels que
    \begin{equation}
        f_A(x_i)\neq 0
    \end{equation}
    pour \( x\leq r\) et
    \begin{equation}
        f_A(x_i) = 0
    \end{equation}
    pour \( i > r \).
\end{lemma}

\begin{proof}
    Soit \( V\) le sous-espace de \( \eK^n\) engendré par les colonnes de \( A\). Nous considérons la base canonique \( \{ e_i \}\) de \( \eK^n\), ainsi que \( v_i\) le vecteur créé par la \( i\)\ieme\ colonne de \( A\). Nous avons
    \begin{equation}
        v_i=f_A(e_i).
    \end{equation}
    Les vecteurs \( v_i\) engendrent \( V\), donc nous pouvons en extraire une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooTZUDooFEgymQ}. Soit donc \( \{ v_j \}_{i\in J}\) une base de \( V\) avec \( J\subset\{ 1,\ldots, n \}\).

    La base de \( \eK^n\) que nous cherchons commence par les vecteurs \( \{ e_j \}_{j\in J}\). Ces vecteurs vérifient \( f_A(e_j)=v_j\neq 0\) parce que des vecteurs d'une base ne sont jamais nuls.

    % Note : toute la ligne suivante fait des références qui peuvent être vers le futur parce que ce sont des choses qui ne sont
    %        pas utilisées dans la démonstration.
    Pour la suite de la base, nous pourrions penser au théorème de la base incomplète\footnote{Théorème \ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu}.}, mais les vecteurs ainsi complétant la base ne sont pas garantis de s'annuler par \( f_A\). Voir l'exemple \ref{EXooRKVQooZOGDEf}.

    L'idée est d'utiliser le noyau de \( f_A\) qui est un sous-espace vectoriel par la proposition \ref{PROPooRLLPooKYzsJp}. Soit une base\footnote{Cette base contient \( n-r\) éléments, mais ce n'est pas très important pour la suite.} \(  \{ z_k \}  \) de \( \ker(f)\). Les vecteurs \( \{ e_j \}_{j\in J}\) forment une base de \( \Image(f_A)\). Vu que les \( z_i\) forment une base de \( \ker(f_A)\), le théorème du rang \ref{ThoGkkffA} dit alors que \( \{ e_j \}_{j\in J}\cup \{ z_k \}\) est une base de \( \eK^n\).

    Il y a \( r\) éléments dans \( J\) parce que l'espace engendré par les colonnes de \( A\) est de dimension \( r\) par hypothèse. Donc il y a \( n-r\) éléments dans les \( z_k\) pour que le tout ait le bon nombre d'éléments.
\end{proof}

\begin{example}     \label{EXooRKVQooZOGDEf}
    Soit la matrice
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\
            2    &   2
        \end{pmatrix}.
    \end{equation}
    Elle est de rang \( 1\). En suivant l'idée de la démonstration, nous commençons la base de \( \eR^2\) par le vecteur \( e_1\) qui vérifie
    \begin{equation}
        f_A(e_1)=\begin{pmatrix}
            1    \\
            2
        \end{pmatrix}.
    \end{equation}
    L'utilisation du théorème de la base incomplète ne permet pas de trouver un second vecteur de base \( v\) tel que \( f_A(v)=0\). En effet ce théorème donne juste l'existence d'une completion de la base, mais pas de propriétés particulières de la base obtenue. Elle pourrait donner \( v=e_2\) comme second vecteur de base. Mais alors
    \begin{equation}
        f_A(v)=f_A(e_2)=\begin{pmatrix}
            1    \\
            2
        \end{pmatrix}\neq 0.
    \end{equation}

    Au contraire, le noyau de \( f_A\) est donné par le sous-espace engendré par \( \begin{pmatrix}
        1    \\
        -1
    \end{pmatrix}\). Une base convenable est donc \( \{ e_1, e_1-e_2 \}\).
\end{example}

\begin{proposition}     \label{PROPooEGNBooIffJXc}
    Le rang d'une application linéaire\footnote{Définition \ref{DefALUAooSPcmyK}.} est égal au rang de sa matrice\footnote{Définition \ref{DEFooCSGXooFRzLRj}.} dans n'importe quelle base.
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices équivalentes et semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefBLELooTvlHoB}
    Deux relations d'équivalence entre les matrices.
    \begin{enumerate}
        \item   \label{ItemPFXCooOUbSCt}
    Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) si il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\).
\item
    Deux matrices sont \defe{semblables}{matrices!similitude} si il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
    \end{enumerate}
\end{definition}

\begin{lemma}   \label{LemZMxxnfM}
    Une matrice de rang\footnote{Définition~\ref{DEFooCSGXooFRzLRj}.} \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
    Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\), qui existe par le lemme~\ref{LEMVecsaRgFixe}.

    Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\); ses colonnes sont donc précisément les \( f_i \), si bien que
    \begin{equation}
        APe_i=Af_i=\begin{cases}
            0    &   \text{si } i>r\\
            \neq 0    &    \text{sinon}.
        \end{cases}
    \end{equation}
    La matrice \( AP\) se présente donc sous la forme
    \begin{equation}
        AP=\begin{pmatrix}
            M    &   0    \\
            *    &   0
        \end{pmatrix}
    \end{equation}
    où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
    \begin{equation}
        QAPe_i=\begin{cases}
            e_i    &   \text{si } i<r\\
            0    &    \text{sinon}.
        \end{cases}.
    \end{equation}
    Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{corollary}[Équivalence et rang]      \label{CorGOUYooErfOIe}
    Deux matrices sont équivalentes\footnote{Définition~\ref{DefBLELooTvlHoB}\ref{ItemPFXCooOUbSCt}.} si et seulement si elles sont de même rang.
\end{corollary}

\begin{proof}
    D'abord il y a des implicites dans l'énoncé. Vu que nous voulons soit par hypothèse soit par conclusion que les matrices \( A\) et \( B\) soient équivalentes, nous supposons qu'elles ont même dimension. Soient donc \( A\) et \( B\) deux matrices carrées d'ordre \( n \).

    Par le lemme~\ref{LemZMxxnfM}, deux matrices de même rang \( r\) sont équivalentes à \( J_r\). Elles sont donc équivalentes entre elles.

    Inversement, supposons que \( A\) et \( B\) soient deux matrices équivalentes : \( A=PBQ^{-1}\) avec \( P\) et \( Q\) inversibles. Alors
    \begin{subequations}
        \begin{align}
            \Image(PBQ^{-1})&=\{ PBQ^{-1}v\tq v\in \eK^n \}\\
            &=PB\underbrace{\{ Q^{-1}v\tq v\in \eK^n \}}_{=\eK^n}\\
            &=P\big( B(\eK^n) \big).
        \end{align}
    \end{subequations}
    L'ensemble \( B(\eK^n)\) est un sous-espace vectoriel de \( \eK^n\). Vu que le rang de \( P\) est maximum, la dimension de \( P\big( B(\eK^n) \big)\) est la même que celle de \( B(\eK^n)\). Par conséquent
    \begin{equation}
        \dim\Big( \Image(PBQ^{-1}) \Big)=\dim\big( B(\eK^n) \big)=\rang(B).
    \end{equation}
    Le membre de gauche de cela n'est autre que \( \rang(A)=\dim\big( \Image(PBQ^{-1}) \big)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithme des facteurs invariants}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Algorithme des facteurs invariants\cite{KXjFWKA}]   \label{PropPDfCqee}
    Soit \( (\eA,\delta)\) un anneau euclidien muni de son stathme  et \( U\in \eM(n \times m,\eA)\). Alors il existe \( d_1,\ldots, d_s\in \eA^*\) et des matrices \( P\in\GL(m,\eA)\), \( Q\in \GL(n,\eA)\) tels que nous ayons
    \begin{equation}
        U=P \begin{pmatrix}
            \begin{matrix}
                d_1    &       &       \\
                    &   \ddots    &       \\
                    &       &   d_s
            \end{matrix}&   0    \\
            0    &   0
        \end{pmatrix}Q
    \end{equation}
    avec \( d_i\divides d_{i+1}\) pour tout \( i\).
\end{proposition}
\index{anneau!euclidien!facteurs invariants}
\index{algorithme!facteurs invariants}

\begin{proof}
    Nous allons donner la preuve plus ou moins sous forme d'algorithme.

    D'abord si \( U=0\) c'est bon, on a la réponse. Sinon, nous prenons l'élément \( (i_0,j_0)\) dont le stathme est le plus petit et nous l'amenons en \( (1,1)\) par les permutations
    \begin{equation}
        \begin{aligned}[]
            C_1&\leftrightarrow C_{j_0}\\
            L_1&\leftrightarrow L_{i_0}
        \end{aligned}
    \end{equation}
    Ensuite nous traitons la première colonne jusqu'à amener des zéros partout en dessous de \( u_{11}\) de la façon suivante : pour chaque ligne successivement nous calculons la division euclidienne
    \begin{equation}
        u_{i1}=qu_{11}+r_i,
    \end{equation}
    et nous faisons
    \begin{equation}
        L_i\to L_i-qL_1,
    \end{equation}
    c'est-à-dire que nous enlevons le maximum possible et il reste seulement \( r_i\) en \( u_{i1}\). Vu que le but est de ne laisser que des zéros dans la première colonne, si le reste n'est pas zéro nous ne sommes pas content\footnote{Si il est zéro, nous passons à la ligne suivante}. Dans ce cas nous permutons \( L_1\leftrightarrow L_i\), ce qui aura pour effet de strictement diminuer le stathme de \( u_{11}\) parce qu'on va mettre en \( u_{11}\) le nombre \( r_i\) dont le stathme est strictement plus petit que celui de \( u_{11}\).

    En faisant ce jeu de division euclidienne puis échange, on diminue toujours le stathme de \( u_{11}\), donc ça finit par s'arrêter, c'est-à-dire qu'à un certain moment la division euclidienne de \( u_{i1}\) par \( u_{11}\) va donner un reste zéro et nous serons content.

    Une fois la première colonne ramenée à la forme
    \begin{equation}
        C_1=\begin{pmatrix}
            u_{11}    \\
            0    \\
            \vdots    \\
            0
        \end{pmatrix},
    \end{equation}
    nous faisons tout le même jeu avec la première ligne en faisant maintenant des sommes divisions et permutations de colonnes. Notons que ce faisant nous ne changeons plus la première colonne.

    En fin de compte nous trouvons une matrice\footnote{Nous nommons toujours par la même lettre \( U\) la matrice originale et la modifiée, comme il est d'usage en informatique.}
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Si l'élément \( u_{11}\) ne divise pas un des éléments de \( A\), disons \( a_{ij}\), alors nous faisons
    \begin{equation}
        C_1\to C_1-C_j.
    \end{equation}
    Cela nous détruit un peu la première colonne, mais ne change pas \( u_{11}\). Nous avons maintnant
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             *   &       &       &       \\
             u_{ij}   &       &   A    &       \\
             *   &       &       &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Et nous refaisons tout le jeu depuis le début. Cependant lorsque nous allons nous attaquer à la ligne \( i\), \( u_{11}\) ne divisera pas \( u_{ij}\), ce qui donnera lieu à une division euclidienne et un échange \( L_1\leftrightarrow L_i\). L'échange consistant à mettre \( r_i\) à la place de \( u_{11}\) et inversement  diminuera encore strictement le stathme. Encore une fois nous allons travailler jusqu'à avoir la matrice sous la forme
    \begin{equation}    \label{EqADcNVgI}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix},
    \end{equation}
    sauf que cette fois le stathme de \( u_{11}\) est strictement plus petit que la fois précédente. Si \( u_{11}\) ne divise toujours pas tous les éléments de \( A\), nous recommençons encore et encore. En fin de compte nous finissons par avoir une matrice de la forme \eqref{EqADcNVgI} avec \( u_{11}\) qui divise tous les éléments de \( A\).

    Une fois que cela est fait, il faut continuer en recommençant tout sur la matrice \( A\). Nous avons maintenant
    \begin{equation}
        U=\begin{pmatrix}
            \begin{matrix}
                u_{11}  &       \\
                &   u_{22}
            \end{matrix}&   0    \\
            0    &   B
        \end{pmatrix}.
    \end{equation}
    Sous cette forme nous avons \( u_{11}\divides u_{22}\) et \( u_{11}\) divise tous les éléments de \( B\). En effet \( u_{11}\) divisant tous les éléments de \( A\), il divise toutes les combinaisons de ces éléments. Or tout l'algorithme ne consiste qu'à prendre des combinaisons d'éléments.

    Nous finissons donc bien sur une matrice comme annoncée. De plus n'ayant effectué que des combinaisons de lignes, nous avons seulement multiplié par des matrices inversibles (lemme~\ref{LemyrAXQs}).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Changement de base}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit un espace vectoriel \( V\) muni de deux bases \( (e_i)_{i=1,\ldots, n}\) et \( (f_{\alpha})_{\alpha=1,\ldots, n}\). Les deux bases sont liées entre elles par
\begin{equation}        \label{EQooFRQRooSMsQQB}
    f_{\alpha}=\sum_iQ_{i\alpha}e_i.
\end{equation}
Ici \( Q\) n'est pas une application linéaire \( V\to V\) : \( Q\) est seulement un tableau de nombres, donnant les coordonnées des vecteurs \( f_{\alpha}\) dans la base de \( e_i\). Éventuellement \( Q\) peut être vu comme une application linéaire \( \eK^n\to \eK^n\).

Dans la suite nous nommerons \( Q^{-1}\) la matrice inverse de \( Q\). Inverse au sens des bêtes tableaux de nombres, sans interprétation en tant qu'application linéaire. De même pour \( Q^t\) qui est la transposée de \( Q\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : vecteurs de base}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooIHZGooOZoYZd}
    Soit un espace vectoriel \( V\) sur \( \eK\) ainsi que deux bases \( (e_i)_{i=1,\ldots, n}\), \( (f_{\alpha})_{\alpha=1,\ldots, n}\) de \( V\) liées par \( f_{\alpha}=\sum_iQ_{i\alpha}e_i\). Alors
    \begin{equation}    \label{EQooZQPAooAbKAdg}
        e_i=\sum_{\alpha}Q^{-1}_{\alpha i}f_{\alpha}.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous multiplions l'égalité \( f_{\alpha}=\sum_iQ_{i\alpha}e_i\) par le nombre\footnote{Attention à la bonne interprétation de ce nombre : on fait bien référence à l'élément situé en \( (\alpha, j) \) de la matrice \( Q^{-1} \), et pas autre chose.} \( Q^{-1}_{\alpha j}\in \eK\)  et nous sommons sur \( \alpha\) :
    \begin{equation}
        \sum_{\alpha}Q^{-1}_{\alpha j}f_{\alpha}=\sum_{i\alpha}(A_{i\alpha}Q^{-1}_{\alpha j})e_i=e_j.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : coordonnées}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooNYYOooHqHryX}
    Soit un espace vectoriel \( V\) sur \( \eK\). Soient deux bases \( (e_i)_{i=1,\ldots, n}\) et \( (f_{\alpha})_{\alpha=1,\ldots, n}\) liées par \( f_{\alpha}=\sum_iQ_{i\alpha}e_i\). Nous considérons un même vecteur dans les deux bases : \( \sum_ix_ie_i=\sum_{\alpha}y_{\alpha}f_{\alpha}\). Alors
    \begin{enumerate}
        \item       \label{ITEMooIBAEooNaUnPD}
            \( y_{\alpha}=\sum_iQ^{-1}_{\alpha i}x_i\)
        \item       \label{ITEMooKPWTooMwdbPu}
            \( x_i=\sum_{\alpha}Q_{i\alpha}y_{\alpha}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit un vecteur \( x\in V\). Il peut être écrit dans les deux bases :
    \begin{equation}        \label{EQooGSJMooGQstMx}
        x=\sum_ix_ie_i=\sum_{\alpha}y_{\alpha}f_{\alpha}.
    \end{equation}
    En remplaçant \( e_i\) par sa valeur \eqref{EQooZQPAooAbKAdg} nous avons l'égalité
    \begin{equation}
        \sum_{i\alpha}x_iQ^{-1}_{\alpha i}f_{\alpha}=\sum_{\alpha}y_{\alpha}f_{\alpha}.
    \end{equation}
    Puisque les \( f_{\alpha}\) sont linéairement indépendants, l'égalité des sommes donne l'égalité de chacun des termes :
    \begin{equation}
        y_{\alpha}=\sum_ix_iQ^{-1}_{\alpha i}.
    \end{equation}
    En identifiant \( x\in V\) au vecteur dans \( \eK^n\) de ses coordonnées dans la base \( \{ e_i \}\) nous pouvons écrire
    \begin{equation}
        y_{\alpha}=(Q^{-1}x)_{\alpha},
    \end{equation}
    Le point \ref{ITEMooIBAEooNaUnPD} est prouvé.

    En ce qui concerne le point \ref{ITEMooKPWTooMwdbPu}, nous repartons encore de \eqref{EQooGSJMooGQstMx}, mais nous y substituons la définition des \( f_{\alpha}\) :
    \begin{equation}
        \sum_{i}x_ie_i=\sum_{\alpha i}y_{\alpha}Q_{i\alpha}e_i.
    \end{equation}
    Vous voulez des détails ? Allez, une étape de plus que le strict nécessaire : nous écrivons
    \begin{equation}
        \sum_i\big( x_i-\sum_{\alpha}y_{\alpha}Q_{i\alpha} \big)e_i=0.
    \end{equation}
    Par linéaire indépendance des \( e_i\), nous avons annulation de tous les coefficients, c'est-à-dire
    \begin{equation}
        x_i=\sum_{\alpha}Q_{i\alpha}y_{\alpha},
    \end{equation}
    comme annoncé.
\end{proof}

\begin{normaltext}
    Attention à l'ordre des indices dans la dernière égalité : la matrice \( Q\) vient avec les indices dans l'ordre \( i\alpha\), tandis que la matrice \( Q^{-1}\) vient avec les indices dans l'ordre opposé : \( \alpha i\). C'est pour cela qu'il est intéressant de noter avec des lettres latines les indices se rapportant à la première base et avec des lettres grecques ceux se rapportant à la seconde base.
\end{normaltext}

\begin{normaltext}      \label{NORMooNWKZooPMwYTO}
    Les formules de changement de coordonnées de la proposition \ref{PROPooNYYOooHqHryX} s'écrivent souvent de la façon suivante :
    \begin{enumerate}
            \item       \label{ITEMooLHQCooBRvSlp}
                \( y_{\alpha}=(Q^{-1}x)_{\alpha}\)
            \item       \label{ITEMooNXUGooJIeoBf}
                \( y=Q^{-1}x\).
            \item       \label{ITEMooEFILooNENamW}
                \( x_i=(Qy)_i\)
            \item       \label{ITEMooMOKHooFEJvIW}
                \( x=Qy\)
    \end{enumerate}
    Ces égalités reposent sur un petit paquet d'abus de notations qu'il convient de bien comprendre. Ici, \( x\) et \( y\) sont les éléments de \( \eK^n\) donnés par les composantes de \( x\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\), et \( Q\) est vu comme une matrice, un opérateur linéaire sur \( \eK^n\). Autrement dit, le choix des bases permet d'identifier \( V\) avec \( \eK^n\) et la matrice \( Q\) avec l'application linéaire \( f_Q\) de la proposition \ref{PROPooGXDBooHfKRrv}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une application linéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooNZBEooWyCXTw}
    Soit une application linéaire \( t\colon V\to V\) de matrices \( A\) et \( B\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\). Si les bases sont liées par
    \begin{equation}
        f_{\alpha}=\sum_iQ_{i\alpha}e_i,
    \end{equation}
    alors les matrices \( A\) et \( B\) sont liées par
    \begin{equation}
        B=Q^{-1}AQ.
    \end{equation}
\end{proposition}

\begin{proof}
    L'hypothèse sur le fait que \( A\) et \( B\) sont les matrices de \( t\) signifie que pour tout \( x\in V\),
    \begin{equation}
        t(x)=\sum_{ij}A_{ji}x_ie_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\beta}f_{\alpha}.
    \end{equation}
    En remplaçant \( e_j\) par son expression \eqref{EQooZQPAooAbKAdg} en termes des \( f_{\alpha}\) et \( x_i\) par son expression \( x_i=(Qy)_i\) (proposition \ref{PROPooNYYOooHqHryX}), nous avons
    \begin{subequations}
        \begin{align}
            (By)_{\alpha}&=\sum_{ij\alpha}A_{ji}(Qy)_iQ^{-1}_{\alpha j}f_{\alpha}\\
            &=\sum_{i \alpha}(Q^{-1}A)_{\alpha i}(Qy)_if_{\alpha}\\
            &=\sum_{\alpha}(Q^{-1} AQy)_{\alpha}f_{\alpha}.
        \end{align}
    \end{subequations}
    Vu que les \( f_{\alpha}\) forment une base nous en déduisons \( Q^{-1}AQy=By\). Et vu que \( y\) est un élément quelconque de \( \eK^n\), nous en déduisons l'égalité de matrices
    \begin{equation}        \label{ooWKTYooOJfclT}
        B=Q^{-1}AQ.
    \end{equation}
\end{proof}
Il s'agit bien d'une égalité de matrices, ou à la limite d'applications linéaires sur \( \eK^n\), et non d'une égalité d'application linéaire sur \( V\).

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de polynômes}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecEspacePolynomes}

Attention : les polynômes en soi sont définis par la définition~\ref{DEFooFYZRooMikwEL}.

Pour chaque \( k>0\) donné nous définissons
\begin{equation}
    \mathcal{P}_\eR^k=\{p:\eR\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k, \, a_i\in\eR,\,\forall i=0,\ldots,k\}.
\end{equation}
Il est facile de se convaincre que la somme de deux polynômes de degré inférieur ou égal à \( k\) est encore un polynôme de degré inférieur ou égal à \( k\). En outre il est clair que la multiplication par un scalaire ne peut pas augmenter le degré d'un polynôme. L'ensemble \( \mathcal{P}_\eR^k\) est donc un espace vectoriel muni des opérations héritées de \( \mathcal{P}_{\eR}\).

La base canonique de l'espace \( \mathcal{P}_\eR^k\) est donnée par les monômes \( \mathcal{B}=\{x\mapsto x^j \,|\, j=0, \ldots, k\}\). Le fait que cela soit une base est vraiment facile à démontrer et est un exercice très utile si vous ne l'avez pas encore vu dans un cours précédent.
%TODOooAJWIooAfLBIo Prouver que c'est une base, en faire un lemme.

Nous allons maintenant étudier trois applications linéaires de \( \mathcal{P}_\eR^k\) vers d'autres espaces vectoriels.
\begin{description}
  \item[L'isomorphisme canonique  $\phi:\mathcal{P}_\eR^k \to\eR^{k+1}$] Nous définissons \( \phi\) par les relations suivantes
\[
  \phi(x^j)=e_{j+1}, \qquad \forall j\in\{0,\dots, k\}.
\]
Cela veut dire que pour tout \( p\) dans $\mathcal{P}_\eR^k$, avec \( p(x)=a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k\), l'image de \( p\) par \( \phi\) est
\[
  \phi(p)=\phi\left(\sum_{j=0}^k a_j x^j\right)=\sum_{j=0}^k a_j e_{j+1}.
\]
\begin{example} Soit \( k=5\) on a
  \begin{equation}
    \phi(-8-7x-4x^2+4x^3+2x^5)=
  \begin{pmatrix}
    -8\\
    -7\\
    -4\\
    4\\
    0\\
    2
  \end{pmatrix}.
  \end{equation}
\end{example}

Cette application est clairement bijective et respecte les opérations d'espace vectoriel, donc c'est un isomorphisme d'espaces vectoriels. L'existence d'un isomorphisme entre \( \mathcal{P}_\eR^k\)  et $\eR^{k+1}$ est un cas particulier du théorème qui dit que  pour chaque $m$ dans $\eN_0$ fixée, tous les espaces vectoriels sur $\eR$ de dimension $m$ sont isomorphes à $\eR^m$. Vous connaissez peut être déjà ce théorème depuis votre cours d'algèbre linéaire.
    \item[La dérivation $d: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k-1}$] L'application de dérivation \( d\) fait exactement ce qu'on attend d'elle
\[
  d(x^0)=d(1)=0, \qquad d(x^j)=j x^{j-1}, \quad \forall j\in\{1,\dots, k\}.
\]
Cette application n'est pas injective, parce que l'image de $p$ ne dépend pas de la valeur de $a_0$, donc si deux polynômes sont les mêmes à une constante près ils auront la même image par $d$.

\begin{example} Soit \( k=3\) on a
    \begin{equation}
        d(-8-12x+4x^3)= -12 (1) + 4 (3x^2) = -12+12 x^2.
    \end{equation}

    Noter que \( d(-30-12x+4x^3)=d(-8-12x+4x^3)\). Cela confirme, comme mentionné plus haut, que la dérivée n'est pas injective.
\end{example}
    \item[L'intégration $I: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k+1}$] Nous pouvons définir une application qui est <<à une constante près>> l'application inverse de la dérivation. Cette application est définie sur les éléments de base par
        \begin{equation}
            I(x^j)= \frac{x^{j+1}}{j+1}.
        \end{equation}
        Bien entendu la raison d'être et la motivation de cette définition apparaîtront lorsque nous développerons une théorie générale de l'intégration.

\begin{example}
    Soit \( k=4\) on a
    \begin{equation}
        I(6+2x+x^2+x^4)= 6x+x^2+\frac{x^3}{3}+\frac{x^5}{5}.
    \end{equation}
\end{example}

Remarque: étant donné que dans la définition de \( I\) nous avons décidé d'intégrer entre zéro et \( x\), tous les polynômes dans \( \mathcal{P}_\eR^{k+1}\) qui sont l'image par \( I\) d'un polynôme de \( \mathcal{P}_\eR^{k}\) ont \( a_0=0\). Cela veut dire que nous pouvons générer toute l'image de \( I\) en utilisant un sous-ensemble de la base canonique de \( \mathcal{P}_\eR^{k+1}\), en particulier \( \mathcal{B}_1=\{x\mapsto x^j \,|\, j=1, \ldots, k\}\subset \mathcal{B}\) nous suffira. Cela n'est guère surprenant, parce que l'image par une application linéaire d'un espace vectoriel de dimension finie ne peut pas être un espace de dimension supérieure.
\end{description}

Les applications de dérivation et intégration correspondent évidemment à des applications linéaires de \( \mathcal{P}_\eR\) dans lui-même.

L'espace de tous les polynômes étant de dimension infinie, il peut servir de contre-exemple assez simple. Dans la sous-section~\ref{SubSecPOlynomesCE}, nous verrons que toutes les normes ne sont pas équivalentes sur l'espace des polynômes.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------


\begin{proposition}     \label{PropProjScal}
    Si nous écrivons \( \pr_Y\) l'opération de projection sur la droite qui sous-tend \( Y\), alors nous avons
    \begin{equation}
        \| \pr_YX \|=\frac{ X\cdot Y }{ \| Y \| }.
    \end{equation}
\end{proposition}

\begin{proof}
    Les vecteurs \( X\) et \( Y\) sont des flèches dans l'espace. Nous pouvons choisir un système d'axe orthogonal tel que les coordonnées de \( X\) et \( Y\) soient
    \begin{equation}
        \begin{aligned}[]
            X&=\begin{pmatrix}
                x   \\
                y   \\
                0
            \end{pmatrix},
            &Y&=\begin{pmatrix}
                l   \\
                0   \\
                0
            \end{pmatrix}
        \end{aligned}
    \end{equation}
    où \( l\) est la longueur du vecteur \( Y\). Pour ce faire, il suffit de mettre le premier axe le long de \( Y\), le second dans le plan qui contient \( X\) et \( Y\), et enfin le troisième axe dans le plan perpendiculaire aux deux premiers.

    Un simple calcul montre que \( X\cdot Y=xl+y\cdot 0+0\cdot 0=xl\). Par ailleurs, nous avons \( \| \pr_YX \|=x\). Par conséquent,
    \begin{equation}
        \| \pr_YX \|=\frac{ X\cdot Y }{ l }=\frac{ X\cdot Y }{ \| Y \| }.
    \end{equation}
\end{proof}

\begin{corollary}
    Si la norme de \( Y\) est \( 1\), alors le nombre \( X\cdot Y\) est la longueur de la projection de \( X\) sur \( Y\).
\end{corollary}

\begin{proof}
    Poser \( \| Y \|=1\) dans la proposition~\ref{PropProjScal}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropEJBZooTNFPRj}
    Si \( A\) est la matrice d'une application linéaire, alors le rang de cette application linéaire est égal au rang de \( A \), c'est-à-dire à la taille de la plus grande matrice carrée de déterminant non nul contenue dans \( A\).
\end{proposition}

\begin{definition}  \label{DefJPGSHpn}
    Soit \( E\) un espace vectoriel sur \( \eK\).

    Une \defe{forme linéaire}{forme linéaire} sur \( E \) est une application linéaire de \( E \) sur son corps de base \( \eK\).

    Le \defe{dual algébrique}{dual algébrique} de \( E\), noté \( E^*\), est l'ensemble des formes linéaires sur \( E\). Ainsi, \( E^* = \GL(E,\eK)\).
\end{definition}

Nous verrons plus tard qu'en dimension infinie, les applications linéaires ne sont pas toujours continues. Nous définirons donc aussi un concept de dual topologique. Voir la proposition~\ref{PROPooQZYVooYJVlBd}, la remarque~\ref{RemOAXNooSMTDuN} et la définition~\ref{DEFooKSDFooGIBtrG}.

\begin{lemmaDef}      \label{DEFooTMSEooZFtsqa}
    Si \( E\) est un espace vectoriel et si \( \{ e_i \}\) est une base de \( E\), alors nous définissons
    \begin{equation}
        \begin{aligned}
            e^*\colon E & \to \eK                 \\
                    e_j & \mapsto \delta_{i,j},
        \end{aligned}
    \end{equation}
    est la prolongation par linéarité.

    Ces élements du dual \( E^*\) forment une base appelée \defe{base duale}{base!duale}.
\end{lemmaDef}
Notons que si \( v\in E\) est un vecteur, ça n'a aucun sens à priori de parler de \( v^*\). Il s'agit bien de définir \emph{toute} la base \( \{ e_i^* \}\) à partir de toute la base \( \{ e_i \}\).

\begin{lemma}[\cite{BIBooQVDOooFONdrf}]       \label{LEMooQLWNooYUpGdo}
    Soit un espace vectoriel \( E\) de dimension finie.
    \begin{enumerate}
        \item       \label{ITEMooHHTLooNCjgfn}
            Si \( \alpha\) est une forme linéaire non nulle sur \( E\), alors il existe \( x\in E\) tel que \( \alpha(x)=1\).
        \item       \label{ITEMooBYAAooUWBKDk}
            Si \( x\neq 0\) dans \( E\), alors il existe une forme linéaire \( \alpha\) sur \( E\) telle que \( \alpha(x)=1\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    En deux parties.
    \begin{subproof}
    \item[Pour \ref{ITEMooHHTLooNCjgfn}]
        Puisque \( \alpha\) est non nulle nous pouvons considérer \( v\in E\) tel que \( \alpha(v)\neq 0\). Alors en posant \( x=\alpha(v)^{-1}  v\), nous avons le résultat.
    \item[Pour \ref{ITEMooBYAAooUWBKDk}]
        Soit un vecteur non nul que nous écrivons sous la forme \( x=\sum_{i=1}^nx_ie_i\) (pour une certaine base \( (e_i)_{i=1,\ldots, n}\) de \( E\)). Supposons que \( x_k\neq 0\). Alors la forme
        \begin{equation}
            \begin{aligned}
                \alpha\colon  E & \to \eK \\
                              y & \mapsto y_k/x_k
            \end{aligned}
        \end{equation}
        fait l'affaire.
    \end{subproof}
\end{proof}

\begin{lemma}       \label{LEMooKTREooBrnWVz}
    Soit un espace vectoriel de dimension finie \( E\) sur le corps \( \eK\). Si \( (\alpha_1,\ldots,  \alpha_n)\) est une base de \( E^*\), alors l'application
    \begin{equation}
        \begin{aligned}
            \Phi\colon E&\to \eK^n \\
            x&\mapsto \big( \alpha_1(x),\ldots, \alpha_n(x) \big)
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels.

\end{lemma}

\begin{proof}
    En deux parties.
    \begin{subproof}
        \item[\( \Phi\) est injective]
            Soit \( z\in \ker(\Phi)\). Nous avons \( \alpha_i(z)=0\) pour tout \( i\). Si \( z\neq 0\), alors le lemme \ref{LEMooQLWNooYUpGdo} dit qu'il existe \( \beta\in E^*\) tel que \( \beta(z)\neq 0\).

    Décomposons un tel \( \beta\) dans la base de \( \{\alpha_i\}\) :
    \begin{equation}
        \beta=\sum_{i=1}^n\beta_i\alpha_i.
    \end{equation}
    Alors nous avons
    \begin{equation}
        0\neq \beta(z)=\sum_{i=1}^n\beta_i\underbrace{\alpha_i(z)}_{=0}=0.
    \end{equation}
    Contradiction. Donc \( \ker(\Phi)=\{ 0 \}\) et \( \Phi\) est injective.

\item[\( \Phi\) est surjective]
    Les espaces vectoriels \( E\), \( E^*\) et \( \eK^n\) ont tout trois, une dimension \( n\). Donc \( \Phi\) est une application linéaire injective entre deux espaces de même dimension. Elle est donc surjective par le corolaire \ref{CORooCCXHooALmxKk}.
    \end{subproof}
\end{proof}


\begin{propositionDef}[\cite{BIBooQVDOooFONdrf}]       \label{PROPooDBPGooPagbEB}
    Soit un espace vectoriel \( E\) de dimension finie sur le corps \( \eK\). Toute base du dual \( E^*\) est duale d'une unique base de \( E\). Cette base est dite \defe{préduale}{base préduale}.
\end{propositionDef}

\begin{proof}
    Nous considérons une base \( \mF=(\alpha_1,\ldots, \alpha_n)\) de \( E^*\). Nous devons prouver qu'il existe une unique base de \( E\) dont la base duale est \( \mF\).

    \begin{subproof}
    \item[Existence]
        Le lemme \ref{LEMooKTREooBrnWVz} nous indique que l'application
        \begin{equation}
            \begin{aligned}
                \Phi\colon  E & \to \eK^n \\
                            x & \mapsto \big( \alpha_1(x),\ldots, \alpha_n(x) \big)
            \end{aligned}
        \end{equation}
        est un isomorphisme d'espaces vectoriels.

        Soit la base canonique de \( \eK^n\) : \( (\epsilon_1,\ldots, \epsilon_n)\). Puisque \( \Phi\) est un isomorphisme, \( \big( \Phi^{-1}(\epsilon_i) \big)_{i=1,\ldots, n}\) est une base de \( E\). Nous allons montrer qu'elle est préduale de \( (\alpha_i)\). Nous posons \( e_i=\Phi^{-1}(\epsilon_i)\) et nous calculons :
        \begin{subequations}
            \begin{align}
                \alpha_i(e_j) & =\alpha_i\big( \Phi^{-1}(\epsilon_j) \big)      \\
                              & =\Phi\big( \Phi^{-1}(\epsilon_j) \big)_i        \label{SUBEQooACVAooVNwzMq}\\
                              & =(\epsilon_i)_j                                 \\
                              & =\delta_{i,j}                                   \label{SUBEQooOYNHooSVOOyz}
            \end{align}
        \end{subequations}
        Justifications :
        \begin{itemize}
            \item Pour \ref{SUBEQooACVAooVNwzMq}, nous remarquons que \( \alpha_i(x)=\Phi(x)_i\).
            \item Pour \ref{SUBEQooOYNHooSVOOyz}, nous utilisons le fait que les \( \epsilon_j\) forment la base canonique de \( \eK^n\).
        \end{itemize}

    \item[Unicité]
        Soit une base préduale \( (e_i)\) de \( (\alpha_i)\). Nous avons, par définition, que \( \alpha_i(e_j)=\delta_{i,j}\). Donc
        \begin{equation}
            \big( \alpha_1(e_j),\ldots, \alpha_n(e_j) \big)=\epsilon_j.
        \end{equation}
        Nous appliquons \( \Phi^{-1}\) à cette dernière équation pour obtenir \( e_j=\Phi^{-1}(\epsilon_j)\). Donc les \( e_j\) sont déterminés de façon unique à partir des \( \alpha_i\).
    \end{subproof}
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooEQSMooHVzbfz}
    Soit \( E\), un espace vectoriel, et \( F\) un sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
    \begin{equation}    \label{Eqiiyple}
        F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
    \end{equation}
\end{definition}

Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) muni du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition~\ref{DEFooEQSMooHVzbfz}, au contraire, est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
    B^o=\{ x\in E\tq \omega(x)=0, \forall\omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual et on prend une notation précise pour dire qu'on remonte au pré-dual et non qu'on va au dual du dual.

\begin{proposition} \label{PropXrTDIi}
    Soient un espace vectoriel \( E\) et \( F\), un sous-espace vectoriel de \( E\). Nous avons
    \begin{equation}
        \dim F+\dim F^{\perp}=\dim E.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème~\ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\).

    D'abord, ce sont des éléments de \( F^{\perp}\), parce que si \( i\leq p\) et si \( k\geq 1\), nous avons \( e^*_{p+k}(e_i)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

    Ensuite, en tant que partie d'une base de \( F^*\), c'est une partie libre. Il reste à montrer que c'est générateur.


    Enfin \( F^{\perp}\subset\Span\{ e_{k}^*, k \in \{p+1, \dots, n\}\}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{k=p+1}^n\omega_ke^*_k\).
\end{proof}

La proposition \ref{PROPooNITTooCYcrrT} donnera une version plus terre à terre de la proposition \ref{PropXrTDIi} en disant que si nous avons un produit scalaire, alors \( V=F\oplus F^{\perp}\) où \( F^{\perp}\) est cette fois défini comme l'orthogonal pour le produit scalaire.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Représentation de groupe}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Représentation]      \label{DEFooXVMSooXDIfZV}
    Soit un groupe \( G\). Une \defe{représentation}{représentation} de \( G\) est un couple \( (V,\rho)\) où \( V\) est un espace vectoriel et \( \rho\) est une application \( \rho\colon G\to \GL(V)\) vérifiant
    \begin{equation}
        \rho(g)\circ\rho(h)=\rho(gh).
    \end{equation}
    pour tout \( g,h\in G\).
\end{definition}

\begin{definition}
    Une représentation\footnote{Définition \ref{DEFooXVMSooXDIfZV}.} est \defe{fidèle}{représentation!fidèle} si elle est injective en tant qu'application \( G\to \GL(V)\). Ce ne sont pas chacun des \( \rho(g)\) qui doivent être injectifs. La dimension de \( V\) est le \defe{degré}{degré!d'une représentation} de la représentation \( (V,\rho)\).
\end{definition}

\begin{proposition}     \label{PROPooHNQOooSzeEFG}
    Soit un corps \( \eK\). Si \( G\) est un groupe dans \( \eM(n,\eK)\) (c'est-à-dire un groupe de matrices à coefficients dans \( \eK\)), alors l'application
    \begin{equation}
        \begin{aligned}
                \rho\colon G&\to \GL(\eK^n) \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    où \( f_A\) est l'application linéaire associée à \( A\) est une représentation de \( G\).
\end{proposition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Somme directe d'espaces vectoriels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Si \( V\) et \( W\) sont des espaces vectoriels, ce que nous notons \( V\oplus W\) n'est rien d'autre que l'espace vectoriel de l'ensemble \( V\times W\).

\begin{propositionDef}[\cite{ooXISFooTypogf,BIBoo4034093}]       \label{DEFooJKAWooKkkkwm}
    Si \( V\) et \( W\) sont des espaces vectoriels sur le même corps \( \eK\), alors les définitions
    \begin{enumerate}
        \item
            \( (v_1,w_1)+(v_2,w_2)=(v_1+v_2,w_1+w_2)\)
        \item
            \( \lambda(v,w)=(\lambda v,\lambda w)\)
    \end{enumerate}
    donnent une structure d'espace vectoriel sur \( V\times W\).

    Cet espace sera noté \( V\oplus W\) et est appelé \defe{somme directe}{somme directe} de \( V\) et \( W\).
\end{propositionDef}

\begin{definition}[Sous-espaces en somme directe\cite{BIBooJADFooBZwsEa}]       \label{DEFooIJDNooRUDUYF}
    Soient un espace vectoriel \( E\) ainsi que des sous-espaces vectoriels \( \{ F_i \}_{i\in I}\) (\( I\) est un ensemble fini ou infini). Nous disons que les \( F_i\) sont \defe{en somme directe}{somme directe} si pour tout élément \( u\in\sum_{i\in I}F_i\), il existe un unique ensemble \( \{ u_i \}_{i\in I}\) tel que
    \begin{enumerate}
        \item
            \( u=\sum_{i\in I}u_i\)
        \item
            \( u_i\in F_i\) pout tout \( i\),
        \item
            \( \{ j\in I\tq u_j\neq 0 \}\) est fini.
    \end{enumerate}
\end{definition}

\begin{lemma}[\cite{MonCerveau, BIBooJADFooBZwsEa}]       \label{LEMooDQMQooInVVDY}
    Soient un espace vectoriel \( E\) ainsi que des sous-espaces vectoriels \( F_i\). Nous avons équivalence entre les assertions suivantes.

    \begin{enumerate}
        \item
            Les \( F_i\) sont en somme directe\footnote{Définition \ref{DEFooIJDNooRUDUYF}.}.
        \item
            Si \( \sum_{i\in I}u_i=0\) avec \( u_i\in F_i\) et si \( \{ j\in I\tq u_j\neq 0 \}\) est fini, alors tous les \( u_i\) sont nuls.
        \item
            Chaque espace \( F_k\) est en somme directe avec la somme des précédents, c'est à dire que pour tout \( k\),
            \begin{equation}
                \left( \sum_{i=1}^{k-1}F_i \right)\cap F_k=\{ 0 \}.
            \end{equation}
        \item   \label{ITEMooPLXGooCOQgen}
            Pour tout \( k\),
            \begin{equation}
                F_k\cap\left( \sum_{i\neq k}F_i \right)=\{ 0 \}.
            \end{equation}
    \end{enumerate}
\end{lemma}

\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooCASNooEqisqa}
    Soient \( E\) un espace vectoriel de dimension finie, et deux sous-espaces \( F_1\) et \( F_2\) satisfaisant
    \begin{enumerate}
        \item
            \( F_1\cap F_2=\{ 0 \}\),
        \item
            \( \dim(F_1)+\dim(F_2)\geq \dim(E)\).
    \end{enumerate}
    Alors \( E=F_1\oplus F_2\).
\end{proposition}

\begin{proof}
    Soient une base \( \{ e_i \}_{i\in I}\) de \( F_1\) et \( \{ f_{\alpha} \}\) de \( F_2\). Nous commençons par prouver que la partie \( B=\{ e_i \}\cup \{ f_{\alpha} \}\) est libre.

    Supposons en effet avoir des coefficients \( a_i\) et \( b_{\alpha}\) tels que
    \begin{equation}
        \sum_ia_ie_i+\sum_{\alpha}b_{\alpha}f_{\alpha}=0.
    \end{equation}
    Cela implique que \( \sum_ia_ie_i=-\sum_{\alpha}b_{\alpha}f_{\alpha}\). Or \( \sum_ia_ie_i\in F_1\) et \( -\sum_{\alpha}b_{\alpha}f_{\alpha}\in F_2\). Donc les éléments \( \sum_ia_ie_i\) et \( \sum_{\alpha}b_{\alpha}f_{\alpha}\) sont dans \( F_1\cap F_2=\{ 0 \}\). Nous avons alors les égalités
    \begin{equation}
        \sum_ia_ie_i=0
    \end{equation}
    et
    \begin{equation}
        \sum_{\alpha}b_{\alpha}f_{\alpha}=0.
    \end{equation}
    La première implique \( a_i=0\) pour tout \( i\) et la seconde implique \( b_{\alpha}=0\) pour tout \( \alpha\).

    Donc \( B\) est une partie libre de \( E\) contenant \( \dim(F_1)+\dim(F_2)\geq \dim(E)\) éléments. La proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooZNLDooBISkJyBS} nous indique alors qu'en réalité \( \dim(F_1)+\dim(F_2)=\dim(E)\). Comme \( B\) est une partie libre contenant \( \dim(E)\) éléments, c'est une base par la proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooSGGCooOUsuBs}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Structure réelle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{enwiki:990936192,BIBoo4034093,MonCerveau}]     \label{DEFooCIFSooVmcNtE}
    Soit un espace vectoriel \( E\) sur \( \eC\). Il existe une application \( \sigma\colon E\to E\) telle que
    \begin{enumerate}
        \item
            \( \sigma^2=\sigma\) (involution)
        \item
            Pour tout \( \alpha,\beta\in \eR\), \( f(\alpha x+\beta y)=\bar \alpha \sigma(x)+\bar \beta \sigma(y)\).
    \end{enumerate}
    Une telle application est une \defe{structure réelle}{structure réelle} sur \( E\).
\end{propositionDef}

\begin{proof}
    La proposition \ref{PROPooHDCEooMhDjPi} nous permet de considérer une base \( \{ e_i \}_{i\in I}\) de \( E\). Alors nous définissons
    \begin{equation}
        \sigma\big( \sum_{i\in I}\lambda_ie_i \big)=\sum_{i\in I}\bar\lambda_ie_i.
    \end{equation}
    Notez que la somme est toujours finie.
\end{proof}

\begin{proposition}     \label{PROPooPZHPooNdarzg}
    Soit un espace vectoriel \( E\) sur \( \eC\) et une structure réelle\footnote{Définition \ref{DEFooCIFSooVmcNtE}.} \( \sigma\) sur \( E\). Nous posons
    \begin{equation}
        E_{\eR}=\{ v\in E\tq \sigma(v)=v \}.
    \end{equation}
    Alors
    \begin{enumerate}
        \item
            La partie \( E_{\eR}\) est un espace vectoriel réel.
        \item
            Nous avons la décomposition en somme directe\footnote{Définition \ref{DEFooJKAWooKkkkwm}.}
            \begin{equation}
                E=E_{\eR}\oplus iE_{\eR}.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    En plusieurs parties.
    \begin{subproof}
    \item[Espace vectoriel réel]
        Si \( v,w\in E_{\eR}\), alors
        \begin{equation}
            \sigma(v+w)=\sigma(v)+\sigma(w)=v+w,
        \end{equation}
        et si \( \lambda\in \eR\),
        \begin{equation}
            \sigma(\lambda x)=\lambda\sigma(x)=\lambda x.
        \end{equation}
        Donc \( E_{\eR}\) est un espace vectoriel réel.
    \item[Première somme directe]
        Nous définissons
        \begin{subequations}
            \begin{align}
                E^+ & =\{ v\in E\tq \sigma(v)=v   \},   \\
                E^- & =\{ v\in E\tq \sigma(v)=-v  \}
            \end{align}
        \end{subequations}
        Nous prouvons que
        \begin{equation}
            \begin{aligned}
                \psi\colon E^+\times E^-  & \to E \\
                              (a,b)         & \mapsto a+b
            \end{aligned}
        \end{equation}
        est un isomorphisme d'espaces vectoriels.

        Puisque \( \psi\) est linéaire, il suffit de prouver qu'elle est bijective.
        \begin{subproof}
        \item[Surjectif]
            Si \( v\in E\), alors en posant \( v_+=\frac{ 1 }{2}\big( v+\sigma(v) \big)\), \( v_-=\frac{ 1 }{2}\big( v-\sigma(v) \big)\), nous avons
            \begin{equation}
                \begin{aligned}[]
                    v&=v_+ + v_-, & v_+ & \in E^+,  & v_- & \in E^-,
                \end{aligned}
            \end{equation}
            et donc \( v=\psi(v_+,v_-)\).
        \item[Injectif]
            Supposons \( \psi(a,b)=\psi(\alpha,\beta)\). Alors \( a+b=\alpha+\beta\) et donc \( a-\alpha=\beta-b\). Comme \( a-\alpha\in E^+\)  et \( \beta-b\in E^- \), nous savons que \( a-\alpha=\beta-b\in E^+\cap E^-\). Étant donné que \( E^+\cap E^-=\{ 0 \}\), nous avons \( a-\alpha=\beta-b=0\).
    \end{subproof}
    Nous avons donc la somme directe \( E=E^+\oplus E^-\).
\item[Conclusion]
    Par définition, \( E^+=E_{\eR}\). Il nous reste à voir que \( E^-=iE^+\). Nous prouvons les inclusions dans les deux sens.
    \begin{subproof}
    \item[\( E^-\subset iE^+\)]
        Soit \( v\in E^-\). Nous avons \( iv\in E^+\); en effet
        \begin{equation}
            \sigma(iv)=\bar i\sigma(v)=-i\sigma(v)=iv.
        \end{equation}
        Donc \( iv\in E^+\) pour \( v\in E^-\).
    \item[$iE^+\subset E^-$]
        Soit \( v\in E^+\), et voyons que \( iv\in E^-\). En effet,
        \begin{equation}
            \sigma(iv)=-i\sigma(v)=-iv.
        \end{equation}
    \end{subproof}
            \end{subproof}
\end{proof}

\begin{normaltext}
    Lorsque nous avons une structure réelle \( \sigma\) sur un espace vectoriel complexe \( E\), nous écrivons \( E=E_{\eR}\oplus iE_{\eR}\) sans préciser dans la notation «\( E_{\eR}\)» que cet ensemble dépend du choix de \( \sigma\). En particulier si \( F\) est un sous-espace vectoriel de \( E\), nous utiliserons la notation \( F_{\eR}\) relativement à la même involution que celle utilisée pour \( E\).
\end{normaltext}

\begin{lemma}
    Soit un espace vectoriel complexe \( E\) muni d'une structure réelle \( \sigma\). Si \( F\) est un sous-espace de \( E\) alors \( F_{\eR}=E_{\eR}\cap F\).
\end{lemma}

\begin{proof}
    Par définition,
    \begin{equation}
        F_{\eR}=\{ v\in F\tq \sigma(v)=v \}.
    \end{equation}
    \begin{subproof}
    \item[\( F_{\eR}\subset F\)]
           C'est dans la définition de \( F_{\eR}\) (sous-ensemble de \( F\)).
       \item[\( F_{\eR}\subset E_{\eR}\)]
           Si \( v\in F_\eR\), alors \( \sigma(v)=v\). Mais cette égalité est précisément celle qui permet d'être dans \( E_{\eR}\).
    \end{subproof}
\end{proof}

Vous remarquerez que ce lemme ne fonctionne que parce que nous avons choisi la même structure réelle sur \( F\) que sur \( E\).
