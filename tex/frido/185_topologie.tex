% This is part of Mes notes de mathématique
% Copyright (c) 2008-2020
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Topologie sur l'ensemble des réels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooGKHYooMwHQaD}

Nous allons à présent donner la topologie sur \( \eR\) et ainsi résoudre les questions laissées en suspens lors de la construction des réels, voir~\ref{NormooHRDZooRGGtCd}.


Afin de pouvoir étudier la topologie des espaces métriques, il faut savoir quelques propriétés des réels parce que nous allons étudier la fonction distance qui est une fonction continue à valeurs dans les réels.

La valeur absolue de la définition~\ref{DefKCGBooLRNdJf}\ref{ItemooWUGSooRSRvYC} permet de définir une norme sur \( \eR\).
\begin{lemma}
    L'application
    \begin{equation}
         x\mapsto | x |
    \end{equation}
     est une norme sur $\eR$.
\end{lemma}

\begin{proof}
  Grâce au lemme \ref{LemooANTJooYxQZDw} et à la remarque \ref{RemooJCAUooKkuglX}, on a, pour tous \(x,\ y,\ \lambda \in \eR \):
\begin{enumerate}
\item $| x |=0$ implique $x=0$,
\item $| \lambda x |=| \lambda | |x |$,
\item $| x+y |\leq | x |+| y |$,
\end{enumerate}
et donc, les conditions de la définition \ref{DefNorme} sont immédiatement vérifiées.
\end{proof}

\begin{normaltext}      \label{ooLCMFooQjMaxV}
    Nous verrons plus tard que cette norme donne lieu à une structure d'espace topologique. Tant sur \( \eQ\) que sur \( \eR\), nous considérons la topologie métrique correspondant à cette norme (hors cas rarissimes qui seront signalés). De plus, nous utiliserons toujours les caractérisations de la proposition~\ref{PropooUEEOooLeIImr} pour parler de suites convergentes et de suites de Cauchy.
\end{normaltext}

\begin{proposition}     \label{PropooUHNZooOUYIkn}
    Les rationnels sont denses dans les réels.
\end{proposition}
\index{densité!de \( \eQ\) dans \( \eR\)}

\begin{proof}
    Soient \( r\in \eR\) et \( \epsilon\in \eR^+\). Nous devons prouver l'existence d'un rationnel dans \( B(x,\epsilon)\). Le lemme~\ref{LemooHLHTooTyCZYL} dit qu'il existe un rationnel dans \( \mathopen] x-\epsilon/2 , x+\epsilon/2 \mathclose[\) et donc dans \( B(x,\epsilon)\).
\end{proof}

\begin{proposition}[\cite{MonCerveau}] \label{PropSLCUooUFgiSR}
    Quel que soit le réel \( r\), il existe une suite croissante de rationnels convergente vers \( r\).
\end{proposition}

\begin{proof}
    Soient \( x\in \eR\) et \( \delta\in \eR\); vu que \( x-\delta\) et \( x\) sont des réels, le lemme~\ref{LemooHLHTooTyCZYL} donne un élément \( x_{\delta}\in \eQ \) tel que
    \begin{equation}
        x-\delta<x_{\delta}<x.
    \end{equation}
    Il suffit alors de pêcher parmi ces \( x_{\delta}\) pour trouver une suite croissante, et on montrera que cette suite converge vers \( x \).

    Soit \( x_0\) un rationnel plus petit que \( x\). Nous posons \( \delta_0=x-x_0\) et ensuite :
    \begin{subequations}
        \begin{numcases}{}
            \delta_i=x-x_i\\
            x_{i+1}=x_{\delta_i/2} \in \eQ.
        \end{numcases}
    \end{subequations}
    Ainsi nous avons pour tout \( i\) les inégalités
    \begin{equation}
        x_i=x-\delta_i<x-\frac{ \delta_i }{ 2 }<x_{i+1}<x.
    \end{equation}
    La suite \( (x_i) \) est donc une suite de rationnels, croissante et toujours plus petite que \( x\). Mais nous avons à chaque étape \( \delta_{i+1}<\frac{ \delta_i }{ 2 }\), ce qui implique que la suite des  \( \delta_i \) converge vers \( 0 \). Soit \( \epsilon>0\). Il existe \( k_0\) tel que pour tout \( k > k_0 \), \( \delta_k<\epsilon\). Pour un tel \( k \), nous avons alors
    \begin{equation}
        x_{k+1}\in B(x,\frac{ \delta_k }{ 2 })\subset B(x,\epsilon).
    \end{equation}
Tous les \( x_k \), pour \( k > k_0 + 1 \), sont tels que \( |x - x_k| < \epsilon \): la suite des \( x_k \) converge donc vers \( x \).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Compacité pour les réels}
%---------------------------------------------------------------------------------------------------------------------------

Pour la définition générale d'un compact, c'est \ref{DefJJVsEqs}.

\begin{proposition}     \label{PROPooBFSAooKSugMj}
    Les parties compactes de \( \eR\) sont fermées et bornées.
\end{proposition}

\begin{proof}
Prouvons d'abord qu'un ensemble compact est borné. Pour cela, supposons que $K$ est un compact non borné vers le haut\footnote{Nous laissons à titre d'exercice le cas où $K$ est borné par le haut et pas par le bas.}. Donc il existe une suite infinie de nombres strictement croissante $x_1<x_2<\ldots$ tels que $x_i\in K$. Prenons n'importe quel recouvrement ouvert de la partie de $K$ plus petite ou égale à $x_1$, et complétons ce recouvrement par les ouverts $\mO_i=]x_{i-1},x_i[$. Le tout forme bien un recouvrement de $K$ par des ouverts.

Il n'y a cependant pas moyen d'en tirer un sous recouvrement fini parce que si on ne prend qu'un nombre fini parmi les $\mO_i$, on en aura fatalement un maximum, disons $\mO_k$. Dans ce cas, les points $x_{k+1}$, $x_{k+1}$,\ldots ne seront pas dans le choix fini d'ouverts.

Cela prouve que $K$ doit être borné.

Pour prouver que $K$ est fermé, nous allons prouver que le complémentaire est ouvert. Et pour cela, nous allons prouver que si le complémentaire n'est pas ouvert, alors nous pouvons construire un recouvrement de $K$ dont on ne peut pas extraire de sous recouvrement fini.

Si $\eR\setminus K$ n'est pas ouvert, il possède un point, disons $x$, tel que tout voisinage de $x$ intersecte $K$. Soit $B(x,\epsilon_1)$, un de ces voisinages, et prenons $k_1\in K\cap B(x,\epsilon_1)$. Ensuite, nous prenons $\epsilon_2$ tel que $k_1$ n'est pas dans $B(x,\epsilon_1)$, et nous choisissons $k_2\in K\cap B(x,\epsilon_2)$. De cette manière, nous construisons une suite de $k_i\in K$ tous différents et de plus en plus proches de $x$. Prenons un recouvrement quelconque par des ouverts de la partie de $K$ qui n'est pas dans $B(x,\epsilon_1)$. Les nombres $k_i$ ne sont pas dans ce recouvrement.

Nous ajoutons à ce recouvrement les ensembles $\mO=]k_i,k_{i+1}[$. Le tout forme un recouvrement (infini) par des ouverts dont il n'y a pas moyen de tirer un sous recouvrement fini, pour exactement la même raison que la première fois.
\end{proof}

\begin{theorem}[Borel-Lebesgue]   \label{ThoBOrelLebesgue}
    Un intervalle de \( \eR\) est compact si et seulement si il est de la forme \( \mathopen[ a , b \mathclose]\).
\end{theorem}

\begin{proof}
    Tous les intervalles de \( \eR\) sont listés dans la proposition \ref{PROPooHPMWooQJXCAS}. Un compact est fermé et borné (proposition \ref{PROPooBFSAooKSugMj}). Donc les intervalles dont une borne est \( \pm\infty\) ne sont pas compacts. Parmi les intervalles \( \mathopen] a , b \mathclose[\), \( \mathopen] a , b \mathclose]\), \( \mathopen[ a , b \mathclose[\) et \( \mathopen[ a , b \mathclose]\), seul le dernier est fermé. Nous avons prouvé que si un intervalle est compact, alors il est de la forme \( \mathopen[ a , b \mathclose]\). 

    Nous prouvons à présent l'implication inverse : tous les intervalles de la forme \( \mathopen[ a , b \mathclose]\) sont compacts.

    Soit $\Omega$, un recouvrement du segment $[a,b]$ par des ouverts, c'est-à-dire que
    \begin{equation}
        [a,b]\subseteq\bigcup_{\mO\in\Omega}\mO.
    \end{equation}
    Nous notons par $M$ le sous-ensemble de $[a,b]$ des points $m$ tels que l'intervalle $[a,m]$ peut être recouvert par un sous-ensemble fini de $\Omega$. C'est-à-dire que $M$ est le sous-ensemble de $[a,b]$ sur lequel le théorème est vrai. Le but est maintenant de prouver que $M=[a,b]$.
    \begin{description}
        \item[$M$ est non vide] En effet, $a\in M$ parce que il existe un ouvert $\mO\in\Omega$ tel que $a\in\mO$. Donc $\mO$ tout seul recouvre l'intervalle $[a,a]$.
        \item[$M$ est un intervalle] Soient $m_1$, $m_2\in M$. Le but est de montrer que si $m'\in[m_1,m_2]$, alors $m'\in M$. Il y a un sous recouvrement fini de l'intervalle $[a,m_2]$ (par définition de $m_2\in M$). Ce sous recouvrement fini recouvre évidemment aussi $[a,m']$ parce que $[a,m']\subseteq [a,m_2]$, donc $m'\in M$.
        \item[$M$ est une ensemble ouvert] Soit $m\in M$. Le but est de prouver qu'il y a un ouvert autour de $m$ qui est contenu dans $M$. Mettons que $\Omega'$ soit un sous recouvrement fini qui contienne l'intervalle $[a,m]$. Dans ce cas, on a un ouvert $\mO\in\Omega'$ tel que $m\in\mO$. Tous les points de $\mO$ sont dans $M$, vu qu'ils sont tous recouverts par $\Omega'$. Donc $\mO$ est un voisinage de $m$ contenu dans $M$.
        \item[$M$ est un ensemble fermé] $M$ est un intervalle qui commence en $a$, en contenant $a$, et qui finit on ne sait pas encore où. Il est donc soit de la forme $[a,m]$, soit de la forme $[a,m[$. Nous allons montrer que $M$ est de la première forme en démontrant que $M$ contient son supremum $s$. Ce supremum est un élément de $[a,b]$, et donc il est contenu dans un des ouverts de $\Omega$. Disons $s\in\mO_s$. Soit $c$, un élément de $\mO_s$ strictement plus petit que $c$; étant donné que $s$ est supremum de $M$, cet élément $c$ est dans $M$, et donc on a un sous recouvrement fini $\Omega'$ qui recouvre $[a,c]$. Maintenant, le sous recouvrement constitué de $\Omega'$ et de $\mO_s$ est fini et recouvre $[a,s]$.
    \end{description}
    Nous pouvons maintenant conclure : le seul intervalle non vide de $[a,b]$ qui soit à la fois ouvert et fermé est $[a,b]$ lui-même (proposition \ref{PropHSjJcIr}), ce qui prouve que $M=[a,b]$, 
    et donc que $[a,b]$ est compact\footnote{Si vous n'aimez pas le coup du fermé et ouvert, le lemme \ref{LemOACGWxV} donne une autre preuve.}.
\end{proof}


\begin{lemma}[\cite{JUwQXOF}]\label{LemOACGWxV}
    Si \( a<b\in \eR\) alors le segment \( \mathopen[ a , b \mathclose]\) est compact\footnote{Définition~\ref{DefJJVsEqs}}.
\end{lemma}
\index{compact!intervalle \( \mathopen[ a , b \mathclose]\)}

\begin{proof}
    Soit \( \{ \mO_i \}_{i\in I}\) un recouvrement de \( \mathopen[ a , b \mathclose]\) par des ouverts. Nous posons
    \begin{equation}
        M=\{ x\in\mathopen[ a , b \mathclose]\tq \mathopen[ a , x \mathclose] \text{ admet un sous-recouvrement fini extrait de } \{ \mO_i \}_{i\in I} \}.
    \end{equation}
    Notre but est de prouver que \( b\in M\).
    \begin{subproof}

    \item[\( a\) est dans \( M\)]

        Le point \( a\) est naturellement dans un des \( \mO_i\). L'intervalle \( \mathopen[ a , a \mathclose]\) est donc recouvert par un seul des \( \mO_i\).

    \item[\( M\) est un intervalle]

        Soient \( m\in M\) et \( m'\in\mathopen[ a , m [\). Le sous-recouvrement fini qui recouvre \( \mathopen[ a , m \mathclose]\) recouvre a fortiori \( \mathopen[ a , m' \mathclose]\).

    \item[Les trois possibilités restantes]
        À ce niveau de la preuve, il reste trois possibilités pour \( M\) soit il est de la forme \( \mathopen[ a , c \mathclose]\) ou \( \mathopen[ a , c [\) avec \( c<b\), soit il est de la forme \( \mathopen[ a , b \mathclose]\). Nous allons maintenant éliminer les deux premiers cas.

    \item[Ce que \( M\) n'est pas]

        D'abord \( M\) n'est pas de la forme \( \mathopen[ a , c [\) avec \( c<b\). Par l'absurde, commençons par considérer \( \mO_{i_0}\) un ouvert du recouvrement qui contient \( c\); choisissons  \(m \in \mO_{i_0}\) tel que \( m<c\). Alors \( m \in M \), et, si nous joignons \( \mO_{i_0}\) à un recouvrement fini de \( \mathopen[ a , m \mathclose]\) alors nous avons un recouvrement fini de \( \mathopen[ a , c \mathclose]\). On en déduit \( c\in M\).

        Ensuite \( M\) n'est pas de la forme \( \mathopen[ a , c \mathclose]\) avec \( c<b\). En effet si on a un recouvrement fini de \( \mathopen[ a , c \mathclose]\) par des ouverts, alors un de ces ouverts contient \( c\) et donc contient des éléments de \( \mathopen[ a , b \mathclose]\) plus grands que \( c\).
    \end{subproof}
    Nous déduisons que \( M=\mathopen[ a , b \mathclose]\) et qu'il est possible d'extraire un sous-recouvrement fini recouvrant \( \mathopen[ a , b \mathclose]\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]\label{LemCKBooXkwkte}
    Si \( K_1\) et \( K_2\) sont des compacts dans \( \eR\) alors \( K_1\times K_2\) est compact dans \( \eR^2\).
\end{lemma}

\begin{proof}
    Soit \( \{ \mO_i \}_{i\in I}\) un recouvrement de \( K_1\times K_2\) par des ouverts; grâce au lemme~\ref{LemOWVooZKndbI} nous pouvons supposer que ce sont des carrés. Pour chaque \( x\in K_1\), l'ensemble \( \{ x \}\times K_2\) est compact et donc recouvert par un nombre fini des \( \mO_i\). Soit \( R_x\) un ensemble fini des \( \mO_i\) recouvrant \( \{ x \}\times K_2\).

    Vu que \( R_x\) est une collection finie de carrés nous pouvons considérer \( m_x\), le minimum des rayons. L'ensemble \( K_1\) est recouvert par les boules \( B(x,m_x)\) et il existe donc une collection finie de \( \{ x_i \}_{i\in A}\) tels que \( B(x_i,m_{x_i})\) recouvre \( K_1\).

    Alors \( \{ R_{x_i} \}_{i\in A}\) recouvre \( K_1\times K_2\) parce que \( R_{x_i}\) recouvre l'ensemble \( B(x_i,m_{x_i})\times \{ K_2 \}\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Conséquence: les fermés bornés sont compacts}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Théorème de Borel-Lebesgue] \label{ThoXTEooxFmdI}
    Une partie d'un espace vectoriel normé réel de dimension finie est compacte si et seulement si elle est fermée et bornée.
\end{theorem}
\index{théorème!Borel-Lebesgue}
\index{compact!fermé et borné}

\begin{proof}
    Sens direct.
    \begin{subproof}
    \item[Compact implique borné]
        En effet si \( K\) est non borné dans \( E\) alors \( K\) contient une suite \( (x_n)\) avec \( \| x_n \|>n\). Les boules \( B_i(x_i,\frac{ 1 }{3})\) sont disjointes. On pose \( \mO_0=\complement\bigcup_i\overline{ B(x_i,\frac{1}{ 5 }) }\), qui est ouvert comme complément d'un fermé. Pour \( i\geq 1\) nous posons \( \mO_i=B(x_i,\frac{1}{ 4 })\). Nous avons
        \begin{equation}
            K\subset\bigcup_{i\in \eN}\mO_i
        \end{equation}
        mais vu que \( x_i\) est uniquement dans \( \mO_i\), nous ne pouvons pas extraire de sous-recouvrement fini.
    \item[Compact implique fermé]
        Cela est la proposition~\ref{PropUCUknHx}.
    \end{subproof}
    Sens réciproque.
    \begin{subproof}
    \item[Un intervalle fermé et borné est compact dans \( \eR\)]
        C'est le lemme~\ref{LemOACGWxV}.
    \item[Un produit de segments est compact]
        Le produit de deux compacts de \( \eR\) est un compact dans \( \eR^2\) par le lemme~\ref{LemCKBooXkwkte}.
    \item[Un fermé et borné est compact]
        Soit \( K\) fermé et borné. Vu que \( K\) est borné, il est contenu dans un produit de segments. L'ensemble \( K\) est donc compact parce que fermé dans un compact, lemme~\ref{LemnAeACf}.
    \end{subproof}
\end{proof}

\begin{example}[Compacité de la boule unité]
    La boule unité fermée \( \overline{ B(0,1) }\) d'un espace vectoriel normé de dimension finie est compacte parce que fermée et bornée. En dimension infinie, cela n'est plus le cas. Certes la boule unité est encore fermée et bornée, mais elle n'est plus compacte. En effet nous allons donner un recouvrement par des ouverts duquel il ne sera pas possible d'extraire un sous-recouvrement fini.

    Autour de chacune des extrémités des vecteurs de base, nous considérons la boule \( A_i=B(e_i,\frac{1}{ 3 })\). Ensuite aussi l'ouvert
    \begin{equation}
        B(0,1)\setminus\bigcup_i\overline{ B(e_i,\frac{1}{ 4 })}.
    \end{equation}
    Le tout recouvre \( B(0,1)\) mais toutes les premières boules sont nécessaires.
\end{example}
\index{compact!boule unité}

Le théorème de Bolzano-Weierstrass \ref{THOooRDYOooJHLfGq} nous permettra de prouver plus simplement la non compacité en dimension infinie. Voir l'exemple~\ref{ExEFYooTILPDk}.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Suites et limites dans les réels}
%---------------------------------------------------------------------------------------------------------------------------

\subsubsection{Limites, convergence}
%////////////////////////////////

Dans le cas de suites réelles, nous avons la caractérisation suivante qui est souvent donnée comme une définition lorsque seule la topologie sur \( \eR\) est considérée.
\begin{proposition}[Limite d'une suite numérique]	\label{PropLimiteSuiteNum}
	La suite $(x_n)$ est convergente si et seulement s'il existe un réel $\ell$ tel que
	\begin{equation}		\label{EqDefLimSuite}
		\forall \epsilon>0,\,\exists N\in\eN\tq\forall n\geq N,\,| x_n-\ell |<\epsilon.
	\end{equation}
	Dans ce cas, le nombre $\ell$ est la limite de la suite $(x_n)$.
\end{proposition}
\index{convergence!suite numérique}
\index{limite!suite numérique}

\begin{propositionDef}		\label{PROPooOSXCooJWXkWH}
    Une suite $(x_n)$ dans un espace vectoriel normé $V$ est convergente\footnote{Définition \ref{DefXSnbhZX}.} si et seulement si il existe un élément $\ell\in V$ tel que
	\begin{equation}
		\forall \varepsilon>0,\,\exists N\in\eN\tq n\geq N\Rightarrow \| x_n-l \|<\varepsilon.
	\end{equation}
	Dans ce cas, $\ell$ est la limite de la suite $(x_n)$.
\end{propositionDef}
    \index{convergence!dans un espace vectoriel normé}

\begin{proof}
    En deux parties.
    \begin{subproof}
        \item[Sens direct]
            Si \( x_n\to \ell\) et si \( \epsilon>0\) il existe \( N_{\epsilon}\) tel que pour tout \( n\geq N\) nous avons \( x_n\in B(\ell,\epsilon)\) (parce que cette boule est un ouvert contenant \( \ell\)). Vue la définition d'une boule, cette condition s'écrit bien \( \| x_n-\ell \|<\epsilon\).

        \item[Sens inverse]

            Dans l'autre sens, soit \( \mO\) un ouvert contenant \( \ell\). Par définition de la topologie, il existe \( \epsilon>0\) tel que \( B(\ell,\epsilon)\subset \mO\). La condition \eqref{EqDefLimSuite} nous assure qu'il existe \( N_{\epsilon} \) tel que pour tout \( n\geq N_{\epsilon}\) nous ayons
            \begin{equation}
             x_n\in B(\ell,\epsilon)\subset\mO,
             \end{equation}
            ce qui assure que la suite \( (x_n)\) converge vers \( \ell\) pour la topologie métrique de \( V\).
    \end{subproof}
\end{proof}

Une façon équivalente d'exprimer le critère \eqref{EqDefLimSuite} est de dire que pour tout $\epsilon$ positif, il existe un rang $N\in\eR$ tel que l'intervalle $\mathopen[ \ell-\epsilon , \ell+\epsilon \mathclose]$ contient tous les termes $x_n$ au-delà de $N$.

Il est à noter que le rang $N$ dont il est question dans la définition de suite convergente dépend de~$\epsilon$.

\begin{definition}      \label{DEFooHNCTooMlQUvx}
    Nous disons qu'une suite réelle $(x_n)$ converge\footnote{Voir la définition~\ref{PropLimiteSuiteNum} pour plus de détail.} vers $\ell$ lorsque pour tout $\varepsilon$, il existe un $N$ tel que
    \begin{equation}
        n>N\Rightarrow | x_n-\ell |\leq\varepsilon.
    \end{equation}
\end{definition}

Le concept fondamental de cette définition est la notion de valeur absolue qui permet de donner la «distance» entre deux réels. Dans un espace vectoriel normé quelconque, cette notion est généralisée par la distance associée à la norme (définition~\ref{DefNorme}). Nous pouvons donc facilement définir le concept de convergence d'une suite dans un espace vectoriel normé.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Opérations sur les limites}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooIQOAooJPMoDD}
    Soient des suites à valeurs réelles \( (a_i)\) et \( (b_j)\) si elles sont convergentes, alors la suite \( ab\) est convergente et
    \begin{equation}
        \big( \lim_ia_i \big)\big( \lim_jb_j \big)=\lim_i(a_ib_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous nommons \( a\) et \( b\) les limites des suites \( (a_i)\) et \( (b_j)\). Soit \( \epsilon>0\) ainsi que \( i\in \eN\). Nous avons la majoration
    \begin{subequations}
        \begin{align}
            | a_ib_i-ab |&\leq | a_ib_i-a_ib |+| a_ib-ab |\\
            &\leq | a_i | |b_i-b |+b| a_i-a |.
        \end{align}
    \end{subequations}
    Vu que la suite \( (a_i)\) est convergente, elle est bornée. Nous pouvons donc majorer \( | a_i |\) par \( R>0\) qui ne dépend pas de \( i\). Soit \( \eta>0\) tel que \( (R+b)\eta<\epsilon\). Alors en prenant \( i\) assez grand pour que \( | b_i-b |<\eta\) et \( | a_i-a |<\eta\), nous avons bien
    \begin{equation}
        | a_ib_i-ab |\leq (R+b)\eta<\epsilon.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooICZMooGfLdPc}
    Soient des suites \( (x_n)\) et \( (y_n)\) dans un espace vectoriel normé \( V\). Si \( x_n\stackrel{V}{\longrightarrow}x\) et \( y_n\stackrel{V}{\longrightarrow}y\), alors
    \begin{equation}
        x_n+y_n\stackrel{V}{\longrightarrow}x+y.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). Nous considérons \( N\) tel que si \( n\geq N\), alors \( \| x_n-x \|\leq \epsilon\) et \( \| y_n-y \|\leq \epsilon\). En utilisant l'inégalité \ref{DefNorme}\ref{ItemDefNormeiii},
    \begin{equation}
        \| x_y+y_n-(x+y) \|\leq \| x_n-x \|+\| y_n-y \|\leq 2\epsilon.
    \end{equation}
    Donc la suite \( (x_n+y_n)\) converge vers \( x+y\).
\end{proof}

 
\subsection{Exemples}
%//////////////////////////

\begin{example}
	Quelques suites usuelles.
	\begin{enumerate}
		\item
			La suite $x_n=\frac{1}{ n }$ converge vers $0$.
		\item
			La suite $x_n=(-1)^n$ ne converge pas.
	\end{enumerate}
\end{example}

Deux limites pour voir comment ça fonctionne.
\begin{lemma}
    Si \( r>1\) nous avons :
    \begin{enumerate}
        \item
            \( \lim_{n\to \infty} r^n=\infty\).
        \item
            \( \lim_{n\to \infty} \frac{ r^n }{ n }=\infty\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que \( r>1\) nous pouvons écrire \( r=1+\delta\) avec \( \delta>0\). La formule du binôme de Newton \eqref{EqNewtonB} nous donne
    \begin{equation}
        (1+\delta)^n=\sum_{k=0}^n{k\choose n}\delta^k>{1\choose n}\delta=n\delta.
    \end{equation}
    La proposition \ref{ThoooKJTTooCaxEny} (\( \eR\) est archimédien) nous indique que \( n\delta\) est arbitrairement grand lorsque \( n\) est grand, quelle que soit \( \delta>0\). Cela finit la preuve de la première limite.

    Pour la seconde, nous posons \( a_n=\frac{ r^n }{ n }\). Nous avons
    \begin{equation}
        \frac{ a_{n+1} }{ a_n }=\frac{ n }{ n+1 }r.
    \end{equation}
    Vu que \( \frac{ n }{ n+1 }\to 1\), la suite \( \frac{ n }{ n+1 }r\) tend vers \( r>0\), et en particulier pour tout \( \delta>0\) tel que \( r>1+\delta\), il existe \( N\in \eN\) tel que, pour tout \( n > N \),
    \begin{equation}
        \frac{ n }{ n+1 }r>1+\delta.
    \end{equation}
    Soit maintenant \( k\in \eN\). En utilisant un produit télescopique,
    \begin{equation}
        a_{N+k}=a_N\frac{ a_{N+1} }{ a_N }\frac{ a_{N+2} }{ a_{N+1} }\cdots\frac{ a_{N+k} }{ a_{N+k-1} }>a_N(1+\delta)^{k-1}.
    \end{equation}
    Or \( (1+\delta)^{k-1}\) tend vers \( \infty\) lorsque \( k\to \infty\) par le premier point. Donc nous avons \( \lim_{n\to \infty} r^n/n=\infty\).
\end{proof}


\begin{definition}      \label{DEFooEWRTooKgShmT}
    Nous disons que deux suites \( (u_n)\) et \( (v_n)\) sont \defe{équivalentes}{equivalence@équivalence!de suites} s'il existe une fonction \( \alpha\colon \eN\to \eR\) telle que
    \begin{enumerate}
        \item
            pour tout \( n\) à partir d'un certain rang, \( u_n=v_n\alpha(n)\)
        \item
            \( \alpha(n)\to 1\).
    \end{enumerate}
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Suites croissantes et bornées}
%---------------------------------------------------------------------------------------------------------------------------

Une suite est dite \defe{contenue}{} dans un ensemble $A$ si $x_n\in A$ pour tout $n$. Une suite est \defe{bornée supérieurement}{bornée!suite} s'il existe un $M$ tel que $x_n\leq M$ pour tout $n$. De la même manière, la suite est bornée inférieurement s'il existe un $m$ tel que $x_n\geq m$ pour tout $n$.

Le lemme suivant est souvent utilisé pour prouver qu'une suite est convergente.
\begin{lemma}		\label{LemSuiteCrBorncv}
	Une suite croissante et bornée supérieurement converge. Une suite décroissante bornée inférieurement est convergente.
\end{lemma}

Une erreur courante est de croire que la borne est la limite : le lemme n'affirme pas ça. Par contre il est vrai que la borne donne \ldots hum \ldots une borne inférieure (ou supérieure) pour la limite.

\begin{theorem}[Bolzano-Weierstrass, thème \ref{THEMEooQQBHooLcqoKB}]     \label{THOooRDYOooJHLfGq}
    Toute suite contenue dans un compact admet une sous-suite convergente.
\end{theorem}

\begin{proof}
    Nous faisons la preuve par l'absurde en supposant que \( (x_k)\) n'admette pas de sous-suite convergente. Soit \( a\in K\); aucune sous-suite de \( (x_k)\) ne converge vers \( a\). En particulier, il existe un voisinage ouvert \( \mO_a\) de \( a\) et une partie finie \( I_a\) de \( \eN\) tel que \( x_k\in \mO_a\) seulement pour \( k\in I_a\).

    Les ouverts \( \mO_a\) recouvrent \( K\); nous pouvons en extraire un sous-recouvrement fini (c'est la définition \ref{DefJJVsEqs} de la compactié). Nous avons donc des points \( a_1,\ldots, a_n\) tels que 
    \begin{equation}
        K\subset \bigcup_{i=1}^n\mO_{a_i}
    \end{equation}
    et tels que pour chaque \( \mO_{a_i}\), nous avons \( x_k\in \mO_{a_i}\) seulement pour \( k\in I_{a_i}\). Bien entendu, toute la suite est dans \( K\) et donc dans l'union.

    En conclusion, nous avons \( \eN=\bigcup_{i=1}^n I_{a_i}\), ce qui prouve que \( \eN\) est un ensemble fini. Contradiction avec la proposition \ref{PROPooBYKCooGDkfWy} qui dit que \( \eN\) est infini.
\end{proof}

% Inutile de replacer cette proposition plus loin : on en a besoin pour démontrer Weierstrass. Quitte à maintenir, il faut réénoncer pour un espace vectoriel normé et prouver.
\begin{proposition}		\label{PropCvRpComposante}
	Une suite $(x_n)$ dans $\eR^m$ est convergente dans $\eR^m$ si et seulement si les suites de chaque composante sont convergentes dans $\eR$. Dans ce cas nous avons
	 \begin{equation}
		 \lim x_n=\Big( \lim(x_n)_1,\lim (x_n)_2,\ldots,\lim (x_n)_m \Big)
	 \end{equation}
	 où $(x_n)_k$ dénote la $k$-ième composante de $(x_n)$.
\end{proposition}

\begin{example}
	La suite $x_n=\big( \frac{1}{ n },1-\frac{1}{ n } \big)$ converge vers $(0,1)$ dans $\eR^2$. En effet, en utilisant la proposition~\ref{PropCvRpComposante}, nous devons calculer séparément les limites
	\begin{equation}
		\begin{aligned}[]
			\lim\frac{1}{ n }&=0\\
			\lim\big( 1-\frac{1}{ n } \big)&=1.
		\end{aligned}
	\end{equation}
\end{example}

\begin{example}
	Étant donné que la suite $(-1)^n$ n'est pas convergente, la suite $x_n=\big( (-1)^n,\frac{1}{ n } \big)$ n'est pas convergente dans $\eR^2$.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Suites adjacentes}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{ooZZNWooSIipwW}]       \label{DEFooDMZLooDtNPmu}
    Les suites \( (a_n)\) et \( (b_n)\) sont \defe{adjacentes}{suites adjacentes} si l'une est croissante, l'autre décroissante et si \( a_n-b_n\to 0\).
\end{definition}

\begin{theorem}[Théorème des suites adjaentes]      \label{THOooZJWLooAtGMxD}
    Nous considérons des suites adjacentes \( (a_n)\) et \( (b_n)\) avec \( (a_n)\) croissante et \( (b_n)\) décroissante. Alors
    \begin{enumerate}
        \item
            \( b_n\geq a_n\) pour tout \( n\),
        \item
            \( a_n\leq b_q\) pour tout \( n\) et \( q\). C'est-à-dire que toute la suite \( a\) est plus petite que toute la suite \( b\).
        \item
            les suites \( a\) et \( b\) sont convergentes,
        \item
            les suites \( a\) et \( b\) convergent vers la même limite, notée \( \ell\),
        \item
            nous avons \( a_n\leq \ell\leq b_n\) pour tout \( n\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    La suite \( n\mapsto b_n-a_n\) est décroissante parce que \( b_n-a_n\geq b_{n+1}-a_{n+1}\). Comme en plus \( b_a-a_n\to 0\) nous avons
    \begin{equation}
        b_n-a_n\geq 0
    \end{equation}
    pour tout \( n\in \eN\). De plus \( a_n\leq b_0\) pour tout \( n\) parce que si \( a_N>b_0\) alors, \( b\) étant décroissante, \( a_N>b_0\geq b_N\) qui est contraire à ce que nous venons de prouver. La suite \( a\) étant croissante et majorée, elle est convergente\footnote{Proposition \ref{LemSuiteCrBorncv}.}; notons \( \ell\) sa limite.

    La suite \( b\) peut maintenant être écrite par
    \begin{equation}
        b_n=(b_n-a_n)+a_n
    \end{equation}
    qui est une somme de deux suites convergentes. Elle est donc convergente et sa limite est la somme des limites\footnote{Proposition \ref{PROPooICZMooGfLdPc}.}, donc
    \begin{equation}
        \lim_{n\to \infty} b_n=\lim_{n\to \infty} (b_n-a_n)+a_n=0+\ell=\ell.
    \end{equation}
    Voila. Donc les suites \( a\) et \( b\) convergent et ont la même limite.

    Pour tout \( n,q\in \eN\) nous avons l'inégalité \( a_n\leq b_q\). En prenant la limite \( n\to \infty\) nous trouvons
    \begin{equation}
        \ell\leq b_q
    \end{equation}
    pour tout \( q\). Et de la même façon, \( b_n\geq a_q\) donne \( \ell\geq a_q\). L'un avec l'autre donne
    \begin{equation}
        a_q\leq \ell\leq b_q
    \end{equation}
    pour tout \( q\in \eN\).
\end{proof}

\begin{proposition}[\cite{ooXFPIooCLUvzV}]      \label{PROPooXOOCooGMqJNe}
    Soit une suite \( (a_n)\) dans \( \eR\).  Nous supposons que les suites extraites \( (a_{2n})\) et \( (a_{2n+1})\) convergent vers la même limite notée \( \ell\).

    Alors \( a_n\to \ell\).
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). Il existe \( N_1\) tel que \( | a_{2n}-\ell |\leq \epsilon\) dès que \( n\geq N_1\). Il existe également \( N_2\) dès que \( | a_{2n+1}-\ell |\leq \epsilon\) dès que \( n\geq N_2\).

    Nous posons \( N=\max\{ 2N_1,2N_2+2 \}\) et nous avons, pour tout \( n\geq N\) :
    \begin{equation}
        | a_n-\ell |\leq \epsilon,
    \end{equation}
    c'est-à-dire que \( a\to \ell\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Limite supérieure et inférieure}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}      \label{ooMVZAooVVCOnP}
    Soit \( (a_n)\) une suite dans \( \bar \eR\). Les limites suivantes existent dans \( \bar \eR\)
    \begin{equation}
        \limsup_{n\to\infty}a_n=\lim_{n\to \infty}\big( \sup_{k\geq n}a_k \big)
    \end{equation}
    et
    \begin{equation}
        \liminf_{n\to \infty}a_n=\lim_{n\to\infty}\big( \inf_{k\geq n}a_k \big).
    \end{equation}
    Elles sont nommées \defe{limite supérieure}{limite!supérieure} et la \defe{limite inférieure}{limite!inférieure} de la suite \( (a_k)\).
\end{lemmaDef}
\nomenclature[Y]{\( \limsup a_n\)}{limite supérieure}
\nomenclature[Y]{\( \liminf a_n\)}{limite inférieure}

\begin{proof}
    Pour la limite supérieure, l'ensemble des \( k\geq n\) est de plus en plus petit lorsque \( n\) grandit. Donc les ensembles \( A_n=\{ a_k\tq k\geq n \}\) sont emboîtés et la suite \( n\to \sup A_n\) est une suite décroissante. Elle a donc une limite dans \( \bar \eR\).
\end{proof}

\begin{normaltext}      \label{ooEEQJooRMFzVR}
    En ce qui concerne les suites d'ensembles, utiles en théorie des probabilités, nous définissons de même. Si les \( A_n\) sont des parties de \( \Omega\), nous définissons la \defe{limite supérieure}{limite!supérieure} et la \defe{limite inférieure}{limite!inférieure} de la suite \( A_n\) par
\begin{equation}
    \limsup_{n\to\infty}A_n=\bigcap_{n\geq 1}\bigcup_{k\geq n}A_k
\end{equation}
et
\begin{equation}
    \liminf_{n\to\infty}A_n=\bigcup_{n\geq 1}\bigcap_{k\geq n}A_k
\end{equation}

Nous avons
\begin{equation}
    \limsup A_n=\{ \omega\in\Omega\tq \omega\in A_n\text{pour une infinité de } n \}.
\end{equation}
\end{normaltext}

\begin{lemma}     \label{ooAQTEooYDBovS}
    Nous avons les formules pratiques suivantes :
    \begin{subequations}
        \begin{align}
            \limsup a_n&=\inf_{n\geq 1}\big( \sup_{k\geq n}a_k \big)\\
            \liminf a_n&=\sup_{n\geq 1}\big( \inf_{k\geq n}a_k \big).
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    La suite \( n\mapsto \sup_{k\geq n}a_k\) est une suite décroissante, donc la limite est l'infimum. Même argument pour l'autre.
\end{proof}

\begin{lemma}       \label{ooIQIKooXWwAmM}
    La suite \( (a_n)\) dans \( \eR\) converge si et seulement si
    \begin{equation}
        \limsup a_n=\liminf a_n.
    \end{equation}
    Dans ce cas, \( \lim a_n=\limsup a_n=\liminf a_n\).
\end{lemma}

\begin{proof}
    Nous commençons par supposer que \( \limsup a_n=\liminf a_n=l\), et nous prouvons que \( \lim a_n\) existe et vaut \( l\). Soit \( \epsilon>0\). Il existe \( N\) tel que si \( n\geq N\) nous avons
    \begin{equation}
        \big| \sup_{k\geq n}a_k-l \big|<\epsilon
    \end{equation}
    et
    \begin{equation}
        \big| \inf_{k\geq n}a_k-l \big|<\epsilon.
    \end{equation}
    Pour tout \( k\geq N\) nous avons alors \( a_k\leq l+\epsilon\) et \( a_k\geq l-\epsilon\). Cela donne \( a_n\in B(l,\epsilon)\), c'est-à-dire \( a_k\to l\) par la proposition~\ref{PropLimiteSuiteNum}.

    Dans l'autre sens, nous supposons que \( \lim_n a_n=l\) et nous prouvons que les limites supérieures et inférieures sont toutes deux égales à \( l\). Soit \( \epsilon>0\) et \( N_{\epsilon}\) tel que \( | a_n-l |<\epsilon\) pour tout \( n\geq N_{\epsilon}\). Si \( n\geq N_{\epsilon}\) nous avons
    \begin{equation}
        \big| \sup_{k\geq n}a_k-l \big|\leq \epsilon
    \end{equation}
    et donc la limite de \( \sup_{k\geq n}a_k\) lorsque \( n\to \infty\) est bien \(l\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ouverts, voisinage, topologie}
%---------------------------------------------------------------------------------------------------------------------------

Lorsque $x\in E$, nous rappelons qu'un voisinage\footnote{Définition~\ref{DefFermeVoisinage}.} de $x$ est n'importe quel sous-ensemble de $E$ qui contient une boule ouverte centrée en $x$. La proposition \ref{ThoPartieOUvpartouv} nous dit qu'un ensemble est ouvert s'il contient un voisinage de chacun de ses points. Au passage, rappelons que l'ensemble vide est ouvert.

Pour rappel, la remarque \ref{RemQDRooKnwKk}\ref{ITEMooUIHJooXAFaIz} dit que l'ensemble des boules ouvertes d'un espace métrique génère la topologie de l'espace.

Nous rappelons qu'une partie $A$ d'un espace métrique est dite bornée\footnote{Définition~\ref{DefEnsembleBorne}.} s'il existe une boule\footnote{À titre d'exercice, convainquez-vous que l'on peut dire boule \emph{ouverte} ou \emph{fermée} au choix sans changer la définition.} qui contient $A$.

Mais revenons à \( \eR \)\dots
\begin{lemma}  \label{LemSupOuvPas}
    Une partie ouverte de \( \eR\) ne contient pas son supremum.
\end{lemma}

\begin{proof}
Soit $\mO$, un ensemble ouvert et $s$, son supremum. Si $s$ était dans $\mO$, on aurait un voisinage $B=B(s,r)$ de $s$ contenu dans $\mO$. Le point $s+r/2$ est alors à la fois dans $\mO$ et plus grand que $s$, ce qui contredit le fait que $s$ soit un supremum de $\mO$.
\end{proof}

Par le même genre de raisonnements, on montre que l'union et l'intersection de deux ouverts sont encore des ouverts.

\begin{remark}
L'intersection d'une \emph{infinité} d'ouverts n'est pas spécialement un ouvert comme le montre l'exemple suivant :
\[
  \mO_i=]1,2+\frac{ 1 }{ i }[.
\]
Tous les ensembles $\mO_i$ contiennent le point $2$ qui est donc dans l'intersection. Mais quel que soit le $\epsilon>0$ que l'on choisisse, le point $2+\epsilon$ n'est pas dans $\mO_{(1/\epsilon)+1}$. Donc aucun point au-delà de $2$ n'est dans l'intersection, ce qui prouve que $2$ ne possède pas de voisinages contenus dans $\bigcap_{i=1}^{\infty}\mO_i$.
\end{remark}

\begin{proposition}     \label{PROPooANIOooIJHelX}
Quels que soient les ensembles $A$ et $B$ dans $\eR$, nous avons
\[
  \sup(A\cap B)\leq\sup A\leq\sup(A\cup B).
\]
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Intervalles et connexité}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons déterminer tous les sous-ensembles connexes\footnote{Définition~\ref{DefIRKNooJJlmiD}.} de $\eR$. Pour cela nous relisons d'abord la notion d'intervalle donnée en~\ref{DefEYAooMYYTz} ainsi que la proposition \ref{PROPooHPMWooQJXCAS} qui liste tous les intervalles de \( \eR\). La partie \( I\subset \eR\) est un intervalle si pour tout \( a,b\in I\), tout nombre entre \( a\) et \( b\) est également dans \( I\). Cette définition englobe tous les exemples connus d'intervalles ouverts, fermés avec ou sans infini : $[a,b]$, $[a,b[$, $]-\infty,a]$, \ldots L'ensemble \( \eR\) lui-même est un intervalle.

Si \( I\) est un intervalle, les nombres \( \inf(I)\) et \( \sup(I)\)\footnote{Qui existent par la proposition~\ref{DefSupeA}, quitte à poser \( \pm\infty\) comme infimum et supremum lorsque \( I\) n'est pas borné.} sont les \defe{extrémités}{extrémité!d'un intervalle} de \( I\).

\begin{definition}      \label{DefLISOooDHLQrl}
	Étant donnés deux points $a$ et $b$ dans $\eR^p$ on appelle \defe{segment}{segment!dans $\eR^p$} d'extrémités $a$ et $b$, et on note $[a,b]$, l'image de $[0,1]$ par l'application $s: [0,1]\to \eR^p$, $s(t)= (1-t)a+tb$.  On pose $]a,b[=s\left(]0,1[\right)$, et  $]a,b]=s\left(]0,1]\right)$.
\end{definition}
Il faut observer que le segment $[a,b]$ est une courbe orientée : certes en tant que ensembles, $[a,b]=[b,a]$, mais si nous regardons la fonction de $t$ correspondante à $[b,a]$, nous voyons qu'elle va dans le sens inverse de celle qui correspond à $[a,b]$. Nous approfondirons ces questions lorsque nous parlerons d'arcs paramétrés autour de la section~\ref{SecArcGeometrique}.

Le segment $[b,a]$ est l'image de l'application $r\colon [0,1]\to \eR^p$ donnée par $r(t)=(1-t)b+ta$.

\begin{proposition} \label{PropInterssiConn}
    Une partie de $\eR$ est connexe si et seulement si c'est un intervalle.
\end{proposition}
\index{connexité!et intervalles}

\begin{proof}
    La preuve est en deux parties. D'abord nous démontrons que si un sous-ensemble de $\eR$ est connexe, alors c'est un intervalle; et ensuite nous démontrons que tout intervalle est connexe.

    Afin de prouver qu'un ensemble connexe est toujours un intervalle, nous allons prouver que si un ensemble n'est pas un intervalle, alors il n'est pas connexe. Prenons $A$, une partie de $\eR$ qui n'est pas un intervalle. Il existe donc $a$, $b\in A$ et un $x_0$ entre $a$ et $b$ qui n'est pas dans $A$. Comme le but est de prouver que $A$ n'est pas connexe, il faut couper $A$ en deux ouverts disjoints. L'élément $x_0$ qui n'est pas dans $A$ est le bon candidat pour effectuer cette coupure. Prenons $M$, un majorant de $A$ et $m$, un minorant de $A$, et définissons
    \begin{align*}
        \mO_1&=]m,x_0[\\
        \mO_2&=]x_0,M[.
    \end{align*}
    Si $A$ n'a pas de minorant, nous remplaçons la définition de $\mO_1$ par $]-\infty,x_0[$, et si $A$ n'a pas de majorant, nous remplaçons la définition de $\mO_2$ par $]x_0,\infty[$. Dans tous les cas, ce sont deux ensembles ouverts dont l'union recouvre tout $A$. En effet, $\mO_1\cup \mO_2$ contient tous les nombres entre un minorant de $A$ et un majorant sauf $x_0$, mais on sait que $x_0$ n'est pas dans $A$. Cela prouve que $A$ n'est pas connexe.

    Jusqu'à présent nous avons prouvé que si un ensemble n'est pas un intervalle, alors il ne peut pas être connexe. Pour remettre les choses à l'endroit, prenons un ensemble connexe, et demandons-nous s'il peut être autre chose qu'un intervalle ? La réponse est \emph{non} parce que s'il était autre chose, il ne serait pas connexe.

    Prouvons à présent que tout intervalle est connexe. Pour cela, nous refaisons le coup de \href{http://fr.wikipedia.org/wiki/Contraposée}{la contraposée}. Nous allons donc prendre une partie $A$ de $\eR$, supposer qu'elle n'est pas connexe et puis prouver qu'elle n'est alors pas un intervalle. Nous avons deux ouverts disjoints $\mO_1$ et $\mO_2$ tels que $A\subset \mO_1\cup \mO_2$. Notons \( A_1 = A \cap \mO_1 \) et  \( A_2 = A \cap \mO_2 \); et prenons $a\in A_1$ et $b\in A_2$. Pour fixer les idées, on suppose que $a<b$. Maintenant, le jeu est de montrer qu'il existe une point $x_0$ entre $a$ et $b$ qui ne soit pas dans $A$ (cela montrerait que $A$ n'est pas un intervalle). Nous allons prouver que c'est le cas du point
    \[
      x_0=\sup\{ x\in\mO_1\tq x<b \}.
    \]
    Étant donné que l'ensemble $\mA=\{ x\in\mO_1\tq x<b \}$ est ouvert\footnote{C'est l'intersection entre l'ouvert $\mO_1$ et l'ouvert $\{x\tq x<b \}$.}, le point $x_0$ n'est pas dans l'ensemble par le lemme~\ref{LemSupOuvPas}. Nous avons donc
    \begin{itemize}
        \item soit $x_0$ n'est pas dans $\mO_1$,
        \item soit $x_0\leq b$,
        \item soit les deux en même temps.
    \end{itemize}
    Nous allons montrer qu'un tel $x_0$ ne peut pas être dans $A$. D'abord, remarquons que $\sup\mA\leq\sup\mO$ parce que $\mA$ est une intersection de $\mO$ avec quelque chose. Ensuite, il n'est pas possible que $x_0$ soit dans $\mO_2$ parce que tout élément de $\mO_2$ possède un voisinage contenu dans $\mO_2$. Un point de $\mO_2$ est donc toujours strictement plus grand que le supremum de $\mO_1$.

    Maintenant, remarque que si $x_0\leq b$, alors $x_0=b$, sinon $b$ serait un majorant de $\mA$ plus petit que $x_0$, ce qui n'est pas possible vu que $x_0$ est le supremum de $\mA$ et donc le plus petit majorant. Oui mais si $x_0=b$, c'est que $x_0\in\mO_2$, ce qu'on vient de montrer être impossible. Nous voilà déjà débarrassé des deuxièmes et troisièmes possibilités.

    Si la première possibilité est vraie, alors $x_0$ n'est pas dans $A$ parce qu'on a aussi prouvé que $x_0\notin\mO_2$. Or n'être ni dans $\mO_1$ ni dans $\mO_2$ implique de ne pas être dans $A$. Ce point $x_0=\sup\mA$ est donc hors de $A$.

    Oui, mais comme $a\in\mA$, on a obligatoirement que $x_0\geq a$. Mais par construction, on a aussi que $x_0\leq b$ (ici, l'inégalité est même stricte, mais ce n'est pas important). Donc
    \[
      a\leq x_0\leq b
    \]
    avec $a$, $b\in A$, et $x_0\notin A$. Cela finit de prouver que $A$ n'est pas un intervalle.
\end{proof}

\begin{theorem}[Théorème des bornes atteintes]\label{ThoMKKooAbHaro}
    Une fonction à valeurs réelles continue sur un compact est bornée et atteint ses bornes.

	C'est-à-dire qu'il existe $x_0\in K$ tel que $f(x_0)=\inf\{ f(x)\tq x\in K \}$ ainsi que $x_1$ tel que $f(x_1)=\sup\{ f(x)\tq x\in K \}$.
\end{theorem}
\index{compact!et fonction continue}

\begin{proof}
    Soient un espace topologique compact \( K\) et une fonction continue \( f\colon K\to \eR\). Alors le théorème~\ref{ThoImCompCotComp} indique que \( f(K)\) est compact. Par conséquent \( f(K)\) est un fermé borné de \( \eR\) par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}. Vu que \( f(K)\) est borné, la fonction \( f\) est bornée.

    De plus \( f(K)\) étant fermé, son infimum est un minimum et son supremum est un maximum : il existe \( x\in K\) tel que \( f(x)=\sup f(K)\) et il existe \( y\in K\) tel que \( f(y)=\inf f(K)\).
\end{proof}

Le théorème suivant est essentiellement inutile pour les raisons suivantes :
\begin{itemize}
    \item 
        Il est un cas particulier du théorème~\ref{ThoBWFTXAZNH} qui donne pour tout espace métrique, l'équivalence entre la compacité et la compacité séquentielle.
    \item
        Il est un cas particulier du théorème \ref{THOooRDYOooJHLfGq} qui le donne pour tous les espaces compacts.
    \item
        Il utilise le cas particulier de \( \eR\), qui n'est pas démontré directement dans le Frido.
\end{itemize}
Bref, nous ne le laissons que pour le lecteur qui n'aurait pas en tête d'autres définitions de «compact» à part «fermé borné».

% Pour les raisons invoquées, il ne fait pas faire de références vers ce théorème. Le label ici ne sert qu'à le mettre dans l'index thématique.
\begin{theorem}[Théorème de Bolzano-Weierstrass]		\label{ThoBolzanoWeierstrassRn}
	Toute suite contenue dans un compact de \( \eR^m\) admet une sous-suite convergente.
\end{theorem}

\begin{proof}
    Nous rappelons qu'une partie compacte de \( \eR^n\) est fermée et bornée par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}.

    Soit $(x_n)$ une suite contenue dans une partie bornée de $\eR^m$. Considérons $(a_n)$, la suite réelle des premières composantes des éléments de $(x_n)$ : pour chaque $n\in\eN$, le nombre $a_n$ est la première composante de $x_n$. Étant donné que la suite $(x_n)$ est bornée, il existe un $M$ tel que $\| x_n \|<M$. La croissance de la fonction racine carrée donne
	\begin{equation}
        | a_n |\leq\| x_n \|\leq M.
	\end{equation}
    La suite $(a_n)$ est donc une suite réelle bornée et donc contient une sous-suite convergente par le théorème correspondant dans \( \eR\) :  \ref{ThoBWFTXAZNH}. Soit $a_{I_1}$ une sous-suite convergente de $(a_n)$. Nous considérons maintenant $x_{I_1}$, c'est-à-dire la suite de départ dont on a enlevé tous les éléments qu'il faut pour qu'elle converge en ce qui concerne la première composante.

	Si nous considérons la suite $b_{I_1}$ des \emph{secondes} composantes de $x_{I_1}$, nous en extrayons, de la même façon que précédemment, une sous-suite convergente, c'est-à-dire que nous avons un $I_2\subset I_1$ tel que $b_{I_2}$ est convergent. Notons que $a_{I_2}$ est une sous-suite de la (sous) suite convergente $x_{I_1}$, et donc $a_{I_2}$ est encore convergente.

	En continuant ainsi, nous construisons une sous-sous-sous-suite $x_{I_3}$ telle que la suite des \emph{troisièmes} composantes est convergente. Lorsque nous avons effectué cette procédure $m$ fois, la suite $x_{I_m}$ est une suite dont toutes les composantes convergent, et donc est une suite convergente par la proposition~\ref{PropCvRpComposante}.

	Le tableau suivant donne un petit schéma de la façon dont nous procédons. Les $\bullet$ sont les éléments de la suite que nous gardons, et les $\times$ sont ceux que nous «jetons».
	\begin{equation}
		\begin{array}{lccccccccccc}
			x_{\eN}	&	\bullet&\bullet&\bullet&\bullet&\bullet&\bullet&\bullet&\bullet&\bullet&\bullet&\ldots\\
			x_{I_1}	&	\times&\bullet&\bullet&\times&\bullet&\times&\times&\bullet&\bullet&\bullet&\ldots\\
			x_{I_2}	&	\times&\bullet&\times&\times&\bullet&\times&\times&\bullet&\bullet&\times&\ldots\\
			\vdots\\
			x_{I_m}	&	\times&\times&\times&\times&\bullet&\times&\times&\times&\bullet&\times&\ldots
		\end{array}
	\end{equation}
	La première ligne, $x_{\eN}$, est la suite de départ.
\end{proof}

\begin{corollary}   \label{CorFHbMqGGyi}
    Si un suite est croissante et bornée alors elle est convergente.
\end{corollary}

\begin{proof}
    Nous nommons \( (x_n)\) la suite et nous prenons un majorant \( M\). Toute la suite est alors contenue dans le compact \( \mathopen[ x_0 , M \mathclose]\), ce qui donne une sous-suite \( (x_{\alpha(n)})\) convergente par le théorème de Bolzano-Weierstrass~\ref{THOooRDYOooJHLfGq}. Si \( \ell\) est la limite de cette sous-suite alors nous avons \( \ell\geq x_n\) pour tout \( n\).

    Pour tout \( \epsilon>0\) il existe \( K\) tel que si \( n>K\) alors \( | \ell-x_{\alpha(n)} |<\epsilon\). Vu que \( \ell\) majore la suite nous avons même
    \begin{equation}
        x_{\alpha(n)}+\epsilon>\ell.
    \end{equation}
    Vu que la suite est croissante pour tout \( m>\alpha(K)\) nous avons \( x_m+\epsilon>l\), ce qui signifie \( | x_m-\ell |<\epsilon\).
\end{proof}
Nous aurons une version pour les fonctions croissantes et bornées en la proposition~\ref{PropMTmBYeU}.

La proposition suivante dit que la notion d'ensemble non dénombrable ne prend pas réellement de force entre \( \eR\) et \( \eR^n\) : il n'y a pas moyen de caser \( \eR\) dans \( \eR^n\) de façon à ce qu'il y tienne à son aise.

\begin{proposition}
    Une partie non dénombrable de \( \eR^n\) possède un point d'accumulation\footnote{Définition \ref{DEFooGHUUooZKTJRi}.}.
\end{proposition}

\begin{proof}
    Soit une partie \( A\subset \eR^n\) sans point d'accumulation. Nous allons prouver que \( A\) est dénombrable.

    Soient les compacts \( K_n=\overline{ B(0,n) }\). La partie \( A\cap K_n\) est finie; sinon elle aurait une partie en bijection avec \( \eN\) (proposition~\ref{PROPooUIPAooCUEFme}) et donc une suite. Or une suite dans un compact possède un point d'accumulation par le théorème~\ref{THOooRDYOooJHLfGq}.

    Donc tous les \( A\cap K_n\) sont finis. Vu que \( A=\bigcup_nA\cap K_n\), l'ensemble \( A\) est une réunion dénombrable d'ensembles finis. Il est donc dénombrable.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Recouvrement d'un compact par des intervalles ouverts}
%---------------------------------------------------------------------------------------------------------------------------

Soit un ensemble \( E\) et un ensemble \( \mA\) de parties de \( E\). Soit \( A\in \mA\). Nous aimerions savoir quelles sont les éléments de \( \mA\) qui sont atteignables en partant de \( A\) et en ne «sautant» que d'intersection en intersection.

Nous notons \( \mA=\{ B_i \}_{i\in I}\) où \( I\) est un ensemble d'indices (un ensemble quelconque).
\begin{subequations}
    \begin{align}
        s_1(A)&=\{  i\in I\tq B_i\cap A\neq \emptyset   \}\\
        \sigma_1(A)&=\bigcup_{B\in s_1(A)}B.
    \end{align}
\end{subequations}
Et ensuite :
\begin{subequations}
    \begin{align}
        s_{k+1}(A)&=\{ i\in I\tq B_i\cap \sigma_k(A)\neq \emptyset \}\\
        \sigma_{k+1}(A)&=\bigcup_{B\in s_{k+1}(A)}B
    \end{align}
\end{subequations}

\begin{lemma}
    Soient un intervalle \( A\) de \( \eR\) et \( \mA=\{ I_i \}_{i=1,\ldots, N}\) un recouvrement de \( A\) par des intervalles ouverts. Si \( I_1\cap A\neq \emptyset\) alors
    \begin{enumerate}
        \item
            \( \sigma_{N}=\sigma_{N+1}\)
        \item
            \( A\subset \sigma_N(I_1)\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Si \( \sigma_{k+1}=\sigma_k\), alors tous les \( \sigma_{k+l}\) sont identiques. De plus si \( \sigma_{k+1}\neq \sigma_k\), alors \( \sigma_{k+1}\) contient au moins un élément de plus que \( \sigma_k\). Donc \( \Card(\sigma_k)\geq k\) et en particulier \( N\leq \Card(\sigma_N)\leq N\). Cela prouve le premier point.

    L'ensemble \( \sigma_N(I_1)\) est une union d'ouverts et est donc un ouvert. Quitte à renuméroter nous écrivons
    \begin{equation}
        \sigma_N(I_1)=I_1\cup \ldots \cup I_n.
    \end{equation}
    L'ensemble 
    \begin{equation}
        \tau=\bigcup_{k=n+1}^NI_k
    \end{equation}
    est ouvert et est disjoint de \( \sigma_N(I_1)\) parce que si \( I_l\) ($I_l\geq n+1$) intersectait \( \sigma_N(I_1)\), nous aurions \( l\in s_{N+1}\) ou encore \( I_l\subset \sigma_{N+1}\setminus\sigma_N\).

    Donc \( \tau\) et \( \sigma_N\) sont deux ouverts disjoints qui recouvrent \( A\). Vu que \( A\) est un intervalle, il est connexe\footnote{Définition \ref{DefIRKNooJJlmiD} et proposition \ref{PropInterssiConn}.}. Donc soit \( A\subset \tau\) soit \( A\subset \sigma_N\). Comme \( I_1\cap A\neq \emptyset\) nous sommes dans le cas \( A\subset \sigma_N\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Connexité par arcs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
  Le sous-ensemble $A \subset \eR^n$ est \defe{connexe par arcs}{connexe!par arc} si pour tout $x, y \in A$, il existe un chemin\footnote{Attention : ici quand on dit \emph{chemin}, on demande que l'application soit continue. Dans de nombreux cours de géométrie différentielle, on demande $ C^{\infty}$. Il faut s'adapter au contexte.} contenu dans $A$ les reliant, c'est-à-dire une application continue
  \begin{equation*}
    \gamma : [0,1] \to \eR^n \tq \gamma(0) = x~\text{et}~\gamma(1) = y
  \end{equation*}
  avec $\gamma(t) \in A$ pour tout $t\in [0,1]$.
\end{definition}

\begin{normaltext}      \label{NORMooQXKVooXOmMlX}
    La connexité d'un ensemble n'implique pas sa connexité par arc. Il suffit pour cela de prendre un ensemble constitué de deux connexes reliés par un chemin de longueur infinie (le graphe d'une fonction de type \( \sin(1/x)\) par exemple).
\end{normaltext}

%TODOooMKKNooEyQMvU : placer ici l'exemple de ce graphe à qui on ajoute le segment vertical de (0,0) à (0,1).

%TODOooAMMAooCrYxvK : mettre dans l'ordre dans ceci :
\input{212_topologie}

\begin{example}
    Nous étudions l'exemple suivant :
    \begin{equation}
        A_1 = \{ (x, y ) \in \eR^2 \; | \; 2y^2+4y+2<x\leq \sqrt{4- y^2},\, y\in [-1.5, 0.5[ \}.
    \end{equation}

 On commence par tracer la  parabole  $x=2y^2+4y+2$, la circonférence $x^2+y^2=4$ et les droites $y=-1.5$ et $y=1/2$. On voit tout de suite que l'aire délimitée par les quatre courbes est donnée par l'union de deux parties. Dans la première $\sqrt{4- y^2}\leq x\leq 2y^2+4y+2$, $y\in [0,0.5]$ et dans l'autre $2y^2+4y+2\leq x\leq \sqrt{4- y^2}$, $y\in [-1.5, 0]$. L'ensemble $A_1$ est contenu dans la deuxième, \ref{LabelFigLAfWmaN}. L'intérieur de $A_1$ est donné par $\Int(A_1) = \{ (x, y ) \in \eR^2 \; | \; 2y^2+4y+2<x< \sqrt{4- y^2},\, y\in ]-1.5, 0[ \}$, et sa frontière est l'union de 3 morceaux de courbe $\ell_1$, $\ell_2$, $\ell_3$:
      \begin{equation}
        \begin{aligned}
          &\ell_1=\{(x,y)\, |\, x=2y^2+4y+2,\, y\in [-1.5, 0] \}\\
          &\ell_2=\{(x,y)\, |\, x=\sqrt{4-y^2},\, y\in [-1.5, 0] \}\\
          &\ell_3=\{(x,y)\, |\, x\in [0.5, \sqrt{7/4}]\, y=-1.5 \}.
        \end{aligned}
      \end{equation}

%The result is on figure \ref{LabelFigLAfWmaN}. % From file LAfWmaN
\newcommand{\CaptionFigLAfWmaN}{}
\input{auto/pictures_tex/Fig_LAfWmaN.pstricks}
\end{example}

\begin{example}     \label{EXooNRMCooNNWODu}
    Nous étudions l'exemple
    \begin{equation}
 A_2 = \{ ( t , \sin(1/t) ) \in \eR^2 \; | \; t \in ]0, 1/2] \; \}
    \end{equation}
    de la figure \ref{LabelFigYWxOAkh}.

    L'ensemble $A_2$ est une courbe. Comme la fonction sinus est bornée $A_2$ est borné. L'intérieur de $A_2$ est vide, touts ses points sont de points de frontière. $A_2$ n'est pas fermé parce que il lui manque le morceau de droite verticale $I=\{0\}\times\{-1,1\}$, qui est partie de sa fermeture. En fait, lorsque $t$ tend vers $0$ la fonction sinus oscille de plus en plus vite et on peut trouver au moins un point du type $(t,\sin(t))$ dans tout voisinage d'un point en $I$. Cet exemple est très important parce qu'il nous aide à comprendre l'expression \emph{point d'accumulation}.\ref{LabelFigYWxOAkh}. % From file YWxOAkh
    \newcommand{\CaptionFigYWxOAkh}{La figure de l'exemple \ref{EXooNRMCooNNWODu}.}
\input{auto/pictures_tex/Fig_YWxOAkh.pstricks}
\end{example}

\begin{example}
    Nous étudions
    \begin{equation}
A_3 = \eN \times \eQ = \{ (x, y ) \in \eR^2 \; | \; x \in \eN , y \in \eQ \}.
    \end{equation}
    L'ensemble $A_3$ n'est pas ouvert, ni fermé, ni borné dans la topologie de $\eR^2$. En fait, comme on a vu dans les exercices du cours, $\eQ$ a intérieur vide et sa fermeture est $\eR$. L'ensemble $\eN$, par contre est fermé et non borné. On peut remarquer que tous les points de $\eN$ sont points isolés. La fermeture de $A_3$ est alors $\eN\times \eR$ et son intérieur est vide. On peut dessiner la fermeture de cet ensemble comme une famille de droites verticales $x=n$, pour tout $n$ dans $\eN$.
\end{example}


	En général, lorsqu'un ensemble est donné par des inégalités, prendre la fermeture consiste à transformer les inégalités strictes en inégalités non strictes; prendre l'intérieur consiste à rendre stricte toutes les inégalités; la frontière consiste \emph{en gros} à transformer toutes les inégalités en égalités (nous allons voir que pour la frontière, c'est un peu plus de travail). Comprenez bien que cela n'est vrai que «en général». Il faut toujours bien regarder sur chaque exemple s'il n'y a pas l'un ou l'autre point problématique.

	La proposition \ref{Propfaposfxposcont} sera une des clefs pour dire que si une inégalité stricte est satisfaite en un point, alors elle sera satisfaite en tout point dans un voisinage. Voir aussi l'exercice \ref{exoEspVectoNorme0008}.

\begin{example} \label{ItemExoEVN3i}
    Nous considérons l'ensemble.
    \begin{equation}
A_1 = \{ (x, y ) \in \eR^2 \; | \; x^2 - 5x + 6 < y \leq 2 \}.
    \end{equation}
			Si un point $(x,y)\in\eR^2$ est tel que $x^2-5x+6<y$, alors dans une boule centrée en $(x,y)$ (de rayon $r_1$), l'inégalité reste vraie (parce que la fonction $x^2-5x+6-y$ est une fonction continue). De la même manière, si nous avons $y<2$ en $(x,y)$, alors nous avons encore l'inégalité dans une boule de rayon $r_2$. En prenant $r=\min\{ r_1,r_2 \}$, les deux inégalités restent vraies dans la boule de rayon $r$.

			Donc les points $(x,y)$ tels que $x^2 - 5x + 6 < y < 2$ sont dans l'intérieur de $A_1$.
			
			Pour les mêmes raisons, autour d'un point $(x,y)$ tel que $x^2-5x+6>y$, nous pouvons trouver une boule dans laquelle l'inégalité reste stricte. Ces points ne sont donc pas dans l'adhérence de $A_1$. Un point qui vérifie $x^2-5x+6= y= 2$ est par contre dans l'adhérence parce que dans toute boule, on pourra trouver un $x$ tel que $x^2-5x+6<y$, et un $y$. L'adhérence est donc donnée par les inéquations
			\begin{equation}
				\bar A_1\equiv x^2-5x+6\leq y\leq 2.
			\end{equation}
			
			La frontière est donnée par les points de l'adhérence qui ne sont pas dans l'intérieur de $A_1$. Attention : {\bf ne pas dire} que la frontière est alors donnée simplement en remplaçant les inégalités par des égalités : $\partial A_1\equiv x^2-5x+6= y= 2$. Quel est cet ensemble ?

			Trouver la frontière demande un peu plus de travail. Le point marqué sur la figure \ref{LabelFigAdhIntFr} est sur la frontière parce que toute boule intersecte l'intérieur et l'extérieur. Cela est dû au fait que, sur ce point, nous ayons $x^2-5x+6=y$ en même temps que $y<2$. Donc si on prend une boule assez petite, on conserve $y<2$, mais on obtient des points tels que $x^2-5x+6<y$. 

			En voyant le dessin, la chose à faire pour écrire la frontière est de trouver les deux points d'intersection entre la parabole et la droite horizontale. Ces points sont les points $(x,y)$ qui satisfont au système
			\begin{subequations}
				\begin{numcases}{}
					x^2-5x+6=y\\
					y=2.
				\end{numcases}
			\end{subequations}
			En substituant la seconde équation dans la première, il vient $x^2-5x+6=2$, ce qui nous donne à résoudre le polynôme du second degré $x^2-5x+4=0$. Les solutions sont $x=1$ et $x=4$, et les deux points d'intersection sont les points $P=(1,2)$ et $Q=(4,2)$. Les points de la frontière de $A_1$ sont donc donnés par 
			\begin{equation}
				\begin{aligned}[]
					\partial A_1&=\{ (x,y)\in\eR^2\tqs x^2-5x+6=y\text{ et } 1\leq x\leq 4 \}\\
						&\quad\cup\{ (x,y)\in\eR^2\tqs y=2\text{ et } 1\leq x\leq 4 \}.
				\end{aligned}
			\end{equation}
			
			\newcommand{\CaptionFigAdhIntFr}{En hachuré : l'intérieur; en trait plein : la frontière. L'adhérence est l'union des deux. Exercice \ref{exoEspVectoNorme0003},\ref{ItemExoEVN3i}.}
			\input{auto/pictures_tex/Fig_AdhIntFr.pstricks}

			Notez que les points de la parabole qui sont sur la frontière ne font pas partie de l'ensemble $A_1$ lui-même, tandis que ceux de la frontière qui sont sur la droite horizontale en font partie sauf \( (4,2)\) et \( (1,2)\).
\newcommand{\CaptionFigAdhIntFrDeux}{Notez que le point d'angle fait partie de la frontière, mais pas de l'ensemble. Exercice \ref{exoEspVectoNorme0003},\ref{ItemExoEVN3ii}.}
\input{auto/pictures_tex/Fig_AdhIntFrDeux.pstricks}

			L'intérieur de $A_1$ n'étant pas égal à $A_1$, cet ensemble n'est pas ouvert; de la même manière, vu que $\bar A_1\neq A_1$, l'ensemble n'est pas fermé. L'ensemble $A_1$ est par contre borné parce qu'il est contenu par exemple dans la boule de centre $(0,0)$ et de rayon $5$. Les points d'accumulation de \( A_1\) sont les points de sa fermeture.
    
\end{example}

\begin{example}\label{ItemExoEVN3ii} 
    Nous étudions
    \begin{equation}
    A_2 = \{ (x, y ) \in \eR^2 \; | \; x+ 1 < y < 2x \}.
    \end{equation}

    Pour les mêmes raisons que dans l'exemple \ref{ItemExoEVN3i} l'intérieur est donné par
			\begin{equation}
				\Int(A_2)\equiv x+1<y<2x;
			\end{equation}
			L'adhérence est donnée par
			\begin{equation}
				\overline{ A_2 }\equiv x+1\leq y\leq 2x,
			\end{equation}
			Pour la frontière, les deux droites dont il est question dans la définition de $A_2$ (les droites $y=x+1$ et $y=2x$) se coupent en $x=1$ (refaire soi-même le dessin de la figure \ref{LabelFigAdhIntFrDeux}). Lorsque $x<1$, les conditions $x+1<y$ et $y<2x$ sont incompatibles : aucun point de $A_2$ n'est dans la partie $x<1$ du plan. Lorsque $x>1$, alors les points situés \emph{entre} les deux droites font partie de $A_2$. La frontière est donc donnée par ces deux droites pour $x\geq 1$. 

			Étant donné que $\Int(A_2)=A_2$, cet ensemble est ouvert (et donc pas fermé par la proposition \ref{PropTopologieAx}\ref{ItemPropTopologieAxiv}). Il n'est par contre pas borné parce qu'il contient des point $(x,y)$ avec des $x$ arbitrairement grands.

\end{example}


\begin{example}
    Nous étudions l'ensemble
    \begin{equation}
 A_3 = \{ ( t , 2t ) \in \eR^2 \; | \; t \in [0, 1] \; \}.
    \end{equation}

			L'ensemble $A_3$ est un petit segment de droite. Son intérieur est vide parce que toute boule centrée en un point de la droite intersecte l'extérieur de la droite. Son adhérence et sa frontière sont $A_3$ lui-même parce que nous considérons les valeurs de $t$ dans $\mathopen[ 0 , 1 \mathclose]$ qui est un intervalle fermé. Si l'intervalle avait été ouvert, l'adhérence et la frontière auraient été trouvés en fermant :
			\begin{equation}
                \overline{ \{ (t,2t)\tqs t\in\mathopen[ 0 , 1 [\, \}}=\{ (t,2t)\tqs t\in\mathopen[ 0 , 1 ] \}
			\end{equation}
			Étant donné que son adhérence est égal à lui-même, cet ensemble est fermé (et donc pas ouvert). Il est également borné parce qu'il est contenu dans une boule de rayon $3$.
\end{example}

\begin{example}
    Nous étudions l'ensemble
    \begin{equation}
        A_4 = \eQ \times \eQ = \{ (x, y ) \in \eR^2 \; | \; x \in \eQ , y \in \eQ \}.
    \end{equation}
			Dans $\eR$ nous savons que $\bar\eQ=\eR$, $\Int(\eQ)=\emptyset$ et $\partial\eQ=\eR$ parce que toute boule centrée en un rationnel contient un irrationnel, et inversement, toute boule centrée en un irrationnel contient un rationnel. Dans $\eR^2$ nous avons le même phénomène parce dans la boule $B\big( (p,q),r \big)$ avec $(p,q)\in\eQ\times\eQ$, se trouvent en particulier les points de la forme $(p,x)$ avec $x\in B(q,r)\subset\eR$. Évidement, certains de ces $x$ ne sont pas dans $\eQ$ et par conséquent, la boule $B\big( (p,q),r \big)$ contient les points $(p,x)\notin\eQ\times\eQ$.

			De la même manière, si $(x,y)$ est un point de $\eR^2$, dans toute boule centrée en $(x,y)$, il y aura un élément de $\eQ^2$.

			Par conséquent, $\Int(\eQ\times\eQ)=\emptyset$, $\overline{ \eQ\times\eQ }=\eR\times\eR$ et $\partial(\eQ\times\eQ)=\eR^2$.

			Il n'est ni ouvert ni fermé (parce qu'il n'est égal ni à son intérieur ni à sa fermeture). Il n'est pas borné non plus parce qu'il existe des nombres rationnels arbitrairement grands.
\end{example}



\begin{example}
    Nous étudions l'ensemble
    \begin{equation}
        A_5 = \{ (x, y ) \in \eR^2 \; | \; x \in ]0, 1[, \sin \frac 1x < y < 3 \}.
    \end{equation}

			La fonction $x\mapsto\sin(\frac{1}{ x })$ est une des fonctions dont le graphe doit être connu. La figure \ref{LabelFigAdhIntFrTrois} montre la situation. Comme d'habitude, il est fortement recommandé de refaire le dessin soi-même.
\newcommand{\CaptionFigAdhIntFrTrois}{Les points qui dont sur l'axe vertical entre $0$ et $3$ sont sur la frontière, mais pas dans l'ensemble $A_5$.}
\input{auto/pictures_tex/Fig_AdhIntFrTrois.pstricks}

			L'ensemble $A_5$ est ouvert parce que les conditions $x\in\mathopen] 0 , 1 \mathclose[$ et $\sin\frac{1}{ x }<y<3$ sont des conditions «ouvertes» au sens où si un point les vérifient, alors on peut trouver une boule dans lequel ces conditions restent vérifiées. Cela prouve que $\Int(A_5)=A_5$.

			La fermeture de $A_5$ contient en outre les points tels que $\sin\frac{1}{ x }=y$ entre $x=0$ et $x=1$ (les bornes étant incluses) ainsi que les points des trois segments de droites suivants:
			\begin{equation}
				\begin{aligned}[]
					\{ (0,y)\tqs y\in\mathopen[ -1 , 3 \mathclose] \}\\
					\{ (x,3)\tqs x\in\mathopen[ 0 , 1 \mathclose] \}\\
					\{ (1,y)\tqs y\in\mathopen[ \sin(1) , 3 \mathclose] \}.
				\end{aligned}
			\end{equation}

			La frontière est composée de ces trois segments et du graphe de la fonction $\sin\frac{1}{ x }$ entre $0$ et $1$.

			L'ensemble $A_5$ est borné parce qu'il est contenu par exemple dans la boule centrée en $(0,0)$ et de rayon $10$. Il est ouvert et donc pas fermé.
\end{example}


\begin{example}\label{ItemexoEspVectoNorme0003iv}
    Nous étudions l'ensemble
    \begin{equation}
        A_6 = \bigcup _{ n \in \eN_0} \{ ( \frac 1n, y ) \; | \; y \in [0,1] \; \}.
    \end{equation}

			L'ensemble $A_6$ est une union infinie de segments de droites verticaux, voir figure \ref{LabelFigAdhIntFrSix}
\newcommand{\CaptionFigAdhIntFrSix}{Le segment sur l'axe vertical entre $y=0$ et $y=1$ fait partie de l'adhérence et de la frontière, mais pas de l'ensemble $A_6$ lui-même.}
\input{auto/pictures_tex/Fig_AdhIntFrSix.pstricks}
				L'intérieur est vide parce qu'autour de tout réel de la forme $\frac{1}{ n }$, il y a un réel qui n'est pas de cette forme. En ce qui concerne la frontière et l'adhérence, il s'agit de l'union de tous ces segments plus le segment en $x=0$.

			En effet, la boule de rayon $r$ autour du point $(0,y)$ le point $(\frac{1}{ n },y)$ avec $n$ assez grand pour que $\frac{1}{ n }<r$.

\end{example}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Topologie de la droite réelle complétée}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooKRRUooSlZSmM}

Nous introduisons l'ensemble \( \bar\eR=\eR\cup\{ \pm\infty \}\). À présent les symboles \( +\infty\) et \( -\infty\) n'ont aucune signification particulière; il s'agit seulement de deux éléments que nous ajoutons à \( \eR\) pour former un ensemble que nous notons \( \bar \eR\).

Pas plus tard qu'immédiatement nous leur donnons une signification en définissant une topologie sur \( \bar\eR\). Les ouverts sur \( \bar \eR\) sont
\begin{enumerate}
    \item
        tous les ouverts de \( \eR\),
    \item
        les intervalles de la forme \( \mathopen] -\infty , a \mathclose[\) pour tous les \( a\in \eR\),
    \item
        les intervalles de la forme \( \mathopen] a , +\infty \mathclose[\) pour tous les \( a\in \eR\),
    \item
        la topologie engendrée par toutes ces parties de \( \bar \eR\).
\end{enumerate}

Par construction, les boules de \( \eR\) et les intervalles \( \mathopen] -\infty , a \mathclose[\) et \( \mathopen] a , +\infty \mathclose[\) forment une base de topologie pour \( \bar \eR\).

Si \( f\) est une fonction \( f\colon \eR\to \eR\), que signifie \( \lim_{x\to \infty} f(x)\) ? Il s'agit de considérer la fonction élargie
\begin{equation}
    \begin{aligned}
        \tilde f\colon \bar \eR&\to \bar\eR \\
        x&\mapsto \begin{cases}
            f(x)    &   \text{si } x\in \eR\\
            0    &    \text{si } x=\pm\infty.
        \end{cases}
    \end{aligned}
\end{equation}
Ensuite, c'est la définition topologie usuelle de la limite. Notons que les limites en \( a\) ne dépendent pas de la valeur effective de \( f\) en \( a\), donc le prolongement par \( 0\) est sans conséquences. Nous pouvions tout aussi bien prolonger par \( 4\).

Le même raisonnement tient pour donner un sens à \( \lim_{x\to a} f(x)=\pm \infty\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques mots à propos de la droite réelle complétée} 
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    La \defe{droite réelle complétée}{droite réelle complétée} est l'ensemble \( \eR\cup\{ \pm \infty \}\) où \( \pm\infty\) sont deux nouveaux éléments. Nous la notons \( \overline{ \eR }\) pour des raisons que nous verrons à peine plus bas.
\end{definition}

Cette définition ne servirait à rien si nous n'y mettions pas une topologie pour positionner les éléments \( \pm\infty\) par rapport à ceux qui existaient déjà dans \( \eR\).

\begin{definition}[Topologie sur \( \bar\eR\)]
La topologie sur \(\bar \eR\) est celle sur \( \eR\) à laquelle nous ajoutons les voisinages de \( \pm\infty\) de la façon suivante. Une partie \( V\) de \( \bar \eR\) est un voisinage de \( +\infty\) s'il existe \( m>0\) tel que \( \mathopen] m , +\infty \mathclose]\subset V\).
\end{definition}

Le lemme suivant justifie la notation \( \overline{ \eR }\) pour la droite réelle complétée\footnote{Mais ne justifie pas le qualificatif «complété» parce que l'espace métrique \( \eR\) était déjà complet.}.
\begin{lemma}       \label{LEMooPZXHooEEXsTC}
    L'adhérence de \( \eR\) dans \( \overline{ \eR }\) est \( \overline{ \eR }\).
\end{lemma}

Pour la suite nous utilisons la notation (pratique en probabilité)
\begin{equation}
    \{ f<a \}=\{ x\in S\tq f(x)<a \}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Limite pointée ou épointée ?}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooVHKCooYRFgrb}

Si vous êtes dans l'enseignement en France\footnote{En particulier si vous voulez passer l'agrégation.}, vous devriez lire ceci à propos de limite pointée. Dans tous les autres cas, la limite pointée est une notion qui ne vous intéresse à priori pas.

\begin{definition}[\cite{ooCNVFooHdbArS}]
    Soient $X$ et $Y$ deux espaces topologiques, $A$ une partie de $X$, $f$ une application de $A$ dans $Y$, $a$ un point de $X$ adhérent à $A$ et \(\ell \) un point de $Y$. On dit que \( \ell\) est une \defe{limite pointée}{limite pointée} de $f$ au point $a$ si pour tout voisinage $V$ de \( \ell\), il existe un voisinage $W$ de a tel que pour tout point $x$ de $W\cap A$, l'image $f(x)$ appartient à $V$.
\end{definition}

La notion de limite pointée ne diffère de la limite que du fait que pour calculer la limite pointée en \( a\), nous tenons compte des valeurs de \( f\) sur \emph{tout} le voisinage de \( a\), y compris le point \( a\) lui-même.

Le choix entre la limite pointée ou épointée a été discuté en de nombreuses occasions \cite{BIBooKNWHooBRoxme,BIBooNUKAooVMqppa,BIBooDILKooUcmUVD,BIBooJDPPooVONaQV}.

\begin{enumerate}
    \item       \label{ITEMooXSRLooMVwIHU}
        Dans la majorité des cas, la limite pointée donne le même résultat que la limite parce que, fondamentalement, si nous voulons calculer une limite de \( f\) au point \( a\), c'est que \( f\) n'est pas définie en \( a\). C'est en particulier toujours le cas pour les limites en l'infini ou les limites définissant les dérivées.
    \item
        La limite pointée est un peu plus simple au départ.
    \item
        La limite épointée est un peu plus riche. Par exemple si on dit « la limite de \( f\) en \( a\) existe » , ça donne une régularité pour \( f\) autour de \( 0\) que la limite pointée ne parvient pas à exprimer.
    \item
        La limite pointée n'est connue qu'en France.
\end{enumerate}

Le point \ref{ITEMooXSRLooMVwIHU} est le plus important parce qu'il explique pourquoi il y a moyen de finir l'agrégation, et même de faire de la recherche en ne tombant jamais sur un cas où la différence est importante.

\begin{example}
    La fonction \( f\) donnée par
    \begin{equation}
        f(x)=\begin{cases}
            0    &   \text{si } x\neq 0\\
            4    &    \text{si }x=0
        \end{cases}
    \end{equation}
    a une limite épointée pour \( x\to 0\) qui vaut \( 0\). Elle n'a par contre pas de limite pointée en \( 0\).

    Cette fonction est l'exemple-type de différence entre limite usuelle et limite pointée.
\end{example}

\begin{example}
    Si vous voulez un cas dans lequel la différence se voit de façon macroscopique, aller lire le lemme \ref{LEMooYLIHooFBQyzC}, sa démonstration et l'exemple \ref{EXooHSYNooBZhDbE}.
\end{example}

Dans le Frido, nous choisissons de prendre la limite épointée comme définition de limite. Nous donnons ici quelques raisons pour ce choix.

\begin{enumerate}
    \item
       C'est la définition unanimement acceptée dans la communauté mathématique hors France.
   \item
       La limite pointée ne donne à peu près rien de nouveau par rapport à la continuité.
       
       Ce que le concept de limite apporte est la possibilité d'étudier le comportement de \( f\) pour les points «proches» de \( a\), sans regarder la valeur en \( a\) lui-même. Si l'idée est de regarder le comportement «proche» de \( a\) y compris au point \( a\) lui-même, c'est la notion de continuité qui fait le travail.

       Donc les contextes dans lesquels le concept de limite est intéressant sont justement les contextes dans lesquels la fonction étudiée n'existe pas au point étudié. Dans ce cas, les limites pointées et épointées coïncident.
\end{enumerate}

Que devez-vous faire ?
\begin{description}
    \item[Enseignement en France] La notion de limite pointée est celle nommée «limite» dans les programmes, et ce que nous nommons ici «limite» est nommé «limite épointée». Peut-être pour induire en erreur tout le reste de la planète ?
    \item[Recherche] Si vous faites de la recherche où que ce soit y compris en France, la seule définition de limite est la limite dite «épointée», celle qui sera toujours utilisée dans le Frido.
    \item[Doctorat] Vous commencez un doctorat en math, et vous avez vu la limite pointée comme seule définition de limite durant vos études ? Oubliez-la. Ou alors attendez-vous à vous à de sérieux quiproquos lorsque vous discuterez de mathématique avec des étrangers. 

        Disons clairement que si vous utilisez la limite pointée devant des non Français, ils se diront juste que vous devriez relire vos cours de base. Et si vous leur expliquez, il y a de bonnes chance qu'ils ne vous croient pas.
\end{description}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le théorème de composition}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Une des raisons fréquentes pour utiliser la limite pointée est que le théorème de composition est plus simple\cite{BIBooDILKooUcmUVD}.

Le voici avec des limites pointées. Pour faire la différence, j'adopte la notation \( {LP}\) pour la limite pointée et \( {LE}\) pour la limite épointée.
\begin{proposition}     \label{PROPooCQZZooMiZfQE}
    Soient \( f\) et \( g\) des fonctions \( \eR\to \eR\). Si \( {LP}_{x\to a}g(x)=\ell\) et si \( {LP}_{y\to \ell}f(y)=b\), alors
    \begin{equation}
        {LP}_{x\to a} (f\circ g)(x)={LP}_{y\to \ell} f(y)=b.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). L'hypothèse de limite pour \( f\) donne \( \eta>0\) tel que 
    \begin{equation}        \label{EQooLWGIooLqKThy}
        | y-\ell |<\eta \Rightarrow | f(y)-b |<\epsilon.
    \end{equation}

    Soit \( \delta>0\) tel que \( | x-a |<\delta\) implique \( | g(x)-\ell |<\eta\).

    Avec tout ça, si \( | x-a |<\delta\) nous avons \( | g(x)-\ell |<\eta\) et en appliquant l'implication \eqref{EQooLWGIooLqKThy} à \( y=g(x)\) nous trouvons \( | f\big( g(x) \big)-b |<\epsilon\).
\end{proof}

Avant d'énoncer et de démontrer le résultat correspondant pour les limites épointées, nous avons besoin d'un lemme, pour comprendre la différence d'hypothèse.

\begin{lemma}
    Si \( {LP}_{x\to a}f(x)=b\) alors il y a deux possibilités : 
    \begin{enumerate}
        \item
            Soit \( f\) est définie en \( a\) et alors \( f\) y est continue,
        \item 
            soit \( f\) n'est pas définie en \( a\) et alors poser \( f(a)=b\) donne un prolongement continu.
    \end{enumerate}
\end{lemma}

\begin{proposition}     \label{PROPooNWCMooCaDMex}
    Soient \( f\) et \( g\) des fonctions \( \eR\to \eR\). Nous supposons \( {LE}_{x\to a}g(x)=\ell\) et que \( f\) admette le prolongement continu \(b\) en \( \ell\)\footnote{Cette hypothèse est équivalente à dire que \( f\) a une limite pointée \( b\) en \( \ell\), c'est-à-dire la même hypothèse que dans la proposition \ref{PROPooNWCMooCaDMex}.} Alors
    \begin{equation}
        {LE}_{x\to a}(f\circ g)(x)=b.
    \end{equation}
\end{proposition}
Bon. Woaw. La différence est énorme.

\begin{proof}
    Soit \( \epsilon>0\). Par hypothèse de prolongement continu, il existe \( \eta>0\) tel que
    \begin{equation}
        | t-\ell |<\eta\Rightarrow | f(y)-b |<\epsilon.
    \end{equation}
    Soit \( \delta>0\) tel que
    \begin{equation}
        0<| x-a |<\delta\Rightarrow | g(x)-\ell |\leq \eta.
    \end{equation}
    Avec ce \( \delta\) nous avons que \( | 0<| x-a |<\delta |\) implique \( | f\big( g(x) \big)-b |<\epsilon\).
\end{proof}

Cela est surement une raison de présenter la limite pointée chez les petits. Mais ce n'est pas une raison pour les grands, que du contraire. L'énoncé de la proposition \ref{PROPooCQZZooMiZfQE} est à peine plus compliquée que celui de \ref{PROPooCQZZooMiZfQE}, mais elle dit un peu plus alors que la démonstration est la même.

Donc non, la proposition \ref{PROPooNWCMooCaDMex} n'ajoute pas d'hypothèse par rapport à \ref{PROPooCQZZooMiZfQE}. Au contraire, elle en enlève : la proposition \ref{PROPooNWCMooCaDMex} demande pour \( g\) une limite épointée au lieu d'une limite pointée. En ce qui concerne \( f\), les hypothèses sont les mêmes. La proposition \ref{PROPooNWCMooCaDMex} concerne une classe de fonctions un peu plus grande.

Au final, la proposition «épointée» \ref{PROPooNWCMooCaDMex} est un poil plus compliquée, mais elle a une hypothèse un peu plus faible et une conclusion un peu plus faible (existence d'une limite pointée). Bref, il s'agit d'un résultat différent. Mais comme maintenant nous sommes grands, nous sommes prêts à avoir des énoncés plus compliquée pour avoir des résultats plus complets.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Et les filtres ?}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si vous ne savez pas ce qu'est un filtre, vous pouvez sauter ces paragraphes. Sinon, vous pouvez vous dire que le débat «limite pointée» contre «limite épointée» n'a aucun sens parce que de toutes façons, la bonne façon de définir une limite passe par des filtres.

Alors le mieux est de se demander comment on construit, à partir de la notion de filtre, le nombre \( \lim_{x\to a} f(a)\) ?

Pas de bol, ça dépend du filtre choisi. Le premier filtre auquel on pense pour trouver une définition raisonnable de la limite de \( f(x)\) quand \( x\to a\) est le filtre des voisinages de \( a\). La notion de limite associée est la limite pointée. En ce sens la limite pointée est plus naturelle que la limite épointée. Cependant «naturel» signifie souvent «le premier qui nous tombe sous la main», ce qui ne signifie pas spécialement «le plus intéressant à utiliser».

La notion de limite épointée est la limite associée au filtre des voisinages épointés. Ce n'est, certes, pas le premier filtre qui nous tombe sous la main, mais il est, au moins dans le cadre de l'étude des fonctions sur \( \eR^n\), le plus efficace; celui qui donne le plus de nouvelles informations par rapport à la continuité.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

\begin{definition}[\cite{ooUQBZooCAKfrE}]      \label{DEFooEEQGooNiPjHz}
    Soient trois espaces vectoriels \( E,F\) et \( V\) sur le même corps commutatif \( \eK\). Une application \( b\colon E\times F\to V\) est \defe{bilinéaire}{application bilinéaire} si elle est séparément linéaire en ses deux variables, c'est-à-dire si
    \begin{enumerate}
        \item 
            \( b(u_1+u_2,v)=b(u_1,v)+b(u_2,v)\),
        \item
            \( b(u,v_1+v_2)=b(u,v_1)+b(u,v_2)\)
        \item
            \( b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,u_1,u_2\in E\), \( v,v_1,v_2\in F\) et pour tout \( \lambda\in \eK\).

    Dans le cas \( E=F\) et \( V=\eK\), nous parlons de \defe{forme bilinéaire}{forme!bilinéaire} sur \( E\).

    Nous parlons de forme bilinéaire \defe{symétrique}{forme bilinéaire symétrique} si de plus \( b(u,v)=b(v,u)\).
\end{definition}

\begin{normaltext}
    Une application bilinéaire \( E\times E\to \eK\) n'est pas une application linéaire; la distinction est importante. La linéarité est
    \begin{equation}
        b(\lambda u,\lambda v)= b\big( \lambda(u,v) \big)=\lambda b(u,v)
    \end{equation}
    et la bilinéarité est
    \begin{equation}
        b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v).
    \end{equation}
    En réalité la seule forme qui soit à la fois linéaire et bilinéaire est la forme identiquement nulle : la condition
    \begin{equation}
        b(\lambda u,\lambda v)=\lambda^2b(u,v)=\lambda b(u,v)
    \end{equation}
    pour tout \( \lambda\in \eK\) implique \( b(u,v)=0\).
\end{normaltext}

\begin{example}[\cite{BIBooJMSXooYUADgm}]
    L'application
    \begin{equation}
        \begin{aligned}
            b\colon \eM(n,\eK)\times \eM(n,\eK)&\to \eK \\
            (A,B)&\mapsto \trace(AB) 
        \end{aligned}
    \end{equation}
    est une forme bilinéaire symétrique.

    La vérification est un calcul :
    \begin{equation}
        \trace(BA)=\sum_{i}(BA)_{ii}=\sum_{ik}B_{ik}A_{ki}=\sum_{ik}A_{ki}A_{ik}=\sum_k(AB)_{kk}=\trace(AB).
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit scalaire, produit hermitien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Définie positive, thème~\ref{THEMEooYEVLooWotqMY}]      \label{DEFooJIAQooZkBtTy}
    Si $g$ est une application bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} sur un espace vectoriel \( E\) nous disons qu'elle est
    \begin{enumerate}
        \item
            \defe{définie positive}{application!définie positive} si $g(x,x)\geq 0$ pour tout $x\in E$ et $g(x,x)=0$ si et seulement si $x=0$.
        \item
            \defe{semi-définie positive}{application!semi-définie positive} si $g(x,x)\geq 0$ pour tout $x\in E$. Nous dirons aussi parfois qu'elle est simplement «positive».
        \end{enumerate}
\end{definition}
Cela est évidemment à lier à la définition~\ref{DefAWAooCMPuVM} et la proposition~\ref{PROPooUAAFooEGVDRC} : une application bilinéaires est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{definition}\label{DefVJIeTFj}
    Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel réel est une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition~\ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

La définition suivante est utile pour celles qui veulent faire de la relativité\footnote{Voir le théorème \ref{THOooYHDWooWxVovH} qui établit les transformations de Lorentz.}.
\begin{definition}      \label{DEFooLPBGooXLxubc}
    Un \defe{produit pseudo-scalaire}{produit pseudo-scalaire} sur un espace vectoriel réel est une forme bilinéaire et symétrique.
\end{definition}

Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj}
    Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel de dimension finie muni d'un produit scalaire (définition~\ref{DefVJIeTFj}).
\end{definition}
Avouez que c'est drôle qu'un espace vectoriel est euclidien lorsqu'il possède une \emph{multiplication} alors qu'un anneau est euclidien lorsqu'il possède une \emph{division} (voir la définition~\ref{DefAXitWRL}). C'est pas très profond, mais si ça peut vous servir de moyen mnémotechnique\ldots

\begin{definition}[\cite{ooJUXBooVrwvfP}]  \label{DefMZQxmQ}
    Soit \( E\) est un espace vectoriel sur \( \eC\). Une application \( \langle ., .\rangle \colon E\times E\to \eC\) est \defe{sesquilinéaire à droite}{sesquilinéaire} si pour tout \( x,y\in E\) et pour tout \( \lambda\in \eC\),
    \begin{enumerate}
        \item
            \( \langle \lambda x, y\rangle =\lambda\langle x,y, \rangle =\langle x, \bar\lambda y\rangle \),
        \item
            \( \langle x+y, z\rangle =\langle x, y\rangle+\langle y, z\rangle  \),
        \item
            \( \langle x, y+z\rangle =\langle x, y\rangle +\langle x, z\rangle \).
    \end{enumerate}
    Cette forme est \defe{hermitienne}{hermitienne} si de plus
    \begin{equation}
        \langle x, y\rangle =\overline{ \langle y, x\rangle  }.
    \end{equation}
    Un \defe{produit hermitien}{produit hermitien} est une forme hermitienne strictement définie positive, c'est-à-dire telle que \( \langle x, x\rangle \geq 0\) pour tout \( x\in E\) et \( \langle x, x\rangle =0\) si et seulement si \( x=0\).
\end{definition}

\begin{example}
    L'ensemble \( E=\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\)  est muni d'une forme sesquilinéaire
    \begin{equation}    \label{EqFormSesqQrjyPH}
        \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
    \end{equation}
    pour tout \( x,y\in\eC^n\). Cela est un espace vectoriel hermitien.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme, produit scalaire et Cauchy-Schwarz (cas réel)}
%---------------------------------------------------------------------------------------------------------------------------

Dans la suite, le produit scalaire de \( x\) et \( y\) pourra être noté indifféremment par \( x\cdot y\), \( \langle x, y\rangle \) ou \( b(x,y)\) lorsque une forme bilinéaire est donnée.

Nous rappelons au passage que les espaces vectoriels réels sont susceptibles de recevoir un produit scalaire, alors que les espaces vectoriels complexes sont susceptibles de recevoir un produit hermitien. Bien que de nombreux résultats soient identiques ou très similaires, ces deux notions sont à ne pas confondre.

Nous commençons par prouver qu'un produit scalaire étant donné, nous pouvons définir une norme par la formule \( \| x \|^2=\langle x, x\rangle \). Pour cela nous aurons besoin de l'inégalité de Cauchy-Schwarz.

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas réel]      \label{ThoAYfEHG}
    Soit un espace vectoriel muni d'un produit scalaire \( (x,y)\mapsto x\cdot y\). En posant\footnote{Attention à la notation : pour l'instant nous ne savons pas que c'est une norme. Ce sera justifié dans la proposition~\ref{PropEQRooQXazLz}.}
    \begin{equation}
        \| x \|=\sqrt{ x\cdot x },
    \end{equation}
    nous avons
    \begin{equation}        \label{EQooZDSHooWPcryG}
		| x\cdot y |\leq \| x \|\| y \|.
	\end{equation}
    Nous avons une égalité si et seulement si \( x\) et \( y\) sont multiples l'un de l'autre.
\end{theorem}
\index{Cauchy-Schwarz}
\index{inégalité!Cauchy-Schwarz}

\begin{proof}
	Étant donné que les deux membres de l'inéquation sont positifs, nous allons travailler en passant au carré afin d'éviter les racines carrés dans le second membre.

	Nous considérons le polynôme
	\begin{equation}
		P(t)=\| x+ty \|^2=(x+ty)\cdot(x+ty)=x\cdot x+x\cdot ty+ty\cdot x+t^2y\cdot y.
	\end{equation}
    En utilisant la bilinéarité (pour sortir les \( t\)) et la symétrique du produit scalaire, puis en ordonnant les termes selon les puissances de $t$,
	\begin{equation}
		P(t)=\| y \|^2t^2+2(x\cdot y)t+\| x \|^2.
	\end{equation}
    %TODOooRUEZooGCVyQZ : faire la résolution.
	Cela est un polynôme du second degré en $t$ dont le signe est toujours positif (ou nul). Par conséquent le discriminant\footnote{Le fameux $b^2-4ac$.} doit être négatif ou nul. Nous avons donc
	\begin{equation}
		\Delta=4(x\cdot y)^2-4\| x \|^2\| y \|^2\leq 0,
	\end{equation}
	ce qui donne immédiatement
	\begin{equation}
		(x\cdot y)^2\leq\| x \|^2\| y \|^2.
	\end{equation}

    En ce qui concerne le cas d'égalité, si nous avons \( x\cdot y=\| x \|\| y \|\), alors le discriminant \( \Delta\) ci-dessus est nul et le polynôme \( P\) admet une racine double \( t_0\). Pour cette valeur nous avons
    \begin{equation}
        P(t_0)=| x+t_0y |=0,
    \end{equation}
    ce qui implique \( x+t_0y=0\) et donc que \( x\) et \( y\) sont liés.
\end{proof}

La proposition suivante montre que toute norme dérivant d'un produit scalaire vérifie l'identité du parallélogramme. Ce résultat sert souvent à prouver que des normes ne dérivent pas d'un produit scalaire. C'est le cas de la norme \( N(x,y)=| x |+| y |\) du lemme \ref{LEMooRWJYooOIJkZc} ainsi que du théorème de Weinersmith \ref{THOooCCMBooGulxkQ}.
\begin{proposition}[Norme dérivant d'un produit scalaire] \label{PropEQRooQXazLz}
    Si \( x,y\mapsto x\cdot y\) est un produit scalaire sur un espace vectoriel réel \( E\). Nous posons \( \| x \|=\sqrt{x\cdot x}\). Alors
    \begin{enumerate}
        \item
            L'opération \( \| . \|\) est une norme\footnote{Définition \ref{DefNorme}.}.
        \item
            Cette norme vérifie l'identité du parallélogramme :
            \begin{equation}        \label{EqYCLtWfJ}
                \| x-y \|^2+\| x+y \|^2=2\| x \|^2+2\| y \|^2.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    En deux parties.
    \begin{subproof}
        \item[C'est une norme]
            Nous allons nous contenter de prouver l'inégalité triangulaire. Si \( x,y\in E\) nous avons
            \begin{equation}
                \| x+y \|=\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}.
            \end{equation}
            Par l'inégalité de Cauchy-Schwarz, théorème~\ref{ThoAYfEHG} nous avons aussi
            \begin{equation}
                2x\cdot y\leq 2\| x \|\| y \|.
            \end{equation}
            Nous pouvons donc majorer ce qui est dans la racine carrée :
            \begin{equation}
                \| x \|^2+\| y \|^2+2x\cdot y\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|=\big( \| x \|+\| y \| \big)^2.
            \end{equation}
            En remettant les bouts ensemble,
            \begin{equation}
                \| x+y \|  =\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}  \leq \sqrt{\big( \| x \|+\| y \| \big)^2}=\| x \|+\| y \|.
            \end{equation}

        \item[Inégalité du parallélogramme]
            Cette assertion est seulement un calcul :
            \begin{equation}
                \begin{aligned}[]
                    \| x-y \|^2+\| x+y \|^2&=(x-y)\cdot (x-y)+(x+y)\cdot(x+y)\\
                    &=x\cdot x-x\cdot y-y\cdot x+y\cdot y\\
                    &\quad +x\cdot x+x\cdot y+y\cdot x+y\cdot y\\
                    &=2x\cdot x+2y\cdot y\\
                    &=2\| x \|^2+2\| y \|^2.
                \end{aligned}
            \end{equation}
    \end{subproof}
\end{proof}

\begin{normaltext}
    Un produit scalaire fourni donc toujours une norme et donc une topologie. Il ne faudrait cependant pas croire que toute norme dérive d'un produit scalaire, même pas en dimension finie. Et ce, malgré l'équivalence de toutes les normes du théorème~\ref{ThoNormesEquiv} dont vous avez déjà peut-être entendu parler.
\end{normaltext}


L'intérêt du lemme suivant sera apparent en \ref{NORMooNKBCooKziIjx}.
\begin{lemma}   \label{LEMooRWJYooOIJkZc}
    Sur \( \eR^2\), l'application \( N(x,y)=| x |+| y |\) est une norme\footnote{Définition \ref{DefNorme}.} qui ne dérive pas d'un produit scalaire\footnote{La norme d'un produit scalaire est la proposition  \ref{PropEQRooQXazLz}.}.
\end{lemma}

\begin{proof}
    Nous commençons par montrer que \( N\) est une norme. Il faut vérifier les trois conditions de la définition \ref{DefNorme}.
    \begin{enumerate}
        \item
            Il faut utiliser le lemme \ref{LemooANTJooYxQZDw}\ref{ItemooNVDIooSuiSoB} dans les deux sens. Si \( (x,y)=(0,0)\), alors évidemment \( N(x,y)=0\). Dans l'autre sens, si \( N(x,y)=0\) nous avons
            \begin{equation}
                0=| x |+| y |\geq | x |.
            \end{equation}
            Donc \( | x |\leq 0\), mais comme \( | x |\geq 0\), nous avons \( | x |=0\) et donc \( x=0\). Le même raisonnement tient pour \( y\).
        \item
            En tenant compte du fait que \( | \lambda x |=| \lambda | |x |\), nous avons
            \begin{equation}
                N\big( \lambda(x,y) \big)=N(\lambda x,\lambda y)=| \lambda | |x |+| \lambda | |y |=| \lambda |(| x |+| y |)=| \lambda |N(x,y).
            \end{equation}
        \item
            Nous avons le calcul
            \begin{subequations}
                \begin{align}
                    N\big( (x,y)+(a,b) \big)&=N(x+a,y+b)\\
                    &=| x+a |+| y+b |\\
                    &\leq | x |+| a |+| y |+| b |       \label{SUBEQooIXKWooTNQFnu}\\
                    &=N(x,y)+N(a,b)
                \end{align}
            \end{subequations}
            Justification : pour \eqref{SUBEQooIXKWooTNQFnu} nous avons utilité \( | a+b |\leq | a |+| b |\), du lemme \ref{LemooANTJooYxQZDw}.
    \end{enumerate}
    Pour voir qu'elle ne dérive pas d'un produit scalaire, nous montrons qu'elle ne vérifie pas l'identité du parallélogramme de la proposition \ref{PropEQRooQXazLz}.

    Voici un petit bout de code qui nous permet de ne pas faire de recherches à la main :
    \lstinputlisting{tex/sage/sageSnip018.sage}

    Il est vite vu qu'avec \( v=(-1,1)\) et \( w=(1,1)\), l'identité du parallélogramme n'est pas vérifiée.
\end{proof}

\begin{lemma}[\cite{KXjFWKA}]   \label{LemLPOHUme}
    Soit \( V\) un espace vectoriel muni d'un produit scalaire et de la norme associée. Si \( x,y\in V\) satisfont à \( \| x+y \|=\| x \|+\| y \|\), alors il existe \( \lambda\geq 0\) tel que \( x=\lambda y\).
\end{lemma}

\begin{proof}
    Quitte à raisonner avec \( x/\| x \|\) et \( y/\| y \|\), nous supposons que \( \| x \|=\| y \|=1\). Dans ce cas l'hypothèse signifie que \( \| x+y \|^2=4\). D'autre part en écrivant la norme en termes de produit scalaire,
    \begin{equation}
        \| x+y \|^2=\| x \|^2+\| y \|^2+2\langle x, y\rangle ,
    \end{equation}
    ce qui nous mène à affirmer que \( \langle x, y\rangle =1=\| x \|\| y \|\). Nous sommes donc dans le cas d'égalité de l'inégalité de Cauchy-Schwarz\footnote{Théorème~\ref{ThoAYfEHG}.}, ce qui nous donne un \( \lambda\) tel que \( x=\lambda y\). Étant donné que \( \| x \|=\| y \|=1\) nous avons obligatoirement \( \lambda=\pm 1\), mais si \( \lambda=-1\) alors \( \langle x, y\rangle =-1\), ce qui est le contraire de ce qu'on a prétendu plus haut. Par souci de cohérence, nous allons donc croire que \( \lambda=1\).
\end{proof}

\begin{proposition}			\label{PropVectsOrthLibres}
	si $v_1,\cdots,v_k$ sont des vecteurs non nuls, orthogonaux deux à deux, alors ces vecteurs forment une famille libre.
\end{proposition}

\begin{lemma}       \label{LEMooYXJZooWKRFRu}
    Une isométrie d'un espace euclidien fixe l'origine.
\end{lemma}

\begin{proof}
    Soit une isométrie \( f\) d'un espace euclidien : \( f(x)\cdot f(y)=x\cdot y\) pour tout \( x,y\in E\). En particulier pour \( x=0\) nous avons
    \begin{equation}
        f(0)\cdot f(y)=0
    \end{equation}
    pour tout \( y\). Vu que \( f\) est une bijection, nous avons \( f(0)\cdot x=0\) pour tout \( x\). Comme le produit scalaire est non dégénéré cela implique que \( f(0)=0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Cauchy-Schwarz etc. cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas complexe\cite{HilbertLi}]      \label{THOooSUCBooFnpkaF}
     Soit un espace vectoriel complexe muni d'un produit hermitien \( \langle ., .\rangle \). Alors pour tout vecteurs \( x,y\) nous avons
     \begin{equation}
         | \langle x, y\rangle  |\leq \| x \|\| y \|
     \end{equation}
     où nous avons posé \( \| x \|=\sqrt{ \langle x, x\rangle  }\).
\end{theorem}

\begin{proof}
    Si \( \langle x, y\rangle =0\), le résultat est évident; nous supposons que non. Nous posons
    \begin{equation}
        \theta=\frac{ \langle x, y\rangle  }{ | \langle x, y\rangle  | }.
    \end{equation}
    C'est un élément de \( \eC\) de norme \( 1\). Nous avons
    \begin{equation}
        \langle \frac{1}{ \theta }x, y\rangle =\frac{ | \langle x, y\rangle  | }{ \langle x, y\rangle  }\langle x, y\rangle =| \langle x, y\rangle  |\geq 0
    \end{equation}
    où le symbole «\( \geq\)» signifie «est réel et positif». Nous posons \( x'=\frac{1}{ \theta }x\) et nous considérons \( t\in \eR\). Remarquons que \( \| x' \|^2=\| x \|^2\) :
    \begin{equation}
        \| x' \|^2=\langle x', x'\rangle =\frac{1}{ \theta\bar\theta }\langle x, x\rangle =\| x \|^2
    \end{equation}
    parce que \( | \theta |=1\).

    En utilisant le fait que \( \langle a, b\rangle +\langle b, a\rangle =\real(\langle a, b\rangle )\) nous avons :
    \begin{subequations}
        \begin{align}
            0\leq \| x'+ty \|^2&=\| x' \|^2+t\langle x', y\rangle +t\langle y, x'\rangle +t^2\| y \|^2\\
            &=\| y \|^2t^2+2\real(\langle x', y\rangle )t+\| x' \|^2.
        \end{align}
    \end{subequations}
    Cela est un polynôme de degré \( 2\) en \( t\) qui n'est jamais strictement négatif. Autrement dit, il a au maximum une seule racine, ce qui signifie que son discriminant est négatif ou nul :
    \begin{equation}
        \real(\langle x', y\rangle )^2-\| y \|^2\| x' \|^2\leq 0.
    \end{equation}
    Mais nous avons choisi \( x'\) de telle sorte que \( \langle x', y\rangle =| \langle x, y\rangle  |\in \eR\) et \( \| x' \|^2=\| x \|^2\); nous avons donc
    \begin{equation}
        | \langle x, y\rangle  |^2\leq \| x \|^2\| y \|^2,
    \end{equation}
    comme il se devait.
\end{proof}

\begin{proposition}[Identité du parallélogramme\cite{BIBooXLLGooAFwpyU}]       \label{PROPooSSYJooHAXAnC}
    Soit une espace vectoriel complexe \( E\) muni d'un produit hermitien \( \langle ., .\rangle \). Nous posons \( \| x \|=\sqrt{ \langle x, x\rangle  }\). Nous avons
    \begin{enumerate}
        \item
            \( \| . \|\) est une norme.
        \item
            Elle vérifie l'identité du parallélogramme :
            \begin{equation}
                \| x+y \|^2+\| x-y \|^2=2\| x \|^2+2\| b \|^2
            \end{equation}
            pour tout \( x,y\in E\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    En ce qui concerne le fait que \( \| . \|\) soit une norme, tout est essentiellement dans la définition \ref{DefMZQxmQ} d'un produit hermitien. Voyons tout de même l'inégalité triangulaire. Nous avons :
    \begin{subequations}
        \begin{align}
            \| x+y \|^2&=\langle x+y, x+y\rangle\\
            &=\| x \|^2+\| y \|^2+\langle x, y\rangle +\langle y, x\rangle\\
            &=\| x \|^2+\| y \|^2+2\Re\big( \langle x, y\rangle  \big)\\
            &\leq\| x \|^2+\| y \|^2+2|\Re\big( \langle x, y\rangle  \big)|\\
            &\leq\| x \|^2+\| y \|^2+2| \langle x, y\rangle  |\\
            &\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|\label{SUBEQooQGQBooMRJcUc}\\
            &=\big( \| x \|+\| y \| \big)^2.
        \end{align}
    \end{subequations}
    Pour \eqref{SUBEQooQGQBooMRJcUc} nous avons utilisé Cauchy-Schwarz \ref{THOooSUCBooFnpkaF}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Propriétés du produit scalaire]
	Si $X$ et $Y$ sont des vecteurs de $\eR^3$, alors
	\begin{description}
		\item[Symétrie] $X\cdot Y=Y\cdot X$;
		\item[Linéarité] $(\lambda X+\mu X')\cdot Y=\lambda(X\cdot Y)+\mu(X'\cdot Y)$ pour tout $\lambda$ et $\mu$ dans $\eR$;
		\item[Défini positif] $X\cdot X\geq 0$ et $X\cdot X=0$ si et seulement si $X=0$.
	\end{description}
\end{proposition}
Note : lorsque nous écrivons $X=0$, nous voulons voulons dire $X=\begin{pmatrix}
	0	\\
	0	\\
	0
\end{pmatrix}$.


\begin{definition}
	La \defe{norme}{norme!vecteur} du vecteur $X$, notée $\| X \|$, est définie par
	\begin{equation}
		\| X \|=\sqrt{X\cdot X}=\sqrt{x^2+y^2+z^2}
	\end{equation}
	si $X=(x,y,z)$. Cette norme sera parfois nommée «norme euclidienne».
\end{definition}
Cette définition est motivée par le théorème de Pythagore. Le nombre $X\cdot X$ est bien la longueur de la «flèche» $X$. Plus intrigante est la définition suivante :
\begin{definition}
	Deux vecteurs $X$ et $Y$ sont \defe{orthogonaux}{orthogonal!vecteur} si $X\cdot Y=0$.
\end{definition}
Cette définition de l'orthogonalité est motivée par la proposition suivante.

\begin{proposition}		\label{PropProjScal}
	Si nous écrivons $\pr_Y$  l'opération de projection sur la droite qui sous-tend $Y$, alors nous avons
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	Les vecteurs $X$ et $Y$ sont des flèches dans l'espace. Nous pouvons choisir un système d'axe orthogonal tel que les coordonnées de $X$ et $Y$ soient
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x	\\
				y	\\
				0
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				l	\\
				0	\\
				0
			\end{pmatrix}
		\end{aligned}
	\end{equation}
	où $l$ est la longueur du vecteur $Y$. Pour ce faire, il suffit de mettre le premier axe le long de $Y$, le second dans le plan qui contient $X$ et $Y$, et enfin le troisième axe dans le plan perpendiculaire aux deux premiers.

	Un simple calcul montre que $X\cdot Y=xl+y\cdot 0+0\cdot 0=xl$. Par ailleurs, nous avons $\| \pr_YX \|=x$. Par conséquent,
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ l }=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proof}

\begin{corollary}
	Si la norme de $Y$ est $1$, alors le nombre $X\cdot Y$ est la longueur de la projection de $X$ sur $Y$.
\end{corollary}

\begin{proof}
	Poser $\| Y \|=1$ dans la proposition~\ref{PropProjScal}.
\end{proof}

\begin{remark}
    Outre l'orthogonalité, le produit scalaire permet de savoir l'angle entre deux vecteurs à travers la définition~\ref{DEFooSVDZooPWHwFQ}. D'autres interprétations géométriques du déterminant sont listées dans le thème~\ref{THMooUXJMooOroxbI}.
\end{remark}

Nous sommes maintenant en mesure de déterminer, pour deux vecteurs quelconques $u$ et $v$, la projection orthogonale de $u$ sur $v$. Ce sera le vecteur $\bar u$ parallèle à $v$ tel que $u-\bar u$ est orthogonal à $v$. Nous avons donc
\begin{equation}
    \bar u=\lambda v
\end{equation}
et
\begin{equation}
    (u-\lambda v)\cdot v=0.
\end{equation}
La seconde équation donne $u\cdot v-\lambda v\cdot v=0$, ce qui fournit $\lambda$ en fonction de $u$ et $v$ :
\begin{equation}
    \lambda=\frac{ u\cdot v }{ \| v \|^2 }.
\end{equation}
Nous avons par conséquent
\begin{equation}
    \bar u=\frac{ u\cdot v }{ \| v \|^2 }v.
\end{equation}
Armés de cette interprétation graphique du produit scalaire, nous comprenons pourquoi nous disons que deux vecteurs sont orthogonaux lorsque leur produit scalaire est nul.

Nous pouvons maintenant savoir quel est le coefficient directeur d'une droite orthogonale à une droite donnée. En effet, supposons que la première droite soit parallèle au vecteur $X$ et la seconde au vecteur $Y$. Les droites seront perpendiculaires si $X\cdot Y=0$, c'est-à-dire si
\begin{equation}
	\begin{pmatrix}
		x_1	\\
		y_1
	\end{pmatrix}\cdot\begin{pmatrix}
		y_1	\\
		y_2
	\end{pmatrix}=0.
\end{equation}
Cette équation se développe en
\begin{equation}		\label{Eqxuyukljsca}
	x_1y_1=-x_2y_2.
\end{equation}
Le coefficient directeur de la première droite est $\frac{ x_2 }{ x_1 }$. Isolons cette quantité dans l'équation \eqref{Eqxuyukljsca} :
\begin{equation}
	\frac{ x_2 }{ x_1 }=-\frac{ y_1 }{ y_2 }.
\end{equation}
Donc le coefficient directeur de la première est l'inverse et l'opposé du coefficient directeur de la seconde.

\begin{example}
	Soit la droite $d\equiv y=2x+3$. Le coefficient directeur de cette droite est $2$. Donc le coefficient directeur d'une droite perpendiculaires doit être $-\frac{ 1 }{ 2 }$.
\end{example}

\begin{proof}[Preuve alternative]
	La preuve peut également être donnée en ne faisant pas référence au produit scalaire. Il suffit d'écrire toutes les quantités en termes des coordonnées de $X$ et $Y$. Si nous posons
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x_1	\\
				x_2	\\
				x_2
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				y_1	\\
				y_2	\\
				y_3
			\end{pmatrix},
		\end{aligned}
	\end{equation}
	l'inégalité à prouver devient
	\begin{equation}
		(x_1y_1+x_2y_2+x_3y_3)^2\leq (x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2).
	\end{equation}
	Nous considérons la fonction
	\begin{equation}
		\varphi(t)=(x_1+ty_1)^2+(x_2+ty_2)^2+(x_3+ty_3)^2
	\end{equation}
	En tant que norme, cette fonction est évidemment positive pour tout $t$. En regroupant les termes de chaque puissance de $t$, nous avons
	\begin{equation}
		\varphi(t)=(y_1^2+y_2^2+y_3^2)t^2+2(x_1y_1+x_2y_2+x_3y_3)t+(x_1^2+x_2^2+x_3^2).
	\end{equation}
	Cela est un polynôme du second degré en $t$. Par conséquent le discriminant doit être négatif. Nous avons donc
	\begin{equation}
		4(x_1y_1+x_2y_2+x_3y_3)^2-(x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2)\leq 0.
	\end{equation}
	La thèse en découle aussitôt.
\end{proof}

\begin{proposition}     \label{PROPooVSVMooZrqxdc}
	La norme euclidienne a les propriétés suivantes :
	\begin{enumerate}
		\item
			Pour tout vecteur $X$ et réel $\lambda$,  $\| \lambda X \|=| \lambda |\| X \|$. Attention à ne pas oublier la valeur absolue !
		\item
			Pour tout vecteurs $X$ et $Y$, $\| X+Y \|\leq \| X \|+\| Y \|$.
	\end{enumerate}
\end{proposition}

\begin{proof}
    Pour le second point, nous avons les inégalités suivantes :
	\begin{subequations}
		\begin{align}
			\| X+Y \|^2&=\| X \|^2+\| Y \|^2+2X\cdot Y\\
			&\leq\| X \|^2+\| Y \|^2+2|X\cdot Y|\\
			&\leq\| X \|^2+\| Y \|^2+2\| X \|\| Y \|\\
			&=\big( \| X \|+\| Y \| \big)^2
		\end{align}
	\end{subequations}
    Nous avons utilisé d'abord la majoration $| x |\geq x$ qui est évidente pour tout nombre $x$; et ensuite l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Pythagore}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons donner une preuve du théorème de Pythagore.

\begin{theorem}[Pythagone\cite{MonCerveau}]     \label{THOooHXHWooCpcDan}
    Soient \( A,B,S\in \eR^2\) un triangle rectangle en \( A\), c'est-à-dire tel que
    \begin{equation}        \label{EQooRAWAooBxlBcZ}
        (B-A)\cdot (A-S)=0.
    \end{equation}
    Alors
    \begin{equation}
        \| S-B \|^2=\| S-A \|^2+\| B-A \|^2.
    \end{equation}
\end{theorem}

\begin{proof}
    En développant l'hypothèse \eqref{EQooRAWAooBxlBcZ} nous avons :
    \begin{equation}    \label{EQooYTDGooXzYQwi}
        B\cdot A-B\cdot S-\| A \|^2+A\cdot S=0.
    \end{equation}
    Et de même,
    \begin{equation}
        \| S-B \|^2=(S-B)\cdot(S-B)=\| S \|^2-2B\cdot S+\| B \|^2.
    \end{equation}
    En substituant dans cette dernière \( B\cdot S\) par \( B\cdot S=B\cdot A -\| A \|^2+A\cdot S \) tirée de \eqref{EQooYTDGooXzYQwi}, nous trouvons
    \begin{equation}
        \| S-B \|^2=\| S \|^2-2B\cdot A+2\| A \|^2-2A\cdot S+\| B \|^2=\| S-A \|^2+\| B-A \|^2.
    \end{equation}
\end{proof}

Je profite de l'occasion pour montrer mon scepticisme quant aux preuves de Pythagore basées sur différents pliages et découpages des carrés construits sur les côtés du triangle. Pour autant que je le sache, la géométrie dans «le plan» (c'est-à-dire pas dans \( \eR^2\) muni de son produit scalaire) ne définit pas «longueur» et «aire». Donc bon \ldots Il y a peut-être moyen de s'en sortir, mais je ne le connais pas.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooTNTNooRjhuJZ}
	Soient $u$ et $v$, deux vecteurs de $\eR^3$. Le \defe{produit vectoriel}{produit!vectoriel} de $u$ et $v$ est le vecteur $u\times v$ défini par
    \begin{equation}        \label{EQooCUJRooFuFPaZ}
		u\times v=\det\begin{pmatrix}
			e_1	&	e_2	&	e_3	\\
			u_1	&	u_2	&	u_3	\\
			v_1	&	v_2	&	v_3
		\end{pmatrix}
    \end{equation}
	où les vecteurs $e_1$, $e_2$ et $e_3$ sont les vecteurs de la base canonique de $\eR^3$.
\end{definition}

\begin{lemma}
    Le produit vectoriel \( u\times v\) est également exprimé par
    \begin{subequations}        \label{EQSooOWGZooNYruoy}
        \begin{align}
            u\times v&=(u_2v_3-u_3v_2)e_1+(u_3v_1-u_1v_3)e_2+(u_1v_2-u_2v_1)e_3     \label{SEBEQooVROKooRpUOIr}\\
                &=\sum_{i,j,k}\epsilon_{ijk}v_iw_je_k
        \end{align}
    \end{subequations}
    où $\epsilon_{ijk}$ est défini par $\epsilon_{xyz}=1$ et ensuite $\epsilon_{ijk}$ est $1$ ou $-1$ suivant que la permutation des $x$, $y$ et $z$ est paire ou impaire. C'est-à-dire que \( \epsilon_{ijk}\) est la signature de la permutation qui amène \( (1,2,3)\) sur \( (i,j,k)\).
\end{lemma}

\begin{proof}
    Il s'agit seulement de développer explicitement le déterminant \eqref{EQooCUJRooFuFPaZ}.
\end{proof}

\begin{normaltext}
    Mettons que \( a\times b=v\). En calculant le même produit vectoriel dans la base \( f_i=-e_i\), les composantes de \( a\) et \( b\) changent de signe et la formule \eqref{EQSooOWGZooNYruoy} dit que le produit vectoriel ne change pas. On serait tenter d'écrire, dans la base \( \{ f_i \}\)
    \begin{equation}
        (-a)\times (-b)=v,
    \end{equation}
    tout en pleurant parce que dans la base des \( f_i\), le vecteur \( v\) devient \( -v\).

    Il a des personnes que cela tracasse tellement qu'on entend parler de «le produit vectoriel est une pseudo-vecteur sous \( \SO(2)\)».

    Il suffit d'être clair. Le produit vectoriel n'est défini que sur \( \eR^3\), et est définit par sa formule dans la base canonique, point barre. Si vous avez des vecteurs \( a\) et \( b\) dont vous connaissez les composantes dans une autre base, vous devez calculer les composantes dans la base canonique, utiliser la formule pour trouver les composantes de \( a\times b\) dans la base canonique. Ensuite, si ça vous chante, vous pouvez calculer à nouveau les composantes de \( a\times b\) dans une autre base.

    Tout cela pour dire que le produit vectoriel n'est pas une opération très généralisable. Il est possible, pour sembler plus intrinsèque, de tenter cette définition : le produit vectoriel \( a\times b\) est le vecteur perpendiculaire à \( a\) et \( b\), de longueur égale à l'aire du parallélogramme construit sur \( a\) et \( b\).

    Cette «définition» a plusieurs inconvénients.
    \begin{itemize}
        \item Elle demande quand même un produit scalaire et des aires; bref, elle demande une structure métrique,
        \item Elle ne donne pas le sens. En effet, dans \( \eR^3\), il y a deux vecteurs de longueur donnée perpendiculaires à \( a\) et \( b\). Il faut donc préciser le sens. Cela revient à donner une orientation et donc, fondamentalement, à choisir une base.
    \end{itemize}
    
    Bref, on retiendra que le produit vectoriel est une opération accrochée à \( \eR^3\) et a sa base canonique.
\end{normaltext}

Une des principales utilités du produit vectoriel est donnée dans la proposition suivante.
\begin{proposition}     \label{PROPooIQMTooFHNjfu}
    Si \( u\) et \( v\) sont des vecteurs de \( \eR^3\) alors le vecteur \( u\times v\) est perpendiculaire à \( u\) et à \( v\).
\end{proposition}
La chose importante à retenir est que le produit vectoriel permet de construire un vecteur simultanément perpendiculaire à deux vecteurs donnés. Le vecteur $u\times v$ est donc linéairement indépendant de $u$ et $v$. En pratique, si $u$ et $v$ sont déjà linéairement indépendants, alors le produit vectoriel permet de compléter une base de $\eR^3$.

\begin{lemmaDef}
    Nous avons l'égalité suivante pour tout \( u,v,w\in \eR^3\) :
    \begin{equation}        \label{EQooKJYUooSQgfXU}
        (u\times v)\cdot w=\det\begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix}.
    \end{equation}
    Le résultat est nommé le \defe{produit mixte}{produit!mixte} de trois vecteurs de \( \eR^3\).
\end{lemmaDef}

\begin{normaltext}
    Nous avons donné un nom à la combinaison \( (u\times v)\cdot w\). J'imagine que vous voyez pourquoi nous ne considérons pas la combinaison $(u\cdot v)\times w$.
\end{normaltext}

Le lemme suivant donne un moyen compliqué et peu pratique de calculer la valeur absolue du produit mixte. La formule \eqref{EQooWZUQooYydphW} ne sera utilisée que pour faire le lien entre un jacobien et un élément de volume en dimension trois lorsque nous verrons les intégrales sur des variétés. Voir l'équation \eqref{EQooYIJSooHtkXfu}. 

% TODO lier à la vraie définition de l'intégrale sur une carte quand elle sera faite.
% TODOooWZMDooEJhpNS
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooSMWNooCmEZeY}
    Le produit mixte peut également être exprimé par
    \begin{equation}        \label{EQooWZUQooYydphW}
           |(u\times v)\cdot w|^2=\det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
    Si nous notons 
    \begin{equation}
        a= \begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix},
    \end{equation}
    il faut simplement remarquer que
    \begin{equation}
           \begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=aa^t.
    \end{equation}
    Donc au niveau des déterminants, en utilisant les propositions \ref{PROPooHQNPooIfPEDH} et le lemme \ref{LEMooCEQYooYAbctZ} nous avons
    \begin{equation}
           \det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=\det(aa^t)=\det(a)\det(a^t)=\det(a)^2.
    \end{equation}
    Et maintenant, par définition, \( \det(a)=(u\times w)\cdot w\). Donc le résultat annoncé.
\end{proof}

\begin{proposition}		 \label{PropScalMixtLin}
	Les applications produit scalaire, vectoriel et mixte sont multilinéaires. Spécifiquement, nous avons les propriétés suivantes.
	\begin{enumerate}
		\item
			Les applications produit scalaire et vectoriel sont bilinéaires. C'est-à-dire que pour tout vecteurs $a$, $b$, $c$ et pour tout nombre $\alpha$ et $\beta$ nous avons
    \begin{equation}
        \begin{aligned}[]
            a\times (\alpha b +\beta c)&=\alpha(a\times b)+\beta(a\times c)\\
            (\alpha a+\beta b)\times c&=\alpha(a\times c)+\beta(b\times c).
        \end{aligned}
    \end{equation}

        \item
            Le produit mixte est trilinéaire.
		\item
			Le produit vectoriel est antisymétrique, c'est-à-dire $u\times v=-v\times u$.
		\item
			Nous avons $u\times v=0$ si et seulement si $u$ et $v$ sont colinéaires, c'est-à-dire si et seulement si l'équation $\alpha u+\beta v=0$ a une solution différente de la solution triviale $(\alpha,\beta)=(0,0)$.
		\end{enumerate}
\end{proposition}

\begin{proposition}[Identité de Lagrange\cite{ooHFUZooGakvHi}]     \label{PROPooMXAIooJureOD}
    Si \( x,y\in \eR^n\), alors
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
    \end{equation}
    Et si \( n=3\) alors
    \begin{equation}
        \| x\times y \|=\| y \|^2\| y \|^2-(x\cdot y)^2.
    \end{equation}
\end{proposition}

\begin{proof}
    C'est un calcul. D'abord nous avons
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_ix_i^2\sum_jy_j^2-\big( \sum_k x_ky_k  \big)^2=\sum_{ij}x_i^2y_j^2-\sum_{kl}x_ky_kx_ly_l.
    \end{equation}
    Ensuite nous coupons les sommes de la façon suivante
    \begin{equation}
        \sum_{ij}=\sum_j\sum_{i<j}+\sum_j(i=j)+\sum_j\sum_{i>j}
    \end{equation}
    pour obtenir
    \begin{equation}
        \begin{aligned}[]
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\sum_{i<j}x_i^2y_j^2+\sum_jx_j^2y_j^2+\sum_j\sum_{i>j}x_i^2y_j^2\\
                &\quad-\sum_l\sum_{k<l}x_ky_kx_ly_l-\sum_kx_k^2y_k^2-\sum_l\sum_{k>l}x_ky_kx_ly_l.
        \end{aligned}
    \end{equation}
    Il y a deux termes qui se simplifient. Notez que si \( A_{kl}\) est symétrique en \( kl\) nous avons
    \begin{equation}
        \sum_l\sum_{k<l}A_{kl}=\sum_k\sum_{l<k}A_{lk}=\sum_k\sum_{l<k}A_{kl}.
    \end{equation}
    La première égalité était seulement un renommage des indices. Le coup des indices symétriques est justement ce qu'il se passe dans les deux termes en\( x_ky_kx_ly_l\), donc nous les regroupons :
    \begin{subequations}
        \begin{align}
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\big( \sum_{i<j}x_i^2x_j^2+\sum_{i>j}x_i^2y_j^2-2\sum_{i>j}x_iy_ix_jy_j \big)\\
            &=\sum_j\sum_{i<j}(x_i^2y_j^2+x_j^2y_i^2-2x_iy_ix_jy_j)\\
            &=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
        \end{align}
    \end{subequations}
    Voila qui prouve la première formule. Pour la seconde, il faut seulement poser \( n=3\) et écrire les sommes explicitement.

    \begin{itemize}
        \item 
    Pour \( j=1\), la somme sur \( i\) est \( \sum_{i<1}\), c'est-à-dire aucun termes.
\item
    Pour \( j=2\), il y a seulement \( i=1\), donc le terme \( (x_1y_2-x_2y_1)^2\).

\item
    Pour \( j=3\), il y a les termes \( i=1\) et \( i=2\), donc les termes \( (x_1y_3-x_3y_1)^2+(x_2y_3-x_3y_2)^2\).
    \end{itemize}
    Ces trois termes collectés sont justement les composants (au carré) de \( x\times y\) données dans la formule \eqref{SEBEQooVROKooRpUOIr}.
\end{proof}

Les trois vecteurs de base $e_x$, $e_y$ et $e_y$ ont des produits vectoriels faciles à retenir :
\begin{equation}
    \begin{aligned}[]
        e_x\times e_y&=e_z\\
        e_y\times e_z&=e_x\\
        e_z\times e_x&=e_y
    \end{aligned}
\end{equation}

Les deux formules suivantes, qui mêlent le produit scalaire et le produit vectoriel, sont souvent utiles en analyse vectorielle :
\begin{equation}
	\begin{aligned}[]
		(u\times v)\cdot w&=u\cdot(v\times w)\\
		(u\times v)\times w&=-(v\cdot w)u+(u\cdot w)v		\label{EqFormExpluxxx}
	\end{aligned}
\end{equation}
pour tout vecteurs $u$, $v$ et $w$ dans $\eR^3$. Nous les admettons sans démonstration. La seconde formule est parfois appelée \defe{formule d'expulsion}{formule!d'expulsion (produit vectoriel)}.

\begin{example}
    Calculons le produit vectoriel $v\times w$ avec
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                3    \\
                -1    \\
                1
            \end{pmatrix}&w=\begin{pmatrix}
                1    \\
                2    \\
                -1
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Les vecteurs s'écrivent sous la forme $v=3e_x-e_y+e_z$ et $w=e_x+2e_y-e_z$. Le produit vectoriel s'écrit
    \begin{equation}
        \begin{aligned}[]
            (3e_x-e_y+e_z)\times (e_x+2e_y-e_z)&=6e_x\times e_y-3e_x\times e_z\\
                                &\quad -e_y\times e_x + e_y\times e_z\\
                                &\quad + e_z\times e_x + 2e_z\times e_y\\
                                &=6e_z+3e_y+e_z+e_x+e_y-2e_x\\
                                &=-e_x+4e_y+7e_z.
        \end{aligned}
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si $a$, $b$ et $c$ sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre $a\cdot(b\times c)$. En écrivant le produit vectoriel sous forme de somme de trois déterminants $2\times 2$, nous avons
\begin{equation}
    \begin{aligned}[]
        a\cdot& (b\times c)\\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
        \begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}e_x-\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}e_y+\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\right)\\
        &=a_1\begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}-a_2\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}+a_3\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\\
        &=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3    \\
            c_1    &   c_2    &   c_3
        \end{vmatrix}.
    \end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
    a\cdot (b\times c)=\begin{vmatrix}
        a_1    &   a_2    &   a_3    \\
        b_1    &   b_2    &   b_3    \\
        c_1    &   c_2    &   c_3
    \end{vmatrix}.
\end{equation}

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}     \label{PROPooTUVKooOQXKKl}
    Le produit vectoriel\footnote{Définition \ref{DEFooTNTNooRjhuJZ}.} $a\times b$ est un vecteur orthogonal à $a$ et $b$.
\end{proposition}

\begin{proof}
    Vérifions que $a\perp (a\times b)$. Pour cela, nous calculons $a\cdot (a\times b)$, c'est-à-dire le produit mixte
    \begin{equation}
        a\cdot(a\times b)=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3
        \end{vmatrix}=0.
    \end{equation}
    L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

Ces résultats admettent une intéressante généralisation.
\begin{lemma}       \label{LEMooFRWKooVloCSM}
    Soit \( X\in \eR^n\) ainsi que \( v_1,\ldots, v_{n-1}\in \eR^n\). Alors
    \begin{enumerate}
        \item
            Nous avons
            \begin{equation}        \label{EQooMQNPooRHHBjz}
                \det(X,v_1,\ldots, v_{n-1})=X\cdot
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
        \item
            Le vecteur
            \begin{equation}
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
            est orthogonal à tous les \( v_i\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que les deux côtés de \eqref{EQooMQNPooRHHBjz} vus comme fonctions de \( X\), sont des applications linéaires de \( \eR^n\) dans \( \eR\), il suffit de vérifier l'égalité sur une base.

    Nous posons \( \tau_i\colon \eR^n\to \eR^{n-1}\),
    \begin{equation}
        \tau_i(v)_k=\begin{cases}
            v_k    &   \text{si } k<i\\
            v_{k+1}    &    \text{si } k\geq i\text{.}
        \end{cases}
    \end{equation}
    et nous avons d'une part
    \begin{equation}
        e_k\cdot
                \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =\det\begin{pmatrix}
                     \tau_kv_1   \\
                     \vdots   \\
                     \tau_kv_{n-1}
                 \end{pmatrix}
            \end{equation}
     et d'autre part,
     \begin{equation}
         \det(e_k,v_1,\ldots, v_{n-1})=\det
         \begin{pmatrix}
             0&&&\\
             \vdots&&&\\
             1&v_1&\cdots&v_{n-1}\\
             \vdots&&&\\
             0&&&
         \end{pmatrix}=\det(\tau_k v_1,\ldots, \tau_k v_{n-1}).
     \end{equation}
     La première assertion est démontrée.

     En ce qui concerne la seconde, il suffit d'appliquer la première et se souvenir qu'un déterminant est nul lorsque deux lignes sont égales\footnote{Corolaire \ref{CORooAZFCooSYINvBl}.}. En effet :
     \begin{equation}
         v_k\cdot \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =
                 \det(v_k,v_1,\ldots, v_n)=0.
     \end{equation}
\end{proof}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Procédé de Gram-Schmidt}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Procédé de Gram-Schmidt]    \label{PropUMtEqkb}
    Un espace euclidien possède une base orthonormée.
\end{proposition}
\index{espace!euclidien}
\index{Gram-Schmidt}

\begin{proof}
    Soit \( E\) un espace euclidien et \( \{ v_1,\ldots, v_n \}\), une base quelconque de \( E\). Nous posons d'abord
    \begin{equation}
        \begin{aligned}[]
            f_1&=v_1,&e_1&=\frac{ f_1 }{ \| f_1 \| }.
        \end{aligned}
    \end{equation}
    Ensuite
    \begin{equation}
        \begin{aligned}[]
            f_2&=v_2-\langle v_2, e_1\rangle e_1,&e_2&=\frac{ f_2 }{ \| f_2 \| }.
        \end{aligned}
    \end{equation}
    Notons que \( \{ e_1,e_2 \}\) est une base de \( \Span\{ v_1,v_2 \}\). De plus elle est orthogonale :
    \begin{equation}
        \langle e_1, f_2\rangle =\langle e_1, v_2\rangle -\langle v_2, e_1\rangle \underbrace{\langle e_1, e_1\rangle}_{=1} =0.
    \end{equation}
    Le fait que \( \| e_1 \|=\| e_2 \|=1\) est par construction. Nous avons donc donné une base orthonormée de \( \Span\{ v_1,v_2 \}\).

    Nous continuons par récurrence en posant
    \begin{equation}
        \begin{aligned}[]
            f_k&=v_k-\sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i,&e_k&=\frac{ f_k }{ \| f_k \| }.
        \end{aligned}
    \end{equation}
    Pour tout \( j<k\) nous avons
    \begin{equation}
        \langle e_j, f_k\rangle =\langle e_j, v_k\rangle -\sum_{i=1}^{k-1}\langle v_k, e_i\rangle \underbrace{\langle e_i, e_j\rangle}_{=\delta_{ij}} =0
    \end{equation}
\end{proof}
Cet algorithme de Gram-Schmidt nous donne non seulement l'existence de bases orthonormée pour tout espace euclidien, mais aussi le moyen d'en construire à partir de n'importe quelle base.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Approximation}
%---------------------------------------------------------------------------------------------------------------------------

Le lemme suivant est surtout intéressant en dimension infinie.
\begin{lemma}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); il existe une suite \( (v_n)\) dans \( A\) telle que \( v_n\stackrel{V}{\longrightarrow}v\) et \( \| v_n \|\leq \| v \|\) pour tout \( n\).
\end{lemma}

\begin{proof}
    Vu que \( A\) est dense, il existe une suite \( a_n\) dans \( A\) telle que \( a_n\to v\). Ensuite il suffit de poser
    \begin{equation}
        v_n=\frac{ n }{ n+1 }\frac{ \| v \| }{ \| a_n \| }a_n.
    \end{equation}
    Par construction nous avons toujours
    \begin{equation}
        \| v_n \|=\frac{ n }{ n+1 }\| v \|\leq \| v \|.
    \end{equation}
    Et de plus, la norme étant continue\footnote{Où dans le calcul suivant nous utilisons la continuité de la norme ? Posez-vous la question.},
    \begin{equation}
        \lim_{n\to \infty} v_n=\lim_{n\to \infty} \frac{ n }{ n+1 }\lim_{n\to \infty} \frac{ \| v \| }{ \| v_n \| }\lim_{n\to \infty} v_n=v.
    \end{equation}

    Le fait que \( v_n\) soit dans \( A\) est dû au fait que \( A\) soit vectoriel.
\end{proof}

\begin{proposition}     \label{PROPooVEMGooYKhMFy}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); pour tout \( a\in \eR\) nous avons
    \begin{equation}
        \sup\{ | v\cdot a |\tq a\in A\text{ et }\| a \|\leq \lambda \}=\lambda\| v \|.
    \end{equation}
\end{proposition}

\begin{proof}
    D'abord pour tout \( a\in A\) vérifiant \( \| a \|\leq \lambda\) l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG} donne
    \begin{equation}
        | v\cdot a |\leq \| v \|\| a \|\leq \lambda\| v \|.
    \end{equation}
    Donc le supremum dont on parle est majoré par \( \lambda\| v \|\).

    Il nous faut l'inégalité dans l'autre sens. Par densité nous pouvons choisir une suite \( v_n\in A\) tel que \( v_n\to v\). Ensuite nous posons
    \begin{equation}
        a_n=\frac{ \lambda }{ \| v_n \| }v_n.
    \end{equation}
    Nous avons \( \| a_n \|=\lambda\) pour tout \( n\) et
    \begin{equation}
        | v\cdot a_n |=\frac{ \lambda }{ \| v_n \| }| v\cdot v_n |,
    \end{equation}
    et en passant à la limite,
    \begin{equation}
        \lim_{n\to \infty} | v\cdot a_n |=\frac{ \lambda }{ \| v \| }\| v\cdot v \|=\lambda\| v \|.
    \end{equation}
    Donc l'ensemble sur lequel nous prenons le supremum contient une suite convergente vers \( \lambda\| v \|\). Le supremum est donc au moins aussi grand que cela.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques exemples de normes sur \texorpdfstring{$\eR^n$}{Rn}}
%---------------------------------------------------------------------------------------------------------------------------

Il est possible de définir de nombreuses normes sur $\eR^n$. Citons-en quelques-unes.

\begin{propositionDef}      \label{PROPooCLZRooIRxCnZ}
    Les formules suivantes définissent des normes sur \( \eR^n\).
    \begin{enumerate}
        \item
    Les normes $\| . \|_{L^p}$ ($p\in\eN$) sont définies de la façon suivante :
    \begin{equation}		\label{EqDeformeLp}
        \| x \|_{L^p}=\Big( \sum_{i=1}^n| x_i |^p\Big)^{1/p},
    \end{equation}
    pour tout $x=(x_1,\ldots,x_n)\in\eR^n$.
\item
    La norme $L^2$ est la \defe{norme euclidienne}{norme!euclidienne}.
\item
    Nous définissons également la \defe{norme supremum}{norme!supremum} par
    \begin{equation}
	    \| x \|_{\infty}=\max_i| x_i |.
    \end{equation}
    \end{enumerate}
\end{propositionDef}

\begin{proof}
    Point par point\quext{Preuve non terminée}.
    \begin{enumerate}
        \item
            Le cas \( p=1\) est déjà fait dans le lemme \ref{LEMooRWJYooOIJkZc}.
        \item
    Le fait que \( x\mapsto\| x \|_{L^2}\) soit une norme provient de la propriété suivante :
    \begin{equation}
        \sqrt{ (a+b)^2 }\leq \sqrt{ a^2 }+\sqrt{ b^2 },
    \end{equation}
    laquelle se démontre en passant au carré :
    \begin{equation}        \label{EQooRYNYooTzZpPz}
        (a+b)^2=a^2+b^2+2ab\leq a^2+b^2+2| ab |=\big( \sqrt{ a^2 }+\sqrt{ b^2 } \big)^2.
    \end{equation}
\item
    \end{enumerate}
\end{proof}

Parmi ces normes, celles qui seront le plus souvent utilisées dans ces notes sont
\begin{equation}
	\begin{aligned}[]
		\| x \|_{L^1}&=\sum_{i=1}^n| x_i |,\\
		\| x \|_{L^2}&=\Big( \sum_{i=1}^n| x_i |^2 \Big)^{1/2}.
	\end{aligned}
\end{equation}

\newcommand{\CaptionFigDistanceEuclide}{La \emph{norme} euclidienne induit la \emph{distance} euclidienne. D'où son nom. Le point $C$ est construit aux coordonnées $(A_x,B_y)$.}
\input{auto/pictures_tex/Fig_DistanceEuclide.pstricks}

Soient $A=(A_x,A_y)$ et $B=(B_x,B_y)$ deux éléments de $\eR^2$. La distance\footnote{Ne pas confondre «distance» et «norme».} euclidienne entre $A$ et $B$ est donnée par $\| A-B \|_2$. En effet, sur la figure~\ref{LabelFigDistanceEuclide}, la distance entre les points $A$ et $B$ est donnée par
\begin{equation}
	| AB |^2=| AC |^2+| CB |^2=| A_x-B_x |^2+| A_y-B_y |^2,
\end{equation}
par conséquent,
\begin{equation}
	| AB |=\sqrt{| A_x-B_x |^2+| A_y-B_y |^2}=\| A-B \|_2.
\end{equation}

\begin{remark}
	Si $A$, $B$ et $C$ sont trois points dans le plan $\eR^2$, alors l'inégalité triangulaire $| AB |\leq| AC |+| CB |$ est précisément la propriété~\ref{ItemDefNormeiii} de la norme (définition~\ref{DefNorme}). En effet l'inégalité triangulaire s'exprime de la façon suivante en termes de la norme $\| . \|_2$ :
	\begin{equation}	\label{EqNDeuxAmBNNdd}
		\| A-B \|_2\leq \| A-C \|_2+\| C-B \|_2.
	\end{equation}
	En notant $u=A-C$ et $v=C-B$, l'équation \eqref{EqNDeuxAmBNNdd} devient exactement la propriété de définition de la norme :
	\begin{equation}
		\| u+v \|_2\leq \| u \|_2+\| v \|_2.
	\end{equation}
	Ceci explique pourquoi cette propriété des normes est appelée «inégalité triangulaire».
\end{remark}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équivalence des normes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{normes_equiv}

Au premier coup d'œil, les notions dont nous parlons dans ce chapitre ont l'air très générales. Nous prenons en effet n'importe quel espace vectoriel $V$ de dimension finie, et nous le munissons de n'importe quelle norme (rien que dans $\eR^m$ nous en avons défini une infinité par l'équation \eqref{EqDeformeLp}). À partir de ces données, nous définissons les boules, la topologie, l'adhérence, etc.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{En dimension finie}
%---------------------------------------------------------------------------------------------------------------------------

Dans $\eR^n$, les normes $\| . \|_{L^1}$, $\| . \|_{L^2}$ et $\| . \|_{\infty}$ ne sont pas égales. Cependant elles ne sont pas complètement indépendantes au sens où l'on sent bien que si un vecteur sera grand pour une norme, il sera également grand pour les autres normes; les normes «vont dans le même sens». Cette notion est précisée par le concept de norme équivalente.

\begin{definition}		\label{DefEquivNorm}
    Deux normes $N_1$ et $N_2$ sur $\eR^m$ sont \defe{\wikipedia{fr}{Norme_équivalente}{équivalentes}}{equivalence@équivalence!norme}\index{norme!équivalence}\index{équivalence!de norme} s'il existe deux nombres réels strictement positifs $k_1$ et $k_2$ tels que
	\begin{equation}
		k_1N_1(x)\leq N_2(x)\leq k_2 N_1(x),
	\end{equation}
	pour tout $x$ dans $\eR^m$. Dans ce cas nous écrivons que $N_1\sim N_2$.
\end{definition}

\begin{lemma}       \label{LEMooHAITooWdtLAN}
    La définition de norme équivalentes donne une relation d'équivalence (définition~\ref{DefHoJzMp}) sur l'ensemble des normes existantes sur $\eR^m$.
\end{lemma}

\begin{proposition} \label{PropLJEJooMOWPNi}
    Pour \( \eR^N\), nous avons les équivalences de normes $\| . \|_{L^1}\sim\| . \|_{L^2}$, $\| . \|_{L^1}\sim\| . \|_{\infty}$ et $\| . \|_{L^2}\sim\| . \|_{\infty}$. Plus précisément nous avons les inégalités
    \begin{enumerate}
        \item\label{ItemABSGooQODmLNi}
           $ \| x \|_2\leq \| x \|_1\leq\sqrt{n}\| x \|_2$
        \item\label{ItemABSGooQODmLNii}
            $\| x \|_{\infty}\leq \| x \|_1\leq n \| x \|_{\infty}$
        \item\label{ItemABSGooQODmLNiii}
            $\| x \|_{\infty}\leq \| x \|_2\leq \sqrt{n}\| x \|_{\infty}$
    \end{enumerate}
\end{proposition}


\begin{proof}
    En mettant au carré la première inégalité nous voyons que nous devons vérifier l'inégalité
    \begin{equation}
        | x_1 |^2+\cdots+| x_n |^2\leq\big( | x_1 |+\cdots+| x_n | \big)^2
    \end{equation}
    qui est vraie parce que le membre de droite est égal au carré de chaque terme plus les double produits. La seconde inégalité provient de l'inégalité de Cauchy-Schwarz (théorème~\ref{ThoAYfEHG}) sur les vecteurs
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                1/n    \\
                \vdots    \\
                1/n
            \end{pmatrix},
            &w&=\begin{pmatrix}
                | x_1 |    \\
                \vdots    \\
                | x_n |
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous trouvons
    \begin{equation}
        \frac{1}{ n }\sum_i| x_i |\leq\sqrt{n\cdot\frac{1}{ n^2 }}\sqrt{\sum_i| x_i |^2},
    \end{equation}
    et par conséquent
    \begin{equation}
        \sum_i| x_i |\leq\sqrt{n}\| x \|_2.
    \end{equation}

    La première inégalité de~\ref{ItemABSGooQODmLNiii} se démontre en remarquant que si \( a\) et \( b\) sont positifs, \( a\leq\sqrt{a^2+b}\). En appliquant cela à \( a=\max_i| x_i |\), nous avons
    \begin{equation}
        \max_i| x_i |\leq\sqrt{ | x_1 |^2+\cdots+| x_n |^2  }
    \end{equation}
    parce que \( \max_i| x_i |\) est évidemment un des termes de la somme. Pour la seconde inégalité de~\ref{ItemABSGooQODmLNiii}, nous avons
    \begin{equation}
        \sqrt{\sum_k| x_k |^2}\leq\left( \sum_k\max_i| x_i |^2 \right)^{1/2}=\sqrt{n}\| x \|_{\infty}.
    \end{equation}
    Pour obtenir cette inégalité, nous avons remplacé tous les termes \( | x_k |\) par le maximum.
\end{proof}

Pour les autres normes \( \| . \|_p\), il y a des inégalités dans \ref{THOooPPDPooJxTYIy} et \ref{CORooMBQMooWBAIIH}; voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.

En réalité, toutes les normes \( \| . \|_{L^p}\) et \( \| . \|_{\infty}\) sont équivalentes et, plus généralement, nous avons le résultat suivant, très étonnant à première vue, et en réalité assez difficile à prouver :
\begin{theorem}[\cite{TrenchRealAnalisys}]		\label{ThoNormesEquiv}
	Sur un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
\end{theorem}
% TODO : la preuve est à la page 583 de Trench.

\begin{corollary}       \label{CORooBRDYooLmGJDE}
    Soit \( V\) un espace vectoriel de dimension finie et \( \| . \|_1\), \( \| . \|_2\) deux normes sur \( V\). Alors l'identité \( \id\colon V\to V\) est un isomorphisme d'espace topologique \( (V,\| . \|_1)\to (V,\| . \|_2)\).

    De plus les ouverts sont les mêmes : une partie de \( V\) est ouverte dans \( (V,\| . \|_1)\) si et seulement si elle est ouverte dans \( (V,\| . \|_2)\).
\end{corollary}

\begin{normaltext}      \label{NORMooNKBCooKziIjx}
    Le lemme \ref{LEMooRWJYooOIJkZc} donnera une norme sur \( \eR^2\) qui ne dérive pas d'un produit scalaire. Vu que toutes les normes sur \( \eR^2\) produisent la même topologie (c'est le corolaire~\ref{CORooBRDYooLmGJDE}), il y a parfaitement moyen pour deux espaces vectoriels topologiques d'être isomorphes alors que l'un a une norme dérivant d'un produit scalaire et l'autre non.
\end{normaltext}

\begin{normaltext}
    Le théorème d'équivalence de norme sera utilisé pour montrer que l'ensemble des formes quadratiques non dégénérées de signature \( (p,q)\) est ouvert dans l'ensemble des formes quadratiques, proposition~\ref{PropNPbnsMd}. Plus généralement il est utilisé à chaque fois que l'on fait de la topologie sur les espaces de matrices en identifiant \( \eM(n,\eR)\) à \( \eR^{n^2}\), pour se rassurer en se disant que ce qu'on fait ne dépend pas de la norme choisie.

    Voir aussi ce qu'on en fait en \ref{NORMooDAZZooDiGFoW} pour démontrer la différentiabilité à partir des dérivées partielles.
\end{normaltext}

\begin{proposition}[\cite{MonCerveau}] \label{PROPooNTCFooEcwZwt}
    Let \( V\) be a finite dimensional complex vector space. For a basis \( B=\{ e_1,\ldots, e_n \}\) of \( V\) we define
    Soit un espace vectoriel \( V\) de dimension finie sur \( \eC\). Pour une base \( B= \{ e_i \}\) de \( V\) nous définissons
    \begin{equation}        \label{EQooEGXVooLASQIC}
        \| \sum_kv_ke_k \|_B= \sqrt{ \sum_k| v_k |^2 }.
    \end{equation}
    \begin{enumerate}
        \item
            La formule \eqref{EQooEGXVooLASQIC} définit une norme sur \( V\).
        \item
            Si \( B\) et \( B'\) sont des bases de \( V\), alors les topologies induites par le norme \( \| . \|_B\) et \( \| . \|_{B'}\) sont égales.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous commençons par fixer une base \( B=\{ e_i \}_{i=1,\ldots, n}\) de \( V\). Cette base nous permet de définir
    \begin{equation}
        \begin{aligned}
            \varphi\colon V&\to \eC^n \\
            \sum_kv_ke_k&\mapsto (v_1,\ldots, v_n). 
        \end{aligned}
    \end{equation}
    Cette application linéaire permet d'écrire
    \begin{equation}
        \| v \|_V=\| \varphi(v) \|_{\eC^n}.
    \end{equation}
    À partir de là, la vérification des propriétés de la définition \ref{DefNorme} est immédiate. Par exemple :
    \begin{equation}
        \| v+w \|=\| \varphi(v+w) \|=\| \varphi(v)+\varphi(w) \|\leq \| \varphi(v) \|+\| \varphi(w) \|=\| v \|+\| w \|.
    \end{equation}

    En ce qui concerne la seconde assertion, c'est le théorème \ref{ThoNormesEquiv}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Contre-exemple en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecPOlynomesCE}

Lorsque nous considérons des espaces vectoriels de dimension infinie, les choses ne sons plus aussi simples. Nous voyons ici sur l'exemple de l'espace des polynômes que le théorème~\ref{ThoNormesEquiv} n'est plus valable si on enlève l'hypothèse de dimension finie.

On considère l'ensemble des fonctions polynomiales à coefficients réels sur  l'intervalle $[0,1]$.
\begin{equation}
\mathcal{P}_\eR([0,1])=\{p:[0,1]\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots, \, a_i\in\eR,\,\forall i\in \eN\}.
\end{equation}
Cet ensemble, muni des opérations usuelles de somme entre polynômes et multiplications par les scalaires, est un espace vectoriel.

Sur $\mathcal{P}(\eR)$ on définit les normes suivantes
\begin{equation}
\begin{aligned}
&\|p\|_\infty=\sup_{x\in[0,1]}\{p(x)\},\\
&\|p\|_1 =\int_0^1|p(x)|\, dx,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}.\\
\end{aligned}
\end{equation}
Les inégalités suivantes sont  immédiates
\begin{equation}
\begin{aligned}
&\|p\|_1 =\int_0^1|p(x)|\, dx\leq \|p\|_\infty,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}\leq \|p\|_\infty,\\
\end{aligned}
\end{equation}
mais la norme $\|\cdot\|_\infty$ n'est  équivalente ni à $\|\cdot\|_1$, ni à $\|\cdot\|_2$. Soit $p_k(x)= x^k$. Alors
\begin{equation}
\begin{aligned}
&\|p_k\|_\infty=1,\\
&\|p_k\|_1 =\int_0^1x^k\, dx=  \frac{1}{k+1},\\
&\|p_k\|_2 =\left(\int_0^1x^{2k}\, dx\right)^{1/2}=\sqrt{\frac{1}{2k+1}}.
\end{aligned}
\end{equation}
Pour $k\to \infty$ les normes $\|p_k\|_1$, $\|p_k\|_2$ tendent vers zéro, alors que la norme $\|p_k\|_\infty$ est constante, donc les normes ne sont pas équivalentes parce que il n'existe pas un nombre positif $m$ tel que
\begin{equation}
\begin{aligned}
& m \|p_k\|_\infty\leq \|p_k\|_1 ,\\
& m \|p_k\|_\infty\leq \|p_k\|_2 ,\\
\end{aligned}
\end{equation}
uniformément pour tout $k$ dans $\eN$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit fini d'espaces vectoriels normés}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{sec_prod}

Dans cette sections nous parlons de produits finis d'espaces. Cela ne signifie pas que chacun des espaces soient séparément de dimension finie.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Distance et norme produit}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[Distance produit]    \label{DefZTHxrHA}
    Si \( (E_1,d_1)\),\ldots, \( (E_n,d_n)\) sont des espaces métriques alors la formule
    \begin{equation} 
        d(x,y)=\max_{i=1,\ldots, n}d_i(x_i,y_i)
    \end{equation}
    définit une distance sur le produit cartésien \( E=E_1\times\ldots\times E_n\). Elle est la \defe{distance produit}{distance produit}.
\end{propositionDef}

La définition de la norme sur un produit d'espaces vectoriels normés découle immédiatement de la définition de la distance~\ref{DefZTHxrHA} :
\begin{lemmaDef}[\cite{ooALKGooMAzKpz}]  \label{DefFAJgTCE}
    Soient $V$ et $W$ deux espaces vectoriels normés. 
    \begin{enumerate}
        \item
            L'ensemble
            \begin{equation}
            V\times W=\{(v,w)\,|\, v\in V,\, w\in W\}
            \end{equation}
            est un espace vectoriel.
        \item 
            L'opération
            \begin{equation}	\label{EqNormeVxWmax}
                \|(v,w) \|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}.
            \end{equation}
            est une norme\footnote{Définition \ref{DefNorme}.} sur \( V\times V\).
        \item
            La topologie de cette norme est la même que la topologie produit sur \( V\times W\).
    \end{enumerate}
    L'espace vectoriel \( V\times W\) muni de cette norme est l'\defe{espace produit}{produit!d'espaces vectoriels normés} de $V$ et $W$. La topologie ainsi définie de deux manières est la topologie qui sera toujours considérée dans le cas de produit d'espaces vectoriels normés.
\end{lemmaDef}

\begin{proof}
    En plusieurs parties.
    \begin{subproof}
        \item[Espace vectoriel]
                Il est presque immédiat de vérifier que le produit cartésien $V\times W$ est un espace vectoriel pour les opération de somme et multiplication par les scalaires définies composante par composante. C'est-à-dire,  si $(v_1,w_1)$, $(v_2,w_2)$ sont dans $V\times W$ et $a$, $b$ sont des scalaires, alors
                \begin{equation}
                    a (v_1,w_1)+ b(v_2,w_2)=(av_1,aw_1)+ (bv_2,bw_2)=(av_1+bv_2,aw_1+bw_2).
                \end{equation}

            \item[Norme]
                On doit vérifier les trois conditions de la définition~\ref{DefNorme}.
                \begin{itemize}
                    \item Soit $(v,w)$ dans $V\times W$ tel que $\|(v,w)\|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}=0$. Alors $\|v\|_V=0$ et $\|w\|_W=0$, donc $v=0_V$ et $w=0_W$. Cela implique $(v,w)=(0_v,0_w)=0_{V\times W}$.
                    \item Pour tout $a$ dans $\eR$ et $(v,w)$ dans $V\times W$, la norme $\|a (v,w)\|_{V\times W}$ se calcule de la façon suivante :
                        \begin{equation}
                            \|a (v,w)\|_{V\times W}= \max\{ \| av \|_V,\| aw \|_W \} =|a|\max\{\|v\|_{V},\|w\|_W\}=|a|\|(v,w)\|_{V\times W}.
                        \end{equation}
                    \item Soient $(v_1,w_1)$ et $(v_2,w_2)$ dans $V\times W$.
                    \begin{equation}
                        \begin{aligned}
                            \|(v_1,w_1)+(v_2,w_2)\|_{V\times W}&=\max\{\|v_1+v_2\|_{V},\|w_1+w_2\|_W\}\\
                            &\leq \max\{\|v_1\|_V+\|v_2\|_{V},\|w_1\|_W+\|w_2\|_W\}\\
                            &\leq\max\{\|v_1\|_V,\|w_1\|_W\}+ \max\{\|v_2\|_{V},\|w_2\|_W\}\\
                            &=\|(v_1,w_1)\|_{V\times W}+\|(v_2,w_2)\|_{V\times W}.
                        \end{aligned}
                    \end{equation}
                \end{itemize}
            \item[Équivalence]

    Dans cette preuve, nous considérons la «topologie de \( V\times W\)» comme étant la topologie produit et «la topologie métrique de \( V\times W\)» la topologie de la norme produit.
    \begin{subproof}
        \item[Dans un sens]
            La proposition \ref{LEMooKJJNooMHNcSP} dit qu'une prébase de \( V\times W\) est donnée par 
            \begin{equation}
                \big\{   B(v,r)\times B(w,s)\tq v\in V;w\in W;r,s>0   \big\}.
            \end{equation}
            Nous prouvons maintenant que la partie \( S= B(v_0,r)\times B(w_0,s)\) est un ouvert de l'espace \( \big( V\times W,\| . \|_{V\times W} \big)\). Pour cela nous prouvons que tout élément de \( S\) contient un voisinage métrique contenu dans \( S\).

            Soit \( (v_1,w_1)\in S\). Nous posons
            \begin{equation}
                d\big( (v_1,w_1), (v_0,w_0) \big)=\delta<\max\{ r,s \}.
            \end{equation}
            Nous considérons \( \epsilon>0\) et nous montrons que si \( \epsilon\) est assez petit, \( B\big( (v_1,w_1),\epsilon \big)\subset S\). Pour cela nous considérons \( (v,w)\in B\big( (v_1,w_1),\epsilon \big)\) et nous calculons un tout petit peu :
            \begin{subequations}
                \begin{align}
                    d\big( (v,w),(v_0,w_0) \big)&\leq d\big( (v,w),(v_1,w_1) \big)+d\big( (v_1,w_1),(v_0,w_0) \big)\\
                    &<\epsilon+\delta.
                \end{align}
            \end{subequations}
            Si \( \epsilon\) est assez petit, le tout reste plus petit que \( \max\{ r,s \}\).

            Donc \( S\) est bien un ouvert métrique par le théorème \ref{ThoPartieOUvpartouv}. Vu que la topologie métrique contient une prébase de la topologie produit, tout ouvert de la topologie produit est un ouvert de la topologie métrique.
        \item[Dans l'autre sens]
            Soient un ouvert métrique \( \mO\) ainsi que \( (v_0,w_0)\in \mO\); il existe \( r>0\) tel que
            \begin{equation}
                B\big( (v_0,w_0),r \big)\subset \mO.
            \end{equation}
            Nous affirmons que \( B(v_0,r)\times B(w_0,r)\) est contenu dans \( \mO\), de telle sorte que \( \mO\) soit un ouvert de la topologie produit. Pour \( (v_1,w_1)\in B(v_0,r)\times B(w_0,r)\) nous avons
            \begin{equation}
                    d\big( (v_1,w_1),(v_0,w_0) \big)=\max\{ \| v_1-v_0 \|,\| w_1-w_0 \| \}<r
            \end{equation}
            parce que \( v_1\in B(v_0,r)\) et \( w_1\in B(w_0,r)\).

            Donc tout élément de \( \mO\) admet un voisinage «produit» contenu dans \( \mO\); donc \( \mO\) est ouvert pour le topologie produit.
    \end{subproof}
    \end{subproof}
\end{proof}

\begin{normaltext}
    En particulier, pour la topologie de la norme maximum, la convergence d'une suite implique la convergence «composante par composante» par la proposition~\ref{PROPooNRRIooCPesgO}.
\end{normaltext}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Topologie réelle en dimension $n$}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ouverts et fermés}
%---------------------------------------------------------------------------------------------------------------------------

La définition suivante est là juste pour la facilité des notations; nous ne disons pas qu'elle est liée à la topologie sur \( \eR^n\).
\begin{definition}  \label{DefZVuBbqp}
	La \defe{boule ouverte}{boule!ouverte} de centre $x_0 \in \eR^n$ et de rayon $r \in
	\eR^+$ est définie par
	\begin{equation}
		B(x_0,r) = \{ x \in \eR^n \tq \norme{x - x_0} < r \},
	\end{equation}
	tandis que la \defe{boule fermée}{boule!fermée} de centre $x_0$ et de rayon $r$ est
	\begin{equation}
        \overline{  B(x_0,r)} = \{ x \in \eR^n \tq \norme{x - x_0} \leq r \};
	\end{equation}
	la différence est que l'inégalité dans la première est stricte.
\end{definition}

\begin{propositionDef}
    Sur \( \eR^n\), nous considérons les deux topologies suivantes :
    \begin{enumerate}
        \item       \label{ITEMooWACPooFBAWhx}
            la topologie produit \( \eR\times \ldots\times \eR\) des espaces topologiques \( (\eR,| . |)\),
        \item       \label{ITEMooJPJHooGTuLen}
            la topologie de la norme
            \begin{equation}
                \| (x_1,\ldots, x_n) \|_{\infty}=\max_i\{ | x_i | \},
            \end{equation}
        \item       \label{ITEMooEBYQooXiOOtb}
            la topologie de la norme
            \begin{equation}
                \| (x_1,\ldots, x_n) \|_2=\sqrt{ \sum_{i=1}^nx_i^2 }.
            \end{equation}
    \end{enumerate}
    Ces deux topologies sont égales et sont la topologie que nous allons toujours considérer sur \( \eR^n\) (saut mention très explicite du contraire).
\end{propositionDef}

\begin{proof}
    Les topologies \ref{ITEMooWACPooFBAWhx} et \ref{ITEMooJPJHooGTuLen} sont identiques par le lemme \ref{DefFAJgTCE}. Les topologies \ref{ITEMooJPJHooGTuLen} et \ref{ITEMooEBYQooXiOOtb} sont identiques par la proposition \ref{PropLJEJooMOWPNi} (ou plus généralement par le théorème \ref{ThoNormesEquiv} qui donne l'équivalence de toutes les normes).
\end{proof}

\begin{proposition}\label{PROPooEQYJooBbPiAj}
    Une partie \( A\) de \( \eR^n\) est ouverte si et seulement si pour tout \( a\in A\) il existe \( r>0\) tel que \( B(a,r)\subset A\).
\end{proposition}
Cette proposition est évidemment à mettre en rapport avec le théorème~\ref{ThoPartieOUvpartouv}.

Le lemme suivant justifie le vocabulaire des définitions~\ref{DefZVuBbqp}.
\begin{lemma}   \label{LemMESSExh}
    Pour tout $x \in \eR^n$ et tout $r >0$ la boule \( B(x,r)\) est ouverte.
\end{lemma}

\begin{proof}
    Afin de prouver que la boule est ouverte, nous prenons un point $p\in B(x,r)$, et nous allons montrer qu'il existe une boule autour de $p$ qui est contenue dans $B(x,r)$.

    Étant donné que $p\in B(x,r)$, nous avons $d(p,x)<r$. Prouvons que la boule $B\big(p,r-d(p,x)\big)$ est contenue dans $B(x,r)$. Pour cela, nous prenons $p'\in B\big(p,r-d(p,x)\big)$, et nous essayons de prouver que $p'\in B(x,r)$. En effet, en utilisant l'inégalité triangulaire,
    \begin{equation}
	    d(x,p')\leq d(x,p)+d(p,p')\leq d(x,p)+r-d(p,x)=r.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Intérieur, adhérence et frontière}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
  Soient $A \subset \eR^n$ et $x \in \eR^n$. Le point $x$ est \defe{intérieur}{intérieur} à $A$ s'il existe une boule autour de $x$ complètement contenue dans $A$. L'ensemble des points intérieurs à $A$ est noté $\Int A$ ou $\mathring A$, de sorte qu'on a précisément
  \begin{equation*}
    x \in \Int A \iffdefn  \exists \epsilon > 0 \tq
    B(x,\epsilon) \subset A.
  \end{equation*}
\end{definition}

\begin{normaltext}

La notion d'adhérence a déjà été définie en~\ref{DEFooSVWMooLpAVZR}, et précisé par le lemme~\ref{LEMooILNCooOFZaTe}. Dans le cas de \( \eR^n\) dans lequel les boules forment une base de la topologie nous pouvons encore préciser de la façon suivante:
\begin{equation}
	x \in \Adh A \iffdefn \forall \epsilon > 0, B(x,\epsilon) \cap A \neq \emptyset
\end{equation}
\end{normaltext}

\begin{proposition}
Pour $A \subset \eR^n$, nous avons
\begin{equation*}
	\Int A \subseteq A  \subseteq \Adh A
\end{equation*}
\end{proposition}

\begin{definition}      \label{DEFooACVLooRwehTl}
  La \defe{frontière}{frontière} ou le \defe{bord}{bord} de $A$ est défini par $\partial A = \Adh A \setminus \Int A$. L'ensemble $A$ est un \defe{ouvert}{ouvert} si $A = \Int A$, et c'est un \defe{fermé}{fermé} si $A = \Adh A$.
\end{definition}

\begin{lemma}[Caractérisation équivalente de la frontière]      \label{LEMooEUYEooYcUfKr}
    Soient \( X\) un espace topologique et \( S\subset X\). Un point \( x\in X\) est dans \( \partial S\) si et seulement si tout voisinage de \( x\) contient un point de \( S\) et un point de \( S^c\).
\end{lemma}

\begin{proof}
    Supposons que tout voisinage de \( x\) contienne un point de \( S\) et un point de \( S^c\). Alors \( x\in Adh(S)\) (définition~\ref{DEFooSVWMooLpAVZR}), mais pas dans l'intérieur de \( S\) parce que \( x\) ne possède pas de voisinage contenu dans \( S\). Donc \( x\in \partial S\).

    À l'inverse, si \( x\in\partial S\) alors \( x\) est dans l'adhérence de \( S\) et tout voisinage de \( x\) contient un point de \( S\). Mais \( x\) n'est pas dans l'intérieur de \( S\) et tout voisinage de \( x\) contient un point qui n'est pas dans \( S\), aka un point de \( S^c\).
\end{proof}

\begin{corollary}
    Un ensemble et son complémentaire ont même frontière.
\end{corollary}

\begin{proof}
    Conséquence du lemme~\ref{LEMooEUYEooYcUfKr}. Les points de \( \partial(S^c)\) sont caractérisés par le fait que tout voisinage contient un point de \( S^c\) et un point de \( (S^c)^c=S\).
\end{proof}

\begin{example}
    Soit \( X=\mathopen[ 0 , 1 \mathclose]\) muni de la topologie de la distance \( | x-y |\) (définition~\ref{ThoORdLYUu}). Les points \( 0\) et \( 1\) \emph{ne sont pas} dans la frontière de $X$. En effet une boule ouverte autour de \( 1\) est un ensemble de la forme
    \begin{equation}
        B(1,r)=\{ x\in X\tq | x-1 |<r \}=\mathopen] 1-r , 1 \mathclose]
    \end{equation}
    où nous avons supposé \( r<1\).

    Les points \( 0\) et \( 1\) sont par contre sur la frontière de \( \mathopen[ 0 , 1 \mathclose]\) lorsque cet ensemble est vu comme partie de l'espace métrique \( \eR\).
\end{example}

\begin{lemma}[Passage de douane\cite{ooDKEWooFqlDyN,ooWBUCooAdPjMK}]        \label{LEMooLKWEooItGnkP}
    Dans un espace topologique, toute partie connexe qui rencontre à la fois une partie \( A\) et son complémentaire rencontre nécessairement la frontière de \( A\).
\end{lemma}

\begin{proof}
    Nommons \( \gamma\) la partie connexe qui intersecte \( A\) et \( A^c\). Les ouverts \( \Int(A)\) et \( X\setminus \bar A\) ne peuvent pas recouvrir \( \gamma\) parce que ce sont deux ouverts disjoints alors que \( \gamma\) est connexe (voir la définition~\ref{DefIRKNooJJlmiD} de la connexité). Donc \( \gamma\) doit contenir des points qui sont dans \( \bar A\) mais pas dans \( \Int(A)\). C'est-à-dire des points de \( \partial A\).
\end{proof}

On vérifiera que les notations et les dénominations sont cohérentes en prouvant la proposition suivante.
\begin{proposition}Pour $\epsilon > 0$,
  \begin{enumerate}
  \item l'adhérence de $B(x,\epsilon)$ est $\bar B(x,\epsilon)$,
  \item l'intérieur de $\bar B(x,\epsilon)$ est $B(x,\epsilon)$,
  \item la boule ouverte $B(x,\epsilon)$ est un ouvert,
  \item la boule fermée $\bar B(x,\epsilon)$ est un fermé.
  \end{enumerate}
\end{proposition}

Nous avons également les liens suivants entre intérieur, adhérence, ouvert, fermé et passage au complémentaire (noté ${}^c$)~:
\begin{proposition}
Si $A \subset \eR^n$ et $A^c = \eR^n\setminus A$, nous
  avons
  \begin{enumerate}
  \item $(\Int A)^c = \Adh (A^c)$ et $(\Adh A)^c = \Int
    (A^c)$,
  \item $A$ est ouvert si et seulement si $A^c$ est fermé,
  \item $\Int A$ est le plus grand ouvert contenu dans $A$,
  \item $\Adh A$ est le plus petit fermé contenant $A$,
  \end{enumerate}
\end{proposition}

\begin{example} \label{ExBFLooUNyvbw}
    Il n'est en général pas vrai que \( \overline{ A\cap B }=\bar A\cap \bar B\). Par exemple si \( A=\mathopen[ 0 , 1 [\) et \( B=\mathopen] 1 , 2 \mathclose]\). Dans ce cas, \( A\cap B=\emptyset\) alors que \( \bar A\cap\bar B=\{ 1 \}\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Point d'accumulation, point isolé}
%---------------------------------------------------------------------------------------------------------------------------

Les définitions de point d'accumulation et de point isolé sont \ref{DEFooGHUUooZKTJRi} et \ref{DEFooXIOWooWUKJhN}. Nous voyons maintenant ce que ces définitions donnent dans le cas de l'espace topologique \( \eR\).

\begin{lemma}
    Soit $D\subset\eR$. Un point $a\in D$ est isolé dans $D$ si et seulement si il existe $\varepsilon>0$ tel que
    \begin{equation}
        \mathopen[ a-\varepsilon , a+\varepsilon \mathclose]\cap D=\{ a \}.
    \end{equation}
    Autrement dit, il existe un intervalle autour de $a$ dans lequel $a$ est le seul élément de $D$.
\end{lemma}

\begin{lemma}
    Un point $a\in \eR$ est un point d'accumulation de $D$ si pour tout $\varepsilon>0$,
    \begin{equation}
        \Big( \mathopen[ a-\varepsilon , a+\varepsilon \mathclose]\setminus\{ a \} \Big)\cap D\neq\emptyset.
    \end{equation}
    Autrement dit, quel que soit l'intervalle autour de  $a$ que l'on considère, le point $a$ n'est pas tout seul dans $D$.
\end{lemma}

\begin{example}
	Prenons $D=\mathopen[ 0 , 1 [\cup\mathopen] 2 , 3 \mathclose]$. Cet ensemble n'a pas de point isolé, et l'ensemble de ses points d'accumulation est $\mathopen[ 0 , 1 \mathclose]\cup\mathopen[ 2,3  \mathclose]$.

	Notez que les points $1$ et $2$ sont des points d'accumulation de $D$ qui ne font pas partie de $D$. Il est possible d'être un point d'accumulation de $D$ sans être dans $D$, mais pour être un point isolé dans $D$, il faut être dans $D$.
\end{example}

\begin{example}
	Soit $D=\{ \frac{1}{ n }\}_{n\in\eN}$. Tous les points de cet ensemble sont des points isolés (vérifier !).  Aucun point de $D$ n'est point d'accumulation. Cependant $0$ est un point d'accumulation.
\end{example}

\begin{example}     \label{EXooWOYQooJolaTV}
    Soit \( D=\mathopen] 1 , 2 \mathclose[\cup\{ 12 \}\). Le point \( 12\) est adhérence, mais pas d'accumulation parce que le voisinage \( \mathopen] 9 , 14 \mathclose[\) n'intersectionne pas \( D\setminus \{ 12 \}\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Limite de suite}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Limite d'une suite dans $\eR^m$]
	Une suite de points $(x_n)$ dans $\eR^m$ est dite \defe{convergente}{convergence!suite dans $\eR^m$} s'il existe un élément $\ell\in\eR^m$ tel que
	\begin{equation}	\label{EqCondLimSuite}
		\forall\varepsilon>0,\,\exists N\in \eN\tq\,\forall n\geq N,\,\| x_n-\ell \|<\varepsilon.
	\end{equation}
	Dans ce cas, nous disons que $\ell$ est la \defe{limite}{limite!suite dans $\eR^m$} de la suite $(x_n)$ et nous écrivons $\lim x_n=\ell$ ou plus simplement $x_n\to \ell$.
\end{definition}
Notez aussi la similarité avec la définition~\ref{PropLimiteSuiteNum}.

\begin{remark}
	Nous n'écrivons pas «$\lim_{n\to\infty}x_n$» parce que, lorsqu'on parle de suites, la limite est \emph{toujours} lorsque $n$ tend vers l'infini. Il n'y a aucun intérêt à chercher par exemple $\lim_{n\to 4}x_n$ parce que cela vaudrait $x_4$ et rien d'autre.

	Ceci est une différence importante avec les limites de fonctions.
\end{remark}

\begin{lemma}[Unicité de la limite]
	Il ne peut pas y avoir deux nombres différents qui satisfont à la condition \eqref{EqCondLimSuite}. En d'autres termes, si $\ell$ et $\ell'$ sont deux limites de la suite $(x_n)$, alors $\ell=\ell'$.
\end{lemma}

\begin{proof}
	Soit $\varepsilon>0$. Nous considérons $N$ tel que
	\begin{equation}
		\| x_n-\ell \|<\varepsilon
	\end{equation}
	pour tout $n\geq N$, et $N'>0$ tel que
	\begin{equation}
		\| x_n-\ell' \|<\epsilon
	\end{equation}
	pour tout $n>N'$. Maintenant, nous prenons $n$ plus grand que $N$ et $N'$ de telle façon que les deux équations pour $x_n$ soient vérifiées en même temps. Alors
	\begin{equation}
		\| \ell-\ell' \|=\| \ell-x_n+x_n-\ell' \|\leq\| \ell-x_n \|+\| x_n-\ell' \|<2\varepsilon.
	\end{equation}
	Cela prouve que $\| \ell-\ell' \|=0$.
\end{proof}
Le théorème de Bolzano-Weierstrass~\ref{ThoBWFTXAZNH} dit que dans le cas métrique, la compacité séquentielle est équivalente à la compacité.

%TODO : le théorème sur l'équivalence des normes sur les espaces vectoriels normés devrait être énoncé comme le fait que si N1 et N2 sont deux normes sur V, alors
%       nous avons un isomorphisme d'espace topologique (V,N1) ~ (V,N2). L'isomorphisme étant donné par l'identité.
