% This is part of Mes notes de mathématique
% Copyright (c) 2011-2013,2016-2020,2022-2023, 2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%-------------------------------------------------------
\subsection{Groupe topologique}
%----------------------------------------------------


\begin{definition}      \label{DEFooCHZVooHnvSgW}
	Un \defe{groupe topologique}{groupe topologique} est un groupe \( G\) muni d'une topologie telle que les applications \( (g,h)\mapsto gh\) et \( g\mapsto g^{-1}\) sont continues.
\end{definition}

\begin{proposition}[\cite{MonCerveau,BIBooARJKooLuqoxW}]	\label{PROPooYNAXooWlUwfJ}
	Soit un ouvert \( S\) d'un groupe topologique\footnote{Définition \ref{DEFooCHZVooHnvSgW}.} \( G\). Il existe \( W\subset G\) tel que
	\begin{enumerate}
		\item
		      \( W\) est un voisinage ouvert de \( e\).
		\item
		      \( W\subset S\)
		\item
		      \( W=W^{-1}\)
		\item
		      \( WW\subset S\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Construction de \( W_1\)]
		%-----------------------------------------------------------
		Vu que \( S\) est ouvert et que l'inversion \(i \colon G\to G  \) est continue, la partie \( i^{-1}(S)=i(S)\) est ouverte. Nous posons \( W=i(S)\cap S\).
		\begin{enumerate}
			\item
			      \( W_1\) est ouvert en tant qu'intersection d'ouverts.
			\item
			      \( W_1=W_1^{-1}\) par le lemme \ref{LEMooQYETooZsTFMg}.
			\item
			      \( e\in W_1\) et donc \( W_1\) est un voisinage ouvert de \( e\).
		\end{enumerate}
	\end{subproof}

	\spitem[Construction de \( W_2\)]
	%-----------------------------------------------------------
	La multiplication \(m \colon G\times G\to  G  \) est continue. Nous posons \( U=m^{-1}(S)\). C'est un ouvert contenant \( (e,e)\). En vertu de la définition de la topologie produit \ref{DefIINHooAAjTdY}, il existe des voisinages ouverts \( A,B\subset G\) de \( e\) tels que \( A\times B\subset U\). Nous posons \( A'=A\cap B\) et nous avons encore \( A'\times A'\subset U\). Nous posons alors \( W_2=A'\cap i(A')\) de telle sorte à avoir
	\begin{equation}
		W_2\times W_2\subset A'\times A'\subset U.
	\end{equation}
	La partie \( W_2\) vérifie
	\begin{enumerate}
		\item
		      \( W_2=W_2^{-1}\)
		\item
		      \( W_2W_2\subset S\) parce que \( W_2\times W_2\subset U=m^{-1}(S)\).
	\end{enumerate}

	\spitem[Définition de \( W\)]
	%-----------------------------------------------------------
	Nous posons \( W=W_1\cap W_2\).
	\spitem[Vérification des propriétés]
	%-----------------------------------------------------------
	Vérifions que \( W\) vérifie les propriétés demandées.
	\begin{subproof}
		\spitem[\( W=W^{-1}\)]
		%-----------------------------------------------------------
		Nous avons :
		\begin{subequations}
			\begin{align}
				x & \in W\Rightarrow x\in W_1\Rightarrow x^{-1}\in W_1  \\
				x & \in W\Rightarrow x\in W_2\Rightarrow x^{-1}\in W_2,
			\end{align}
		\end{subequations}
		et donc donc si \( x\in W\) alors \( x^{-1}\in W_1\cap W_2=W\).
		\spitem[\( W\subset S\)]
		%-----------------------------------------------------------
		Parce que \( W\subset W_1 = S\cap i(S)\subset S\).
		\spitem[\( W\) est ouvert]
		%-----------------------------------------------------------
		Comme intersection d'ouverts.

		\spitem[\( WW\subset S\)]
		%-----------------------------------------------------------
		Soient \( x,y\in W\). Vu que \( x,y\in W_2\) nous avons
		\begin{equation}
			(x,y)\in W_2\times W_2\subset A'\times A'\subset U=m^{-1}(S)
		\end{equation}
		Donc \( xy=m(x,y)\in S\).
	\end{subproof}
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Action de groupe et connexité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Groupe agissant continûment\cite{BIBooXGHGooNQUvHR}]	\label{DEFooFRFGooPkzoyV}
	Si \(\phi \colon G\times X\to X  \) est une action du groupe\footnote{Définition \ref{DefActionGroupe}.} topologique \( G\) sur l'espace topologique \( X\), nous disons que \( G\) \defe{agit continument}{action continue} sur \( X\) lorsque \( \phi\) est une application continue pour la topologie produit\footnote{Définition \ref{DEFooJHXKooQApPtQ}.} sur \( G\times E\).
\end{definition}


\begin{proposition}[\cite{BIBooXGHGooNQUvHR}]	\label{PROPooVWHJooXaMvTj}
	Soit un groupe \( G\) agissant sur l'ensemble \( E\). Pour chaque \( x\in E\) nous considérons l'application
	\begin{equation}
		\begin{aligned}
			\phi_x\colon G & \to \Orb(x)       \\
			g              & \mapsto g\cdot x.
		\end{aligned}
	\end{equation}
	Alors il existe une unique application \( \tilde \phi_x \colon G/\Stab(x)\to \Orb(x) \) telle que
	\begin{equation}
		\phi_x=\tilde \phi_x\circ\pi.
	\end{equation}
	Cette application \( \tilde \phi_x\) est une bijection et
	\begin{equation}		\label{EQooIOGMooEjMaCP}
		\tilde \phi_x^{-1}=\pi\circ\phi_x^{-1}.
	\end{equation}
\end{proposition}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Existence et unicité]
		%-----------------------------------------------------------
		Nous montrons que si \( g\sim h\) alors \( \phi_x(g)=\phi_x(h)\). La proposition \ref{PROPooCONJooTpJwBe} donnera l'existence et l'unicité. Soient \( g,h\in G\). La condition \( g\sim h\) signifie qu'il existe \( z\in\Stab(x)\) tel que \( g=hz\) (c'est la définition du quotient \( G/\Stab(x)\)). Nous avons alors
		\begin{equation}
			\phi_x(g)=\phi_x(hz)=(hz)\cdot x=h\cdot(z\cdot x)=h\cdot h=\phi_x(h).
		\end{equation}
		Cela montre l'existence et l'unicité.

		\spitem[Surjective]
		%-----------------------------------------------------------
		Soit \( y\in \Orb(x)\). Il existe \( g\in G\) tel que \( y=g\cdot x\). Nous avons alors
		\begin{equation}
			y=\phi_x(g)=\tilde \phi_x([g]).
		\end{equation}

		\spitem[Injective]
		%-----------------------------------------------------------
		Soient \( g,h\in G\) tels que \( \tilde \phi_x([g])=\tilde \phi_x([h])\). Nous avons donc \( \phi_x(g)=\phi_x(h)\) et donc \( g\cdot x=h\cdot x\). En agissant avec \( g^{-1}\) des deux côtés nous trouvons
		\begin{equation}
			x=(g^{-1}h)\cdot x.
		\end{equation}
		Autrement dit \( g^{-1}h\in \Stab(x)\). Nous avons \(h=g(g^{-1} h)\in g\Stab(x)\). Donc \( h\sim g\).

		\spitem[Inverse]
		%-----------------------------------------------------------
		Il nous reste à prouver la formule \eqref{EQooIOGMooEjMaCP}. Soit \( y\in\Orb(x)\). Nous avons
		\begin{equation}
			\phi_x^{-1}(y)=\{ g\in G\tq g\cdot x=y \},
		\end{equation}
		et donc
		\begin{subequations}
			\begin{align}
				(\pi\circ\phi_x^{-1})(y) & =\{ [g]\in G/\Stab(x)\tq g\cdot x=y \}                \\
				                         & =\{ [g]\in G/\Stab(x)\tq  \tilde \phi_x([g])  =y \}   \\
				                         & =\{ \alpha\in G/\Stab(x)\tq\tilde \phi_x(\alpha)=y \} \\
				                         & =\tilde \phi_x^{-1}(y).
			\end{align}
		\end{subequations}
	\end{subproof}
\end{proof}


\begin{theorem}[\cite{BIBooXGHGooNQUvHR}]     \label{ThojrLKZk}
	Si \( G\) est un groupe topologique compact agissant continûment\footnote{Définitions \ref{DefActionGroupe} et \ref{DEFooFRFGooPkzoyV}.} sur l'espace topologique séparé \( X\), alors pour tout \( x\in X\), l'application
	\begin{equation}
		\begin{aligned}
			\tilde \phi_x\colon G/\Stab(x) & \to X            \\
			[g]                            & \mapsto g\cdot x
		\end{aligned}
	\end{equation}
	définie en \ref{PROPooVWHJooXaMvTj} est un homéomorphisme\footnote{Isomorphisme d'espaces topologiques, définition \ref{DEFooYPGQooMAObTO}.}.
\end{theorem}

\begin{proof}
	En vertu de la proposition \ref{PROPooYKLBooQuqnfA}, l'application \( \tilde \phi_x\) est continue parce que \( \tilde \phi_x\circ\pi=\phi_x\) est continue.

	Nous devons maintenant prouver que \( \tilde \phi_x^{-1}\) est continue. Nous utilisons pour cela la proposition \ref{LEMooATLRooEKnlro}. Par définition de la topologie quotient (définition \ref{DEFooHWSYooZZLXQU}), la projection canonique est continue. Étant donné que \( G\) est compact, l'espace \( G/\Stab(x)=\pi(G)\) est compact (théorème \ref{ThoImCompCotComp}).

	Soit un fermé \( F\) dans \( G/\Stab(x)\). La partie \( F\) est compacte par le lemme \ref{LemnAeACf}\ref{ITEMooNKAKooQoNddr}. Donc \( \tilde \phi_x(F)\) est compacte dans \( X\). Donc \( \tilde \phi_x(F)\) est fermé par le lemme \ref{LemnAeACf}\ref{ITEMooAZWVooLyPDeY}.
\end{proof}

\begin{lemma}       \label{LemkLRAet}
	Si \( G\) et \( H\) sont des groupes topologiques tels que \( G/H\) et \( H\) sont connexes\footnote{Définition~\ref{DefIRKNooJJlmiD}.}, alors \( G\) est connexe.
\end{lemma}

\begin{proof}
	Soit \( f\colon G\to \{ 0,1 \}\) une fonction continue. Considérons l'application
	\begin{equation}
		\begin{aligned}
			\tilde f\colon G/H & \to \{ 0,1 \} \\
			[g]                & \mapsto f(g).
		\end{aligned}
	\end{equation}
	D'abord nous montrons qu'elle est bien définie. En effet si \( h\in H\) nous aurions \( \tilde f([gh])=f(gh)\), mais étant donné que \( H\) est connexe, l'ensemble \( gH\) est également connexe; la fonction continue \( f\) est donc constante sur \( gH\). Nous avons donc \( f(gh)=f(g)\).

	Étant donné que \( G/H\) est également connexe, la fonction \( \tilde f\) doit être constante. Si \( g_1\) et \( g_2\) sont deux éléments du groupe, nous avons \( f(g_1)=\tilde f([g_1])=\tilde f([g_2])=f(g_2)\). Nous en déduisons que \( f\) est constante et que \( G\) est connexe.
\end{proof}

\begin{normaltext}
	La connexité de \( \SO(3)\) peut être démontrée en suivant les lignes de \cite{BIBooTMGMooRAJMcy}. Le corolaire \ref{CORooKKNWooWEQukb} permet de dire que les éléments de \( \SO(3)\) ont une valeur propre égale à \( 1\).
\end{normaltext}

\begin{theorem}     \label{THOooYQFNooPaYmaP}
	Pour tout \( n\geq 2\), le groupe \( \SO(n)\) est connexe, le groupe \( \gO(n)\) a deux composantes connexes.
\end{theorem}

\begin{proof}
	La seconde assertion découle de la première parce que les matrices de déterminant \( 1\) et celles de déterminant \( -1\) ne peuvent pas être reliées par un chemin continu tandis que l'application
	\begin{equation}
		M\mapsto \begin{pmatrix}
			-1 &   &   \\
			   & 1 &   \\
			   &   & 1
		\end{pmatrix}M
	\end{equation}
	est un homéomorphisme entre les matrices de déterminant \( 1\) et celles de déterminants \( -1\). Montrons donc que \( G=\SO(n)\) est connexe par arcs pour \( n\geq 2\) en procédant par récurrence sur la dimension.

	Nous acceptons le résultat pour \( G=\SO(2)\). Notons que nous en avons besoin pour prouver que la sphère \( S^{n-1}\) est connexe.

	Le groupe \( \SO(n)\) agit, par définition, de façon transitive sur la sphère \( S^{n-1}\). Soit \( a\in S^{n-1}\), nous avons
	\begin{subequations}
		\begin{align}
			G\cdot a & =S^{n-1}        \\
			G_a      & \simeq \SO(n-1)
		\end{align}
	\end{subequations}
	où \( G_a\) est le fixateur de \( a\) dans \( G\). Pour montrer le second point, nous considérons \( \{ e_i \}\), la base canonique de \( \eR^n\) et \( M\in G\) telle que \( Ma=e_1\). Le fixateur de \( e_1\) est évidemment isomorphe à \( \SO(n-1)\) parce qu'il est constitué des matrices de la forme
	\begin{equation}
		\begin{pmatrix}
			1      & 0         & \ldots & 0           \\
			0      & a_{11}    & \ldots & a_{1,n-1}   \\
			\vdots & \vdots    & \ddots & \vdots      \\
			0      & a_{n-1,1} & \ldots & a_{n-1,n-1}
		\end{pmatrix}
	\end{equation}
	où \( (a_{ij})\in \SO(n-1)\). L'application
	\begin{equation}
		\begin{aligned}
			\alpha\colon G_{e_1} & \to G_{a}         \\
			A                    & \mapsto M^{-1}A M
		\end{aligned}
	\end{equation}
	est un isomorphisme entre \( G_a\) et \( \SO(n-1)\). Le théorème~\ref{ThojrLKZk} nous montre alors que, en tant qu'espaces topologiques,
	\begin{equation}
		G/G_a=S^{n-1}.
	\end{equation}
	L'hypothèse de récurrence montre que \( G_a=\SO(n-1)\) est connexe tandis que nous savons que \( S^{n-1}\) est connexe. Le lemme~\ref{LemkLRAet} conclut que \( G=\SO(n)\) est connexe.
\end{proof}

\begin{lemma}       \label{LemIbrsFT}
	Une bijection continue entre un espace compact et un espace séparé est un homéomorphisme.
\end{lemma}

\begin{proposition}     \label{PROPooTVHJooBRmUCd}
	Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes.
\end{proposition}

\begin{proof}
	Soit \( G(n)\) le groupe \( \SU(n)\) ou \( \gU(n)\). Ce groupe opère transitivement sur la sphère complexe
	\begin{equation}
		S_{\eC}^{n-1}=\{ z\in \eC^n\tq | z |=1 \}.
	\end{equation}
	En notons, pour chaque \( k\), \( z_k=x_k+iy_k\), nous considérons l'application continue
	\begin{equation}
		\begin{aligned}
			s\colon S_{\eC}^{n-1} & \to S^{2n-1}                       \\
			(z_1,\ldots, z_n)     & \mapsto (x_1,y_1,\ldots, x_n,y_n).
		\end{aligned}
	\end{equation}
	Vérifions que \( s\) prend bien ses valeurs dans \( S^{2n-1}\). Nous avons
	\begin{equation}
		| s(z_1,\ldots, z_n) |=\sqrt{ \sum_k| x_k |^2+| y_k |^2 }=\sqrt{ \sum_k| z_k |^2 }=1
	\end{equation}
	parce que \( | z_k |^2=| x_k |^2+| y_k |^2\). Le lemme \ref{LemIbrsFT} dit que cette bijection continue est un homéomorphisme. Soit \( a\in S^{n-1}_{\eC}\), nous avons
	\begin{subequations}
		\begin{align}
			G\cdot a & =S^{n-1}_{\eC} \\
			G_a      & \simeq G(n-1).
		\end{align}
	\end{subequations}
	La seconde ligne est un isomorphisme de groupe et un homéomorphisme. Il est donné de la façon suivante. D'abord le fixateur de \( e_1\) dans \( G(n)\) est donné par les matrices de la forme
	\begin{equation}
		\begin{pmatrix}
			1      & 0         & \ldots & 0           \\
			0      & a_{11}    & \ldots & a_{1,n-1}   \\
			\vdots & \vdots    & \ddots & \vdots      \\
			0      & a_{n-1,1} & \ldots & a_{n-1,n-1}
		\end{pmatrix}
	\end{equation}
	où \( (a_{ij})\in G(n-1)\). Par ailleurs si \( M\) est une matrice de \( G(n)\) telle que \( Ma=e_1\), nous avons l'homéomorphisme
	\begin{equation}
		\begin{aligned}
			\alpha\colon G_{e_1} & \to G_a            \\
			A                    & \mapsto M^{-1} AM.
		\end{aligned}
	\end{equation}
	Encore une fois, cela est un homéomorphisme par le lemme~\ref{LemIbrsFT}. Par composition nous avons \( G_a\simeq G(n-1)\) et un homéomorphisme
	\begin{equation}
		G(n)/G_a=S^{n-1}_{\eC}.
	\end{equation}
	Le groupe \( G_a\) et l'ensemble \( S^{n-1}_{\eC}\) étant connexes, le groupe \( G(n)\) est connexe par le lemme~\ref{LemkLRAet}.
\end{proof}

\begin{lemma}[\cite{PAXrsMn}]
	Si \( G\) est un sous-groupe connexe de \( \GL(n,\eC)\) alors son groupe dérivé\footnote{Définition~\ref{DEFooBNLPooShKYXa}.} l'est également.
\end{lemma}
\index{groupe dérivé!de \( \GL(n,\eC)\)}

\begin{proof}
	Soit \( S_m\) l'ensemble des produits de \( m\) commutateurs de \( G\) :
	\begin{equation}
		S_m=\{ g_1,\ldots, g_m\,\text{où les } g_i\text{ sont des commutateurs} \}.
	\end{equation}
	La partie \( S_m\) est l'image de \( G\) par l'application continue
	\begin{equation}
		\begin{aligned}
			\underbrace{G\times \ldots\times G}_{ 2m\text{ facteurs}} & \to G                             \\
			(g_1,h_1,g_2,h_2,\ldots, g_m,h_m)                         & \mapsto [g_1,h_1]\ldots [g_m,h_m]
		\end{aligned}
	\end{equation}
	En tant qu'image d'un connexe par une application continue, \( S_m\) est connexe par le lemme \ref{LemConncontconn}. Puisque les \( S_m\) ont l'identité en commun, le groupe dérivé
	\begin{equation}
		D(G)=\bigcup_{m=1}^{\infty}S_m
	\end{equation}
	est également connexe.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dilatations et transvections}
%---------------------------------------------------------------------------------------------------------------------------

Soit un corps commutatif \( \eK\) et \( n\geq 2\).

\begin{theoremDef}[\cite{PAXrsMn}]     \label{ThoooAZKDooNDcznv}
	Soit une application linéaire \( u\colon E\to E\) dont les points fixes forment un hyperplan noté \( H\) d'équation \( H=\ker(f)\) avec \( f\in E^*\).
	\begin{enumerate}
		\item     \label{ITEMooGTKRooQSPNoI}
		      Les affirmations suivantes sont équivalentes :
		      \begin{enumerate}
			      \item  \label{ITEMooZHYRooFGKaifi}
			            \( \det(u)\neq 1\)
			      \item       \label{ooXKLWooTfUMzV}
			            L'application \( u\) est diagonalisable et a une valeur propre qui vaut \( \det(u)\neq 1\).
			      \item       \label{ooMZPTooCLylbh}
			            \( \Image(u-\id)\nsubseteq H\).
			      \item   \label{ITEMooZHYRooFGKaifiv}
			            Il existe une base de \( E\) dans laquelle la matrice de \( u\) est \( \diag(1,\ldots, 1,\lambda)\) avec \( \lambda\neq 1\).
		      \end{enumerate}
		\item       \label{ITEMooMSJXooUsLCHx}
		      Les affirmations suivantes sont équivalentes :
		      \let\oldthenumii\theenumi
		      \renewcommand{\theenumii}{\roman{enumii}}
		      \begin{enumerate}
			      \item       \label{ITEMooRTIEooOoWCFsa}
			            Il existe \( a\in H\) tel que pour tout \( x\in E\), \( u(x)=x+f(x)a\).
			      \item       \label{ITEMooRTIEooOoWCFsb}
			            Dans une base adaptée, la matrice de \( u\) est donnée par
			            \begin{equation}        \label{EQooFXBDooTgZwMv}
				            \begin{pmatrix}
					            1 &        &   &   \\
					              & \ddots &   &   \\
					              &        & 1 & 1 \\
					              &        &   & 1
				            \end{pmatrix}.
			            \end{equation}
		      \end{enumerate}
		      \let\theenumii\oldtheenumii
		\item
		      Les conditions~\ref{ITEMooZHYRooFGKaifi}-\ref{ITEMooZHYRooFGKaifiv} sont respectées si et seulement si les conditions~\ref{ITEMooRTIEooOoWCFsa}-\ref{ITEMooRTIEooOoWCFsb} ne sont pas respectées (elles sont les négations l'une de l'autre.).
	\end{enumerate}
	Une \defe{dilatation}{dilatation} est soit l'identité soit un endomorphisme qui respecte les conditions \ref{ITEMooGTKRooQSPNoI}.

	Une \defe{transvection}{transvection} est soit l'identité soit un endomorphisme qui vérifie les conditions~\ref{ITEMooMSJXooUsLCHx}.

\end{theoremDef}



\begin{proof}
	Nous allons prouver plein d'implications \ldots
	\begin{subproof}
		\spitem[\ref{ITEMooZHYRooFGKaifi} implique~\ref{ooXKLWooTfUMzV}]
		Le théorème de la base incomplète (voir remarque~\ref{REMooYGJEooEcZQKa}) permet de considérer une base \( \{ e_1,\ldots, e_n \}\) de \( E\) telle que \( \{ e_1,\ldots, e_{n-1} \} \) soit une base de \( H\). Dans cette base, la matrice de \( u\) est de la forme suivante (les cases non remplies sont nulles et les étoiles correspondent à des valeurs inconnues mais pas spécialement nulles) :
		\begin{equation}        \label{EqooPQOEooGUyIwa}
			\begin{pmatrix}
				1 &        &   & *       \\
				  & \ddots &   & \vdots  \\
				  &        & 1 & *       \\
				  &        &   & \lambda
			\end{pmatrix}
		\end{equation}
		Le fait que le déterminant de \( u\) ne soit pas \( 1\) implique que \( \lambda\neq 1\). Par conséquent le polynôme caractéristique
		\begin{equation}
			\chi_u(X)=(1-X)^{n-1}(\lambda-X)
		\end{equation}
		possède une racine \( \lambda\neq 1\), et donc \( u\) possède un vecteur propre \( v\) pour cette valeur\footnote{Proposition~\ref{PropooBYZCooBmYLSc}.}. Le vecteur \( v\) est linéairement indépendant de \( \{ e_1,\ldots, e_{n-1} \}\) (parce que vecteur propre de valeur propre différente). Par conséquent l'ensemble \( \{ e_1,\ldots, e_{n-1},v \}\) est une base par la proposition \ref{PROPooVEVCooHkrldw}. C'est une base de vecteurs propres et donc une base de diagonalisation\footnote{Nous pourrions en dire à peine un peu plus et prouver le point~\ref{ITEMooZHYRooFGKaifiv}, mais cela ne servirait à rien parce que nous voulons prouver les équivalences et qu'il faudra quand même prouver que~\ref{ooMZPTooCLylbh} implique~\ref{ITEMooZHYRooFGKaifiv}.}.
		\spitem[\ref{ooXKLWooTfUMzV} implique~\ref{ooMZPTooCLylbh}]
		Nous nommons maintenant \( \{ e_1,\ldots, e_{n} \}\) la base de diagonalisation. Nous avons \( u(e_n)=\lambda e_n\) avec \( \det(u)=\lambda\neq 1\). Nous avons
		\begin{equation}
			(u-\id)(e_n)=(\lambda-1)e_n\notin H,
		\end{equation}
		ce qui prouve que l'image de \( e_n\) par \( u-\id\) n'est pas dans \( H\).
		\spitem[\ref{ooMZPTooCLylbh} implique~\ref{ITEMooZHYRooFGKaifiv}]
		Reprenons une base \( \{ e_1,\ldots, ,e_n \}\) donnant la matrice \eqref{EqooPQOEooGUyIwa}. Il existe \( x\in E\) tel que \( u(x)-x\) n'est pas dans \( H\), c'est-à-dire tel que \( u\big( u(x)-x \big)\neq u(x)-x\). Nous en déduisons que
		\begin{equation}
			u^2(x)-2u(x)+x\neq 0
		\end{equation}
		ou encore que
		\begin{equation}
			(X-1)^2(u)x\neq 0.
		\end{equation}
		C'est-à-dire que \( (X-1)^2\) n'est pas un polynôme annulateur de \( u\). Or ce serait le cas si \( X-1\) était le polynôme minimal (proposition~\ref{PropAnnncEcCxj}). Le polynôme caractéristique étant \( (X-1)^{n-1}(X-\lambda)\) (et étant annulateur\footnote{Théorème de Cayley-Hamilton~\ref{ThoCalYWLbJQ}.}), le polynôme minimal est de la forme
		\begin{equation}
			\mu_u(X)=\begin{cases}
				(X-1)(X-\lambda) & \text{si } \lambda\neq 1 \\
				X-1              & \text{si } \lambda=1.
			\end{cases}
		\end{equation}
		Dans notre cas nous venons de voir que ce n'est pas \( X-1\) et donc c'est \( (X-1)(X-\lambda)\) avec \( \lambda\neq 1\).

		Nous devons trouver une base de diagonalisation \ldots{} Supposons
		\begin{equation}
			u(e_n)=\sum_{k=1}^{n-1}a_ke_k+\lambda e_n,
		\end{equation}
		dans lequel nous venons de prouver que \( \lambda\neq 1\), et cherchons
		\begin{equation}
			e'_n=\sum_{j=1}^np_je_j
		\end{equation}
		de telle sorte à avoir \( u(e'_n)=\lambda e'_n\). Nous avons
		\begin{equation}
			u(e'_n)=\sum_{j=1}^{n-1}p_ju(e_j)+p_nu(e_n) =\sum_{j=1}^{n-1}(p_j+p_na_j)e_j+p_n\lambda e_n.
		\end{equation}
		En égalisant à \( \lambda\sum_{j=1}^np_je_j\), il vient
		\begin{equation}
			p_j+p_na_j=\lambda p_j
		\end{equation}
		pour tout \( j=1,\ldots, n-1\) et la condition triviale \( p_n\lambda=\lambda p_n\) pour \( j=n\). Nous en déduisons que le choix
		\begin{equation}
			p_j=\frac{ p_na_j }{ \lambda-1 }
		\end{equation}
		fonctionne (parce que \( \lambda\neq 1\) comme nous l'avons démontré plus haut). En bref, il suffit de poser
		\begin{equation}
			e'_n=\sum_{j=1}^{n-1}\frac{ p_na_j }{ \lambda-1 }e_j+p_ne_n
		\end{equation}
		avec \( p_n\) au choix pour avoir une base \( \{ e_1,\ldots, e_{n-1},e'_n \}\) de diagonalisation de \( u\) avec \( \lambda\neq 1\) comme dernière valeur propre.
		\spitem[\ref{ITEMooZHYRooFGKaifiv} implique~\ref{ITEMooZHYRooFGKaifi}] Évident \ldots{} encore faut-il se souvenir d'invoquer l'invariance du déterminant par changement de base.
	\end{subproof}
	Nous avons terminé la première série d'équivalences. Nous continuons avec la seconde.
	\begin{subproof}
		\spitem[\ref{ITEMooRTIEooOoWCFsa} implique~\ref{ITEMooRTIEooOoWCFsb}]
		Nous prenons \( e_{n-1}=a\) et nous complétons en une base de \( H\). Pour \( e_n\) il suffit de prendre n'importe quel vecteur \( v\) tel que \( f(v)\neq 0\) (qui existe parce que \( f=0\) est seulement un hyperplan), et de le normaliser.

		Dans cette base, la matrice de \( u\) a la forme désirée parce que \( u(e_n)=e_n+f(e_n)a=e_n+e_{n-1}\) du fait que \( e_{n-1}=a\) et \( f(e_n)=1\).
		\spitem[\ref{ITEMooRTIEooOoWCFsb} implique~\ref{ITEMooRTIEooOoWCFsa}]
		Soit \( \{ e_1,\ldots, e_n \}\) cette base. En prenant \( a=e_{n-1}\) et en posant \( x=\sum_kx_ke_k\) nous avons
		\begin{equation}
			u(x)=\sum_{k=1}^{n-1}x_ke_k+x_n(e_{n-1}+e_n)=x+x_ne_{n-1}=x+x_na.
		\end{equation}
		Mais puisque \( f(x)=\sum_if_ix_i\), et que \( f(e_i)=0\) pour tout \( i=1,\ldots, n-1\) nous avons \( f(x)=f_nx_n\). Il n'y a cependant pas de raison d'avoir \( f_n=1\). Mais en définissant
		\begin{equation}
			e'_i=\frac{1}{ f_n }e_i
		\end{equation}
		nous avons bien \( u(e'_n)=\frac{1}{ f_n }(e_{n-1}+e_n)=e'_{n-1}+e'_n\). Donc dans cette base nous avons encore la matrice de \( u\) de la forme
		\begin{equation}
			\begin{pmatrix}
				1 &        &   &   \\
				  & \ddots &   &   \\
				  &        & 1 & 1 \\
				  &        &   & 1
			\end{pmatrix},
		\end{equation}
		mais cette fois avec \( f(e'_n)=1\).
	\end{subproof}
	Nous avons terminé avec la seconde série d'équivalences. Il nous reste à prouver que la première est équivalente à la négation de la seconde.
	\begin{subproof}
		\spitem[non~\ref{ooMZPTooCLylbh} implique~\ref{ITEMooRTIEooOoWCFsa}]
		Considérons \( x_0\in E\) tel que \( f(x_0)=1\) et posons \( a=u(x_0)-x_0\in\Image(u-\id)\). Par la négation de~\ref{ooMZPTooCLylbh} nous avons \( a\in H\). De plus \( x_0\notin H\) (sinon \( f(x_0)=0\)) donc \( u(x_0)\neq x_0\) et \( a\neq 0\).

		Nous montrons que ce choix de \( a\) fonctionne : \( u(x)=x+f(x)a\) pour tout \( x\in E\). Nous faisons cela séparément pour \( x\in H\) et pour \( x=x_0\).

		Si \( h\in H\) alors \( u(h)=h\) et \( f(h)=0\) donc \( h+f(h)a=h=u(h)\). Si \( x=x_0\) alors \( u(x_0)=a+x_0\) (c'est la définition de \( a\)) et \( x_0+f(x_0)a=x_0+a\).
		\spitem[\ref{ITEMooRTIEooOoWCFsb} implique non~\ref{ITEMooZHYRooFGKaifi}]
		Dans une base adaptée nous avons
		\begin{equation}
			\begin{pmatrix}
				1 &        &   &   \\
				  & \ddots &   &   \\
				  &        & 1 & 1 \\
				  &        &   & 1
			\end{pmatrix},
		\end{equation}
		et donc \( \det(u)=1\), ce qui contredit~\ref{ITEMooZHYRooFGKaifi}.
	\end{subproof}
\end{proof}


\begin{normaltext}
	Dans le cas des dilatations et des transvections, les points fixes forment un hyperplan.

	Selon cette terminologie, l'application \( x\mapsto \lambda x\) n'est pas une dilatation mais un produit de dilatations.
\end{normaltext}

\begin{proposition}[\cite{BIBChatGPT, MonCerveau, BIooGNKFooTYlRlt}]	\label{PROPooNCKGooTuaYeq}
	Les transvections de \( \GL(n,\eK)\) sont dans \( \SL(n,\eK)\), c'est-à-dire qu'elles ont un déterminant égal à \( 1\).
\end{proposition}

\begin{proof}
	Si vous voulez des matrices, utilisez la définition \ref{LEMooQTRVooAKzucd} et la forme \eqref{EQooFXBDooTgZwMv}. Sous cette forme, le déterminant est clairement \( 1\).

	Soit une transvection \( u(x)=x+f(x)a\) avec \( H=\{ f= \}\) et \( a\in H\). En utilisant le lemme des déterminants \ref{LEMooJVDHooONXNAh}\ref{ITEMooGJOZooVLfZcX}, nous avons \( \det(u)=1+f(a)=1\) parce que \( f(a)=0\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooYTUSooKqDJce}
	Quelques trucs à propos des transvections\footnote{Définition \ref{ThoooAZKDooNDcznv}.}.
	\begin{enumerate}
		\item		\label{ITEMooOGTOooVGYOUA}
		      Si \( \tau\) est une transvection, alors \( \tau^2\) est une transvection.
		\item		\label{ITEMooCSBVooTWYMvd}
		      Les transvections de \( \GL(n,\eK)\) sont dans \( \SL(n,\eK)\)
		\item		\label{ITEMooYQKNooYCbpBP}
		      Toutes les transvections sont conjuguées dans \( \SL(n,\eK)\), c'est-à-dire que si \( \tau_1\) et \( \tau_2\) sont des transvections, alors il existe \( g\in \SL(n,\eK)\) tel que \( \tau_1=g\tau_2g^{-1}\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Plusieurs points.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooOGTOooVGYOUA}]
		%-----------------------------------------------------------
		Nous considérons une base \( \{ e_i \}_{i=1,\ldots,n}\) adaptée à \( \tau\) et par facilité de notations, nous supposons \( \tau(e_1)=e_1\) et \( \tau(e_2)=e_1+e_2\). En posant
		\begin{subequations}
			\begin{numcases}{}
				f_1=ae_1+be_2\\
				f_2=ce_1+de_2,
			\end{numcases}
		\end{subequations}
		et en imposant \( \tau^2(f_1)=f_1\), \( \tau^2(f_2)=f_1+f_2\), il est vite vu que ça marche avec \( a=2\), \( b=0\), \( c=1\) et \( d=1\). Bref, en posant \( f_1=2e_1\) et \( f_2=e_1+e_2\) nous avons bien \( \tau^2(f_1)=f_1\) et \( \tau^2(f_2)=f_1+f_2\), c'est-à-dire que \( \{ f_1,f_2,e_3,\ldots,e_n \}\) est une base adaptée pour \( \tau^2\).

		\spitem[Pour \ref{ITEMooCSBVooTWYMvd}]
		%-----------------------------------------------------------
		C'est la proposition \ref{PROPooNCKGooTuaYeq}.

		\spitem[Pour \ref{ITEMooYQKNooYCbpBP}]
		%-----------------------------------------------------------
		Soient deux transvections \( \tau_1\) et \( \tau_2\). Il existe des applications linéaire inversibles \( P\) et \( Q\) telle que \( A=P\tau_1P^{-1}\) et \( A=Q\tau_2Q^{-1}\) où \( A\) est l'application de la matrice \eqref{EQooFXBDooTgZwMv}. Nous avons alors
		\begin{equation}
			\tau_1=(P^{-1}Q)\tau_2(P^{-1}Q)^{-1}.
		\end{equation}
		Donc \( \tau_1\) et \( \tau_2\) sont conjugués dans \( \GL(n,\eK)\). Nous devons encore prouver qu'elles sont conjuguées dans \( \SL(n,\eK)\).

		Soit \( g\in\GL(n,\eK)\) tel que \( \tau_1=g\tau_2g^{-1}\). Si \( \det(g)=\lambda\), en posant \( h=\lambda^{-1}g\in\SL(n,\eK)\) nous avons \( \tau_1=h\tau_2h^{-1}\). Donc \( \tau_1\) et \( \tau_2\) sont conjuguées par \( h\in\SL(2,\eK)\).
	\end{subproof}
\end{proof}

\begin{remark}
	Nous notons \( E_{ij}\) la matrice qui possède uniquement \( 1\) en position \( (i,j)\). C'est-à-dire que \( \big( E_{ij} \big)_{kl}=\delta_{ik}\delta_{jl}\). Soit \( H\) l'hyperplan des points fixes de \( f\). Dans une base contenant une base de \( H\), la matrice d'une transvection a pour forme type :
	\begin{equation}        \label{EqooZAKHooBjKlTd}
		T_{ij}(\lambda)=\mtu+\lambda E_{ij}
	\end{equation}
	avec \( i\neq j\) et \( \lambda\in \eK\), et une dilatation a pour forme type la matrice diagonale
	\begin{equation}
		D_i(\alpha)=\mtu+(\alpha-1)E_{ii}
	\end{equation}
	avec \( \alpha\in \eK^*\).

	Bien entendu, en choisissant une base quelconque, les matrices des dilatations et des translations peuvent avoir des formes différentes.
\end{remark}

\begin{lemma}       \label{LemooTQJXooGoIxsI}
	Quelques manipulations de lignes et de colonnes pour les matrices.
	\begin{enumerate}
		\item       \label{ITEMooRWANooPAVjkm}
		      La multiplication à gauche par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de ligne
		      \begin{equation}
			      L_i\to L_i+\lambda L_j.
		      \end{equation}
		\item       \label{ITEMooHPSMooWBrSXP}
		      La multiplication à droite par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de colonne
		      \begin{equation}
			      C_j\to C_j+\lambda C_i.
		      \end{equation}
		\item       \label{ITEMooXUGFooKcbrxs}
		      La multiplication à gauche par \( T_{ij}(1)T_{ji}(-1)T_{ij}(1)\) revient à la substitution de lignes
		      \begin{subequations}
			      \begin{numcases}{}
				      L_i\to L_j\\
				      L_j\to -L_i.
			      \end{numcases}
		      \end{subequations}
	\end{enumerate}
\end{lemma}
Notons qu'il n'est pas possible d'inverser deux lignes à l'aide de transvections sans changer un signe parce que les transvections sont de déterminant \( 1\) alors que l'inversion de lignes change le signe du déterminant.

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[Pour~\ref{ITEMooRWANooPAVjkm}]
		Nous devons prouver que
		\begin{equation}
			\big( T_{ij}(\lambda)A \big)_{kl}=\begin{cases}
				A_{kl}                & \text{si } k\neq i \\
				A_{il}+\lambda A_{jl} & \text{si } k=i.
			\end{cases}
		\end{equation}
		Un peu de calcul matriciel avec utilisation modérée des indices donne :
		\begin{subequations}
			\begin{align}
				\big( T_{ij}(\lambda)A \big)_{kl} & =\sum_s\big( T_{ij}(\lambda) \big)_{ks}A_{sl}                \\
				                                  & =\sum_s\delta_{ks}A_{sl}+\lambda\delta_{ik}\delta_{js}A_{sl} \\
				                                  & =A_{kl}+\lambda\delta_{ik}A_{jl}.
			\end{align}
		\end{subequations}
		\spitem[Pour~\ref{ITEMooHPSMooWBrSXP}] C'est la même chose.
		\spitem[Pour~\ref{ITEMooXUGFooKcbrxs}] Si nous appliquons successivement ces trois matrices (de droite à gauche) nous effectuons les substitutions :
		\begin{equation}
			\begin{aligned}[]
				\begin{cases}
					L'_i=L_i+L_j \\
					L'_j=L_j
				\end{cases}
				\text{suivi de }
				\begin{cases}
					L''_i=L'_i \\
					L''_j=L'_j-L'_i
				\end{cases}
				\text{et de}
				\begin{cases}
					L'''_i=L''_i+L''_j \\
					L'''_j=L''_j.
				\end{cases}
			\end{aligned}
		\end{equation}
		En effectuant ces substitutions,
		\begin{equation}
			L'''_i=L''_i+L''_j=L'_i+(L'_j-L'_i)=L'_j=L_j
		\end{equation}
		et
		\begin{equation}
			L'''_j=L''_j=L'_j-L'_i=L_j-(L_i+L_j)=-L_i,
		\end{equation}
		ce qu'il fallait.
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{ooUWTWooPKySTQ}]      \label{PropooFDNRooWFfUDd}
	Soient \( n\geq 2\) et \( \eK\) un corps commutatif.
	\begin{enumerate}
		\item
		      Si \( A\in \GL(n,\eK)\), il existe des transvections \( U_1,\ldots, U_r\), \( V_1,\ldots, V_s\) telles que
		      \begin{equation}        \label{EQooKSQVooIpkdIE}
			      A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
		      \end{equation}
		\item       \label{ITEMooLRYXooSoKRiA}
		      L'ensemble des transvections engendre le groupe spécial linéaire \( \SL(n,\eK)\).
		\item
		      L'ensemble des transvections et des dilatations engendre le groupe linéaire \( \GL(n,\eK)\).
	\end{enumerate}
\end{proposition}


\begin{proof}
	Nous allons montrer que toutes les matrices de \( \SL(n,\eK)\) peuvent être écrites comme produits de matrices de la forme \eqref{EqooZAKHooBjKlTd}. Cela montrera qu'étant donné un endomorphisme \( f\) et une base \emph{pas spécialement liée à \( f\)}, il est possible d'écrire la matrice de \( f\) comme produit de transvections dont les hyperplans invariants sont «contenus» dans cette base. Cela suffit à prouver que les transvections engendrent \( \SL(n,\eK)\) grâce au lemme~\ref{LemFUIZooBZTCiy}.

	Toutes les transvections ont un déterminant égal à \( 1\). Donc le groupe engendré par les transvections est inclus dans \( \SL(2,\eK)\). Soit \( A\in\GL(n,\eK)\); nous allons utiliser le pivot de Gauss pour la diagonaliser. Étant donné que \( A\) est inversible, sa première colonne n'est pas nulle. Si \( A_{i1}\neq 0\) alors une multiplication à gauche par \( L_{1i}\big(   (A_{11}-1)/A_{i1}  \big)\) effectue la substitution
	\begin{equation}
		L_1\to L_1-\frac{ A_{11}-1 }{ A_{i1} }L_i
	\end{equation}
	qui met un \( 1\) en la position \( (1,1)\). Notons que si la première colonne est de la forme
	\begin{equation}
		\begin{pmatrix}
			s      \\
			0      \\
			\vdots \\
			0
		\end{pmatrix}
	\end{equation}
	avec \( s\neq 0\) alors il faut plutôt faire les substitutions \( L_2\to L_2+L_1\) et ensuite \( L_1\to L_1-\frac{1}{ s }L_2\) pour obtenir le même résultat. En effectuant le pivot avec \( A_{11}\), une suite d'opérations sur les lignes et les colonnes donnent
	\begin{equation}
		M_1\ldots M_pAN_1\ldots N_q=\begin{pmatrix}
			1 & 0   \\
			0 & A_1
		\end{pmatrix}
	\end{equation}
	où \( A_1\in\GL(n-1,\eK)\) et \( \det(A_1)=\det(A)\). En continuant de la sorte nous arrivons sur une matrice diagonale\footnote{Attention : les opérations sur les lignes et les colonnes ne sont pas des opérations de similitude. Il n'est pas question de prétendre ici que toutes les matrices de \( \GL(n,\eK)\) sont diagonales, voir la définition~\ref{DefBLELooTvlHoB}.}
	\begin{equation}
		M_1\ldots M_{p'}AN_1\ldots N_{q'}=
		\begin{pmatrix}
			1 &        &   &        \\
			  & \ddots &   &        \\
			  &        & 1 &        \\
			  &        &   & \alpha
		\end{pmatrix}
	\end{equation}
	avec \( \alpha=\det(A)\). En d'autres termes nous avons prouvé qu'il existe des transvections \( U_1,\ldots, U_r\) et \( V_1,\ldots, V_s\) telles que
	\begin{equation}        \label{EQooZYYFooQGCgxU}
		A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
	\end{equation}
	Cela prouve que les transvections et les translations engendrent \( \GL(n,\eK)\). Si \( A\in \SL(n,\eK)\) alors \( D_n\big( \det(A) \big)=1\) et l'équation \eqref{EQooZYYFooQGCgxU} est un produit de transvections.
\end{proof}

\begin{proposition}
	Le groupe \( \GL(n,\eR)\) est engendré par les endomorphismes inversibles diagonalisables.
\end{proposition}

\begin{proof}
	Par la proposition~\ref{PropooFDNRooWFfUDd}, le groupe \( \GL(n,\eR)\) est engendré par les dilatations et les transvections. Il suffit donc de montrer qu'à leur tour, ces deux types d'endomorphismes sont engendrés par les endomorphismes inversibles et diagonalisables.

	Les dilatations sont diagonalisables et inversibles. C'est bon pour elles.

	Soit une transvection \( u\), et une base \( \{ e_i \}_{i=1,\ldots, n}\) dans laquelle \( u\) est de la forme \eqref{EQooFXBDooTgZwMv}. Nous considérons l'endomorphisme \( d\colon E\to E\) défini par \( d(e_k)=ke_k\). Cet endomorphisme est diagonalisable parce que son polynôme minimal, \( \mu_d=\prod_{k=1}^n(X-k)\), est scindé à racines simples (voir le théorème~\ref{ThoDigLEQEXR}).

	Nous avons évidemment \( u= d^{-1}\circ(d\circ u) \) où \( d^{-1}\) est diagonalisable et inversible. Voyons que \( d\circ u\) est également diagonalisable en montrant que \( \mu_d\) est son polynôme minimal (qui est scindé à racines simples).

	Il suffit de montrer que \( \mu_d(d\circ u)(e_k)=0\) pour tout \( k\). Ainsi \( \mu_d\) sera un polynôme annulateur de \( d\circ u\) de degré \( n\), et donc minimal.
	\begin{subproof}
		\spitem[Si \( k\leq n-1\)]
		Alors \( u(e_k)=e_k\) et \( (d\circ u-n)e_k=(k-n)e_k\). Donc :
		\begin{equation}
			\mu_d(d\circ u)(e_k)=(d\circ u-1)(d\circ u-2)\ldots (d\circ u-n)e_k=(k-1)(k-2)\ldots (k-n)e_k=0
		\end{equation}
		parce que dans le produit des \( k-i\), il y en a forcément un de nul.
		\spitem[Si \( k=n\)]
		Dans un premier temps,
		\begin{equation}
			(d\circ u-n)e_n=d(e_n+e_{n-1})-ne_n=ne_n+(n-1)e_{n-1}-ne_n=(n-1)e_{n-1}.
		\end{equation}
		Ensuite
		\begin{subequations}
			\begin{align}
				\big( d\circ u-(n-1) \big)e_{n-1} & =d(e_{n-1})-(n-1)e_{n-1}   \\
				                                  & =d(e_{n-1})-(n-1)e_{n-1}   \\
				                                  & =(n-1)e_{n-1}-(n-1)e_{n-1} \\
				                                  & =0
			\end{align}
		\end{subequations}
	\end{subproof}
	Le polynôme \( \mu_d\) est donc un polynôme scindé à \( n\) racines simples annulateur de \( d\circ u\), qui est alors diagonalisable et inversible (parce que \( u\) et \( d\) le sont).

	Donc sous la forme \( u=d^{-1}(du)\), la transvection \( u\) est écrite comme produit de diagonalisables inversibles.
\end{proof}

\begin{proposition}[\cite{LoFdlw}]      \label{PROPooSAOTooIlpJoY}
	Soient \( n\geq 3\) et \( \eK\) un corps de caractéristique différente de \( 2\). Alors
	\begin{enumerate}
		\item
		      le groupe dérivé\footnote{Définition \ref{DEFooBNLPooShKYXa}.} de \( \GL(n,\eK)\) est \( \SL(n,\eK)\);  \index{groupe!dérivé!de \( \GL(n,\eK)\)}
		\item
		      le groupe dérivé de \( \SL(n,\eK)\) est \( \SL(n,\eK)\).  \index{groupe!dérivé!de \( \SL(n,\eK)\)}
	\end{enumerate}
\end{proposition}

\begin{proof}
	Soit une transvection \( \tau\). Dans une base adaptée nous avons \( \tau(e_1)=e_1+e_2\), \( \tau(e_2)=e_2\). Pour ces vecteurs nous avons \( \tau^2(e_1)=e_1+2e_2\). Étant donné que la caractéristique est différente de \( 2\) nous avons \( 2\neq 0\) et donc \( \tau^2(e_2)\neq e_2\). Tout ça pour dire que \( \tau^2\neq \id\).

	En utilisant le lemme \ref{LEMooYTUSooKqDJce} nous savons que \( \tau^2\) est une transvection et donc que \( \tau^2\) est conjugué à \( \tau\) dans \( \SL(n,\eK)\) : il existe \( g\in \SL(n,\eK)\) telle que \( \tau^2=g\tau g^{-1}\). Donc
	\begin{equation}
		\tau=g\tau g^{-1}\tau^{-1}\in D\big( \SL(n,\eK) \big).
	\end{equation}
	Nous avons donc montré que \( D\big( \SL(n,\eK) \big)\) contient toutes les transvections. À fortiori, \( D\big( \GL(n,\eK) \big)\) contient également toutes les transvections. Par \ref{PropooFDNRooWFfUDd}, nous avons alors
	\begin{subequations}
		\begin{align}
			\SL(n,\eK) & \subset D\big( \SL(n,\eK) \big) \\
			\SL(n,\eK) & \subset D\big( \GL(n,\eK) \big)
		\end{align}
	\end{subequations}
	Mais vu que le déterminant est multiplicatif\footnote{Par le lemme \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy}.}, nous avons aussi \( D\big( \SL(n,\eK) \big)\subset \SL(n,\eK)\), et \( D\big( \GL(n,\eK) \big)\subset\SL(n\eK)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Connexité  de certains groupes}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}           \label{LEMooIPOVooZJyNoH}
	Le groupe \( \gO(n,\eR)\) n'est pas connexe.
\end{lemma}

\begin{proof}
	La non connexité par arcs est facile parce que les éléments de déterminant \( 1\) ne peuvent pas être reliés aux éléments de déterminant \( -1\) par un chemin continu restant dans \( \gO(n)\) à cause du théorème des valeurs intermédiaires~\ref{ThoValInter}.

	En ce qui concerne la connexité, il faut en dire un peu plus.

	Les éléments de \( \gO(n,\eR)\) ont des déterminants égaux à \( 1\) ou à \( -1\). Ces deux parties sont des ouverts (pour la topologie induite de \( \eM(n,\eR)\)). En effet soit \( A\in\SO(n,\eR)\) (la partie contenant les déterminants \( 1\); ce que l'on va dire tient pour l'autre partie). Alors, parce que le déterminant est une fonction continue sur \( \eM(n,\eR)\), il existe un voisinage \( \mO\) de \( A\) dans \( \eM(n,\eR)\) dans lequel le déterminant reste entre \( \frac{ 1 }{2}\) et \( \frac{ 3 }{2}\) (c'est la définition de la continuité avec \( \epsilon=1/2\)). L'ensemble \( \mO\cap\gO(n,\eR)\) est par définition un ouvert de \( \gO(n,\eR)\) et ne contient que des éléments de déterminant \( 1\).

	La partie \( \gO(n,\eR)\) de \( \eM(n,\eR)\) est donc non-connexe selon la définition~\ref{DefIRKNooJJlmiD}.
\end{proof}

\begin{lemma}       \label{LEMooQMXHooZQozMK}
	Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes par arcs.
\end{lemma}

\begin{proof}
	Soient \( A\) une matrice unitaire, et \( Q\) une matrice unitaire qui diagonalise \( A\). Étant donné que les valeurs propres arrivent par paires complexes conjuguées,
	\begin{equation}
		QAQ^{-1}=\begin{pmatrix}
			e^{i\theta_1} &                &        &               &                \\
			              & e^{-i\theta_1} &        &               &                \\
			              &                & \ddots &               &                \\
			              &                &        & e^{i\theta_r} &                \\
			              &                &        &               & e^{-i\theta_r}
		\end{pmatrix}.
	\end{equation}
	Le chemin \( U(t)\) obtenu en remplaçant \( \theta_i\) par \( t\theta_i\) avec \( t\in\mathopen[ 0 , 1 \mathclose]\) joint \( QAQ^{-1}\) à l'identité. Par conséquent \( Q^{-1}U(t)Q\) joint \( A\) à l'unité.
\end{proof}


\begin{theorem}
	Les matrices normales\footnote{Définition~\ref{DefWQNooKEeJzv}.} forment un espace connexe par arc.
\end{theorem}

\begin{proof}
	Soient \( A\) une matrice normale et \( U\) une matrice unitaire qui diagonalise \( A\). Nous considérons \( U(t)\), un chemin qui joint \( \mtu\) à \( U\) dans \( \gU(n)\). Pour chaque \( t\), la matrice
	\begin{equation}
		A(t)=U(t)^{-1} AU(t)
	\end{equation}
	est normale. Nous avons donc trouvé un chemin dans les matrices normales qui joint \( A\) à une matrice diagonale. Il est à présent facile de la joindre à l'identité.

	Toutes les matrices normales étant connexes à l'identité, l'ensemble des matrices normales est connexe.
\end{proof}

\begin{proposition}     \label{PROPooALQCooLZCKrH}
	Le groupe \( \SL(n,\eK)\) est connexe par arcs.
\end{proposition}

\begin{proof}
	Soit \( A\in \SL(n,\eK)\); par la proposition~\ref{PropooFDNRooWFfUDd}\ref{ITEMooLRYXooSoKRiA} nous pouvons écrire
	\begin{equation}
		A=\prod_{c\in X}T_c(\lambda_c)
	\end{equation}
	où \( X\) est une partie de l'ensemble des couples \( (i,j)\) dans \( \{ 1,\ldots, n \}\). En posant
	\begin{equation}
		\begin{aligned}
			\varphi\colon \mathopen[ 0 , 1 \mathclose] & \to \SL(n,\eK)                        \\
			t                                          & \mapsto \prod_{c\in X}T_c(t\lambda_c)
		\end{aligned}
	\end{equation}
	nous avons une application continue de \( A\) vers \( \mtu\), qui, à tout \( t\) fait correspondre la matrice \( \varphi(t)\), inversible de déterminant \( 1\).

	Donc tous les éléments de \( \SL(n,\eK)\) peuvent être reliés à \( \mtu\). Par conséquent, \( \SL(n,\eK)\) est connexe par arcs.
\end{proof}

\begin{proposition}[\cite{ooGKOIooXKUQKk}]\label{PROPooVJNIooMByUJQ}
	Le groupe \( \GL(n,\eC)\) est connexe par arcs.
\end{proposition}

\begin{proof}
	Soient \( A\in\GL(n,\eC)\) et sa décomposition \eqref{EQooKSQVooIpkdIE}. Comme montré précédemment, chacune des transvections peut être reliée à \( \mtu\) par un chemin continu dans \( \SL(n,\eC)\). En ce qui concerne le facteur de translation,  nous ne pouvons pas simplement prendre le chemin donné par \( t\mapsto D_n\big( t\det(A) \big)\) parce que le résultat n'est pas inversible en \( t=0\).

	Puisque \( \eC^*\) est connexe par arcs il existe une application continue \( \alpha\colon \mathopen[ 0 , 1 \mathclose]\to \eC^*\) telle que \( \alpha(0)=\det(A)\in \eC^*\) et \( \alpha(1)=1\). Il suffit alors de prendre \( D_n\big( \alpha(t) \big)\) et nous avons un chemin continu de \( A\) vers \( \mtu\) restant dans \( \GL(n,\eC)\).
\end{proof}

\begin{proposition} \label{PROPooBIYQooWLndSW}
	Le groupe \( \GL(n,\eR)\) a exactement deux composantes connexes par arcs.
\end{proposition}
\index{connexité!le groupe \( \GL^+(n,\eR)\)}

\begin{proof}
	Nous notons \( \GL^+(n,\eR)\) et \( \GL^-(n,\eR)\) les parties de \( \GL(n,\eR)\) formées des applications de déterminant strictement positifs et strictement négatifs respectivement. D'après le théorème des valeurs intermédiaires (théorème~\ref{ThoValInter}), il n'existe pas d'application continue dans \( \GL(n,\eR)\) reliant \( \GL^+(n,\eR)\) à \( \GL^-(n,\eR)\) tout en restant dans les applications de déterminant non nul\footnote{Si \( \varphi\colon \mathopen[ 0 , 1 \mathclose]\to \GL(n,\eR)\) est le chemin, la fonction à mettre dans le théorème des valeurs intermédiaires est la fonction \( f\colon \mathopen[ 0 , 1 \mathclose]\to \eR\) \(t\mapsto \det\big( \varphi(t) \big)\).}.

	Montrons que \( \GL^{\pm}(n,\eR)\) sont connexes par arcs. Si \( A\in\GL^+(n,\eR)\) alors grâce à la décomposition \eqref{EQooKSQVooIpkdIE}, il existe un chemin continu de \( A\) vers \( D_n\big( \det(A) \big)\). Puisque \( \eR^{\pm}\) sont connexes par arcs, il est possible de relier \( D_n\big( \det(A) \big)\) à \( D_n(\pm 1)\) par un chemin continu.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Densité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropDigDensVxzPuo}
	Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\).
\end{proposition}
\index{densité!matrices diagonalisables dans \( \eM(n,\eC)\)}

\begin{proof}
	D'après le lemme de Schur~\ref{LemSchurComplHAftTq}, une matrice de \( \eM(n,\eC)\) est de la forme
	\begin{equation}
		A=Q\begin{pmatrix}
			\lambda_1 & *      & *         \\
			0         & \ddots & *         \\
			0         & 0      & \lambda_n
		\end{pmatrix}Q^{-1}.
	\end{equation}
	Les valeurs propres sont sur la diagonale. La matrice est diagonalisable si les éléments de la diagonales sont tous différents. Il suffit maintenant de considérer \( n\) suites \( (\epsilon^{(r)}_k)_{k\in\eN}\) convergentes vers zéro telles que pour chaque \( k\) les nombres \( \lambda_r+\epsilon^{(r)}_k\) soient tous différents. La suite de matrices
	\begin{equation}
		A_k=Q\begin{pmatrix}
			\lambda_1+\epsilon^{(1)}_k & *      & *                          \\
			0                          & \ddots & *                          \\
			0                          & 0      & \lambda_n+\epsilon^{(n)}_k
		\end{pmatrix}Q^{-1}.
	\end{equation}
	est alors diagonalisable pour tout \( k\) et nous avons \( \lim_{k\to \infty} A_k=A\).
\end{proof}

\begin{proposition} \label{PropQGUPooVudelJ}
	Les matrices inversibles sont denses dans l'ensemble des matrices. C'est-à-dire que \( \GL(n,\eR)\) est dense dans \( \eM(n,\eR)\).
\end{proposition}
\index{densité!de \( \GL(n,\eR)\) dans \( \eM(n,\eR)\)}

\begin{proof}
	Soit \( A\in \eM(n,\eR)\); le lemme de Schur réel~\ref{LemSchureRelnrqfiy} nous permet d'écrire
	\begin{equation}        \label{EQooJHPAooWFVSat}
		A=Q^{-1}
		\begin{pmatrix}
			\lambda_1 &        &           &                 &        \\
			          & \ddots &           &                 &        \\
			          &        & \lambda_r &                 &        \\
			          &        &           & \begin{pmatrix}
				                                 a & b \\
				                                 c & d
			                                 \end{pmatrix} &          \\
			          &        &           &                 & \ddots
		\end{pmatrix}
		Q
	\end{equation}
	avec \( Q\) orthogonale.

	Nous allons définir une suite \( (A_k)\) de matrices inversibles convergeant vers \( A\). Pour cela nous considérons, pour chaque \( i\), une suite \( (\epsilon^{(i)}_k)\) telle que \( \lambda_i+\epsilon^{(i)}_k\neq 0\) et \( \epsilon^{(i)}_k\to 0\). La matrice \( A_k\) est la matrice donnée par \eqref{EQooJHPAooWFVSat} dans laquelle nous remplaçons \( \lambda_i\) par \( \lambda_i+\epsilon^{(i)}_k\).

	En ce qui concerne les blocs, ceux dont le déterminant est non nul, nous n'y touchons pas, et ceux dont le déterminant est nul, nous remplaçons \( a\) par \( a+\epsilon_k\).

	Avec cela, \( Q^{-1}A_kQ\) est une suite dans \( \GL(n,\eR)\) qui converge vers \( A\).
\end{proof}

\begin{proposition}     \label{PROPooZUHOooQBwfZq}
	Si \( A\in\eM(n,\eC)\) alors
	\begin{equation}
		e^{\tr(A)}=\det( e^{A}).
	\end{equation}
\end{proposition}

\begin{proof}
	Ici, \( e^A\) est l'exponentielle, soit d'endomorphisme, soit de matrice définie par la proposition~\ref{PropPEDSooAvSXmY}.

	Le résultat est un simple calcul pour les matrices diagonalisables. Si \( A\) n'est pas diagonalisable, nous considérons une suite de matrices diagonalisables \( A_k\) dont la limite est \( A\) (proposition~\ref{PropDigDensVxzPuo}). La suite
	\begin{equation}
		a_k= e^{\tr(A_k)}
	\end{equation}
	converge vers \(  e^{\tr(A)}\) tandis que la suite
	\begin{equation}
		b_k=\det( e^{A_k})
	\end{equation}
	converge vers \( \det( e^{A})\). Mais nous avons \( a_k=b_k\) pour tout \( k\); les limites sont donc égales.
\end{proof}

\begin{corollary}       \label{CORooOKKSooHrsYOs}
	Si \( X\in\eM(n,\eC)\) alors
	\begin{equation}
		\Dsdd{ \det( e^{tX}) }{t}{0}=\tr(X).
	\end{equation}
\end{corollary}

\begin{proof}
	Nous écrivons la proposition \ref{PROPooZUHOooQBwfZq} pour \( tX\) au lieu de \( X\); pour chaque \( t\) nous avons
	\begin{equation}
		\det\big(  e^{tX} \big)= e^{\tr(tX)}= e^{t\tr(X)}.
	\end{equation}
	La dérivation par rapport à \( t\) en \( t=0\) donne le résultat attendu.
\end{proof}

\begin{theorem}[Cayley-Hamilton\cite{QATooFIHVMw,MOSooRVRrHw}]  \label{ThoHZTooWDjTYI}
	Tout endomorphisme d'un espace vectoriel de dimension finie sur un corps commutatif quelconque annule son propre polynôme caractéristique
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une autre démonstration est donnée par le théorème~\ref{ThoCalYWLbJQ}.
\begin{proof}
	La preuve est divisée en plusieurs étapes.
	\begin{subproof}
		\spitem[Endomorphisme diagonalisable]
		Soit \( u\) un endomorphisme sur un espace vectoriel \( V\) de dimension \( n\) sur un corps \( \eK\) et \( \chi_u\) sont polynôme caractéristique. Nous savons que si \( \lambda\) est une valeur propre de \( u\) alors \( \chi_u(\lambda)=0\) par le théorème~\ref{ThoWDGooQUGSTL}\ref{ItemeXHXhHii}. En combinant avec le lemme~\ref{LemVISooHxMdbr}, si \( x\) est vecteur propre pour la valeur propre \( \lambda\) de \( u\) nous avons
		\begin{equation}
			\chi_u(u)x=\chi_u(\lambda)x=0.
		\end{equation}
		Donc tant que \( u\) possède une base de vecteurs propres nous avons \( \chi_u(u)=0\).

		\spitem[Le cas complexe]

		Nous nous restreignons à présent (et provisoirement) au cas \( \eK=\eC\), ce qui nous donne \( u\in \eM(n,\eC)\). Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\) par la proposition~\ref{PropDigDensVxzPuo}. Si \( A\in \eM(n,\eC)\) nous considérons une suite de matrices diagonalisables \( A_k\stackrel{\eM(n,\eC)}{\longrightarrow}A\). Pour chaque \( k\) nous avons par le point précédent
		\begin{equation}
			\chi_{u_k}(u_k)=0.
		\end{equation}
		Chacune des composantes de \( \chi_{u_k}(u_k)\) est un polynôme en les composantes de \( u_k\), ce qui légitime le passage à la limite :
		\begin{equation}
			\chi_u(u)=0.
		\end{equation}
		Le théorème est établi pour toutes les matrices de \( \eM(n,\eC)\) et donc aussi pour tous les sous-corps de \( \eC\) comme \( \eR\) ou \( \eZ\).

		\spitem[Le cas général]

		Par définition, \( \chi_u(X)=\det(u-X\mtu)\); les coefficients de \( X\) sont des polynômes à coefficients entiers en les composantes de \( u\). En substituant \( u\) à \( X\) nous obtenons une matrice dont chacune des entrées est un polynôme à coefficients entiers en les coefficients de \( u\). Pour chaque \( i\) et \( j\) entre \( 1\) et \( n\) il existe donc un polynôme \( P_{ij}\in \eZ(X_1,\ldots, X_{n^2})\) tel que
		\begin{equation}
			\chi_u(u)_{ij}=P(u_{11},\ldots, u_{nn}).
		\end{equation}
		Ces polynômes ne dépendent pas de \( u\) ni du corps sur lequel on travaille. Notre but est maintenant de prouver que \( P_{ij}=0\).

		Étant donné que le cas complexe (et a fortiori entier) est déjà prouvé nous savons que pour tout \( u\in \eM(n,\eZ)\) nous avons \( P(u_{11},\ldots, u_{nn})=0\). La proposition~\ref{PropTETooGuBYQf} nous donne effectivement \( P=0\), en conséquence de quoi l'endomorphisme \( \chi_u(u)\) est nul.

	\end{subproof}
\end{proof}

\begin{example}
	Pour montrer que chaque composante \( \chi_u(u)\) est bien un polynôme à coefficients entiers en les coefficients de \( u\), voyons l'exemple \( 2\times 2\) : \( u=\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}\). D'abord
	\begin{equation}
		\chi_u(X)=\det\begin{pmatrix}
			a-X & b   \\
			c   & d-X
		\end{pmatrix}=X^2-(a+d)X+ad-cb.
	\end{equation}
	Le coefficient de \( X^2\) est \( 1\), celui de \( X\) est \( -a-d\) et le terme indépendant est \( ad-cb\); tout trois sont des polynômes à coefficients entiers en \( a,b,c,d\). Après substitution de \( X\) par \( u\),
	\begin{equation}
		\chi_u(u)_{ij}=(u^2)_{ij}-(a+d)u_{ij}+ad-cb.
	\end{equation}
	C'est bien un polynôme à coefficients entiers en les entrées de la matrice \( u\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carrée d'une matrice hermitienne positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}     \label{PropVZvCWn}
	Si \( A\in \eM(n,\eC)\) est une matrice hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} positive, alors il existe une unique matrice hermitienne positive \( R\) telle que \( A=R^2\). De plus \( R\) est un polynôme (de \( \eR[X]\)) en \( A\).

	La matrice \( R\) ainsi définie est la \defe{racine carrée}{matrice!racine carrée}\index{racine!carré de matrice!hermitienne positive} de \( A\), et est notée \( \sqrt{A}\)\nomenclature[A]{\( \sqrt{A}\)}{racine d'une matrice hermitienne positive}.
\end{propositionDef}
\index{matrice!semblable}
\index{polynôme!d'endomorphisme}
\index{endomorphisme!diagonalisable}
\index{matrice!hermitienne!racine carrée}
\index{racine!carré!de matrice hermitienne}


\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Existence]
		Étant donné que \( A \) est hermitienne, elle est diagonalisable par une matrice unitaire (proposition~\ref{ThogammwA}), et ses valeurs propres sont réelles et positives (parce que \( A\) est positive). Soit donc \( P\) une matrice unitaire telle que
		\begin{equation}
			P^*AP=\begin{pmatrix}
				\alpha_1 &        &          \\
				         & \ddots &          \\
				         &        & \alpha_n
			\end{pmatrix}
		\end{equation}
		avec \( \alpha_i>0\). Si on pose
		\begin{equation}
			R=P\begin{pmatrix}
				\sqrt{\alpha_1} &        &                 \\
				                & \ddots &                 \\
				                &        & \sqrt{\alpha_n}
			\end{pmatrix}P^*,
		\end{equation}
		alors \( R^2=A\) parce que \( P^*P=\mtu\).
		\spitem[Hermitienne positive]
		La matrice \( R\) est hermitienne parce que, avec un peu de notation raccourcie, \( R=P^*\sqrt{\alpha}P\) et \( R^*=P^*\sqrt{\alpha}P\). D'autre part, elle est positive parce que ses valeurs propres sont les \( \sqrt{\alpha_i}\) qui sont positives.

		\spitem[Polynôme]
		% -------------------------------------------------------------------------------------------- 
		Nous montrons maintenant que la matrice \( R\) est un polynôme en \( A\). Pour cela nous considérons un polynôme \( Q\) tel que \( Q(\alpha_i)=\sqrt{\alpha_i}\) pour tout \( i\). Soit \( \{ e_i \}\) une base de diagonalisation de \( A\) : \( Ae_i=\alpha_ie_i\). Alors c'est encore une base de diagonalisation de \( Q(A)\). En effet si \( Q=\sum_ka_kX^k\), alors
		\begin{equation}
			Q(A)e_i=(\sum_ka_kA^k)e_i=(\sum_ka_k\alpha_i^k)e_i=Q(\alpha_i)e_i=\sqrt{\alpha_i}e_i.
		\end{equation}
		Les valeurs propres de \( Q(A)\) sont donc \( \sqrt{\alpha_i}\). Nous savons maintenant que \( Q(A)\) a la même base de diagonalisation de \( A\) (et donc la même matrice unitaire \( P\) qui diagonalise), c'est-à-dire que
		\begin{equation}
			Q(A)=P^*\begin{pmatrix}
				\sqrt{\alpha_1} &        &                 \\
				                & \ddots &                 \\
				                &        & \sqrt{\alpha_n}
			\end{pmatrix}=R.
		\end{equation}
		Donc oui, \( R\) est un polynôme en \( A\).

		Notons que ce \( Q\) n'est pas du tout unique; il existe une infinité de polynômes envoyant \( n\) nombres donnés sur \( n\) nombres donnés.

		\spitem[Unicité]
		% -------------------------------------------------------------------------------------------- 
		Soit \( S\) une matrice hermitienne positive telle que \( R^2=S^2=A\). D'abord \( S\) commute avec \( A\) parce que
		\begin{equation}
			SA=S^3=S^2S=AS.
		\end{equation}
		Donc \( S\) commute aussi avec \( Q(A)=R\). Étant donné que \( S\) et \( R\) commutent et sont diagonalisables, ils sont simultanément diagonalisables par le corolaire~\ref{CorQeVqsS}. Soient \( D_R=PRP^*\) et \( D_S=PSP^*\) les formes diagonales de \( R\) et \( S\) dans une base de simultanée diagonalisation. Les carrés des valeurs propres de \( R\) et \( S\) étant identiques (ce sont les valeurs propres de \( A\)) et les valeurs propres de \( R\) et \( S\) étant positives, nous déduisons que \( D_R=D_S\) et donc que \( R=P^*D_RP=P^*D_SP=S\).
	\end{subproof}
\end{proof}

Une des applications usuelles de cette proposition est la décomposition polaire.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carrée d'une matrice symétrique positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{JJdQPyK}]   \label{LemTLlTAAf}
	Le groupe orthogonal \( \gO(n,\eR) \) est compact.
\end{lemma}

\begin{proof}
	Nous avons \( O(n)=f^{-1}\big( \{ \mtu_n \} \big)\) où \( f\) est l'application continue \( A\mapsto A^tA\). En tant qu'image inverse d'un fermé par une application continue, le groupe \( O(n)\) est fermé.

	De plus il est borné parce que tous les coefficients d'une matrice orthogonale sont \( \leq 1\), donc \( \| A \|_{\infty}\) pour tout \( A\in O(n)\).
\end{proof}

\begin{proposition} \label{PropPEMDqVT}
	Une matrice symétrique définie ou semi définie positive, admet une unique racine carrée symétrique. Le spectre de la racine carrée est la racine carrée du spectre de la matrice de départ.
\end{proposition}

\begin{proof}
	Propriétés de la racine carrée d'une matrice symétrique
	\begin{subproof}
		\spitem[Existence]
		Soit \( T\) une matrice symétrique et \( Q\) une matrice orthogonale qui diagonalise\footnote{Théorème~\ref{ThoeTMXla}.} \( T\) : \( QTQ^{-1}=D\) avec \( D=\diag(\lambda_i)\) et \( \lambda_i\geq 0\). En posant \( R=Q^{-1}\sqrt{D}Q\), il est vite vérifié que \( R^2=T\) et que \( R\) est symétrique. En ce qui concerne le spectre, \( R\) a pour valeurs propres les \( \sqrt{\lambda_i}\).
		\spitem[Unicité]

		Soit \( R\) une matrice symétrique de \( T\) : \( R^2=T\). Du coup \( R\) et \( T\) commutent : \( RT=R^3=TR\). Par conséquent les espaces propres de \( T\) sont stables sous \( R\). Soit \( E_{\lambda} \) l'un d'eux de dimension \( d\), et \( T_F\), \( R_F\) les restrictions de \( T\) et \( R\) à \( E_{\lambda}\). L'application \( T_F\) est une homothétie et \( R_F^2=T_F=\lambda\mtu\). Mais \( R_F\) est encore une matrice symétrique définie positive, donc nous pouvons considérer une base \( \{ e_1,\ldots, e_d \}\) de \( E_{\lambda}\) qui diagonalise \( R_F\) avec les valeurs propres \( \mu_i\); nous avons donc en même temps
		\begin{subequations}
			\begin{align}
				R_f^2(e_i) & =\mu_i^2 e_i  \\
				T_F(e_i)   & =\lambda e_i,
			\end{align}
		\end{subequations}
		de telle sorte que \( \mu_i^2=\lambda\). Mais les valeurs propres de \( R_F\) sont positives, sont \( \mu_i=\sqrt{\lambda}\) pour tout \( i\). En conclusion \( R_F\) est univoquement déterminé par la donnée de \( T\). Vu que cela est valable pour tous les espaces propres de \( T\) et que ces espaces propres engendrent tout \( E\), l'opérateur \( R\) est déterminé de façon univoque par \( T\).
	\end{subproof}
\end{proof}
Notons que nous n'avons démontré l'unicité qu'au sein des matrices symétriques.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition polaires : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

Nous rappelons que \( S^{++}(n,\eR)\) est l'ensemble des matrices symétriques strictement définies positives, d'après le paragraphe~\ref{NORMooAJLHooQhwpvr}.

\begin{lemma}   \label{LemMGUSooPqjguE}
	La partie \( S^+(n,\eR)\) est fermée dans \( \eM(n,\eR)\).
\end{lemma}

\begin{proof}
	En effet si \( S_k\) est une suite de matrices symétriques convergeant dans \( \eM(n,\eR)\) vers la matrice \( A\), les suites \( (S_k)_{ij}\) et \( (S_k)_{ji}\) des composantes \( ij\) et \( ji\) sont des suites égales, et donc leurs limites sont égales\footnote{Ici nous utilisons le critère de convergence composante par composante et le fait que nous ne sommes pas trop inquiétés par la norme que nous choisissons parce que toutes les normes sont équivalentes par le théorème~\ref{ThoNormesEquiv}.}. Donc la limite est symétrique.

	En ce qui concerne le spectre, le théorème~\ref{ThoeTMXla} nous permet de diagonaliser : \( S_k=Q_kD_kQ_k^{-1}\) où les \( D_k\) sont des matrices diagonales remplies de nombres positifs ou nuls. Comme \( O(n)\) est compact\footnote{Lemme~\ref{LemTLlTAAf}.}, nous avons une sous-suite \( Q_{\varphi(k)}\) convergente : \( Q_{\varphi(k)}\to Q\). Pour chaque \( k\), nous avons
	\begin{equation}
		S_{\varphi(k)}=Q_{\varphi(k)}D_{\varphi(k)}Q^{-1}_{\varphi(k)},
	\end{equation}
	dont la limite existe et vaut \( A\). Puisque pour tout \( k\), \( D_{\varphi(k)}=Q^{-1}_{\varphi(k)}S_{\varphi(k)}Q_{\varphi(k)}\) et que le produit matriciel est continu, la suite \( k\mapsto D_{\varphi(k)}\) est une suite convergente dans \( \eM(n,\eR)\). Nous notons \( D\) sa limite qui est encore une matrice diagonale contenant des nombres positifs ou nuls sur la diagonale.
	\begin{equation}
		A=\lim_{k\to \infty } S_{\varphi(k)}=QDQ^{-1},
	\end{equation}
	et donc le spectre de \( A\) est la limite de ceux des matrices \( D_{\varphi(k)}\). Chacun étant positif, la limite est positive. Donc \( A\in S^+(n,\eR)\).
\end{proof}

\begin{lemma}   \label{LemZKJWqIP}
	La fermeture de l'ensemble des matrices symétriques strictement définies positives est l'ensemble des matrices définies positives : \( \Adh(S^{++}(n,\eR))=S^+(n,\eR)\).
\end{lemma}
\index{densité!de \( S^+(n,\eR)\) dans \( S^{++}(n,\eR)\)}

\begin{proof}
	Le lemme~\ref{LemMGUSooPqjguE} nous dit que \( S^+(n,\eR)\) était fermé. Nous devons prouver que pour tout élément de \( S^+(n,\eR)\), il existe une suite \( (S_k)\) dans \( S^{++}(n,\eR)\) convergeant vers \( S\).

	Si \( S\in S^+(n,\eR)\) alors nous avons la diagonalisation
	\begin{equation}
		S=QDQ^{-1} =Q
		\begin{pmatrix}
			\lambda_1 &        &           \\
			          & \ddots &           \\
			          &        & \lambda_n
		\end{pmatrix}
		Q^{-1}
	\end{equation}
	où \( \lambda_i\geq 0\) pour tout \( i\). Nous définissons
	\begin{equation}
		D_k=
		\begin{pmatrix}
			\lambda_1+\epsilon^{(1)}_k &        &                            \\
			                           & \ddots &                            \\
			                           &        & \lambda_n+\epsilon^{(n)}_k
		\end{pmatrix}
	\end{equation}
	où \( \epsilon^{i}_k\) est une suite convergent vers \( 0\) telle que \( \lambda_i+\epsilon^{(i)}_n>0\) pour tout \( i\) et \( k\). Typiquement si \( \lambda_i>0\) alors \( \epsilon^{(i)}_k=0\) et sinon \( \epsilon^{(i)}_k=1/k\).

	Pour tout \( k\) nous avons \( QD_kQ^{-1}\in S^{++}(n,\eR)\) et de plus \( QD_kQ^{-1}\to QDQ=S\).
\end{proof}

\begin{theorem}[Décomposition polaire de matrices symétriques définies positives\cite{JJdQPyK,AABkVai,WWBTooITOwEn}] \label{ThoLHebUAU}
	En ce qui concerne les matrices inversibles :
	\begin{equation}
		\begin{aligned}
			f\colon O(n,\eR)\times S^{++}(n,\eR) & \to \GL(n,\eR) \\
			(Q,S)                                & \mapsto SQ
		\end{aligned}
	\end{equation}
	est un homéomorphisme\footnote{Cela est en réalité un difféomorphisme, voir la remarque~\ref{RemBJCBooGLiRmG}.}.

	En ce qui concerne les matrices en général :
	\begin{equation}
		\begin{aligned}
			g\colon O(n,\eR)\times S^+(n,\eR) & \to \eM(n,\eR) \\
			(Q,S)                             & \mapsto SQ
		\end{aligned}
	\end{equation}
	est une surjection mais pas une injection.

	De plus les mêmes conclusions tiennent si nous prenons \( (Q,S)\mapsto QS\) au lieu de \( SQ\).
\end{theorem}
\index{groupe!linéaire!décomposition polaire}
\index{endomorphisme!décomposition!polaire}
\index{décomposition!polaire}


\begin{probleme}
	Je crois que les éléments de la décomposition polaire sont des polynômes en \( M\). Écrivez-moi si vous pouvez confirmer ou infirmer.
\end{probleme}

\begin{proof}
	Nous commençons par prouver les résultats concernant les matrices inversibles.
	\begin{subproof}
		\spitem[Existence et unicité]

		Si \( M=SQ\), alors \( MM^t=SQQ^tS^t=S^2\), donc \( S\) doit être une racine carrée symétrique de la matrice définie positive \( MM^t\). La proposition~\ref{PropPEMDqVT} nous dit que ça existe et que c'est unique. Donc \( S\) est univoquement déterminé par \( M\). Maintenant avoir \( Q=MS^{-1}\) est obligatoire (unicité) et fonctionne :
		\begin{equation}
			Q^tQ=(S^{-1})^tM^tMS^{-1}=S^{-1}S^2S^{-1}=\mtu,
		\end{equation}
		donc \( Q\) ainsi défini est orthogonale.

		Notons que ceci ne fonctionne pas lorsque \( M\) n'est pas inversible parce qu'alors \( S\) n'est pas inversible.

		\spitem[Homéomorphisme]

		Le fait que \( f\) soit continue n'est pas un problème : c'est un produit de matrices. Nous devons vérifier que \( f^{-1}\) est continue. Soit une suite convergente \( M_k\to M\) dans \( \GL(n,\eR)\). Si nous nommons \( (Q_k,S_k)\) la décomposition polaire de \( M_k\) et \( (Q,S)\) celle de \( M\), nous devons prouver que \( Q_k\to Q\) et \( S_k\to S\). En effet dans ce cas nous aurions
		\begin{equation}    \label{EqJIkoaJv}
			\lim_{k\to \infty} f^{-1}(M_k)=\lim_{k\to \infty} (Q_k,S_k)=(Q,S)=f^{-1}(M).
		\end{equation}

		Étant donné que \( O(n)\) est compact (lemme~\ref{LemTLlTAAf}), la suite \( (Q_k)\) admet une sous-suite convergente (Bolzano-Weierstrass, théorème~\ref{ThoBWFTXAZNH}) que nous nommons
		\begin{equation}
			Q_{\varphi(k)}\to F\in O(n).
		\end{equation}
		Vu que la suite \( (M_k)\) converge, sa sous-suite converge vers la même limite : \( M_{\varphi(k)}\to M\) et vu que pour tout \( k\) nous avons \( S_k=M_kQ_k^{-1}\),
		\begin{equation}
			S_{\varphi(k)}\to G=MF^{-1}.
		\end{equation}
		Vu que chacune des matrices \( S_{\varphi(k)}\) est symétrique définie positive, la limite est symétrique et semi-définie positive\footnote{Lemme~\ref{LemZKJWqIP}}. Donc \( G\in S^+(n,\eR)\cap \GL(n,\eR)\) parce que de plus \( M\) et \( F\) étant inversibles, \( G\) est inversible. En ce qui concerne la sous-suite nous avons
		\begin{equation}
			M_{\varphi(k)}=S_{\varphi(k)}Q_{\varphi(k)}\to GF=M
		\end{equation}
		où \( F\in O(n)\) et \( G\in S^+(n,\eR)\). Par unicité de la décomposition polaire de \( M\) (partie déjà démontrée), nous avons \( G=S\) et \( F=Q\).

		Nous avons prouvé que toute sous-suite convergente de \( Q_k\) a \( Q\) pour limite. Donc la suite elle-même converge\footnote{Proposition~\ref{PropHNylIAW}, pas difficile.} vers \( Q\). Donc \( Q_k\to Q\). Du coup vu que \( S_k=M_kQ_k^{-1}\) est un produit de suites convergentes, \( S_k\) converge également, vers \( S\) :  \( S_k\to S\).

		Au final l'application \( f^{-1}\) est bien continue parce que les égalités \eqref{EqJIkoaJv} ont bien lieu.
	\end{subproof}

	Nous passons maintenant à la preuve dans le cas des matrices en général.

	Soit \( A\in \eM(n,\eR)\); par densité (lemme~\ref{PropQGUPooVudelJ}), il existe une suite \( (A_k)\) dans \( \GL(n,\eR)\) telle que \( A_k\to A\). Pour chacun des \( k\) nous appliquons la décomposition polaire déjà prouvée : \( A_k=Q_kS_k\). D'abord \( (Q_k)\) est une suite dans le compact\footnote{Lemme~\ref{LemTLlTAAf}.} \( \gO(n,\eR)\) et accepte donc une sous-suite convergente. Quitte à redéfinir la suite de départ, nous supposons pour alléger les notations que \( Q_k\to Q\in \gO(n,\eR)\). Vu que \( Q_k\) est inversible,
	\begin{equation}
		S_k=Q^{-1}_kA_k
	\end{equation}
	Le produit matriciel étant continu nous avons \( S_k\to S\) dans \( \eM(n,\eR)\). Mais \( S^+(n,\eR)\) étant fermé (lemme~\ref{LemMGUSooPqjguE}) nous avons aussi \( S\in S^+(n,\eR)\).
\end{proof}

\begin{remark}  \label{RemBJCBooGLiRmG}
	Pour démontrer que \( f\) est différentiable, nous devons utiliser le théorème d'inversion locale~\ref{ThoXWpzqCn}; cela est fait dans la proposition~\ref{PropWCXAooDuFMjn}.
\end{remark}

\begin{corollary}       \label{CorAWYBooNCCQSf}
	Toute matrice peut être écrite sous la forme \( Q_1DQ_2\) où \( Q_1\) et \( Q_2\) sont orthogonales et \( D\) est diagonale.
\end{corollary}

\begin{proof}
	Si \( A\in\eM(n,\eR)\) alors la décomposition polaire~\ref{ThoLHebUAU} nous donne \( A=SQ\) où \( S\) est symétrique définie positive et \( Q\) est orthogonale. La matrice \( S\) peut ensuite être diagonalisée par le théorème~\ref{ThoeTMXla} : \( S=RDR^{-1}\) où \( D\) est diagonale et \( R\) est orthogonale. Avec ces deux décompositions en main, \( A=SQ=RDR^{-1}Q\). La matrice \( R^{-1}Q\) est orthogonale.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Enveloppe convexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Un point \( a\) d'un ensemble convexe \( C\) est un \defe{point extrémal}{point extrémal dans un convexe} si \( C\setminus \{ a \}\) est convexe.
\end{definition}

\begin{theorem}[\cite{KXjFWKA}] \label{ThoBALmoQw}
	Soit \( E\) un espace euclidien de dimension \( n\geq 1\) et \( \aL(E)\) l'espace des opérateurs linéaires sur \( E\) sur lequel nous considérons la norme subordonnée\footnote{Définition~\ref{DefNFYUooBZCPTr}.} à celle sur \( E\). L'ensemble des points extrémaux de la boule unité fermée de \( \aL(E)\) est le groupe orthogonal \( O(n,\eR)\).
\end{theorem}
\index{densité!points extrémaux dans \( \aL\)}

\begin{proof}
	Nous notons \( \mB\) la boule unité fermée de \( \aL(E)\). Montrons pour commencer que les éléments de \( O(n)\) sont extrémaux dans \( \mB\). D'abord si \( A\in O(E)\) alors \( \| A \|=1\) parce que \( \| Ax \|=\| x \|\). Supposons maintenant que \( A\) n'est pas extrémal, c'est-à-dire qu'il est le milieu d'un segment joignant deux points (distincts) de la boule unité de \( \aL(E)\). Soient donc \( T,U\in\mB\) tels que \( A=\frac{ 1 }{2}(T+U)\). Pour tout \( x\in E\) tel que \( \| x \|=1\) nous avons
	\begin{equation}    \label{EqKTuAIIE}
		1=\| x \|=\| Ax \|=\frac{ 1 }{2}\| Tx+Ux \|\leq \frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)\leq\frac{ 1 }{2}\big( \| T \|+\| U \| \big)\leq 1
	\end{equation}
	Toutes les inégalités sont en réalité des égalités. En particulier nous avons
	\begin{equation}
		\| Tx+Ux \|=\| Tx \|+\| Ux \|,
	\end{equation}
	mais alors nous sommes dans un cas d'égalité dans l'inégalité de Cauchy-Schwarz (théorème~\ref{ThoAYfEHG}) et donc il existe \( \lambda\geq 0\) tel que \( Tx=\lambda Ux\). Mais de plus les \sout{inégalité} égalités \eqref{EqKTuAIIE} nous donnent
	\begin{equation}
		\frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)=1
	\end{equation}
	alors que nous savons que \( \| Tx \|,\| Ux \|\leq 1\), donc \( \| Tx \|=\| Ux \|=1\). La seule possibilité est d'avoir \( \lambda=1\). Nous avons prouvé que \( Tx=Ux\) pour tout \( x\) de norme \( 1\). Nous en déduisons que \( T=U\).

	Au final \( A\) n'est pas le milieu d'un segment dans \( \mB\).

	Nous passons donc à l'inclusion inverse : nous prouvons que les points extrémaux de \( \mB\) sont dans \( O(E)\). Pour cela nous prenons \( U\in\mB\setminus O(E)\) et nous allons montrer que \( U\) n'est pas un point extrémal : nous allons l'écrire comme milieu d'un segment dans \( \mB\).

	Par la seconde partie du théorème de décomposition polaire~\ref{ThoLHebUAU}, il existe \( Q\in O(n,\eR)\) et \( S\in S^+(n,\eR)\) tels que \( U=QS\). Nous diagonalisons \( S\) à l'aide de la matrice orthogonale \( P\) :
	\begin{equation}
		S=PDP^{-1}
	\end{equation}
	avec \( D=\diag(\lambda_i)\). En termes de normes, nous avons
	\begin{equation}
		\| U \|=\| S \|=\| D \|.
	\end{equation}
	En effet vu que \( Q\) est orthogonale, \( \| Ux \|=\| QSx \|=\| Sx \|\) pour tout \( x\), donc \( \| U \|=\| S \|\). De plus pour tout \( x\) nous avons
	\begin{equation}
		\| Sx \|=\| PDP^{-1} x \|=\| DP^{-1}x \|.
	\end{equation}
	Étant donné que \( P^{-1}\) est une bijection, le supremum des \( \| Sx \|\) sera le même que celui des \( \| Dx \|\) et donc \( \| S \|=\| D \|\). Étant donné que par définition \( \| U \|\leq 1\), nous avons aussi \( \| D \|\leq 1\) et donc \( 0\leq\lambda_i\leq 1\) (pour rappel, les valeurs propres de \( D\) sont positives ou nulles parce que \( S\) est ainsi).

	Comme \( U\notin O(E)\), au moins une des valeurs propres n'est pas \( 1\), supposons que ce soit \( \lambda_1\). Alors nous avons \( \alpha,\beta\in\mathopen[ -1 , 1 \mathclose]\) avec \( -1\leq \alpha<\beta\leq 1\) et \( \lambda_1=\frac{ 1 }{2}(\alpha+\beta)\). Nous posons alors
	\begin{subequations}
		\begin{align}
			D_1 & =\diag(\alpha,\lambda_2,\ldots, \lambda_n) \\
			D_2 & =\diag(\beta,\lambda_2,\ldots, \lambda_n).
		\end{align}
	\end{subequations}
	Nous avons bien \( D_1\neq D2\) et \( D_1+ D_2=D\). Par conséquent
	\begin{equation}
		U=\frac{ 1 }{2}\big( QPD_1P^{-1}+QPD_2P^{-1} \big)
	\end{equation}
	avec \( QPD_1P^{-1}\neq QPD_2^{-1}\). La matrice \( U \) est donc le milieu d'un segment. Reste à montrer que ce segment est dans \( \mB\). Pour ce faire, prenons \( x\in E\) et calculons :
	\begin{equation}
		\| QPD_iP^{-1}x \|=\| D_iP^{-1}x \|\leq\| P^{-1}x \|=\| x \|
	\end{equation}
	parce que \( \| D_i \|\leq 1\) et \( P^{-1}\) est orthogonale. Au final la norme de \( QPD_iP\) est plus petite que \( 1\) et donc \( U\) est bien le milieu d'un segment dans \( \mB\), et donc non extrémal.
\end{proof}

\begin{theorem}[\cite{NHXUsTa}] \label{ThoVBzqUpy}
	L'enveloppe convexe de \( O(n)\) dans \( \eM_n(\eR)\) est la boule unité pour la norme induite de \( \| . \|_2\) sur \( \eR^n\).
\end{theorem}
\index{convexité!enveloppe de \( O(n)\)}
\index{groupe!linéaire!enveloppe convexe de \( \Omega(n)\)}

\begin{proof}
	Nous notons \( \mB\) la boule unité fermée de \( \eM(n,\eR)\) et \( \Conv\big( O(n,\eR) \big)\) l'enveloppe convexe de \( O(n,\eR)\). Vu que \( \mB\) est convexe nous avons \( \Conv\big( O(n) \big)\subset\mB\).


	Maintenant nous devons prouver l'inclusion inverse. Pour ce faire nous supposons avoir un élément \( A\in \mB\setminus\Conv\big( O(n) \big)\) et nous allons dériver une contradiction.

	Remarquons que \( O(n)\) est compact par le lemme~\ref{LemTLlTAAf} et que par conséquent \( \Conv(O(n))\) est compacte par le corolaire~\ref{CorOFrXzIf} et donc fermée. Nous considérons un produit scalaire \( (X,Y)\mapsto X\cdot Y\) sur \( \eM\). Vu que \( \Conv\big( O(n) \big)\) est un fermé convexe nous pouvons considérer la projection\footnote{Le théorème de projection : théorème~\ref{ThoWKwosrH}.} sur \( \Conv(A)\) relativement au produit scalaire choisi.

	Nous notons \( P=\pr_{\Conv\big( O(n) \big)}(A)\). En vertu du théorème de projection, nous avons
	\begin{equation}    \label{EqYSisLTL}
		(A-P)\cdot (M-P)\leq 0
	\end{equation}
	pour tout \( M\in\Conv O(n)\). Notons \( B=A-P\) pour alléger les notations. L'équation \eqref{EqYSisLTL} s'écrit
	\begin{equation}    \label{EqQDLZqXQ}
		B\cdot M\leq B\cdot P.
	\end{equation}
	D'autre part vu que \( B \neq 0\) nous avons \( B\cdot B> 0\), c'est-à-dire \( B\cdot (A-P)>0\) et donc
	\begin{equation}
		B\cdot A>B\cdot P.
	\end{equation}
	En combinant avec \eqref{EqQDLZqXQ},
	\begin{equation}        \label{EqIQNlwql}
		B\cdot M\leq B\cdot P<B\cdot A.
	\end{equation}
	Nous utilisons maintenant la décomposition polaire, théorème~\ref{ThoLHebUAU}, pour écrire \( B=QS\) avec \( Q\in O(n)\) et \( S\in S^+(n,\eR)\). Vu que l'inégalité \eqref{EqIQNlwql} tient pour tout \( M\in\Conv(O(n))\), elle tient en particulier pour \( Q\in O(n)\). Donc
	\begin{equation}        \label{EQooOSAXooLbTQAG}
		B\cdot Q<B\cdot A.
	\end{equation}
	Nous nous attachons à présent au produit scalaire \( (X,Y)\mapsto\tr(X^tY)\) de la proposition~\ref{PropMAQoKAg}. D'abord
	\begin{equation}    \label{EaHVxWdau}
		B\cdot Q=\tr(B^tQ)=\tr(S^tQ^tQ)=\tr(S^t)=\tr(S),
	\end{equation}
	et ensuite l'inégalité \eqref{EQooOSAXooLbTQAG} devient
	\begin{equation}
		\tr(S)<B\cdot A=\tr(S^tQ^tA).
	\end{equation}
	Nous choisissons une basse \( \{ e_i \}\) diagonalisant \( S\) : \( Se_i=\lambda_ie_i\) vérifiant automatiquement \( \lambda_i\geq 0\) parce que \( S\) est semi-définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.}. Alors
	\begin{subequations}
		\begin{align}
			\tr(S) & <\tr(S^tQ^tA)                                                                                          \\
			       & =\sum_i\langle S^tQ^tAe_i, e_i\rangle                                                                  \\
			       & =\sum_i\langle Ae_i, QSe_i\rangle                                                                      \\
			       & \leq \sum_i \| Ae_i \| | \lambda_i | \underbrace{\| Qe_i \|}_{=1}                                      \\
			       & \leq \sum_i\lambda_i                                              & A\in\mB\Rightarrow\| Ae_i \|\leq 1 \\
			       & =\tr(S).
		\end{align}
	\end{subequations}
	Il faut noter que la première inégalité est stricte, et donc nous avons une contradiction.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition de Bruhat}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Décomposition de Bruhat]\index{Bruhat (décomposition)}\index{décomposition!Bruhat}    \label{ThoizlYJO}
	Soit \( \eK\) un corps; un élément \( M\in\GL(n,\eR)\) s'écrit sous la forme
	\begin{equation}
		M=T_1P_{\sigma}T_2
	\end{equation}
	où \( T_1\) et \( T_2\) sont des matrices triangulaires supérieures inversibles et où \( P_{\sigma}\) est une matrice de permutations \( \sigma\in S_n\). De plus il y a unicité de \( \sigma\).
\end{theorem}
\index{groupe!permutation}
\index{groupe!linéaire}
\index{matrice}

\begin{proof}
	Afin de rendre les choses plus visuelles, nous nous permettons de donner des exemples au fur et à mesure de la preuve. Nous prenons l'exemple de la matrice
	\begin{equation}
		\begin{pmatrix}
			1 & 3 & 4 \\
			2 & 5 & 6 \\
			0 & 7 & 8
		\end{pmatrix}.
	\end{equation}
	\begin{subproof}
		\spitem[Existence]
		Soit \( M\in \GL(n,\eR)\); puisqu'elle est inversible, on a un indice \( i_1\) maximum tel que \( M_{i_1,1}\neq 0\). Nous changeons toutes les lignes jusque là, c'est-à-dire que nous faisons, pour \( 1\leq i< i_1\),
		\begin{equation}        \label{EqGHUbwR}
			L_i\to L_i-\frac{ M_{i1} }{ M_{i_11} }L_{i_1}.
		\end{equation}
		Voir le lemme~\ref{LemooTQJXooGoIxsI}\ref{ITEMooXUGFooKcbrxs}.

		Nous avons donc obtenu une matrice dont la première colonne est nulle sauf la case numéro \( i_1\). L'opération \eqref{EqGHUbwR} revient à considérer la multiplication par la matrice de transvection
		\begin{equation}
			T_1^{(i)}=T_{ii_1}\left( -\frac{ M_{i1} }{ M_{i_11} } \right)
		\end{equation}
		pour tout \( i<i_1\). Pour rappel nous ne changeons que les lignes \emph{au-dessus} de la \( i_1\). Du coup les matrices \( T^{(i)}_1\) sont triangulaires supérieures. Nous avons donc la nouvelle matrice \( M_1=\left( \prod_{i<i_1}T_1^{(i)} \right)M\) pour laquelle toute la première colonne est nulle sauf un élément.

		Dans le cas de l'exemple, le «pivot» sera la ligne \( (2,5,6)\) et la matrice se transforme à l'aide de la matrice \( T_1=T_{12}(-1/2)\) :
		\begin{equation}    \label{EqyjXIYf}
			\begin{pmatrix}
				1 & -1/2 & 0 \\
				0 & 1    & 0 \\
				0 & 0    & 1
			\end{pmatrix}
			\begin{pmatrix}
				1 & 3 & 4 \\
				2 & 5 & 6 \\
				0 & 7 & 8
			\end{pmatrix}=
			\begin{pmatrix}
				0 & 1/2 & 1 \\
				2 & 5   & 6 \\
				0 & 7   & 8
			\end{pmatrix}.
		\end{equation}

		Maintenant nous faisons de même avec les colonnes (en renommant \( M\) la matrice obtenue à l'étape précédente) :
		\begin{equation}
			C_j\to C_j-\frac{ M_{i_1j} }{ M_{i_11} }C_1,
		\end{equation}
		qui revient à multiplier à droite par les matrices \( T_{1j}(\frac{ M_{i_1i} }{ M_{i_11} })\) avec \( j>1\). Encore une fois ce sont des matrices triangulaires supérieures.

		Dans l'exemple, pour traiter la seconde colonne, nous multiplions \eqref{EqyjXIYf} à droite par la matrice \( T_{12}(-5/2)\) :
		\begin{equation}
			\begin{pmatrix}
				0 & 1/2 & 1 \\
				2 & 5   & 6 \\
				0 & 7   & 8
			\end{pmatrix}
			\begin{pmatrix}
				1 & -5/2 & 0 \\
				0 & 1    & 0 \\
				0 & 0    & 1
			\end{pmatrix}=
			\begin{pmatrix}
				0 & 1/2 & 1 \\
				2 & 0   & 6 \\
				0 & 7   & 8
			\end{pmatrix}.
		\end{equation}
		Appliquer encore la matrice \( T_{13}(-6/2)\) apporte la matrice
		\begin{equation}
			\begin{pmatrix}
				0 & 1/2 & 1 \\
				2 & 0   & 0 \\
				0 & 7   & 8
			\end{pmatrix}.
		\end{equation}
		Enfin nous multiplions la matrice obtenue par \( \frac{1}{ M_{i_11} }\mtu\) pour normaliser à \( 1\) l'élément «pivot» que nous avions choisi. Dans notre exemple nous multiplions par \( 1/2\) pour trouver
		\begin{equation}        \label{Eqduglwu}
			\begin{pmatrix}
				0 & 1/4 & 1/2 \\
				1 & 0   & 0   \\
				0 & 7/2 & 4
			\end{pmatrix}.
		\end{equation}

		La matrice obtenue jusqu'ici possède une ligne et une colonne de zéros avec un \( 1\) à leur intersection, et elle est de la forme
		\begin{equation}
			M'=T_1MT_2
		\end{equation}
		où \( T_1\) et \( T_2\) sont triangulaires supérieures et inversibles, produits de matrices de transvection (et d'une matrice scalaire pour la normalisation).

		Il reste à recommencer l'opération avec la seconde colonne (qui n'est pas toute nulle parce que le déterminant est encore non nul) puis la suivante, etc. Dans notre exemple de l'équation \eqref{Eqduglwu}, nous éliminerions le \( 1/4\) et le \( 4\) en utilisant le \( 7/2\).

		Encore une fois tout cela se fait à l'aide de matrices supérieures parce qu'à chaque étape, les colonnes précédent le pivot sont déjà nulles (sauf un \( 1\)) et ne doivent donc pas être touchées.

		À la fin de ce processus, ce qui reste est une matrice \( TMT'\) qui ne contient plus qu'un seul \( 1\) sur chaque ligne et chaque colonne, c'est-à-dire une matrice de permutations : \( P_{\sigma}=TMT'\) et donc
		\begin{equation}
			M=T^{-1}_{\sigma}(T')^{-1}.
		\end{equation}

		\spitem[Unicité]

		Soient \( \sigma,\sigma\in S_n'\) tels que \( T_1P_{\sigma}T_2=S_1P_{\tau}S_2\) avec \( T_i\) et \( S_i\) triangulaires supérieures et inversibles. En posant \( T=T_2S_2^{-1}\) et \( S=T_1^{-1}S_1\), nous avons
		\begin{equation}
			P_{\sigma}T=SP_{\tau}
		\end{equation}
		où \( S\) et \( T\) sont des matrices triangulaires supérieures et inversibles. Par les calculs de la preuve du lemme~\ref{LemyrAXQs},
		\begin{subequations}
			\begin{numcases}{}
				(P_{\sigma}T)_{kl}=T_{\sigma^{-1}(k)l}\\
				(SP_{\tau})_{kl}=S_{k\tau(l)},
			\end{numcases}
		\end{subequations}
		et donc
		\begin{equation}    \label{EqKlmgOT}
			T_{\sigma^{-1}(k)l}=S_{k\tau(l)}.
		\end{equation}
		En écrivant cette équation avec \( k=\sigma(i)\) (nous rappelons que \( \sigma\) est bijective),
		\begin{equation}
			T_{il}=S_{\sigma(i)\tau(l)}.
		\end{equation}
		Nous savons que les termes diagonaux de \( T\) sont non nuls parce que \( T\) est triangulaire supérieure et inversible (donc pas de colonnes entières nulles). Nous avons donc, en prenant \( i=l=k\),
		\begin{equation}
			0\neq T_{kk}=S_{\sigma(k)\tau(k)}.
		\end{equation}
		La matrice étant triangulaire supérieure, cela implique
		\begin{equation}    \label{EqEmiBTX}
			\sigma(k)\leq\tau(k).
		\end{equation}
		De la même manière en écrivant \eqref{EqKlmgOT} avec \( l=\tau^{-1}(i)\),
		\begin{equation}
			S_{ki}=T_{\sigma^{-1}(k)\tau^{-1}(i)}
		\end{equation}
		et donc
		\begin{equation}
			\sigma^{-1}(k)\leq \tau^{-1}(k).
		\end{equation}
		En écrivant cela avec \( k=\sigma(j)\), nous avons \( j\leq \tau^{-1}\sigma(j)\) et en appliquant enfin \( \tau\),
		\begin{equation}
			\tau(j)\leq \sigma(j).
		\end{equation}
		En comparant avec \eqref{EqEmiBTX}, nous avons \( \sigma=\tau\).
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sous-groupes du groupe linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{KXjFWKA}]       \label{LemOCtdiaE}
	Soit \( V\) un espace vectoriel de dimension finie muni d'une norme euclidienne \( \| . \|\). Soit \( K\) un compact convexe de \( V\) et \( G\), un sous-groupe compact de \( \GL(V)\) tel que
	\begin{equation}
		u(K)\subset K
	\end{equation}
	pour tout \( u\in G\). Alors il existe \( a\in K\) tel que \( u(a)=a\) pour tout \( u\in G\).
\end{lemma}
\index{groupe!linéaire!sous-groupes compacts}
\index{compacité!sous-groupes du groupe linéaire}

\begin{proof}
	Avant de nous lancer dans la preuve, nous avons besoin d'un petit résultat.
	\begin{subproof}
		\spitem[Un pré-résultat]

		Nous commençons par prouver que si \( v\in \aL(V)\) vérifie \( v(K)\subset K\), alors \( v\) a un point fixe dans \( K\). Pour cela nous considérons \( x_0\in K\) et la suite
		\begin{equation}
			x_k=\frac{1}{ k+1 }\sum_{i=0}^kv^i(x_0).
		\end{equation}
		Étant donné que \( K\) est convexe et stable par \( v\), la suite \( (x_k)\) est contenue dans \( K\) et accepte une sous-suite convergente\footnote{C'est Bolzano-Weierstrass, théorème~\ref{ThoBWFTXAZNH}.} que nous allons noter \( x_{\varphi(n)}\) avec \( \varphi\colon \eN\to \eN\) strictement croissante. Soit \( a\in K\) la limite :
		\begin{equation}
			\lim_{n\to \infty} x_{\varphi(n)}=a.
		\end{equation}
		Tant que nous y sommes nous pouvons aussi calculer \( v(x_k)\) :
		\begin{subequations}
			\begin{align}
				v(x_k) & =v\left( \frac{1}{ k+1 }\sum_{i=1}^kv^i(x_0) \right)                     \\
				       & =\frac{1}{ k+1 }\sum_{i=0}^kv^{i+1}(x_0)                                 \\
				       & =x_k+\frac{1}{ k+1 }\Big( v^{k+1}(x_0)-x_0 \Big).      \label{EqUAfcaKG}
			\end{align}
		\end{subequations}
		La norme \( \| v^{k+1}(x_0)-x_0 \|\) est bornée par le diamètre de \( K\), donc en prenant la limite \( k\to \infty\) le second terme de \eqref{EqUAfcaKG} tend vers zéro. En prenant ces égalités en \( k=\varphi(n)\) et en prenant \( n\to\infty\), nous trouvons
		\begin{equation}
			v(a)=a,
		\end{equation}
		c'est-à-dire le résultat que nous voulions dans un premier temps.

		\spitem[Une norme sur \( V\)]

		Nous passons maintenant à la preuve du lemme. D'abord nous remarquons que le groupe \( G\) agit sur \( V\) par \( u\cdot x=u(x)\) et de plus, considérant la fonction continue
		\begin{equation}
			\begin{aligned}
				\alpha\colon G & \to V         \\
				u              & \mapsto u(x),
			\end{aligned}
		\end{equation}
		nous voyons que les orbites de cette action sont compactes en tant qu'image par \( \alpha\) du compact \( G\) (théorème~\ref{ThoImCompCotComp}). Nous posons
		\begin{equation}
			\begin{aligned}
				\nu\colon V & \to \eR^+                        \\
				x           & \mapsto \max_{u\in G}\| u(x) \|.
			\end{aligned}
		\end{equation}
		Cette définition a un sens parce que l'orbite \( \{ u(x)\tq u\in G \}\) est compacte dans \( V\) et donc l'ensemble des normes est compact dans \( \eR\) et admet un maximum. De plus cela donne une norme sur \( V\) parce que nous vérifions les conditions de la définition~\ref{DefNorme} :
		\begin{enumerate}
			\item
			      Pour tout \( x,y\in V\) nous avons :
			      \begin{equation}
				      \nu(x+y)=\max_{u\in G}\| u(x)+u(y) \|\leq \max_{u\in G}\left( \| u(x) \|+\| u(y) \| \right)\leq \nu(x)+\nu(y).
			      \end{equation}
			\item
			      Si \( \nu(x)=0\), alors l'égalité \( \max_{u\in G}\| u(x) \|=0\) nous enseigne que \( \| u(x) \|=0\) pour tout \( u\in G\) et donc en particulier avec \( u=\id\) nous trouvons \( x=0\).
			\item
			      Pour tout \( \lambda\in \eR\) et \( x\in V\),
			      \begin{equation}
				      \nu(\lambda x)=\max_{u\in G}\| u(\lambda x) \|=\max\| \lambda u(x) \|=\max| \lambda |\| u(x) \|=| \lambda |\nu(x).
			      \end{equation}
		\end{enumerate}
		De plus la fonction \( \nu\) est constante sur les orbites de \( G\).

		\spitem[Un point fixe]

		Pour tout \( u\in G\) nous posons
		\begin{equation}
			F_u=\{ x\in K\tq u(x)=x \};
		\end{equation}
		par le pré-résultat, aucun de ces ensembles n'est vide. Ils sont de plus tous fermés par continuité de \( u\) (le complémentaire est ouvert). Nous devons prouver que \( \bigcap_{u\in G}F_u\neq \emptyset\) parce qu'une intersection serait un point fixe de tous les éléments de \( G\). Supposons donc que \( \bigcap_{u\in G}F_u=\emptyset\). Alors les complémentaires des \( F_u\) forment un recouvrement ouvert de \( K\) et nous pouvons en extraire un sous-recouvrement fini par compacité. Soient \( \{ u_i \}_{i=1,\ldots, p}\) les éléments qui réalisent ce recouvrement. Alors
		\begin{equation}
			\bigcap_{i=1}^pF_{u_i}=\emptyset.
		\end{equation}
		Nous considérons l'opérateur
		\begin{equation}
			v=\frac{1}{ p }\sum_{i=1}^pu_i\in\aL(V).
		\end{equation}
		Puisque \( K\) est convexe et stable sous chacun des \( u_i\), nous avons aussi \( v(K)\subset K\) et donc il existe \( a\in K\) tel que \( v(a)=a\). Pour ce \( a\), nous avons
		\begin{subequations}
			\begin{align}
				\nu\big( v(a) \big) & =\nu\left( \frac{1}{ p }\sum_{i=1}^pu_i(a) \right)      \label{EqDXSnwPb} \\
				                    & \leq \frac{1}{ p }\sum_{i=1}^p\nu\left( u_i(a) \right)                    \\
				                    & =\frac{1}{ p }\sum_{i=1}^p\nu(a)                                          \\
				                    & =\nu(a)
			\end{align}
		\end{subequations}
		où nous avons utilisé la constance de \( \nu\) sur les orbites de \( G\). Par ailleurs nous savons que \( v(a)=a\), donc en réalité à gauche dans \eqref{EqDXSnwPb} nous avons \( \nu(a)\) et toutes les inégalités sont des égalités. Nous avons en particulier
		\begin{equation}        \label{EqBMjypoV}
			\nu\left( \sum_{i=1}^pu_i(a) \right) =\sum_{i=1}^p\nu\left( u_i(a) \right).
		\end{equation}
		Notons \( u_0\in G\) l'élément qui réalise le maximum de la définition de \( \nu\) pour le vecteur \( \sum_iu_i(a)\) :
		\begin{equation}
			\nu\left( \sum_i u_i(a) \right)=\| u_0\left( \sum_iu_i(a) \right) \|\leq\sum_i\| u_0u_i(a) \|\leq \sum_i\nu\big( u_i(a) \big).
		\end{equation}
		Mais nous venons de voir (équation \eqref{EqBMjypoV}) que l'expression de gauche est égale à celle de droite. Donc les inégalités sont des égalités et en particulier la première inégalité devient l'égalité
		\begin{equation}
			\| \sum_iu_0u_i(a)  \|=\sum_i\| u_0u_i(a) \|.
		\end{equation}
		En vertu du lemme~\ref{LemLPOHUme}, il existe des nombres positifs \( \lambda_i\) tels que
		\begin{equation}
			u_0u_1(a)=\lambda_2u_0u_2(a)=\ldots =\lambda_pu_0u_p(a).
		\end{equation}
		Du fait que \( u_0\) est inversible nous avons aussi
		\begin{equation}       \label{EqSTQwfIl}
			u_1(a)=\lambda_2u_2(a)=\ldots =\lambda_pu_p(a).
		\end{equation}
		Mais par constance de \( \nu\) sur les orbites nous avons \( \nu(u_i(a))=\nu(u_j(a))\) pour tout \( i\) et \( j\); en appliquant \( \nu\) à la série d'égalités \eqref{EqSTQwfIl}, nous trouvons que tous les \( \lambda_i\) doivent être égaux à \( 1\). En particulier
		\begin{equation}
			u_1(a)=u_2(a)=\ldots =u_p(a).
		\end{equation}

		Nous récrivons maintenant l'équation \( v(a)=a\) avec la définition de \( v\) :
		\begin{equation}
			a=v(a)=\frac{1}{ p }\sum_{i=1}^pu_i(a)=u_j(a)
		\end{equation}
		pour n'importe quel \( j\). Donc
		\begin{equation}
			a\in\bigcap_{i=1}^pF_{u_i},
		\end{equation}
		ce qui contredit notre hypothèse de départ.
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{NHXUsTa,KXjFWKA,RXvMqkd}]     \label{PropQZkeHeG}
	Soit \( G\) un sous-groupe compact de \( \GL(n,\eR)\). Alors
	\begin{enumerate}
		\item
		      Il existe une forme quadratique définie positive \( q\) sur \( \eR^n\) telle que \( G\subset \gO(q)\).
		\item
		      Le groupe \( G\) est conjugué à un sous-groupe de \( \gO(n,\eR)\).
	\end{enumerate}
\end{proposition}
\index{groupe!action!utilisation}
\index{matrice!équivalence!dans le groupe linéaire}
\index{forme!quadratique!groupe orthogonal}
\index{groupe!orthogonal!d'une forme quadratique}
\index{endomorphisme!préservant une forme quadratique}

\begin{proof}
	Nous considérons le (pas tout à fait) morphisme de groupe
	\begin{equation}
		\begin{aligned}
			\rho\colon G & \to \GL\big( \gS(n,\eR) \big)         \\
			u            & \mapsto \rho_u\colon s\mapsto  u^tsu,
		\end{aligned}
	\end{equation}
	et tant que nous en sommes à considérer, nous considérons l'ensemble
	\begin{equation}
		H=\{ M^tM\tq M\in G \}\subset \gS(n,\eR).
	\end{equation}
	Cet ensemble est constitué de matrices définies positives parce que si \( \langle M^tMx, x\rangle =0\), alors \(0= \langle Mx, Mx\rangle =\| Mx \|\), mais \( M\) étant inversible, cela implique que \( x=0\). Qui plus est, cet ensemble est compact dans \( \GL(n,\eR)\) en tant qu'image du compact \( G\) par l'application continue \( M\mapsto M^tM\). L'enveloppe convexe \( K=\Conv(H)\) est alors également compacte par le théorème~\ref{CorOFrXzIf}. Enfin nous considérons \( L=\rho(G)\), qui est un sous-groupe compact de \( \GL\big( \gS(n,\eR) \big)\) parce que \( \rho_u\rho_v=\rho_{vu}\in\rho(G)\). Nous remarquons que \( \rho_u\) étant linéaire, elle préserve les combinaisons convexes et donc pour tout \( u\in G\), \( \rho_u(K)\subset K\).

	Bref, \( L\) est un sous-groupe compact de \( \GL(n,\eR)\) préservant le compact \( K\) de \( \gS(n,\eR)\). Par le lemme~\ref{LemOCtdiaE}, il existe \( s\in K\) tel que \( \rho_u(s)=s\) pour tout \( u\in G\). Ou encore :
	\begin{equation}
		u^tsu=s
	\end{equation}
	pour tout \( u\in G\). Fort de ce \( s\) bien particulier, nous considérons la forme quadratique associée : \( q(x)=x^tsx\). Cette forme est définie positive parce que \( s\) l'est. Nous avons \( G\subset \gO(q)\) parce que si \( u\in G\) alors
	\begin{equation}
		q\big( ux \big)=(ux)^tsux=x^t\underbrace{u^tsu}_{=s}x=q(x).
	\end{equation}
	Le premier point est prouvé.

	La matrice \( s\) est symétrique et définie positive. Le théorème \ref{ThoeTMXla} nous permet donc de la diagonaliser en \( \diag(\lambda_1,\ldots, \lambda_n)\) avec \( \lambda_i>0\), et ensuite transformée en la matrice \( \mtu_n\) par la matrice \( \diag(1/\sqrt{\lambda_i})\). Nous avons donc une matrice \( a\in\GL(n,\eR)\) telle que \( a^tsa=\mtu_n\). Avec ça, si \( u\in G\), nous avons
	\begin{equation}
		(a^{-1}ua)^t(a^{-1} ua)=(a^{-1}ua)^t\mtu_n(a^{-1} ua)=a^tu^t(a^t)^{-1}a^tsaa^{-1}ua=a^tu^tsua=a^tsa=\mtu,
	\end{equation}
	ce qui prouve que \( a^{-1} ua\) est dans \( \gO(n,\eR)\), et donc que \( a^{-1} G a\subset \gO(n,\eR)\).
\end{proof}
