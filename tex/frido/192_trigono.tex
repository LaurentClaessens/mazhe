% This is part of (everything) I know in mathematics
% Copyright (c) 2011-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Un peu de structure de \texorpdfstring{\( \gO(n)\)}{O(n)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Valeurs propres dans \( \gO(n)\)}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{ooPWOHooHwgPzO}]      \label{PROPooVEJGooWnqtMm}
    Soit une matrice \( A\in O(n)\). Si \( \lambda\in \eC\) est une valeur propre de \( A\), alors \( \bar\lambda\) est également une valeur propre de \( A\), et de plus \( | \lambda |=1\).
\end{proposition}

\begin{proof}
    Dire que \( \lambda\in \eC\) est une valeur propre de \( A\) signifie qu'il existe \( x\in \eC^n\) (non nul) tel que \( Ax=\lambda x\). Vu que les éléments de la matrice \( A\) sont réels,
    \begin{equation}
        A\bar x=\bar A\bar x=\overline{ Ax }=\overline{ \lambda x }=\bar \lambda\bar x.
    \end{equation}
    Donc \( \bar \lambda\) est une valeur propre de \( A\) pour le vecteur propre \( \bar x\).

    Soit \( \lambda\)  une valeur propre de \( A\) de vecteur propre \( x\). Alors nous avons d'une part
    \begin{equation}
        \langle \overline{ Ax }, Ax\rangle =\langle A^tA\bar x, x\rangle =\langle x, \bar x\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle \overline{ Ax }, Ax\rangle =\langle \bar \lambda \bar x, \lambda x\rangle =| \lambda |^2\langle \bar x, x\rangle .
    \end{equation}
    Vu que \( x\neq 0\) nous avons aussi \( \langle \bar x, x\rangle \neq 0\). Par conséquent \( | \lambda |^2=1\) et \( | \lambda |=1\).
\end{proof}

\begin{lemma}[\cite{ooPWOHooHwgPzO}]        \label{LEMooNEDQooNRmASH}
    Soit un espace vectoriel euclidien \( E\) de dimension finie et une isométrie \( f\) de \( E\). Soit \( F\) un sous-espace de \( E\) stable par \( f\). Alors \( F^{\perp}\) est stable par \( f\).
\end{lemma}

\begin{proof}
    La restriction \( f_F\colon F\to F\) est encore une isométrie; elle est donc inversible : pour tout $y\in F$, il existe \( x\in F\) tel que \( y=f(x)\). Soit \( a\in F^{\perp}\); nous montrons que \( f(a)\in F^{\perp}\). Soit donc \( y\in F\) et calculons :
    \begin{equation}
        \langle y, f(a)\rangle =\langle f(x), f(a)\rangle =\langle x,a, \rangle =0
    \end{equation}
    parce que \( x\in F^{\perp}\).
\end{proof}

\begin{proposition}[\cite{ooPWOHooHwgPzO}]      \label{PROPooOMORooWzsrDB}
    Soit une isométrie \( f\colon \eR^3\to \eR^3\).
    \begin{enumerate}
        \item
            L'application linéaire \( f\) possède au moins une valeur propre réelle qui vaut \( \pm 1\).
        \item
            Il existe une base orthonormée de \( \eR^3\) dans laquelle la matrice de \( f\) est de la forme
            \begin{equation}
                \begin{pmatrix}
                    \lambda    &   0    &   0    \\
                    0    &   \cos(\theta)    &   -\epsilon\cos(\theta)    \\
                    0    &   \sin(\theta)    &   \epsilon\cos(\theta)
                \end{pmatrix}
            \end{equation}
            avec \( \epsilon,\lambda=\pm 1\) et \( \theta\in \mathopen[ 0 , 2\pi \mathclose[\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le polynôme caractéristique de \( f\), donné par \( \det(f-\lambda\id)\), est à coefficients réels et de degré \( 3\). Il possède dont au moins une solution réelle par le corollaire~\ref{CORooKKNWooWEQukb}. Soit donc une valeur propre réelle \( \lambda\) de \( \chi_f\); par le lemme~\ref{PROPooVEJGooWnqtMm} nous avons \( \lambda=\pm 1\). Soit \( u_1\) le vecteur propre correspondant. Nous notons \( F\) l'espace engendré par \( u_1\).

    Nous avons \( f(F)=F\) et donc \( f(F^{\perp})=F^{\perp}\) par le lemme~\ref{LEMooNEDQooNRmASH}. Soit une base orthonormée \( \{ u_2,u_3 \}\) de \( F^{\perp}\) et la matrice \( B\) de la restriction \( f_{p}\) à \( F^{\perp}\). Vu que l'application \( f_p\) est une isométrie de \( F^{\perp}\), la matrice \( B\) est, par le lemme~\ref{LEMooAJMAooXPSKtS}, de la forme
    \begin{equation}
        B=\begin{pmatrix}
            \cos(\theta)    &   -\epsilon\sin(\theta)    \\
            \sin(\theta)    &   \epsilon\cos(\theta)
        \end{pmatrix}
    \end{equation}
    pour un certain \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) et \( \epsilon=\pm 1\).

    Dans la base \( \{u_1,u_2,u_3\}\) de \( \eR^3\), la matrice de \( f\) est alors
    \begin{equation}
        \begin{pmatrix}
            \lambda    &   0    \\
            0    &   B
        \end{pmatrix},
    \end{equation}
    comme annoncé.
\end{proof}

Pour classifier les isométries de \( \eR^3\), nous pouvons nous baser sur les possibilités de la matrice donne dans le lemme~\ref{PROPooOMORooWzsrDB}. Il y a essentiellement quatre possibilités suivant les valeurs de \( \lambda=\pm 1\) et \( \epsilon=\pm 1\).

\begin{subproof}
    \item[Si \( \epsilon=\lambda=1\)] Alors la matrice est
    \begin{equation}
        A=\begin{pmatrix}
            1    &   0    &   0    \\
            0    &   \cos(\theta)    &   -\sin(\theta)    \\
            0    &   \sin(\theta)    &   \cos(\theta)
        \end{pmatrix}
    \end{equation}
    et l'isométrie correspondante est la rotation d'angle \( -\theta\) autour de la droite de \( u_1\).

    \item[Si \( \epsilon=\lambda=-1\)]
    Alors la matrice est
    \begin{equation}
        A=\begin{pmatrix}
            -1    &  0     &   0    \\
            0    &   \cos(\theta)    &   \sin(\theta)    \\
            0    &   \sin(\theta)    &   -\cos(\theta)
        \end{pmatrix}
    \end{equation}
    Cette application est plus subtile, parce que même dans le plan \( \Span(u_2,u_3)\), ce n'est pas une rotation. Nous allons montrer qu'il s'agit d'une réflexion autour de la droite d'angle \( \theta/2\) dans le plan \( \Span(u_2,u_3)\). Nous nommons \( D\) cette droite. Dans la base \( \{ u_1,u_2,u_3 \}\), les points de cette droite sont de la forme\footnote{Les plus acharnés remarquerons que \( \{ u_1,u_2,u_3 \}\) est un ensemble, qui est une base. Mais un ensemble n'est pas ordonné, alors que pour écrire l'équation de droite qui suit, nous supposons un ordre. Je laisse au tel lecteur le soin de trouver une bonne notation.}
    \begin{equation}
        \big( 0,\cos(\theta/2),\sin(\theta/2) \big).
    \end{equation}

    L'image de \( u_1\) par cette réflexion est \(-u_1\), c'est clair.

    Faisons en détail l'image de \( u_3\). Nous devons démontrer que la droite \( D\) coupe le segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) perpendiculairement en son milieu.

    Dans le plan \( \Span(u_2,u_3)\) nous avons \( u_3=\begin{pmatrix}
        0    \\
        1
    \end{pmatrix}\) et \( A(u_3)=\begin{pmatrix}
        \sin(\theta)    \\
        \cos(\theta)
    \end{pmatrix}\). Le milieu du segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) est le point
    \begin{equation}
        M=\left( \frac{ \sin(\theta) }{2},\frac{ 1-\cos(\theta) }{2} \right).
    \end{equation}
    Les formules de duplication d'angle du corollaire~\ref{CORooQZDQooWjMXTF} nous permettent d'écrire \( \sin(\theta)\) et \( \cos(\theta)\) en fonction de \( \sin(\theta/2)\) et \( \cos(\theta/2)\), et donc d'exprimer le point \( M\) de la façon suivante :
    \begin{subequations}
        \begin{align}
            M&=\left( \cos(\theta/2)\sin(\theta/2),\frac{ 1-\big( \cos^2(\theta/2)-\sin^2(\theta/2) \big) }{2} \right)\\
            &=\big( \cos(\theta/2)\sin(\theta/2),\sin^2(\theta/2) \big)\\
            &=\sin(\theta/2)\big( \cos(\theta/2),\sin(\theta/2) \big).
        \end{align}
    \end{subequations}
    Ce point fait donc partie de la droite \( D\). La droite \( D\) coupe le segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) en son milieu.

    En ce qui concerne l'orthogonalité, nous calculons le produit scalaire
    \begin{equation}
            \big( A(u_3)-u_3 \big)\cdot\begin{pmatrix}
                \cos(\theta/2)    \\
                \sin(\theta/2)
            \end{pmatrix}
            =\sin(\theta)\cos(\theta/2)-\big( 1+\cos(\theta) \big)\sin(\theta/2)=0
    \end{equation}
    où nous avons encore utilisé les duplications d'angles et le fait que \( 1=\cos^2(\theta/2)+\sin^2(\theta/2)\) (lemme~\ref{LEMooAEFPooGSgOkF}).

    \item[Si \( \epsilon=-1\) et \( \lambda=1\)] Alors la matrice est
        \begin{equation}
            A=\begin{pmatrix}
                1    &   0    &   0    \\
                0    &   \cos(\theta)    &   \sin(\theta)    \\
                0    &   \sin(\theta)    &   -\cos(\theta)
            \end{pmatrix}.
        \end{equation}
        C'est la symétrie orthogonale par le plan engendré par \( u_1\) et \( v=\cos(\theta/2)u_2+\sin(\theta/2)u_3\).

        Le vecteur \( u_1\) est bien évidemment préservé par \( A\). En ce qui concerne le vecteur \( v\),
        \begin{equation}
            A(v)=\cos(\theta/2)\begin{pmatrix}
                0    \\
                \cos(\theta)    \\
                \sin(\theta)
            \end{pmatrix}+\sin(\theta/2)\begin{pmatrix}
                0    \\
                -\sin(\theta)    \\
                \cos(\theta)
            \end{pmatrix}=
            \begin{pmatrix}
                0    \\
                \cos(\theta/2)    \\
                \sin(\theta/2)
            \end{pmatrix}=v.
        \end{equation}
        Nous avons sauté quelques étapes de calcul mettant en scène les formules de duplication d'angle : exprimer \( \cos(\theta)=\cos^2(\theta/2)-\sin^2(\theta/2)\) et \( \sin(\theta)=2\cos(\theta/2)\sin(\theta/2)\).

        Pour achever, nous devons trouver un vecteur \( w\) perpendiculaire au plan, et montrer qu'il est envoyé par \( A\) sur \( -w\). Un vecteur \( w=xu_1+yu_2+zu_3\) est perpendiculaire au plan si les deux égalités suivantes sont satisfaites :
        \begin{subequations}
            \begin{align}
                \big( \cos(\theta/2)u_2+\sin(\theta/2)u_3 \big)\cdot (xu_1+yu_2+zu_3)=0\\
                u_1\cdot(xu_1+yu_2+zu_3)=0.
            \end{align}
        \end{subequations}
        Nous avons immédiatement \( x=0\) et ensuite la relation
        \begin{equation}        \label{EQooXQMDooTvwrWk}
            y\cos(\theta/2)+z\sin(\theta/2)=0.
        \end{equation}
        En ne regardant que les deux dernières composantes pour alléger l'écriture,
        \begin{equation}
            A(w)=y\begin{pmatrix}
                \cos(\theta)    \\
                \sin(\theta)
            \end{pmatrix}+z\begin{pmatrix}
                \sin(\theta)    \\
                -\cos(\theta)
            \end{pmatrix}.
        \end{equation}
        Le but est de montrer que cela est égal à \( -y\cos(\theta/2)-z\sin(\theta/2)\).

        Notons \( c=\cos(\theta/2)\) et \( s=\sin(\theta/2)\). Alors \( A(w)_2=y(c^2-s^2)+2zcs\). Évacuons tout de suite les deux cas limite : si \( c=0\) alors \( A(w)_2=-y\) (parce que \( s=\pm1\)) et c'est bon. Si \( s=0\), alors \( A(w)_2=y\), mais la relation \eqref{EQooXQMDooTvwrWk} donne \( y=0\), donc c'est bon aussi. Dans la cas générique, \( z=-yc/2\) et
        \begin{equation}
            A(w)_2=y(c^2-s^2)-2cs\frac{ yc }{ s }=-y(c^2+s^2)=-y.
        \end{equation}

        En ce qui concerne \( A(w)_3\), c'est très similaire :
        \begin{equation}
            A(w)_3=2ysc-z(c^2-s^2).
        \end{equation}
        Avec \( z=0\) c'est \( -z\), donc c'est bon. Avec \( c=0\) c'est \( z\) mais \( z=0\). Et pour le cas générique, la substitution \( y=-zs/c\) donne le résultat.


    \item[Si \( \epsilon=1\) et \( \lambda=-1\)] Alors la matrice est
        \begin{equation}
            A=\begin{pmatrix}
                -1    &   0    &   0    \\
                0    &   \cos(\theta)    &   -\sin(\theta)    \\
                0    &   \sin(\theta)    &   \cos(\theta)
            \end{pmatrix}.
        \end{equation}
        Cela est la composition entre la symétrie de plan \( \Span(u_2,u_3)\) et la rotation d'angle \( \theta\) dans ce plan.
\end{subproof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Sous-groupes finis de \( \SO(3)\)}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooWIMMooXOCfSt}
    Points fixes pour \( \SO(3)\).
    \begin{enumerate}
        \item
            Tout élément de \( \SO(3)\) possède une droite de point fixes.
        \item
            Tout élément non trivial de \( \SO(3)\) possède une seule droite de points fixes.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Le polynôme caractéristique d'un élément de \( \SO(3)\) est de degré trois et possède dont (en comptant les multiplicités) trois racines dont une réelle par le corollaire~\ref{CORooKKNWooWEQukb}. Vu que nous sommes en dimension impaire, le coefficient du terme de degré \( 3\) est \( -1\) et le polynôme caractéristique de \( g\in\SO(3)\) s'écrit
    \begin{equation}
        \chi_g(X)=-(X-\lambda_1)(X-\bar\lambda_1)(X-s)
    \end{equation}
    avec \( s=\pm1 \) que nous allons tout de suite fixer. Nous savons que \( \det(g)=\chi_g(0)\) mais aussi que \( \det(g)=1\). Donc
    \begin{equation}
        1=\det(g)=\chi_g(0)=\lambda_1\bar\lambda_1 s=s.
    \end{equation}
    Tout cela pour dire que tout élément de \( \SO(3)\) possède une valeur propre égale à \( 1\), et donc une droite de points fixes.

    Pour continuer, supposons que \( g\) possède deux droites distinctes de points fixes. En particulier \( g\) fixe un plan. Une base orthonormée de \( \eR^3\) peut être choisie en prenant deux vecteurs \( e_1\), \( e_2\) dans ce plan et un vecteur \( e_3\) perpendiculaire au plan.

    Vu que \( g\) est une isométrie, la base reste orthonormée sous l'action de \( g\). Donc \( g\) a pour matrice
    \begin{equation}
        \begin{pmatrix}
            1    &   0    &   0    \\
            0    &   1    &   0    \\
            0    &   0    &   \pm 1
        \end{pmatrix}.
    \end{equation}
    Pour que le déterminant soit \( 1\), il faut que la matrice soit l'identité.
\end{proof}

\begin{proposition}[\cite{MonCerveau,ooYODPooHeNKiQ,fJhCTE,ooBWVZooJiWRvf,ytMOpe}]
    Les sous-groupes finis de \( \SO(3)\) sont :
    \begin{multicols}{2}
        \begin{enumerate}
            \item
                les groupes cycliques \( \eZ/n\eZ\),
            \item
                les groupes diédraux \( D_n\),
            \item
                le groupe alterné \( A_4\),
            \item
                le groupe alterné \( A_5\)
            \item
                le groups symétrique \( S_4\).
        \end{enumerate}
    \end{multicols}
\end{proposition}

\begin{proof}
    Soit \( G\), un sous-groupe fini de \( \SO(3)\). Par la proposition~\ref{PropKBCXooOuEZcS}, les éléments de \( G\) sont des isométries de \( \eR^3\), et le lemme~\ref{LEMooWIMMooXOCfSt} dit que tout élément de \( G\) possède une droite de points fixes.

    Un point de la sphère unité fixé par \( g\in G\) est un \defe{pôle}{pôle} de \( g\). Nous nommons \( \Omega\) l'ensemble des pôles des éléments non triviaux de \( G\).
    \begin{subproof}
        \item[Une action]
            Le groupe \( G\) agit sur \( \Omega\). En effet si \( x\in \Omega\), alors \( x\) est fixé par un élément \( g\). Montrons que \( h(x)\) est également fixé par un élément de \( G\). Par dur : \( (hgh^{-1})h(x)=h(x)\); donc \( h(x)\) est un pôle de \( h gh^{-1}\).

        \item[Les fixateurs sont cycliques]

            Nous montrons à présent que pour tout \( x\in\Omega\), le sous-groupe \( \Fix(x)\) est cyclique. Soit donc \( x\in\Omega\), le plan orthogonal \( \sigma=\Span(x)^{\perp}\) et \( h\in \Fix(x)\). Nous avons \( h(\sigma)=\sigma\). En effet si \( y\in \sigma\) nous avons
            \begin{equation}
                0=y\cdot x=h(y)\cdot h(x)=h(y)\cdot x,
            \end{equation}
            donc \( h(y)\) est perpendiculaire à \( x\). L'inclusion inverse se démontre de même : si \( y\in \sigma\) alors \( y=h\big( h^{-1}(y) \big)\) alors que \( h^{-1}(y)\in \sigma\).

            La restriction de \( h\) à \( \sigma\) est une isométrie de \( \sigma\). Prenant une isométrie \( f\colon \sigma\to \eR^2\), l'application
            \begin{equation}
                \begin{aligned}
                    \varphi\colon \Fix(x)&\to \SO(2) \\
                    h&\mapsto f\circ h\circ f^{-1}.
                \end{aligned}
            \end{equation}
            est un morphisme injectif de groupes. En effet nous avons d'une part
            \begin{equation}
                \varphi(hh')=f\circ h\circ h'\circ f^{-1}=fhf^{-1}fh'f^{-1}=\varphi(h)\varphi(h'),
            \end{equation}
            d'où le morphisme. Et d'autre part, si \( \varphi(h)=\varphi(h')\) alors \( f\circ h\circ f^{-1}=g\circ h'\circ f^{-1}\), qui donne immédiatement \( h=h'\).

            Nous en déduisons que \( \Fix(x)\) est isomorphe à un sous-groupe de \( SO(2)\) (l'image de \( \varphi\)). Le lemme~\ref{LEMooUKEVooAEWvlM} en fait un groupe cyclique.
        \item[Taille des fixateurs]

            Soient \( \Omega_i\) les orbites. Si \( x,y\in \Omega_i\) alors nous montrons que \( | \Fix(x) |=| \Fix(y) |\) avec la bijection
            \begin{equation}
                \begin{aligned}
                    \varphi\colon \Fix(x)&\to \Fix(y) \\
                    h&\mapsto g^{-1} hg
                \end{aligned}
            \end{equation}
            où \( g\) est choisi de façon à avoir \( y=g(x)\) (possible parce que \( x\) et \( y\) sont dans la même orbite). Cela est surjectif parce que si \( k\in\Fix(x)\) alors \( k=\varphi(gkg^{-1})\) et l'on vérifie que \( gkg^{-1}\in\Fix(y)\). L'application \( \varphi\) est également injective parce que si \( ghg^{-1}=gh'g^{-1}\) alors \( h=h'\).

        \item[Un peu de notations]
            Vu que tous les fixateurs des éléments d'une orbite ont la même taille (finie), nous pouvons noter
            \begin{equation}
                n_i=| \Fix(x_i) |
            \end{equation}
            pour \( x_i\in \Omega_i\). Nous notons également \( r\) le nombre d'orbites de \( G\).

            La formule de Burnside du théorème~\ref{THOooEFDMooDfosOw}, avec les notations d'ici, donne
            \begin{equation}
                r=\frac{1}{ | G | }\sum_{g\in G}| \Fix(g) |.
            \end{equation}

        \item[Une belle formule]

            Soit l'ensemble
            \begin{equation}
                A=\{ (g,x)\tq g\in G\setminus\{ e \}, x\in \Fix(g) \}
            \end{equation}
            où par \( \Fix(g)\) nous entendons les pôles de \( G\) fixés par \( g\).

            Il y a \( | G |-1\) possibilités pour la composante \( g\), mais chaque élément \( g\neq e\) possède exactement deux pôles, donc l'ensemble \( A\) contient exactement \( 2(| G |-1)\) éléments.

            Nous pouvons calculer le nombre d'éléments dans \( A \) d'une autre façon : pour chaque \( x\in \Omega\) nous avons \( | \Fix(x) |-1\) éléments de \( G\setminus\{ e \}\) qui fixent \( x\). Donc
            \begin{equation}
                | A |=\sum_{x\in\Omega}\big( | \Fix(x) |-1 \big).
            \end{equation}
            Mais \( | \Fix(x) |\) est constant sur les orbites. Nous coupons donc la somme sur \( \Omega\) en plusieurs sommes sur les orbites \( \Omega_i\) :
            \begin{equation}
                | A |=\sum_{i=1}^r\sum_{x\in \Omega_i}\big( | \Fix(x) |-1 \big)=\sum_i| \Omega_i |(n_i-1).
            \end{equation}
            En égalisant les deux façons de calculer \( | A |\), nous déduisons la formule
            \begin{equation}        \label{EQooHMLJooTGRBAl}
                2\big( | G |-1 \big)=\sum_i| \Omega_i |(n_i-1).
            \end{equation}

            Nous utilisons ensuite la relation orbite-stabilisateur, proposition~\ref{Propszymlr} : \( | \Fix(x_i) | |\Omega_i |=| G |\); la formule \eqref{EQooHMLJooTGRBAl} devient
            \begin{equation}
                2\big( | G |-1 \big)=\sum_i| G |-\sum_i\frac{ | G | }{ n_i }=r| G |+| G |\sum_i\frac{1}{ n_i },
            \end{equation}
            ou encore, en simplifiant par \( | G |\) :
            \begin{equation}        \label{EQooAMVBooDVcYeG}
                2-\frac{ 2 }{ | G | }=r-\sum_i\frac{1}{ n_i }=\sum_{i=1}^r\left( 1-\frac{1}{ n_i } \right).
            \end{equation}

            Nous pouvons aussi repartir de \eqref{EQooHMLJooTGRBAl} et sommer de façon plus simple \( \sum_i| \Omega_i |=| \Omega |\) pour obtenir
            \begin{equation}        \label{EQooTHUIooUEXsNl}
                2\big( | G |-1 \big)=r| G |-| \Omega |
            \end{equation}
            où \( \Omega\) est l'ensemble des pôles de \( G\setminus\{ e \}\).

        \item[Quelles sont les possibilités ?]

            Les nombres \( | G |\), \( r\) et \( n_i\) sont des entiers. Nous allons voir qu'il n'y a pas des centaines de possibilités pour satisfaire la relation \eqref{EQooAMVBooDVcYeG}. D'abord, pour toute valeur de \( | G |\) (strictement plus grande que \( 1\)),
            \begin{equation}
                1\leq 2-\frac{ 2 }{ | G | }<2.
            \end{equation}
            Ensuite, si \( g\) fixe \( x\) alors \( g^{-1}\) fixe également \( x\), de sorte que \( n_i=| \Fix(x_i) |\geq 2\) pour tout \( i\). Donc tous les termes dans la somme à droite de \eqref{EQooAMVBooDVcYeG} sont dans \( \mathopen[ \frac{ 1 }{2} , 1 \mathclose[\). Nous avons donc au minimum deux termes, et au maximum trois. Autrement dit : \( r=2\) ou \( r=3\).

            \item[Si \( r=2\)]

                Le plus simple est de repartir de \eqref{EQooTHUIooUEXsNl}. En posant \( r=2\) nous trouvons tout de suite \( | \Omega |=2\). Il y a donc exactement deux pôles pour l'action de \( G\) sur la sphère unité.

                Tous les éléments de \( G\) laissent donc le même axe invariant et \( G\) est un sous-groupe des isométries du plan qui lui est perpendiculaire. Autrement dit, \( G\) est un sous-groupe fini de \( \SO(2)\) et donc cyclique par le lemme~\ref{LEMooUKEVooAEWvlM}.

            \item[Les possibilités pour \( r=3\)]

                Nous devons voir les solutions entières \( (n_1,n_2,n_3,| G |)\) de
                \begin{equation}
                    2-\frac{ 2 }{ | G | }=3-\frac{1}{ n_1 }-\frac{1}{ n_2 }-\frac{1}{ n_3 }<2.
                \end{equation}
                Il faut en particulier que
                \begin{equation}
                    \frac{1}{ n_1 }+\frac{1}{ n_2 }+\frac{1}{ n_3 }>1,
                \end{equation}
                ce qui signifie qu'au moins un des \( n_i\) doit être \( 1\) ou \( 2\), mais qu'il n'est pas possible que tous les \( n_i\) soient plus grands ou égaux à \( 3\). Vu que \( n_i=| \Fix(x_i) |\geq 2\), nous en déduisons qu'au moins un des \( n_i\) doit valoir \( 2\). Nous posons donc \( n_1=2\).

                De plus, nous savons que les \( n_i\) doivent diviser \( | G |\). Donc \( | G |\) est pair.

            \item[Si \( n_2=2\)]

                Nous sommes dans le cas \( r=3\), \( n_1=2\), \( n_2=2\). Nous avons
                \begin{equation}
                    \frac{1}{ n_3 }=\frac{ 2 }{ | G | },
                \end{equation}
                mais aussi \( n_3=| G |/| \Omega_3 |\) d'où nous déduisons que \( | \Omega_3 |=2\). Nous avons donc une orbite à deux éléments. Soit \( \Omega_3=\{ x,y \}\) avec \( x\neq y\).

                Le groupe \( \Fix(x)\) est un groupe à \( | G |/2\) éléments. Il est donc normal par le lemme~\ref{LemSkIOOG}. Si \( g\in G\) est tel que \( g(x)=y\) alors nous avons \( \Fix(y)=g\Fix(x)g^{-1}\), mais comme \( \Fix(x)\) est normal nous avons \( \Fix(x)=\Fix(y)\). Donc tous les éléments de \( \Fix(x)\) fixent \( x\) et \( y\). Le groupe \( \Fix(x)\) est donc un sous-groupe de \( \SO(2)\) est est cyclique comme vu plus haut.

                Mais de plus nous avons forcément \( y=-x\) parce qu'un élément de \( G\) qui fixe un point fixe également l'opposé. Vu que \( \Omega_3=\{ x,-x \}\), il existe \( s\in G\) tel que \( s(x)=-x\). Évidemment, \( s\) n'est pas dans \( \Fix(x)\) et les points fixes de \( s\) ne sont pas parmi \( x\) et \( -x\). Donc l'élément \( s^2\) a au moins \( 4\) points fixes : les deux de \( s\) ainsi que \( x\) et \( -x\). Il a donc au moins deux droites de points fixes et est donc l'identité : \( s^2=e\).

                De plus, vu que \( s(y)\) doit être égal soit à \( x\) soit à \( y\), et vu que \( s(x)=y\), l'injectivité de \( s\) donne \( s(y)=x\).

                Soit \( a\), un générateur de \( \Fix(x)\). Nous allons montrer que \( G=\gr(s,sa)\). Nous avons déjà
                \begin{subequations}
                    \begin{align}
                        (sa)(x)=s(x)=y\\
                        (sa)(y)=s(y)=x.
                    \end{align}
                \end{subequations}
                Donc \( sa\) inverse \( x\) et \( y\). Mais \( sa\) a ses propres deux points fixes (qui ne sont ni \( x\) ni \( y\)). L'élément \( (sa)^2\) a deux quatre points fixes sur la sphère unité : \( x\), \( y\) et les deux de \( sa\). Nous en déduisons que \( (sa)^2=e\).

                Nous nous souvenons que \( a\) est un générateur \( \Fix(x)\). Mais \( a=s\cdot sa\), donc \( a^k=(ssa)^k\). Nous en déduisons que \( \gr(s,sa)\) contient au moins \( \Fix(x)\).

                D'autre part si \( h\) et \( h'\) sont des éléments distincts dans \( \Fix(x)\), alors \( sh\) et \( sh'\) sont des éléments distincts de \( \gr(s,sa)\) qui ne sont pas dans \( \Fix(x)\). Autrement dit, la partie
                \begin{equation}
                    A=\{ sh\tq h\in Fix(x) \}
                \end{equation}
                est une partie de même cardinal que \( \Fix(x)\) tout en n'ayant aucune intersection avec \( \Fix(x)\) (note : l'identité n'est pas dans \( A\)). Mais \( | \Fix(x) |=| G |/2\), donc \( A\cup\Fix(x)=G\). Et justement \( A\cup G\subset \gr(s,sa)\). Nous en déduisons que \( \gr(s,sa)=G\).

                Le théorème~\ref{THOooYITHooTNTBuG} nous assure que le groupe \( G\) est alors le groupe diédral parce que les éléments \( s\) et \( sa\) vérifient les relations données en~\ref{NORMooCCUEooRRENed}.

            \item[Si \( r=3\), les autres cas possibles]

                Nous repartons de \eqref{EQooAMVBooDVcYeG} en posant \( r=3\). Nous obtenons ceci :
                \begin{equation}
                    1+\frac{ 2 }{ | G | }=\frac{1}{ n_1 }+\frac{1}{ n_2 }+\frac{1}{ n_3 }.
                \end{equation}
                Nous avons déjà vu que \( n_1=2\) était obligatoire, et que tous les cas où deux des \( n_i\) sont égaux à \( 2\) sont déjà couverts. Donc \( n_2\) et \( n_3\) vallent \( 3\) ou plus.

                Nous trions les \( n_i\) dans l'ordre croissant. Si \( n_2=4\) ou plus, alors \( n_3\) vaut \( 4\) ou plus. Mais
                \begin{equation}
                    \frac{ 1 }{2}+\frac{1}{ 4 }+\frac{1}{ 4 }=1<1+\frac{ 2 }{ | G | }.
                \end{equation}
                Donc \( n_3=3\) est obligatoire. Nous avons alors l'inégalité suivante qui restreint \( n_3\) :
                \begin{equation}
                    \frac{1}{ n_3 }=\frac{1}{ 6 }+\frac{ 3 }{ | G | }>\frac{1}{ 6 }.
                \end{equation}
                Donc \( n_3\) est \( 3\), \( 4\) ou \( 5\).

                Les derniers cas à couvrir sont :
                \begin{itemize}
                    \item \( (n_1,n_2,n_3)=(2,3,3)\). Dans ce cas, \( \frac{ 7 }{ 6 }=1+\frac{ 2 }{ | G | }\), donc \( | G |=12\).
                    \item \( (n_1,n_2,n_3)=(2,3,4)\). Dans ce cas, \( | G |=24\).
                    \item \( (n_1,n_2,n_3)=(2,3,5)\). Dans ce cas, \( | G |=60\).
                \end{itemize}

            \item[Le cas \( (2,3,3)\)]

                Nous utilisons les relations \( n_i| \Omega_i |=| G |\) pour savoir la taille des orites. Nous avons :
                \begin{enumerate}
                    \item
                        \( 2| \Omega_1 |=12\), donc \( | \Omega_1 |=6\),
                        \( 3| \Omega_2 |=12\), donc \( | \Omega_2 |=4\),
                        \( 3| \Omega_3 |=12\), donc \( | \Omega_3 |=4\).
                \end{enumerate}

                Nous avons \( G\cdot \Omega_2=\Omega_2\). D'une part parce que, par définition d'une orbite, \( G\cdot\Omega_2\subset\Omega_2\), et d'autre part parce que si \( x\in\Omega_2\), alors \( g^{-1}(x)\in\Omega_2\) et \( g\big( g^{-1}(x) \big)=x\); donc \( \Omega_2\) est bien dans l'image de \( \Omega_2\) par \( G\). Nous avons donc un morphisme \( s\colon G\to S_{\Omega_2}\) que nous allons immédiatement voir comme
                \begin{equation}
                    s\colon G\to S_4
                \end{equation}
                où \( S_4\) est le groupe des permutations de \( \{ 1,2,3,4 \}\).

                Voyons que \( s\) est injective. Si \( s(g)=s(h)\), alors \( s(gh^{-1})=\id\). Autrement dit, l'élément \( sh^{-1}\) de \( G\) est l'identité sur \( \Omega_2\) qui contient \( 4\) éléments. Fixant \( 4\) points (au moins), l'élément \( sh^{-1}\) est l'identité. Par conséquent
                \begin{equation}
                    s\colon G\to s(G)\subset S_4
                \end{equation}
                est un isomorphisme entre \( G\) et un sous-groupe de \( S_4\). Mais \( | G |=12\) et \( | S_4 |=24\), donc \( G\) est d'indice deux dans \( S_4\) et est donc le groupe alterné \( A_4\) par la proposition~\ref{PROPooCPXOooVxPAij}\ref{ITEMooGGAHooRYgNqq}.

            \item[Le cas \( (2,3,4)\)]

                Nous avons \( | G=24 |\) et les orbites ont pour taille :
                \begin{itemize}
                    \item \( 2| \Omega_1 |=24\), donc \( | \Omega_1 |=12\),
                    \item \( 3| \Omega_2 |=24\), donc \( | \Omega_2 |=8\),
                    \item \( 4| \Omega_3 |=24\), donc  \( | \Omega_3 |=6\).
                \end{itemize}

                \begin{subproof}
                    \item[\( \Omega_2\) vient par paires]

                        Soit \( x\in \Omega\) tel que \( | \Fix(x) |=3\). Alors \( x\in\Omega_2\) parce que \( x\) est forcément dans un des \( \Omega_i\) et tout élément \( x_i\) de \( \Omega_i\) vérifie \( | \Fix(x_i) |=n_i\). Mais comme les éléments de \( \SO(3)\) sont des applications linéaires, ceux qui fixent \( x\) fixent également \( -x\). Cela pour dire que si \( x\in\Omega_2\), alors \( -x\in\Omega_2\). Nous avons donc quatre éléments distincts \( a_1\), \( a_2\), \( a_3\) et \( a_4\) tels que
                    \begin{equation}
                        \Omega_2=\{ \pm a_1,\pm a_2,\pm a_3,\pm a_4 \}.
                    \end{equation}

                \item[Action sur les couples]

                    Nous prétendons que \( G\) agit sur l'ensemble des couples \( \{ \pm a_i \}\). C'est encore la linéarité qui joue : l'élément \( g(a_i)\) est forcément un des \( \pm a_k\) (éventuellement \( k=i\)). Si \( g(a_i)=a_k\), alors \( g(-a_i)=-a_k\). Autrement dit, pour tout \( i\), il existe un \( k\) tel que \( g\big( \{ a_i,-a_i \} \big)=\{ a_k,-a_k \}\). Cette association \( i\mapsto k\) est bijective (sinon \( g\) ne serait pas bijective), et fournit donc un morphisme de groupes
                    \begin{equation}
                        s\colon G\to S_4.
                    \end{equation}

                \item[\( s\) est injective]

                    Nous prouvons à présent que \( s(g)=\id\) si et seulement si \( g=e\). Dans un sens c'est évident : \( g(e)=\id\). Dans l'autre sens, nous devons prouver que si \( g(a_i)\in \pm a_i\) pour tout \( i\) alors \( g=e\).

                    Si \( g(a_i)=a_i\) pour tout \( i\), alors \( g\) stabilise \( 4\) points et l'affaire est pliée. Nous supposons qu'au moins un des \( a_i\) n'est pas stabilisé par \( g\). Pour fixer les idées nous disons que c'est \( a_1\). Nous avons donc \( g(a_1)=-a_1\). (oui : \( g(a_1)=-a_1\) et non \( \pm a_k\) pour un autre \( k\) parce que nous sommes sous l'hypothèse que \( g\) stabilise les couples)

                    L'élément \( g^2\) fixe tout \( \Omega_2\); donc \( g^2=e\). Nommons \( \pm b\) les points fixes de \( g\). Si \( b\in \Omega_2\) alors \( | \Fix(b) |=3\), c'est-à-dire que les éléments de \( G\) qui fixent \( b\) sont dans un groupe d'ordre \( 3\), et le corollaire~\ref{CorpZItFX} nous indique que ces éléments ne peuvent être que d'ordre \( 1\) ou \( 3\), pas deux. Nous en déduisons que \( b\) n'est pas dans \( \Omega_2\) et donc que \( g(a_i)=-a_i\) pour tout \( i\).

                    Jusqu'à présent nous avons prouvé que si \( g\in \ker(s)\) est non trivial,  alors \( g(a_i)=-a_i\) pour tout \( i\).

                    Soit maintenant \( h\in G\). Vu que \( \Omega_2\) est une orbite, \( h(a_i)\in \Omega_2\) et nous notons \( h(a_i)=\epsilon a_k\) avec \( \epsilon=\pm 1\) et éventiellement \( k=i\) ou éventiellement pas. Nous avons :
                    \begin{equation}
                        (h^{-1}gh)(a_i)=\epsilon (h^{-1} g)(a_k)=-\epsilon h^{-1}(a_k)=-\epsilon^2 a_i=-a_i.
                    \end{equation}
                    Donc \( g\) et \( h^{-1} g h\) ont même restriction à \( \Omega_2\). En particulier \( h^{-1} ghg^{-1}\) est l'identité sur \( \Omega_2\) et est donc l'identité.

                    Pour tout \( h\) nous avons \( g=h^{-1} gh\). Les points fixes de \( h^{-1}g h\) sont \( \pm h^{-1}(b)\), mais aussi \( \pm b\). Nous avons donc égalité d'ensemble \( \{ h(b),-h(b) \}=\{ b,-b \}\) pour tout \( h\in G\) (notez le changement de notation \( h\to h^{-1}\)). Cela signifie que \( \{ b,-b \}\) est une orbite de \( G\). Maizon'a pas d'orbites de cardinal deux; contradition. Nous en déduisons que \( e\) est l'unique élément de \( \ker(s)\).

                    \item[Conclusion]

                        La partie \( s(G)\) est un sous-groupe de \( S_4\) isomorphe à \( G\). Mais au niveau des cardinaux, \( | G |=24\) en même temps que \( | S_4 |=24\). Donc \( G\simeq s(G)\simeq S_4\).

                \end{subproof}
                \end{subproof}

        Nous passons au cas \( (2,3,5)\), et comme ça va être long et douloureux\footnote{Mais pas autant que le théorème~\ref{THOooSTHXooXqLBoT}, cependant.}, nous sautons un niveau d'indentation.

                Au niveau du cardinal de \( G\),
                \begin{equation}
                    \frac{1}{ 2 }+\frac{1}{ 3 }+\frac{1}{ 5 }=1+\frac{ 2 }{ | G | },
                \end{equation}
                donc \( | G |=60\). Et pour les orbites, \( | \Omega_1 |=30\), \( | \Omega_2 |=20\), \( | \Omega_3 |=12\).

                La proposition~\ref{PROPooUBIWooTrfCat} nous indique que le seul groupe simple d'ordre \( 60\) est le groupe \( A_5\). Nous allons donc nous atteler à prouver que \( G\) est simple. Vous êtes prêts ?


                \begin{subproof}
                    \item[Fixateurs et ordres]

                Tous les éléments de \( G\) sont dans un fixateur de type \( \Fix(x)\), et comme l'ordre d'un élément divise l'ordre du groupe (corollaire~\ref{CorpZItFX}), tous les éléments de \( G\) ont un ordre \( 2\), \( 3\) ou \( 5\). Nous sommes dans un cas très particuliers parce que
                \begin{itemize}
                    \item Les trois nombres \( 2\), \( 3\) et \( 5\) sont des nombres premiers distincts. Donc «diviser \( n_i\)» signifie pratiquement «être égal à \( n_i\)», surtout lorsqu'on parle de l'ordre d'un élément, qui ne peut pas être \( 1\).
                    \item Il existe une seule orbite de chaque taille.
                \end{itemize}
            Nous notons \( G(n_i)\) l'ensemble des éléments de \( G\) d'ordre \( n_i\). Les parties \( G(n_i)\) ne contiennent pas l'identité.

            \item[\( g\in G(n_i)\) implique \( \Fix(g)\subset \Omega_i\)]

                Si \( g\in G(n_i)\) et \( x\in\Fix(g)\) alors \( x\in \Omega_i\). En effet \( x\in Fix(g)\) signifie \( g(x)=x\) et donc aussi \( g\in\Fix(x)\). Donc l'ordre de \( g\) divise \( | Fix(x) |\), alors que l'ordre de \( g\) est \( n_i\) et que les possibilités pour \( | \Fix(x) |\) sont exactement les \( n_i\), lesquels sont premiers entre eux. Donc \( | \Fix(x) |=n_i\) et \( x\in \Omega_i\).

            \item[\( | \Fix(x) |=n_i\) implique \( x\in \Omega_i\)]

                Comme plus haut, \( g\in\Fix(x)\) implique que l'ordre de \( g\) divise \( n_i\) et est donc égal à \( n_i\). Autrement dit, \( g\in G(n_i)\). De plus \( g\in\Fix(x)\) implique \( x\in\Fix(g)\). Par le cas juste au-dessus nous déduisons \( x\in\Omega_i\).

            \item[\( a\) et \( -a\) dans la même orbite]

                Nous avons évidemment \( \Fix(a)=\Fix(-a)\) du fait que les éléments de \( G\) sont des applications linéaires. Si \( | \Fix(a) |=n_i\) alors \( a\in\Omega_i\) et aussi \( | \Fix(-a) |=| \Fix(a) |=n_i \) et aussi \( -a\in \Omega_i\).

            \item[Nombre de \( \Fix(x_i)\)]

                Soient \( a,b\in \Omega_i\). Nous avons \( | \Fix(a) |=| \Fix(b) |=n_i\) et \( \Fix(a)=\Fix(b)\) si et seulement si \( b=-a\) parce qu'un élément qui fixe \( a\) et \( b\) fixe automatiquement \( a\), \( b\), \( -a\), et \( -b\). Aucun élément non trivial ne peut fixer quatre points distincts. Autrement dit,
                \begin{equation}
                    \Fix(a)\cap\Fix(b)=\begin{cases}
                        \Fix(a)    &   \text{si } a=\pm b\\
                        \{ e \}    &    \text{sinon. }
                    \end{cases}
                \end{equation}
                Chaque élément \( x_i\in \Omega_i\) a son fixateur (il y en aurait \( | \Omega_i |=60/n_i\)), mais ces fixateurs sont égaux deux à deux, donc il y a seulement \( \frac{ 60 }{ 2n_i }\) groupes distincts de la forme \( | \Fix(x_i) |\) avec \( x_i\in \Omega_i\).

            \item[Récapitulatif]

                En reprenant ce que nous venons de dire avec \( i=1,2,3\) nous trouvons :
                \begin{enumerate}
                    \item
                        \( n_1=2\), avec \( | \Omega_1 |=30\) et \( 15\) groupes du type \( \Fix(x_1)\) avec \( x_1\) parcourant \( \Omega_1\).
                    \item
                        \( n_2=3\), avec \( | \Omega_2 |=20\) et \( 10\) groupes du type \( \Fix(x_2)\) avec \( x_2\) parcourant \( \Omega_2\).
                    \item
                        \( n_3=5\), avec \( | \Omega_3|=12\) et \( 6\) groupes du type \( \Fix(x_3)\) avec \( x_3\) parcourant \( \Omega_3\).
                \end{enumerate}
                Un élément non trivial de \( G\) se trouve forcément dans un et un seul de ces sous-groupes. Plus précisément, si \( g\in G(n_i)\) alors \( g\) est dans un des \( \Fix(x_i)\) avec \( x_i\in \Omega_i\).

                Comptons pour être sûr de ne pas s'être trompé. Chacune des lignes décrit \( 30\) éléments de \( G\); par exemple pour la seconde ligne donn \( 10\) groupes de taille \( | \Fix(x_2) |=n_2=3\). Mais tout ces groupes ont pour intersection exactement \( \{ e \}\). Donc le comptage des éléments se fait comme suit :
                \begin{equation}
                    3\times 30-15-10-6+1.
                \end{equation}
                Le dernier \( +1\) est parce que nous aurions décompté l'identité une fois de trop. Bref, on a bien \( 60\) éléments comme il se doit.

            \item[Un ensemble à calculer deux fois]

                Soient les ensembles \( A_2\), \( A_3\) et \( A_5\) définis par
                \begin{equation}
                    A_i=\{ (g,a)\in G(n_i)\times \Omega_i\tq g(a)=a  \}
                \end{equation}
                où \( G(n_i)\) est la partie de \( G\) des éléments d'ordre \( n_i\).

                Nous avons
                \begin{equation}
                    | A_i |=\sum_{g\in G(n_i)}| \Fix(g)\cap \Omega_i |.
                \end{equation}
                Mais les éléments de \( G(n_i)\) sont d'ordre \( n_i\), et par ce que nous avons dit plus haut, tous les éléments de \( \Fix(g)\) sont dans \( \Omega_i\). Donc \( \Fix(g)\cap \Omega_i=\Fix(g)\). Nous avons alors
                \begin{equation}
                    | A_i |=\sum_{g\in G(n_i)}| \Fix(g) |=2|G(n_i)|
                \end{equation}
                parce que \( | \Fix(g) |=2\) pour tout \( g\).

                En compatant \( | A_i |\) dans l'autre sens, nous avons
                \begin{equation}        \label{EQooBHIIooVcGgFd}
                    | A_i |=\sum_{x\in \Omega_i}|  \Fix(x)\cap G(n_i) |
                \end{equation}
                Vu que \( x\in \Omega_i\), les éléments de \( \Fix(x)\) sont d'ordre \( n_i\)\footnote{Encore et toujoure parce que les éléments de \( \Fix(x)\) ont un ordre qui divise \( | \Fix(x) |=n_i\) et que \( n_i\) est premier, et que nous avons exclu l'identité.} (sauf \( e\)), et comme \( G(n_i)\) est justement l'ensemble des éléments d'ordre \( n_i\) dans \( G\) nous avons \( \Fix(x)\cap G(n_i):\Fix(x)\setminus\{ e \}\). Cela pour dire que
                \begin{subequations}
                    \begin{align}
                        | A_i |&=\sum_{x\in \Omega_i}\Big( |\Fix(x)|-1\Big)\\
                        &=\sum_{x\in \Omega_i}| \Fix(x) |-\sum_{x\in \Omega_i}1\\
                        &=\sum_{x\in \Omega_i}n_i-| \Omega_i |  & | \Fix(x) |=n_i \text{ pcq }x\in\Omega_i\\
                        &=| \Omega_i |n_i-| \Omega_i |=| G |-| \Omega_i |.
                    \end{align}
                \end{subequations}
                En égalisant cela à la valeur \( 2|G(n_i)|\) déjà trouvée, nous déduisons les valeurs des \( | G(n_i) |\) :
                \begin{equation}
                    | G(n_i) |=\frac{ | G |-| \Omega_i | }{2}.
                \end{equation}
                Nous avons alors
                \begin{enumerate}
                    \item
                        \( | G(2) |=15\)
                    \item
                        \( | G(3) |=20\)
                    \item
                        \( | G(5) |=24\)
                \end{enumerate}

            \item[Les Sylow de \( G\)]

                Les \( p\)-Sylow sont définis en~\ref{DEFooPRCHooVZdwST}, et le super théorème qui répond à toutes les questions est le théorème~\ref{ThoUkPDXf}. Dans notre cas, les diviseurs premiers de \( | G |=60\) sont \( 2\), \( 3\) et \( 5\). Il faut faire attention au $2$ parce que sa plus haute puissance dans la décomposition de \( 60\) est \( 4\) et non \( 2\). Nous avons :
                \begin{enumerate}
                    \item
                        Un \( 2\)-Sylow est un sous-groupe d'ordre \( 4\).
                    \item
                        Un \( 3\)-Sylow est un sous-groupe d'ordre \( 3\).
                    \item
                        Un \( 5\)-Sylow est un sous-groupe d'ordre \( 5\).
                \end{enumerate}
                Entre autres :
                \begin{enumerate}
                    \item
                        Les \( 10\) sous-groupes \( \Fix(x_2)\) avec \( x_2\in \Omega_2\) sont des \( 3\)-Sylow de \( G\).
                    \item
                        Les \( 6\) sous-groupes \( \Fix(x_3)\) avec \( x_3\in \Omega_3\) sont des \( 5\)-Sylow de \( G\).
                    \item
                        Les \( 15\) sous-groupes \( \Fix(x_1)\) avec \( x_1\in \Omega_1\) sont d'ordre $2$ et ne sont donc pas des \( 2\)-Sylow de \( G\).
                \end{enumerate}

            \item[Tous les \( 3\)-Sylow et les \( 5\)-Sylow]

                Nous avons déjà trouvé \( 10\) \( 3\)-Sylow et \( 6\) \( 5\)-Sylow. Nous motrons à présent qu'il n'y en a pas d'autres. Le théorème de Sylow~\ref{ThoUkPDXf}\ref{ItemkYbdzZ} nous indique que le nombre \( n_3\) de \( 3\)-Sylow est :
                \begin{itemize}
                    \item diviseur de \( 60\),
                    \item dans \( [1]_3\)
                    \item au moins \( 10\).
                \end{itemize}
                Les diviseurs de \( 60\) sont :
                \begin{equation}
                    1,5,3,15,2,10,6,30,4,20,12,60.
                \end{equation}
                Le seul qui vérifie toutes les conditions est \( 10\). Donc \( G\) possède seulement \( 10\) \( 3\)-Sylow et ils sont tous de la forme \( \Fix(x_2)\) avec \( x_2\in \Omega_2\).

                Même raisonnement pour les \( 5\)-Sylow : il faut
                \begin{itemize}
                    \item diviseur de \( 60\),
                    \item dans \( [1]_5\)
                    \item au moins \( 6\).
                \end{itemize}
                La seule possibilité est \( 6\).

            \item[Sous-groupe normal]

                Soit \( H\), un sous-groupe normal de \( G\). Notre but étant de prouver que \( G\) est simple, nous voulons prouver que \( H\) est soit \( \{ e \}\) soit \( G\). Nous supposons que \( H\) est non trivial, et nous allons prouver que \( H=G\).

                Le théorème de Lagrange~\ref{ThoLagrange}\ref{ITEMooDPKSooNpOusd} nous dit que \( | H |\) divise \( | G |\). Le nombre \( | H |\) ne peut donc avoir que \( 2\), \( 3\) et \( 5\) comme facteurs premiers. Avec une mention spéciale pour le \( 2\) : \( | H |\) pourrait être divisible aussi par \( 4\).

            \item[Diviseurs de \( | H |\)]

                Soit un sous-groupe normal \( H\) de \( G\). Vu que c'est un sous-groupe sont ordre divise celui de \( G\) (encore et toujours le théorème de Lagrange~\ref{ThoLagrange}), et donc les facteurs premiers de \( | H |\) ne peuvent être que \( 2\), \( 3\) et \( 5\).

            \item[Si \( | H |\) est divisible en \( 3\)]

                Alors \( H\) contient au moins un \( 3\)-Sylow. Mais nous avons vu que les \( 3\)-Sylow de \( H\) sont les \( 3\)-Sylow de \( G\). Donc \( H\) contient tous les \( 3\)-Sylow de \( G\), parce que les \( 3\)-Syow sont conjugués et \( H\) est normal.

                Soit \( E\) l'ensemble des sous-groupes de \( H\). Vu qu'il est normal, \( H\) agit sur \( E\) par conjugaison, et les \( 3\)-Sylow forment une orbite. Si \( \alpha\) est un \( 3\)-Sylow, la formule des classes (proposition~\ref{Propszymlr}\ref{ITEMooCWUGooCOFHYk}) nous donne
                \begin{equation}
                    | H |=| \Fix(\alpha) | |\mO_{\alpha} |.
                \end{equation}
                Mais l'orbite \( \mO_{\alpha}\) de \( \alpha\) est l'ensemble des \( 3\)-Sylow, de sorte que \( | \mO_{\alpha} |=10\). Donc \( | H |\) est divisible en \( 10\).

                Mais il y a pire : \( H\) contient au moins les \( 10\) sous-groupes \( \Fix(x_2)\) pour \( x_2\in \Omega_2\). Ce sont \( 10\) groupes de \( | \Fix(x_2) |=3\) éléments. En décomptant \( e\) qui est dans l'intersection, cela fait
                \begin{equation}
                    10\times | \Fix(x_2) |-10+1=21
                \end{equation}
                éléments. Donc \( H\) contient au moins \( 21\) éléments. Le nombre \( | H |\) est donc :
                \begin{itemize}
                    \item diviseur de \( 60\)
                    \item multiple de \( 10\)
                    \item au moins \( 21\).
                \end{itemize}
                Donc c'est \( 30\) ou \( 60\).

            \item[Si \( | H |\) est divisible en \( 5\)]

                Le même raisonnement tient et \( | H |\) est \( 30\) ou \( 60\).

    \end{subproof}

        Nous restons avec les possibilités \( | H |\) égal à \( 2\), \( 4\), \( 30\) ou \( 60\).

    \begin{subproof}

            \item[Si \( | H | = 4\)]

                Alors \( H\) contient au moins un \( 2\)-Sylow. Un \( 2\)-Sylow de \( H\) est un sous-groupe contenant \( 4\) éléments qui sont d'ordre \( 2^m\). Le seul \( m\) possible dans \( G\) est \( m=1\). Vu qu'un \( 2\)-Sylow de \( H\) contient \( 4\) éléments, nous sommes dans le cas où \( H\) est un \( 2\)-Sylow. Il est donc le seul \( 2\)-Sylow de \( H\) parce que \( H\) est normal et que tous les \( 2\)-Sylow sont conjugués.

                Mais tous les sous-groupes d'ordre \( 2\) sont contenus dans un \( 2\)-Sylow. En particulier tous les \( 15\) groupes \( \Fix(x_1)\) sont dans l'unique \( 2\)-Sylow \( H\) qui est soit-disant d'ordre \( 4\). IL y a là une belle impossibilité.

                Donc le cas \( | H |=4\) est hors-concours.

            \item[Si \( | H |=2\)]

                Alors \( H=\{ e,g \}\) avec \( g^2=e\). Si \( h\in G\), l'élément \( hgh^{-1}\) ne peut être que \( e\) ou \( g\) (parce que \( H\) est normal). Le premier cas est \( g=e\), et le second donne \( gh=hg\). Donc \( g\) est dans le centre de \( G\) : il commute avec tous les éléments de \( G\).

                Vu que \( g\in G(2)\), nous avons que les éléments \( a\in\Fix(g)\) sont forcément dans \( \Omega_1\) parce que les points dont les fixateurs sont formés d'éléments d'ordre \( 2\) sont dans \( \Omega_1\). Soit \( h\in G\). Nous avons \( g=hgh^{-1}\) et donc aussi
                \begin{equation}
                    \big( hgh^{-1} \big)\big( h(a) \big)=hg(a)=h(a),
                \end{equation}
                donc \( h(a)\) et \( -h(a)\) sont des points fixes de \( hgh^{-1}\). Ce sont donc également de points fixes de \( g\). Nous en déduisons que \( g\) a pour points fixes les points \( a\), \( -a\), \( h(a)\) et \( -h(a)\). Vu que \( g\) n'est pas \( e\), ces quatre points ne peuvent pas être distincts. Vu que \( h(a)\) ne peut pas être \( -h(a)\), nous avons forcément \( h(a)=\pm a\).

                Donc l'orbite de \( a\) ne contiendrait que \( 2\) éléments. Pas possible.

            \item[Si \( | H |=30\)]

               À part \( | H |=60\), le dernier cas à traiter est \( | H |=30\). Nous rappellons obligeament que
               \begin{enumerate}
                   \item
                       \( | G(2) |=15\)
                   \item
                       \( | G(3) |=20\)
                   \item
                       \( | G(5) |=24\).
               \end{enumerate}
               Si \( H\) possède \( 30\) éléments, le théorème de Sylow dit que \( H\) contient au moins un \( 3\)-Sylow et un \( 5\)-Sylow, et donc tous. Vu que pour \( 3\) et \( 5\), les Sylow de \( H\) et de \( G\) sont les mêmes et bien idéntifiés, nous allons nous baser dessus. Le sous-groupe \( H\) contient tous les \( 3\) et \( 5\)-Sylow, donc le comptage des éléments est :
                \begin{equation}
                    10\times | \Fix(x_2) |+6\times | \Fix(x_3) |-15=45.
                \end{equation}
                Nous aurions aussi pu ajouter \( +4-1\) pour compter au moins un \( 2\)-Sylow.

                Donc dès que \( H\) compte \( 30\) éléments, il en compte \( 45\) et donc \( 60\) parce qu'il n'y a pas de diviseurs de \( 60\) entre \( 45\) et \( 60\).

    \end{subproof}

\end{proof}

\begin{probleme}
    La démonstration des groupes finis de \( \SO(3)\) est longue. Je me demande si il n'y a pas moyen de faire plus court. Par exemple \cite{ooYODPooHeNKiQ} utilise le théorème de Cauchy~\ref{THOooSUWKooICbzqM} que je n'utilise pas. D'autre part, toutes les références me semble utiliser plus ou moins implicitement le fait que si le sous-groupe normal \( H\) contient un élément de \( G(n_i)\), alors il les contiennent tous. J'avoue ne pas trop comprendre pourquoi.
\end{probleme}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Systèmes de coordonnées}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooWTPRooZbOSzO}

La trigonométrique nous offre de nouveaux systèmes de coordonnées qui peuvent se révéler pratique de certains cas : les coordonnées polaires sur \( \eR^2\) ainsi que les coordonnées cylindriques et sphériques sur \( \eR^3\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Ce que ça signifie intuitivement}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

On a vu qu'un point $M$ dans $\eR^2$ peut être représenté par ses abscisses $x$ et ses ordonnées $y$. Nous pouvons également déterminer le même point $M$ en donnant un angle et une distance comme montré sur la figure~\ref{LabelFigJWINooSfKCeA}.
\newcommand{\CaptionFigJWINooSfKCeA}{Un point en coordonnées polaires est donné par sa distance à l'origine et par l'angle qu'il faut avec l'horizontale.}
\input{auto/pictures_tex/Fig_JWINooSfKCeA.pstricks}


Le même point $M$ peut être décrit indifféremment avec les coordonnées $(x,y)$ ou bien avec $(r,\theta)$.

\begin{remark}
	L'angle $\theta$ d'un point n'étant a priori défini qu'à un multiple de $2\pi$ près, nous convenons de toujours choisir un angle $0\leq\theta<2\pi$. Par ailleurs l'angle $\theta$ n'est pas défini si $(x,y)=(0,0)$.

	La coordonnée $r$ est toujours positive.
\end{remark}

Nous avons dans l'idée de définir \( r\) et \( \theta\) par les formules
\begin{subequations}		\label{EqrthetaxyPoal}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta).
	\end{numcases}
\end{subequations}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées polaires : le théorème}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{theorem}[Coordonnées polaires\cite{MonCerveau}]     \label{THOooBETSooXSQhdX}
    Soit l'application
    \begin{equation}
        \begin{aligned}
            T\colon \mathopen[ 0 , \infty \mathclose[\times \mathopen[ 0 , 2\pi \mathclose[&\to \eR^2 \\
                (r,\theta)&\mapsto \begin{pmatrix}
                    r\cos(\theta)    \\ 
                    r\sin(\theta)    
                \end{pmatrix}.
        \end{aligned}
    \end{equation}
    \begin{enumerate}
        \item       \label{ITEMooNGOKooFCXmwy}
            L'application \( T\) est surjective.
        \item       \label{ITEMooMCIOooJiBvug}
            L'application
            \begin{equation}
                T\colon \mathopen] 0 , \infty \mathclose[\times \mathopen[ 0 , 2\pi \mathclose[\to \eR^2\setminus\{ (0,0) \}
            \end{equation}
            est bijective.
        \item       \label{ITEMooZFRGooQPDUtX}
            En considérant la demi-droite \( D=\{ (x,0) \}_{x\geq 0}\), l'application
            \begin{equation}
                T\colon \mathopen] 0 , \infty \mathclose[\times \mathopen] 0 , 2\pi \mathclose[\to \eR^2\setminus D
            \end{equation}
            est un \(  C^{\infty}\)-difféomorphisme\footnote{L'application est de classe \(  C^{\infty}\) et son inverse est également de classe \(  C^{\infty}\). Le plus souvent, vous voulez seulement utiliser ce théorème dans le but de faire un changement de variables dans une intégrale; vous n'avez donc besoin que d'un \( C^1\)-difféomorphisme.}.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Une bonne partie de ce théorème est une conséquence de \ref{PROPooKSGXooOqGyZj}. La vraie nouveauté de ce théorème sera la régularité.  Nous démontrons point par point.
    \begin{enumerate}
        \item
            Pour \ref{ITEMooNGOKooFCXmwy}. Soit \( a=(x,y)\in \eR^2\). Nous avons \( a/\| a \|\in S^1\). Par la proposition \ref{PROPooKSGXooOqGyZj}, il existe \( \theta\in \mathopen[ 0 , 2\pi \mathclose]\) tel que
            \begin{equation}
                \frac{ a }{ \| a \| }=\big( \cos(\theta),\sin(\theta) \big).
            \end{equation}
            Alors \( a=  \| a \|\big( \cos(\theta),\sin(\theta) \big)= T(\| a \|,\theta)\). Voila. L'application \( T\) est surjective.
        \item
            Pour \ref{ITEMooMCIOooJiBvug}. En ce qui concerne la surjectifivé,
            \begin{equation}
                T\big( 0,\mathopen[ 2 , 2\pi \mathclose[ \big)=\{ (0,0) \}.
            \end{equation}
            Donc le point \ref{ITEMooNGOKooFCXmwy} donne le surjectif lorsque nous enlevons d'un côté les points avec \( r=0\) et de l'autre le point \( (0,0)\). 
            
            Pour l'injectivité, nous supposons \( T(r_1,\theta_1)=T(r_2,\theta_2)\). Vu que \( \| T(t,\theta) \|=r\), nous avons tout de suite \( r_1=r_2\). Nous restons donc avec l'égalité
            \begin{equation}
                \begin{pmatrix}
                    \cos(\theta_1)    \\ 
                    \sin(\theta_1)    
                \end{pmatrix}=\begin{pmatrix}
                    \cos(\theta_2)    \\ 
                    \sin(\theta_2)    
                \end{pmatrix}.
            \end{equation}
            La proposition \ref{PROPooKSGXooOqGyZj} dit alors que \( \theta_1=\theta_2\).
        \item
            Pour \ref{ITEMooZFRGooQPDUtX}. L'application \( T\) est injective en tant que restriction d'une application injective. Pour le surjectif, soit \( a\in \eR^2\setminus D\). Vu que \( a\notin D\), nous avons \( \| a \|\neq 0\) et il est légitime de dire, comme plus haut, qu'il existe \( \theta\in \mathopen[ 0 , 2\pi \mathclose[\) tel que
                \begin{equation}
                    \frac{ a }{ \| a \| }=\begin{pmatrix}
                        \cos(\theta)    \\ 
                        \sin(\theta)    
                    \end{pmatrix}.
                \end{equation}
                Ce \( \theta\) n'est pas zéro parce que \( \theta=0\) donne le point \( (1,0)\) qui est sur \( D\).

                En ce qui concerne l'inverse, nous n'allons pas nous lancer dans une étude subtile de la fonction \eqref{EQooSAYFooRFVSPc}; nous avons déjà démontré la continuité dans le lemme \ref{LEMooEQVRooMAffCw}, et monter dans les dérivées nous semble un peu compliqué. Au lieu de cela, nous allons faire en deux étapes :
                \begin{itemize}
                    \item Prouver que \( T\) est de classe \( C^p\) pour tout \( p\) en invoquant seulement des théorèmes à proposition de différentielle,
                    \item
                        En déduire que \( T^{-1}\) est également \( C^p\) pour tout \( p\) en invoquant le théorème d'inversion locale \ref{ThoXWpzqCn}.
                \end{itemize}

                Les applications \( (r,\theta)\mapsto r\), \( (r,\theta)\mapsto \sin(\theta)\) et \( (r,\theta)\mapsto \cos(\theta)\) sont de classe \(  C^{\infty}\) grâce au lemme \ref{LEMooDDUZooLwXkRp}. Le lemme \ref{LemDiffProsuid} sur la différentiabilité du produit montre alors que les fonctions \( T_1\) et \( T_2\) données par
                \begin{subequations}
                    \begin{align}
                        T_1(r,\theta)=r\cos(\theta)\\
                        T_2(r,\theta)=r\sin(\theta)
                    \end{align}
                \end{subequations}
                sont différentiables\footnote{Si vous voulez seulement avoir un \( C^1\)-difféomorphisme, calculez explicitement la différentielle et montrez que c'est continu. Vous n'avez pas à utiliser la proposition \ref{PROPooWNCGooHbmcVb} ni rien des prodiuts tensoriels.}. Mieux, la proposition \ref{PROPooWNCGooHbmcVb} montre que ces fonctions \( T_1\) et \( T_2\) sont de classe \( C^p\) pour tout \( p\), c'est-à-dire qu'elles sont de classe \(  C^{\infty}\). Cela montre que les coordonnées polaires sont de classe \(  C^{\infty}\), et il faut encore parler de l'inverse.

                En ce qui concerne la différentielle,
                \begin{equation}
                    dT_{(r,\theta)}(u,v)=\begin{pmatrix}
                        u\cos(\theta)-rv\sin(\theta)    \\ 
                        u\sin(\theta)+rv\cos(\theta)    
                    \end{pmatrix}.
                \end{equation}
                Donc la matrice de la différentielle est
                \begin{equation}
                    dT_{(r,\theta)}=\begin{pmatrix}
                        \cos(\theta)    &  -r\sin(\theta)    \\ 
                        \sin(\theta)    &   r\cos(\theta),    
                    \end{pmatrix}
                \end{equation}
                dont le déterminant est \( r\) (lemme \ref{LEMooAEFPooGSgOkF} utilisé). Donc la différentielle en \( (r,\theta)\) est une application linéaire inversible parce que \( r\neq 0\) aux points que nous considérons. L'application \( dT_{(r,\theta)}\) est bicontinue parce que nous sommes en dimension finie. Tout cela pour dire que le théorème d'inversion local \ref{ThoXWpzqCn} fonctionne et \( T^{-1}\) est \( C^p\) dès que \( T\) est \( C^p\). 

                Vu que \( T\) est de classe \( C^p\) pour tout \( p\), l'inverse \( T^{-1}\) est également \( C^p\) pour tout \( p\), c'est-à-dire que \( T^{-1}\) est de classe \(  C^{\infty}\).
    \end{enumerate}
\end{proof}

\begin{definition}
    Ce que nous appelons \defe{les coordonnées polaires}{coordonnées polaires} est l'application 
    \begin{equation}
        \begin{aligned}
            T\colon \mathopen[ 0 , \infty \mathclose[\times \mathopen[ 0 , 2\pi \mathclose[&\to \eR^2 \\
                (r,\theta)&\mapsto \begin{pmatrix}
                    r\cos(\theta)    \\ 
                    r\sin(\theta)    
                \end{pmatrix}.
        \end{aligned}
    \end{equation} 
    du théorème \ref{THOooBETSooXSQhdX}\ref{ITEMooZFRGooQPDUtX}. Selon les circonstances, nous considérons l'une ou l'autre des restrictions pour avoir une bijection ou un difféomorphisme.
\end{definition}

\begin{example}     \label{EXooSDHDooJzDioW}
	Soit à calculer
	\begin{equation}
		\lim_{(x,y)\to(0,0)}\frac{ x^2+y^2 }{ x-y }.
	\end{equation}

    Nous introduisons la fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eR^2\setminus\{ x=y \}&\to \eR \\
            (x,y)&\mapsto \frac{ x^2+y^2 }{ x-y }. 
        \end{aligned}
    \end{equation}
    Une idée souvent fructueuse pour traiter ce genre de limite est de passer aux coordonnées polaires. Attention, si on veut faire les choses très explicitement, c'est un peu lourd en notations. Il s'agit de poser
    \begin{equation}
        \begin{aligned}
        f\colon \big( \mathopen] 0 , \infty \mathclose[\times\mathopen[ 0 , 2\pi \mathclose[ \big)\setminus\big\{ \eR\times\{ \frac{ \pi }{ 4 }\}\cup\eR\times \{ \frac{ 5\pi }{ 4 } \} \}&\to \eR \\
            (r,\theta)&\mapsto \frac{ r^2 }{ r\big( \cos(\theta)-\sin(\theta) \big) }. 
        \end{aligned}
    \end{equation}
    Bon. À strictement parler, nous aurions pu dire que \( g\) est définie pour \( r=0\), mais vu que nous voulons seulement calculer la limite pour \( r\to 0\), on n'a pas besoin de la valeur en zéro. De plus les coordonnées polaires ne sont pas bijectives en l'origine. Donc bon \ldots on s'en passe.

    Quel est le lien entre \( f\) et \( g \) ? Du point de vue du calcul, le lien est qu'on a remplacé \( x\) par \( r\cos(\theta)\) et \( y\) par \( r\sin(\theta)\). Le vrai lien est l'égalité
    \begin{equation}
        g=f\circ T
    \end{equation}
    où \( T\) est l'application de coordonnées polaires dont les principales propriétés sont données dans le théorème \ref{THOooBETSooXSQhdX}\ref{ITEMooMCIOooJiBvug}.

    Soit un voisinage \( B\big( (0,0), R \big)\) de \( (0,0)\) dans \( \eR^2\). Le but est de montrer que les valeurs \( f(B)\) se regroupent autour d'une valeur \( \ell\) lorsque \( R\to 0\). Soyons plus précis et nommons \( \ell\) le candidat limite. Soit \( \epsilon>0\); nous devons trouver \( R>0\) tel que \( f\Big( B\big( (0,0),R \big) \Big)\subset B(\ell,\epsilon)\).

    Pour \( R>0\), nous avons
    \begin{equation}
        B\big( (0,0),R \big)=T\big( \mathopen[ 0 , R \mathclose[\times \mathopen[ 0 , 2\pi \mathclose[ \big),
    \end{equation}
    donc
    \begin{equation}
        f(B)=g\big( \mathopen[ 0 , R \mathclose[\times \mathopen[ 2 , 2\pi \mathclose[ \big).
    \end{equation}
    Soit \( r<R\). Nous avons
    \begin{equation}
        \lim_{\theta\to \pi/4} g(r,\theta)=\infty.
    \end{equation}
    Donc \( f(B)\) contient des valeurs arbitrairement grandes, quelle que soit la valeur de \( R\). Il n'y a donc pas de limite possibles.
    
    Si vous voulez un argument un peu plus imagé, en voici un\footnote{Qui satisfera tous vos professeurs, pourvu que vous ayez compri que ce qui se cache est une histoire de valeurs de \( f\) prises sur un voisinage de \( (0,0)\).} basé sur une combinaison entre la méthode des coordonnées polaires et la méthode des chemins.
    
	Certes \emph{pour chaque $\theta$} nous avons $\lim_{r\to 0} g(r,\theta)=0$, mais il ne faut pas en déduire trop vite que la limite $\lim_{(x,y)\to(0,0)}g(x,y)$ vaut zéro parce que prendre la limite $r\to 0$ avec $\theta$ fixé revient à prendre la limite le long de la droite d'angle $\theta$.

	Il n'est pas possible de majorer $g(r,\theta)$ par une fonction ne dépendant pas de $\theta$ parce que cette fonction tend vers l'infini lorsque $\theta\to\pi/4$. Est-ce que cela veut dire que la limite n'existe pas ? Cela veut en tout cas dire que la méthode des coordonnées polaires ne parvient pas à résoudre l'exercice. Pour conclure, il faudra encore un peu travailler.

    Nous pouvons essayer de calculer le long d'un chemin plus général \( (r(t),\theta(t))\). Choisissons \( r(t)=t\) puis cherchons \( \theta(t)\) de telle sorte à avoir
    \begin{equation}        \label{EqICrDSe}
        \cos\theta(t)-\sin\theta(t)=t^2.
    \end{equation}
    Le mieux serait de résoudre cette équation pour trouver \( \theta(t)\). Mais en réalité il n'est pas nécessaire de résoudre : montrer qu'il existe une solution suffit. Nous pouvons supposer que \( t^2<1\). Pour \( \theta=\pi/4\) nous avons \( \cos(\theta)-\sin(\theta)=0\) et pour \( \theta=0\) nous avons \( \cos(\theta)-\sin(\theta)=1\). Le théorème des valeurs intermédiaires nous enseigne alors qu'il existe une valeur de \( \theta\) qui résout l'équation \eqref{EqICrDSe}.

    % Laisser en deux lignes, parce que la seconde référence est ok vers le futur.
    Pour être rigoureux, nous devons aussi montrer que la fonction \( \theta(t)\) est continue. Pour cela il faudrait utiliser le théorème de la fonction implicite~\ref{ThoRYN_jvZrZ}.
    Nous verrons dans l'exemple~\ref{ExmeASDLAf} comment s'en sortir sans théorème de la fonction implicite, au prix de plus de calculs.
\end{example}

Les coordonnées polaires sont données par le difféomorphisme
\begin{equation}
	\begin{aligned}
		g\colon \mathopen]0,\infty\mathclose[\times\mathopen]0,2\pi\mathclose[ &\to\eR^2\setminus D\\
		(r,\theta)&\mapsto \big( r\cos(\theta),r\sin(\theta) \big)
	\end{aligned}
\end{equation}
où $D$ est la demi-droite $y=0$, $x\geq 0$. Le fait que les coordonnées polaires ne soient pas un difféomorphisme sur tout $\eR^2$ n'est pas un problème pour l'intégration parce que le manque de difféomorphisme est de mesure nulle dans $\eR^2$. Le jacobien est donné par
\begin{equation}
	Jg=\det\begin{pmatrix}
	\partial_rx	&	\partial_{\theta}x	\\
	\partial_ry	&	\partial_{\theta}y
\end{pmatrix}=\det\begin{pmatrix}
	\cos(\theta)	&	-r\sin(\theta)	\\
	\sin(\theta)	&	r\cos(\theta)
\end{pmatrix}=r.
\end{equation}

La fonction qui donne les coordonnées polaires est
\begin{equation}
    \begin{aligned}
        \varphi\colon \eR^+\times\mathopen] 0 , 2\pi \mathclose[&\to \eR^2 \\
        (r,\theta)&\mapsto\begin{pmatrix}
            r\cos(\theta)    \\
            r\sin(\theta)
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Son Jacobien vaut
\begin{equation}
    J_{\varphi}(r,\theta)=\det\begin{pmatrix}
        \frac{ \partial x(r,\theta) }{ \partial r }    &   \frac{ \partial x(r,\theta) }{ \partial \theta }    \\
        \frac{ \partial y(r,\theta) }{ \partial r }    &   \frac{ \partial y(r,\theta) }{ \partial \theta }
    \end{pmatrix}=
    \begin{vmatrix}
        \cos(\theta)    &   -r\sin(\theta)    \\
        \sin(\theta)    &   r\cos(\theta)
    \end{vmatrix}=r.
\end{equation}

\begin{proposition}     \label{PROPooFLUAooDsyMXO}
    Soit la fonction
    \begin{equation}
        \begin{aligned}
        T\colon \mathopen] O , +\infty \mathclose[\times \eR&\to \eR^2\setminus\{(0,0)\} \\
            (r,\theta)&\mapsto \big( r\cos(\theta),r\sin(\theta) \big).
        \end{aligned}
    \end{equation}
    \begin{enumerate}
        \item
            Elle est surjective.
        \item
        Pour tout \( a\in \eR\), l'application \( T\) est bijective sur la bande \( \mathopen] 0 , +\infty \mathclose[\times \mathopen[ a-\pi , a+\pi \mathclose[\).
        \item
            Si \( a=0\), la fonction inverse est donnée par
            \begin{equation}
                T^{-1}(x,y)=\big( \sqrt{ x^2+y^2 },\arctg(y/x) \big).
            \end{equation}
    \end{enumerate}
\end{proposition}

    Soit $P=(x,y)$ un élément dans $\eR^2$, on dit que $r=\sqrt{x^2+y^2}$ est le rayon de $P$ et que $\theta=\arctg (y/x) $ est son argument principal. L'origine ne peut pas être décrite en coordonnées polaires parce que si son rayon est manifestement zéro, on ne peut pas lui associer une valeur univoque de l'angle $\theta$.

\begin{example}
L'équation du cercle de rayon $a$ et centre $(0, 0)$ en coordonnées polaires est $r=a$.
\end{example}

\begin{example}
	Une équation possible pour la demi-droite $x=y$, $x>0$,  est $\theta=\pi/4$.
\end{example}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Transformation inverse : théorie}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Voyons la question inverse : comment retrouver $r$ et $\theta$ si on connait $x$ et $y$ ? Tout d'abord,
\begin{equation}
	r=\sqrt{x^2+y^2}
\end{equation}
parce que la coordonnée $r$ est la distance entre l'origine et $(x,y)$. Comment trouver l'angle ? Nous supposons $(x,y)\neq (0,0)$. Si $x=0$, alors le point est sur l'axe vertical et nous avons
\begin{equation}
	\theta=\begin{cases}
		\pi/2	&	\text{si }y>0\\
		3\pi/2	&	 \text{si }y<0
	\end{cases}
\end{equation}
Notez que si $y<0$, conformément à notre convention $\theta\geq 0$, nous avons noté $\frac{ 3\pi }{2}$ et non $-\frac{ \pi }{ 2 }$.

Supposons maintenant le cas général avec $x\neq 0$. Les équations \eqref{EqrthetaxyPoal} montrent que
\begin{equation}
	\tan(\theta)=\frac{ y }{ x }.
\end{equation}
Nous avons donc
\begin{equation}
	\theta=\tan^{-1}\left( \frac{ y }{ x } \right).
\end{equation}
La fonction inverse de la fonction tangente est celle définie plus haut.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Transformation inverse : pratique}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le code suivant utilise \href{http://www.sagemath.org}{Sage}.

\lstinputlisting{tex/frido/calculAngle.py}

Son exécution retourne :
\begin{verbatim}
(sqrt(2), 1/4*pi)
(sqrt(5), pi - arctan(1/2))
(6, 1/6*pi)
\end{verbatim}
Notez que ce sont des valeurs \emph{exactes}. Ce ne sont pas des approximations, Sage travaille de façon symbolique.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées polaires : dérivées partielles}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le changement de coordonnées pour les polaires est la fonction
\begin{equation}
    f\begin{pmatrix}
        r    \\
        \theta
    \end{pmatrix}=\begin{pmatrix}
        x    \\
        y
    \end{pmatrix}=\begin{pmatrix}
        r\cos\theta    \\
        r\sin\theta
    \end{pmatrix}.
\end{equation}
Considérons une fonction $g$ sur $\eR^2$, et définissons la fonction $\tilde g$ par
\begin{equation}
    \tilde g(r,\theta)=g(r\cos\theta,r\sin\theta).
\end{equation}
La formule \eqref{EqDerCompofg} permet de trouver les dérivées partielles de $g$ par rapport à $r$ et $\theta$ en termes de celles par rapport à $x$ et $y$ de $g$.

Pour faire le lien avec les notations du point précédent, nous avons
\begin{equation}
    \begin{aligned}[]
        f_1(r,\theta)&=r\cos(\theta)\\
        f_2(r,\theta)&=r\sin(\theta)\\
        (x_1,x_2)&\to(r,\theta)\\
        (y_1,y_2)&\to(x,y).
    \end{aligned}
\end{equation}
Nous avons donc
\begin{equation}
    \begin{aligned}[]
        \frac{ \partial \tilde g }{ \partial r }(r,\theta)&=\sum_{i=1}^2\frac{ \partial g }{ \partial x_i }\big( f(r,\theta) \big)\frac{ \partial f_i }{ \partial r }(r,\theta)\\
        &=\frac{ \partial g }{ \partial x }(r\cos\theta,r\sin\theta)\frac{ \partial \big( r\cos\theta \big) }{ \partial r }(r,\theta)\\
        &\quad+\frac{ \partial g }{ \partial y }(r\cos\theta,r\sin\theta)\frac{ \partial \big( r\sin\theta\big) }{ \partial r }(r,\theta)\\
        &=\cos\theta\frac{ \partial g }{ \partial x }(r\cos\theta,r\sin\theta)+\sin\theta\frac{ \partial g }{ \partial y }(r\cos\theta,r\sin\theta).
    \end{aligned}
\end{equation}

Prenons par exemple $g(x,y)=\frac{1}{ x^2+y^2 }$. Étant donné que
\begin{equation}
    \frac{ \partial g }{ \partial x }=\frac{ -2x }{ (x^2+y^2)^2 },
\end{equation}
nous avons
\begin{equation}
    \frac{ \partial g }{ \partial x }(r\cos\theta,r\sin\theta)=\frac{ -2\cos\theta }{ r^3 }.
\end{equation}
En utilisant la formule,
\begin{equation}
    \frac{ \partial \tilde g }{ \partial r }(r,\theta)=\cos(\theta)\left( \frac{ -2\cos\theta }{ r^3 } \right)+\sin(\theta)\left( \frac{ -2\sin\theta }{ r^3 } \right)=-\frac{ 2 }{ r^3 }.
\end{equation}
Nous pouvons vérifier directement que cela est correct. En effet
\begin{equation}
    \tilde g(r,\theta)=g(r\cos\theta,r\sin\theta)=\frac{1}{ r^2 },
\end{equation}
dont la dérivée par rapport à $r$ vaut $-2/r^3$.

En ce qui concerne la dérivée par rapport à $\theta$, nous avons
\begin{equation}
    \begin{aligned}[]
    \frac{ \partial \tilde g }{ \partial \theta }&=\frac{ \partial g }{ \partial x }(r\cos\theta,r\sin\theta)\frac{ \partial \big( r\cos(\theta) \big) }{ \partial \theta }+\frac{ \partial g }{ \partial y }(r\cos\theta,r\sin\theta)\frac{ \partial \big( r\sin(\theta) \big) }{ \partial \theta }\\
    &=\left( \frac{ -2\cos\theta }{ r^3 } \right)(-r\sin\theta)+\left( \frac{ -2\sin\theta }{ r^3 } \right)(r\cos\theta)\\
    &=0.
    \end{aligned}
\end{equation}

En résumé et avec quelques abus de notation :
\begin{equation}
    \begin{aligned}[]
        \frac{ \partial \tilde g }{ \partial r }&=\cos(\theta)\frac{ \partial g }{ \partial x }+\sin(\theta)\frac{ \partial g }{ \partial y }\\
        \frac{ \partial \tilde g }{ \partial \theta }&=-r\sin(\theta)\frac{ \partial g }{ \partial x }+r\cos(\theta)\frac{ \partial g }{ \partial y }\\
    \end{aligned}
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées cylindriques}
%---------------------------------------------------------------------------------------------------------------------------

Les \defe{coordonnées cylindriques}{coordonnées!cylindrique} sont un perfectionnement des coordonnées polaires. Il s'agit simplement de donner le point $(x,y,z)$ en faisant la conversion $(x,y)\mapsto(r,\theta)$ et en gardant le $z$. Les formules de passage sont
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta)\\
		z=z.
	\end{numcases}
\end{subequations}

Soit $T$ la fonction de $]0, +\infty[\times \eR^2$ dans $\eR^3\setminus\{(0,0,0)\}$ définie par
\begin{equation}
  \begin{array}{lccc}
    T: &]0, +\infty[\times \eR\times \eR & \to & \eR^3\setminus\{(0,0,0)\}\\
 & (r, \theta, z)&\mapsto& (r\cos \theta, r \sin \theta, z),
  \end{array}
\end{equation}
Cette fonction est surjective. Elle est bijective sur chaque bande de la forme  $]0, +\infty[\times [a-\pi,a+\pi[\times \eR$, $a$ dans $\eR$. Il n'y a presque rien de nouveau par rapport aux coordonnées polaires. Les coordonnées  cylindriques sont intéressantes si on décrit un objet invariant par rapport aux rotations autour de l'axe des $z$.

\begin{example}
Il faut savoir ce que décrivent les équations les plus simples en coordonnées cylindriques,
\begin{itemize}
\item $r\leq a$, pour $a$ constant dans  $]0, +\infty[$, est le cylindre de hauteur infinie qui a pour axe l'axe des $z$ et pour base le disque de rayon $a$ centré à l'origine,
\item $r= a$ est  la surface du cylindre,
\item $\theta = b$ est un demi-plan ouvert et sa fermeture contient l'axe des $z$,
\item $z=c$ est un plan parallèle au plan $x$-$y$.
\end{itemize}
\end{example}

\begin{example}
  Un demi-cône qui a  son sommet en l'origine et  pour axe l'axe des $z$ est décrit par $z=d r$.  Si $d$ est positif  il s'agit  de la moitié supérieure du cône, si $d<0$ de la moitié inférieure.
\end{example}

\begin{example}
 De même,  la sphère de rayon $a$ et centrée à l'origine est l'assemblage des calottes $z=\sqrt{a^2-r^2}$ et $z=-\sqrt{a^2-r^2}$.
\end{example}

En ce qui concerne les coordonnées cylindriques, le Jacobien est donné par
\begin{equation}
    J(r,\theta,z)=\begin{vmatrix}
        \frac{ \partial x }{ \partial r }    &   \frac{ \partial x }{ \partial \theta }    &   \frac{ \partial x }{ \partial z }    \\
        \frac{ \partial y }{ \partial r }    &   \frac{ \partial y }{ \partial \theta }    &   \frac{ \partial y }{ \partial z }    \\
        \frac{ \partial z }{ \partial r }    &   \frac{ \partial z }{ \partial \theta }    &   \frac{ \partial z }{ \partial z }
    \end{vmatrix}=
    \begin{vmatrix}
        \cos\theta    &   -r\sin\theta    &   0    \\
        \sin\theta    &   r\cos\theta    &   0    \\
        0    &   0    &   1
    \end{vmatrix}=r.
\end{equation}
Nous avons donc $dx\,dy\,dz=r\,dr\,d\theta\,dz$.

\begin{subequations}
    \begin{numcases}{}
        x=r\cos\theta\\
        y=r\sin\theta\\
        z=z
    \end{numcases}
\end{subequations}
avec \( r\in\mathopen] 0 , \infty \mathclose[\), \( \theta\in\mathopen[ 0 , 2\pi [\) et \( z\in\eR\). Le jacobien vaut \( r\).

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\subsection{Coordonnées sphériques}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit $T$ la fonction de $]0, +\infty[\times \eR^2$ dans $\eR^3\setminus\{(0,0,0)\}$ définie par
\begin{equation}
  \begin{array}{lccc}
    T: &]0, +\infty[\times \eR\times \eR & \to & \eR^3\setminus\{(0,0,0)\}\\
 & (\rho, \theta, \phi)&\mapsto& (\rho\cos \theta\sin \phi, \rho \sin \theta\sin \phi, \rho\cos \phi),
  \end{array}
\end{equation}
Cette fonction est surjective. Elle est bijective sur chaque bande de la forme  $]0, +\infty[\times [a-\pi,a+\pi[\times [b-\pi/2, b+\pi/2[$, $a$ et $b$ dans $\eR$.  Si $a =0$ et $b=-\pi/2$ la fonction inverse $T^{-1}$ est donnée donnée
\begin{equation}
  \begin{array}{lccc}
    T: &\eR^3\setminus\{(0,0,0)\} & \to & ]0, +\infty[\times [-\pi,\pi[\times [0, \pi[\\
 & (x,y,z)&\mapsto& \left(\sqrt{x^2+y^2+z^2}, \arctg \frac{y}{x}, \arccos \left(\frac{z}{\sqrt{x^2+y^2+z^2}}\right)\right).
  \end{array}
\end{equation}
Soit $ P$ un point dans $\eR^3$. L'angle $\phi$ est l'angle entre le demi-axe positif des $z$ et le vecteur $\overrightarrow{OP}$, $\rho$ est la norme de $\overrightarrow{OP}$ et $\theta$ est l'argument en coordonnées polaires de la projection de $\overrightarrow{OP}$ sur le plan $x$-$y$.

\begin{remark}
	Dans la littérature, les angles $\theta$ et $\phi$ sont parfois inversés (voire, changent de nom, par exemple $\varphi$ au lieu de $\phi$). Il faut donc être très prudent lorsqu'on veut utiliser dans un cours des formules données dans un autre cours.
\end{remark}

\begin{example}
Il faut connaitre le sens des équations plus simples,
\begin{itemize}
\item $\rho\leq a$, pour $a$ constant dans  $]0, +\infty[$, est la boule fermée de rayon $a$ centrée à l'origine,
\item $\rho= a$ est  la sphère de rayon $a$ centrée à l'origine,
\item $\theta = b$ est un demi-plan ouvert et sa fermeture contient l'axe des $z$,
\item $\phi= c$ est un demi-cône qui a  son sommet à l'origine et  pour axe l'axe des $z$.  Si $c$ est positif  il s'agit  de la moitié supérieure du cône, si $d<0$ de la moitié inférieure.
\end{itemize}
 \end{example}

Les \defe{coordonnées sphériques}{coordonnées!sphériques} sont ce qu'on appelle les «méridiens» et «longitudes» en géographie. Les formules de transformation sont
\begin{subequations}		%\label{SubEqsCoordSphe}
	\begin{numcases}{}
		x=\rho\sin(\theta)\cos(\varphi)\\
		y=\rho\sin(\theta)\sin(\varphi)\\
		z=\rho\cos(\theta)
	\end{numcases}
\end{subequations}
avec $0\leq\theta\leq\pi$ et $0\leq\varphi<2\pi$.

\begin{remark}
	Attention : d'un livre à l'autre les conventions sur les noms des angles changent. N'essayez donc pas d'étudier par cœur des formules concernant les coordonnées sphériques trouvées autre part. Par exemple sur le premier dessin de \href{http://fr.wikipedia.org/wiki/Coordonnées_sphériques}{wikipédia}, l'angle $\varphi$ est noté $\theta$ et l'angle $\theta$ est noté $\Phi$. Mais vous noterez que sur cette même page, les conventions de noms de ces angles changent plusieurs fois.
\end{remark}

Les coordonnées sphériques sont données par
\begin{equation}		\label{EqChmVarSpherique}
	\left\{
\begin{array}{lllll}
x=r\cos\theta\sin\varphi	&			&r\in\mathopen] 0 , \infty \mathclose[\\
y=r\sin\theta\sin\varphi	&	\text{avec}	&\theta\in\mathopen] 0 , 2\pi \mathclose[\\
z=r\cos\varphi			&			&\phi\in\mathopen] 0 , \pi \mathclose[.
\end{array}
\right.
\end{equation}
Le jacobien associé est $Jg(r,\theta,\varphi)=-r^2\sin\varphi$. Rappelons que ce qui rentre dans l'intégrale est la valeur absolue du jacobien.

\begin{subequations}
    \begin{numcases}{}
        x=\rho\cos\theta\sin\phi\\
        y=\rho\sin\theta\sin\phi\\
        z=\rho\cos\phi
    \end{numcases}
\end{subequations}
avec \( \rho\in\mathopen] 0 , \infty \mathclose[\), \( \theta\in\mathopen[ 0 , 2\pi [\) et \( \phi\in\mathopen[ 0 , \pi [\). Le jacobien vaut \( -\rho^2\sin(\phi)\).

N'oubliez pas que lorsqu'on effectue un changement de variables dans une intégrale, la \emph{valeur absolue} du jacobien apparaît.

Cependant notre convention de coordonnées sphériques fait venir \( \sin(\phi)\) avec \( \phi\in\mathopen[ 0 , \pi [\); vu que le signe de \( \sin(\phi)\) y est toujours positif, cette histoire de valeur absolue est sans grandes conséquent. Ce n'est pas le cas de toutes les conventions possibles.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    \subsubsection{Coordonnées sphériques : inverse}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Trouvons le changement inverse, c'est-à-dire trouvons $\rho$, $\theta$ et $\varphi$ en termes de $x$, $y$ et $z$. D'abord nous avons
\begin{equation}
	\rho=\sqrt{x^2+y^2+z^2}.
\end{equation}
Ensuite nous savons que
\begin{equation}
	\cos(\theta)=\frac{ z }{ \rho }
\end{equation}
détermine de façon unique\footnote{Le problème $\rho=0$ ne se pose pas; pourquoi ?} un angle $\theta\in\mathopen[ 0 , \pi \mathclose]$. Dès que $\rho$ et $\theta$ sont connus, nous pouvons poser $r=\rho\sin\theta$ et alors nous nous trouvons avec les équations
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\varphi)\\
		y=r\sin(\varphi),
	\end{numcases}
\end{subequations}
qui sont similaires à celles déjà étudiées dans le cas des coordonnées polaires.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Calcul de limites}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Beaucoup de techniques de calcul de limites fonctionnent bien avec les fonctions trigonométriques, entre autres grâce à l'utilisation des coordonnées polaires de la proposition~\ref{PROPooFLUAooDsyMXO}. De plus, le théorème de la fonction implicite Nous en voyons quelques exemples à présent.

\begin{example}[Limite et prolongement par continuité] \label{ExQWHooGddTLE}
    La fonction
    \begin{equation}
        f(x)=\frac{ \cos(x)-1 }{ x }
    \end{equation}
    n'est pas définie en \( x=0\).

    Nous avons vu dans l'équation \eqref{SUBEQooTTNNooXzApSM} que \( \cos(0)=1\), donc la limite
    \begin{equation}
        \lim_{x\to 0} \frac{ \cos(x)-1 }{ x }
    \end{equation}
    est la limite définissant la dérivée de cosinus en \( 0\) (ici, le \( x\) joue le rôle de \( \epsilon\)). Le lemme~\ref{LEMooBBCAooHLWmno} nous donne la dérivée du cosinus comme étant le sinus. Nous avons donc :
    \begin{equation}
        \lim_{x\to 0} \frac{ \cos(x)-1 }{ x }=\sin(0)=0,
    \end{equation}
    et nous définissons le prolongement par continuité :
    \begin{equation}
        \tilde f(x)=\begin{cases}
            \frac{ \cos(x)-1 }{ x }    &   \text{si } x\neq 0\\
            0    &    \text{sinon}.
        \end{cases}
    \end{equation}

    Encore une fois, le graphe de la fonction \(\tilde f\) ne présente aucune particularité autour de \( x=0\).
    \begin{center}
        \input{auto/pictures_tex/Fig_RPNooQXxpZZ.pstricks}
    \end{center}
\end{example}

\begin{example}[Un calcul heuristique de limite]        \label{EXooINLRooPzRWEA}
    Soit à calculer la limite suivante :
    \begin{equation}
        \lim_{x\to 0} \frac{  e^{-2\cos(x)+2}\sin(x) }{ \sqrt{ e^{2\cos(x)+2}}-1 }.
    \end{equation}
    La stratégie que nous allons suivre pour calculer cette limite est de développer certaines parties de l'expression en série de Taylor, afin de simplifier l'expression. La première chose à faire est de remplacer $ e^{y(x)}$ par $1+y(x)$ lorsque $y(x)\to 0$. La limite devient
    \begin{equation}
        \lim_{x\to 0} \frac{ \big( -2\cos(x)+3 \big)\sin(x) }{ \sqrt{-2\cos(x)+2} }.
    \end{equation}
    Nous allons maintenant remplacer $\cos(x)$ par $1$ au numérateur et par $1-x^2/2$ au dénominateur. Pourquoi ? Parce que le cosinus du dénominateur est dans une racine, donc nous nous attendons à ce que le terme de degré deux du cosinus donne un degré un en dehors de la racine, alors que du degré un est exactement ce que nous avons au numérateur : le développement du sinus commence par $x$.

    Nous calculons donc
    \begin{equation}
        \begin{aligned}[]
            \lim_{x\to 0} \frac{ \sin(x) }{ \sqrt{-2\left( 1-\frac{ x^2 }{ 2 } \right)+2} }=\lim_{x\to 0} \frac{ \sin(x) }{ x }=1.
        \end{aligned}
    \end{equation}
    Tout ceci n'est évidemment pas très rigoureux, mais en principe vous avez tous les éléments en main pour justifier les étapes.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode des coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooWCGMooPrXSpt}

La proposition suivante exprime la définition de la limite en d'autres termes, et va être pratique dans le calcul de certaines limites.
\begin{proposition}		\label{PropMethodePolaire}
	Soit $f\colon D\subset\eR^m\to \eR^n$, $a$ un point d'accumulation de $D$ et $\ell\in \eR^n$. Nous définissons
	\begin{equation}
		E_r=\{ f(x)\tq x\in B(a,r)\cap D \},
	\end{equation}
	et
	\begin{equation}
		s_r=\sup\{ \| v-\ell \|\tq v\in E_r \}.
	\end{equation}
	Alors nous avons $\lim_{x\to a} f(x)=\ell$ si et seulement si $\lim_{r\to 0} s_r=0$.
\end{proposition}

Dans cette proposition, $E_r$ représente l'ensemble des valeurs atteintes par $f$ dans un rayon $r$ autour de $a$. Le nombre $s_r$ sélectionne, parmi toutes ces valeurs, celle qui est la plus éloignée de $\ell$ et donne la distance. En d'autres termes, $s_r$ est la distance maximale entre $f(x)$ et $\ell$ lorsque $x$ est à une distance au maximum $r$ de $a$.

Lorsque nous avons affaire à une fonction $f\colon \eR^2\to \eR$, cette proposition nous permet de calculer facilement les limites en passant aux coordonnées polaires.

\begin{example}		\label{ExempleMethodeTrigigi}
	Reprenons la fonction de l'exemple~\ref{ExFNExempleMethodeTrigigi}:
	\begin{equation}
		f(x,y)=\frac{ xy }{ x^2+y^2 }.
	\end{equation}
	Son domaine est $\eR^2\setminus\{ (0,0) \}$. Nous voulons calculer $\lim_{(x,y)\to(0,0)}f(x,y)$. Écrivons la définition de $E_r$~:
	\begin{equation}
		E_r=\{ f(x,y)\tq (x,y)\in B\big( (0,0),r \big) \}.
	\end{equation}
	Les points de la boule sont, en coordonnées polaires, les points de la forme $(\rho,\theta)$ avec $\rho<r$. La chose intéressante est que $f(\rho,\theta)$ est relativement simple (plus simple que la fonction départ). En effet en remplaçant tous les $x$ par $\rho\cos(\theta)$ et tous les $y$ par $\rho\sin(\theta)$, et en utilisant le fait que $\cos^2(\theta)+\sin^2(\theta)=1$, nous trouvons
	\begin{equation}		\label{Eq2807fpolairerhodeuxcossin}
		f(\rho,\theta)=\frac{ \rho^2\cos(\theta)\sin(\theta) }{ \rho^2 }=\cos(\theta)\sin(\theta).
	\end{equation}
	Cela signifie que
	\begin{equation}
		E_r=\{ \cos(\theta)\sin(\theta)\tq\theta\in\mathopen[ 0 , 2\pi [ \}.
	\end{equation}
	Prenons $\ell$ quelconque. Le nombre $s_r$ est le supremum des
	\begin{equation}
		\| \ell-\cos(\theta)\sin(\theta) \|
	\end{equation}
	lorsque $\theta$ parcours $\mathopen[ 0 , 2\pi \mathclose]$. Nous ne sommes pas obligés calculer la valeur exacte de $s_r$. Ce qui compte ici est que $s_r$ ne vaut certainement pas zéro, et ne dépend pas de $r$. Donc il est impossible d'avoir $\lim_{r\to 0} s_r=0$, et la fonction donnée n'a pas de limite en $(0,0)$.
\end{example}

Nous pouvons retenir cette règle pour calculer les limites lorsque $(x,y)\to(0,0)$ de fonctions $f\colon \eR^2\to \eR$ :
\begin{enumerate}
	\item
		passer en coordonnées polaires, c'est-à-dire remplacer $x$ par $\rho\cos(\theta)$ et $y$ par $\rho\sin(\theta)$;
	\item
		nous obtenons une fonction $g$ de $\rho$ et $\theta$. Si la limite $\lim_{r\to 0} g(r,\theta)$ n'existe pas ou dépend de $\theta$, alors la fonction n'a pas de limite. Si on peut majorer $g$ par une fonction ne dépendant pas de $\theta$, et que cette fonction a une limite lorsque $r\to 0$, alors cette limite est la limite de la fonction.
\end{enumerate}

La vraie difficulté de la technique des coordonnées polaires est de trouver le supremum de $E_r$, ou tout au moins de montrer qu'il est borné par une fonction qui a une limite qui ne dépend pas de $\theta$. Une des situations classiques dans laquelle c'est facile est lorsque la fonction se présente comme une fonction de $r$ multiplié par une fonction de $\theta$.

\begin{example}		\label{Exemplexyxsqysq}
	Soit à calculer la limite
	\begin{equation}
		\lim_{(x,y)\to(0,0)}xy\left( \frac{ x^2-y^2 }{ x^2+y^2 }\right).
	\end{equation}
	Le passage aux coordonnées polaires donne
	\begin{equation}
		f(r,\theta)=r^2\sin\theta\cos\theta(\cos^2\theta-\sin^2\theta).
	\end{equation}
	Déterminer le supremum de cela est relativement difficile. Mais nous savons que de toutes façons, la quantité $\sin\theta\cos\theta(\cos^2\theta-\sin^2\theta)$ est bornée par $1$. Donc
	\begin{equation}
		\| f(r,\theta) \|\leq r^2.
	\end{equation}
	Maintenant la règle de l'étau montre que $\lim_{(x,y)\to(0,0)}f(x,y)$ est zéro.

	La situation vraiment gênante serait celle avec une fonction de $\theta$ qui risque de s'annuler dans un dénominateur.
\end{example}

L'exemple~\ref{EXooSDHDooJzDioW} donnera un cas où la méthode fonctionne plus difficilement. Entre autres parce qu'il utilisera en même temps la méthode des chemins et celle des coordonnées polaires.

\begin{example}\label{ExmeASDLAf}
	Considérons fonction
	\begin{equation}
		f(x,y)=\frac{ x^2+y^2 }{ x-y }.
	\end{equation}
	Une mauvaise idée pour prouver que la limite n'existe pas pour $(x,y)\to(0,0)$ est de considérer le chemin $(t,t)$. En effet, la fonction n'existe pas sur ce chemin. Or la méthode des chemins parle uniquement de chemins contenus dans le domaine de la fonction.

	Nous prouvons que la limite n'existe pas en trouvant des chemins le long desquels les limites sont différentes. Si nous essayons le chemin \( (t,kt)\) avec \( k\) constant, nous trouvons
    \begin{equation}
        f(t,kt)=\frac{ t(1+k^2) }{ 1-k }.
    \end{equation}
    La limite \( t\to 0\) est hélas toujours \( 0\). Nous ne pouvons donc pas conclure.

    Nous allons maintenant utiliser la même technique que celle utilisée en coordonnées polaires. Vous noterez que dans ce cas, travailler en cartésiennes donne lieu à des calculs plus longs.  L'astuce consiste à prendre \( k\) non constant et à chercher par exemple \( k(t)\) de façon à avoir
    \begin{equation}
        \frac{ 1+k(t)^2 }{ 1-k(t) }=\frac{1}{ t }.
    \end{equation}
    Avec une telle fonction \( k\), la fonction \( t\mapsto f(t,tk(t))\) serait la constante \( 1\). L'équation à résoudre pour \( k\) est
    \begin{equation}
        tk^2+k+(t-1)=0,
    \end{equation}
    et les solutions sont
    \begin{equation}
        k(t)=\frac{ -1\pm\sqrt{1-4t(t-1)} }{ 2t }.
    \end{equation}
    Nous proposons donc les chemins
    \begin{equation}
        \begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            t    \\
            \frac{ -1\pm\sqrt{1-4t(t-1)}    }{2}
        \end{pmatrix}
    \end{equation}
    Nous devons vérifier deux points. D'abord que ce chemin est bien défini, et ensuite que \( tk(t)\) tend bien vers zéro lorsque \( t\to 0\) (sinon \( (t,k(t)t)\)) n'est pas un chemin passant par \( (0,0)\). Lorsque \( t\) est petit, ce qui se trouve sous la racine est proche de \( 1\) et ne pose pas de problèmes. Ensuite,
    \begin{equation}
        \lim_{t\to 0} tk(t)=\frac{ -1\pm 1 }{ 2 }.
    \end{equation}
    En choisissant le signe \( +\), nous trouvons un chemin qui nous convient.

    Ce que nous avons prouvé est que
    \begin{equation}
        f\left( t,   \frac{ -1+\sqrt{1-4t(t-1)}    }{2}\right)=1
    \end{equation}
    pour tout \( t\). Le long de ce chemin, la limite de \( f\) est donc \( 1\). Cette limite est différente des limites obtenues le long de chemins avec \( k\) constant. La limite \( \lim_{(x,y)\to (0,0)} f(x,y)\) n'existe donc pas.
\end{example}

\begin{example}\label{seno}
	Considérons la fonction (figure~\ref{LabelFigsenotopologo})

	\begin{equation}
		f(x,y)=\begin{cases}
			\sqrt{x^2+y^2}\sin\frac{1}{ x^2+y^2 }	&	\text{si }(x,y)\neq(0,0)\\
			0	&	 \text{si }(x,y)=(0,0),
		\end{cases}
	\end{equation}
    et cherchons la limite $(x,y)\to(0,0)$. Le passage en coordonnées polaires\footnote{Proposition~\ref{PROPooFLUAooDsyMXO}.} donne
	\begin{equation}		\label{EqFoncRho2907}
		f(\rho,\theta)=\rho\sin\frac{1}{ \rho }.
	\end{equation}
	Pour calculer la limite de cela lorsque $\rho\to 0$, nous remarquons que
	\begin{equation}
		0\leq|\rho\sin\frac{1}{ \rho }|\leq\rho
	\end{equation}
	parce que $\sin(\frac{1}{ \rho })\leq 1$ quel que soit $\rho$. Or évidemment $\lim_{\rho\to 0} \rho=0$, donc la limite de la fonction \eqref{EqFoncRho2907} est zéro et ne dépend pas de $\theta$. Nous en concluons que $\lim_{(x,y)\to(0,0)}f(x,y)=0$.
\end{example}
\newcommand{\CaptionFigsenotopologo}{La fonction de l'exemple~\ref{seno}.}
\input{auto/pictures_tex/Fig_senotopologo.pstricks}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode du développement asymptotique}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooRAKKooAnpvkE}

Nous savons  que nous pouvons développer certaines fonctions en série grâce au développement de Taylor (théorème~\ref{ThoTaylor}). Lorsque nous avons une limite à calculer, nous pouvons remplacer certaines parties de la fonction à traiter par la formule \eqref{subeqfTepseqb}. Cela est très utile pour comparer des fonctions trigonométrique à des polynômes.

\begin{example}		\label{ExamLimSinxxa}
    Nous calculons la limite
    \begin{equation}
        \lim_{x\to 0} \frac{ \sin(x) }{ x }
    \end{equation}
    à l'aide d'un tout petit développement en série. Pour information, la proposition~\ref{PROPooNBRBooEQBypy} nous fera la même limite sans développements.

    Une manière de la prouver est d'écrire
    \begin{equation}
		\sin(x)=x+h(x)
	\end{equation}
	avec $h\in o(x)$, c'est-à-dire $\lim_{x\to 0} h(x)/x=0$. Alors nous avons
	\begin{equation}
		\lim_{x\to 0} \frac{ \sin(x) }{ x }=\lim_{x\to 0} \frac{ x+h(x) }{ x }=\lim_{x\to 0} \frac{ x }{ x }+\lim_{x\to 0} \frac{ h(x) }{ x }=1.
	\end{equation}
\end{example}

L'utilisation de la proposition~\ref{PropLimCompose} permet d'utiliser cette technique dans le cadre de limites à plusieurs variables. Reprenons l'exemple~\ref{ExamLimSinxxa} un tout petit peu modifié :

\begin{example}
	Soit à calculer $\lim_{(x,y)\to(0,0)}f(x,y)$ où
	\begin{equation}
		f(x,y)=\frac{ \sin(xy) }{ xy }.
	\end{equation}
	La première chose à faire est de voir $f$ comme la composée de fonctions $f=f_1\circ f_2$ avec
	\begin{equation}
		\begin{aligned}
			f_1\colon \eR&\to \eR \\
			t&\mapsto \frac{ \sin(t) }{ t }
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\begin{aligned}
			f_2\colon \eR^2&\to \eR \\
			(x,y)&\mapsto xy.
		\end{aligned}
	\end{equation}
	Étant donné que $\lim_{(x,y)\to(0,0)}f_2(x,y)=0$, nous avons $\lim_{(x,y)\to(0,0)}f(x,y)=\lim_{t\to 0} f_1(t)=1$.
\end{example}

La proposition suivante reprend la limite de l'exemple~\ref{ExamLimSinxxa}.
\begin{proposition}     \label{PROPooNBRBooEQBypy}
    Nous avons la limite
    \begin{equation}\label{sinsurx}
      \lim_{x\to 0} \frac{\sin(x)}{x} = 1.
    \end{equation}
\end{proposition}

\begin{proof}
    On commence par observer que la fonction $g(x)=\frac{\sin(x)}{x}$ est un rapport entre deux fonctions impaires et est donc une fonction paire. Nous pouvons alors nous réduire à considérer le cas où $x$ est positif. La première étape de cette preuve nous dit que $g(x)\leq 1$ pour tout $x\in\eR^{+,*}$.

    Nous voulons étudier le comportement de $g$ dans un voisinage de $0$. Nous pouvons alors supposer que $x$ soit inférieur à $\pi/2$. Soit $D = (1, \tan (x))$. On voit très bien dans le dessin que l'aire du triangle de sommets $O$, $D$ et $C$ est supérieure à l'aire du secteur circulaire de sommets $O$, $A$ et $C$. Ces deux aires peuvent \^etre calculées très facilement et nous obtenons
    \begin{equation*}
      \frac{\sin(x)}{2\cos(x)} \geq \frac{x}{2}.
    \end{equation*}
    À partir de cette dernière inégalité nous pouvons écrire
    \begin{equation*}
      1\geq \frac{\sin(x)}{x}\geq \cos(x).
    \end{equation*}
    En prenant la limite lorsque $x$ tend vers $0$ dans les trois membres de l'inégalité la règle de l'étau nous permet d'obtenir la limite remarquable  \eqref{sinsurx}.
\end{proof}

\begin{example}     \label{EXooETZYooYsKPDJ}
Les dérivées partielles de la fonction $f(x,y)=xy^3+\sin y$ au point $(0,\pi)$ sont
\[
\partial_xf(0,\pi)=\frac{ \partial f }{ \partial x }(0,\pi)=\lim_{\begin{subarray}{l}
    t\to 0\\ t\neq 0
  \end{subarray}} \frac{(t\pi^3+\sin \pi)-(\sin \pi)}{t}= \pi^3,
\]
\[
\partial_yf(0,\pi)=\frac{ \partial f }{ \partial y }(0,\pi)=\lim_{\begin{subarray}{l}
    t\to 0\\ t\neq 0
  \end{subarray}} \frac{0(\pi+t)^3+\sin (t+\pi)-0\cdot \pi^3}{t}= \cos \pi=-1,
\]
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Changement de variables dans une intégrale}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooOOPPooZLbaEH}

Le théorème~\ref{THOooUMIWooZUtUSg} manque un peu d'exemples. Nous allons en voir quelques-uns maintenant.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Changement de variables}
%---------------------------------------------------------------------------------------------------------------------------

Le domaine $E=\{ (x,y)\in\eR^2\tq x^2+y^2<1 \}$ s'écrit plus facilement $E=\{ (r,\theta)\tq r<1 \}$ en coordonnées polaires. Le passage aux coordonnées polaires permet de transformer une intégration sur un domaine rond à une intégration sur le domaine rectangulaire $\mathopen]0,2\pi\mathclose[\times\mathopen]0,1\mathclose[$. La question est évidemment de savoir si nous pouvons écrire
\begin{equation}
	\int_Ef=\int_{0}^{2\pi}\int_0^1f(r\cos\theta,r\sin\theta)drd\theta.
\end{equation}
Hélas, non; la vie n'est pas aussi simple.

\begin{definition}
    Un \defe{difféomorphisme}{difféomorphisme} est une application $g\colon A\to B$ telle que $g$ et $g^{-1}\colon B\to A$ soient de classe $C^1$.
\end{definition}

\begin{theorem}
Soit $g\colon A\to B$ un difféomorphisme. Soient $F\subset B$ un ensemble mesurable et borné et $f\colon F\to \eR$ une fonction bornée et intégrable. Supposons que $g^{-1}(F)$ soit borné et que $Jg$ soit borné sur $g^{-1}(F)$. Alors
\begin{equation}
	\int_Ff(x)dy=\int_{g^{-1}(F)f\big( g(x) \big)}| Jg(x) |dx
\end{equation}
\end{theorem}
Pour rappel, $Jg$ est le déterminant de la matrice \href{http://fr.wikipedia.org/wiki/Matrice_jacobienne}{jacobienne} (aucun lien de \href{http://fr.wikipedia.org/wiki/Jacob}{parenté}) donnée par
\begin{equation}
	Jg=\det\begin{pmatrix}
	\partial_xg_1	&	\partial_yg_1	\\
	\partial_xg_2	&	\partial_tg_2
\end{pmatrix}.
\end{equation}

Comme dans les intégrales simples, il y a souvent moyen de trouver un changement de variables qui simplifie les expressions.  Le domaine $E=\{ (x,y)\in\eR^2\tq x^2+y^2<1 \}$ par exemple s'écrit plus facilement $E=\{ (r,\theta)\tq r<1 \}$ en coordonnées polaires. Le passage aux coordonnées polaires permet de transformer une intégration sur un domaine rond à une intégration sur le domaine rectangulaire $\mathopen]0,2\pi\mathclose[\times\mathopen]0,1\mathclose[$. La question est évidemment de savoir si nous pouvons écrire
\begin{equation}
	\int_Ef=\int_{0}^{2\pi}\int_0^1f(r\cos\theta,r\sin\theta)drd\theta.
\end{equation}
Hélas ce n'est pas le cas. Il faut tenir compte du fait que le changement de base dilate ou contracte certaines surfaces.

Soit $\varphi\colon D_1\subset\eR^2\to D_2\subset \eR^2$ une fonction bijective de classe $C^1$ dont l'inverse est également de classe $C^1$. On désigne par $x$ et $y$ ses composantes, c'est-à-dire que
\begin{equation}
    \varphi(u,v)=\begin{pmatrix}
        x(u,v)    \\
        y(u,v)
    \end{pmatrix}
\end{equation}
avec $(u,v)\in D_1$.

\begin{theorem}     \label{ThoChamDeVarIntDDf}
    Soit une fonction continue $f\colon D_2\to \eR$. Alors
    \begin{equation}
        \int_{\varphi(D_1)}f(x,y)dxdy=\int_{D_1}f\big( x(u,v),y(u,v) \big)| J_{\varphi}(u,v) |dudv
    \end{equation}
    où $J_{\varphi}$ est le Jacobien de $\varphi$ c'est-à-dire
    \begin{equation}
        J_{\varphi}(u,v)=\det\begin{pmatrix}
            \frac{ \partial x }{ \partial u }    &   \frac{ \partial x }{ \partial v }    \\
            \frac{ \partial y }{ \partial u }    &   \frac{ \partial u }{ \partial v }
        \end{pmatrix}.
    \end{equation}
\end{theorem}
Ne pas oublier de prendre la valeur absolue lorsqu'on utilise le Jacobien dans un changement de variables.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    Calculons la surface du disque $D$ de rayon $R$. Nous devons calculer
    \begin{equation}
        \int_Ddxdy.
    \end{equation}
    Pour passer au polaires, nous savons que le disque est décrit par
    \begin{equation}
        D=\{ (r,\theta)\tq 0\leq r\leq R,0\leq\theta\leq 2\pi \}.
    \end{equation}
    Nous avons donc
    \begin{equation}
        \int_Ddxdy=\int_{D}r\,drd\theta=\int_0^{2\pi}\int_0^Rr\,drd\theta=2\pi\int_0^Rr\,dr=\pi R^2.
    \end{equation}
\end{example}

\begin{example}     \label{ExpmfDtAtV}
    Montrons comment intégrer la fonction $f(x,y)=\sqrt{1-x^2-y^2}$ sur le domaine délimité par la droite $y=x$ et le cercle $x^2+y^2=y$, représenté sur la figure~\ref{LabelFigHFAYooOrfMAA}. Pour trouver le centre et le rayon du cercle $x^2+y^2=y$, nous commençons par écrire $x^2+y^2-y=0$, et ensuite nous reformons le carré : $y^2-y=(y-\frac{ 1 }{2})^2-\frac{1}{ 4 }$.

\newcommand{\CaptionFigHFAYooOrfMAA}{Passage en polaire pour intégrer sur un morceau de cercle.}
\input{auto/pictures_tex/Fig_HFAYooOrfMAA.pstricks}

    Le passage en polaire transforme les équations du bord du domaine en
    \begin{equation}
        \begin{aligned}[]
            \cos(\theta)&=\sin(\theta)\\
            r^2&=r\sin(\theta).
        \end{aligned}
    \end{equation}
    L'angle $\theta$ parcours donc $\mathopen] 0 , \pi/4 \mathclose[$, et le rayon, pour chacun de ces $\theta$ parcours $\mathopen] 0 , \sin(\theta) \mathclose[$. La fonction à intégrer se note maintenant $f(r,\theta)=\sqrt{1-r^2}$. Donc l'intégrale à calculer est
    \begin{equation}		\label{PgOMRapIntMultFubiniBoutCercle}
        \int_{0}^{\pi/4}\left( \int_0^{\sin(\theta)}\sqrt{1-r^2}r\,rd \right).
    \end{equation}
    Remarquez la présence d'un $r$ supplémentaire pour le jacobien.

    Notez que les coordonnées du point $P$ sont $(1,1)$.
\end{example}

En pratique, lors du passage en coordonnées polaires, le «$dxdy$» devient «$r\,drd\theta$».

\begin{example}
    On veut évaluer l'intégrale de la fonction $f(x,y)= x^2+y^2$ sur la région $V$ suivante :
    \[
    V=\{(x,y) \in \eR^2\,\vert\, x^2+y^2\leq 1,\, x>0,\, y>0\}.
    \]
    On peut faire le calcul directement,
    \[
    \int_{V}f(x,y)\, dV=\int_0^1\int_0^{\sqrt{1-x^2}}x^2+y^2\, dy\,dx=\int_0^1\left(x^2\sqrt{1-x^2} + \frac{(1-x^2)^{3/2}}{3}\right) dx
    \]
    mais c'est un peu ennuyeux. On peut simplifier beaucoup les calculs avec un changement de variables vers les coordonnées polaires. Dans ce cas, on sait bien que le difféomorphisme à utiliser est $\phi(r,\theta)=(r\cos \theta, r\sin\theta)$. Le jacobien  $J_{\phi}$ est
    \begin{equation}
     J_{\phi}(r, \theta)= \left\vert\begin{array}{cc}
    \cos \theta & \sin \theta \\
    -r\sin \theta  & r\cos \theta
    \end{array}\right\vert= r,
    \end{equation}
    qui est toujours positif. D'une part, la fonction $f$ peut s'écrire sous la forme $f(\phi(r,\theta))=r^2$ et d'autre part, $\phi^{-1}(V)=]0,1]\times]0, \pi/2[$. Par conséquent, la formule du changement de variables nous donne
    \[
    \int_{V}f(x,y)\, dV=\int_0^{\pi/2}\int_0^{1}r^3 dr\,d\theta=\int_0^{\pi/2}\frac{1}{4}\,d\theta=\frac{\pi}{8}.
    \]
\end{example}

\begin{example}
    Montrons comment intégrer la fonction $f(x,y)=\sqrt{1-x^2-y^2}$ sur le domaine délimité par la droite $y=x$ et le cercle $x^2+y^2=y$, représenté sur la figure~\ref{LabelFigQXyVaKD}. Pour trouver le centre et le rayon du cercle $x^2+y^2=y$, nous commençons par écrire $x^2+y^2-y=0$, et ensuite nous reformons le carré : $y^2-y=(y-\frac{ 1 }{2})^2-\frac{1}{ 4 }$.
    \newcommand{\CaptionFigQXyVaKD}{Passage en polaire pour intégrer sur un morceau de cercle.}
\input{auto/pictures_tex/Fig_QXyVaKD.pstricks}

    Le passage en polaire transforme les équations du bord du domaine en
    \begin{equation}
        \begin{aligned}[]
            \cos(\theta)&=\sin(\theta)\\
            r^2&=r\sin(\theta).
        \end{aligned}
    \end{equation}
    L'angle $\theta$ parcours donc $\mathopen] 0 , \pi/4 \mathclose[$, et le rayon, pour chacun de ces $\theta$ parcours $\mathopen] 0 , \sin(\theta) \mathclose[$. La fonction à intégrer se note maintenant $f(r,\theta)=\sqrt{1-r^2}$. Donc l'intégrale à calculer est
    \begin{equation}		\label{PgRapIntMultFubiniBoutCercle}
        \int_{0}^{\pi/4}\left( \int_0^{\sin(\theta)}\sqrt{1-r^2}r\,rd \right).
    \end{equation}
    Remarquez la présence d'un $r$ supplémentaire pour le jacobien.

    Notez que les coordonnées du point $P$ sont $(1,1)$.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées cylindriques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    On veut calculer le volume de la région $A$ définie par  l'intersection entre la boule unité et le cylindre qui a pour base un disque de rayon $1/2$ centré en $(0, 1/2)$
    \[
    A=\{(x,y,z) \in\eR^3 \,\vert\, x^2+y^2+z^1\leq 1\}\cap\{(x,y,z) \in \eR^3\,\vert\, x^2+(y-1/2)^2\leq 1/4\}.
    \]
    On peut décrire $A$ en coordonnées cylindriques
    \begin{equation}
      \begin{aligned}
        A=\Big\{(r,\theta,z) &\in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\,\\
    & -\pi/2<\theta<\pi, \, 0<r\leq \sin\theta, \, -\sqrt{1-r^2}\leq z\leq\sqrt{1-r^2} \Big\}.
      \end{aligned}
    \end{equation}
    Le jacobien de ce changement de variables,  $J_{cyl}$, est
    \begin{equation}
     J_{cyl}(r, \theta), z= \left\vert\begin{array}{ccc}
    \cos \theta & \sin \theta & 0\\
    -r\sin \theta  & r\cos \theta &0 \\
    0&0&
    \end{array}\right\vert= r,
    \end{equation}
    qui est toujours positif. Le volume de $A$ est donc
    \[
    \int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi/2}^{\pi/2}\int_0^{\sin\theta}\int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}} r dz\,dr\,d\theta=\frac{2\pi}{8}+\frac{8}{9}.
    \]
\end{example}

\begin{example}[Volume d'un solide de révolution]
Soit $g:[a,b]\to\eR_+$ une fonction continue et positive. On dit que le solide $A$ décrit par
\[
A=\left\{(x,y,z)\in\eR^3\, \vert \, z\in[a,b], \,\sqrt{x^2+y^2}\leq g^2(z) \right\}
\]
est un solide de révolution. Afin de calculer son volume, on peut décrire $A$ en coordonnées cylindriques,
\[
A=\left\{(r,\theta,z) \in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\, a\leq z\leq b, \, 0<r^2\leq g^2(z) \right\}.
\]
Le jacobien de ce changement de variables est  $J_{cyl}=r$, comme dans l'exemple précédent. Le volume de $A$ est donc
\[
\int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_a^{b}\int_{-\pi}^{\pi}\int_{0}^{g(z)} r  \,dr\,d\theta\, dz=\int_a^{b} \pi g^2(z) \, dz.
\]
Cette formule peut être utilisée pour tout solide de révolution.
\end{example}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées sphériques}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le calcul est un peu plus long :
\begin{equation}
    \begin{aligned}[]
        J(\rho,\theta,\varphi)&=\begin{vmatrix}
            \frac{ \partial x }{ \partial \rho }    &   \frac{ \partial x }{ \partial \theta }    &   \frac{ \partial x }{ \partial \varphi }    \\
            \frac{ \partial y }{ \partial \rho }    &   \frac{ \partial y }{ \partial \theta }    &   \frac{ \partial y }{ \partial \varphi }    \\
            \frac{ \partial z }{ \partial \rho }    &   \frac{ \partial z }{ \partial \theta }    &   \frac{ \partial z }{ \partial \varphi }
        \end{vmatrix}\\
        &=
        \begin{vmatrix}
            \sin\theta\cos\varphi    &   \rho\cos\theta\cos\varphi    &   -\rho\sin\theta\sin\varphi    \\
            \sin\theta\sin\varphi    &   \rho\cos\theta\sin\varphi    &   -\rho\sin\theta\cos\varphi    \\
            \cos\theta               &   -\rho\sin\theta              &   0
        \end{vmatrix}\\
        &=\rho^2\sin\theta.
    \end{aligned}
\end{equation}
Donc
\begin{equation}
    dx\,dy\,dz=\rho^2\sin(\theta)\,d\rho\,d\theta\,d\varphi.
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées sphériques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    On veut calculer le volume du cornet de glace  $A$
    \[
    A=\left\{(x,y,z)\in\eR^3\, \vert \, (x,y)\in \mathbb{S}^2, \,\sqrt{x^2+y^2}\leq z\leq \sqrt{1-x^2-y^2} \right\}.
    \]
    On peut décrire $A$ en coordonnées sphériques.
    \[
    A=\{(\rho,\theta,\phi) \in ]0, +\infty[\times [-\pi,\pi[\times [0,\pi[\,\vert\, 0<\phi\leq\pi/4, \, 0<\rho\leq 1 \}.
    \]
    Le jacobien de ce changement de variables  $J_{sph}$ est
    \begin{equation}
     J_{sph}(\rho, \theta, \phi)= \left\vert\begin{array}{ccc}
    \cos \theta \sin\phi & \sin \theta\sin\phi & \cos\phi\\
    -\rho\sin \theta\sin\phi  & \rho\cos \theta\sin\phi & 0 \\
    \rho\cos\theta\cos\phi&\rho\sin\theta\cos\phi& -\rho\sin\phi
    \end{array}\right\vert= \rho^2\sin\phi,
    \end{equation}
    Le volume de $A$ est donc
    \[
    \int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi}^{\pi}\int_0^{\pi/4}\int_{0}^{1}\rho^2\sin\phi \,d\rho\,d\phi\,d\theta=\frac{2\pi}{3}\left(1-\frac{1}{\sqrt{2}}\right).
    \]
\end{example}

\begin{example}[Une petite faute à ne pas faire]
    Si nous voulons calculer le volume de la sphère de rayon $R$, nous écrivons donc
    \begin{equation}
        \int_0^Rdr\int_{0}^{2\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi=4\pi R=\frac{ 4 }{ 3 }\pi R^3.
    \end{equation}
    Ici, la valeur absolue n'est pas importante parce que lorsque $\phi\in\mathopen] 0,\pi ,  \mathclose[$, le sinus de $\phi$ est positif.

    Des petits malins pourraient remarquer que le changement de variable \eqref{EqChmVarSpherique} est encore une paramétrisation de $\eR^3$ si on intervertit le domaine des angles :
    \begin{equation}
        \begin{aligned}[]
            \theta&\colon 0 \to \pi\\
            \phi	&\colon 0\to 2\pi,
        \end{aligned}
    \end{equation}
    alors nous paramétrons encore parfaitement bien la sphère, mais hélas
    \begin{equation}		\label{EqVolumeIncorrectSphere}
        \int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{2\pi}r^2 \sin(\phi)d\phi=0.
    \end{equation}
    Pourquoi ces «nouvelles» coordonnées sphériques sont-elles mauvaises ? Il y a que quand l'angle $\phi$ parcours $\mathopen] 0 , 2\pi \mathclose[$, son sinus n'est plus toujours positif, donc la \emph{valeur absolue} du jacobien n'est plus $r^2\sin(\phi)$, mais $r^2\sin(\phi)$ pour les $\phi$ entre $0$ et $\pi$, puis $-r^2\sin(\phi)$ pour $\phi$ entre $\pi$ et $2\pi$. Donc l'intégrale \eqref{EqVolumeIncorrectSphere} n'est pas correcte. Il faut la remplacer par
    \begin{equation}
        \int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi- \int_0^Rdr\int_{0}^{\pi}d\theta\int_{\pi}^{2\pi}r^2 \sin(\phi)d\phi = \frac{ 4 }{ 3 }\pi R^3
    \end{equation}

\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Un autre système utile}
%---------------------------------------------------------------------------------------------------------------------------

Un changement de variables que l'on voit assez souvent est
\begin{subequations}
    \begin{numcases}{}
        u=x+y\\
        v=x-y.
    \end{numcases}
\end{subequations}
Afin de calculer son jacobien, il faut d'abord exprimer $x$ et $y$ en fonctions de $u$ et $v$ :
\begin{subequations}
    \begin{numcases}{}
        x=(u+v)/2\\
        y=(u-v)/2.
    \end{numcases}
\end{subequations}
La matrice jacobienne est
\begin{equation}
    \begin{pmatrix}
        \frac{ \partial x }{ \partial u }    &   \frac{ \partial x }{ \partial v }    \\
        \frac{ \partial y }{ \partial u }    &   \frac{ \partial y }{ \partial v }
    \end{pmatrix}=
    \begin{pmatrix}
        \frac{ 1 }{2}    &   \frac{ 1 }{2}    \\
        \frac{ 1 }{2}    &   -\frac{ 1 }{2}
    \end{pmatrix}.
\end{equation}
Le déterminant vaut $-\frac{1}{ 2 }$. Nous avons donc
\begin{equation}
    dxdy=\frac{ 1 }{2}dudv.
\end{equation}
Nous insistons sur le fait que c'est $\frac{ 1 }{2}$ et non $-\frac{ 1 }{2}$ qui intervient parce que que la formule du changement de variable demande d'introduire la \emph{valeur absolue} du jacobien.

\begin{example}
    Calculer l'intégrale de la fonction $f(x,y)=x^2-y^2$ sur le domaine représenté sur la figure~\ref{LabelFigVWFLooPSrOqz}. % From file VWFLooPSrOqz
\newcommand{\CaptionFigVWFLooPSrOqz}{Un domaine qui s'écrit étonnament bien avec un bon changement de coordonnées.}
\input{auto/pictures_tex/Fig_VWFLooPSrOqz.pstricks}

    Les droites qui délimitent le domaine d'intégration sont
    \begin{equation}
        \begin{aligned}[]
            y&=-x+2\\
            y&=x-2\\
            y&=x\\
            y&=-x
        \end{aligned}
    \end{equation}
    Le domaine est donc donné par les équations
    \begin{subequations}
        \begin{numcases}{}
            y+x<2\\
            y-x>-2\\
            y-x<0 \\
            y+x>0.
        \end{numcases}
    \end{subequations}
    En utilisant le changement de variables $u=x+y$, $v=x-y$ nous trouvons le domaine $0<u<2$, $0<v<2$. En ce qui concerne la fonction, $f(x,y)=(x+y)(x-y)$ et par conséquent
    \begin{equation}
        f(u,v)=uv.
    \end{equation}
    L'intégrale à calculer est simplement
    \begin{equation}
        \int_0^2\int_0^2 uv\,dudv=\int_0^2 u\,du\left[ \frac{ v^2 }{ 2 } \right]_0^2=2\int_0^2u\,du=4.
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Table de caractères du groupe diédral}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecWMzheKf}
Cette section vient de \cite{KXjFWKA}; nous avons comme but d'établir la table des caractères des représentations complexes du groupe diédral \( D_n\).
\index{groupe!de permutation}
\index{groupe!diédral!générateurs (utilisation)}
\index{représentation!groupe diédral}
\index{caractère!groupe diédral}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Représentations de dimension un}
%---------------------------------------------------------------------------------------------------------------------------

Nous nous occupons des représentations de \( D_n\) sur \( \eC\). Les applications linéaires \( \eC\to \eC\) sont seulement les multiplications par des nombres complexes. Nous cherchons donc \( \psi\colon D_n\to \eC^*\).

Nous savons que \( D_n\) est généré\footnote{Voir proposition~\ref{PropLDIPoZ} et tout ce qui suit.} par \( s\) et \( r\). Vu que \( s^2=1\), nous avons
\begin{equation}
    \psi(s)^2=\psi(s^2)=\psi(1)=1,
\end{equation}
donc \( \psi(s)\in\{ -1,1 \}\). Nous savons aussi que \( srsr=1\), donc
\begin{equation}
    \psi(s)^2\psi(r)^2=1,
\end{equation}
ce qui donne \( \psi(r)\in\{ -1,1 \}\).

Nous avons donc quatre représentations de dimension un données par
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &\psi(r)=1&\psi(r)=-1\\
        \hline\hline
        \psi(s)=1&\rho^{++}&\rho^{+-}\\
        \hline
        \psi(s)=-1&\rho^{-+}&\rho^{--}\\
        \hline
    \end{array}
\end{equation*}
Attention au fait que nous devons aussi avoir la relation \( \psi(r)^n=\psi(r^n)=1\). Donc \( \psi(r)\) doit être une racine \( n\)\ieme\ de l'unité. Nous allons donc devoir avoir un compte différent selon la parité de \( n\). Nous en reparlerons à la fin, au moment de faire les comptes. En ce qui concerne les caractères correspondants,
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &r^k&sr^k\\
        \hline\hline
        \chi^{++}&1&1\\
        \hline
        \chi^{+-}&(-1)^k&(-1)^k\\
        \hline
        \chi^{-+}&1&-1\\
        \hline
        \chi^{--}&(-1)^k&(-1)^{k+1}\\
        \hline
    \end{array}
\end{equation*}
Étant donné qu'ils sont tous différents, ce sont des représentations deux à deux non équivalentes, lemme~\ref{LempUSOlo}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Représentations de dimension deux}
%---------------------------------------------------------------------------------------------------------------------------

Nous cherchons maintenant les représentations \( \rho\colon D_n\to \End(\eC^2)\). Ici nous supposons connue la liste des éléments de \( D_n\) donnée par le corollaire~\ref{CorWYITsWW}. Soit \( \omega= e^{2i\pi/n}\) et \( h\in \eZ\); nous considérons la représentation \( \rho^{(h)}\) de \( D_n\) définie par
\begin{subequations}
    \begin{align}
        \rho^{(h)}(r^k)&=\begin{pmatrix}
            \omega^{hk}    &   0    \\
            0    &   \omega^{-hk}
        \end{pmatrix}\\
        \rho^{(h)}(st^k)&=\begin{pmatrix}
            0    &   \omega^{-hk}    \\
            \omega^{hk}    &   0
        \end{pmatrix}.
    \end{align}
\end{subequations}
Cela donne bien \( \rho^{(h)}\) sur tous les éléments de \( D_n\) par la proposition~\ref{PropLDIPoZ}. Nous pouvons restreindre le domaine de \( h\) en remarquant d'abord que \( \rho^{(h)}=\rho^{(h+n)}\), et ensuite que les représentations \( \rho^{(h)}\) et \( \rho^{(-h)}\) sont équivalentes. Un opérateur d'entrelacement est donné par \( T=\begin{pmatrix}
    0    &   1    \\
    1    &   0
\end{pmatrix}\), et il est facile de vérifier que \( T\rho^{(h)}(x)=\rho^{-h}(x)T\) avec \( x=r^k\) puis avec \( x=sr^k\).

Donc \( \rho^{(h)}\simeq\rho^{(-h)}\simeq\rho^{(n-h)}\) et nous pouvons restreindre notre étude à \( 0\leq h\leq \frac{ n }{2}\).

Nous allons séparer les cas \( n=0\), \( h=n/2\) et les autres. En effet si nous notons par commodité \( a=\omega^h\), alors un vecteur \( (x,y)\) est vecteur propre de \( \rho^{(h)}(s)\) et de \( \rho^{(h)}(r)\) si et seulement s'il vérifie les systèmes d'équations
\begin{subequations}        \label{SubEqsGXZoxLq}
    \begin{numcases}{}
        ax=\lambda x\\
        \frac{1}{ a }y=\lambda y
    \end{numcases}
\end{subequations}
et
\begin{subequations}    \label{SubEqsFYZmzhT}
    \begin{numcases}{}
        \frac{1}{ a }y=\mu x\\
        ax=\mu y
    \end{numcases}
\end{subequations}
avec \( \lambda\) et \( \mu\) des nombres non nuls. Une représentation sera réductible si et seulement si ces deux systèmes acceptent une solution non nulle commune. Il est vite vu que si \( x\neq 0\) et \( y\neq 0\), alors \( a^2=1\), ce qui signifie \( h=0\) ou \( h=n/2\). Sinon, il n'y a pas de solutions, et la représentation associée est irréductible.

\begin{enumerate}
    \item
        \( h=0\). Nous avons
        \begin{equation}
            \begin{aligned}[]
                \rho^{(0)}(r^k)&=\begin{pmatrix}
                    1    &   0    \\
                    0    &   1
                \end{pmatrix}& \rho^{(0)}(sr^k)=\begin{pmatrix}
                    0    &   1    \\
                    1    &   0
                \end{pmatrix},
            \end{aligned}
        \end{equation}
        donc le caractère de cette représentation est \( \chi^{(0)}(r^k)=2\) et \( \chi^{(0)}(sr^k)=0\). Donc nous avons
        \begin{equation}
            \chi^{(0)}=\chi^{++}+\chi^{-+}.
        \end{equation}
        Il y a maintenant (au moins) quatre façons de voir que la représentation \( \rho^{(0)}\) est réductible.
        \begin{description}

            \item[Première méthode]
                Trouver un opérateur d'entrelacement. Pour cela nous calculons les matrices :
        \begin{subequations}
            \begin{align}
                S(r)&=(\rho^{++}\oplus \rho^{-+})(r^k)=\begin{pmatrix}
                    \rho^{++}(r^k)    &   0    \\
                    0  &   \rho^{-+}(r^k)
                \end{pmatrix}=\begin{pmatrix}
                    1    &   0    \\
                    0    &   1
                \end{pmatrix}\\
                S(sr^k)&=(\rho^{++}\oplus \rho^{-+})(sr^k)=\begin{pmatrix}
                    \rho^{++}(sr^k)    &   0    \\
                    0  &   \rho^{-+}(sr^k)
                \end{pmatrix}=\begin{pmatrix}
                    1    &   0    \\
                    0    &   -1
                \end{pmatrix}\\
            \end{align}
        \end{subequations}
        Nous cherchons une matrice \( T\) telle que \( TS(r^k)=\rho^{(0)}(r^k)T\) et \( TS(sr^k)=\rho^{(0)}(sr^k)T\). Étant donné que \( S(r^k)=\mtu=\rho^{(0)}(r^k)\), la première contrainte n'en est pas une. Nous pouvons vérifier qu'avec \( T=\begin{pmatrix}
            1    &   1    \\
            1    &   -1
        \end{pmatrix}\), nous avons bien
        \begin{equation}
            T\begin{pmatrix}
                1    &   0    \\
                0    &   -1
            \end{pmatrix}=\begin{pmatrix}
                0    &   1    \\
                1    &   0
            \end{pmatrix}.
        \end{equation}
        Donc ce \( T\) entrelace \( \rho^{++}\oplus \rho^{-+}\) avec \( \rho^{(0)}\) qui sont donc deux représentations équivalentes. Donc \( \rho^{(0)}\) est réductible et ça ne nous intéresse pas de la lister.
            \item[Seconde méthode]
                Invoquer le théorème \ref{ThoWGkfADd}\ref{ItemZReOWoHi} et dire que les représentations sont équivalentes parce que les caractères sont égaux.

    \item[Troisième méthode]
        Utiliser le théorème~\ref{ThoWGkfADd}\ref{ItemZReOWoHii} et calculer \( \langle \chi^{(0)}, \chi^{(0)}\rangle \) :
        \begin{subequations}
            \begin{align}
                \langle \chi^{(0)}, \chi^{(0)}\rangle &=\frac{1}{ | D_n | }\sum_{g\in D_n}| \chi^{(0)}(g) |^2\\
                &=\frac{1}{ 2n }\big(4+0+4(n-1)\big)\\
                &=2.
            \end{align}
        \end{subequations}
        Ici le \( 4\) est pour le \( 1\), le zéro est pour les termes \( sr^k\) et \( 4(n-1)\) est pour les \( n-1\) termes \( r^k\). Vu que le résultat n'est pas \( 1\), la représentation \( \rho^{(0)}\) n'est pas irréductible.

    \item[Quatrième méthode]
        Regarder les solutions des systèmes \eqref{SubEqsGXZoxLq} et \eqref{SubEqsFYZmzhT} dont nous avons parlé plus haut.

    \end{description}

    La première méthode a l'avantage d'être simple et ne demander aucune théorie particulière à part les définitions. La seconde méthode est la plus rapide, mais demande un théorème très puissant. La troisième utilise également un théorème assez avancé, mais a l'avantage sur les deux autres méthodes de ne pas avoir besoin de savoir a priori un candidat décomposition de \( \rho^{0)}\); cette méthode est applicable même sans faire la remarque que \( \chi^{(0)}=\chi^{++}+\chi^{-+}\).

    Quoi qu'il en soit, nous ne listons pas \( \chi^{(0)}\) dans notre \href{http://fr.wikipedia.org/wiki/Aide:Unicode}{table de caractères}.

    \item
        \( h=n/2\). Vu que \( \omega^{n/2}= e^{i\pi}=-1\), nous avons
        \begin{equation}
            \begin{aligned}[]
                \rho^{(n/2)}(r^k)&=\begin{pmatrix}
                    (-1)^k    &   0    \\
                    0    &   (-1)^k
                \end{pmatrix}&
                \rho^{(n/2)}(sr^k)&=\begin{pmatrix}
                    0   &   (-1)^k    \\
                    (-1)^k    &  0
                \end{pmatrix}&
            \end{aligned},
        \end{equation}
        et donc
        \begin{subequations}
            \begin{align}
                \chi^{(n/2)}(r^k)&=2(-1)^k\\
                \chi^{(n/2)}(sr^k)&=0.
            \end{align}
        \end{subequations}
        Il est vite vu que \( \chi^{(n/2)}=\chi^{+-}+\chi^{-+}\). Ergo la représentation \( \rho^{(n/2)}\) n'est pas irréductible.

    \item
        \( 0<h<\frac{ n }{2}\). Dans ce cas nous avons \( \omega^h\neq \omega^{-h}\), et en regardant les systèmes d'équations donnés plus haut, nous voyons que \( \rho^{(h)}(s)\) et \( \rho^{(h)}(r)\) n'ont pas de vecteurs propres communs. Donc ces représentations sont irréductibles.

        Nous devons cependant encore vérifier si elles sont deux à deux non équivalentes. Supposons que pour \( h\neq h'\) nous ayons une matrice \( T\in \GL(2,\eC)\) telle que \( T\rho^{(h)}(r)T^{-1}=\rho^{(h')}(r)\). Cela impliquerait en particulier que les matrices \( \rho^{(h)}(r)\) et \( \rho^{(h')}(r)\) aient même valeurs propres. Nous aurions donc \( \{ \omega^h,\omega^{-h} \}=\{ \omega^{h'},\omega^{-h'} \}\). Mais cela est impossible avec \( 0<h<h'<\frac{ n }{2}\). Donc toutes ces représentations sont distinctes.

\end{enumerate}

Le caractère de la représentation \( \rho^{(h)}\) est \( \chi^{(h)}(r^k)=\omega^{hk}+\omega^{-hk}=2\cos\left( \frac{ 2\pi hk }{ n } \right)\).

Nous ajoutons donc la ligne suivante à notre liste :
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &r^k&sr^k\\
        \hline\hline
        \chi^{(h)}&2\cos\left( \frac{ 2\pi hk }{ n } \right)&0\\
        \hline
    \end{array}
\end{equation*}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le compte pour \texorpdfstring{$ n$}{n} pair}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons \( 4\) représentations de dimension \( 1\) puis \( \frac{ n }{2}-1\) représentations de dimension \( 2\). En tout nous avons
\begin{equation}
 \frac{ n }{2}+3
\end{equation}
représentations irréductibles modulo équivalence. Cela fait le compte en vertu des classes de conjugaisons listées en~\ref{SubsubsecROVmHuM}. Pour rappel, le nombre de représentations non équivalentes est égal au nombre de classes de conjugaison par le corollaire~\ref{CorbdcVNC}. Notons que c'est cela qui justifie le fait que nous ne devons pas chercher d'autres représentations. Nous sommes sûrs de les avoir toutes trouvées.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le compte pour \texorpdfstring{$ n$}{n} impair}
%---------------------------------------------------------------------------------------------------------------------------

Nous avions fait mention plus haut du fait que si \( \psi\) est une représentation de dimension \( 1\), le nombre \( \psi(r)\) devait être une racine \( n\)\ieme\ de l'unité. Donc en dimension \( 1\) nous avons seulement les représentations \( \rho^{++}\) et \( \rho^{-+}\). Pour celles de dimension \( 2\), nous en avons \( \frac{ n-1 }{2}\). En tout nous avons donc
\begin{equation}
    \frac{ n+3 }{2}
\end{equation}
représentations irréductibles modulo équivalence. Cela fait le compte en vertu des classes de conjugaisons listées en~\ref{GJIzDEP}.
