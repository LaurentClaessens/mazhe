% This is part of Le Frido
% Copyright (c) 2008-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

\begin{definition}[\cite{ooUQBZooCAKfrE}]      \label{DEFooEEQGooNiPjHz}
	Soient trois espaces vectoriels \( E,F\) et \( V\) sur le même corps commutatif \( \eK\). Une application \( b\colon E\times F\to V\) est \defe{bilinéaire}{application bilinéaire} si elle est séparément linéaire en ses deux variables, c'est-à-dire si
	\begin{enumerate}
		\item
		      \( b(u_1+u_2,v)=b(u_1,v)+b(u_2,v)\),
		\item
		      \( b(u,v_1+v_2)=b(u,v_1)+b(u,v_2)\)
		\item
		      \( b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v)\)
	\end{enumerate}
	pour tout \( u,u_1,u_2\in E\), \( v,v_1,v_2\in F\) et pour tout \( \lambda\in \eK\).

	\begin{enumerate}
		\item

		      Dans le cas \( E=F\) et \( V=\eK\), nous parlons de \defe{forme bilinéaire}{forme!bilinéaire} sur \( E\).
		\item

		      Nous parlons de forme bilinéaire \defe{symétrique}{forme bilinéaire symétrique} si de plus \( b(u,v)=b(v,u)\).
	\end{enumerate}
\end{definition}


\begin{definition}[\cite{BIBooWVWZooZqliJt}]   \label{DefBSIoouvuKR}
	Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme quadratique} sur \( E\) est une application \( q\colon E\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon E\times E\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in E\).

	L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.

\end{definition}

La topologie sur \( Q(E)\) sera la topologie métrique donnée dans le lemme \ref{LEMooWRCIooWkMpoF}.

\begin{definition}[Application bilinéaire définie positive, thème~\ref{THEMEooYEVLooWotqMY}]      \label{DEFooJIAQooZkBtTy}
	Si \( b\) est une application bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} sur un espace vectoriel \( E\) nous disons qu'elle est
	\begin{enumerate}
		\item		\label{ITEMooRVGDooQmNqyg}
		      \defe{définie positive}{application!définie positive} si \( b(x,x)\geq 0\) pour tout \( x\in E\) et \( b(x,x)=0\) si et seulement si \( x=0\).
		\item
		      \defe{semi-définie positive}{application!semi-définie positive} si \( b(x,x)\geq 0\) pour tout \( x\in E\). Nous dirons aussi parfois qu'elle est simplement «positive».
	\end{enumerate}
	Note : nous disons souvent «strictement définie positive» pour «définie positive», pour éviter les confusions.
\end{definition}

\begin{normaltext}
	Une application bilinéaire \( E\times E\to \eK\) n'est pas une application linéaire; la distinction est importante. La linéarité est
	\begin{equation}
		b(\lambda u,\lambda v)= b\big( \lambda(u,v) \big)=\lambda b(u,v)
	\end{equation}
	et la bilinéarité est
	\begin{equation}
		b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v).
	\end{equation}
	En réalité la seule forme qui soit à la fois linéaire et bilinéaire est la forme identiquement nulle : la condition
	\begin{equation}
		b(\lambda u,\lambda v)=\lambda^2b(u,v)=\lambda b(u,v)
	\end{equation}
	pour tout \( \lambda\in \eK\) implique \( b(u,v)=0\).
\end{normaltext}

\begin{example}[\cite{BIBooJMSXooYUADgm}]       \label{EXooKFBOooJjXZwu}
	L'application
	\begin{equation}
		\begin{aligned}
			b\colon \eM(n,\eK)\times \eM(n,\eK) & \to \eK            \\
			(A,B)                               & \mapsto \trace(AB)
		\end{aligned}
	\end{equation}
	est une forme bilinéaire symétrique.

	La vérification est un calcul :
	\begin{equation}
		\trace(BA)=\sum_{i}(BA)_{ii}=\sum_{ik}B_{ik}A_{ki}=\sum_{ik}A_{ki}B_{ik}=\sum_k(AB)_{kk}=\trace(AB).
	\end{equation}
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dégénérescence d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\), une forme bilinéaire symétrique non dégénérée  sur l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\) où \( \eK\) est un corps de caractéristique différente de \( 2\). Nous notons \( q\) la forme quadratique associée.

\begin{definition}      \label{DEFooNUBFooLfCqaK}
	Une forme bilinéaire est \defe{non dégénérée}{forme!bilinéaire!non dégénérée} si \( b(x,z)=0\) pour tout \( z\) implique \( x=0\).
\end{definition}

\begin{lemma}   \label{LemyKJpVP}
	Soit \( b\) une forme bilinéaire non dégénérée. Si \( x\) et \( y\) sont tels que \( b(x,z)=b(y,z)\) pour tout \( z\), alors \( x=y\).
\end{lemma}

\begin{proof}
	C'est immédiat du fait de la linéarité en le premier argument et de la non-dégénérescence : si \( b(x,z)-b(y,z)=0\) alors
	\begin{equation}
		b(x-y,z)=0
	\end{equation}
	pour tout \( z\), ce qui implique \( x-y=0\).
\end{proof}

%-------------------------------------------------------
\subsection{Orthogonal pour une forme bilinéaire}
%----------------------------------------------------

\begin{definition}	\label{DEFooENZGooXMWfUy}
	Soit un espace vectoriel \( E\) et une forme bilinéaire symétrique \( b\). Si \( A\subset E\) nous notons
	\begin{equation}
		A^{\perp}=\{ z\in E\tq b(z,x)=0\,\forall x\in A \}.
	\end{equation}
	C'est l'\defe{orthogonal}{orthogonal pour une forme bilinéaire} de \( A\) par rapport à la forme \( b\).
\end{definition}

\begin{definition}		\label{DEFooQQBQooKJdwxO}
	Soit un espace vectoriel \( E\) et une forme bilinéaire symétrique \( b\). Le \defe{noyau}{noyau d'une forme bilinéaire} est la partie
	\begin{equation}
		\ker(b)=\{ z\in E\tq b(z,x)=0\forall x\in E \}.
	\end{equation}
\end{definition}

\begin{proposition}[\cite{BIBooMFNLooTqdBSf}]		\label{PROPooWPKRooUAnVzd}
	Soit un espace vectoriel \( E\) sur le corps \( \eK\). Soit une forme bilinéaire symétrique \( b\) sur \( E\). Si \( A\) et \( B\) sont des parties de \( E\) nous avons :
	\begin{enumerate}
		\item		\label{ITEMooCGEIooKpSNXS}
		      \( A^{\perp}\) est un sous-espace vectoriel.
		\item	\label{ITEMooRRAOooTaVYgm}
		      Si \( A\subset B\) alors \( B^{\perp}\subset A^{\perp}\).
		\item	\label{ITEMooBKXFooEpObWC}
		      \( A^{\perp}=\Span(A)^{\perp}\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooCGEIooKpSNXS}]
		%-----------------------------------------------------------

		Soient \( x,y\in A^{\perp}\), \( a\in A\) et \( \lambda\in \eK\). Nous avons alors
		\begin{equation}
			b(x+\lambda y,a)=b(x,a)+\lambda b(y,a)=0
		\end{equation}

		\spitem[Pour \ref{ITEMooRRAOooTaVYgm}]
		%-----------------------------------------------------------

		Soient \( x\in B^{\perp}\) et \( a\in A\). Nous avons \( b(x,a)=0\) parce que \( a\in A\subset B\) et \( x\in B^{\perp}\).
		\spitem[Pour \ref{ITEMooBKXFooEpObWC}]
		%-----------------------------------------------------------
		Deux inclusions à prouver.

		\begin{subproof}
			\spitem[Première inclusion]
			%-----------------------------------------------------------
			Nous avons \( A\subset \Span(A)\), donc le point \ref{ITEMooRRAOooTaVYgm} montre que \( \Span(A)^{\perp}\subset A^{\perp}\).
			\spitem[Deuxième inclusion]
			%-----------------------------------------------------------
			Soit \( x\in A^{\perp}\) et \( y\in\Span(A)\). Nous avons \( y=\sum_i\lambda_ia_i\) pour certains \( a_i\in A\) et \( \lambda_i\in \eK\). Alors
			\begin{equation}
				b(x,y)=\sum_i\lambda_ib(x,a_i)=0.
			\end{equation}
		\end{subproof}
	\end{subproof}
\end{proof}



\begin{proposition}[\cite{BIBooMFNLooTqdBSf}]		\label{PROPooSACFooQTsiJL}
	Soit un espace vectoriel \( E\) sur le corps \( \eK\). Soit une forme bilinéaire symétrique \( b\) sur \( E\). Si \( V\) est un sous-espace de \( E\), alors :
	\begin{enumerate}
		\item		\label{ITEMooYHJUooQnbKQc}
		      Si \(b_V \colon V\times V\to \eK  \) est la restriction de \( b\), alors \(  V\cap V^{\perp}=\ker(b_V)\)\footnote{Définition du noyau de \( b\), définition \ref{DEFooQQBQooKJdwxO}.}.
		\item		\label{ITEMooUZZFooDbuddF}
		      \( (V+W)^{\perp}=V^{\perp}\cap W^{\perp}\).
		\item	\label{ITEMooSZAGooLgkzYw}
		      \( V\subset (V^{\perp})^{\perp}\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooYHJUooQnbKQc}]
		%-----------------------------------------------------------

		En deux parties.
		\begin{subproof}
			\spitem[\( V\cap V^{\perp}\subset \ker(b_V)\)]
			%-----------------------------------------------------------
			Soit \( x\in V\cap V^{\perp}\). Si \( v\in V\), nous avons \( b(x,v)=0\) parce que \( x\in V^{\perp}\).
			\spitem[\( \ker(b_V) \subset V\cap V^{\perp}\)]
			%-----------------------------------------------------------
			Soit \( x\in\ker(b_V)\). Nous avons \( x\in V\) parce que \( x\in \ker(b_V)\subset V\). Pour montrer que \( x\in V^{\perp}\) nous considérons \( y\in V\) et nous remarquons que \( b(x,y)=0\) parce que \( x\in V^{\perp}\).

		\end{subproof}
		\spitem[Pour \ref{ITEMooUZZFooDbuddF}]
		%-----------------------------------------------------------
		En deux parties.
		\begin{subproof}
			\spitem[\( (V+W)^{\perp}\subset V^{\perp}\cap W^{\perp}\)]
			%-----------------------------------------------------------
			Soit \( x\in (V+W)^{\perp}\). Si \( v\in V\), alors \( v\in V+W\), donc \( f(x,v)=0\). Cela prouve que \( x\in V^{\perp}\). Nous prouvons que \( x\in W^{\perp}\) de même.
			\spitem[\(  V^{\perp}\cap W^{\perp} \subset  (V+W)^{\perp}\)]
			%-----------------------------------------------------------
			Un élément de \( V+W\) est de la forme \( v+w\) avec \( v\in V\) et \( w\in W\). Nous avons
			\begin{equation}
				b(x,v+w)=b(x,v)+b(x,w)=0.
			\end{equation}
		\end{subproof}
		\spitem[Pour \ref{ITEMooSZAGooLgkzYw}]
		%-----------------------------------------------------------
		Si \( x\in V\), alors \( b(x,z)=0\) pour tout \( V^{\perp}\).
	\end{subproof}
\end{proof}

\begin{proposition}		\label{PROPooMAZZooRZZoHj}
	Soit une forme bilinéaire non dégénérée \( b\) sur l'espace vectoriel \( E\) de dimension finie. L'application
	\begin{equation}
		\begin{aligned}
			\varphi_b\colon E & \to E^*        \\
			x                 & \mapsto b(x,.)
		\end{aligned}
	\end{equation}
	est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Linéaire]
		%-----------------------------------------------------------

		Le fait que \( \varphi_b\) soit linéaire est une conséquence de la bilinéarité de \( b\).
		\spitem[Injectif]
		%-----------------------------------------------------------
		Si \( \varphi_b(v)=\varphi_b(w)\), alors pour tout \( x\in E\) nous avons \( \varphi_b(v)x=\varphi_b(w)x\), c'est-à-dire
		\begin{equation}
			b(v-w,x)=0.
		\end{equation}
		Vu que \( b\) est non dégénérée, cela implique \( v-w=0\).
		\spitem[Isomorphisme]
		%-----------------------------------------------------------
		Nous savons que \( \dim(E)=\dim(E^*)\) par le lemme \ref{DEFooTMSEooZFtsqa}. Une application linéaire injective entre espaces de même dimension est toujours une bijection par le corolaire \ref{CORooCCXHooALmxKk}.
	\end{subproof}

\end{proof}

\begin{lemma}[\cite{BIBooRQIOooMoPqXU}]		\label{LEMooRXMMooAvvOjF}
	Soit une forme bilinéaire non dégénérée \( b\) sur l'espace vectoriel \( E\) de dimension finie. Soit un sous-espace vectoriel \( V\). Nous avons
	\begin{equation}		\label{EQooLFJBooNNIHgP}
		\dim(E)=\dim(V)+\dim(V^{\perp}).
	\end{equation}
\end{lemma}

\begin{proof}
	Nous considérons les applications
	\begin{equation}
		\begin{aligned}
			\varphi_b\colon E & \to E^*         \\
			x                 & \mapsto b(x,.).
		\end{aligned}
	\end{equation}
	et l'application de restriction
	\begin{equation}
		\begin{aligned}
			r\colon E^* & \to V^*           \\
			\alpha      & \mapsto \alpha|_F
		\end{aligned}
	\end{equation}

	\begin{subproof}
		\spitem[Théorème du rang]
		%-----------------------------------------------------------
		Nous appliquons le théorème du rang \ref{EQooUEOQooLySRiE} à l'application \( r\) :
		\begin{equation}		\label{EQooSABBooUdWyZL}
			\rank(r)+\dim\big(\ker(r))=\dim(E).
		\end{equation}

		\spitem[\( \rank(r)=\dim(V)\)]
		%-----------------------------------------------------------

		Étant donné que \( r\) est surjective, \(\rank(r)=\dim(V^*)=\dim(V)\). La seconde égalité est l'égalité des dimensions du lemme \ref{DEFooTMSEooZFtsqa}.

		\spitem[\( \varphi_b(V^{\perp})\subset \ker(r)\)]
		%-----------------------------------------------------------
		Soit \( x\in V^{\perp}\). Pour tout \( v\in V\) nous avons
		\begin{equation}
			r\big(\varphi_b(x))v=\varphi_b(x)v=b(x,v)=0.
		\end{equation}
		Donc \( \varphi_b(x)\in\ker(r)\).
		\spitem[\(\varphi_b \colon V^{\perp}\to \ker(r)  \) est surjective]
		%-----------------------------------------------------------
		Soit \( \alpha\in \ker(r)\). Vu que \( \varphi_b\) est une bijection, il existe \( x\in E\) tel que \( \alpha=\varphi_b(x)\), c'est-à-dire \( \alpha=b(x,\cdot)\). Nous avons \( \alpha(y)=0\) pour tout \( y\in V\), c'est-à-dire \( b(x,y)=0\) pour tout \( y\in V\), ce qui signifie que \( x\in V^{\perp}\). Donc \( \alpha\in \varphi_b(V^{\perp})\).

		\spitem[\( \dim(V^{\perp})=\dim\big(\ker(r)\big)\)]
		%-----------------------------------------------------------
		L'application \( \varphi_b\) étant globalement injective, elle est une bijection entre \( V^{\perp}\) et \( \ker(r)\). Donc ces deux ont la même dimension.

		\spitem[Conclusion]
		%-----------------------------------------------------------
		Nous pouvons reprendre l'équation \eqref{EQooSABBooUdWyZL} et y mettre nos découvertes : \( \rank(r)=\dim(V)\) et \( \dim\big( \ker(r) \big)=\dim(V^{\perp})\). Cela donne la formule demandée
		\begin{equation}
			\dim(V)+\dim(V^{\perp})=\dim(E).
		\end{equation}
	\end{subproof}
\end{proof}

\begin{lemma}		\label{LEMooYYGLooYIDmoa}
	Si \( V\) est un sous-espace de \( E\) de dimension finie, alors
	\begin{equation}
		(V^{\perp})^{\perp}=V.
	\end{equation}
\end{lemma}

\begin{proof}
	Nous avons déjà vu dans la proposition \ref{PROPooSACFooQTsiJL} que \( V\subset (V^{\perp})^{\perp}\).

	Nous écrivons maintenant la formule \eqref{EQooLFJBooNNIHgP} pour \( V\), et pour \( V^{\perp}\) (qui est un sous-espace vectoriel par la proposition \ref{PROPooWPKRooUAnVzd}\ref{ITEMooCGEIooKpSNXS}) :
	\begin{subequations}
		\begin{align}
			\dim(V)+\dim(V^{\perp})=\dim(E) \\
			\dim(V^{\perp})+\dim\Big((V^{\perp})^{\perp}\Big)=\dim(E).
		\end{align}
	\end{subequations}
	En soustrayant ces deux équations membre à membres, nous trouvons \( \dim(V)=\dim\Big((V^{\perp})^{\perp}\Big)\). Vu que \( V\) s'injecte (par l'identité) dans \( (V^{\perp})^{\perp}\) et qu'ils ont la même dimension, ils sont égaux.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Formes quadratiques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooOQIDooPSOeXL}
	Soit une forme quadratique \( Q\) sur \( E\). Si \( F\) est un sous-espace de \( E\), alors
	\begin{equation}
		\dim(F)+\dim(F^{\perp})\geq n
	\end{equation}
	où \( F^{\perp}\) est l'orthogonal par rapport à \( Q\).
\end{lemma}

\begin{proof}
	Nous posons \( p=\dim(F)\). Nous considérons une base \( \{ f_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \{ f_i \}_{i=1,\ldots, p}\) est une base de \( F\)\footnote{Théorème de la base incomplète, \ref{THOooOQLQooHqEeDK}.}. Nous posons
	\begin{equation}
		\begin{aligned}
			\phi\colon E & \to F                           \\
			x            & \mapsto \sum_{i=1}^pB(x,f_i)f_i
		\end{aligned}
	\end{equation}
	où \( B\) est la forme bilinéaire associée à \( Q\). Ce \( \phi\) est une application linéaire à qui nous appliquons le théorème du rang \eqref{EQooUEOQooLySRiE} :
	\begin{equation}        \label{EQooCLWLooCFxVDq}
		\dim(E)=\rank(\phi)+\dim\big( \ker(\phi) \big).
	\end{equation}
	Mais vu que l'image de \( \phi\) est dans \( F\), nous avons \( \rank(\phi)\leq \dim(F)\). De plus, \( \ker(\phi)=F^{\perp}\). Donc \eqref{EQooCLWLooCFxVDq} devient
	\begin{equation}
		\dim(E)\leq \dim(F)+\dim(F^{\perp}).
	\end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooUOZOooYvEcji}
	Soit un espace vectoriel \( E\) de dimension finie et un sous-espace \( F\) sur lequel la forme quadratique \( Q\) est strictement définie positive ou négative. Alors
	\begin{equation}
		E=F\oplus F^{\perp}.
	\end{equation}
\end{lemma}

\begin{proof}
	D'abord nous montrons que \( F\cap F^{\perp}=\{ 0 \}\). Si \( v\neq 0\) est dans \( F\), alors \( Q(v)>0\), et donc \( v\) n'est pas dans \( F^{\perp}\). Donc \( F\cap F^{\perp}\subset \{ 0 \}\). L'inclusion inverse est immédiate.

	Nous avons vu dans le lemme \ref{LEMooOQIDooPSOeXL} que
	\begin{equation}
		\dim(E)\leq \dim(F)+\dim(F^{\perp}).
	\end{equation}
	Vu que \( F\) et \( F^{\perp}\) n'ont pas d'intersection autre que \( \{ 0 \}\), nous avons
	\begin{equation}
		\dim(E)\geq\dim(F\oplus F^{\perp}) = \dim(F)+\dim(F^{\perp}) \geq\dim(E).
	\end{equation}
	Toutes ces inégalités sont donc des égalités et \( \dim(E)=\dim(F)+\dim(F^{\perp})\).
\end{proof}

\begin{lemma}       \label{LEMooWRCIooWkMpoF}
	La topologie considérée sur \( Q(E)\)\footnote{\( Q(E)\) sont les formes quadratiques réelles sur \( E\), définition \ref{DefBSIoouvuKR}.} est celle de la norme
	L'application
	\begin{equation}  \label{EqZYBooZysmVh}
		\begin{aligned}
			N\colon Q(E) & \to \eR                            \\
			q            & \mapsto\sup_{\| x \|_E=1}| q(x) |,
		\end{aligned}
	\end{equation}
	est une norme.

	L'application
	\begin{equation}  \label{EQooJETQooIjxRWu}
		\begin{aligned}
			N\colon S(n,\eR) & \to \eR                                  \\
			A                & \mapsto  \sup_{\| x \|_E=1}| x\cdot Ax |
		\end{aligned}
	\end{equation}
	est une norme.  À droite, nous trouvons le produit scalaire usuel sur \( \eR^n\) et la valeur absolue usuelle sur \( \eR\).

	Ces normes sont celles que nous considérons pour la topologie sur \( Q(E)\) et \( S(n,\eR)\).
\end{lemma}

\begin{proposition} \label{PROPooZLXVooOsXCcB}
	Soit une forme bilinéaire \( b\) et la forme quadratique associée \( q\). Alors nous avons l'\defe{identité de polarisation}{identité de polarisation} :
	\begin{equation}    \label{EqMrbsop}
		b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
	\end{equation}
\end{proposition}

\begin{proof}
	Il suffit de substituer dans le membre de droite \( q(x)=b(x,x)\) et d'utiliser la bilinéarité :
	\begin{subequations}
		\begin{align}
			q(x)+q(y)-q(x-y) & =b(x,x)+b(y,y)-b(x-y,x-y)                  \\
			                 & =b(x,x)+b(y,y)-b(x,x)+b(x,y)+b(y,x)-b(y,y) \\
			                 & =2b(x,y)
		\end{align}
	\end{subequations}
	où nous avons utilisé le fait que \( b\) est symétrique : \( b(x,y)=b(y,x)\).
\end{proof}

\begin{proposition}[Matrice associée à une forme bilinéaire] \label{PropFSXooRUMzdb}
	Soit \( \{ e_i \}\) une base de \( E\). L'application
	\begin{equation}
		\begin{aligned}
			\phi\colon Q(E) & \to S(n,\eR)                             \\
			q               & \mapsto \big(   b(e_i,e_j)   \big)_{i,j}
		\end{aligned}
	\end{equation}
	où \( b\) est forme bilinéaire associée à \( q\) est une bijection linéaire et continue\footnote{Pour les topologies des normes données dans le lemme \ref{LEMooWRCIooWkMpoF}.}.
\end{proposition}

\begin{proof}
	Si \( \phi(q)=\phi(q')\); alors
	\begin{equation}
		q(x)=\sum_{i,j}\phi(q)_{ij}x_ix_j=\sum_{i,j}\phi(q')_{ij}x_ix_j=q'(x).
	\end{equation}
	Donc \( q=q'\). L'application \( \phi\) est donc injective

	De plus elle est surjective parce que si \( B\in S(n,\eR)\) alors la forme quadratique
	\begin{equation}
		q(x)=\sum_{i,j}B_{ij}x_ix_j
	\end{equation}
	a évidemment \( B\) comme matrice associée. L'application \( \phi\) est donc surjective.

	Notre application \( \phi\) est de plus linéaire parce que l'association d'une forme quadratique à la forme bilinéaire associée est linéaire.

	En ce qui concerne la continuité, nous la prouvons en zéro en considérant une suite convergente \( q_n\stackrel{Q(E)}{\longrightarrow}0\). C'est-à-dire que
	\begin{equation}
		\sup_{\| x \|=1}| q_n(x) |\to 0.
	\end{equation}
	Nous rappelons l'identité de polarisation\footnote{Proposition \ref{PROPooZLXVooOsXCcB}.} :
	\begin{equation}
		b_n(x,y)=\frac{ 1 }{2}\big( q_n(x-y)-q_n(x)-q_n(y) \big).
	\end{equation}
	En ce qui concerne deux des trois termes, il n'y a pas de problèmes :
	\begin{equation}
		\big| \phi(q_n)_{ij} \big|=\big| b_n(e_i,e_j) \big|\leq\frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\frac{ 1 }{2}\big| q_n(e_i) \big|+\frac{ 1 }{2}\big| q_n(e_j) \big|.
	\end{equation}
	Si \( n\) est assez grand, nous avons tout de suite
	\begin{equation}
		\big| \phi(q_n)_{ij} \big|\leq \frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\epsilon.
	\end{equation}
	Nous définissons \( e_{ij}\) et \( \alpha_{ij}\) de telle sorte que \( e_i-e_j=\alpha_{ij}e_{ij}\) avec \( \| e_{ij} \|=1\). Si \( \alpha=\max\{ \alpha_{ij},1 \}\) alors nous avons
	\begin{equation}
		q_n(e_i-e_j)=\alpha_{ij}^2q_n(e_{ij})\leq \alpha^2q_n(e_{ij}).
	\end{equation}
	Il suffit maintenant de prendre \( n\) assez grand pour avoir \( \sup_{\| x \|=1}| q_n(x) |\leq \frac{ \epsilon }{ \alpha^2 }\) pour avoir
	\begin{equation}
		\big| \phi(q_n)_{ij} \big|\leq \frac{ \epsilon }{2}+\frac{ \epsilon }{ \alpha^2 }.
	\end{equation}
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isotropie}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Isotropie\cite{BIBooJAAFooZbUJFg}]   \label{DefVKMnUEM}
	Quelques définitions.
	\begin{enumerate}
		\item
		      Un vecteur est \defe{isotrope}{isotrope!vecteur} pour \( b\) si il est perpendiculaire à lui-même. En d'autres termes, \( x\) est isotrope si et seulement si \( b(x,x)=0\).
		\item
		      Un sous-espace \( W\subset E\) est \defe{isotrope}{isotrope!sous-espace} si \( W\cap W^{\perp}\neq \{ 0 \}\).
		\item

		      Un sous-espace \( W\subset E\) est \defe{totalement isotrope}{isotrope!totalement} si pour tout \( x,y\in W\), nous avons \( b(x,y)=0\).
	\end{enumerate}

	Le \defe{cône isotrope}{isotrope!cône} de \( b\) est l'ensemble de ses vecteurs isotropes :
	\begin{equation}
		C(b)=\{ x\in E\tq b(x,x)=0 \}.
	\end{equation}
\end{definition}
Nous introduisons quelques notations. D'abord pour \( y\in E\) nous notons
\begin{equation}
	\begin{aligned}
		\Phi_y\colon E & \to \eR        \\
		x              & \mapsto b(x,y)
	\end{aligned}
\end{equation}
et ensuite
\begin{equation}
	\begin{aligned}
		\Phi\colon E & \to E^*         \\
		y            & \mapsto \Phi_y.
	\end{aligned}
\end{equation}

\begin{definition}		\label{DEFooHKQVooPuWZiP}
	Le fait pour une forme bilinéaire \( b\) d'être dégénérée signifie que l'application \( \Phi\) n'est pas injective. Le \defe{noyau}{noyau!d'une forme bilinéaire} de la forme bilinéaire est celui de \( \Phi\), c'est-à-dire
	\begin{equation}
		\ker(b)=\{ z\in E\tq b(z,y)=0\,\forall y\in E \}.
	\end{equation}
	Autrement dit, \( \ker(b)=E^{\perp}\) où le perpendiculaire est pris par rapport à \( b\).
\end{definition}
Notons tout de même que nous utilisons la notation \( \perp\) même si \( b\) est dégénérée et éventuellement pas positive; c'est-à-dire même si la formule \( (x,y)\mapsto b(x,y)\) ne fournit pas un produit scalaire.


\begin{lemma}	\label{LEMooOTVGooRAOyaD}
	Si \( E\) est de dimension finie, et si \( V\) est un sous-espace non isotrope\footnote{Définition \ref{DefVKMnUEM}.} de \( E\), alors \( V^{\perp}\) est également non-isotrope.
\end{lemma}

\begin{proof}
	En dimension finie, le lemme \ref{LEMooYYGLooYIDmoa} nous indique que \( (V^{\perp})^{\perp}=V\). Si \( V^{\perp}\) était isotrope nous aurions \( V^{\perp}\cap (V^{\perp})^{\perp}\neq \emptyset\). Cela donne alors immédiatement \( V^{\perp}\cap V\neq \emptyset\), qui serait que \( V\) est isotrope.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.


\begin{lemma}       \label{LEMooLKNTooSfLSHt}
	Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
	L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'identité de polarisation de la proposition \ref{PROPooZLXVooOsXCcB}. Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

\begin{definition}      \label{DEFooGECOooCCGVXG}
	Soit une forme quadratique \( q\) sur \( E\). Nous disons que \( v,w\in E\) sont \defe{\( q\)-orthogonaux}{\( q\)-orthogonal} si
	\( b(v,w)=0\) où \( b\) est la forme bilinéaire associée à \( q\) par le lemme \ref{LEMooLKNTooSfLSHt}.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice associée à une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooAOGPooXWXUcN}
	Soit une forme bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.} \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous définissons les nombres
	\begin{equation}    \label{EQooCUGFooRlKUtu}
		B_{\alpha\beta}=b(f_{\alpha},f_{\beta}),
	\end{equation}
	qui forment une matrice symétrique dans \( \eM(n,\eK)\). Cette matrice est la \defe{matrice associée}{matrice d'une forme bilinéaire} à la forme bilinéaire \( b\).

	La matrice d'une forme quadratique est celle associée à sa forme bilinéaire associée.
\end{definition}

\begin{lemma}       \label{LEMooDCIOooTlVZMR}
	Soit une forme bilinéaire \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous notons \( B\) la matrice de \( b\) (definition \ref{DEFooAOGPooXWXUcN}) et \( q\) la forme quadratique associée.

	Alors nous avons
	\begin{equation}        \label{EQooQFMWooVKVLMx}
		b(x,y)=\sum_{\alpha\beta}B_{\alpha\beta}x_{\alpha}y_{\beta}.
	\end{equation}
	et
	\begin{equation}
		b(x,y)=x\cdot By.
	\end{equation}
	où le point est le produit scalaire usuel (composante par composante).
\end{lemma}

\begin{proof}
	Si \( x=\sum_{\alpha}x_{\alpha}f_{\alpha}\) et \( y=\sum_{\beta}y_{\beta}f_{\beta}\) :

	En utilisant la convention \eqref{EQooAXRJooUwHbjB} et les choses autour (voir aussi \ref{SECooBTTTooZZABWA}),
	\begin{equation}
		b(x,y)=\sum_{\alpha}x_{\alpha}\sum_{\beta}B_{\alpha\beta}y_{\beta}=\sum_{\alpha}x_{\alpha}(By)_{\alpha}=x\cdot By.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooOKTGooOYukoB}
	Soient une forme quadratique \( q\) sur \( \eR^n\) et une application linéaire \(\phi \colon \eR^n\to \eR^n  \). La matrice de \( q\circ \phi\) est
	\begin{equation}
		\phi^tq\phi
	\end{equation}
	où l'on a noté \( q\) la matrice de \( q\) et \( \phi\) celle de \( \phi\).
\end{proposition}

\begin{proof}
	C'est un calcul direct de \( (q\circ\phi)(x)\) :
	\begin{subequations}
		\begin{align}
			q\big( \phi(x) \big) & =\sum_{ij}q_{ij}\phi(x)_i\phi(x)_j                                                                       \\
			                     & = \sum_{ij}\sum_k\sum_lq_{ij\phi_{ik}}x_k\phi_{jl}x_l            & \text{prop. \ref{PROPooGXDBooHfKRrv}} \\
			                     & = \sum_{kl}\Big( \sum_{ij}\phi_{ik}q_{ij}\phi_{jl} \Big)x_kx_l\
			                     & = \sum_{kl} (\phi^tq\phi)_{kl}x_lx_l.
		\end{align}
	\end{subequations}
	Et voilà.
\end{proof}


\begin{proposition}[Voir la section \ref{SECooBTTTooZZABWA}]     \label{PROPooLBIOooUpzxXA}
	Soit une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}} \( b\colon V\times V\to \eK\) dont la matrice\footnote{Définition~\ref{EQooCUGFooRlKUtu}.} dans la base \( \{ e_i \}\) est \( A\) et celle dans la base \( \{ f_{\alpha} \}\) est \( B\). Nous supposons que les bases sont liées par \( f_{\alpha}=\sum_{i}Q_{i\alpha}e_i\). Alors
	\begin{equation}        \label{EQooZUVTooKjqnJj}
		B=Q^tAQ.
	\end{equation}
\end{proposition}

\begin{proof}
	Soit \( x,x'\in V\) de coordonnées \( (x_i)\) et \( (x'_i)\) dans la base \( \{ e_i \}\) et \( (y_{\alpha})\), \( (y'_{\alpha})\) dans la base \( \{ f_{\alpha} \}\). Par définition de la matrice associée à une forme bilinéaire,
	\begin{equation}
		b(x,x')=\sum_{ij}A_{ij}x_ix'_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
	\end{equation}
	En remplaçant les \( x_i\) et \( x'_i\) par leurs valeurs en fonction de \( y_{\alpha}\) et \( y'_{\beta}\) données par la proposition \ref{PROPooNYYOooHqHryX}, nous trouvons
	\begin{subequations}
		\begin{align}
			b(x,x') & =\sum_{ij\alpha\beta}A_{ij}Q_{i\alpha}y_{\alpha}Q_{j\beta}y'_{\beta} \\
			        & =\sum_{\alpha\beta}(Q^tAQ)_{\alpha\beta}y_{\alpha}y'_{\beta}
		\end{align}
	\end{subequations}
	où \( Q^t\) désigne la transposée de la matrice \( Q\) :  \( Q^t_{ij}=Q_{ji}\). Vu que les nombres \( y_{\alpha}\) et \( y'_{\beta}\) sont arbitraires nous déduisons\footnote{Lemme~\ref{LEMooLXAHooPRyHaF}.} que \( B=Q^tAQ\).
\end{proof}

\begin{remark}      \label{REMooNEJLooSqgeih}
	Notons que cette «loi de transformation» n'est pas la même que celle pour une application linéaire\footnote{Proposition \ref{PROPooNZBEooWyCXTw}.}. Ici nous avons \( Q^t\) alors que pour les applications linéaires nous avions \( Q^{-1}\).

	Pour cette raison, tant que nous travaillons avec des bases orthonormées, c'est-à-dire tant que \( Q\) est orthogonale\footnote{Définition~\ref{DefMatriceOrthogonale}.}, nous pouvons confondre une application linéaire avec une application bilinéaire en passant par la matrice. Mais cette identification n'est pas du tout canonique : elle repose sur le fait que les bases soient orthonormées.

	Il en découle que la réduction des endomorphismes et la réduction des formes bilinéaires ne sont pas tout à fait les mêmes théories. Par exemple la pseudo-diagonalisation simultanée (corolaire~\ref{CorNHKnLVA}) est un résultat de réduction de forme bilinéaire et non d'endomorphismes.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométrie, forme quadratique et bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
	La forme quadratique \( q(x)=x_1^2+x_2^2\) donne la norme euclidienne. La forme bilinéaire associée est \( b(x,y)=x_1y_1+x_2y_2\), qui est le produit scalaire usuel.
\end{example}

Il ne faudrait pas déduire trop vite que la formule \( \| x \|^2=q(x)\) donne une norme dès que \( q\) est non dégénérée. En effet \( q\) peut ne pas être définie positive. La forme \( q(x)=x_1^2-x_2^2\) prend des valeurs positives et négatives. A fortiori \( d(x,y)=q(x-y)\) ne donne pas toujours une distance.

\begin{definition}      \label{DEFooECTUooRxBhHf}
	Une \defe{isométrie}{isométrie!de forme quadratique} pour la forme quadratique \( q\) est une application bijective \( f\colon V\to V\) telle que
	\begin{equation}
		q(x-y)=q\big( f(x)-f(y) \big).
	\end{equation}
	Dans les cas où \( q\) donne une distance, alors c'est une isométrie au sens usuel.
\end{definition}

\begin{definition}[Thème \ref{THMooVUCLooCrdbxm}]      \label{DEFooIQURooMeQuqX}
	Soit un espace vectoriel \( E\) muni d'une forme bilinéaire \( b\). Une \defe{isométrie}{isométrie (forme bilinéaire)} pour \( b\) est une bijection \( f\colon E\to E\) telle que
	\begin{equation}
		b\big( f(x),f(y) \big)=b(x,y)
	\end{equation}
	pour tout \( x,y\in E\).
\end{definition}

\begin{lemma}   \label{LemewGJmM}
	Soient \( q\) une forme quadratique et \( b\) la forme bilinéaire associée par le lemme~\ref{LEMooLKNTooSfLSHt}. Une application \( f\colon E\to E\) telle que \( f(0)=0\) est une isométrie pour \( b\) si et seulement si elle est une isométrie pour \( q\).
\end{lemma}

\begin{proof}
	Pour une application bijective \( f\colon E\to E\) telle que \( f(0)=0\), nous devons prouver l'équivalence des propriétés suivantes :
	\begin{enumerate}
		\item
		      \( b\big( f(x),f(y) \big)=b(x,y)\) pour tout \( x,y\in E\);
		\item
		      \( q\big( f(x)-f(y) \big)=q(x-y)\) pour tout \( x,y\in E\).
	\end{enumerate}

	Dans le sens direct, en posant \( x=y\) nous trouvons tout de suite \( q(f(x))=q(x)\); ensuite en utilisant la distributivité de \( b\),
	\begin{subequations}
		\begin{align}
			q\big( f(x)-f(y) \big) & =b\big( f(x)-f(y),f(x)-f(y) \big)                            \\
			                       & =q\big( f(x) \big)-2b\big( f(x),f(y) \big)+q\big( f(y) \big) \\
			                       & =q(x)+q(y)-2b(x,y)                                           \\
			                       & =q(x-y).
		\end{align}
	\end{subequations}

	Dans l'autre sens, nous commençons par remarquer que l'hypothèse \( f(0)=0\) donne \( q(x)=q\big( f(x) \big)\). Ensuite nous utilisons l'identité de polarisation \eqref{EqMrbsop} :
	\begin{subequations}
		\begin{align}
			b\big( f(x),f(y) \big) & =\frac{ 1 }{2}\big[ q\big( f(x) \big)+q\big( f(y) \big)-q\big( f(x-y) \big) \big] \\
			                       & =\frac{ 1 }{2}\big[ q(x)+q(y)-q(x-y) \big]                                        \\
			                       & =b(x,y).
		\end{align}
	\end{subequations}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométries}
%---------------------------------------------------------------------------------------------------------------------------

Voici un théorème pas toujours bien énoncé dans les cours de physique qui font de la relativité. Au moment de «prouver» les transformations de Lorentz\footnote{Théorème \ref{THOooYHDWooWxVovH}.}, beaucoup oublient de justifier pourquoi elles devraient être linéaires.
\begin{theorem}[\cite{ooQFKAooFnllQU}]     \label{ThoDsFErq}
	Une isométrie\footnote{Définition \ref{DEFooIQURooMeQuqX}.} d'une forme bilinéaire non dégénérée est linéaire.
\end{theorem}

\begin{proof}
	Soient une forme bilinéaire non-dégénérée \( b\) sur l'espace vectoriel \( E\) ainsi qu'une isométrie \( f\) pour icelle. Soit \( z\in E\); étant donné que \( f\) est bijective nous pouvons considérer l'élément \( f^{-1}(z)\in E\) et calculer
	\begin{subequations}
		\begin{align}
			b\big( f(x+y),z \big) & =b\big( f(x+y),f(f^{-1}(z)) \big) \\
			                      & =b(x+y,f^{-1}(z))                 \\
			                      & =b(x,f^{-1}(z))+b(y,f^{-1}(z))    \\
			                      & =b(f(x),z)+b(f(y),z)              \\
			                      & =b\big( f(x)+f(y),z \big),
		\end{align}
	\end{subequations}
	donc \( f(x+y)=f(x)+f(y)\) par le lemme~\ref{LemyKJpVP}.

	De la même façon on trouve \( b\big( f(\lambda x),z \big)=b\big( \lambda f(x),z \big)\) qui prouve que \( f(\lambda x)=\lambda f(x)\) et donc que \( f\) est linéaire.
\end{proof}

\begin{example}
	Une isométrie peut ne pas être linéaire quand la forme bilinéaire est dégénérée. Par exemple pour la forme bilinéaire sur \( \eR^2\) donnée par
	\begin{equation}
		b\big( (a,b),(x,y) \big)=ax,
	\end{equation}
	nous pouvons faire
	\begin{equation}
		f(x,y)=\begin{pmatrix}
			x \\
			\lambda(x,y)
		\end{pmatrix}
	\end{equation}
	où \( \lambda\) est n'importe quoi.
\end{example}

\ifbool{isGiulietta}{
	\begin{remark}
		Des preuves alternatives.
		\begin{enumerate}
			\item
			      En utilisant un peut plus d'indices et un peu plus de mots comme «tenseurs», peut être trouvée dans \cite{BIBooMBAGooNCUaMT}. Le fait que la preuve donnée soit tensorielle me fait penser que le résultat peut encore être généralisé.
			\item
			      Et encore une autre preuve, utilisant des techniques de groupes de Lie sera la proposition~\ref{PROPooDVIWooAFDNPy}.
		\end{enumerate}
	\end{remark}
}
{}

\begin{definition}      \label{DEFooVTXWooVXfUnc}
	Soient deux espaces vectoriels \( E\) et \( V\). Une application \( f\colon E\to V\) est \defe{affine}{application affine} si il existe une application linéaire \( u\colon E \to V\) et un élément \( \alpha\in V\) tel que
	\begin{equation}
		f(x)=u(x)+\alpha
	\end{equation}
	pour tout \( x\in E\).
\end{definition}

\begin{theorem}		\label{THOooHSGHooNCLebs}
	Soit un espace vectoriel \( E\) muni d'une forme quadratique \( q\). Soit une isométrie \( f\colon E\to E\) pour \( q\). Alors
	\begin{enumerate}
		\item
		      si \( f(0)=0\), alors \( f\) est linéaire;
		\item
		      si \( f(0)\neq 0\) alors \( f\) est affine\footnote{Définition \ref{DEFooVTXWooVXfUnc}.}.
	\end{enumerate}
\end{theorem}

\begin{proof}
	Nous considérons la forme bilinéaire associée \( b\). Si \( f(0)=0\), nous savons par le lemme~\ref{LemewGJmM} que \( b\big( f(x),f(y) \big)=b(x,y)\). La proposition \ref{ThoDsFErq} nous dit alors que \( f\) est linéaire.


	Si \( f(0)\neq 0\), alors nous posons \( g(x)=f(x)-f(0)\) qui vérifie \( g(0)=0\) et
	\begin{equation}
		q\big( g(x)-g(y) \big)=q\big( f(x)-f(0)-f(y)+f(0) \big)=q(x-y).
	\end{equation}
	Nous pouvons donc appliquer le premier point à \( g\), déduire que \( g\) est linéaire et donc que \( f\) est affine.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooHSWIooKwiibH}
	Soit un espace vectoriel \( E\). Soient des points \( a,x_1,\ldots,x_n\) tels que \( \{ x_i-a \}_{i=1,\ldots,n}\) forme une base de \( E\). Si \(f \colon E\to E  \) est une application affine fixant \( a\) et tous les \( x_i\), alors \( f\) est l'identité.
\end{proposition}

\begin{proof}
	Nous décomposons \( f(x)=u(x)+\alpha\) où \( u\) est linéaire et \( \alpha\in E\). Nous avons, pour tout \( i\) :
	\begin{subequations}
		\begin{numcases}{}
			u(a)-a=\alpha\\
			u(x_i)-x_i=\alpha.
		\end{numcases}
	\end{subequations}
	En faisant la soustraction, nous obtenons \( u(x_i-a)=x_i-a\). Donc \( u\) est une application linéaire fixant une base. Elle est donc l'identité. À partir de là, il est immédiat que \( \alpha=0\).
\end{proof}

Nous pouvons maintenant particulariser tout cela au cas de \( \eR^n\) muni du produit scalaire usuel et de la norme associée pour voir quel résultat nous avons à peine prouvé.

Nous notons ici \( T(n)\) le groupe des translations sur \( \eR^n\). Un élément de \( T(n)\) est une translation \( \tau_v\) donnée par un vecteur \( v\) et agissant sur \( \eR^n\) par
\begin{equation}
	\begin{aligned}
		\tau_v\colon \eR^n & \to \eR^{n}  \\
		x                  & \mapsto x+v.
	\end{aligned}
\end{equation}
Ce groupe est isomorphe au groupe abélien \( (\eR^n,+)\), et nous allons souvent identifier \( \tau_v\) à \( v\).

Vous savez par culture générale que les isométries de \( \eR^n\) pour le produit scalaire usuel sont les matrices orthogonales. En voici une petite généralisation (pensez à \( \eta=\mtu\) dans le cas du produit scalaire usuel).
\begin{proposition}     \label{PROPooSYQMooEnZFdp}
	Soit une forme bilinéaire \( b\) sur \( \eR^n\) de matrice symétrique \( \eta\). Si \( A\) est la matrice d'une application linéaire \( \eR^n\to \eR^n\) telle que
	\begin{equation}
		b(Ax,Ay)=b(x,y)
	\end{equation}
	pour tout \( x,y\in\eR^n\), alors
	\begin{equation}
		A^t\eta A=\eta.
	\end{equation}
\end{proposition}

\begin{proof}
	En suivant la formule générale \eqref{EQooQFMWooVKVLMx},
	\begin{equation}
		b(Ax,Ay)=\sum_{ij} \eta_{ij} (Ax)_i(Ay)_j=\sum_{ijkl}\eta_{ij}A_{ik}A_{jl}x_ky_l.
	\end{equation}
	En imposant que ce soit égal à \( \sum_{kl}\eta_{kl}x_ky_l\) pour tout \( x,y\) nous avons la contrainte
	\begin{equation}
		\sum_{ij}\eta_{ij}A_{ik}A_{jl}=\eta_{kl}
	\end{equation}
	qui signifie exactement \( A^t\eta A=\eta\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Signature, théorème de Sylvester}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Signature\cite{BIBooXOWGooAPWTfT}]       \label{DEFooWDCLooDkRYLK}
	Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( Q\) sur un espace vectoriel \( E\) de dimension finie \( n\). L'\defe{indice d'inertie}{indice d'inertie} de \( Q\) est le nombre
	\begin{equation}
		q=\max\{ \dim(F)\tq Q(v)<0\,\forall v\in F\setminus\{ 0 \} \}.
	\end{equation}
	Nous définissons aussi
	\begin{equation}
		p=\max\{ \dim(G)\tq Q(v)>0\,\forall v\in G\setminus\{ 0 \} \}.
	\end{equation}
	Le couple \( (p,q)\) est la \defe{signature}{signature!forme quadratique} de \( Q\).
\end{definition}

\begin{definition}[Rang d'une forme quadratique]        \label{DEFooVITQooQaMaTF}
	Si \( Q\colon E\to \eK\) est une forme quadratique, nous considérons l'application
	\begin{equation}
		\begin{aligned}
			f_Q\colon E & \to E^*                              \\
			x           & \mapsto \big[ y\mapsto B(x,y) \big].
		\end{aligned}
	\end{equation}
	Le \defe{rang}{rang d'une forme quadratique} de \( Q\) est le rang de l'application linéaire \( f_Q\).
\end{definition}

\begin{proposition}     \label{PROPooLRZQooSfprff}
	Le rang d'une forme quadratique est le rang de sa matrice dans n'importe quelle base.
\end{proposition}

\begin{proof}
	Nous considérons une forme quadratique \( Q\) sur l'espace vectoriel \( E\). Sa trace est, par définition, la trace de l'application linéaire \( f_Q\) de la définition \ref{DEFooVITQooQaMaTF}. Or cette dernière trace ne dépend pas des bases choisies sur \( E\) et \( E^*\). Nous la calculons donc maintenant.

	Soit une base \( \{ e_i \}\) de \( E\) ainsi que sa base duale \( \{ e_i^* \}\) de \( E^*\). Si \( v=\sum_kv_ke_k\in E\), alors
	\begin{equation}
		f_Q(e_i)v=\sum_kv_kB(e_i,e_k)=\sum_kQ_{ik}v_k
	\end{equation}
	où nous avons noté \( B\) la forme bilinéaire associée à \( Q\) et utilisé la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à la forme quadratique \( Q\). Nous avons donc \( f_Q(ei)=\sum_kQ_{ik}e_k^*\) ou encore
	\begin{equation}
		f_Q(e_i)_k=Q_{ik},
	\end{equation}
	ce qui signifie, par \ref{ITEMooKZYYooZPTkpq} que la matrice associée à \( f_Q\) est la matrice \( Q^t\).

	Le rang de \( f_Q\) est donc celui de \( Q^t\), qui est le même que celui de la matrice \( Q\) (ici, nous avons noté \( Q\) la matrice de la forme quadratique \( Q\)). Le rang de \( f_Q\) est celui de sa matrice par la proposition \ref{PROPooEGNBooIffJXc}.
\end{proof}


\begin{lemma}[\cite{BIBooXOWGooAPWTfT}]     \label{LEMooISHCooVDJEKo}
	Soient une forme quadratique \( Q\) ainsi que deux bases \( Q\)-orthogonales \( \{ e_1,\ldots, e_n \}\) et \( \{ e'_1,\ldots, e'_n \}\). Nous posons
	\begin{subequations}
		\begin{align}
			r  & =\Card\{ e_i\tq Q(e_i)>0 \}  \\
			r' & =\Card\{ e_i\tq Q(e'_i)>0 \} \\
			s  & =\Card\{ e_i\tq Q(e_i)<0 \}  \\
			s' & =\Card\{ e_i\tq Q(e'_i)<0 \}
		\end{align}
	\end{subequations}
	Alors \( r=r'\) et \( s=s'\).
\end{lemma}

\begin{proof}
	Nous posons
	\begin{subequations}
		\begin{align}
			I & =\{ i\tq Q(e_i)\geq 0 \}  \\
			J & =\{ j\tq Q(e_j')\leq 0 \}
		\end{align}
	\end{subequations}
	Nous commençons par prouver que \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre. Supposons pour cela que
	\begin{equation}
		\sum_{i\in I}x_ie_i+\sum_{j\in J}y_je'_j=0,
	\end{equation}
	et posons \( z=\sum_{i\in I}x_ie_i\). Nous avons
	\begin{equation}        \label{EQooWGKAooElpETd}
		Q(z)=\sum_{i\in I}x_i^2Q(e_i)\geq 0.
	\end{equation}
	Mais nous avons aussi \( z=-\sum_{j\in J}y_je'_j\), donc
	\begin{equation}        \label{EQooJYOCooZPXmTf}
		Q(z)=\sum_{j\in J}y_j^2Q(e'_j)\leq 0.
	\end{equation}
	Donc \( Q(z)=0\). Vu \eqref{EQooWGKAooElpETd}, et le fait que \( Q(e_i)>0\), avoir \( Q(z)=0\) impose \( x_i=0\) pour tout \( i\). La relation \eqref{EQooJYOCooZPXmTf} nous donne aussi immédiatement que les \( y_j\) sont nuls. Donc la partie \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre.

	Le lemme \ref{LemytHnlD} nous indique qu'une partie libre est toujours de cardinal plus petit ou égal à la dimension de l'espace\footnote{Ici nous utilisons l'hypothèse que \( V\) est de dimension finie.}. Tout ça pour dire que
	\begin{equation}
		\underbrace{\Card(I)}_{=r}+\underbrace{\Card(J)}_{=n-r'}\leq n,
	\end{equation}
	et donc \( r\leq r'\).

	Le même raisonnement, en partant de \( I=\{ i\tq Q(e_i)\leq 0 \}\) et de \( J=\{ j\tq Q(e'_j)>0 \}\), prouve que \( r'\leq r\).

	La preuve de \( s=s'\) est du même tonneau.
\end{proof}

Pour l'équivalence de formes quadratiques, voir la définition \ref{DEFooOLWYooMwhMJp} et la proposition \ref{PROPooBWXMooLsgyKm}.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit scalaire, produit hermitien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}\label{DefVJIeTFj}
	Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel réel est une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition~\ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

La définition suivante est utile pour \randomGender{ceux}{celles} qui veulent faire de la relativité\footnote{Voir le théorème \ref{THOooYHDWooWxVovH} qui établit les transformations de Lorentz.}.
\begin{definition}      \label{DEFooLPBGooXLxubc}
	Un \defe{produit pseudo-scalaire}{produit pseudo-scalaire} sur un espace vectoriel réel est une forme bilinéaire et symétrique.
\end{definition}

\begin{definition}      \label{DEFooZBWTooIqXwRp}
	Nous dirons que deux vecteurs sont \defe{orthogonaux}{orthogonal} lorsque leur produit scalaire\footnote{Définition \ref{DefVJIeTFj}.} est nul. Nous écrivons que \( u\perp v\) lorsque \( \langle u, v\rangle =0\).

	Si \( \{ e_i \}_{i=1,\ldots, n}\) est une base de \( E\), nous disons qu'elle est \defe{orthonormée}{base orthonormée} si
	\begin{equation}
		\langle e_i, e_j\rangle =\delta_{ij}.
	\end{equation}
\end{definition}

\begin{lemma}       \label{LEMooLPUFooVCvnwW}
	Un produit scalaire est toujours non dégénéré\footnote{Définition \ref{DEFooNUBFooLfCqaK}.}.
\end{lemma}

\begin{proof}
	Si \(  b\) est dégénérée, il existe un \( x\neq 0\) tel que \( b(x,z)=0\) pour tout \( z\). En particulier \( b(x,x)=0\), en contradiction avec la condition \ref{DEFooJIAQooZkBtTy}\ref{ITEMooRVGDooQmNqyg} de la stricte positivité.
\end{proof}


Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj}
	Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel réel de dimension finie muni d'un produit scalaire (définition~\ref{DefVJIeTFj}).
\end{definition}
Avouez que c'est drôle qu'un espace vectoriel est euclidien lorsqu'il possède une \emph{multiplication} alors qu'un anneau est euclidien lorsqu'il possède une \emph{division} (voir la définition~\ref{DefAXitWRL}). C'est pas très profond, mais si ça peut vous servir de moyen mnémotechnique\ldots

\begin{propositionDef}[Produit scalaire dans \( \eR^n\), thème \ref{THEMEooUJVXooZdlmHj}]     \label{PROPooSKVRooDGVCYj}
	Si \( x,y\in \eR^n\), nous définissons
	\begin{equation}        \label{EQooFITHooEXDCGd}
		x\cdot y=\sum_{i=1}^n x_iy_i =x_1y_1+x_2y_2+\cdots+x_ny_n.
	\end{equation}
	Cela est un produit scalaire.

	Ce produit scalaire est le \defe{produit scalaire}{produit scalaire sur \( \eR^n\)} qui sera toujours considéré. C'est de lui qui découle toujours la norme, et la topologie de \( \eR^n\). Il sera aussi souvent noté \( \langle x, y\rangle \).
\end{propositionDef}

\begin{proof}
	Il faut que \( b(x,y)=\sum_ix_iy_i\) soit bilinéaire, symétrique, strictement définie positive. Ce sont toutes des vérifications immédiates. Par exemple pour la symétrique, nous avons \( \sum_{i}x_iy_i=\sum_iy_ix_i\).
\end{proof}


Calculons par exemple le produit scalaire de deux vecteurs de la base canonique : \( \langle e_i, e_j\rangle\). En utilisant la formule de définition et le fait que \( (e_i)_k=\delta_{ik}\), nous avons
\begin{equation}
	\langle e_i, e_j\rangle =\sum_{k=1}^m\delta_{ik}\delta_{jk}.
\end{equation}
Nous pouvons effectuer la somme sur \( k\) en remarquant qu'à cause du \( \delta_{ik}\), seul le terme avec \( k=i\) n'est pas nul. Effectuer la somme revient donc à remplacer tous les \( k\) par des \( i\) :
\begin{equation}
	\langle e_i, e_j\rangle =\delta_{ii}\delta_{ji}=\delta_{ji}.
\end{equation}

Une des propriétés intéressantes du produit scalaire est qu'il permet de décomposer un vecteur dans une base, comme nous le montre la proposition suivante.

\begin{proposition}     \label{PropScalCompDec}
	Si nous notons \( v_i\) les composantes du vecteur \( v\), c'est-à-dire si \( v=\sum_{i=1}^m v_ie_i\), alors nous avons \( v_j=\langle v, e_j\rangle\).
\end{proposition}

\begin{proof}
	\begin{equation}    \label{Eqvejscalcomp}
		v\cdot e_j=\sum_{i=1}^m\langle v_ie_i, e_j\rangle =\sum_{i=1}^mv_i\langle e_i, e_j\rangle =\sum_{i=1}^mv_i\delta_{ij}
	\end{equation}
	En effectuant la somme sur \( i\) dans le membre de droite de l'équation \eqref{Eqvejscalcomp}, tous les termes sont nuls sauf celui où \( i=j\); il reste donc
	\begin{equation}
		v\cdot e_j=v_j.
	\end{equation}
\end{proof}

Le produit scalaire ne dépend en réalité pas de la base orthogonale choisie.

\begin{lemma}       \label{LEMooZMCWooDfbrIq}
	Si \( \{ e_i \}\) est la base canonique, et si \( \{ f_i \}\) est une autre base orthonormale\footnote{Définition \ref{DEFooZBWTooIqXwRp}.}, alors si \( u\) et \( v\) sont deux vecteurs de \( \eR^m\), nous avons
	\begin{equation}
		\sum_i u_iv_j=\sum_iu'_iv'_j
	\end{equation}
	où \( u_i\) sont les composantes de \( u\) dans la base \( \{ e_i \}\) et \( u'_i\) sont celles dans la base \( \{ f_i \}\).
\end{lemma}

\begin{proof}
	La preuve demande un peu d'algèbre linéaire. Étant donné que \( \{ f_i \}\) est une base orthonormale, il existe une matrice \( A\) orthogonale (\( AA^t=\mtu\)) telle que \( u'_i=\sum_jA_{ij}u_j\) et idem pour \( v\). Nous avons alors
	\begin{equation}
		\begin{aligned}[]
			\sum_iu'_iv'_j & =\sum_i\left( \sum_jA_{ij} u_j\right)\left( \sum_k A_{ik}v_k \right) \\
			               & =\sum_{ijk}A_{ij}A_{ik}u_jv_k                                        \\
			               & =\sum_{jk}\underbrace{\sum_i(A^t)_{ji}A_{ik}}_{=\delta_{jk}}u_jv_k   \\
			               & =\sum_{jk}\delta_{jk}u_jv_k                                          \\
			               & =\sum_ku_kv_k.
		\end{aligned}
	\end{equation}
\end{proof}

Cette proposition nous permet de réellement parler du produit scalaire entre deux vecteurs de façon intrinsèque sans nous soucier de la base dans laquelle nous exprimons les vecteurs.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Hermitien, unitaire, etc.}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{ooJUXBooVrwvfP}]  \label{DefMZQxmQ}
	Soit \( E\) est un espace vectoriel sur \( \eC\). Une application \( \langle ., .\rangle \colon E\times E\to \eC\) est \defe{sesquilinéaire à droite}{sesquilinéaire} si pour tout \( x,y\in E\) et pour tout \( \lambda\in \eC\),
	\begin{enumerate}
		\item
		      \( \langle \lambda x, y\rangle =\lambda\langle x,y \rangle =\langle x, \bar\lambda y\rangle \),
		\item
		      \( \langle x+y, z\rangle =\langle x, z\rangle+\langle y, z\rangle  \),
		\item
		      \( \langle x, y+z\rangle =\langle x, y\rangle +\langle x, z\rangle \).
	\end{enumerate}
	Définitions supplémentaires :
	\begin{itemize}
		\item
		      La forme \( \langle ., .\rangle \) est \defe{hermitienne}{hermitienne} si de plus
		      \begin{equation}
			      \langle x, y\rangle =\overline{ \langle y, x\rangle  }.
		      \end{equation}
		\item
		      Un \defe{produit hermitien}{produit hermitien} est une forme hermitienne strictement définie positive, c'est-à-dire telle que \( \langle x, x\rangle \geq 0\) pour tout \( x\in E\) et \( \langle x, x\rangle =0\) si et seulement si \( x=0\).
	\end{itemize}
\end{definition}

\begin{propositionDef}     \label{PROPooRPRRooRYEMCB}
	Soit un espace hermitien \( (E,\langle ., .\rangle )\). Soit un opérateur linéaire \( A\in\End(E)\). Il existe un unique opérateur \( B\in\End(E)\) tel que
	\begin{equation}
		\langle Ax, y\rangle =\langle x, By\rangle
	\end{equation}
	pour tout \( x,y\in E\).

	L'opérateur \( B\) ainsi défini est noté \( A^{\dag}\) et est nommé \defe{hermitien conjugué}{conjugué hermitien} de \( A\).
\end{propositionDef}

\begin{lemma}       \label{LEMooBOXMooSDyCfm}
	Au niveau des matrices, nous avons
	\begin{equation}
		A^{\dag}_{ij}=A^*_{ji}
	\end{equation}
	où \( z^*\) est le conjugué complexe.
\end{lemma}

\begin{definition}      \label{DEFooKEBHooWwCKRK}
	Quelque définitions.
	\begin{enumerate}
		\item
		      Un opérateur \( A\) est \defe{hermitien}{opérateur hermitien} si \( A^{\dag}=A\).
		\item
		      Un opérateur \( A\) est \defe{unitaire}{opérateur unitaire} si \( A^{\dag}=A^{-1}\).
	\end{enumerate}
	Note : le conjugué hermitien est parfois noté \( A^*\) au lieu de \( A^{\dag}\).
\end{definition}

\begin{normaltext}
	Le mot «hermitien» est réservé aux opérateurs sur des espaces hermitiens, c'est-à-dire des espaces vectoriels sur \( \eC\). Le mot «autoadjoint» par contre est plutôt utilisé dans le cadre d'opérateurs sur les espaces réels. En conséquence de quoi, ces deux mots sont synonymes, mais il est préférable d'utiliser «hermitien» lorsque l'espace vectoriel est sur \( \eC\) et «autoadjoint» lorsqu'il est sur \( \eR\).

	L'ensemble des opérateurs autoadjoints de \( E\) est noté \( \gS(E)\)\nomenclature[A]{\( \gS(E)\)}{Les opérateurs autoadjoints de \( E\)}. Cette notation provient du fait que dans \( \eR^n\) muni du produit scalaire usuel, les opérateurs autoadjoints sont les matrices symétriques.
\end{normaltext}

\begin{remark}
	Le fait d'être hermitien n'implique en rien le fait d'être inversible.
\end{remark}

\begin{normaltext}
	Les normes associées aux produits scalaires font intervenir une racine carré, et donc devront être données plus tard. Voir le thème \ref{THEMEooUJVXooZdlmHj}.
\end{normaltext}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooMWUCooMbJuaJ}
	Nous considérons \(\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\).
	\begin{enumerate}
		\item
		      La formule, pour \( x,y\in \eC^n\),
		      \begin{equation}    \label{EqFormSesqQrjyPH}
			      \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
		      \end{equation}
		      définit une forme sesquilinéaire sur \( \eC^n\).
		\item
		      L'ensemble \( \eC^n\) devient un espace vectoriel hermitien.
	\end{enumerate}
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Éléments de matrice}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooZKWXooWmEzoA}
	Soit une application linéaire \( A\colon \eR^n\to \eR^n\). Nous considérons le produit scalaire usuel\footnote{Définition \ref{PROPooSKVRooDGVCYj}.} sur \( \eR^n\). Alors :
	\begin{enumerate}
		\item
		      Les éléments de matrice de \( A\) sont donnés par \( A_{ij}=e_i\cdot Ae_j\).
		\item
		      Nous avons la formule \( x\cdot Ay=\sum_{kl}A_{kl}x_ky_l \).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Pour \( Ae_j\) nous utilisons la formule \ref{EQooOKOJooYgteNP} avec des notations plus décontractées : \( Ae_j=\sum_kA_{kj}e_k\). Ensuite nous faisons un calcul avec la formule \eqref{EQooFITHooEXDCGd} :
	\begin{equation}
		e_i\cdot Ae_j=e_i\cdot \sum_kA_{kj}e_k=\sum_{k}A_{kj}\delta_{i,k}=A_{ij}.
	\end{equation}
	La seconde formule à prouver est du même tonneau, en utilisant cette fois la formule \eqref{EQooAXRJooUwHbjB} :
	\begin{equation}
		x\cdot Ay=\sum_kx_k(Ay)_k=\sum_{kl}x_kA_{kl}y_l=\sum_{kl}A_{kl}x_ky_l.
	\end{equation}
\end{proof}

La proposition suivante est une version plus «pragmatique» de la proposition \ref{PropXrTDIi}.
\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooNITTooCYcrrT}
	Soient un espace euclidien\footnote{Qui possède un produit scalaire, définition \ref{DefLZMcvfj}.} de dimension finie \( V\) ainsi qu'un sous-espace \( M\). Nous posons
	\begin{equation}
		M^{\perp}=\{ x\in V\tq x\cdot y=0\forall y\in M \}.
	\end{equation}
	Alors \( M\oplus M^{\perp}=V\).
\end{proposition}

\begin{proof}
	D'abord si \( x\in M\cap M^{\perp}\), alors \( x\cdot x=0\) et donc \( x=0\). Donc nous avons déjà \( M\cap M^{\perp}=\{ 0 \}\). Nous considérons une base \( \{b_1,\ldots, b_k\}\) de \( M\), et nous définissons l'application linéaire
	\begin{equation}
		\begin{aligned}
			f\colon V & \to \eR^k                                \\
			x         & \mapsto (x\cdot b_1,\ldots, x\cdot b_k).
		\end{aligned}
	\end{equation}
	Nous avons que \( M^{\perp}=\ker(f)\). Le théorème du rang \ref{ThoGkkffA} nous indique que
	\begin{equation}
		\dim(V)=\dim\big( \ker(f) \big)+\dim\big( \Image(f) \big)\leq \dim(M^{\perp})+k=\dim(M^{\perp})+\dim(M).
	\end{equation}
	Une justification : vu que \( f\) prend ses valeurs dans \( \eR^k\), la dimension de son image est majorée par \( k\).

	Nous en déduisons que
	\begin{equation}
		\dim(M)+\dim(M^{\perp})\geq\dim(V),
	\end{equation}
	et la proposition \ref{PROPooCASNooEqisqa} nous permet de conclure que \( M\oplus M^{\perp}=V\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : pas d'approche naïve}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooGPXVooEYwIiJ}

Il est légitime, si \( t\colon E\to E\) est une application linéaire, de dire que sa transposée soit l'application linéaire \( t^t\colon E\to E\) dont la matrice est la matrice transposée de celle de \( t\). Lorsque nous travaillons sur \( \eR^n\) muni de la base canonique, cela ne pose pas de problème et nous pouvons écrire des égalités du type \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).

\begin{proposition}[Matrice transposée et produit scalaire]     \label{PROPooNARVooEuhweD}
	Soit une matrice réelle \( A\). En utilisant l'application linéaire associée\footnote{Définition \ref{DEFooJVOAooUgGKme}. Ici nous considérons la base canonique sur \( \eR^n\)} \( f_A\colon \eR^n\to \eR^n\), nous avons
	\begin{equation}
		x\cdot f_A(y)=f_{A^t}(x)\cdot y.
	\end{equation}
	Cette formule est souvent écrite \( x\cdot Ay=A^tx\cdot y\) ou \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).
\end{proposition}

\begin{proof}
	Il s'agit d'un calcul utilisant la formule \eqref{EQooBVGHooJhFbMs} et le produit scalaire \eqref{EQooFITHooEXDCGd} :
	\begin{equation}
		x\cdot f_A(y)=\sum_ix_i\big( f_A(y) \big)_i=\sum_ix_i\sum_jA_{ij}y_j=\sum_{ij}A^t_{ji}x_iy_j=\sum_jf_{A^t}(x)_jy_j=f_{A^t}(x)\cdot y.
	\end{equation}
\end{proof}

Hélas nous allons voir que cette façon de définir une transposée est mauvaise.

Soit une application linéaire \( t\colon E\to E\) de matrice \( A\) dans la base \( \{ e_i \}_{i=1,\ldots, n}\) et de matrice \( B\) dans la base \( \{ f_{\alpha} \}_{\alpha=1,\ldots, n}\). Nous notons \( Q\) la matrice de passage d'une base à l'autre :
\begin{equation}
	e_i=\sum_{\alpha}Q_{\alpha i}^{-1}f_{\alpha}.
\end{equation}

Nous nommons \( t_1\) l'application linéaire associée à \( A^t\) dans la base \( \{ e_i \}\) et \( t_2\) l'application linéaire associée à la matrice \( B^t\) dans la base \( \{ f_{\alpha} \}\). Définir la transposée d'une application linéaire comme étant l'application linéaire associée à la transposée de sa matrice ne sera une bonne définition que si \( t_1=t_2\).

La première chose facile à voir est
\begin{equation}        \label{EQooAMHPooUQEkJo}
	t_1(e_i)_j=\sum_k(A^t)_{jk}(e_i)_k=A^t_{ji}=A_{ij}.
\end{equation}
Pour calculer \( t_2(e_i)_j\), c'est un peu plus laborieux :
\begin{subequations}
	\begin{align}
		t_2(e_i) & =\sum_{\alpha}Q_{\alpha i}^{-1} t_2(f_\alpha)=\sum_{\beta\gamma\alpha}Q_{\alpha i}^{-1}B^t_{\gamma\beta}\underbrace{(f_{\alpha})_{\beta}}_{\delta_{\alpha\beta}}f_{\gamma}=\sum_{\beta\gamma}Q_{\beta i}^{-1}B^t_{\gamma\beta}f_{\gamma} \\
		         & =\sum_{j\gamma}(B^tQ^{-1})_{\gamma i}Q_{j\gamma}e_j                                                                                                                                                                                      \\
		         & =\sum_j(QB^tQ^{-1})_{ji}e_j.
	\end{align}
\end{subequations}
Donc \( t_2(e_i)_j=(QB^tQ^{-1})_{ji}\). En tenant compte du fait que \( B=Q^{-1}AQ\) nous avons
\begin{equation}
	t_2(e_i)_j=(QQ^tA^t(Q^{-1})^tQ^{-1})_{ji}.
\end{equation}
Ceci est égal à l'expression \eqref{EQooAMHPooUQEkJo} lorsque \( Q^t=Q^{-1}\). Nous voyons que confondre transposée d'une application linéaire avec transposée de la matrice associée n'est valable que si nous sommes certain de ne considérer que des changements de base par des matrices orthogonales.

C'est la situation typique dans laquelle nous nous trouvons lorsque nous considérons des applications linéaires sur \( \eR^n\) muni de la base canonique, et que nous n'avons aucune intention de changer de base, et encore moins de chercher une base non orthonormale. Cette situation est clairement la situation la plus courante.

\begin{example}[\cite{ooLIOMooBuCPUS}]
	Soit la base canonique \( \{ e_1,e_2 \}\) de \( \eR^2\). Nous considérons l'application linéaire \( t\colon \eR^2\to \eR^2\) définie par
	\begin{subequations}
		\begin{align}
			t(e_1) & =e_1 \\
			t(e_2) & =0.
		\end{align}
	\end{subequations}
	La matrice de \( t\) dans cette base est
	\begin{equation}
		A=\begin{pmatrix}
			1 & 0 \\
			0 & 0
		\end{pmatrix}.
	\end{equation}
	Elle est symétrique : elle vérifie \( A^t=A\). Si nous comptions sur la transposée de matrice pour définir la transposée de \( t\), nous aurions \( t^t=t\).

	Soit maintenant la base \( f_1=e_1\), \( f_2=e_1+e_2\). Nous avons \( t(f_1)=f_1\) et
	\begin{equation}
		t(f_2)=t(e_1)+t(e_2)=e_1=f_1.
	\end{equation}
	Donc la matrice de \( t\) dans cette base est
	\begin{equation}
		B=\begin{pmatrix}
			1 & 1 \\
			0 & 0
		\end{pmatrix}.
	\end{equation}
	Et là, nous avons \( B^t\neq B\). Donc en comptant sur cette base pour définir la transposée de \( t\) nous aurions \( t^t\neq t\).
\end{example}

\begin{normaltext}      \label{NooMZVRooExWVKJ}
	Autrement dit, la façon «usuelle» de voir la transposée d'une application linéaire, ne fonctionne dans les livres pour enfants uniquement parce qu'on y considère toujours \( \eR^n\) muni de la base canonique ou de bases orthonormées.

	Notons que nous avons tout de même les notions d'opérateur adjoint et autoadjoint pour parler d'application orthogonale sans passer par la transposée, voir~\ref{DEFooYKCSooURQDoS}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : la bonne approche}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooZLPAooKTITdd}
	Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
	\begin{equation}
		f^t(\omega)(x)=\omega\big( f(x) \big).
	\end{equation}
	pour tout \( \omega\in F^*\) et \( x\in E\).
\end{definition}


\begin{proposition}[transposée et conjuguée\cite{MonCerveau}]		\label{PROPooFAVAooDevBrT}
	Soit une application linéaire \(A \colon \eR^n\to \eR^n  \) ainsi que l'isomorphisme canonique \(\phi \colon \eR^n\to (\eR^n)^*  \). Nous avons
	\begin{equation}
		A^{\dag}=\phi\circ A^t\circ \phi
	\end{equation}
	où \( A^t\) est l'opérateur transposé, défini en \ref{DefooZLPAooKTITdd}, et \( A^{\dag}\) est l'hermitien conjugué défini en \ref{PROPooRPRRooRYEMCB}.
\end{proposition}

\begin{normaltext}		\label{NORMooFUZYooWeXjp}
	Étant données les propositions \ref{PROPooFAVAooDevBrT}, le fait que l'isomorphisme soit canonique, que la matrice de l'hermitien conjugué est la matrice transposée (dans le cas réel, par le lemme \ref{LEMooBOXMooSDyCfm}), nous écrirons souvent \( A^t\) pour \( A^{\dag}\), et donc nous écrirons des égalités comme
	\begin{equation}
		x\cdot Ay=A^tx\cdot y
	\end{equation}
	alors qu'il faudrait écrire
	\begin{equation}
		x\cdot Ay=A^{\dag}x\cdot y
	\end{equation}

	Notons également que cette remarque est une raison pour laquelle je préfère noter \( A^t\) au lieu de \( {}^tA\).
\end{normaltext}

\begin{lemma}       \label{LEMooEMNNooPquZMg}
	Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).

	Autrement dit, en utilisant l'application \( \psi\colon \eM\to \aL(F^*,E^*)\) de la proposition \ref{PROPooGXDBooHfKRrv},
	\begin{equation}
		\psi(A^t)=f^t.
	\end{equation}
\end{lemma}

\begin{proof}
	Attention aux indices, ça va chauffer\footnote{Et merci à Alain Vigne pour m'avoir fait remarquer qu'il fallait mettre de l'ordre dans les indices.}.

	Nous allons montrer que \( f^t=\psi(A^t)\) sur la base \( \{ g_i^* \}\), et pour cela nous appliquons \( f^t(g_i^*)\) à \( x\in E\) :
	\begin{subequations}
		\begin{align}
			f^t(g_i^*)x & =g_i^*\big( f(x) \big)                                     & \text{Définition  \ref{DefooZLPAooKTITdd}} \\
			            & =g_i^*\big( \sum_kx_kf(e_k) \big)                                                                       \\
			            & =g_i^*\big( \sum_{kl}x_kA_{lk}g_l \big)                    & \text{eq. \eqref{EQooOKOJooYgteNP}}        \\
			            & =\sum_{kl}x_kA_{lk}\underbrace{g_i^*(g_l)}_{=\delta_{i,l}}                                              \\
			            & =\sum_kx_kA_{ik}                                                                                        \\
			            & =\sum_k(A^t)_{ki}x_k                                                                                    \\
			            & =\sum_k(A^t)_{ki}e^*_k(x).
		\end{align}
	\end{subequations}
	Voilà. Donc nous avons
	\begin{equation}
		f^t(g_i^*)=\sum_k(A^t)_{ki}e^*_k=\psi(A^t)g_i^*.
	\end{equation}
\end{proof}

\begin{normaltext}
	Intuitivement,  les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carrée de déterminant non nul.

	Nous donnons maintenant une vraie preuve de ce résultat.
\end{normaltext}

\begin{lemma}[\cite{BIBooETROooZIGZkN}]   \label{LemSEpTcW}
	Si \( f\colon E\to F\) est une application linéaire, alors
	\begin{equation}
		\rank(f)=\rank(f^t).
	\end{equation}
\end{lemma}

\begin{proof}
	Soient \( n=\dim(E)\) et \( r=\dim(F)\).
	Nous posons \( \dim\ker(f)=p\) et donc \( \rank(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
	\begin{equation}
		g_i=f(e_{p+i})
	\end{equation}
	pour \( i=1,\ldots, n-p\). C'est-à-dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
	\begin{equation}
		\sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
	\end{equation}
	alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), c'est-à-dire \( \sum_ka_ke_{p+k}\in\ker(f)\). Comme une base de \( \ker(f)\) est \( \{ e_1,\ldots, e_p \}\), il existe des \( b_l\) tels que \( \sum_{k}a_ke_{p+k}=\sum_{l=1}^pb_le_l\). Autrement dit,
	\begin{equation}
		\sum_{k}a_ke_{p+k}-\sum_{l=1}^pb_le_l=0,
	\end{equation}
	qui est une combinaison linéaire nulle des \( e_i\). Donc \( a_k=b_l=0\) pour tout \( k\) et \( l\). Tout ça pour dire que les \( \{ g_i \}_{i=1,\ldots, n-p}\) est libre.

	Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous pouvons les compléter en une base de \( F\) :
	\begin{equation}
		\{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}.
	\end{equation}

	Nous prouvons maintenant que \( \rank(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) forment une partie libre (et donc l'espace image de \( f^t\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
	\begin{equation}
		f^t(g^*_i)(e_k)=g_i^*(f(e_k)),
	\end{equation}
	Si \( k=1,\ldots, p\), alors \( f(e_k)=0\) et donc \( g_i^*(f(e_k))=0\); si \( k=p+l\) alors
	\begin{equation}
		f^t(g_i^*)(e_k)=g_i^*(f(e_{k+l}))=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
	\end{equation}
	Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
	\begin{equation}
		\rank(f^t)\geq n-p=\rank(f).
	\end{equation}
	En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
	\begin{equation}
		\rank\big( (f^t)^t \big)\geq \rank(f^t)
	\end{equation}
	et donc, sachant que \( (f^t)^t=f\), nous obtenons \( \rank(f)=\rank(f^t)\).

\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]        \label{PropWOPIooBHFDdP}
	Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
	\begin{equation}
		\Image(f^t)=\ker(f)^{\perp}.
	\end{equation}
\end{proposition}

\begin{proof}
	Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est-à-dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est-à-dire que \( \omega\in (\ker f)^{\perp}\).

	Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
	\begin{subequations}
		\begin{align}
			\dim\big( \Image(f^t) \big) & =\rank(f^t)                                                              \\
			                            & =\rank(f)                         & \text{lemme~\ref{LemSEpTcW}}         \\
			                            & =\dim(E)-\dim\ker(f)              & \text{théorème~\ref{ThoGkkffA}}      \\
			                            & =\dim\big( (\ker f)^{\perp} \big) & \text{proposition~\ref{PropXrTDIi}}.
		\end{align}
	\end{subequations}
\end{proof}

\begin{lemma}[\cite{ooEPEFooQiPESf}]
	Soit \( \eK\) un corps, \( E\) et \( F\) deux \( \eK\)-espaces vectoriels de dimension finie et une application linéaire \( f\colon E\to F\). L'application \( f\) est injective si et seulement si sa transposée\footnote{Définition~\ref{DefooZLPAooKTITdd}.} \( f^t\) est surjective.
\end{lemma}

\begin{proof}
	Supposons que \( f\) soit injective. Alors par le lemme~\ref{LEMooDAACooElDsYb}, il existe \( g\colon F\to E\) tel que \( g\circ f=\id|_E\). Nous avons alors aussi \( (g\circ f)^t=\id|_{E^*}\), mais \( (g\circ f)^t=f^t\circ g^t\), donc \( f^t\) est surjective.

	Inversement, nous supposons que \( f^t\colon F^*\to E^*\) est surjective. Alors en nous souvenant que \( E\) et \( F\) sont de dimension finie et en faisant jouer les identifications \( (f^t)^t=f\) et \( (E^*)^*=E\) nous savons qu'il existe \( s\colon E^*\to F^*\) tel que \( f^t\circ s=\id|_{E^*}\). En passant à la transposée,
	\begin{equation}
		s^t\circ f=\id|_{E},
	\end{equation}
	qui implique que \( f\) est injective.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}
	Soit \( E=\Poly_n(\eR)\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in \Poly_n(\eR)^*\),
	\begin{equation}
		f_i(P)=P(a_i).
	\end{equation}
	La partie \( \{ f_1,\ldots, f_n \}\) est une base de \( \Poly_n(\eR)^*\).

	Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme de Lagrange} aux points \( (a_i)\) sont les polynômes de la base préduale de la base \( \{ f_i \}\).
\end{lemmaDef}

\begin{proof}
	Nous prouvons que l'orthogonal est réduit au singleton nul :
	\begin{equation}
		\Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}.
	\end{equation}
	La proposition~\ref{PropXrTDIi} conclura. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

\begin{proposition}
	Les polynômes de Lagrange aux points \( (a_i)_{i=1,\ldots, n}\) sont donnés par
	\begin{equation}
		P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
	\end{equation}
\end{proposition}

\begin{proof}
	Il suffit de vérifier que \( f_j(P_i)=\delta_{i,j}\). Nous avons
	\begin{equation}
		f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
	\end{equation}
	Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dual de \texorpdfstring{\(  \eM(n,\eK)\)}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
	Soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
	\begin{equation}
		\begin{aligned}
			f_A\colon \eM(n,\eK) & \to \eK          \\
			M                    & \mapsto \tr(AM).
		\end{aligned}
	\end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}


\begin{proof}
	Nous considérons l'application
	\begin{equation}
		\begin{aligned}
			f\colon \eM(n,\eK) & \to \eM(n,\eK)^* \\
			A                  & \mapsto f_A
		\end{aligned}
	\end{equation}
	et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM(n,\eK)\) et \( \left(\eM(n,\eK)\right)^*\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{ik}\delta_{jl}\) :
	\begin{equation}
		0 = f_A(E_{ij})
		= \sum_{k}(AE_{ij})_{kk}
		= \sum_{kl}A_{kl}(E_{ij})_{lk}
		= \sum_{kl}A_{kl}\delta_{il}\delta_{jk}
		= A_{ji}.
	\end{equation}
	Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
	Soient un corps \( \eK\) ainsi qu'une application \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( M,N\in \eM(n,\eK)\) on ait
	\begin{equation}
		\phi(MN)=\phi(NM).
	\end{equation}
	Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
	La proposition~\ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(MN)=f_A(NM)\), c'est-à-dire
	\begin{equation}
		\Tr(AMN)=\Tr(ANM)
	\end{equation}
	pour toutes matrices \( M, N\in \eM(n,\eK)\). L'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.} appliqué au membre de droite nous donne \( \Tr(AMN)=\Tr(MAN)\), ce qui signifie que
	\begin{equation}
		\Tr\big( (AM-MA)N \big)=0
	\end{equation}
	ou encore que \( f_{AM-MA}=0\), et ce, pour toute matrice \( M\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
	\begin{equation}
		AM=MA
	\end{equation}
	pour tout \( M\in\eM(n,\eK)\). En particulier, en prenant pour \( M \) les fameuses matrices \( E_{ij}\) et en calculant un peu,
	\begin{equation}
		A_{li}\delta_{j,m}=\delta_{i,l}A_{jm}
	\end{equation}
	pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
	\begin{equation}
		\phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
	\end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]       \label{CorICUOooPsZQrg}
	Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
	Soit \( \mH\) un hyperplan de \( \eM(n,\eK)\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition~\ref{PropHOjJpCa} nous donne \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme~\ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
	\begin{equation}
		J_r=\begin{pmatrix}
			\mtu_r & 0 \\
			0      & 0
		\end{pmatrix}.
	\end{equation}
	Pour tout \( M\in \eM(n,\eK)\) nous avons
	\begin{equation}
		\phi(M)=\Tr(AM)=\Tr(PJ_rQM)=\Tr(J_rQMP),
	\end{equation}
	la dernière égalité découlant de l'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.}. Ce que nous cherchons est \( M\in \GL(n,\eK)\) telle que \( \phi(M)=0\). Nous commençons par trouver \( N\in\GL(n,\eK)\) telle que \( \Tr(J_rN)=0\). Celle-là est facile : c'est
	\begin{equation}
		N=\begin{pmatrix}
			0          & 1 \\
			\mtu_{n-1} & 0
		\end{pmatrix}.
	\end{equation}
	Les éléments diagonaux de \( J_rN\) sont tous nuls. Par conséquent en posant \( M=Q^{-1}NP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}
