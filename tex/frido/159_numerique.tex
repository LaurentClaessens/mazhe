% This is part of Mes notes de mathématique
% Copyright (C) 2010-2020, 2023, 2025
%   Laurent Claessens
% See the file LICENCE.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Conditionnement d'une matrice}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooQGLRooZQzzsA}

Soit le système d'équations linéaires \( Au=b\) avec la matrice inversible \( A\) ainsi que le système perturbé \( (A+\Delta A)u'=(b+\Delta b)\). Nous notons \( \Delta u=u'-u\) et nous voudrions pouvoir dire des choses de l'erreur relative \( \frac{ \| \Delta u \| }{ \| u \| }\).

\begin{example}[\cite{ooLMMRooUXhOdx}]
	Soit la matrice
	\begin{equation}
		A=\begin{pmatrix}
			10 & 7 \\
			7  & 5
		\end{pmatrix}
	\end{equation}
	et \( b=\begin{pmatrix}
		32 \\
		23
	\end{pmatrix}\). La solution de \( Au=b\) est \( u=\begin{pmatrix}
		-1 \\
		6
	\end{pmatrix}\). Si nous conservons la même matrice mais nous considérons \( b=\begin{pmatrix}
		32.1 \\
		22.9
	\end{pmatrix}\). La solution devient \( u'=\begin{pmatrix}
		0.2 \\
		4.3
	\end{pmatrix}\)

	En norme \( \| . \|_{\infty}\) nous avons\footnote{La proposition~\ref{PropLJEJooMOWPNi}\ref{ItemABSGooQODmLNiii} montre que si nous voulions des estimations en norme \( \| . \|_2\), il y aurait au maximum un facteur \( \sqrt{2}\) par-ci par-là.}
	\begin{equation}
		\frac{ \| \Delta b \| }{ \| b \| }=\frac{ 0.1 }{ 32 }=0.003125
	\end{equation}
	et
	\begin{equation}
		\frac{ \| \Delta u \| }{ \| u \| }=\frac{ 1.7 }{ 6 }=0.28.
	\end{equation}
	Cela montre environ amplification d'un facteur \( 100\) entre l'erreur sur \( b\) et l'erreur sur la solution.
\end{example}

\begin{definition}      \label{DEFooBKQWooJuoCGX}
	Le \defe{conditionnement}{conditionnement!d'une matrice inversible} de la matrice inversible \( A\in \GL(n,\eC)\) est le nombre positif
	\begin{equation}
		\Cond(A)=\| A \|\| A^{-1} \|.
	\end{equation}
\end{definition}

Cette dénomination sera justifiée par le corolaire~\ref{CORooXKPWooJVHVvh} parce qu'il est évident que le conditionnement d'une matrice est lié au conditionnement du problème de résolution d'un système linéaire.

\begin{remark}
	Le conditionnement dépend de la norme choisie, mais cette dependence est contrôlée par la proposition~\ref{PropLJEJooMOWPNi} qui nous indique que si le conditionnement d'une matrice est grand dans une norme, il sera grand dans une autre norme.

	D'autre part, lorsque nous écrirons \( \| A \|\) nous supposerons toujours que \( \| . \|\) est une norme d'algèbre\footnote{Définition~\ref{DefJWRWQue}.} et donc que nous avons toujours
	\begin{equation}
		\| AB \|\leq \| A \|\| B \|.
	\end{equation}
	De plus nous supposerons toujours avoir une norme subordonnée à une norme sur l'espace \( \eC^n\), de telle sorte à avoir
	\begin{equation}
		\| Au \|\leq \| A \|\| u \|
	\end{equation}
	pour tout \( u\in\eC^n\). Voir aussi le lemme~\ref{LEMooIBLEooLJczmu}.
\end{remark}

\begin{proposition}[\cite{ooLMMRooUXhOdx}]
	Si \( A\) est une matrice inversible et si \( \alpha\in \eC\) nous avons :
	\begin{enumerate}
		\item
		      \( \Cond(A)\geq 1\)
		\item
		      \( \Cond(A)=\Cond(A^{-1})\)
		\item
		      \( \Cond(\alpha A)=\Cond(A)\).
	\end{enumerate}
	Si \( Q\in\gO(n)\) alors
	\begin{enumerate}
		\item
		      Nous avons \( \Cond_2(Q)=1\) où \( \Cond_2\) est le conditionnement pour la norme \( \| . \|_2\).
		\item
		      Nous avons aussi
		      \begin{equation}
			      \Cond_2(A)=\Cond_2(AQ)=\Cond_2(QA).
		      \end{equation}
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous savons que \( \Cond(\mtu)=1\) et donc
	\begin{equation}
		1=\| \mtu \|\leq \| A \|\| A^{-1} \|
	\end{equation}
	parce que la norme utilisée est une norme matricielle.

	Les deux autres formules sont évidentes à partit du fait que la définition du conditionnement de \( A\) est symétrique entre \( A\) et \( A^{-1}\).

	En ce qui concerne les formules relatives à la matrice orthogonale \( Q\) nous savons par la proposition~\ref{PropKBCXooOuEZcS}\ref{ITEMooOWMBooHUatNb} qu'une matrice orthogonale est une bijection de l'ensemble \(  \{ x\in \eR^n\tq \| x \|=1 \}  \). Par conséquent
	\begin{equation}
		\| AQ \|=\sup_{x\tq \| x \|=1}\| AQx \|=\sup_{ Q^{-1}x\tq \| x \|=1  }\| AQQ^{-1}x \|=\| A \|.
	\end{equation}
	Donc \( \| AQ \|=\| A \|\). Les assertions s'ensuivent immédiatement en remarquant que \( Q^{-1}\) est également orthogonale.
\end{proof}

Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} et le théorème~\ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque~\ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{proposition}[\cite{ooLMMRooUXhOdx}]      \label{PROPooNUAUooIbVgcN}
	Soit une matrice inversible \( A\in\GL(n,\eC)\), et \( \mu_1\leq\ldots\leq \mu_n\) les valeurs propres de \( A^*A\). Alors nous avons la formule
	\begin{equation}
		\Cond_2(A)=\sqrt{ \frac{ \mu_n }{ \mu_1 }}.
	\end{equation}
\end{proposition}

\begin{proof}
	Par le théorème~\ref{THOooNDQSooOUWQrK}, la norme de \( A\) est liée au au rayon spectral de \( A^*A\) par
	\begin{equation}
		\| A \|_2=\sqrt{ \rho(A^*A) }=\sqrt{ \mu_n }.
	\end{equation}
	Vu que le spectre de \( AA^*\) est le même que celui de \( A^*A\) (lemme~\ref{LEMooHUGEooVYhZdZ}) nous avons aussi
	\begin{equation}
		\| A^{-1} \|_2=\sqrt{ \rho\big( (A^{-1})^*A^{-1} \big) }=\sqrt{ \rho\big( (A^*A)^{-1} \big) }=\frac{1}{ \sqrt{ \mu_1 } }
	\end{equation}
	parce que la plus grande valeur propre de \( (A^*A)^{-1}\) est l'inverse de la plus petite de \( A^*A\).

	Ces deux calculs étant,
	\begin{equation}
		\Cond_2(A)=\| A \|_2\| A^{-1} \|_2=\sqrt{ \frac{ \mu_n }{ \mu_1 } }.
	\end{equation}
\end{proof}

\begin{probleme}
	À mon avis ce qui est dans la proposition~\ref{PropSOOooGoMOxG} est le conditionnement de la matrice ou sa racine carrée ou un truc du genre. Il faut voir le lien entre les valeurs propres de \( A\) et celles de \( AA^*\).
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Perturbation du vecteur}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Système linéaire : perturbation du vecteur\cite{ooLMMRooUXhOdx}]        \label{PROPooGIXFooAhJkIs}
	Soit une matrice inversible \( A\) et les systèmes d'équations linéaires
	\begin{subequations}        \label{EQooYQIGooPXqWoX}
		\begin{align}
			Au=b \\
			Au'=b'.
		\end{align}
	\end{subequations}
	En notant \( \Delta u=u'-u\) et \( \Delta b=b'-b\) nous avons
	\begin{equation}        \label{EQooESXRooMYuvRa}
		\frac{ \| \Delta u \| }{ \| u \| }\leq \Cond(A)\frac{ \| \Delta b \| }{ \| b \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	En soustrayant les équations \eqref{EQooYQIGooPXqWoX} nous avons \( \Delta b=A\Delta u\), et donc \( \Delta u=A^{-1} \Delta b\). D'une part nous avons alors
	\begin{equation}
		\| \Delta u \|\leq \| A^{-1} \|\| \Delta b \|.
	\end{equation}
	Et d'autre part, \( \| b \|\leq \| A \|\| u \|\), ce qui donne
	\begin{equation}
		\frac{ \| b \| }{ \| A \| }\leq \| u \|.
	\end{equation}
	En mettant les deux ensemble,
	\begin{equation}
		\frac{ \| \Delta u \| }{ \| u \| }\leq \frac{ \| A^{-1} \|\| \Delta b \| }{ \| b \| }\| A \|=\Cond(A)\frac{ \| \Delta b \| }{ \| b \| }.
	\end{equation}
\end{proof}

Le corolaire suivant justifie le nom «conditionnement» au conditionnement d'une matrice.
\begin{corollary}       \label{CORooXKPWooJVHVvh}
	Soit \( A\in \GL(n,\eC)\) fixée et le problème de résoudre \( Au=b\), c'est-à-dire la fonction
	\begin{equation}
		F(u,b)=Au-b.
	\end{equation}
	\begin{enumerate}
		\item
		      Ce problème est stable pour toute valeur de \( b\).
		\item
		      Nous avons une majoration pour le conditionnement relatif\footnote{Si vous doutez de la norme à prendre, lisez la remarque~\ref{REMooAIKIooJEBEqi}} :
		      \begin{equation}        \label{EQooZHQJooTMKYfr}
			      K_{rel}(\eta,b_0)\leq \Cond(A).
		      \end{equation}
	\end{enumerate}
\end{corollary}

\begin{proof}
	\begin{subproof}
		\spitem[Stabilité]
		Vu que \( A\) est inversible, il existe une solution unique à tout système de la forme \( Au=b'\). De plus \( u(b)=A^{-1} b\), donc
		\begin{equation}
			\| u(b)-u(b_0) \|= \| A^{-1}(b-b_0) \|\leq \| A^{-1} \|\| b-b_0 \|,
		\end{equation}
		de telle sorte que la condition~\ref{DEFooYIFAooSJbMkC}\ref{ItemProbStableB} fonctionne avec \( K=\| A^{-1} \|\).
		\spitem[Conditionnement]
		En partant de la définition~\ref{DEFEQooSXDBooYbvGrC}, et en utilisant la majoration de la proposition~\ref{PROPooGIXFooAhJkIs} sous la forme
		\begin{equation}
			\| u(b)-u(b_0) \|\leq \Cond(A)\| u(b_0) \|\frac{ \| \Delta b \| }{ \| b_0 \| },
		\end{equation}
		nous obtenons :
		\begin{subequations}
			\begin{align}
				K_{rel}(b_0,\eta) & =K_{abs}(b_0,\eta)\frac{ \| b_0 \| }{ \| u(b_0) \| }                                                                         \\
				                  & =\sup_{\| b-b_0 \|\leq \eta}\frac{ \| u(b)-u(b_0) \| }{ \| b-b_0 \| }\frac{ \| b_0 \| }{ \| u(b_0) \| }                      \\
				                  & \leq \sup_b\Cond(A)\frac{ \| u(b_0) \| }{ \| b_0 \| }\| \Delta b \|\frac{1}{ \| b-b_0 \| }\frac{ \| b_0 \| }{ \| u(b_0) \| } \\
				                  & =\Cond(A).
			\end{align}
		\end{subequations}
	\end{subproof}
\end{proof}

\begin{remark}      \label{REMooAIKIooJEBEqi}
	La notion de conditionnement relatif dépend aussi de la norme choisie. Dans la formule \eqref{EQooZHQJooTMKYfr} il faut prendre le conditionnement \( \Cond(A)\) pour la norme dans laquelle le \( K_{rel}\) est écrit. Encore une fois, toutes les normes étant équivalentes,  cette majoration est à constante près bonne pour toutes les normes. Si la dimension est très grande, cette constante peut par contre être grande.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Perturbation de la matrice}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Système linéaire : perturbation de la matrice\cite{ooLMMRooUXhOdx}]
	Soient les systèmes linéaires
	\begin{subequations}
		\begin{align}
			Au=b \\
			A'u'=b
		\end{align}
	\end{subequations}
	avec \( A\) et \( A'\) inversibles. Nous notons \( \Delta A=A'-A\). Alors
	\begin{enumerate}
		\item       \label{ITEMooJMTKooSEBavB}
		      \begin{equation}
			      \frac{ \| \Delta u \| }{ \| u' \| }\leq \Cond(A)\frac{ \| \Delta A \| }{ \| A \| }
		      \end{equation}
		\item
		      \begin{equation}
			      \frac{ \| \Delta u \| }{ \| u \| }\leq \Cond(A)\frac{ \| \Delta A \| }{ \| A \| }\big( 1+\alpha(\| \Delta A \|) \big)
		      \end{equation}
		      où \( \lim_{x\to 0} \alpha(x)=0\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	D'abord nous avons
	\begin{subequations}
		\begin{align}
			0 & =A'u'-Au               \\
			  & =(A'-A)u'-Au'-Au       \\
			  & =\Delta Au'+A\Delta u.
		\end{align}
	\end{subequations}
	Par conséquent, \( \Delta u=-A^{-1}(\Delta A)u'\) et
	\begin{equation}        \label{EQooYYITooSSczEj}
		\| \Delta u \|\leq \| A^{-1} \|\| \Delta A \|\| u' \|.
	\end{equation}
	Donc
	\begin{equation}
		\frac{ \| \Delta u \| }{ \| u' \| }\leq   \| A^{-1} \|\| A \|\frac{ \| \Delta A \| }{ \| A \| }   =\Cond(A)\frac{ \| \Delta A \| }{ \| A \| }.
	\end{equation}
	Cela est~\ref{ITEMooJMTKooSEBavB}.

	Pour l'autre inégalité, nous avons \( A'=A+\Delta A\) et donc
	\begin{equation}
		\| A'^{-1} \|=\| (A+\Delta A)^{-1} \|
	\end{equation}
	Nous repartons alors de \eqref{EQooYYITooSSczEj} en changeant le rôle de \( A\) et \( A'\) (et donc aussi de \( u\) et \( u'\)). Ce changement étant, \( \| \Delta u \|\) et \( \| \Delta A \|\) ne changent pas. Nous avons :
	\begin{subequations}
		\begin{align}
			\frac{ \| \Delta u \| }{ \| u \| } & \leq \| A'^{-1} \|\| \Delta A \|                                                             \\
			                                   & =\| (A+\Delta A)^{-1} \|\| \Delta A \|\frac{ \Cond(A) }{ \| A \|\| A^{-1} \| }               \\
			                                   & =\frac{ \| (A+\Delta A)^{-1} \| }{ \| A^{-1} \| }\frac{ \| \Delta A \| }{ \| A \| }\Cond(A).
		\end{align}
	\end{subequations}
	Il reste à voir que
	\begin{equation}
		\lim_{\| \Delta A \|\to 0} \frac{ \| (A+\Delta A)^{-1} \| }{ \| A^{-1} \| }=1,
	\end{equation}
	ou autrement dit que
	\begin{equation}        \label{EQooJURGooFvYiAs}
		\lim_{A\to A'} \frac{ \| A'^{-1} \| }{ \| A^{-1} \| }=1
	\end{equation}
	où la limite est celle dans \( \GL(n,\eC)\). Par définition de la topologie, la norme est continue (quelle qu'elle soit par l'équivalence de norme~\ref{ThoNormesEquiv}). Par le théorème~\ref{ThoCINVBTJ}, l'application \( A\mapsto A^{-1}\) est également continue et commute donc avec la limite. Nous avons donc
	\begin{equation}
		\lim_{A'\to A}\| A'^{-1} \|=\| (\lim_{A'\to A} A')^{-1} \|=\| A^{-1} \|.
	\end{equation}
	Donc la limite du quotient \eqref{EQooJURGooFvYiAs} est bien \( 1\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Système linéaires (généralités)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit un système d'équations linéaires \( Ax=b\) avec \( A\in \eM(n,\eR)\). Le problème est évidemment de savoir si il existe une unique solution \( x\) et de la déterminer. Nous supposons l'existence et l'unicité. C'est-à-dire que les conditions équivalentes\footnote{L'équivalence est la proposition~\ref{PropYQNMooZjlYlA}\ref{ITEMooNZNLooODdXeH}.} sont vérifiées :
\begin{enumerate}
	\item
	      \( A\) est inversible, c'est-à-dire qu'il existe une matrice notée \( A^{-1}\) telle que \( AA^{-1}=A^{-1}A=\mtu\).
	\item
	      \( \det(A)\neq 0\).
\end{enumerate}
Note : si  nous avons un système pas carré du type \( Bx=v\) avec \( B\in \eM(n\times m)\) alors nous pouvons nous ramener à un système carré en écrivant
\begin{equation}
	B^tBx=B^tv.
\end{equation}
Mais attention : bien que \( B^tB\) soit symétrique et semi-définie positive, certaines valeurs propres peuvent être nulles.

\begin{normaltext}
	Deux choses générales en calcul numérique :
	\begin{enumerate}
		\item
		      On ne calcule pas l'inverse d'une matrice.
		\item
		      On ne calcule même pas son déterminant.
	\end{enumerate}
	Par conséquent nous ne faisons pas \( x=A^{-1}v\).

	Il faut garder en tête le fait que dans la pratique, la matrice \( A\) possède des millions de lignes et colonnes, si pas pire. Pour une matrice de taille de l'ordre du million, il y a \( 1000\) milliards d'entrées. Si on compte \( 32\) bits par nombre (précision simple, définition~\ref{DEFooEIOZooYLDVjs}), c'est-à-dire \( 4\) octets, il faut \( 4000\) giga-octets pour enregistrer la matrice. Même pour la mémoire actuellement disponible, ce n'est pas rien. Surtout que souvent, la précision simple n'est pas utilisée, mais la précision double, ce qui donne \( 8000\) giga pour enregistrer la matrice.

	Heureusement, dans la majorité des cas pratiques, les matrices géantes qui apparaissent sont pleines de zéros.

\end{normaltext}

\begin{definition}
	Une matrice est \defe{creuse}{matrice!creuse} si elle possède beaucoup de zéros. Une matrice non creuse est dite \defe{dense}{matrice!dense}.

	Notons que lorsqu'on parle de matrice comprenant beaucoup de «zéros», nous pensons à des éléments très petits, et non de vrai zéros.
\end{definition}
Les matrices creuses ne sont pas mémorisées entièrement, mais plutôt comme un dictionnaire \( (i,j,v)\) qui donne la valeur \( v\) de \( A_{ij}\).

\begin{definition}
	Une matrice est de «grande dimension» si elle ne peut pas être mise en mémoire sur un ordinateur donné. Sur certains ordinateurs, ça commence à \( 5000\) inconnues. Mais sur des plus forts, on peut aller jusqu'au million ou le milliard.
\end{definition}

Si la matrice est de petite dimension, il est possible d'utiliser des méthodes dites «directes». Sinon, il faudra utiliser des méthodes itératives.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Les méthodes directes}
%---------------------------------------------------------------------------------------------------------------------------

Une méthode directe consiste à successivement transformer un système \( A^{(0)}x=b^{(0)}\) en de nouveaux systèmes \( A^{(i)}x=b^{(i)}\) dont la solution est identique jusqu'à obtenir un système \( A^{(n-1)}x=b^{(n-1)}\) qui est à résolution immédiate.

L'avantage d'une méthode directe est qu'elle fournit une réponse exacte, pour autant que les calculs intermédiaires soient bien faits (ce qui n'est pas le cas sur un ordinateur).

Une méthode directe fonctionne en général avec un nombre de pas fixés par la taille du système. Par exemple pour un système \( n\times n\), la méthode de Gauss demande exactement \( n\) pas, et il n'y a pas moyen de faire mieux. Or chaque pas demande de recalculer tous les éléments de la matrice. Encore une fois, si la matrice a une taille de l'ordre du milliard, cela fait \( 10^{18}\) éléments à recalculer un milliard de fois (sans compter les éléments du vecteur \( b\)). Infaisable.


Souvent une méthode directe passe par une factorisation \( A=BC\) avec \( B,C\in \eM(n\times n)\).

Quelques types de matrices dont la résolution est immédiate :
\begin{itemize}
	\item Matrice diagonale.
	\item Matrice orthogonale parce que si \( A\) est orthogonale alors \( Ax=v\) se résout par \( x=A^tv\) qui n'est pas particulièrement lourd à faire numériquement.
	\item Matrice triangulaire.
\end{itemize}

\begin{remark}
	Pour une matrice diagonale, le déterminant et l'inverse sont faciles. Mais également pour la triangulaire. Pour une matrice triangulaire, le déterminant est le produit des éléments diagonaux, et il se fait qu'il y a une algorithme facile pour calculer l'inverse.

	Donc en fait les matrices à résolution immédiates sont des matrices pour lesquelles l'inverse et le déterminant sont facile à calculer.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthodes itératives}
%---------------------------------------------------------------------------------------------------------------------------

Si la matrice est trop grande, il n'est pas possible de faire des manipulations de matrices à chaque itération.

En général, les méthodes itératives ne convergent pas toujours. Mais lorsqu'une méthode converge, c'est une propriété de la matrice, et donc la convergence aura lieu pour tout vecteur de départ \( x_0\). Cela est très différent du cas des équations non linéaires type Newton pour lesquelles la convergence peut fortement dépendre du point de départ.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Système linéaires (méthodes directes)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les matrices que nous sommes autorisés à inverser sont les matrices
\begin{itemize}
	\item orthogonales : l'inverse est la transposée
	\item diagonales : l'inverse est diagonale avec les inverses sur la diagonale
	\item triangulaires : nous en parlons maintenant.
\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Inversion de matrice triangulaire}
%---------------------------------------------------------------------------------------------------------------------------

Si \( T\) est une matrice triangulaire (mettons supérieure pour fixer les idées), il est possible d'en calculer l'inverse sans trop d'efforts. Notons \( B\) la matrice inverse que nous allons construire ligne par ligne. Vu que \( BT=\mtu\) nous avons
\begin{equation}
	\delta_{1j}=\sum_{k=1}^nB_{1k}T_{kj}=\sum_{k=1}^jB_{1k}T_{kj}
\end{equation}
parce que \( T_{kj}=0\) pour \( k>j\). Donc nous pouvons calculer les éléments \( B_{1j} \) un par un parce que chacun ne dépend que des précédents. Le même procédé fonctionne pour les autres lignes :
\begin{equation}
	\delta_{ij}=\sum_{k=1}^jB_{ij}T_{kj}.
\end{equation}
Et tu notes que le calcul peut être parallélisé : le calcul de la ligne numéro \( j\) ne dépend pas du résultat des autres lignes.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transformation gaussienne}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Transformation gaussienne\cite{ooPFJDooUQIwHZ}]		\label{DEFooOJRKooDcrbeU}
	Soit \( x\in \eR^n\) avec \( x_k\neq 0\). La \( k\)\ieme\ \defe{transformation gaussienne}{transformation!gaussienne} pour \( x\) est la matrice
	\begin{equation}
		M_k(x)=\mtu-T_k(x)
	\end{equation}
	où \( T_k(x)\) est la matrice unité à qui on a ajouté le vecteur
	\begin{equation}
		\tau_k(x)=
		\begin{pmatrix}
			0           \\
			\vdots      \\
			0           \\
			x_{k+1}/x_k \\
			\vdots      \\
			x_n/x_k
		\end{pmatrix}
	\end{equation}
	à la \( k\)\ieme\ colonne.
\end{definition}
Autrement dit, la matrice \( M_k(x)\) est la matrice
\begin{equation}        \label{EQooMWXLooBDtsKS}
	M_k(x)=\begin{pmatrix}
		1      &        &              &   &        &   \\
		0      & \ddots &              &   &        &   \\
		\vdots & 0      & 1            &   &        &   \\
		\vdots & \vdots & -x_{k+1}/x_k & 1 &        &   \\
		\vdots & \vdots & \vdots       & 0 & \ddots &   \\
		0      & 0      & -x_n/x_k     & 0 &        & 1
	\end{pmatrix}
\end{equation}
En coordonnées nous avons
\begin{equation}
	\big( M_k(x)\big)_{ij}=\delta_{ij}-\tau_k(x)_i\delta_{kj}.
\end{equation}

\begin{normaltext}
	Les matrices de transformation gaussienne sont des matrices triangulaires de diagonale unitaire (c'est-à-dire avec des \( 1\) sur la diagonale).
\end{normaltext}

\begin{lemma}
	Si \( x\in \eR^n\) alors nous avons
	\begin{equation}
		M_k(x)x=\begin{pmatrix}
			x_1    \\
			\vdots \\
			x_k    \\
			0      \\
			\vdots \\
			0
		\end{pmatrix}.
	\end{equation}
\end{lemma}

\begin{proof}
	Nous avons
	\begin{equation}
		\big( M_k(x)x \big)_i=\sum_lM_k(x)_{il}x_l=\sum_l\big( \delta_{il}-\tau_k(x)_i\delta_{kl} \big)x_l=x_i-\tau_k(x)_ix_k.
	\end{equation}
	Si \( i\leq k\) nous avons \( \tau_k(x)_i=0\) et donc \(  \big( M_k(x)x \big)_i=x_i   \). Si par contre \( i\geq k+1\) alors \( \tau_k(x)_i=\frac{ x_i }{ x_k }\) et alors \( \big( M_k(x)x \big)_i=0\).
\end{proof}

\begin{lemma}       \label{LEMooPFWWooUmMsVH}
	Si \( y\in \eR^n\) vérifie \( y_i=0\) pour \( i>k\) alors \( M_{k+1}(x)y=y\).
\end{lemma}

\begin{proof}
	C'est une simple vérification :
	\begin{equation}
		\big( M_{k+1}(x)y \big)_i=\sum_l\big( \delta_{il}-\tau_{k+1}(x)_i\delta_{k+1,l} \big)y_l=y_i-\tau_{k+1}(x)_iy_{k+1}.
	\end{equation}
	Mais comme \( y_{k+1}=0\) il nous reste automatiquement \( y_i\).
\end{proof}
Le sens de ce lemme est si un vecteur est déjà «gaussiannisé» au niveau \( k\), alors en lui appliquant une transformation gaussienne de niveau plus élevé que \( k\), il ne change pas. Ce fait est important parce qu'il assure que lorsque l'on avance dans le processus de Gauss, chaque étape ne détruit pas les précédentes.

Le lemme suivant nous indique que l'inverse d'une matrice de transformation gaussienne est facile à calculer\footnote{Elle rentre d'ailleurs dans la catégorie des matrices triangulaires dont nous avons déjà discuté l'inverse.}.

\begin{lemma}       \label{LEMooFHZDooZiKdbr}
	L'inverse de la transformation gaussienne
	\begin{equation}
		M_k(x)_{ij}=\delta_{ij}-\tau_k(x)_i\delta_{kj}.
	\end{equation}
	est la matrice donnée par
	\begin{equation}
		M_k(x)^{-1}_{ij}=\delta_{ij}+\tau_k(x)_i\delta_{kj}.
	\end{equation}
	Autrement dit, il suffit de changer le signe de la partie non diagonale.
\end{lemma}

\begin{proof}
	Il s'agit d'une simple vérification, utilisant le produit matriciel explicite, et en remarquant que \( \tau_k(x)_k=0\) pour tout \( k\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de Gauss pour résoudre des systèmes d'équations linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Pour résoudre un système d'équations linéaires, on procède comme suit:
\begin{enumerate}
	\item Écrire le système sous forme matricielle. \[\text{p.ex. } \begin{cases} 2x+3y &= 5 \\ x+2y &= 4 \end{cases} \Leftrightarrow \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right) \]
	\item Se ramener à une matrice avec un maximum de \( 0\) dans la partie de gauche en utilisant les transformations admissibles:
	      \begin{enumerate}
		      \item Remplacer une ligne par elle-même + un multiple d'une autre;
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{L_1  - 2. L_2 \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right) \]
		      \item Remplacer une ligne par un multiple d'elle-même;
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{-L_1  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 2 & 4 \end{array}\right) \]
		      \item Permuter des lignes.
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 0 & -2 \end{array}\right)  \stackrel{L_1  \mapsto L_2' \text{ et } L_2  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \]
	      \end{enumerate}
	\item Retransformer la matrice obtenue en système d'équations.
	      \[\text{p.ex. }  \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \Leftrightarrow \begin{cases} x &= -2 \\ y &= 3 \end{cases}  \]
\end{enumerate}

\textbf{Remarques :}
\begin{itemize}
	\item Si on obtient une ligne de zéros, on peut l'enlever:
	      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \Leftrightarrow  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \end{array}\right) \]
	\item Si on obtient une ligne de zéros suivie d'un nombre non-nul, le système d'équations n'a pas de solution:
	      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 7 \end{array}\right) \Leftrightarrow  \begin{cases} \cdots \\ \cdots \\ 0x + 0y + 0z = 7 \end{cases} \Rightarrow \textbf{Impossible} \]
	\item Si on moins d'équations que d'inconnues, alors il y a une infinité de solutions qui dépendent d'un ou plusieurs paramètres:
	      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 1 & 0 & -2 & 2 \\ 0 & 1 & 3 & 0 \end{array}\right) \Leftrightarrow  \begin{cases} x - 2z = 2 \\ y + 3z = 0 \end{cases} \Leftrightarrow  \begin{cases} x = 2 + 2\lambda \\ y = -3\lambda \\ z = \lambda \end{cases} \]
\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de Gauss sans pivot (décomposition LU)}
%---------------------------------------------------------------------------------------------------------------------------

La méthode de Gauss est souvent aussi appelée méthode «LU» qui va décomposer \( A=LU\) où \( L\) est triangulaire inférieure et \( U\) est triangulaire supérieure. La décomposition est même plus précise que cela : on demande que \( L\) ait seulement des \( 1\) sur la diagonale.

Si \( A\) est une matrice nous notons
\begin{equation}
	\Delta_k(A)= (A_{ij})_{1\leq i,j\leq k}
\end{equation}
la matrice tronquée dont nous ne gardons que le carré \( k\times k\) en haut à gauche.

\begin{lemma}       \label{LEMooXEJFooGiYoyb}
	Soit \( S\) une matrice triangulaire inférieure. Soient également \( A\) et \( B\) telles que \( B=SA\). Alors
	\begin{equation}
		\Delta_k(B)=\Delta_k(S)\Delta_k(A).
	\end{equation}
\end{lemma}

\begin{proof}
	En effet nous avons
	\begin{equation}       \label{EQooHBZZooHtjjsE}
		\Delta_k(B)_{ij}=\sum_{l=1}^nS_{il}A_{lj}.
	\end{equation}
	Dans la somme sur \( l\) il ne reste que les termes \( l\leq i\). Mais dans le calcul des éléments de matrice \( \Delta_k(B)_{ij}\), nous avons évidemment \( i,j\leq k\). Donc \( l\leq i\leq k\). Les seuls éléments de matrice de \( A\) qui sont utilisés dans la somme \eqref{EQooHBZZooHtjjsE} sont les éléments \( A_{lj}\) avec \( l,j\leq k\).

	Nous pouvons donc limiter la somme à \( l=k\) au lieu de \( l=n\) et écrire \( \Delta_k(A)_{lj}\) au lieu de \( A_{lj}\).

	Même chose en ce qui concerne \( S\). À partir du moment où \( l\) est limité à \( k\), les éléments \( S_{il}\) et \( \Delta_k(S)_{il}\) sont les mêmes.
\end{proof}

\begin{theorem}[Décomposition \( LU\)\cite{ooDANFooPSmBfd,MonCerveau}]       \label{THOooUXKJooYaPhiu}
	Soit une matrice \( A\) inversible telle que \( \det(\Delta_k(A))\neq 0\) pour tout \( k\). Alors il existe un unique couple de matrices \( (L,U)\) telles que
	\begin{itemize}
		\item \( U\) soit triangulaire supérieure
		\item \( L\) soit triangulaire inférieure, de diagonale unité
		\item \( A=LU\).
	\end{itemize}
	De plus pour tout \( k\leq n\) nous avons
	\begin{equation}
		\Delta_k(A)=\Delta_k(L)\Delta_k(U).
	\end{equation}
\end{theorem}

\begin{proof}
	Nous allons prouver par récurrence le fait suivant : pour tout \( 1\leq k\leq n-1\) il existe des matrices \( E_i\) (\( i=1,\ldots, k\)) telles que en posant
	\begin{equation}
		A_k=E_{k}\ldots E_1A,
	\end{equation}
	\begin{itemize}
		\item \( E_j\) est une transformation gaussienne\footnote{Transformation gaussienne, définition \ref{DEFooOJRKooDcrbeU}.} pour la \( j\)\ieme\ colonne,
		\item pour tout \( j\leq k\), \( A_{ij}=0\) dès que \( i>j\). Autrement dit la matrice \( A_k\) est triangulaire supérieure jusqu'à (non comprise) la \( (k+1)\)\ieme\ colonne (laquelle est quelconque). Exemple pour fixer les idées : pour une matrice \( A\in \eM(4\times 4) \), la matrice \( A_2\) doit avoir la forme
		      \begin{equation}
			      A_2=E_2E_1A=\begin{pmatrix}
				      * & * & *           & * \\
				      0 & * & *           & * \\
				      0 & 0 & \circledast & * \\
				      0 & 0 & *           & *
			      \end{pmatrix}
		      \end{equation}
		      où les éléments notés \(*\) sont à priori non nuls,
		\item l'élément de matrice \( (A_k)_{ k+1,k+1  }  \) est non nul (celui entouré dans l'exemple).
	\end{itemize}

	La chose un peu triste dans cette démonstration est que l'initialisation va être très ressemblante au pas de récurrence.
	\begin{subproof}
		\spitem[Initialisation : \( k=1\)]

		Vu que \( \Delta_1(A)\) est inversible, l'élément \( A_{11}\) est non nul. Il existe donc une transformation gaussienne \( E_1\) telle que la première colonne de la matrice \( A_1=E_1A\) soit nul sauf la première composante. En particulier \( (A_1)_{21}=0\).

		Par le lemme~\ref{LEMooXEJFooGiYoyb}, nous avons \( \Delta_2(A_1)=\Delta_2(E_1)\Delta_2(A)\), donc\footnote{Le déterminant est multiplicatif, proposition~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy}.}
		\begin{equation}
			\det\big( \Delta_2(A_1) \big)=\det\big( \Delta_2(E_1) \big)\det\big( \Delta_2(A) \big).
		\end{equation}
		Étant donnée la forme \eqref{EQooMWXLooBDtsKS}, toutes les matrices du type \( \Delta_k(E_i)\) ont un déterminant unité, et par hypothèse \( \Delta_2(A)\) est inversible, donc de déterminant non nul. Par conséquent \( \det\big( \Delta_2(A_1) \big)\neq 0\). Mais comme ce déterminant est le produit des éléments diagonaux (c'est une matrice triangulaire), ces derniers ne sont pas nuls. Finalement, \( (A_1)_{22}\neq 0\).

		\spitem[Le pas de récurrence]

		Nous supposons avoir \( A_k=E_k\ldots E_1A\) avec \( (A_k)_{k+1,k+1}\neq 0\). Alors il existe une transformation gaussienne \( E_{k+1}\) de la \( (k+1)\)\ieme\ colonne telle que \( A_{k+1}=E_{k+1}A_k\) soit une matrice dont la \( (k+1)\)\ieme\ colonne n'ait que des zéros en dessous de la \( (k+1)\)\ieme\ position. Vu le lemme~\ref{LEMooPFWWooUmMsVH}, cette transformation n'affecte pas les colonnes précédentes.

		La matrice \( A_{k+1}\) est donc triangulaire supérieure jusqu'à la \( (k+1)\)\ieme\ colonne.

		Vu que le produit \( E_{k+1}\ldots E_1\) est une matrice triangulaire inférieure, le lemme~\ref{LEMooXEJFooGiYoyb} fonctionne encore et nous avons
		\begin{equation}
			\Delta_{k+1}(A_{k+1})=\Delta_{k+1}(E_{k+1}\ldots E_1)\Delta_{k+1}(A).
		\end{equation}
		En ce qui concerne les déterminants, par hypothèse, nous avons \( \det\big( \Delta_{k+1}(A) \big)\neq 0\) ainsi que \( \det\big( \Delta_{k+1}(E_{k+1}\ldots E_1) \big)=1\). Donc
		\begin{equation}
			\det\big( \Delta_{k+1}(A_{k+1}) \big)\neq 0.
		\end{equation}
		Cette matrice étant triangulaire de déterminant non nul, ses éléments diagonaux sont tous non nuls; en particulier \( (A_{k+1})_{k+2,k+2}\neq 0\).
	\end{subproof}

	En poussant la récurrence jusqu'au bout, la matrice
	\begin{equation}
		A_{n-1}=E_{n-1}\ldots E_nA
	\end{equation}
	est triangulaire supérieure.

	Nous posons alors \(   L=(E_{n-1}\ldots E_n)^{-1}  \) et \( U=A_{n-1}\). Cela prouve l'existence parce que
	\begin{equation}
		A=(E_{n-1}\ldots E_1)^{-1}A_{n_1}.
	\end{equation}
	Encore une fois, le lemme~\ref{LEMooXEJFooGiYoyb} nous donne
	\begin{equation}
		\Delta_k(A)=\Delta_k\Big( (E_{n_1}\ldots E_1)^{-1} \Big)\Delta_k(A_{n-1}),
	\end{equation}
	ou encore \( \Delta_k(A)=\Delta_k(L)\Delta_k(U)\).

	En ce qui concerne l'unicité, si \( A=L_1U_1=L_2U_2\) alors \( L_2^{-1}L_1=U_2U_1^{-1} \). Vu qu'à gauche nous avons une matrice triangulaire inférieure et que à droite nous avons une triangulaire inférieure, nous savons que les deux membres représentent une matrice diagonale. Mais à gauche, la diagonale est unitaire. Donc les deux membres représentent la matrice unité.
\end{proof}

\begin{normaltext}
	En pratique, pour résoudre \( Ax=b\), il faut seulement appliquer les transformations gaussiennes à la matrice élargie \( (A|b)\) pour finir sur un système du type
	\begin{equation}
		Ux=b'
	\end{equation}
	qui est immédiatement soluble. Autrement dit, en effectuant les annulations de colonnes, la matrice \( U\) est «gratuite».

	Il n'est pas indispensable de calculer la matrice \( L\) qui, elle, demande à chaque étape de se souvenir de la matrice \( E_i\) utilisée. Si il faut résoudre plusieurs systèmes \( Ax_i=b_i\), nous pouvons encore travailler avec la matrice encore plus élargie \( (A|b_1\ldots b_m)\).

	Si par contre nous ne connaissons pas à l'avance l'ensemble des vecteurs \( b\) avec lesquels il faudra résoudre le système, il est bon de calculer la décomposition \( A=LU \) in extenso, c'est-à-dire de garder une trace des matrices \( L\) et \( U\) séparément. Dans ce cas, résoudre \( Ax=b\) revient à résoudre \( Ly=b\), et ensuite \( Ux=y\). Ce sont deux systèmes de résolution directe parce que les matrices sont triangulaires.
\end{normaltext}

\begin{normaltext}
	Le fait que
	\begin{equation}
		\Delta_k(A)=\Delta_k(L)\Delta_k(U)
	\end{equation}
	nous dit que si après avoir calculé \( L\) et \( U \) nous remarquons que le système est un peu plus petit ou un peu plus grand que prévu, tout le travail n'est pas perdu. En particulier si le système est plus petit que prévu, l'adaptation de \( L\) et \( U\) est immédiate.
\end{normaltext}

Notons que \( U\) et \( L\) sont inversibles, et que \( \det(L)=1\). Donc \( \det(U)=\det(A)\).

\begin{example}
	Pour travailler la méthode de Gauss pour le système \( Ax=b\), nous introduisons la matrice un peu augmentée \( (A|b)\). Nous faisons un exemple. Soit à résoudre
	\begin{equation}
		\begin{pmatrix}
			2  & 1 & 3    \\
			4  & 3 & 10   \\
			-2 & 1 & 7  3
		\end{pmatrix}
		\begin{pmatrix}
			x \\
			y \\
			z
		\end{pmatrix}=\begin{pmatrix}
			11 \\
			28 \\
			3
		\end{pmatrix}.
	\end{equation}
	Nous introduisons la matrice augmentée
	\begin{equation}
		(A|b)^{(0)}=\begin{pmatrix}
			2  & 1 & 3  & 11 \\
			4  & 3 & 10 & 28 \\
			-2 & 1 & 7  & 3
		\end{pmatrix}.
	\end{equation}
	Le premier pas consiste à annuler tous les éléments sous la diagonale de la première colonne. Autrement dit, nous prenons le \( 2\) comme pivot. Nous introduisons les multiplicateurs \( l_{ij}= \frac{ A_{ij} }{ A_{i1} }\). La nouvelle matrice est :
	\begin{equation}
		(A|b)^{(1)}=\begin{pmatrix}
			2 & 1 & 3  & 11 \\
			0 & 1 & 4  & 6  \\
			0 & 2 & 10 & 14
		\end{pmatrix}
	\end{equation}
	où nous avons utilisé les multiplicateurs \( l_{21}=2\), \( l_{31}=-1\).

	Et la matrice suivante est :
	\begin{equation}
		(A|b)^{(2)}=\begin{pmatrix}
			2 & 1 & 3 & 11 \\
			0 & 1 & 4 & 6  \\
			0 & 0 & 2 & 2
		\end{pmatrix}
	\end{equation}
	où nous avons utilisé le multiplicateur \( l_{32}=2\).

	Cela est un système de résolution immédiate :
	\begin{subequations}
		\begin{numcases}{}
			2x+y+3z=11\\
			y+4z=6\\
			2z=2.
		\end{numcases}
	\end{subequations}
	La troisième donne \( z=1\). Ensuite \( y+4=6\), donc \( y=2\). Et la première donne : \( 2x+2+3=11\), c'est-à-dire \( 2x=6\), enfin : \( x=3\).

	Solution : \( (x,y,z)=(3,2,1)\).

	Nous notons surtout que dans \( (A|b)^{(2)}\) nous avons une matrice triangulaire supérieure. Où est la matrice triangulaire inférieure ? En réalité la matrice \( L\) est la matrice des multiplicateurs :
	\begin{equation}
		L=\begin{pmatrix}
			1  & 0 & 0 \\
			2  & 1 & 0 \\
			-1 & 2 & 1
		\end{pmatrix}.
	\end{equation}
\end{example}

Le problème de cette méthode est que faisant ainsi nous risquons d'avoir un zéro sur un des pivots. Par exemple tomber sur
\begin{equation}
	(A|b)=\begin{pmatrix}
		2 & 1 & 3  & 11 \\
		0 & 0 & 4  & 6  \\
		0 & 2 & 10 & 14
	\end{pmatrix}.
\end{equation}
Le zéro sur la deuxième ligne nous ennuie si nous voulons tout faire dans l'ordre. Mais notons qu'en échangeant les deux dernières lignes, tout va bien : le système donné par
\begin{equation}
	(A|b)=\begin{pmatrix}
		2 & 1 & 3  & 11 \\
		0 & 2 & 10 & 14 \\
		0 & 0 & 4  & 6
	\end{pmatrix}
\end{equation}
fonctionne très bien. Et même tellement bien qu'il est de résolution immédiate, dans ce cas.

Un autre problème est que si un des pivots est \( 10^{-14}\), le multiplicateur sera de l'ordre \( 10^{14}\), qui est mal représenté en mémoire. Il est donc bon de prendre les pivots le plus grand possible. Si le pivot est le plus grand nombre en valeur absolue d'une colonne, alors les nombres \( x_{k+i}/x_k\) qui entrent dans la matrice de transformation gaussienne sont des nombres dans \( \mathopen[ -1 , 1 \mathclose]\) qui sont bien représentés en mémoire.

Tout cela nous incite à développer une méthode de Gauss qui permet de tenir une trace des permutations.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice de permutation élémentaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Une \defe{matrice de permutation élémentaire}{matrice!permutation!élémentaire} est une matrice obtenue en permutant deux lignes de la matrice identité. Nous notons \( P_{ij}\) la matrice obtenue en inversant les lignes \( i\) et \( j\) de la matrice identité.
\end{definition}

Les autres opérations sur les lignes sont répertoriées en \ref{DEFooKJNNooOcIJuu}.

\begin{example}
	\begin{equation}
		\begin{pmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{pmatrix}\to
		\begin{pmatrix}
			0 & 1 & 0 \\
			1 & 0 & 0 \\
			0 & 0 & 1
		\end{pmatrix}=P_{12}.
	\end{equation}
\end{example}

\begin{lemma}
	La matrice \( P_{ij}A\) est la matrice \( A\) avec ses lignes \( i\) et \( j\) inversées.
\end{lemma}

\begin{proof}
	Il suffit d'écrire
	\begin{equation}
		(P_{ij}A)_{kl}=\sum_m(P_{ij})_{km}A_{ml}
	\end{equation}
	et de faire trois cas selon que \( k=i\), \( k=j\) ou \( k\) différent de \( i\) et \( j\). Si \( k=i\) alors \( (P_{ij})_{im}=\delta_{mj}\) et si \( k\) est différent de \( i\) et \( j\) alors \( (P_{ij})_{mk}=\delta_{km}\) (troisième cas similaire au premier).
\end{proof}

Et la matrice \( AP_{12}\) est la \( A\) avec ses deux premières \emph{colonnes} échangées.

Avec ces notations, notre matrice \( (A|b)^{0'}\) est
\begin{equation}
	P_{12}(A|b)^{(0)}.
\end{equation}
Puis la matrice \( (A|b)^{(1')}\) est
\begin{equation}
	P_{23}(A|b)^{(1)}.
\end{equation}
Et la matrice \( P\) qui arrive dans \( PA=LU\) est la matrice \(P= P_{23}P_{21}\), qui est une matrice de permutations non élémentaire. Elle vaut :
\begin{equation}
	P=\begin{pmatrix}
		0 & 1 & 0 \\
		0 & 0 & 1 \\
		1 & 0 & 0
	\end{pmatrix}.
\end{equation}

\begin{lemma}[\cite{ooJZHZooLRqIsV}]        \label{LEMooYIYIooYhnaOt}
	Si \( i,j>k\) alors les matrices de permutation élémentaires ont la relation de «commutation» suivante avec les transformations gaussiennes :
	\begin{equation}
		M_k(x)P_{ij}=P_{ij}M_k\big(  P_{ij}(x) \big).
	\end{equation}
\end{lemma}

\begin{proof}
	Il suffit de calculer les éléments de matrice :
	\begin{equation}
		\big( P_{ij}M_k(x) \big)_{st}=(P_{ij})_{st}-\sum_m(P_{ij})_{sm}\tau_k(x)_{m}\delta_{kt},
	\end{equation}
	mais
	\begin{equation}
		\sum_m(P_{ij})_{sm}\tau_k(x)_{m}=\big( P_{ij}\tau_k(x) \big)_s=\tau_k\big( P_{ij}(x) \big)_s
	\end{equation}
	parce que \( i,j>k\) implique que dans \( P_{ij}\tau_k(x)\) nous inversons deux élément non nuls de \( \tau_k(x)\), tout en laissant le \( k\)\ieme\ élément. Le dénominateur ne change pas et il s'agit réellement d'une inversion de ligne. Donc
	\begin{equation}        \label{EQooIBVJooTOWCGT}
		\big( P_{ij}M_k(x) \big)_{st}=(P_{ij})_{st}-\tau_k\big( P_{ij}x \big)_s\delta_{kt}.
	\end{equation}
	De l'autre côté,
	\begin{equation}
		\big( M_k(y)P_{ij} \big)_{st}=(P_{ij})_{st}-\tau_k(y)_s(P_{ij})_{kt}.
	\end{equation}
	Mais comme \( i,j>k\) la \( k\)\ieme\ ligne de \( P_{ij}\) est la même que celle de la matrice unité, donc \( (P_{ij})_{kt}=\delta_{kt}\).
	\begin{equation}
		\big( M_k(y)P_{ij} \big)_{st}=(P_{ij})_{st}-\tau_k(y)_s\delta_{kt}.
	\end{equation}
	Cela correspond bien à \eqref{EQooIBVJooTOWCGT}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Méthode de Gauss avec pivot partiel (décomposition PLU)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{L'idée}
%---------------------------------------------------------------------------------------------------------------------------

À chaque pas, nous faisons une permutation de ligne. Nous permutons à chaque pas la première ligne avec celle qui a le pivot le plus grand (en valeur absolue). Donc :
\begin{equation}
	(A|b)^{(0)}=\begin{pmatrix}
		2  & 1 & 3  & 11 \\
		4  & 3 & 10 & 28 \\
		-2 & 1 & 7  & 3
	\end{pmatrix}
\end{equation}
Nous commençons par déplacer des lignes :
\begin{equation}
	(A|b)^{(0')}=\begin{pmatrix}
		4  & 3 & 10 & 28 \\
		2  & 1 & 3  & 11 \\
		-2 & 1 & 7  & 3
	\end{pmatrix}.
\end{equation}
Les multiplicateurs sont \( l_{21}=1/2\) et \( l_{31}=-1/2\). Le fait est que les multiplicateurs ont toujours le plus grand dénominateur possible et nous avons alors toujours \( 0\leq | l_{ij} |\leq 1\), qui sont des nombres relativement petits, et bien représentés en mémoire.

Nous avons la nouvelle matrice
\begin{equation}
	(A|b)^{(1)}=\begin{pmatrix}
		4 & 3    & 10 & 28 \\
		0 & -1/2 & -2 & -3 \\
		0 & 5/2  & 12 & 17
	\end{pmatrix}.
\end{equation}
Le pivot serait \( -1/2\). Nous cherchons un pivot plus grand en dessous de ce \( -1/2\) (et pas au dessus, sinon on casserait les zéros déjà trouvés).
Nous trouvons le \( 5/2\) qui est plus grand. Nous permutons donc les deux dernières lignes :
\begin{equation}
	(A|b)^{(1')}=\begin{pmatrix}
		4 & 3    & 10 & 28 \\
		0 & 5/2  & 12 & 17 \\
		0 & -1/2 & -2 & -3
	\end{pmatrix}
\end{equation}
où le pivot est maintenant \( l_{32}=-1/5\). La matrice suivante :
\begin{equation}
	(A|b)^{(1)}=\begin{pmatrix}
		4 & 3   & 10  & 28  \\
		0 & 5/2 & 12  & 17  \\
		0 & 0   & 2/5 & 2/5
	\end{pmatrix}
\end{equation}

Dans ce cas, la matrice \( L\) n'est pas aussi simple à construire parce que nous avons permuté des choses. Dans ce cas, la matrice \( L\) est encore de la forme
\begin{equation}
	L=\begin{pmatrix}
		1 & 0 & 0 \\
		. & 1 & 0 \\
		. & . & 1
	\end{pmatrix}.
\end{equation}
Mais vu  que nous avons permuté les lignes \( 2\) et \( 3\) au deuxième pas, nous devons permuter \( l_{21}\) et \( l_{31}\) avant de remplir la matrice \( L\) avec les multiplicateurs :
\begin{equation}
	L=\begin{pmatrix}
		1    & 0    & 0 \\
		-1/2 & 1    & 0 \\
		1/2  & -1/5 & 1
	\end{pmatrix}.
\end{equation}

Notons que ces \( L\) et \( U\) ne sont pas les mêmes que le \( L U\) obtenu sans pivot. Où est l'unicité ? Elle est que en fait maintenant nous n'avons pas \( A=LU\), mais
\begin{equation}
	PA=LU
\end{equation}
où \( P\) est une matrice de permutation.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le théorème}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Méthode de Gauss avec pivot partiel\cite{ooJZHZooLRqIsV}]       \label{PROPooGCPAooDrlrGu}
	Soit une matrice inversible \( A\in\eM(n,\eC)\). Il existe
	\begin{itemize}
		\item une matrice de permutations \( P\)
		\item une matrice triangulaire inférieure de diagonale unitaire \( L\),
		\item une matrice triangulaire supérieure inversible \( U\)
	\end{itemize}
	telles que
	\begin{equation}
		PA=LU.
	\end{equation}
\end{proposition}
Notons que cette proposition ne demande que l'hypothèse d'inversibilité pour \( A\). Il n'y a pas d'hypothèses sur tous les mineurs comme c'était le cas avec Gauss sans pivot.

\begin{proof}
	Nous prouvons par récurrence qu'il existe des matrices \( Q_k\), \( E_1\), \ldots, \( E_k\) et \( A_k\) telles que
	\begin{equation}        \label{EQooOBXWooXxwXSe}
		Q_kA=E_1\ldots E_kA_k
	\end{equation}
	avec
	\begin{enumerate}
		\item
		      \( Q_k\) est une matrice de permutation
		\item
		      \( E_i\) est une transformation gaussienne sur la \( i\)\ieme\ ligne
		\item
		      \( A_k\) est triangulaire supérieure jusqu'à la \( k\)\ieme\ colonne.
	\end{enumerate}

	Sachant que \( \det(Q_k)=\pm 1\), et que \( \det(E_i)=1\), le passage au déterminant dans \eqref{EQooOBXWooXxwXSe} nous donne \( \det(A_k)\neq 0\) et si nous notons \( \Omega_k(A)\) la matrice tronquée de \( A\), ne gardant que les entrées plus grandes que \( k\), nous avons
	\begin{equation}
		\det(A_k)=\prod_{i=1}^k(A_k)_ii\det\big( \Omega_{k+1}(A_k) \big).
	\end{equation}
	Donc  : \( (A_k)_{ii}\neq 0\) pour \( i\leq k\) et \( \det\big( \Omega_{k+1}(A_k) \big)\neq 0\).

	Pour fixer les idées, voici une image de \( k=2\) :
	\begin{equation}
		\input{auto/pictures_tex/Fig_FCUEooTpEPFoeQ.pstricks}
	\end{equation}

	Étant donné que \( \det\big( \Omega_{k+1}(A_k) \big)\neq 0\), parmi les nombres \( (A_k)_{i,k+1}\) (\( i\geq k+1\)), au moins un est non nul et nous posons \( r_{k+1}\) tel que \( | (A_k)_{r_{k+1},k+1} | \) soit maximum parmi ces éléments.

	Le nombre \( r_{k+1}\) est enregistré parce qu'il servira à écrire la matrice \( P\) plus tard. Les matrices \( E_i\) ne sont pas enregistrées, parce que nous verrons qu'elles vont encore changer. Seule la dernière sera enregistrée.

	La composante \( (k+1,k+1)\) de la matrice
	\begin{equation}
		P_{r_{k+1},k+1}A_k
	\end{equation}
	est non nulle et peut donc servir de pivot. Soit \( M_{k+1}\) la transformation gaussienne pour la \( (k+1)\)\ieme\ colonne de la matrice \( P_{r_{k+1},k+1}A_k\). La matrice
	\begin{equation}        \label{EQooCFIFooNDvPFE}
		A_{k+1}=M_{k+1}P_{r_{k+1},k+1}A_k
	\end{equation}
	est alors une matrice triangulaire supérieure jusqu'à la \( (k+1)\)\ieme\ colonne. En posant \( E_{k+1}=M_{k+1}^{-1}\) nous avons
	\begin{equation}
		P_{r_{k+1},k+1}E_{k+1}A_{k+1}=A_k,
	\end{equation}
	et nous nous sentons en droit de récrire l'équation de départ \eqref{EQooOBXWooXxwXSe} :
	\begin{equation}
		Q_kA=E_1\ldots E_kA_k=E_1\ldots E_kP_{r_{k+1},k+1}E_{k+1}A_{k+1}.
	\end{equation}
	Le lemme~\ref{LEMooYIYIooYhnaOt} nous permet de ramener la matrice \( P_{r_{k+1},k+1}\) en première position, quitte à modifier un peu (pas beaucoup) chacune des matrices \( E_i\) (\( i=1,\ldots, k\)). C'est pour cela que nous n'enregistrons pas les matrices \( E_i\). Nous avons donc
	\begin{equation}
		P_{r_{k+1},k+1}Q_kA=E'_1\ldots E'_kE_{k+1}A_{k+1}
	\end{equation}
	où
	\begin{itemize}
		\item Le produit \( P_{r_{k+1},k+1}Q_k\) est encore une matrice de permutation, et mieux : elle vaut
		      \begin{equation}
			      \prod_{i=1}^{k+1}P_{r_i,i}.
		      \end{equation}
		      Cela montre qu'il est suffisant d'enregistrer les nombres \( r_i\) pour reconstituer cette partie.
		\item
		      La matrice \( E'_i\) est une transformation gaussienne pour la \( i\)\ieme\ colonne.
		\item
		      La matrice \( A_{k+1}\) est triangulaire supérieure jusqu'à la \( k+1\)\ieme\ ligne.
	\end{itemize}

	La récurrence est maintenant finie et nous pouvons écrire avec \( k=n\) :
	\begin{equation}        \label{EQooFUEUooHVPFwn}
		Q_nA=E_1\ldots E_nA_n
	\end{equation}
	où le produit \( E_1\ldots E_n\) est triangulaire inférieure et \( A_n\) est triangulaire supérieur.

	Maintenant nous enregistrons la matrice \( U=A_n\), le produit \( L=\prod_{i=1}^nE_n\) et les nombres \( r_i\) qui permettent de retrouver \( P\).
\end{proof}

Note : dans l'équation \eqref{EQooFUEUooHVPFwn} nous avons bien entendu massivement renommé les \( E_i'\) en \( E_i\). En réalité la matrice \( E_1\) vient avec \( n\) primes sur la tête.

Dans les exemples~\ref{ExooNTECooXvTcoh}, \ref{EXooNVRNooJgQmQc} et~\ref{EXooNCRSooTfmPFr}, nous allons résoudre le système
\begin{equation}
	\begin{pmatrix}
		10^{-9} & 1 \\
		1       & 1
	\end{pmatrix}\begin{pmatrix}
		x_1 \\
		x_2
	\end{pmatrix}=\begin{pmatrix}
		1 \\
		2
	\end{pmatrix}
\end{equation}
d'abord de façon exacte, et ensuite en supposant une machine ne tenant que \( 8\) chiffres significatifs en utilisant la méthode de Gauss avec ou sans pivot.

Commençons par voir comment se passe en pratique la décomposition \( PA=LU\) de Gauss avec pivot partiel.

\begin{example}     \label{EXooAZTDooTUXZJb}
	Décomposons la matrice
	\begin{equation}
		A=\begin{pmatrix}
			1 & 2 & 3 \\
			2 & 5 & 0 \\
			3 & 8 & 0
		\end{pmatrix}.
	\end{equation}
	Sur la première colonne, le plus grand nombre est \( 3\). Nous commençons par permuter la première et la troisième ligne en utilisant la matrice de permutation \( P_1=P_{3,1}\) et nous enregistrons \( r_1=3\). Nous avons alors la matrice
	\begin{equation}        \label{EQooJCCLooOZVajj}
		A_0'=\begin{pmatrix}
			3 & 8 & 0 \\
			2 & 5 & 0 \\
			1 & 2 & 3
		\end{pmatrix}.
	\end{equation}
	Pour trouver la matrice \( A_1\) nous suivons l'équation \eqref{EQooCFIFooNDvPFE}. Bien que le résultat net soit des combinaisons de lignes : \( L_2\to L_2-2L_1/3\) et \( L_3\to L_3-L_1/3\) (que nous pourrions savoir dès à présent), il est important de passer par la matrice gaussienne pour obtenir la matrice \( L_1\).

	La matrice de transformation gaussienne pour la première colonne de \eqref{EQooJCCLooOZVajj} est :
	\begin{equation}
		M_1=\begin{pmatrix}
			1    & 0 & 0 \\
			-2/3 & 1 & 0 \\
			-1/3 & 0 & 1
		\end{pmatrix}
	\end{equation}
	et \( L_1=M_1^{-1}\). Le lemme~\ref{LEMooFHZDooZiKdbr} nous dit comment calculer facilement cet inverse :
	\begin{equation}
		L_1=\begin{pmatrix}
			1   & 0 & 0 \\
			2/3 & 1 & 0 \\
			1/3 & 0 & 1
		\end{pmatrix}
	\end{equation}
	En suivant l'équation \eqref{EQooCFIFooNDvPFE} nous posons \( A_1=M_1A'_0\) :
	\begin{equation}        \label{EQooUBRPooRJbCYn}
		A_1=
		\begin{pmatrix}
			1    & 0 & 0 \\
			-2/3 & 1 & 0 \\
			-1/3 & 0 & 1
		\end{pmatrix}
		\begin{pmatrix}
			3 & 8 & 0 \\
			2 & 5 & 0 \\
			1 & 3 & 3
		\end{pmatrix}=
		\begin{pmatrix}
			3 & 8    & 0 \\
			0 & -1/3 & 0 \\
			0 & -2/3 & 3
		\end{pmatrix}
	\end{equation}
	et nous avons
	\begin{equation}
		Q_1A=L_1A_1
	\end{equation}
	où \( L_1\), \( A_1\) et \( r_1=3\) sont enregistrés. La matrice \( Q_1\) peut être retrouvée en sachant \( r_1\) parce que \( P\) est la matrice de permutation \( P_{r_1,1}\).

	Nous travaillons maintenant sur la deuxième colonne de \( A_1\). Le plus grand élément en valeur absolue (sur ou sous la diagonale) est \( -2/3\). Nous posons \( r_2=3\) et
	\begin{equation}
		A'_1=\begin{pmatrix}
			3 & 8    & 0 \\
			0 & -2/3 & 3 \\
			0 & -1/3 & 0
		\end{pmatrix}
	\end{equation}
	et la matrice gaussienne pour la deuxième colonne est
	\begin{equation}
		M_2=\begin{pmatrix}
			1 & 0    & 0 \\
			0 & 1    & 0 \\
			0 & -1/2 & 1
		\end{pmatrix}
	\end{equation}
	Le \( -1/2\) provient du calcul \( -\big( (-1/3)/(-2/3) \big)\). L'inverse de cette matrice est facile :
	\begin{equation}
		L_2=\begin{pmatrix}
			1 & 0   & 0 \\
			0 & 1   & 0 \\
			0 & 1/2 & 1
		\end{pmatrix}
	\end{equation}
	et la matrice suivante à enregistrer est
	\begin{equation}
		A_2=M_2P_{3,2}A_1=M_2A'_1=\begin{pmatrix}
			3 & 8    & 0    \\
			0 & -2/3 & 3    \\
			0 & 0    & -3/2
		\end{pmatrix}.
	\end{equation}
	Notons toutefois que pour calculer cette matrice, seul le dernier élément demande un calcul. La première colonne ne change pas (par construction), la seconde gagne un zéro en dernière ligne (la matrice \( M_2\) sert à ça) et sur la dernière colonne, seule la dernière ligne est sujette à changement.

	Avec la matrice \( A_2\), la trigonalisation supérieure est faite. La décomposition n'est cependant pas terminée. Nous devons encore trouver la partie triangulaire inférieure. Nous en sommes à
	\begin{equation}
		Q_1A=L_1A_1=L_1P_{3,2}L_2A
	\end{equation}
	où \( Q_1\) est la première matrice de permutation.

	Utilisant le lemme~\ref{LEMooYIYIooYhnaOt}, il est facile de permuter \( L_{1}\) avec \( P_{3,2}\) :
	\begin{equation}
		L_1P_{3,2}=P_{3,2}
		\underbrace{
			\begin{pmatrix}
				1   & 0 & 0 \\
				1/3 & 1 & 0 \\
				2/3 & 0 & 1
			\end{pmatrix}}_{L'_1}
	\end{equation}
	Nous avons donc
	\begin{equation}
		P_{3,2}P_{3,1}A=L'_1L_2A
	\end{equation}
	Deux multiplications matricielles plus tard nous terminons :
	\begin{equation}
		PA=LU
	\end{equation}
	avec
	\begin{equation}
		\begin{aligned}[]
			P & =\begin{pmatrix}
				     0 & 0 & 1 \\
				     1 & 0 & 0 \\
				     0 & 1 & 0
			     \end{pmatrix},     &
			L & =\begin{pmatrix}
				     1   & 0   & 0 \\
				     1/3 & 1   & 0 \\
				     2/3 & 1/2 & 1
			     \end{pmatrix},     &
			U & =A_2=\begin{pmatrix}
				         3 & 8    & 0    \\
				         0 & -2/3 & 3    \\
				         0 & 0    & -3/2
			         \end{pmatrix}.
		\end{aligned}
	\end{equation}
\end{example}

Notons que Sage utilise la méthode de Gauss avec pivots :
\lstinputlisting{tex/sage/sageSnip006.sage}
Mais attention : Sage crée une décomposition \( A=PLU\) et non \( PA=LU\). D'où le fait que la matrice de permutations de Sage est l'inverse de celle donnée ici.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{D'un point de vue algorithmique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}
	Je ne suis pas certain de l'optimalité de ce que je raconte ici. Je décris simplement ce que j'ai fait pour écrire mon programme \href{https://github.com/LaurentClaessens/finitediff}{finitediff}.

	Si vous êtes expert en calcul numérique, n'hésitez pas à donner votre avis.
\end{probleme}

Nous décrivons à présent la décomposition \( A=PLU\) (du théorème~\ref{PROPooGCPAooDrlrGu}, avec le \( P \) à droite). En suivant l'exemple~\ref{EXooAZTDooTUXZJb} nous voyons assez bien comment créer les matrices \( U\) et \( P\) au fur et à mesure. La construction de \( L\) est peut-être moins évidente.

Écrivons un exemple très explicite pour
\begin{equation}
	A=\begin{pmatrix}
		2  & 1 & 3  \\
		4  & 3 & 10 \\
		-2 & 1 & 7
	\end{pmatrix}.
\end{equation}
Nous commençons par permuter des lignes pour avoir un grand pivot :
\begin{equation}
	P_{12}A=\begin{pmatrix}
		4  & 3 & 10 \\
		2  & 1 & 3  \\
		-2 & 1 & 7
	\end{pmatrix}.
\end{equation}
Et nous effectuons l'élimination avec la matrice
\begin{equation}
	M_1=\begin{pmatrix}
		1    & 0 & 0 \\
		-1/2 & 1 & 0 \\
		1/2  & 0 & 1
	\end{pmatrix}.
\end{equation}
Cela donne le premier résultat :
\begin{equation}        \label{EQooKTBLooHeOkgk}
	M_1P_{12}A=\begin{pmatrix}
		4 & 3    & 10 \\
		0 & -1/2 & -2 \\
		0 & 5/2  & 12
	\end{pmatrix}
\end{equation}
Nous continuons avec \( P_{23}\) pour avoir un nouveau grand pivot :
\begin{equation}
	P_{23}M_1P_{12}A=\begin{pmatrix}
		4 & 3    & 10 \\
		0 & 5/2  & 12 \\
		0 & -1/2 & -2
	\end{pmatrix}.
\end{equation}
Nous utilisons la matrice
\begin{equation}
	M_2=\begin{pmatrix}
		1 & 0   & 0 \\
		0 & 1   & 0 \\
		0 & 1/5 & 1
	\end{pmatrix}
\end{equation}
et au final :
\begin{equation}
	M_2P_{23}M_1P_{12}A=\begin{pmatrix}
		4 & 3   & 10  \\
		0 & 5/2 & 12  \\
		0 & 0   & 2/5
	\end{pmatrix}=U.
\end{equation}
L'égalité obtenue est
\begin{equation}
	M_2P_{23}M_1P_{12}A=U.
\end{equation}
Pour avoir la décomposition \( PLU\) il faut écrire
\begin{equation}        \label{EQooWKUYooUBQYtc}
	A=P_{12}M_1^{-1}P_{23}M_2^{-1}U,
\end{equation}
et permuter \( P_{23}\) avec \( M_1^{-1}\), ce qui est facile par le lemme~\ref{LEMooYIYIooYhnaOt}.

\begin{remark}
	Nous ne devons permuter la matrice \( M_k\) avec une matrice de permutations qu'à partir de la deuxième étape. En effet l'équation \eqref{EQooKTBLooHeOkgk} revient à
	\begin{equation}
		A=M_1^{-1}P_{12}m_U
	\end{equation}
	qui est dans le bon ordre. Ce n'est qu'à partir de la seconde étape que des matrices de permutations apparaissent à droite des matrices gaussiennes.
\end{remark}

Cependant dans un cas \( 4\times 4\), cette méthode deviendrait fastidieuse parce que nous aurions encore des étapes à faire. En repartant de \eqref{EQooWKUYooUBQYtc}, mais avec \( m_U\) (la matrice pas encore tout à fait triangularisée) au lieu de \( U\), nous aurons, pour un certain \( k>3\) :
\begin{equation}
	M_3P_{3k}M_2P_{23}M_1P_{12}A=U,
\end{equation}
ce qui fait :
\begin{equation}
	A=P_{12}M_1^{-1}P_{23}M_2^{-1}P_{3k}M_3^{-1}U.
\end{equation}
Tous les \( P_{ij}\) peuvent être mis à gauche parce que leurs indices sont toujours strictement supérieurs à ceux des \( M_l\) placés devant eux. Mais c'est fastidieux.

Nous allons donc permuter à chaque étape pour ne retenir que l'important. Si à une certaine étape nous avons
\begin{equation}
	A=P_{1,r_1}\ldots P_{k,r_k}M_1^{-1}\ldots M_k^{-1} m_U
\end{equation}
avec
\begin{equation}
	m_U=P_{k+1,r_{k+1}}M_{k+1}^{-1}U
\end{equation}
alors nous allons directement permuter \( P_{k+1,r_{k+1}}\) avec tous les \( M_i^{-1}\). Si nous notons \( P_k\) la permutation (pas élémentaire) à l'étape \( k\) et \( L_k\) la matrice triangulaire inférieure à de l'étape \( k\),
\begin{equation}
	A=P_kL_km_U=P_kL_kP_{k+1,r_{k+1}}M_{k+1}^{-1}m_U'.
\end{equation}
Nous enregistrons alors \( P_{k+1}=P_kP_{k+1,r_{k+1}}\) et pour \( L_{k+1}\) nous partons de \( L_k\) et nous faisons deux opérations suivantes :
\begin{itemize}
	\item nous permutons, sur ses colonnes non triviales, les indices \( k+1\) et \( r_{k+1}\),
	\item nous multiplions par \( M_{k+1}^{-1}\), ce qui revient à simplement lui ajouter une colonne non triviale.
\end{itemize}
Notons que \( r_{k+1}\geq k+1\), de telle sorte que sur les colonnes non triviales (qui sont jusqu'au numéro \( k\)), la permutation des lignes \( k+1\) et \( r_{k+1}\) ne change pas l'aspect de la matrice : elle reste multi-gaussienne de dernière colonne \( k\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Exemples}
%---------------------------------------------------------------------------------------------------------------------------

Nous nous lançons dans la résolution du système
\begin{equation}
	\begin{pmatrix}
		10^{-9} & 1 \\
		1       & 1
	\end{pmatrix}\begin{pmatrix}
		x_1 \\
		x_2
	\end{pmatrix}=\begin{pmatrix}
		1 \\
		2
	\end{pmatrix}.
\end{equation}

\begin{example}     \label{ExooNTECooXvTcoh}
	Nous commençons de façon exacte, par la méthode de Gauss sans pivot. La première transformation gaussienne est
	\begin{equation}
		E_1=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix}
	\end{equation}
	et nous calculons
	\begin{equation}
		E_1A=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix}\begin{pmatrix}
			10^{-9} & 1 \\
			1       & 1
		\end{pmatrix}=
		\begin{pmatrix}
			10^{-9} & 1         \\
			0       & 1-10^{-9}
		\end{pmatrix}.
	\end{equation}
	Vu que cette dernière est triangulaire supérieure, nous avons fini la méthode de Gauss et \( U=E_1A\). En ce qui concerne la matrice \( L\), elle est donnée par \( L=E_1^{-1}\), c'est-à-dire
	\begin{equation}
		L=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix}^{-1}=
		\begin{pmatrix}
			1    & 0 \\
			10^9 & 1
		\end{pmatrix}.
	\end{equation}
	Au final nous avons la décomposition \( A=LU\) exacte suivante :
	\begin{equation}
		\begin{aligned}[]
			L & =\begin{pmatrix}
				     1    & 0 \\
				     10^9 & 1
			     \end{pmatrix} & U & =\begin{pmatrix}
				                          10^{-9} & 1      \\
				                          0       & 1-10^9
			                          \end{pmatrix}.
		\end{aligned}
	\end{equation}
	Résoudre le système \( Ax=b\) revient à résoudre \( LUx=b\) et donc résoudre successivement les systèmes
	\begin{subequations}
		\begin{numcases}{}
			Ly=b\\
			Ux=y.
		\end{numcases}
	\end{subequations}
	D'abord le système
	\begin{equation}
		\begin{pmatrix}
			1    & 0 \\
			10^9 & 1
		\end{pmatrix}\begin{pmatrix}
			y_1 \\
			y_2
		\end{pmatrix}=\begin{pmatrix}
			1 \\
			2
		\end{pmatrix}
	\end{equation}
	donne \( y_1=1\) et \( y_2=2-10^9\).

	Ensuite nous résolvons
	\begin{equation}
		\begin{pmatrix}
			10^{-9} & 1      \\
			0       & 1-10^9
		\end{pmatrix}\begin{pmatrix}
			x_1 \\
			x_2
		\end{pmatrix}=\begin{pmatrix}
			1 \\
			2-10^9
		\end{pmatrix}.
	\end{equation}
	Cela donne
	\begin{equation}
		\begin{pmatrix}
			x_1 \\
			x_2
		\end{pmatrix}=\begin{pmatrix}
			-\frac{ 10^9 }{ 1-10^9 } \\
			\frac{ 2-10^9 }{ 1-10^9 }
		\end{pmatrix}\simeq\begin{pmatrix}
			1 \\
			1
		\end{pmatrix}.
	\end{equation}

	C'est également le résultat que trouve Sage :
	\lstinputlisting{tex/sage/sageSnip007.sage}
\end{example}

\begin{example}[\cite{ooJZHZooLRqIsV}]     \label{EXooNVRNooJgQmQc}
	Nous recommençons tout le calcul avec une précision limitée à \( 8\) chiffres significatifs, sans pivot.

	Nous avons à nouveau la transformation gaussienne
	\begin{equation}
		E_1=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix},
	\end{equation}
	mais pour calculer \( U\) nous effectuons le produit matriciel
	\begin{equation}
		U=E_1A=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix}\begin{pmatrix}
			10^{-9} & 1 \\
			1       & 1
		\end{pmatrix}=\begin{pmatrix}
			10^{-9} & 1 \\
			0       & *
		\end{pmatrix}.
	\end{equation}
	Nous détaillons à présent le calcul de l'élément noté \( *\). Le calcul de \( 10^9\ominus 1\) donne
	\begin{equation}
		999999999=9.99999999\times 10^{8},
	\end{equation}
	mais la précision étant limitée à \( 8\) chiffres, un arrondi arrive. Étant donné que le premier chiffres supprimé est un \( 9\) nous retombons sur \( 10^9\), et donc notre machine à précision limitée donnera
	\begin{equation}
		U=\begin{pmatrix}
			10^{-9} & 1       \\
			0       & -10^{9}
		\end{pmatrix}.
	\end{equation}
	Ensuite le calcul de \( L=E_1^{-1}\) ne cause pas de problèmes :
	\begin{equation}
		L=\begin{pmatrix}
			1     & 0 \\
			-10^9 & 1
		\end{pmatrix}.
	\end{equation}
	Maintenant il s'agit de résoudre les systèmes \( Ly=b\) et \( Ux=y\). Du système
	\begin{equation}
		\begin{pmatrix}
			1    & 0 \\
			10^9 & 1
		\end{pmatrix}\begin{pmatrix}
			y_1 \\
			y_2
		\end{pmatrix}=\begin{pmatrix}
			1 \\
			2
		\end{pmatrix}
	\end{equation}
	nous tirons tout de suite \( y_1=1\) et ensuite \( 10^9+y_2=2\), c'est-à-dire \( y_2=2-10^9\), qui en précision limitée donne encore \( y_2=-10^9\). À résoudre maintenant :
	\begin{equation}
		\begin{pmatrix}
			10^{-9} & 1     \\
			0       & -10^9
		\end{pmatrix}\begin{pmatrix}
			x_1 \\
			x_2
		\end{pmatrix}=\begin{pmatrix}
			1 \\
			-10^9
		\end{pmatrix}.
	\end{equation}
	Cela donne immédiatement \( x_2=1\) et ensuite
	\begin{equation}
		10^{-9}x_1+1=1,
	\end{equation}
	donc \( x_1=0\). La solution trouvée est
	\begin{equation}        \label{EQooBGWEooVGSVoe}
		\begin{pmatrix}
			x_1 \\
			x_2
		\end{pmatrix}=\begin{pmatrix}
			0 \\
			1
		\end{pmatrix},
	\end{equation}
	qui est complètement faux au niveau de la première variable.
\end{example}

\begin{example}[\cite{ooJZHZooLRqIsV}]     \label{EXooNCRSooTfmPFr}
	Nous résolvons encore le même système en précision limitée, mais en utilisant cette fois la méthode de Gauss avec pivot partiel.

	Le plus grand élément de la première colonne est \( 1\); nous utilisons donc la permutation \( P_{1,2}\) :
	\begin{equation}
		P_{1,2}A=\begin{pmatrix}
			1       & 1 \\
			10^{-9} & 1
		\end{pmatrix}.
	\end{equation}
	La matrice de transformation gaussienne pour la première colonne de cette matrice est
	\begin{equation}
		M_1=\begin{pmatrix}
			1        & 0 \\
			-10^{-9} & 1
		\end{pmatrix}
	\end{equation}
	et nous posons
	\begin{equation}
		A_1=M_1P_{1,2}A=
		\begin{pmatrix}
			1        & 0 \\
			-10^{-9} & 1
		\end{pmatrix}
		\begin{pmatrix}
			1       & 1 \\
			10^{-9} & 1
		\end{pmatrix}
		=\begin{pmatrix}
			1 & 1          \\
			0 & -10^{-9}+1
		\end{pmatrix}=
		\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}=U
	\end{equation}
	où un arrondi a eu lieu pour \( -10^{-9}+1=1\). En inversant \( M_1\) nous avons
	\begin{equation}
		L_1=M_1^{-1}=\begin{pmatrix}
			1       & 0 \\
			10^{-9} & 1
		\end{pmatrix}.
	\end{equation}
	La décomposition est
	\begin{equation}
		A=\underbrace{\begin{pmatrix}
				0 & 1 \\
				1 & 0
			\end{pmatrix}
		}_{P}
		\underbrace{
			\begin{pmatrix}
				1       & 0 \\
				10^{-9} & 1
			\end{pmatrix}}_{L}
		\underbrace{
			\begin{pmatrix}
				1 & 1 \\
				0 & 1
			\end{pmatrix}}_{U}
	\end{equation}
	Le moment de résoudre est venu. Vu que \( PLUx=b\) nous devons résoudre les systèmes
	\begin{subequations}
		\begin{numcases}{}
			Pz=b\\
			Ly=z\\
			Ux=y.
		\end{numcases}
	\end{subequations}
	Pour \( z\) c'est facile :
	\begin{equation}
		z=\begin{pmatrix}
			2 \\
			1
		\end{pmatrix}.
	\end{equation}
	Pour \( y\) il y a un arrondi :
	\begin{equation}
		\begin{pmatrix}
			1       & 0 \\
			10^{-9} & 1
		\end{pmatrix}\begin{pmatrix}
			y_1 \\
			y_2
		\end{pmatrix}=\begin{pmatrix}
			2 \\
			1
		\end{pmatrix}.
	\end{equation}
	Tout de suite : \( y_1=2\) et ensuite \( 2\times 10^{-9}+y_2=1\), ce qui donne \( y_2=1\ominus 2\times 10^{-9}=1\). Donc
	\begin{equation}
		y=\begin{pmatrix}
			2 \\
			1
		\end{pmatrix}.
	\end{equation}
	Et enfin pour \( x\) c'est le système
	\begin{equation}
		\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}\begin{pmatrix}
			x_1 \\
			x_2
		\end{pmatrix}=\begin{pmatrix}
			2 \\
			1
		\end{pmatrix}.
	\end{equation}
	Nous avons \( x_2=1\) et ensuite \( x_1+1=2\) c'est-à-dire \( x_1=1\). Au final la solution trouvée est
	\begin{equation}
		x=\begin{pmatrix}
			1 \\
			1
		\end{pmatrix}.
	\end{equation}
	Cette solution est considérablement meilleure que \eqref{EQooBGWEooVGSVoe}.
\end{example}

\begin{normaltext}
	L'utilisation du pivot non seulement assure le fait que la trigonalisation va bien se passer (on évite les zéros en pivot), mais aussi et surtout, en choisissant de prendre le plus grand pivot possible, nous obtenons une meilleur stabilité numérique.
\end{normaltext}
