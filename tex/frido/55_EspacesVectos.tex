% This is part of Mes notes de mathématique
% Copyright (c) 2008-2017
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Parties libres, génératrices, bases et dimension}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Partie libre]
    Si \( E\) est un espace vectoriel, une partie \( A\) de \( E\) est \defe{libre}{libre!partie} si pour tout choix d'un nombre fini d'éléments \( \{ u_i \}_{i=1,\ldots, n}\), l'égalité
    \begin{equation}
        a_1 u_1+\cdots +a_nu_n=0
    \end{equation}
    implique \( a_i=0\) pour tout \( i\) (ici les \( a_i\) sont dans le corps de base).

    Une partie infinie est libre si toute ses parties finies le sont.
\end{definition}

\begin{remark}
    Notons que le vecteur nul n'est dans aucune partie libre, ne fut-ce que parce que \( a0=0\) n'implique pas \( a=0\).
\end{remark}

Si \( A\) est une partie de l'espace vectoriel \( E\) nous notons \( \Span(A)\)\nomenclature[A]{$\Span(A)$}{l'ensemble des combinaisons linéaires finies d'éléments de \( A\)} l'ensemble des combinaisons linéaires finies d'éléments de \( A\). Les coefficients de ces combinaisons linéaires sont dans le corps de base \( \eK\).

\begin{definition}[Partie génératrice]
    Une partie $B$ d'un espace vectoriel \( E\) est \defe{génératrice}{partie!génératrice} si \( \Span(B)=E\).
\end{definition}

\begin{remark}
    Ces définitions demandent des commentaires en dimension infinie\footnote{Nous n'avons pas encore définit le concept de dimension, mais nous nous adressons au lecteur trop pressé.}.

    \begin{enumerate}
        \item
    Tout élément peut être écrit comme combinaison linéaire finie d'une partie génératrice. Cela ne signifie pas que nous pouvons extraire une partie finie qui convient pour tous les éléments à la fois. Lorsque l'espace est de dimension infinie, ceci est particulièrement important.
\item
    La définition séparée de liberté dans le cas des parties infinies a son importance lorsqu'on parle d'espaces vectoriels de dimension infinies (en dimension finie, aucune partie infinie n'est libre) parce que cela fera une différence entre une base algébrique et une base hilbertienne par exemple.
    \end{enumerate}
\end{remark}

\begin{definition}[Base]
    Une \defe{base}{base} de l'espace vectoriel \( E\) est une partie à la fois génératrice et libre.
\end{definition}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooEIQIooXfWDDV}
    Tout élément non nul d'un espace vectoriel se décompose de façon unique en combinaison linéaire finie d'éléments d'une base.
\end{proposition}

\begin{proof}
    Soit un espace vectoriel \( E\) et une base \( \{ e_i \}_{i\in I}\) où \( I\) est un ensemble a priori quelconque. Soit \( v\in E\). Vu que \( E=\Span\{ e_i \}_{i\in I}\), il existe une partie finie \( J\) de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) dans \( \eK\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j.
    \end{equation}
    Cela donne l'existence.

    En ce qui concerne l'unicité, soient \( J \) et \( K\) des parties finies de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) et \( \{ w_{k} \}_{k\in K}\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j=\sum_{k\in K}v_{k}e_{k}.
    \end{equation}
    Nous posons \( L=J\cup K\) et, pour \( l\in L\),
    \begin{equation}
        \alpha_l=\begin{cases}
            v_l    &   \text{si } l\in J\setminus K\\
            w_l    &    \text{si } l\in K\setminus J\\
            v_l-w_l    &    \text{si } l\in K\cap J.
        \end{cases}
    \end{equation}
    Nous avons alors
    \begin{equation}
        \sum_{l\in L}\alpha_le_l=0,
    \end{equation}
    ce qui implique que \( \alpha_l=0\) pour tout \( l\in L\) parce que la partie \( \{ e_i \}_{i\in I}\) est libre et que \( L\) est finie.

    L'unicité de la décomposition de \( v\) signifie que
    \begin{equation}
        \{ j\in J \tq v_j\neq 0 \}=\{ k\in K\tq w_k\neq 0 \}
    \end{equation}
    et que pour \( l\) dans cet ensemble, \( v_l=w_l\).

    Soit \( j\in J\); il y a deux possibilités : \( j\in J\setminus K\) et \( j\in J\cap K\). Dans le premier cas nous avons déjà vu que \( \alpha_j=v_j=0\). Dans le second cas, \( \alpha_j=v_j-w_j=0\), c'est à dire \( v_j=w_j\).

    Donc \( j\in J\) vérifiant \( v_j\neq 0\) implique \( j\in J\cap K\) et l'égalité des coefficients. Idem avec \( k\in K\) tel que \( w_k\neq 0\) implique \( k\in J\cap K\).
\end{proof}

\begin{definition}
    Un espace vectoriel est \defe{de type fini}{type!fini!espace vectoriel} s'il contient une partie génératrice finie. 
\end{definition}
Nous verrons dans les résultats qui suivent que cette définition est en réalité inutile parce qu'une espace vectoriel sera de type fini si et seulement s'il est de dimension finie.

\begin{lemma}       \label{LemytHnlD}
    Si \( E\) a une famille génératrice de cardinal \( n\), alors toute famille de \( n+1\) éléments est liée.
\end{lemma}

\begin{proof}
    Nous procédons par récurrence sur \( n\) -- qui n'est pas exactement la dimension d'un espace vectoriel fixé. Pour \( n=1\), nous avons \( E=\langle e\rangle\) et donc si \( v_1,v_2\in E\) nous avons \( v_1=\lambda_1 e\), \( v_2=\lambda_2e\) et donc \( \lambda_2v_1-\lambda_1v1=0\). Cela prouve le résultat dans le cas de la dimension \( 1\).

    Supposons maintenant que le résultat soit vrai pour \( k<n\), c'est à dire que pour tout espace vectoriel contenant une partie génératrice de cardinal \( k<n\), les parties de \( k+1\) éléments sont liées. Soit maintenant un espace vectoriel muni d'une partie génératrice \( G=\{ e_1,\ldots, e_n \}\) de \( n\) éléments, et montrons que toute partie \( V=\{ v_1,\ldots, v_{n+1} \}\) contenant \( n+1\) éléments est liée. Dans nos notations nous supposons que les \( e_i\) sont des vecteurs distincts et les \( v_i\) également. Nous les supposons également tous non nuls. Étant donné que \( \{ e_i \}\) est génératrice nous pouvons définir les nombres \( \lambda_{ij}\) par
    \begin{equation}
        v_i=\sum_{k=1}^n\lambda_{ij}e_j
    \end{equation}
    Vu que
    \begin{equation}
        v_{n+1}=\sum_{k=1}^n\lambda_{n+1,k}e_k\neq 0,
    \end{equation}
    quitte à changer la numérotation des \( e_i\) nous pouvons supposer que \( \lambda_{n+1,n}\neq 0\). Nous considérons les vecteurs
    \begin{equation}
        w_i=\lambda_{n+1,n}v_i-\lambda_{i,n}v_{n+1}.
    \end{equation}
    En calculant un peu,
    \begin{subequations}
        \begin{align}
            w_i&=\lambda_{n+1,n}\sum_k\lambda_{i,k}e_k-\lambda_{i,n}\sum_k\lambda_{n+1,k}e_k\\
            &=\sum_{k=1}^{n-1}\big( \lambda_{n+1,n}\lambda_{i,k}-\lambda_{i,n}\lambda_{n+1,} \big)e_k
        \end{align}
    \end{subequations}
    parce que les termes en \( e_n\) se sont simplifiés. Donc la famille \( \{ w_1,\ldots, w_n \}\) est une famille de \( n\) vecteurs dans l'espace vectoriel \( \Span\{ e_1,\ldots, e_{n-1} \}\); elle est donc liée par l'hypothèse de récurrence. Il existe donc des nombres \( \alpha_1,\ldots, \alpha_n\in \eK\) non tous nuls tels que
    \begin{equation}        \label{EqOQGGoU}
        0=\sum_{i=1}^n\alpha_iw_i=\sum_{i=1}^n\alpha_i\lambda_{n+1,n}v_i-\left( \sum_{i=1}^n\alpha_i\lambda_{i,n} \right)v_{n+1}.
    \end{equation}
    Vu que \( \lambda_{n+1,n}\neq 0\) et que parmi les \( \alpha_i\) au moins un est non nul, nous avons au moins un des produits \( \alpha_i\lambda_{n+1,n}\) qui est non nul. Par conséquent \eqref{EqOQGGoU} est une combinaison linéaire nulle non triviale des vecteurs de \( \{ v_1,\ldots, v_{n+1} \}\). Cette partie est donc liée.
\end{proof}

\begin{lemma}   \label{LemkUfzHl}
    Soit \( L\) une partie libre et \( G\) une partie génératrice. Soit \( B\) une partie maximale parmi les parties libres \( L'\) telles que \( L\subset L'\subset G\). Alors \( B\) est une base.
\end{lemma}
Qu'entend-on par «maximale» ? La partie \( B\) doit être libre, contenir \( L\), être contenue dans \( G\) et de plus avoir la propriété que \( \forall x\in G\setminus B\), la partie \( B\cup\{ x \}\) est liée.

\begin{proof}
    D'abord si \( G\) est une base, alors toutes les parties de \( G\) sont libres et le maximum est \( B=G\). Dans ce cas le résultat est évident. Nous supposons donc que \( G\) est liée.

    La partie \( B=\{ b_1,\ldots, b_l \}\) est libre parce qu'on l'a prise parmi les libres. Montrons que \( B\) est génératrice. Soit \( x\in G\setminus B\); par hypothèse de maximalité, \( B\cup\{ x \}\) est liée, c'est à dire qu'il existe des nombres \( \lambda_i\), \( \lambda_x\) non tous nuls tels que
    \begin{equation}    \label{EqxfkevM}
        \sum_{i=1}^l\lambda_ib_i+\lambda_xx=0.
    \end{equation}
    Si \( \lambda_x=0\) alors un de \( \lambda_i\) doit être non nul et l'équation \eqref{EqxfkevM} devient une combinaison linéaire nulle non triviale des \( b_i\), ce qui est impossible parce que \( B\) est libre. Donc \( \lambda_x\neq 0\) et
    \begin{equation}
        x=\frac{1}{ \lambda_x }\sum_{i=1}^l\lambda_ib_i.
    \end{equation}
    Donc tous les éléments de \( G\setminus B\) sont des combinaisons linéaires des éléments de \( B\), et par conséquent, \( G\) étant génératrice, tous les éléments de \( E\) sont combinaisons linéaires d'éléments de \( B\). 
\end{proof}

\begin{theorem}[Théorème de la base incomplète] \label{ThonmnWKs}
    Soit \( E\) un espace vectoriel de type fini sur le corps \( \eK\).
    \begin{enumerate}
        \item   \label{ItemBazxTZ}
            Si \( L\) est une partie libre et si \( G\) est une partie génératrice contenant \( L\), alors il existe une base \( B\) telle que \( L\subset B\subset G\).
        \item       \label{ITEMooFVJXooGzzpOu}
            Toute partie libre peut être étendue en une base.
        \item
            Toutes les bases sont finies et ont même cardinal.
    \end{enumerate}
\end{theorem}
\index{théorème!base incomplète}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
    Vu que \( E\) est de type fini, il admet une partie génératrice \( G\) de cardinal fini \( n\). Donc une partie libre est de cardinal au plus \( n\) par le lemme \ref{LemytHnlD}. Soit \( L\), une partie libre contenue dans \( G\) (ça existe : par exemple \( L=\emptyset\)). La partie \( B\) maximalement libre contenue dans \( G\) et contenant \( L\) est une base par le lemme \ref{LemkUfzHl}.
\item
Notons que puisque \( E\) lui-même est générateur, le point \ref{ItemBazxTZ} implique que toute partie libre peut être étendue en une base.
\item
    Soient \( B\) et \( B'\), deux bases. En particulier \( B\) est génératrice et \( B'\) est libre, donc le lemme \ref{LemytHnlD} indique que \( \Card(B')\leq \Card(B)\). Par symétrie on a l'inégalité inverse. Donc \( \Card(B)=\Card(B')\).
    \end{enumerate}
\end{proof}

\begin{remark}      \label{REMooYGJEooEcZQKa}
    Le théorème de la base incomplete \ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu} est ce qui permet de construire une base d'une espace vectoriel en « commençant par» une base d'un sous-espace. En effet si \( H\) est un sous-espace de \( E\) alors une base de \( H\) est une partie libre de \( E\) et donc peut être étendue en une base de \( E\).
\end{remark}

\begin{definition}
    La \defe{dimension}{dimension} d'un espace vectoriel de type fini est la cardinal d'une de ses bases.
\end{definition}
\index{dimension!définition}

\begin{example}
    Il existe une infinité de bases de $\eR^m$. On peut démontrer que le cardinal de toute base de $\eR^m$ est $m$, c'est à dire que toute base de $\eR^m$ possède exactement $m$ éléments.

    La base de $\eR^m$ qu'on dit \defe{canonique}{canonique!base}\index{base!canonique de $\eR^m$} (c.à.d. celle qu'on utilise tout le temps) est $\mathcal{B}=\{e_1,\ldots, e_m\}$, où le vecteur $e_j$ est 
    \begin{equation}\nonumber
      e_j=
    \begin{array}{cc}
      \begin{pmatrix}
        0\\\vdots\\0\\1\\ 0\\\vdots\\0
      \end{pmatrix} & 
      \begin{matrix}
        \quad\\\quad\\\leftarrow\textrm{j-ème} \quad\\\quad\\\quad\\
      \end{matrix}
    \end{array}.
    \end{equation}
    La composante numéro $j$ de $e_i$ est $1$ si $i=j$ et $0$ si $i\neq j$. Cela s'écrit $(e_i)_j=\delta_{ij}$ où $\delta$ est le \defe{symbole de Kronecker}{Kronecker} défini par
    \begin{equation}
        \delta_{ij}=\begin{cases}
            1	&	\text{si }i=j\\
            0	&	 \text{si }i\neq j
        \end{cases}
    \end{equation}
    Les éléments de la base canonique de $\eR^m$ peuvent donc être écrits $e_i=\sum_{k=1}^m\delta_{ik}e_k$.
\end{example}

Le théorème suivant est essentiellement une reformulation du théorème \ref{ThonmnWKs}.
\begin{theorem} \label{ThoBaseIncompjblieG}     \label{ThoMGQZooIgrXjy}
    Soit \( E\) un espace vectoriel de dimension finie et \( \{ e_i \}_{i\in I}\) une partie génératrice de \( E\).

    \begin{enumerate}
        \item
            Il existe \( J\subset I\) tel que \( \{ e_i \}_{i\in J}\) est une base. Autrement dit : de toute partie génératrice nous pouvons extraire une base.
        \item
            Soit \( \{ f_1,\ldots, f_l \}\) une partie libre. Alors nous pouvons la compléter en utilisant des éléments \( e_i\). C'est à dire qu'il existe \( J\subset I\) tel que \( \{ f_k \}\cup\{ e_i \}_{i\in J}\) soit une base.
        \item       \label{ItemHIVAooPnTlsBi}
            Si \( E\) est un espace vectoriel de dimension \( n\), alors toute partie libre contenant \( n\) éléments est une base.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous ne prouvons que le troisième point. Une partie libre contenant \( n\) éléments peut être étendue en une base; si ladite extension est non triviale (c'est à dire qu'on ajoute vraiment au moins un élément) une telle base contiendra une partie de \( n+1\) éléments qui serait liée par le lemme \ref{LemytHnlD}.
\end{proof}

Soit \( F\) un sous-espace vectoriel de l'espace vectoriel \( E\). La \defe{codimension}{codimension} de \( F\) dans \( E\) est
\begin{equation}
    \codim_E(F)=\dim(E/F).
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooULVAooXJuRmr}
    Soient des espaces vectoriels \( E \) et \( F\) sur le corps \( \eK\). Une application \( T\colon E\to F\) est dite \defe{linéaire}{linéaire!application} si
    \begin{itemize}
        \item $T(x+y)=T(x)+T(y)$ pour tout $x$ et $y$ dans \( E\),
        \item $T(\lambda x)=\lambda T(x)$ pour tout $\lambda$ dans $\eK$ et \( x\) dans \( E\).
    \end{itemize}
\end{definition}
Si vous avez bien suivi, les égalités dans la définition \ref{DEFooULVAooXJuRmr} sont des égalités dans \( F\).


\begin{example}
Pour tout $b$ dans $\eR$ la fonction $T_b(x)= bx$ est une application linéaire de $\eR$ dans $\eR$. En effet,
\begin{itemize}
\item  $T_b(x+y)= b(x+y)= bx + by = T_b(x)+T_b(y)$,
\item $T_b(ax)=b(ax)= abx = a T_b(x)$.
\end{itemize}
De la même façon on peut montrer que la fonction $T_{\lambda}$ définie par $T_{\lambda}(x)=bx$ est un application linéaire de $\eR^m$ dans $\eR^m$ pour tout $\lambda$ dans $\eR$ et $m$ dans $\eN$.
\end{example}

\begin{definition}[Quelque ensembles d'applications linéaires]      \label{DEFooOAOGooKuJSup}
    Soient \( E\) et \( F\) des espaces vectoriels. 
    \begin{itemize}
        \item
        L'ensemble des applications linéaires de \( E\) vers \( F\) est noté $\aL(E,F)$\nomenclature{$\aL(E,F)$}{Ensemble des applications linéaires de $E$ dans $F$}.
        \item Une application linéaire \( E\to E\) est un \defe{endomorphisme}{endomorphisme} de \( E\). L'ensemble des endomorphisme de \( E\) est noté \( \End(E)\)\nomenclature[B]{$\End(E)$}{les endomorphismes de \( E\)}.
        \item Un endomorphisme bijectif est un \defe{automorphisme}{automorphisme!d'espace vectoriel}. L'ensemble des automorphismes de \( E\) est noté \( \Aut(E)\)\nomenclature[B]{$\Aut(E)$}{automorphisme de l'espace vectoriel \( E\)}.
        \item
            Une application linéaire bijective \( E\to F\) est un \defe{isomorphisme}{isomorphisme!espaces vectoriels} d'espace vectoriel. L'ensemble des isomorphismes \( E\to F\) est noté \( \GL(E,F)\).
    \end{itemize}
\end{definition}

\begin{remark}
    Les ensembles définis en \ref{DEFooOAOGooKuJSup} concernent la structure d'espace vectoriel seulement. Lorsque nous verrons la notion d'espace vectoriel normé,nous demanderons de plus la continuité, laquelle n'est pas automatique en dimension infinie. Voir aussi les définitions \ref{DEFooTLQUooJvknvi}.
\end{remark}

\begin{definition}  \label{DefDQRooVGbzSm}
    Si \( V\) et \( W\) sont des espaces vectoriels nous munissons \( \aL(V,W)\) d'une structure d'espace vectoriel en définissant la somme et le produit par un scalaire de la façon suivante. Si $T$ et $U$ sont des élément de $\aL(V,W)$ et si $\lambda$ est un réel, nous définissons les éléments $T+U$ et $\lambda T$ par
    \begin{enumerate}
        \item
            $(T+U)(x)=T(x)+U(x)$;
        \item
            $(\lambda T)(x)=\lambda T(x)$
    \end{enumerate}
    pour tout \( x\in V\).
\end{definition}

\begin{normaltext}
    Pour bien faire, il faudrait vérifier que la définition \ref{DefDQRooVGbzSm} produit effectivement une structure d'espace vectoriel sur \( \aL(V,W)\).
\end{normaltext}

\begin{proposition}
    Si \( E\) et \( F\) sont des espaces vectoriels de dimension \( n\) et si \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_i \}_{i=1,\ldots, n}\) sont des bases respectivement de \( F\) et \( F\), alors il existe une unique application linéaire \( A\colon E\to F\) telle que \( E(e_i)=f_i\) pour tout \( i\).
\end{proposition}

\begin{proof}
    En deux parties.\begin{subproof}
        \item[Existence]
            Soit \( v\in E\). Vu que \( \{ e_i \}\) est une base, il se décompose de façon unique en \( v=\sum_iv_ie_i\). Alors définir
            \begin{equation}
                A(v)=\sum_iv_if_i
            \end{equation}
            est une bonne définition et satisfait aux exigences.
        \item[Unicité]
            Soient \( A\) et \( B\) satisfaisant aux exigences. Alors pour tout \( i\) nous avons \( A(e_i)=B(e_i)\). Si \( v\in E\) s'écrit de la forme \( v=\sum_iv_ie_i\) alors la linéarité impose \( A(v)=\sum_iv_iA(e_i)=\sum_iv_iB(e_i)=B(v)\). Donc \( A=B\).
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rang}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}\label{DefALUAooSPcmyK}
    Si  $T\colon V\to W$ est une application linéaire, son \defe{rang}{rang} est la dimension de son image. 
\end{definition}

Le théorème suivant est valable également en dimension infinie; ce sera une des rares incursions en dimension infinie de ce chapitre.
\begin{theorem}[Théorème du rang]       \label{ThoGkkffA}
    Soient \( E\) et \( F\) deux espaces vectoriels (de dimensions finies ou non) et soit \( f\colon E\to F\) une application linéaire. Alors le rang\footnote{Définition \ref{DefALUAooSPcmyK}.} de \( f\) est égal à la codimension du noyau, c'est à dire
       \begin{equation}
           \rang(f)+\dim\ker f=\dim E.
       \end{equation}

       Dans le cas de dimension infinie afin d'éviter les problèmes d'arithmétique avec l'infini nous énonçons le théorème en disant que si \( (u_s)_{s\in S}\) est une base de \( \ker(f)\) et si \( \big( f(v_t) \big)_{t\in T}\) est une base de \( \Image(f)\) alors  \( (u_s)_{s\in s}\cup (v_t)_{t\in T}\) est une base de \( E\).
\end{theorem}
\index{théorème!du rang}

\begin{proof}
    Nous devons montrer que 
    \begin{equation}
          (u_s)_{s\in S}\cup (v_t)_{t\in T}
    \end{equation}
    est libre et générateur.

    Soit \( x\in E\). Nous définissons les nombres \( x_t\) par la décomposition de \( f(x)\) dans la base \( \big( f(v_t) \big)\) :
    \begin{equation}
        f(x)=\sum_{t\in T}x_tf(v_t).
    \end{equation}
    Ensuite le vecteur \( x=\sum_tx_tv_t\) est dans le noyau de \( f\), par conséquent nous le décomposons dans la base \( (u_s)\) :
    \begin{equation}
        x-\sum_tx_tv_t=\sum_s\in S x_su_s.
    \end{equation}
    Par conséquent
    \begin{equation}
        x=\sum_sx_su_s+\sum_tx_tv_t.
    \end{equation}
    
    En ce qui concerne la liberté nous écrivons
    \begin{equation}
        \sum_tx_tv_t+\sum_sx_su_s=0.
    \end{equation}
    En appliquant \( f\) nous trouvons que 
    \begin{equation}
        \sum_tx_tf(v_t)=0
    \end{equation}
    et donc que les \( x_t\) doivent être nuls. Nous restons avec \( \sum_sx_su_s=0\) qui à son tour implique que \( x_s=0\).
\end{proof}
Un exemple d'utilisation de ce théorème en dimension infinie sera donné dans le cadre du théorème de Fréchet-Riesz, théorème \ref{ThoQgTovL}.

\begin{corollary}       \label{CORooCCXHooALmxKk}
    Si \( E\) est un espace vectoriel de dimension finie, alors un endomorphisme de \( E\) est bijectif si et seulement si il est injectif si et seulement si il est surjectif.
\end{corollary}

\begin{proof}
    Si un endomorphisme \( f\colon E\to E\) est surjectif, alors \( \rang(f)=\dim(E)\), ce qui donne, par le théorème du rang \ref{ThoGkkffA}, \( \dim\big( \ker(f) \big)=0\), c'est à dire que \( f\) est injectif.

    De la même façon, si \( f\) est injective, alors \( \dim\big( \ker(f) \big)=0\), ce qui donne \( \rang(f)=\dim(E)\) ou encore que \( f\) est surjective.
\end{proof}

Une conséquence du théorème du rang est que les endomorphismes ont un inverse à gauche et à droite égaux (lorsqu'ils existent).
\begin{corollary}
    Soit un endomorphisme \( f\) d'un espace vectoriel de dimension finie. Si \( f\) admet un inverse à gauche, alors
    \begin{enumerate}
        \item
            \( f\) est bijective,
        \item
            \( f\) admet également un inverse à droite,
        \item
            ils sont égaux.
    \end{enumerate}
    Tout cela tient également en remplaçant «gauche» par «droite».
\end{corollary}

\begin{proof}
    Soit \( g\), un inverse à gauche de \( f\) : \( gf=\id\). Cela implique que \( f\) est injective et que \( g\) est surjective, et donc qu'elles sont toutes deux bijectives par le corollaire \ref{CORooCCXHooALmxKk}. Vu que \( f\) est bijective, elle admet également un inverse à droite, soit \( h\). Nous avons : \( gf=\id\) et \( fh=\id\).

    Alors \( gfh=h\) parce que \( gf=\id\), mais également \( gfh=g\) parce que \( fh=\id\). Donc \( g=h\).
\end{proof}
C'est ce corollaire qui nous permet d'écrire \( f^{-1}\) sans plus de précisions dès que \( f\) est une bijection.

\begin{example}[Ne fonctionne pas en dimension infinie]
    Tout cela ne fonctionne pas en dimension infinie. Par exemple avec une base \( \{ e_k \}_{k\in \eN}\) nous pouvons considérer l'opérateur
    \begin{equation}
        f(e_k)=e_{k+1}.
    \end{equation}
    Il est injectif, mais pas surjectif. Si on pose
    \begin{equation}
        g(e_k)=\begin{cases}
            e_{k-1}    &   \text{si } k\geq 1\\
            0    &    \text{si } k=0
        \end{cases}
    \end{equation}
    alors nous avons \( gf=\id\), mais pas \( fg=\id\) parce que ce \( (fg)(e_0)=0\).
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrices équivalentes et similaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefBLELooTvlHoB}
    Deux relations d'équivalence entre les matrices.
    \begin{enumerate}
        \item   \label{ItemPFXCooOUbSCt}
    Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) s'il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\). 
\item
    Deux matrices sont \defe{semblables}{matrices!similitude} s'il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
    \end{enumerate}
\end{definition}

\begin{lemma}   \label{LemZMxxnfM}
    Une matrice de rang \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\ 
            0    &   0    
        \end{pmatrix}.
    \end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
    Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\).

    Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\). Elle vérifie
    \begin{equation}
        APe_i=Af_i=\begin{cases}
            0    &   \text{si } i>r\\
            \neq 0    &    \text{sinon}.
        \end{cases}
    \end{equation}
    La matrice \( AP\) se présente donc sous la forme
    \begin{equation}
        AP=\begin{pmatrix}
            M    &   0    \\ 
            *    &   0    
        \end{pmatrix}
    \end{equation}
    où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
    \begin{equation}
        QAPe_i=\begin{cases}
            e_i    &   \text{si } i<r\\
            0    &    \text{sinon}.
        \end{cases}.
    \end{equation}
    Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{corollary}[Équivalence et rang]      \label{CorGOUYooErfOIe}
    Deux matrices sont équivalentes\footnote{Définition \ref{DefBLELooTvlHoB}\ref{ItemPFXCooOUbSCt}.} si et seulement si elles ont même rang.
\end{corollary}

\begin{proof}
    D'abord il y a des implicites dans l'énoncé. Vu que nous voulons soit pas hypothèse soit par conclusion que les matrices $A$ et \( B\) soient équivalentes, nous supposons qu'elles ont même dimension. Soient donc \( A\) et \( B\) deux matrices carré représentant des applications linéaires de l'espace vectoriel \( E=\eK^n\) vers lui-même.

    Par le lemme \ref{LemZMxxnfM}, deux matrices de même rang \( r\) sont équivalentes à \( J_r\). Elles sont donc équivalentes entre elles.

    Inversement, supposons que \( A\) et \( B\) soient deux matrices équivalentes : \( A=PBQ^{-1}\) avec \( P\) et \( B\) inversibles. Alors
    \begin{subequations}
        \begin{align}
            \Image(PBQ^{-1})&=\{ PBQ^{-1}v\tq v\in E \}\\
            &=PB\underbrace{\{ Q^{-1}v\tq v\in E \}}_{=E}\\
            &=P\big( B(E) \big).
        \end{align}
    \end{subequations}
    L'ensemble \( B(E)\) est un sous-espace vectoriel de \( E\). Vu que le rang de \( P\) est maximum, la dimension de \( P\big( B(E) \big)\) est la même que celle de \( B(E)\). Par conséquent
    \begin{equation}
        \dim\Big( \Image(PBQ^{-1}) \Big)=\dim\big( B(E) \big)=\rang(B).
    \end{equation}
    Le membre de gauche de cela n'est autre que \( \rang(A)=\dim\big( \Image(PBQ^{-1}) \big)\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Matrice d'une application linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Écriture dans une base}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}\label{ex_affine}
	Soit $m=n$. On fixe $\lambda$ dans $\eR$ et $v$ dans $\eR^m$. L'application $U_{\lambda}$ de $\eR^m$ dans $\eR^m$ définie par $U_{\lambda}(x)=\lambda x+v$ n'est pas une application linéaire, parce que 
\[
U_{\lambda}(ax)=\lambda(ax)+v\neq \lambda(bx+v)=a U_{\lambda}(x).
\]
\end{example}

\begin{example}\label{exampleT_A}
	Soit $A$ une matrice fixée de $\mathcal{M}_{n\times m}$\nomenclature{$\mathcal{M}_{n\times m}$}{l'ensemble des matrices $n\times m$}. La fonction $T_A\colon \eR^m\to \eR^n$ définie par $T_A(x)=Ax$ est une application linéaire. En effet, 
\begin{itemize}
\item  $T_A(x+y)= A(x+y)= Ax + Ay = T_A(x)+T_A(y)$,
\item $T_A(ax)=A(ax)= a(Ax) = a T_A(x)$.
\end{itemize}
\end{example}

On peut observer que, si on identifie $\mathcal{M}_{1\times 1}$ et $\eR$, on obtient le premier exemple comme cas particulier.

\begin{proposition}     \label{PROPooCESFooGOZBNI}
 Toute application linéaire $T$ de $\eR^m$ dans $\eR^n$ s'écrit de manière unique par rapport aux bases canoniques de $\eR^m$ et $\eR^n$ sous la forme
\[
T(x)=Ax,
\]
avec $A$ dans $\mathcal{M}_{n\times m}$.
\end{proposition}

\begin{proof}
  Soit $x$ un vecteur dans $\eR^m$. On peut écrire $x$ sous la forme $ x=\sum_{i=1}^{m}x_i e_i$. Comme $T$ est une application linéaire on a
\[
T(x)=\sum_{i=1}^{m}x_iT(e_i).
\]
Les images de la base de $\eR^m$, $T(e_j), \, j=1,\ldots,m$, sont des éléments de $\eR^n$, donc on peut les écrire sous la forme de vecteurs
\[
T(e_i)=
\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}.
\] 
On obtient alors
\[
T(x)=\sum_{i=1}^{m}x_iT(e_i)=\sum_{i=1}^{m}x_i\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}=
\begin{pmatrix}
  a_{11} \ldots a_{1m}\\
\vdots \ddots \vdots\\
 a_{n1} \ldots a_{nm}\\
\end{pmatrix}
\begin{pmatrix}
  x_1\\
\vdots\\
x_m
\end{pmatrix}=Ax.
\]
\end{proof}

\begin{definition}
  Une application $S: \eR^m\to\eR^n$ est dite \defe{affine}{affine (application)} si elle est la somme d'une application linéaire et d'une application constante. Autrement dit, $S$ est affine s'il existe $T: \eR^m\to\eR^n$, linéaire, telle que $S(x)-T(x)$ soit un vecteur constant dans $\eR^n$. 
\end{definition}

\begin{example}
	Les exemples les plus courants d'applications affines sont les droites et les plans ne passant pas par l'origine.
	\begin{description}
		\item[Les droites] Une droite dans $\eR^2$ (ou $\eR^3$) qui ne passe pas par l'origine est le graphe d'une fonction de la forme $s(x)=ax+b$ (ou $s(t)=u x +v$, avec $u$ et $v$  dans $\eR^2$). On reconnait ici la fonction de l'exemple \ref{ex_affine}.
			
		\item[Les plans]
			De la même façon nous savons que tout plan qui ne passe pas par l'origine dans $\eR^3$ est le graphe d'une application affine, $P(x,y)= (a,b)^T\cdot(x,y)^T+(c,d)^T$.
	\end{description}
\end{example}

\begin{lemma}[\cite{ooEPEFooQiPESf}]        \label{LEMooDAACooElDsYb}
    Soit une application linéaire \( f\colon E\to F\).
    \begin{enumerate}
        \item       \label{ITEMooEZEWooZGoqsZ}
            L'application \( f\) est injective si et seulement s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id|_E\).
        \item
            L'application \( f\) est surjective si et seulement s'il existe \( g\colon F\to E\) telle que \( f\circ g=\id|_F\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Nous démontrons séparément les deux affirmations.
    \begin{enumerate}
        \item
            Si \( f\) est injective, alors \( f\colon E\to \Image(f)\) est un isomorphisme. Si $V$ est un supplémentaire de \( \Image(f)\) dans \( F\) (c'est à dire \( F=\Image(f)\oplus V\)) alors nous pouvons poser \( g(x+v)=f^{-1}(x)\) où \( x+v\) est la décomposition (unique) d'un élément de \( F\) en \( x\in\Image(f)\) et \( v\in V\). Avec cela nous avons bien \( g\circ f=\id\).

            Inversement, s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id\) alors \( f\colon E\to E\) doit être injective. Parce que si \( f(x)=0\) avec \( x\neq 0\) alors \( (g\circ f)(x)=0\neq x\).
        \item
            Si \( f\) est surjective nous pouvons choisir des éléments \( x_1,\ldots, x_p\) dans \( E\) tels que \( \{ f(x_i) \}\) soit une base de \( F\). Ensuite nous définissons
            \begin{equation}
                \begin{aligned}
                    g\colon F&\to E \\
                    \sum_ka_kf(x_k)&\mapsto \sum_kx_k. 
                \end{aligned}
            \end{equation}
            Cela donne \(  f\circ g=\id|_F\) parce que si \( v\in F\) alors \( v=\sum_kv_kf(x_k)\) avec \( v_k\in \eK\), et nous avons
            \begin{equation}
                (f\circ g)(v)=\sum_kv_k(f\circ g)f(x_k)=f\left( \sum_kv_kx_k \right)=\sum_kf(x_k)=v.
            \end{equation}
            
            Inversement, s'il existe \( g\colon F\to E\) tel que \( f\circ g=\id\) alors \( f\) soit être surjective parce que 
            \begin{equation}
                F=\Image(f\circ g)=f\big( \Image(g) \big)\subset \Image(f).
            \end{equation}
    \end{enumerate}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Changement de base : vecteurs de base}
%---------------------------------------------------------------------------------------------------------------------------

Soit un espace vectoriel \( V\) muni d'une base \( \{ e_i \}\) et une application linéaire inversible \( Q\colon V\to V\) dont nous notons \( (Q_{ij})\) les coordonnées dans la base \( \{ e_i \}\). Nous considérons aussi la base \( e'_i=Qe_i\).

Nous exprimons les vecteurs \( e'_i\) en termes des vecteurs \( e_i\) et inversement de la façon suivante. D'abord
\begin{equation}
        e'_i=Qe_i=\sum_{kl}Q_{kl}(e_i)_ke_l=\sum_lQ_{il}e_l
\end{equation}
donc
\begin{equation}        \label{EQooQECYooVWEoLx}
    e'_i=\sum_kQ_{ik}e_k.
\end{equation}

\begin{normaltext}      \label{TEXTooOBOMooOVqUVG}
    Afin de ne pas se tromper dans les indices, il est bon d'utiliser systématiquement les noms ``$ijk\ldots$'' pour les indices de la base \( \{ e_i \}\) et les noms ``\( \alpha\beta\gamma\cdots\)'' pour les indices de la base \( \{ e'_{\alpha} \}\). Nous écrivons donc
    \begin{equation}        \label{EQooZQBZooTzLkLF}
        e'_{\alpha}=\sum_iQ_{i\alpha}e_i.
    \end{equation}
\end{normaltext}

Pour trouver la transformation inverse, nous multiplions par \( (Q^{-1})_{\alpha j}\) et nous sommons sur \( \alpha\) :
\begin{equation}
    \sum_{\alpha}(Q^{-1})_{\alpha j}e'_{\alpha}=\sum_{i \alpha}Q_{i\alpha}Q^{-1}_{\alpha j}e_i=e_j
\end{equation}
donc
\begin{equation}        \label{EQooEZBXooGxpJiO}
    e_j=\sum_{\alpha}(Q^{-1})_{\alpha j}e'_{\alpha}.
\end{equation}

Quelque remarques :
\begin{enumerate}
    \item
        Notons que nous ne pouvons pas écrire formellement quelque chose comme \( f=Qe\). 
    \item
        La matrice \( Q\) vient toujours avec les indices de type \( Q_{i\alpha}\) (latin en premier et grec en second) tandis que la matrice de l'application inverse \( Q^{-1}\) vient toujours avec les indices de type \( (Q^{-1})_{\alpha i}\).

        Ceci est un puissant outil pour ne pas se tromper lors des manipulations d'indices et en particulier lorsque nous renommons des indices.
\end{enumerate}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Changement de base : coordonnées}
%---------------------------------------------------------------------------------------------------------------------------

Un vecteur \( x\) peut s'écrire des façons suivantes dans les deux bases considérées :
\begin{equation}
    x=\sum_ix_ie_i=\sum_{\alpha}x'_{\alpha}e'_{\alpha}.
\end{equation}
En remplaçant \( e_i\) par sa valeur \eqref{EQooEZBXooGxpJiO} nous obtenons l'égalité
\begin{equation}
    \sum_{k\alpha}x_k(Q^{-1})_{\alpha k}e'_{\alpha}=\sum_{\alpha}x'_{\alpha}e'_{\alpha}
\end{equation}
et par unicité de la décomposition d'un vecteur dans une base nous déduisons
\begin{equation}
    x'_{\alpha}=\sum_{k}(Q^{-1})_{\alpha k}x_k.
\end{equation}
De même en replaçant \( e'_{\alpha}\) par sa valeur \eqref{EQooZQBZooTzLkLF} nous trouvons
\begin{equation}        \label{EQooOEHJooCDKUcy}
    x_i=\sum_{\alpha}Q_{i\alpha}x'_{\alpha}.
\end{equation}

Attention : il n'est pas question d'écrire quelque chose comme \( x=Qx'\) parce qu'il n'y a pas de vecteur \( x'\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Changement de base : matrice d'une application linéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit une application linéaire \( f\colon V\to V\) de matrice \( A\) dans la base \( \{ e_i \}\) et de matrice \( A'\) dans la base \( \{ e'_{\alpha} \}\). Notre but est maintenant d'exprimer \( A'\) en termes de \( A\) et de \( Q\).

Nous avons l'égalité
\begin{equation}
    f(x)=\sum_{kl}A_{kl}x_le_k=\sum_{\alpha\beta}A'_{\alpha\beta}x'_{\beta}e'_{\alpha}.
\end{equation}
Il suffit d'y substituer les valeurs \eqref{EQooOEHJooCDKUcy} et \eqref{EQooEZBXooGxpJiO} :
\begin{equation}
    f(x)=\sum_{kl}A_{kl}\sum_{\beta}Q_{l\beta}x'_{\beta}\sum_{\alpha}(Q^{-1})_{\alpha k}e'_{\alpha}=\sum_{\alpha\beta}(Q^{-1}AQ)_{\alpha\beta}x'_{\beta}e'_{\alpha}.
\end{equation}
Cela est égal à \( \sum_{\alpha\beta}A'_{\alpha\beta}x'_{\beta}e'_{\alpha}\) pour tout \( x\). En tenant encore compte de l'unicité de la décomposition du vecteur \( f(x)\) dans la base \( \{ e'_{\alpha} \}\) nous déduisons
\begin{equation}        \label{EQooWWOUooZBeTBe}
    A'=Q^{-1} AQ.
\end{equation}

\begin{remark}
    L'égalité \eqref{EQooWWOUooZBeTBe} est une égalité de matrices, et non une égalité d'applications linéaire. Il n'est pas question d'écrire quelque chose comme \( f'=Q^{-1}\circ f\circ Q\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit une forme bilinéaire \( q\colon V\times V\to \eR\) de matrice \( (q_{ij})\) dans la base \( \{ e_i \}\) et de matrice \( q'_{\alpha\beta}\) dans la base \( \{ e'_{\alpha} \}\). Nous notons le changement de variables \(e'_{\alpha}=\sum_{i}Q_{i\alpha}e_i\). Nous avons l'égalité
\begin{equation}
    q(x,y)=\sum_{ij}q_{ij}x_iy_j=\sum_{\alpha\beta}q'_{\alpha\beta}x'_{\alpha}y'_{\beta}.
\end{equation}
En remplaçant \( x_i\) et \( y_j\) par leurs valeurs en termes des \( x'_{\alpha}\) et \( y'_{\beta}\) nous trouvons
\begin{equation}
    q(x,y)=\sum_{ij\alpha\beta}q_{ij}Q_{i\alpha}x'_{\alpha}Q_{j\beta}y'_{\beta},
\end{equation}
c'est à dire
\begin{equation}        \label{EQooQLLFooToyvoH}
    q'=Q^tqQ.
\end{equation}
Encore une fois, cette égalité est une égalité de matrices, et non de formes bilinéaires, et encore moins d'applications linéaires. 
\begin{remark}
    La «loi de transformation» \eqref{EQooQLLFooToyvoH} n'est pas la même que la loi de transformation \eqref{EQooWWOUooZBeTBe}.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropEJBZooTNFPRj}
    Si $A$ est la matrice d'une application linéaire, alors le rang de cette application linéaire est égal à la taille de la plus grande matrice carré de déterminant non nul contenue dans $A$.
\end{proposition}

\begin{definition}  \label{DefJPGSHpn}
    Si \( E\) est un espace vectoriel sur \( \eK\), le \defe{dual algébrique}{dual!algébrique} de \( E\) est l'ensemble des formes linéaires sur \( E\). Le \defe{dual topologique}{dual!topologique} est l'ensemble des formes linéaires continues de \( E\) vers \( \eK\).
\end{definition}

En dimension infinie, ces deux notions ne coïncident pas, voir la proposition \ref{PROPooQZYVooYJVlBd} et la remarque \ref{RemOAXNooSMTDuN}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooEQSMooHVzbfz}
    Soit \( E\), un espace vectoriel, et \( F\) une sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
    \begin{equation}    \label{Eqiiyple}
        F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
    \end{equation}
\end{definition}

Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) munie du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition \ref{DEFooEQSMooHVzbfz}, au contraire, est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
    B^o=\{ x\in E\tq \omega(x)=0\,\forall \omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual et on prend une notation précise pour dire qu'on remonte au pré-dual et non qu'on va au dual du dual.


\begin{proposition} \label{PropXrTDIi}
    Soit \( E\) un espace vectoriel, et \( F\) un sous-espace vectoriel. Alors nous avons
    \begin{equation}
        \dim F+\dim F^{\perp}=\dim E.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème \ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\). 
    
    Déjà c'est une partie libre en tant que partie d'une base.

    Ensuite ce sont des éléments de \( F^{perp}\) parce que si \( i\leq p\) et si \( k\geq 1\) nous avons \( e^*_{p+k}(e_i^*)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

    Enfin \( F^{\perp}\subset\Span\{ e_{p+1}^*,e_n^* \}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{i=p+1}^n\omega_ke^*_k\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooZLPAooKTITdd}
    Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
    \begin{equation}
        f^t(\omega)(x)=\omega\big( f(x) \big).
    \end{equation}
    pour tout \( \omega\in F^*\) et \( x\in E\).
\end{definition}

\begin{lemma}
    Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).
\end{lemma}

\begin{proof}
    Nous allons montrer que les formes \( f^t(g^*_i)\) et \( \sum_k(A^t)_{ik}g^*_k\) sont égales en les appliquant à un vecteur.

    Par définition de la matrice d'une application linéaire dans une base,
    \begin{equation}
        f^t(g_i^*)=\sum_j(f^t)_{ij}e^*j,
    \end{equation}
    et
    \begin{equation}
        f(e_k)=\sum_lA_{klg_l}.
    \end{equation}
    Du coup, si \( x=\sum_kx_ke_k\), nous avons
    \begin{equation}    \label{EqCzwftH}
        f^t(g_i^*)x=\sum_{kl}x_kg_i^*A_{kl}g_l=\sum_{kl}x_kA_{kl}\delta_{il}=\sum_k x_kA_{ki}=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    D'autre part, 
    \begin{equation}    \label{EqWlQlrR}
        \sum_k(A^t)_{ik}g_k^*x=\sum_{kl}(A^t)_{ik}g^*_kx_le_l=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    Le fait que \eqref{EqCzwftH} et \eqref{EqWlQlrR} donnent le même résultat prouve le lemme.
\end{proof}
En corollaire, les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carré de déterminant non nul. Nous prouvons cependant ce résultat de façon plus intrinsèque.

\begin{lemma}[\href{http://gilles.dubois10.free.fr/algebre_lineaire/dualite.html}{Gilles Dubois}]   \label{LemSEpTcW}
    Si \( f\colon E\to F\) est une application linéaire, alors
    \begin{equation}
        \rang(f)=\rang(f^t).
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons \( \dim\ker(f)=p\) et donc \( \rang(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
    \begin{equation}
        g_i=f(e_{p+i})
    \end{equation}
    pour \( i=1,\ldots, n-p\). C'est à dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
    \begin{equation}
        \sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
    \end{equation}
    alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), ce qui signifierait que \( \sum_ka_ke_{p+k}\) se trouve dans le noyau de \( f\), ce qui est impossible par construction de la base \( \{ e_i \}_{i=1,\ldots, n}\). Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous les complétons en une base
    \begin{equation}
        \{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}
    \end{equation}
    de \( F\).

    Nous prouvons maintenant que \( \rang(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) est libre (et donc l'espace image de \( f^f\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
    \begin{equation}
        f^t(g^*_i)e_k=g_i^*(fe_k),
    \end{equation}
    Si \( k=1,\ldots, p\), alors \( fe_k=0\) et donc \( g_i^*(fe_k)=0\); si \( k=p+l\) alors
    \begin{equation}
        f^t(g_i^*)e_k=g_i^*(fe_{k+l})=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
    \end{equation}
    Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
    \begin{equation}
        \rang(f^t)\geq n-p=\rang(f).
    \end{equation}
    En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
    \begin{equation}
        \rang\big( (f^t)^t \big)\geq \rang(f^t)
    \end{equation}
    et donc, vu que \( (f^t)^t=f\), nous obtenons \( \rang(f)=\rang(f^t)\).
    
\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]        \label{PropWOPIooBHFDdP}
    Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
    \begin{equation}
        \Image(f^t)=\ker(f)^{\perp}.
    \end{equation}
\end{proposition}

\begin{proof}
    Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est à dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est à dire que \( \omega\in (\ker f)^{\perp}\).

    Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
    \begin{subequations}
        \begin{align}
            \dim\big( \Image(f^t) \big)&=\rang(f^t)\\
            &=\rang(f)  &\text{lemme \ref{LemSEpTcW}}\\
            &=\dim(E)-\dim\ker(f)   &\text{théorème \ref{ThoGkkffA}}\\
            &=\dim\big( (\ker f)^{\perp} \big)  &\text{proposition \ref{PropXrTDIi}}.
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}[\cite{ooEPEFooQiPESf}]
    Soit \( \eK\) un corps, \( E\) et \( F\) deux \( \eK\)-espaces vectoriels de dimension finie et une application linéaire \( f\colon E\to F\). L'application \( f\) est injective si et seulement si sa transposée\footnote{Définition \ref{DefooZLPAooKTITdd}.} \( f^t\) est surjective.
\end{lemma}

\begin{proof}
    Supposons que \( f\) soit injective. Alors par le lemme \ref{LEMooDAACooElDsYb}, il existe \( g\colon F\to E\) tel que \( g\circ f=\id|_E\). Nous avons alors aussi \( (g\circ f)^t=\id|_{E^*}\), mais \( (g\circ f)^t=f^t\circ g^t\), donc \( f^t\) est surjective.

    Inversement, nous supposons que \( f^t\colon F^*\to E^*\) est surjective. Alors en nous souvenant que \( E\) et \( F\) sont de dimension finie et en faisons jouer les identifications \( (f^t)^t=f\) et \( (E^*)^*=E\) nous pouvons savons qu'il existe \( s\colon E^*\to F^*\) tel que \( f^t\circ s=\id|_{E^*}\). En passant à la transposée,
    \begin{equation}
        s^t\circ f=\id|_{E},
    \end{equation}
    qui implique que \( f\) est injective.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Transposée : sans le dual}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}        \label{PROBooWNTKooNErOvt}
    J'aimerais une confirmation de ce qui est écrit dans ici à propos du fait que si \( f\) est une application linéaire, \( f^t\) n'est pas bien définie.
\end{probleme}

Il est légitime, si \( f\colon V\to V\) est une application linéaire, de dire que sa transposée soit l'application linéaire \( f^t\colon V\to V\) dont la matrice est la matrice transposée de celle de \( f\). Lorsque nous travaillons sur \( \eR^n\) muni de la base canonique, cela ne pose pas de problèmes et nous pouvons écrire des égalités du type \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).

Hélas nous allons voir que cette façon de définir une transposée est mauvaise. Nous reprenons les notations de \ref{TEXTooOBOMooOVqUVG}. Nous avons
\begin{equation}
    f(x)=\sum_{kl}A_{kl}x_le_k=\sum_{\alpha\beta}A'_{\alpha\beta}x'_{\beta}e'_{\alpha}.
\end{equation}
Nous écrivons l'application transposée par rapport à la première base :
\begin{equation}
    f^{t_1}(x)=\sum_{kl}A_{lk}x_le_k
\end{equation}      
et par rapport à la seconde base :
\begin{equation}        \label{EQooFXDQooEjYCrP}
    f^{t_2}(x)=\sum_{\alpha\beta}A'_{\beta\alpha}x'_{\beta}e'_{\alpha}.
\end{equation}
En replaçant dans \( f^{(t_1)}\) les nombres \( x_l\) et les vecteurs \( e_k\) par les valeurs en termes des \( x'_{\beta}\) et \( e'_{\alpha}\) nous obtenons
\begin{equation}
    f^{t_1}(x)=\sum_{\alpha\beta}(Q^{-1}A^tQ)_{\alpha\beta}x'_{\beta}e'_{\alpha}.
\end{equation}
Nous avons égalité avec \eqref{EQooFXDQooEjYCrP} seulement lorsque \( A'_{\beta\alpha}=(Q^{-1}A^tQ)_{\alpha\beta}\), mais sachant que \( A'=Q^{-1}AQ\) nous avons besoin que
\begin{equation}
    Q^{-1}=Q^t
\end{equation}
(cela étant une égalité de matrice).

\begin{normaltext}      \label{NooMZVRooExWVKJ}
    Autrement dit, la façon «usuelle» de voir la transposée d'une application linéaire ne fonctionne dans les livres pour enfant uniquement parce qu'on n'y considère toujours \( \eR^n\) muni de la base canonique ou de bases orthonormées.
     
    Notons que nous avons tout de mêmes le notions d'opérateur adjoint et autoadjoint pour parler d'application orthogonale sans passer par la transposée, voir \ref{DEFooYKCSooURQDoS}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E=\eR_n[X]\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient les \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in E^*\),
\begin{equation}
    f_i(P)=P(a_i).
\end{equation}
\begin{lemma}
    Ces formes forment une base de \( E^*\).
\end{lemma}

\begin{proof}
    Nous prouvons que l'orthogonal est réduit au nul :
    \begin{equation}
        \Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}
    \end{equation}
    pour que la proposition \ref{PropXrTDIi} conclue. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme!Lagrange} sont les polynômes de la base (pré)duale de la base \( \{ f_i \}\).

\begin{proposition}
    Les polynômes de Lagrange sont donnés par
    \begin{equation}
        P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de vérifier que \( f_j(P_i)=\delta_{ij}\). Nous avons
    \begin{equation}
        f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
    \end{equation}
    Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dual de \texorpdfstring{$ \eM(n,\eK)$}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
    Soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
    \begin{equation}
        \begin{aligned}
            f_A\colon \eM_n(\eK)&\to \eK \\
            M&\mapsto \tr(AM). 
        \end{aligned}
    \end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}


\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eK)&\to \eM(n,\eK)' \\
            A&\mapsto f_A 
        \end{aligned}
    \end{equation}
    et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM_n(\eK)\) et \( \eM_n(\eK)'\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{ik}\delta_{jl}\) :
    \begin{equation}
            0=f_A(E_{ij})
            =\sum_{k}(AE_{ij})_{kk}
            =\sum_{kl}A_{kl}\delta_{il}\delta_{jk}
            =A_{ij}.
    \end{equation}
    Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soit \( \eK\) un corps et \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( X,Y\in \eM(n,\eK)\) on ait
    \begin{equation}
        \phi(XY)=\phi(YX).
    \end{equation}
    Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
    La proposition \ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(XY)=f_A(YX)\), c'est à dire
    \begin{equation}
        \Tr(AXY)=\Tr(AYX)
    \end{equation}
    pour toute matrices \( X,Y\in \eM(n,\eK)\). L'invariance cyclique de la trace nous dit que \( \Tr(AXY)=\Tr(XAY)\), ce qui signifie que
    \begin{equation}
        \Tr\big( (AX-XA)Y \big)=0
    \end{equation}
    ou encore que \( f_{AX-XA}=0\) pour tout \( X\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
    \begin{equation}
        AX=XA
    \end{equation}
    pour tout \( X\in\eM\). En particulier en prenant la fameuse matrice \( E_{ij}\) et en calculant un peu,
    \begin{equation}
        A_{li}\delta_{jm}=\delta_{il}A_{jm}
    \end{equation}
    pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
    \begin{equation}
        \phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
    \end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]       \label{CorICUOooPsZQrg}
    Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
    Soit \( \mH\) un hyperplan de \( \eM\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition \ref{PropHOjJpCa} nous donne \( A\in \eM\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme \ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\ 
            0    &   0    
        \end{pmatrix}.
    \end{equation}
    Pour tout \( X\in \eM\) nous avons
    \begin{equation}
        \phi(X)=\Tr(AX)=\Tr(PJ_rQX)=\Tr(J_rQXP).
    \end{equation}
    Ce que nous cherchons est \( X\in \GL(n,\eK)\) telle que \( \phi(X)=0\). Nous commençons par trouver \( Y\in\GL(n,\eK)\) telle que \( \Tr(J_rY)=0\). Celle-là est facile : c'est
    \begin{equation}
        Y=\begin{pmatrix}
            0    &   1    \\ 
            \mtu_{n-1}    &   0    
        \end{pmatrix}.
    \end{equation}
    Les éléments diagonaux de \( J_rY\) sont tous nuls. Par conséquent en posant \( X=Q^{-1}YP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}
