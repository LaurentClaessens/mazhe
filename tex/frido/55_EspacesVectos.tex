% This is part of Mes notes de mathématique
% Copyright (c) 2008-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Parties libres, génératrices, bases et dimension}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous avons déjà défini (dans~\ref{DEFooKHWZooIfxdNc}) un espace vectoriel comme étant un module sur un corps commutatif. En explicitant un peu, cela donne ceci\cite{ooQLVLooEUrNLS}.

Un espace vectoriel sur le corps \( \eK\) est un ensemble \( V\) muni de deux opérations :
\begin{itemize}
    \item une loi de composition interne \( +\colon V\times V\to V\),
    \item une loi de composition externe \( \cdot\colon \eK\times V\to V\)
\end{itemize}
telles que
\begin{enumerate}
    \item
        \( (V,+)\) soit un groupe abélien,
    \item
        pour tout \( u,v\in V\) et pour tout \( k,k'\in \eK\),
        \begin{subequations}
           \begin{align}
                k(u+v)=(ku)+(kv)\\
                (kk')u=k(k'u)\\
                (k+k')u=(ku)+(k'u)\\
                1u=u
            \end{align}
        \end{subequations}
        où \( 1\) est le neutre de \( \eK\) et où nous avons directement adopté la notation \( ku\) pour \( k\cdot u\).
\end{enumerate}
Si \( u\in V\), nous notons \( -u\) l'inverse de \( u\) dans le groupe \( (V,+)\).

\begin{definition}[Partie libre]
    Si \( E\) est un espace vectoriel, une partie \( A\) de \( E\) est \defe{libre}{libre!partie} si pour tout choix d'un nombre fini d'éléments \( \{ u_i \}_{i=1,\ldots, n}\), l'égalité
    \begin{equation}
        a_1 u_1+\cdots +a_nu_n=0
    \end{equation}
    implique \( a_i=0\) pour tout \( i\) (ici les \( a_i\) sont dans le corps de base).

    Une partie infinie est libre si toutes ses parties finies le sont.
\end{definition}

\begin{remark}
    Notons que le vecteur nul n'est dans aucune partie libre, ne fût-ce que parce que \( a0=0\) n'implique pas \( a=0\).
\end{remark}

Si \( A\) est une partie de l'espace vectoriel \( E\) nous notons \( \Span(A)\)\nomenclature[A]{$\Span(A)$}{l'ensemble des combinaisons linéaires finies d'éléments de \( A\)} l'ensemble des combinaisons linéaires finies d'éléments de \( A\). Les coefficients de ces combinaisons linéaires sont dans le corps de base \( \eK\).

\begin{definition}[Partie génératrice]
    Une partie $B$ d'un espace vectoriel \( E\) est \defe{génératrice}{partie!génératrice} si \( \Span(B)=E\).
\end{definition}

\begin{remark}
    Ces définitions demandent des commentaires en dimension infinie\footnote{Nous n'avons pas encore défini le concept de dimension, mais nous nous adressons au lecteur trop pressé.}.

    \begin{enumerate}
        \item
    Tout élément peut être écrit comme combinaison linéaire finie d'une partie génératrice. Cela ne signifie pas que nous pouvons extraire une partie finie qui convient pour tous les éléments à la fois. Lorsque l'espace est de dimension infinie, ceci est particulièrement important.
\item
    La définition séparée de liberté dans le cas des parties infinies a son importance lorsqu'on parle d'espaces vectoriels de dimension infinies (en dimension finie, aucune partie infinie n'est libre) parce que cela fera une différence entre une base algébrique et une base hilbertienne par exemple.
    \end{enumerate}
\end{remark}

\begin{definition}[Base]        \label{DEFooNGDSooEDAwTh}
    Une \defe{base}{base} de l'espace vectoriel \( E\) est une partie à la fois génératrice et libre.
\end{definition}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooEIQIooXfWDDV}
    Tout élément non nul d'un espace vectoriel possédant une base\footnote{Nous n'avons pas démontré que tout espace vectoriel possède une base. Donc à notre niveau, il est possible que ce théorème soit sans objet pour beaucoup d'espaces.} se décompose de façon unique en combinaison linéaire finie d'éléments d'une base.
\end{proposition}

\begin{proof}
    Soit un espace vectoriel \( E\) et une base \( \{ e_i \}_{i\in I}\) où \( I\) est un ensemble a priori quelconque. Soit \( v\in E\). Vu que \( E=\Span\{ e_i \}_{i\in I}\), il existe une partie finie \( J\) de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) dans \( \eK\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j.
    \end{equation}
    Cela donne l'existence.

    En ce qui concerne l'unicité, soient \( J \) et \( K\) des parties finies de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) et \( \{ w_{k} \}_{k\in K}\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j=\sum_{k\in K}w_{k}e_{k}.
    \end{equation}
    Nous posons \( L=J\cup K\) et, pour \( l\in L\),
    \begin{equation}
        \alpha_l=\begin{cases}
            v_l    &   \text{si } l\in J\setminus K\\
            w_l    &    \text{si } l\in K\setminus J\\
            v_l-w_l    &    \text{si } l\in K\cap J.
        \end{cases}
    \end{equation}
    Nous avons alors
    \begin{equation}
        \sum_{l\in L}\alpha_le_l=0,
    \end{equation}
    ce qui implique que \( \alpha_l=0\) pour tout \( l\in L\) parce que la partie \( \{ e_i \}_{i\in I}\) est libre et que \( L\) est finie.

    L'unicité de la décomposition de \( v\) signifie que
    \begin{equation}
        \{ j\in J \tq v_j\neq 0 \}=\{ k\in K\tq w_k\neq 0 \}
    \end{equation}
    et que pour \( l\) dans cet ensemble, \( v_l=w_l\).

    Soit \( j\in J\); il y a deux possibilités : \( j\in J\setminus K\) et \( j\in J\cap K\). Dans le premier cas nous avons déjà vu que \( \alpha_j=v_j=0\). Dans le second cas, \( \alpha_j=v_j-w_j=0\), c'est-à-dire \( v_j=w_j\).

    Donc \( j\in J\) vérifiant \( v_j\neq 0\) implique \( j\in J\cap K\) et l'égalité des coefficients. Idem avec \( k\in K\) tel que \( w_k\neq 0\) implique \( k\in J\cap K\).
\end{proof}

\begin{definition}
    Un espace vectoriel est \defe{de type fini}{type!fini!espace vectoriel} s'il contient une partie génératrice finie.
\end{definition}
Nous verrons dans les résultats qui suivent que cette définition est en réalité inutile parce qu'une espace vectoriel sera de type fini si et seulement s'il est de dimension finie.

\begin{lemma}       \label{LemytHnlD}
    Si \( E\) a une famille génératrice de cardinal \( n\), alors toute famille de \( n+1\) éléments est liée.
\end{lemma}

\begin{proof}
    Nous procédons par récurrence sur \( n\). Pour \( n=1\), nous avons \( E=\Span(e)\) et donc si \( v_1,v_2\in E\) nous avons \( v_1=\lambda_1 e\), \( v_2=\lambda_2e\) pour certains éléments non nuls \( \lambda_1,\lambda_2\) du corps de base. Nous avons donc \( \lambda_2v_1-\lambda_1v_1=0\). Cela prouve que \( \{ v_1,v_2 \}\) est liée.

    Supposons maintenant que le résultat soit vrai pour \( k<n\), c'est-à-dire que pour tout espace vectoriel contenant une partie génératrice de cardinal \( k<n\), les parties de \( k+1\) éléments sont liées. Soit maintenant un espace vectoriel muni d'une partie génératrice \( G=\{ e_1,\ldots, e_n \}\) de \( n\) éléments, et montrons que toute partie \( V=\{ v_1,\ldots, v_{n+1} \}\) contenant \( n+1\) éléments est liée. Dans nos notations nous supposons que les \( e_i\) sont des vecteurs distincts et les \( v_i\) également. Nous les supposons également tous non nuls. Étant donné que \( \{ e_i \}\) est génératrice nous pouvons définir les nombres \( \lambda_{ij}\) par
    \begin{equation}
        v_i=\sum_{k=1}^n\lambda_{ij}e_j
    \end{equation}
    Vu que
    \begin{equation}
        v_{n+1}=\sum_{k=1}^n\lambda_{n+1,k}e_k\neq 0,
    \end{equation}
    quitte à changer la numérotation des \( e_i\) nous pouvons supposer que \( \lambda_{n+1,n}\neq 0\). Nous considérons les vecteurs
    \begin{equation}
        w_i=\lambda_{n+1,n}v_i-\lambda_{i,n}v_{n+1}.
    \end{equation}
    En calculant un peu,
    \begin{subequations}
        \begin{align}
            w_i&=\lambda_{n+1,n}\sum_k\lambda_{i,k}e_k-\lambda_{i,n}\sum_k\lambda_{n+1,k}e_k\\
            &=\sum_{k=1}^{n-1}\big( \lambda_{n+1,n}\lambda_{i,k}-\lambda_{i,n}\lambda_{n+1,} \big)e_k
        \end{align}
    \end{subequations}
    parce que les termes en \( e_n\) se sont simplifiés. Donc la famille \( \{ w_1,\ldots, w_n \}\) est une famille de \( n\) vecteurs dans l'espace vectoriel \( \Span\{ e_1,\ldots, e_{n-1} \}\); elle est donc liée par l'hypothèse de récurrence. Il existe donc des nombres \( \alpha_1,\ldots, \alpha_n\in \eK\) non tous nuls tels que
    \begin{equation}        \label{EqOQGGoU}
        0=\sum_{i=1}^n\alpha_iw_i=\sum_{i=1}^n\alpha_i\lambda_{n+1,n}v_i-\left( \sum_{i=1}^n\alpha_i\lambda_{i,n} \right)v_{n+1}.
    \end{equation}
    Vu que \( \lambda_{n+1,n}\neq 0\) et que parmi les \( \alpha_i\) au moins un est non nul, nous avons au moins un des produits \( \alpha_i\lambda_{n+1,n}\) qui est non nul. Par conséquent \eqref{EqOQGGoU} est une combinaison linéaire nulle non triviale des vecteurs de \( \{ v_1,\ldots, v_{n+1} \}\). Cette partie est donc liée.
\end{proof}

\begin{lemma}   \label{LemkUfzHl}
    Soit \( L\) une partie libre et \( G\) une partie génératrice. Si l'ensemble des parties libres \( L'\) telles que \( L\subset L'\subset G\) possède un élément maximum\footnote{Encore une fois, à part quelques cas triviaux, il n'est pas clair à ce point que ce maximum existe.}, alors cet élément est une base.
\end{lemma}
Qu'entend-on par «maximale» ? La partie \( B\) doit être libre, contenir \( L\), être contenue dans \( G\) et de plus avoir la propriété que \( \forall x\in G\setminus B\), la partie \( B\cup\{ x \}\) est liée.

\begin{proof}
    D'abord si \( G\) est une base, alors toutes les parties de \( G\) sont libres et le maximum est \( B=G\). Dans ce cas le résultat est évident. Nous supposons donc que \( G\) est liée.

    La partie \( B=\{ b_1,\ldots, b_l \}\) est libre parce qu'on l'a prise parmi les libres. Montrons que \( B\) est génératrice. Soit \( x\in G\setminus B\); par hypothèse de maximalité, \( B\cup\{ x \}\) est liée, c'est-à-dire qu'il existe des nombres \( \lambda_i\), \( \lambda_x\) non tous nuls tels que
    \begin{equation}    \label{EqxfkevM}
        \sum_{i=1}^l\lambda_ib_i+\lambda_xx=0.
    \end{equation}
    Si \( \lambda_x=0\) alors un de \( \lambda_i\) doit être non nul et l'équation \eqref{EqxfkevM} devient une combinaison linéaire nulle non triviale des \( b_i\), ce qui est impossible parce que \( B\) est libre. Donc \( \lambda_x\neq 0\) et
    \begin{equation}
        x=\frac{1}{ \lambda_x }\sum_{i=1}^l\lambda_ib_i.
    \end{equation}
    Donc tous les éléments de \( G\setminus B\) sont des combinaisons linéaires des éléments de \( B\), et par conséquent, \( G\) étant génératrice, tous les éléments de \( E\) sont combinaisons linéaires d'éléments de \( B\).
\end{proof}

\begin{theorem}[Théorème de la base incomplète] \label{ThonmnWKs}
    Soit \( E\) un espace vectoriel de type fini sur le corps \( \eK\).
    \begin{enumerate}
        \item     \label{ItemBazxTZ}
            Si \( L\) est une partie libre et si \( G\) est une partie génératrice contenant \( L\), alors il existe une base \( B\) telle que \( L\subset B\subset G\).
        \item     \label{ITEMooFVJXooGzzpOu}
            Toute partie libre peut être étendue en une base.
        \item     \label{ITEMooFBUAooSSZxgx}
            Toutes les bases sont finies et ont même cardinal.
        \item       \label{ITEMooJIJSooGuJMdt}
            Si \( V\) est un sous-espace vectoriel de \( E\), et si \( L\) est une base de \( V\), alors il existe une base \( E\) qui contient \( L\).
    \end{enumerate}
\end{theorem}
\index{théorème!base incomplète}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
    Vu que \( E\) est de type fini, il admet une partie génératrice \( G\) de cardinal fini \( n\). Donc une partie libre est de cardinal au plus \( n\) par le lemme~\ref{LemytHnlD}. Soit \( L\), une partie libre contenue dans \( G\) (ça existe : par exemple \( L=\emptyset\)). La partie \( B\) maximalement libre contenue dans \( G\) et contenant \( L\) est une base par le lemme~\ref{LemkUfzHl}.
\item
Notons que puisque \( E\) lui-même est générateur, le point~\ref{ItemBazxTZ} implique que toute partie libre peut être étendue en une base.
\item
    Soient \( B\) et \( B'\), deux bases. En particulier \( B\) est génératrice et \( B'\) est libre, donc le lemme~\ref{LemytHnlD} indique que \( \Card(B')\leq \Card(B)\). Par symétrie on a l'inégalité inverse. Donc \( \Card(B)=\Card(B')\).
\item
    La partie \( L\) étant une base de \( V\), elle est en particulier libre dans \( E\). Par le point \ref{ITEMooFVJXooGzzpOu}, \( L\) peut être étendue en une base.
    \end{enumerate}
\end{proof}

\begin{remark}      \label{REMooYGJEooEcZQKa}
    Le théorème de la base incomplète~\ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu} est ce qui permet de construire une base d'une espace vectoriel en « commençant par» une base d'un sous-espace. En effet si \( H\) est un sous-espace de \( E\) alors une base de \( H\) est une partie libre de \( E\) et donc peut être étendue en une base de \( E\).
\end{remark}

\begin{definition}      \label{DEFooWRLKooArTpgh}
    La \defe{dimension}{dimension} d'un espace vectoriel de type fini est le cardinal\footnote{Définition \ref{PROPooJLGKooDCcnWi}.} d'une\footnote{Le théorème de la base incomplète~\ref{ThonmnWKs}\ref{ITEMooFBUAooSSZxgx} montre que cette définition ne souffre d'aucune ambiguïté.} de ses bases.
\end{definition}
\index{dimension!définition}

Il existe une infinité de bases de $\eR^m$. On peut démontrer que le cardinal de toute base de $\eR^m$ est $m$, c'est-à-dire que toute base de $\eR^m$ possède exactement $m$ éléments.

\begin{example}
    La base de \defe{canonique}{canonique!base}\index{base canonique de $\eR^m$} de \( \eR^m\) est la partie $\{e_1,\ldots, e_m\}$, où le vecteur $e_j$ est
    \begin{equation}\nonumber
      e_j=
    \begin{array}{cc}
      \begin{pmatrix}
        0\\\vdots\\0\\1\\ 0\\\vdots\\0
      \end{pmatrix} &
      \begin{matrix}
        \quad\\\quad\\\leftarrow\textrm{j-ème} \quad\\\quad\\\quad\\
      \end{matrix}
    \end{array}.
    \end{equation}
    La composante numéro $j$ de $e_i$ est $1$ si $i=j$ et $0$ si $i\neq j$. Cela s'écrit $(e_i)_j=\delta_{ij}$ où $\delta$ est le \defe{symbole de Kronecker}{Kronecker} défini par
    \begin{equation}
        \delta_{ij}=\begin{cases}
            1	&	\text{si }i=j\\
            0	&	 \text{si }i\neq j
        \end{cases}
    \end{equation}
    Les éléments de la base canonique de $\eR^m$ peuvent donc être écrits $e_i=\sum_{k=1}^m\delta_{ik}e_k$.
\end{example}

Le théorème suivant est essentiellement une reformulation du théorème~\ref{ThonmnWKs}.
\begin{theorem} \label{ThoMGQZooIgrXjy}
    Soit \( E\) un espace vectoriel de dimension finie et \( \{ e_i \}_{i\in I}\) une partie génératrice de \( E\).

    \begin{enumerate}
        \item       \label{ITEMooTZUDooFEgymQ}
            Il existe \( J\subset I\) tel que \( \{ e_i \}_{i\in J}\) est une base. Autrement dit : de toute partie génératrice nous pouvons extraire une base.
        \item       \label{ITEMooCJQGooXwjsfm}
            Soit \( \{ f_1,\ldots, f_l \}\) une partie libre. Alors nous pouvons la compléter en utilisant des éléments \( e_i\). C'est-à-dire qu'il existe \( J\subset I\) tel que \( \{ f_k \}\cup\{ e_i \}_{i\in J}\) soit une base.
    \end{enumerate}
\end{theorem}

\begin{proposition}     \label{PROPooVEVCooHkrldw}
    Si \( E\) est un espace vectoriel de dimension finie \( n\), alors 
    \begin{enumerate}
        \item
            toute partie libre contenant \( n\) éléments est une base,
        \item
            toute partie génératrice contenant \( n\) éléments est une base.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Une partie libre contenant \( n\) éléments peut être étendue en une base; si ladite extension est non triviale (c'est-à-dire qu'on ajoute vraiment au moins un élément) une telle base contiendra une partie de \( n+1\) éléments qui serait liée par le lemme~\ref{LemytHnlD}.

    Pour l'autre assertion, soit une partie génératrice \( \{ v_i \}_{i\in I}\) où \( I\) contient \( n\) éléments. Par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooCJQGooXwjsfm} il existe \( J\subset I\) tel que \( \{ v_j \}_{j\in J}\) soit une base. Si l'inclusion \( J\subset I\) est stricte, alors la base \( \{ v_j \}_{j\in J}\) contiendrait moins de\( n\) éléments, ce qui serait en contradiction avec le théorème \ref{ThonmnWKs}\ref{ITEMooFBUAooSSZxgx}.
\end{proof}

\begin{definition}\label{DefCodimension}
Soit \( F\) un sous-espace vectoriel de l'espace vectoriel \( E\). La \defe{codimension}{codimension} de \( F\) dans \( E\) est
\begin{equation}
    \codim_E(F)=\dim(E/F).
\end{equation}
\end{definition}

\begin{probleme}
Voir que $E/F$ a une structure vectoriel, expliciter sa dimension en fonction de celles de $E$ et $F$.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Et en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------

Dans ZFC, en dimension infinie, il existe aussi une base pour tout espace vectoriel ainsi qu'un théorème de la base incomplète. Nous ne parlerons pas de ce qu'il se passe lorsque nous ne considérons que ZF\footnote{Si vous ne savez pas ce que signifient les sigles «ZF» et «ZFC» vous ne devriez pas être en train de lire ceci, et encore moins penser à le resservir à un jury d'agrégation.}.

\begin{lemma}[\cite{ooXEFKooHikcdE}]        \label{LEMooSSRXooIyfgNz}
    Soient un \( \eK\)-espace vectoriel \( E\) et un sous-espace vectoriel \( V\) de \( E\). Soient encore deux sous-espaces vectoriels \( W_1\) et \( W_2\) tels que
    \begin{enumerate}
        \item
            \( V\cap W_1=\{ 0 \}\);
        \item
            \( V+W_2=E\).
    \end{enumerate}
    Alors il existe un supplémentaire \( W\) de \( V\) tel que \( W_1\subset W\subset W_2\).
\end{lemma}

Juste une remarque : dans le Frido le symbole «\( \subset\)» ne signifie pas une inclusion stricte.

\begin{proof}
    Nous divisons en petits morceaux.
    \begin{subproof}
        \item[Un gros ensemble]
            Soit \( \mA\) l'ensemble des sous-espaces vectoriels \( S\) de \( E\) tels que \( W_1\subset S\subset W_2\) et \( S\cap V=\{ 0 \}\). Vu que \( W_1\subset \mA\), cet ensemble n'est pas vide. De plus \( \mA\) est partiellement ordonné pour l'inclusion.
        \item[\( \mA\) est inductif]
            Nous prouvons maintenant que \( \mA\) est inductif\footnote{Définition~\ref{DefGHDfyyz}.}. Pour cela, soit une partie \( \mA'\) totalement ordonnée et \( U=\bigcup_{A\in \mA'}A\).

            Alors, la partie \( U\) est un sous-espace vectoriel de \( E\). En effet si \( x,y\in U\), alors il existe \( A_1,A_2\in\mA'\) tels que \( x\in A_1\) et \( y\in A_2\). Vu que \( \mA'\) est totalement ordonné, l'un des ensembles parmi \( A_1\) et \( A_2\) est inclus dans l'autre. Sans perdre de généralité, disons \( A_1\subset A_2\). Alors les opérations s'effectuent dans \( A_2 \) : nous avons \( x,y\in A_2\), et donc \( \lambda x\in A_2\subset U\) ainsi que \( x+y\in A_2\subset U\).

            De plus, \( U \) contient \( W_1 \), et est contenu dans \( W_2\). Ainsi, \( U\in \mA\) et majore \( \mA'\) pour l'inclusion. En bref, \( \mA\) est bien inductif.
        \item[Utilisation de Zorn]

            Le lemme de Zorn~\ref{LemUEGjJBc} nous donne alors un maximum \( W\) de \( \mA\). Ce maximum vérifie
            \begin{enumerate}
                \item
                    \( W\cap V=\{ 0 \}\),
                \item
                    \( W_1\subset W\subset W_2\),
                \item
                    pour tout \( W'\in\mA\), nous avons \( W'\subset W\) parce que \( W\) est maximum.
            \end{enumerate}
        \item[Supplémentaire]
            Montrons que ce \( W\) est un supplémentaire de \( V\). Soit \( x\in E\). Le but est de trouver une décomposition de \( x\) en somme d'un élément de \( W\) et un de \( V\). Vu que \( V+W_2=E\) nous avons \( v\in V\) et \( w_2\in W_2\) tels que 
            \begin{equation}
                x=v+w_2. 
            \end{equation}
            Si \( w_2\in W\) alors c'est fait. Sinon \ldots

            Soit \( X=\Span\{ W,w_2 \}\). Vu que \( X\) contient strictement \( W\) et que \( W\) est maximum dans \( \mA\), la partie \( X\) n'est pas un élément de \( \mA\). Vu que \( X\) est un sous-espace vectoriel de \( E\) tel que \( W_1\subset X\subset W_2\), la seule possibilité pour que \( X\) ne soit pas dans \( \mA\) est que \( X\cap V\neq \{ 0 \}\). Soit donc \( y\neq 0\) dans \( X\cap V\). Par définition de \( X\),
            \begin{equation}\label{EqDecompo55:296}
                y=w'+\lambda w_2
            \end{equation}
            pour \( w'\in W\), \( w_2\in W_2\) et \( \lambda\in \eK\). Nous avons \( \lambda\neq 0\), sinon nous aurions \( y\in W\cap V \) et donc \(y = 0 \) puisque \( W \) est dans \( \mA \). La décomposition \eqref{EqDecompo55:296} permet alors d'écrire \( w_2=(y-w')/\lambda\) et finalement
            \begin{equation}
                x=v+\frac{1}{ \lambda }(y-w')=\underbrace{v+\frac{1}{ \lambda }y}_{\in V}-\underbrace{\frac{1}{ \lambda }w'}_{\in W}.
            \end{equation}
            La somme d'espaces vectoriels \( E=V+W\) est donc établie.
    \end{subproof}
\end{proof}

\begin{corollary}
    Tout sous-espace vectoriel d'un espace vectoriel possède un supplémentaire.
\end{corollary}

\begin{proof}
    C'est un cas particulier du lemme~\ref{LEMooSSRXooIyfgNz}.
\end{proof}

\begin{proposition}
    Tout espace vectoriel (non réduit à \( \{ 0 \}\)) possède une base.
\end{proposition}

\begin{proof}
    Soit \( \mA\) l'ensemble des familles libres de \( E\). Il n'est pas vide parce que \( \{ v \}\) en est une dès que \( v\) est non nul dans \( E\). Rapidement :
    \begin{itemize}
        \item l'ensemble \( \mA\) est ordonné pour l'inclusion,
        \item si \( \mA'\) est une partie totalement ordonnée, l'union est un majorant,
        \item donc \( \mA\) est inductif,
        \item soit un maximum \( F\) de \( \mA\).
    \end{itemize}
    La partie \( F\) est libre parce qu'elle est dans \( \mA\). Elle est génératrice parce que si \( v\) n'est pas dans \( \Span(F)\) alors la partie \( F\cup\{ v \}\) est encore libre, et majore strictement $F$ pour l'inclusion, ce qui n'est pas possible.

    Donc \( F\) est une base de \( E\).
\end{proof}

\begin{theorem}[Base incomplète, dimension quelconque]      \label{THOooOQLQooHqEeDK}
    Soit une partie \( \{ e_i \}_{i\in I}\) génératrice de l'espace vectoriel \( E\) (ici, \( I\) est un ensemble quelconque). Soit \( I_0\in I\) tel que \( \{ e_i \}_{i\in I_0}\) soit libre.

    Alors il existe \( I_1\) tel que \( I_0\subset I_1\subset I\) tel que \( \{ e_i \}_{i\in I_1}\) soit une base de \( E\).
\end{theorem}

Note : une telle partie \( I_0\) existe en prenant un singleton. Mais l'existence n'est pas le sujet ici.

\begin{proof}
    Soit \( \mA\) l'ensemble des parties \( J\) de \( I\) telles que \( I_0\subset J\subset I\) et telles que \( \{ e_i \}_{i\in J}\) soit libre.

    Encore une fois, \( \mA\) est inductif pour l'ordre partiel donné par l'inclusion. Soit \( J\) un maximum. Vu que \( J\in\mA\), la partie \( \{ e_i \}_{i\in J}\) est libre. Mais elle est également génératrice parce que si \( e_k\) n'est pas dedans, \( J\) ne serait pas maximum, étant majorée par \( J\cup\{ k \}\).

    Donc \( \{ e_i \}_{i\in J}\) engendre tous les \( e_i\) avec \( i\in I\) et donc tous les éléments de \( E\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Espace librement engendré}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{ooGNYOooGZKGba}]       \label{DEFooCPNIooNxsYMY}
    Soient un ensemble \( S\) et un corps \(\eK \). L'espace vectoriel \defe{librement engendré}{librement engendré} sur \( S\), noté \( F_{\eK}(S)\) est l'ensemble des applications \( S\to \eK\) qui sont non-nulles en un nombre fini de points de \( S\).

    Autrement dit, \( \sigma\colon S\to \eK\) est dans \( F_{\eK}(S) \) si \( \{ x\in S\tq \delta(x)\neq 0 \}\) est fini\footnote{Parce que nous l'aimons bien, nous ne résistons pas à faire un renvoi vers la définition \ref{DefEOZLooUMCzZR}.}.
\end{definition}

Le lemme suivant donne tout son sens à l'expression «librement» engendré. Il dit que \( F(S)\) possède une base indexée par \( S\) lui-même.
\begin{lemma}       \label{LEMooLOPAooUNQVku}
    L'ensemble des applications \( \delta_s\) données par
    \begin{equation}
        \begin{aligned}
            \delta_s\colon S&\to \eK \\
            t&\mapsto \begin{cases}
                1    &   \text{si } t=s\\
                0    &    \text{sinon }
            \end{cases}
        \end{aligned}
    \end{equation}
    avec \( s\in S\) forment une base\footnote{Définition \ref{DEFooNGDSooEDAwTh}.} de \( F(S)\).
\end{lemma}

\begin{proof}
    Pour prouver que les \( \delta_s\) sont générateurs, nous considérons \( g\colon S\to \eK\) non nul sur la partie finie \( \{ s_i \}_{i\in I}\) de \( S\). Alors nous avons
    \begin{equation}
        g=\sum_{i\in I}g(s_i)\delta_{s_i}.
    \end{equation}
    
    Pour prouver que les \( \delta_s\) forment une partie libre, nous supposons avoir \( \lambda_i\in \eK\) tels que
    \begin{equation}
        g=\sum_{i\in I}\lambda_i\delta_{s_i}=0
    \end{equation}
    Soit \( j\in I\). Nous avons
    \begin{equation}
        0=f(s_j)=\sum_{i\in I}\lambda_i \underbrace{\delta_{s_i}(s_j)}_{=\delta_{ij}}=\lambda_j.
    \end{equation}
    Donc les coefficients \( \lambda_i\) sont tous nuls, et nous avons prouvé que la partie est libre.
\end{proof}

Il est parfois pratique d'écrire les éléments de \( F(S)\) comme sommes «formelles» d'éléments de \( S\). Cela va encore lorsque \( S\) est un ensemble n'ayant aucune somme bien définie. 

Mais attention : si \( S=\eR\), l'élément \( 4+7\) de \( F(\eR)\) n'est pas \( 11\). L'élément \( 11\) de \( F(\eR)\) est un élément complètement différent. Bref, il n'est pas judicieux d'écrire les éléments de \( F(S)\) comme des combinaisons linéaires d'éléments de \( S\). Pour \( x\in S\) il vaut mieux écrire explicitement \( \delta_x\) que \( x\). La somme \( \delta_x+\delta_y\) est parfaitement bien définie dans l'esemble des applications de \( S\) vers \( \eK\).

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooULVAooXJuRmr}
    Soient des espaces vectoriels \( E \) et \( F\) sur le corps \( \eK\). Une application \( T\colon E\to F\) est dite \defe{linéaire}{linéaire!application} si
    \begin{itemize}
        \item $T(x+y)=T(x)+T(y)$ pour tout $x$ et $y$ dans \( E\),
        \item $T(\lambda x)=\lambda T(x)$ pour tout $\lambda$ dans $\eK$ et \( x\) dans \( E\).
    \end{itemize}
\end{definition}
Si vous avez bien suivi, les égalités dans la définition~\ref{DEFooULVAooXJuRmr} sont des égalités dans \( F\).

\begin{lemmaDef} \label{DefDQRooVGbzSm}
    L'ensemble de toutes les applications linéaires de \( E\) vers \( F\) est noté \( \aL(E,F)\)\nomenclature{$\aL(E,F)$}{Ensemble des applications linéaires de $E$ dans $F$} et devient un espace vectoriel sur \( \eK\) avec les définitions suivantes :
    \begin{enumerate}
        \item
            \( (T_1+T_2)(x)=T_1(x)+T_2(x)\),
        \item
            \( (\lambda T)(x)=\lambda T(x)\).
    \end{enumerate}
\end{lemmaDef}

\begin{example}
Pour tout $b$ dans $\eR$ la fonction $T_b(x)= bx$ est une application linéaire de $\eR$ dans $\eR$. En effet,
\begin{itemize}
\item  $T_b(x+y)= b(x+y)= bx + by = T_b(x)+T_b(y)$,
\item $T_b(ax)=b(ax)= abx = a T_b(x)$.
\end{itemize}
De la même façon on peut montrer que la fonction $T_{\lambda}$ définie par $T_{\lambda}(x)=bx$ est un application linéaire de $\eR^m$ dans $\eR^m$ pour tout $\lambda$ dans $\eR$ et $m$ dans $\eN$.
\end{example}

\begin{example}\label{ex_affine}
	Soit $m=n$. On fixe $\lambda$ dans $\eR$ et $v$ dans $\eR^m$. L'application $U_{\lambda}$ de $\eR^m$ dans $\eR^m$ définie par $U_{\lambda}(x)=\lambda x+v$ n'est pas une application linéaire lorsque \( v \neq 0 \), parce que si \( a \) est un réel différent de \(0 \) et \( 1 \), alors \( av \neq v \), d'où
\[
U_{\lambda}(ax)=\lambda(ax)+v\neq a(\lambda x+v) =a U_{\lambda}(x).
\]
\end{example}

\begin{example}\label{exampleT_A}
	Soit $A$ une matrice fixée de $\mathcal{M}_{n\times m}$\nomenclature{$\mathcal{M}_{n\times m}$}{l'ensemble des matrices $n\times m$}. La fonction $T_A\colon \eR^m\to \eR^n$ définie par $T_A(x)=Ax$ est une application linéaire. En effet,
    \begin{itemize}
        \item  $T_A(x+y)= A(x+y)= Ax + Ay = T_A(x)+T_A(y)$,
        \item $T_A(ax)=A(ax)= a(Ax) = a T_A(x)$.
    \end{itemize}
\end{example}

On peut observer que, si on identifie $\mathcal{M}_{1\times 1}$ et $\eR$, on obtient le premier exemple comme cas particulier.

\begin{definition}[Quelques ensembles d'applications linéaires]      \label{DEFooOAOGooKuJSup}
    Soient \( E\) et \( F\) des espaces vectoriels.
    \begin{itemize}
        \item
            L'ensemble des applications linéaires de \( E\) vers \( F\) est noté $\aL(E,F)$, comme déjà dit en \ref{DefDQRooVGbzSm}.
        \item Une application linéaire \( E\to E\) est un \defe{endomorphisme}{endomorphisme} de \( E\). L'ensemble des endomorphismes de \( E\) est noté \( \End(E)\)\nomenclature[B]{$\End(E)$}{les endomorphismes de \( E\)}.
        \item Un endomorphisme bijectif est un \defe{automorphisme}{automorphisme!d'espace vectoriel}. L'ensemble des automorphismes de \( E\) est noté \( \Aut(E)\)\nomenclature[B]{$\Aut(E)$}{automorphisme de l'espace vectoriel \( E\)}.
        \item
            Une application linéaire bijective \( E\to F\) est un \defe{isomorphisme}{isomorphisme!espaces vectoriels} d'espace vectoriel. L'ensemble des isomorphismes \( E\to F\) est noté\footnote{Le fait d'utiliser une notation similaire à celle des matrices inversibles n'est pas anodine: le lecteur en est sans doute conscient.} \( \GL(E,F)\).
    \end{itemize}
\end{definition}

\begin{remark}
    Les ensembles définis en~\ref{DEFooOAOGooKuJSup} concernent la structure d'espace vectoriel seulement. Lorsque nous verrons la notion d'espace vectoriel normé, nous demanderons de plus la continuité, laquelle n'est pas automatique en dimension infinie. Voir aussi les définitions~\ref{DEFooTLQUooJvknvi}.
\end{remark}

\begin{definition}
    Si \( E\) est un espace vectoriel, si \( X\) est un espace vectoriel, et si \( f\colon X\to E\) est une application, le \defe{noyau}{noyau!vers un espace vectoriel} de \( f\) est le noyau de \( f\) lorsque \( E\) est vu comme un groupe pour l'addition\footnote{Définition \ref{DEFooWBIYooGNRYOp}.}, c'est-à-dire la partie
    \begin{equation}
        \ker(f)=\{ x\in X\tq f(x)=0 \}.
    \end{equation}
\end{definition}

\begin{proposition}     \label{PROPooRLLPooKYzsJp}
    Le noyau d'une application linéaire est un sous-espace vectoriel.
\end{proposition}

\begin{proof}
    Soit une application linéaire \( f\colon E\to F\). Si \( x,y\in \ker(f)\) et si \( \lambda\in \eK\) alors
    \begin{equation}
        f(x+y)=f(x)+f(y)=0+0=0,
    \end{equation}
    donc \( x+y\in \ker(f)\) et
    \begin{equation}
        f(\lambda x)=\lambda f(x)=0,
    \end{equation}
    donc \( \lambda x\in \ker(f)\).
\end{proof}

\begin{proposition}
    Si \( E\) et \( F\) sont des espaces vectoriels de dimension \( n\) et si \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_i \}_{i=1,\ldots, n}\) sont des bases respectivement de \( E\) et \( F\), alors il existe une unique application linéaire \( T\colon E\to F\) telle que \( T(e_i)=f_i\) pour tout \( i\).
\end{proposition}

\begin{proof}
    En deux parties.\begin{subproof}
        \item[Existence]
            Soit \( v\in E\). Vu que \( \{ e_i \}\) est une base, il se décompose de façon unique en \( v=\sum_iv_ie_i\). Alors définir
            \begin{equation}
                T(v)=\sum_iv_if_i
            \end{equation}
            est une bonne définition et satisfait aux exigences.
        \item[Unicité]
            Soient \( T\) et \( U\) satisfaisant aux exigences. Alors pour tout \( i\) nous avons \( T(e_i)=U(e_i)\). Si \( v\in E\) s'écrit de la forme \( v=\sum_iv_ie_i\) alors la linéarité impose \( T(v)=\sum_iv_iT(e_i)=\sum_iv_iU(e_i)=U(v)\). Donc \( T = U\).
    \end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooJXFIooKDzRWR}
    Soient des espaces vectoriels \( V\) et \( W\) de dimension finie. Soient des bases \( \{e_i\}\) de \( V\) et \( \{f_{\alpha}\}\) de \( W\). Nous posons
    \begin{equation}
        \begin{aligned}
            \varphi_{i\alpha}\colon V&\to W \\
            v&\mapsto v_if_{\alpha} 
        \end{aligned}
    \end{equation}
    où \( v_i\) est défini par la décomposition (unique) \( v=\sum_iv_ie_i\). 

    Alors :
    \begin{enumerate}
        \item
            La partie \( \{\varphi_{i\alpha}\} \) est une base de \( \aL(V,W)\).
        \item       \label{ITEMooPMLWooNbTyJI}
            Au niveau des dimensions : \( \dim\big( \aL(V,W) \big)=\dim(V)\dim(W)\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Il faut prouver que \( \{\varphi_{i\alpha}\}\) est libre et générateur.

    \begin{subproof}
        \item[Générateur]
            Soit une application linéaire \( b\colon V\to W\). En décomposant \( b(v)\) dans la base \( \{f_{\alpha}\}\), nous définissons \( b_{\alpha}\colon V\to \eK\) par
            \begin{equation}
                b(v)=\sum_{\alpha}b_{\alpha}(v)f_{\alpha}.
            \end{equation}
            Nous posons \( b_{\alpha i}=b_{\alpha}(e_i)\). Ainsi,
            \begin{equation}
                b(v)=\sum_{\alpha}v_ib_{\alpha i}f_{\alpha}=\sum_{\alpha i}b_{\alpha i}\varphi_{i\alpha}(v).
            \end{equation}
            Donc \( b\) peut être écrit comme combinaison linéaire des \( \varphi_{i\alpha}\).

        \item[Libre]
            Supposons que \( \sum_{i\alpha}a_{i\alpha}\varphi_{i\alpha}=0\) pour certains coefficients \( a_{i\alpha}\in \eK\). Nous avons, pour tout \( v\in V\) :
            \begin{equation}
                0=\sum_{i\alpha}a_{i\alpha}\varphi_{i\alpha}(v)=\sum_{i\alpha}a_{i\alpha}v_if_{\alpha},
            \end{equation}
            mais comme les \( f_{\alpha}\) forment une base, chaque terme de la somme sur \( \alpha\) est nul :
            \begin{equation}
                \sum_ia_{i\alpha}v_i=0.
            \end{equation}
            Et comme cela est valable pour tout \( v\) et donc pour tout choix de \( v_i\), nous avons \( a_{i\alpha}=0\) pour tout \( i\) et pour tout \( \alpha\).
    \end{subproof}
    La formule de dimension est simplement la cardinalité de la base trouvée; c'est la définition \ref{DEFooWRLKooArTpgh}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Linéarité et bases}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{ooZLSSooMYdbEz}]
    Soient deux espaces vectoriels \( E\) et \( F\). Une application linéaire\footnote{Définition \ref{DEFooULVAooXJuRmr}.} \( f\colon E\to F\) est injective si et seulement si \( \ker\{ f \}=\{ 0 \}\).
\end{proposition}

\begin{proof}
    Nous supposons que \( f\) est injective. Si \( x\in\ker(f)\), alors \( f(x)=0\). Or \( f\) est linéaire, donc \( f(0)=0\). Nous avons donc \( f(x)=f(0)\) et donc \( x=0\) parce que \( f\) est injective.

    Dans l'autre sens, soient \( x,y\) tels que \( f(x)=f(y)\). Par linéarité de \( f\) nous avons \( f(x-y)=0\), et donc \( x-y=0\) parce que \( \ker(f)=\{0\}\). Donc \( x=y\) et \( f\) est injective.
\end{proof}

\begin{proposition}[\cite{ooZLSSooMYdbEz}]      \label{PROPooZFKZooBGLSex}
    Soit \( f\in \aL(E,F)\) où \( E\) et \( F\) sont deux espaces vectoriels.
    \begin{enumerate}
        \item   \label{ITEMooPPMEooIaZqtm}
            Si \( f\) est injective et si \( \{v_i\}_{i\in I}\) est libre, alors \( \{f(v_i)\}_{i\in I}\) est libre.
        \item   \label{ITEMooOZSPooQBrDGi}
            Si \( f\) est surjective et si \( \{v_i\}_{i\in I}\) est génératrice, alors \( \{f(v_i)\}_{i\in I}\) est génératrice.
        \item   \label{ITEMooOIEYooIfdFnv}
            Si \( f\) est une bijection, alors l'image d'une base par \( f\) est une base.
    \end{enumerate}
\end{proposition}

\begin{proof}
    En trois parties.
    \begin{subproof}
        \item[\ref{ITEMooPPMEooIaZqtm}]
            Nous devons montrer que \( \{f(v_j)\}_{j\in J}\) est libre pour tout \( J\) fini dans \( I\). Soit donc une partie finie \( J\in I\) et des scalaires\footnote{Des éléments du corps de base \( \eK\).} tels que \( \sum_{j\in J}\lambda_jf(v_j)=0\). La linéarité de \( f\) donne\footnote{Voir les propriétés de la définition \ref{DEFooULVAooXJuRmr}.}
            \begin{equation}
                f\big( \sum_{i\in J}\lambda_jv_j \big)=0.
            \end{equation}
            Par injectivité de \( f\) nous avons alors \( \sum_j\lambda_jv_j=0\). Vu que les \( v_j\) eux-même forment une partie libre, nous avons \( \lambda_j=0\) pour tout \( j\in J\).
        \item[\ref{ITEMooOZSPooQBrDGi}]
            Soit \( y\in F\). Vu que \( f\) est surjective, il existe \( x\in E\) tel que \( f(x)=y\). Étant donné que \( \{v\i\}_{i\in I}\) est générateur, il existe une partie finie \( J\subset I\) et des scalaires \( \lambda_j\in \eK\) tels que
            \begin{equation}
                x=\sum_{j\in J}\lambda_jv_j.
            \end{equation}
            En appliquant \( f\) aux deux côtés, et en tenant compte de la linéarité de \( f\),
            \begin{equation}
                y=f(x)=\sum_{j\in J}\lambda_jf(v_j),
            \end{equation}
            ce qui prouve que \( y\) est une combinaison linéaire des \( f(v_j)\).
        \item[\ref{ITEMooOIEYooIfdFnv}]
            Une base est à la fois libre et génératrice et une bijection est à la fois injective et surjective. Les deux premiers points permettent de conclure.
    \end{subproof}
\end{proof}

\begin{corollary}[\cite{MonCerveau}]        \label{CORooXIPKooWThOsr}
    Si \( E\) et \( F\) sont des espaces vectoriels isomorphes de dimensions finies. Alors leurs dimensions sont égales.
\end{corollary}

\begin{proof}
    Vu que \( E\) et \( F\) sont isomorphes, il existe une bijection \( f\colon E\to F\). Par la proposition \ref{PROPooZFKZooBGLSex}\ref{ITEMooOIEYooIfdFnv}, l'image d'une base de \( E\) est une base de \( F\). Donc les espaces \( E\) et \( F\) ont des bases contenant le même nombre d'éléments.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Rang}
%---------------------------------------------------------------------------------------------------------------------------

La proposition~\ref{DefALUAooSPcmyK} et le théorème~\ref{ThoGkkffA} sont valables également en dimension infinie; ce sera une des rares incursions en dimension infinie de ce chapitre.
\begin{propositionDef}\label{DefALUAooSPcmyK}
    L'image d'une application linéaire est un espace vectoriel. La dimension de cet espace est le \defe{rang}{rang} de ladite application linéaire.
\end{propositionDef}

\begin{proof}
    Soit une application linéaire \( f\colon E\to F\). Nous considérons \( v,w\) dans l'image de \( f\) ainsi que \( \lambda\) dans le corps de base commun à \( E\) et \( F\).

    Soient \( v_0\in E\) et \( w_0\in E\) tels que \( v=f(v_0)\) et \( w=f(w_0)\). Alors \( v+w=f(v_0+w_0)\) et \( \lambda v=f(\lambda v_0)\). Donc l'image est bien un espace vectoriel.
\end{proof}

\begin{theorem}[Théorème du rang]       \label{ThoGkkffA}
    Soient \( E\) et \( F\) deux espaces vectoriels (de dimensions finies ou non) et soit \( f\colon E\to F\) une application linéaire. 
    
   Si \( (u_s)_{s\in S}\) est une base de \( \ker(f)\) et si \( \big( f(v_t) \big)_{t\in T}\) est une base de \( \Image(f)\) alors 
   \begin{equation}
   (u_s)_{s\in s}\cup (v_t)_{t\in T}
   \end{equation}
   est une base de \( E\).
    
   En dimension finie, nous avons en plus la formule suivante :
   \begin{equation}
       \rang(f)+\dim\ker f=\dim E,
   \end{equation}
   c'est-à-dire que le rang\footnote{Définition~\ref{DefALUAooSPcmyK}.} de \( f\) est égal à la codimension\footnote{Définition~\ref{DefCodimension}.} du noyau.
\end{theorem}
\index{théorème!du rang}

\begin{proof}
    Nous devons montrer que
    \begin{equation}
          (u_s)_{s\in S}\cup (v_t)_{t\in T}
    \end{equation}
    est libre et générateur.

    Soit \( x\in E\). Nous définissons les nombres \( x_t\) par la décomposition de \( f(x)\) dans la base \( \big( f(v_t) \big)\) :
    \begin{equation}
        f(x)=\sum_{t\in T}x_tf(v_t).
    \end{equation}
    Ensuite le vecteur \( x=\sum_tx_tv_t\) est dans le noyau de \( f\), par conséquent nous le décomposons dans la base \( (u_s)\) :
    \begin{equation}
        x-\sum_tx_tv_t=\sum_{s\in S} x_su_s.
    \end{equation}
    Par conséquent
    \begin{equation}
        x=\sum_sx_su_s+\sum_tx_tv_t.
    \end{equation}

    En ce qui concerne la liberté nous écrivons
    \begin{equation}
        \sum_tx_tv_t+\sum_sx_su_s=0.
    \end{equation}
    En appliquant \( f\) nous trouvons que
    \begin{equation}
        \sum_tx_tf(v_t)=0
    \end{equation}
    et donc que les \( x_t\) doivent être nuls. Nous restons avec \( \sum_sx_su_s=0\) qui à son tour implique que \( x_s=0\).
\end{proof}
Un exemple d'utilisation de ce théorème en dimension infinie sera donné dans le cadre du théorème de Fréchet-Riesz, théorème~\ref{ThoQgTovL}.
\ifbool{isGiulietta}{Il existe une généralisation du théorème du rang pour les variétés différentiables en le théorème \ref{THOooSWKVooTJQsXc}.}{}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]      \label{PROPooQCIXooHIyPPq}
    Soit \( E\), un espace vectoriel de dimension finie sur le corps $\eK$. Soient \( V\) et \( W\) des sous-espaces vectoriels de \( E\). Alors
    \begin{equation}
        \dim(V+W)=\dim(V)+\dim(W)-\dim(V\cap W).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous considérons l'application 
    \begin{equation}
        \begin{aligned}
            \varphi\colon V\times W&\to E \\
            (x,y)&\mapsto x+y. 
        \end{aligned}
    \end{equation}
    C'est une application linéaire dont l'image est \( V+W\). Nous avons donc, pour commencer
    \begin{equation}
        \dim(V+W)=\dim\big( \Image(\varphi) \big).
    \end{equation}
    Nous appliquons à présent le théorème du rang \ref{ThoGkkffA} à l'application \( \varphi\) :
    \begin{subequations}
        \begin{align}
            \dim(V+W)&=\dim\big( \Image(\varphi) \big)\\
            &=\dim(V\times W)- \dim\big( \ker(\varphi) \big)\\
            &=\dim(V)+\dim(W)-\dim\big( \ker(\varphi) \big).
        \end{align}
    \end{subequations}
    Nous devons maintenant étudier \( \ker(\varphi)\). D'abord, \( (v,w)\in V\times W\) appartient à \( \ker(\varphi)\) si et seulement si \( v+w=0\). Nous avons donc
    \begin{equation}
        \ker(\varphi)=\{ (x,-x)\tq x\in V\cap W \}.
    \end{equation}
    Nous montrons à partir de cela que \( \dim\big( \ker(\varphi) \big)=\dim(V\cap W)\) en montrant que l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon V\cap W&\to \ker(\varphi) \\
            x&\mapsto (x,-x) 
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels. D'abord \( \psi\) est injective parce que si \( \psi(x)=\psi(y)\), alors \( (x,-x)=(y,-x)\) et donc \( x=y\). Ensuite, \( \psi\) est surjective parce qu'un élément générique de \( \ker(\varphi)\) est \( (x,-x)=\psi(x)\) avec \( x\in V\cap W\). L'application \( \psi\) étant un isomorphisme d'espaces vectoriels, nous avons bien \( \dim\big( \ker(\varphi) \big)=\dim(V\cap W)\).
\end{proof}

\begin{corollary}       \label{CORooCCXHooALmxKk}
    Soient deux espaces vectoriels \( E\) et \( F\) de même dimensions finies\footnote{Les deux mots sont importants : les dimensions doivent être égales et finies.}. Pour une application linéaire \( f\colon E\to F\), les trois conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item
            \( f\) est injective;
        \item
            \( f\) est surjective;
        \item
            \( f\) est bijective.
    \end{enumerate}
\end{corollary}

\begin{proof}
    Si un endomorphisme \( f\colon E\to E\) est surjectif, alors \( \rang(f)=\dim(E)\), ce qui donne, par le théorème du rang~\ref{ThoGkkffA}, \( \dim\big( \ker(f) \big)=0\), c'est-à-dire que \( f\) est injectif.

    De la même façon, si \( f\) est injective, alors \( \dim\big( \ker(f) \big)=0\), ce qui donne \( \rang(f)=\dim(E)\) ou encore que \( f\) est surjective.
\end{proof}

\begin{example}
    Le corollaire \ref{CORooCCXHooALmxKk} n'est pas correct en dimension infinie. Par exemple en prenant \( f(e_1)=f(e_2)=e_1\) et ensuite \( f(e_k)=e_{k-1}\) pour tout \( k\geq 2\). Cette application est surjective mais pas injective.
\end{example}

Une conséquence du théorème du rang est que les endomorphismes ont un inverse à gauche et à droite égaux (lorsqu'ils existent).
\begin{corollary}
    Soit un endomorphisme \( f\) d'un espace vectoriel de dimension finie. Si \( f\) admet un inverse à gauche, alors
    \begin{enumerate}
        \item
            \( f\) est bijective,
        \item
            \( f\) admet également un inverse à droite,
        \item
            ils sont égaux.
    \end{enumerate}
    Tout cela tient également en remplaçant «gauche» par «droite».
\end{corollary}

\begin{proof}
    Soit \( g\), un inverse à gauche de \( f\) : \( gf=\id\). Cela implique que \( f\) est injective et que \( g\) est surjective, et donc qu'elles sont toutes deux bijectives par le corollaire~\ref{CORooCCXHooALmxKk}. Vu que \( f\) est bijective, elle admet également un inverse à droite, soit \( h\). Nous avons : \( gf=\id\) et \( fh=\id\).

    Alors \( gfh=h\) parce que \( gf=\id\), mais également \( gfh=g\) parce que \( fh=\id\). Donc \( g=h\).\footnote{C'est le même argument que celui employé pour la preuve du lemme~\ref{LEMooECDMooCkWxXf}~\ref{ITEMooOIWTooYqmMPP}, à ceci près que nous devions montrer l'existence de l'inverse à droite.}
\end{proof}
C'est ce corollaire qui nous permet d'écrire \( f^{-1}\) sans plus de précisions dès que \( f\) est une bijection.

\begin{example}[Pas en dimension infinie]
    Tout cela ne fonctionne pas en dimension infinie. Par exemple avec une base \( \{ e_k \}_{k\in \eN}\) nous pouvons considérer l'opérateur
    \begin{equation}
        f(e_k)=e_{k+1}.
    \end{equation}
    Il est injectif, mais pas surjectif. Si on pose
    \begin{equation}
        g(e_k)=\begin{cases}
            e_{k-1}    &   \text{si } k\geq 1\\
            0    &    \text{si } k=0
        \end{cases}
    \end{equation}
    alors nous avons \( gf=\id\), mais pas \( fg=\id\) parce que ce \( (fg)(e_0)=0\).
\end{example}

\begin{lemma}       \label{LEMooRZDTooEuLTrO}
    Si \( E\) et \( F\) sont des espaces vectoriels et si \( f\colon E\to F\) est une application linéaire inversible, alors son inverse est également linéaire.
\end{lemma}

\begin{proof}
    Nous avons \( f^{-1}(x+y)=f^{-1}(x)+f^{-1}(y)\). En effet,
    \begin{equation}
        f\big( f^{-1}(x)+f^{-1}(y) \big)=f\big( f^{-1}(x) \big)+f\big( f^{-1}(y) \big)=x+y.
    \end{equation}
    De la même façon,
    \begin{equation}
        f\big( \lambda f^{-1}(x) \big)=\lambda x, 
    \end{equation}
    donc \( f^{-1}(\lambda x)=\lambda f^{-1}(x)\).
\end{proof}

\begin{proposition}     \label{PROPooHLUYooNsDgbn}
    Soient un espace vectoriel \( E\) de dimension finie, un endomorphisme \( f\colon E\to E\) et une partie \( \{v_i\}_{i\in I}\) tel que \( \{f(v_i)\}_{i\in I}\) soit une base.

    Alors \( \{v_i\}_{i\in I}\) est une base.
\end{proposition}

\begin{proof}
    Soit \( x\in E\). Il existe une partie finie \( J\subset I\) et des scalaires \( \lambda_j\) tels que 
    \begin{equation}
        x=\sum_j\lambda_jf(v_j)=f\big( \sum_j\lambda_jv_j \big),
    \end{equation}
    ce qui prouve que \( f\) est surjective. Le corollaire \ref{CORooCCXHooALmxKk} nous dit alors que \( f\) est une bijection. L'application inverse est également linéaire par le lemme \ref{LEMooRZDTooEuLTrO}.

    Une application linéaire bijective (comme \( f^{-1}\)) transforme une base en une base par la proposition \ref{PROPooZFKZooBGLSex}. Donc 
    \begin{equation}
        f^{-1}\big( \{f(v_i)\} \big)
    \end{equation}
    est une base.
\end{proof}

\begin{proposition}     \label{PROPooADESooATJSrH}
    Soit un espace vectoriel \( E\) de dimension finie et deux applications linéaires \( f,g\colon E\to E\) telles que \( g\circ f=\id\). Alors \( f\) et \( g\) sont bijectives.
\end{proposition}

\begin{proof}
    En plusieurs étapes
    \begin{subproof}
        \item[\( f\) est injective]
            Si \( f(x)=f(y)\), alors en appliquant \( g\) nous avons 
            \begin{equation}
                g\big( f(x) \big)=g\big( f(y) \big),
            \end{equation}
            ce qui donne \( x=y\).
        \item[\( f\) est surjective]
            C'est maintenant le corollaire \ref{CORooCCXHooALmxKk}.
        \item[\( g\) est surjective]
            Pour tout \( x\in E\) nous avons \( g\big( f(x) \big)=x\). Donc l'image de \( f(E)\) par \( g\) est $E$. 
        \item[\( g\) est injective]
            C'est maintenant le corollaire \ref{CORooCCXHooALmxKk}.
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Injection, surjection}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
  Une application $S: \eR^m\to\eR^n$ est dite \defe{affine}{affine (application)} si elle est la somme d'une application linéaire et d'une application constante. Autrement dit, $S$ est affine s'il existe $T: \eR^m\to\eR^n$, linéaire, telle que $S(x)-T(x)$ soit un vecteur constant dans $\eR^n$.
\end{definition}

\begin{example}     \label{EXooSUTAooEaQCuZ}
	Les exemples les plus courants d'applications affines sont les droites et les plans ne passant pas par l'origine.
	\begin{description}
		\item[Les droites] Une droite dans $\eR^2$ (ou $\eR^3$) qui ne passe pas par l'origine est l'image d'une fonction de la forme $s(t) =u t +v$, avec $t \in \eR$, et $u$ et $v$ dans $\eR^2$ ou $\eR^3$ selon le cas. 

		En choisissant des coordonnées adéquates, les droites peuvent être aussi vues comme graphes de fonctions affines. Dans le cas de $\eR^2$, on retrouve la fonction de l'exemple~\ref{ex_affine}, pour \( n = m = 1 \).

		\item[Les plans]
			De la même façon nous savons que tout plan qui ne passe pas par l'origine dans $\eR^3$ est le graphe d'une application affine, $P(x,y)= (a,b)^T\cdot(x,y)^T+(c,d)^T$, lorsque les coordonnées sont bien choisies.
	\end{description}
\end{example}

\begin{lemma}[\cite{ooEPEFooQiPESf}]        \label{LEMooDAACooElDsYb}
    Soit une application linéaire \( f\colon E\to F\).
    \begin{enumerate}
        \item       \label{ITEMooEZEWooZGoqsZ}
            L'application \( f\) est injective si et seulement s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id|_E\).
        \item
            L'application \( f\) est surjective si et seulement s'il existe \( g\colon F\to E\) telle que \( f\circ g=\id|_F\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Nous démontrons séparément les deux affirmations.
    \begin{enumerate}
        \item
            Si \( f\) est injective, alors \( f\colon E\to \Image(f)\) est un isomorphisme. Si $V$ est un supplémentaire de \( \Image(f)\) dans \( F\) (c'est-à-dire \( F=\Image(f)\oplus V\)) alors nous pouvons poser \( g(x+v)=f^{-1}(x)\) où \( x+v\) est la décomposition (unique) d'un élément de \( F\) en \( x\in\Image(f)\) et \( v\in V\). Avec cela nous avons bien \( g\circ f=\id\).

            Inversement, s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id\) alors \( f\colon E\to E\) doit être injective. Parce que si \( f(x)=0\) avec \( x\neq 0\) alors \( (g\circ f)(x)=0\neq x\).
        \item
            Si \( f\) est surjective nous pouvons choisir des éléments \( x_1,\ldots, x_p\) dans \( E\) tels que \( \{ f(x_i) \}\) soit une base de \( F\). Ensuite nous définissons
            \begin{equation}
                \begin{aligned}
                    g\colon F&\to E \\
                    \sum_k a_k f(x_k)&\mapsto \sum_k a_k x_k.
                \end{aligned}
            \end{equation}
            Cela donne \(  f\circ g=\id|_F\) parce que si \( v\in F\) alors \( v=\sum_kv_kf(x_k)\) avec \( v_k\in \eK\), et nous avons
            \begin{equation}
                (f\circ g)(v)=\sum_k v_k (f\circ g) \left(f(x_k)\right)
                             =f\left( \sum_k v_k x_k \right)
                             =\sum_k v_k f(x_k) = v.
            \end{equation}

            Inversement, s'il existe \( g\colon F\to E\) tel que \( f\circ g=\id\) alors \( f\) doit être surjective parce que
            \begin{equation}
                F=\Image(f\circ g)=f\big( \Image(g) \big)\subset \Image(f).
            \end{equation}
    \end{enumerate}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les matrices et les applications linéaires sont deux choses différentes. Une application linéaire\footnote{Définition \ref{DEFooULVAooXJuRmr}.} est une application d'un espace vectoriel vers un autre, et une matrice est un simple tableau de nombres sur lesquels nous définissons des opérations, de telle sorte à fournir une structure d'espace vectoriel. Le lien entre ces opérations et les opérations correspondantes sur les applications linéaires sera fait plus tard.

%TODO: fixer les notations d'ensembles M( n x m, K) et matrice identité dans l'index des notations.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit un anneau \( \eA\) ainsi que des entiers \( m\), \( n\) strictement positifs. L'ensemble \( \eM(n\times n,\eA)\) est l'ensemble des applications
    \begin{equation}
        \{ 1,\ldots, n \}\times \{ 1,\ldots, m \}\to \eA,
    \end{equation}
    et est appelé ensemble des \defe{matrices}{matrice} \(n\times m\) sur \( \eA \).
\end{definition}
Si \( A\) est une matrice, nous notons \( A_{ij}\) au lieu de \( A(i,j)\) l'image de \( (i,j)\) par l'application \( A\).


\begin{definition}
Quelques ensembles de matrices particuliers.
  \begin{enumerate}
  \item Si \( n=m\), alors:
  \begin{itemize}
    \item nous disons que la matrice est \defe{carrée}{carrée!matrice},
    \item nous notons \( \eM(n,\eA)\) pour \( \eM(n\times n,\eA)\),
    \item \( n \) est appelée \defe{ordre}{ordre!d'une matrice carrée} de la matrice.
  \end{itemize}
  \item Si \( n = 1 \), alors la matrice est appelée \defe{matrice-ligne}{matrice-ligne}.
    \item Si \( m = 1 \), alors la matrice est appelée \defe{matrice-colonne}{matrice-colonne}.
  \end{enumerate}
\end{definition}

\begin{normaltext}
    On note les isomorphismes naturels \( \eM(1\times m,\eA) \simeq \eA^m\) et \( \eM(n\times 1,\eA) \simeq \eA^n\).
\end{normaltext}

\begin{lemmaDef}
    L'ensemble \( \eM(n\times m, \eA)\) muni des opérations
    \begin{description}
        \item[Somme] \( (A+B)_{ij}=A_{ij}+B_{ij}\),
        \item[Produit par un scalaire] \( (\lambda A)_{ij}=\lambda A_{ij}\),
    \end{description}
    pour tout \( A,B\in \eM(n\times m,\eA ) \) et \( \lambda\in \eA \) est un \( \eA\)-module (définition \ref{DEFooHXITooBFvzrR}).
\end{lemmaDef}

\begin{lemmaDef}
    Avec la multiplication
    \begin{equation}
        \begin{aligned}
             \eM(n\times p,\eA)\times \eM(p\times m,\eA)&\to \eM(n\times m,\eA) \\
             (A,B)&\mapsto (AB)_{ij}=\sum_{k=1}^pA_{ik}B_{kj},
        \end{aligned}
    \end{equation}
    l'espace \( \eM(n,\eK)\) est une \( \eK\)-algèbre\footnote{Définition \ref{DefAEbnJqI}.}.
\end{lemmaDef}

\begin{definition}
    Pour un élément \( A\in \eM(n\times m, \eA)\) nous définissons encore
    \begin{description}
        \item[La transposée] \( A^t_{ij}=A_{ji}\),
        \item[La trace] \( \tr(A)=\sum_iA_{ii}\).
    \end{description}
\end{definition}


\begin{remark}
    Quelque remarques directes sur les définitions.
    \begin{enumerate}
        \item
            La motivation de cette définition pour le produit apparaîtra plus loin, mais le Frido n'étant pas un livre d'introduction, j'imagine que le lecteur a déjà une idée.
        \item
            Nous verrons plus loin en \ref{SUBSECooGPXVooEYwIiJ} que la définition de transposée d'une application linéaire n'est pas tout à fait évidente; elle sera la définition \ref{DefooZLPAooKTITdd}.

            Ici nous avons bien défini la transposée d'une matrice, pas d'une application linéaire.
    \end{enumerate}
\end{remark}

\begin{remark}
    Quelque remarques à propos de structures supplémentaires.
\begin{enumerate}
    \item Nous utiliserons (presque) tout le temps des matrices à coefficients dans un corps. Il est clair que, si \( \eK \) est un corps (commutatif), alors \( \eM(n\times m,\eK) \) a une structure d'espace vectoriel sur \( \eK \).
    \item Par ailleurs, sur les matrices carrées d'ordre \( n \) fixé, le produit de deux matrices est bien défini. Ainsi, \( \eM(n,\eA)\) se voit conférer une structure d'anneau, dont le neutre pour la multiplication est la matrice carrée \( \mtu_n\) (notée aussi \( \mtu\) lorsqu'il n'y a pas d'ambiguïté sur la taille), donnée par
\begin{equation}
    \mtu_{ij}=\begin{cases}
        1    &   \text{si } i=j\\
        0    &    \text{sinon.}
    \end{cases}
\end{equation}
Il est vite vu que si \( A\) est une matrice carrée d'ordre \( n \), alors \( A\mtu=\mtu A=A\).
\end{enumerate}
\end{remark}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooUXDRooWZbMVN}
    Si \( A\), \( B\) et \( C\) sont des matrices nous avons
    \begin{enumerate}
        \item
            \( (AB)^t=B^tA^t\),
        \item
            \( \tr(ABC)=\tr(CAB)\).
    \end{enumerate}
    La seconde égalité est importante et est nommée \defe{invariance cyclique}{invariance cyclique!trace} de la trace.
\end{lemma}

\begin{proof}
    La première est un simple calcul :
    \begin{equation}
        (AB)^t_{ij}=(AB)_{ji}=\sum_kA_{jk}B_{ki}=\sum_kA^t_{kj}B^t_{ik}=(B^tA^t)_{ij}.
    \end{equation}
    Pour la seconde :
    \begin{equation}
        \tr(ABC)=\sum_{ikl}A_{ik}B_{kl}C_{li}=\sum_{ikl}C_{li}A_{ik}B_{kl}=\sum_l(CAB)_{ll}=\tr(CAB).
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Application linéaire associée}
%---------------------------------------------------------------------------------------------------------------------------

Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases\footnote{C'est le théorème~\ref{ThonmnWKs} qui nous permet de considérer des bases. Et ce théorème ne fonctionne que parce que nous avons supposé une dimension finie.} \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\). 

Nous considérons l'application
\begin{equation}        \label{EQooVZQWooMyFFeO}
    \begin{aligned}
        \psi\colon \eM(n\times m, \eK)&\to \aL(E,F) \\
        A&\mapsto f_A 
    \end{aligned}
\end{equation}
où \( f_A\) est définie par
\begin{equation}
    f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}
\end{equation}
si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).

Nous allons prouver un certain nombre de résultats montrant que cette application a toutes les propriétés imaginables permettant d'identifier les matrices aux applications linéaires : elle est un isomorphisme pour toutes les structure que vous pouvez raisonnablement imaginer.

À cette application \( \psi\) il manque cependant une propriété importante : elle n'est pas canonique. Elle dépend des bases choisies. Autrement dit : nous avons a priori autant d'applications \( \psi\) différentes qu'il y a de choix de bases sur \( E\) et \( F\)\quext{Bonne question. Est-ce qu'il y a moyen de construire deux choix de bases donnant la même application \( \psi\) ? Écrivez-moi si vous savez la réponse.}. %TODOooFKSRooBtOZYc

Nous allons prouver maintenant quelque résultats montrant que les matrices et les applications linéaires, dans le cas des espaces vectoriels \( \eK^n\) sont deux présentations de la même chose.

\begin{normaltext}
    Lorsque \( A\in \eM(n,\eK)\) est une matrice et \( x\in \eK^n\) un vecteur, nous notons \( Ax\) l'élément de \( \eK^n\) donné par
    \begin{equation}        \label{EQooQFVTooMFfzol}
        (Ax)_i=\sum_jA_{ij}x_j.
    \end{equation}
    Autrement dit, \( Ax=f_A(x)\).

    Cette convention et de nombreuses autres à propos de matrice sera rappelée dans \ref{SECooBTTTooZZABWA}.
\end{normaltext}

\begin{proposition}      \label{PROPooGXDBooHfKRrv}
    Soient deux espaces vectoriels de dimension finie \( E,F\) sur le corps \( \eK\). Nous considérons les bases \( \{ e_i \}\) pour \( E\) et \( \{ f_{\alpha} \}\) pour \( F\). 

    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon \eM(n\times m, \eK)&\to \aL(E,F) \\
            A&\mapsto f_A 
        \end{aligned}
    \end{equation}
    où \( f_A\) est définie par
    \begin{equation}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}
    \end{equation}
    si \( x_i\) sont les coordonnées de \( x\in E\) dans la base \( \{ e_i \}\).

    Alors
    \begin{enumerate}
        \item       \label{ITEMooKZYYooZPTkpq}
            Nous avons
            \begin{equation}
                f_A(e_i)_{\alpha}=A_{\alpha i}.
            \end{equation}
        \item       \label{ITEMooHSMLooRJZref}
            L'application \( \psi\) est une bijection.
    \end{enumerate}
\end{proposition}

Remarque : les bases ne sont supposées être canoniques en aucun sens du terme. Les dimensions de \( E\) et \( F\) ne sont pas non plus supposées identiques.

\begin{proof}
    En nous rappelant que \( (e_j)_i=\delta_{ij}\) nous avons
    \begin{equation}
        f_A(e_j)=\sum_{i\alpha}A_{\alpha i}(e_j)_if_{\alpha}=\sum_{\alpha}A_{\alpha j}f_{\alpha},
    \end{equation}
    donc \( f_A(e_i)_{\alpha}=A_{\alpha i}\). Cela prouve la formule du point \ref{ITEMooKZYYooZPTkpq}.

    Prouvons que \( \psi\) est injective. Si \( f_A=f_B\), nous avons en particulier \( f_A(e_i)_{\alpha}=f_B(e_i)_{\alpha}\) et donc \( A_{\alpha i}=B_{\alpha i}\).

    Prouvons que \( \psi\) est surjective. Pour cela nous considérons \( f\in \aL(E,F)\) et nous posons \( A_{\alpha i}=f(e_i)_{\alpha}\). Nous avons alors \( f=f_A\) parce que
    \begin{equation}
        f_A(x)=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}=\sum_{i\alpha}f(e_i)_{\alpha}x_if_{\alpha}=\sum_{\alpha}f(\sum_ix_ie_i)_{\alpha}f_{\alpha}=\sum_{\alpha}f(x)_{\alpha}f_{\alpha}=f(x).
    \end{equation}
\end{proof}

La proposition suivante montre que le produit matriciel correspond à la composition d'applications linéaires, pourvu que l'on travaille avec les bases canoniques sur \( \eK^n\).
\begin{proposition}[\cite{MonCerveau}]      \label{PROPooIYVQooOiuRhX}
    Soit un corps commutatif \( \eK\). Nous considérons des espaces vectoriels \( E\) et \( F\) munis de bases \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_{\alpha}\}_{\alpha\in 1,\ldots, m} \).

    L'application déjà définie\footnote{Notez la position du \( n\) et du \( m\). Sachez noter les bornes des sommes écrites dans la démonstration.}
    \begin{equation}
        \psi\colon \eM(m\times n,\eK)\to \aL(E,F)
    \end{equation}
    est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Le fait que \( \psi\) soit une bijection est la proposition \ref{PROPooGXDBooHfKRrv}. Nous devons montrer que c'est linéaire. 

    Pour \( \lambda\in \eK\) nous avons le calcul
    \begin{equation}
        \psi(\lambda A)(e_k)=f_{\lambda A}(e_k)=\sum_{\alpha i}(\lambda A)_{\alpha i}\underbrace{(e_k)_i}_{=\delta_{ki}}f_{\alpha}=\lambda\sum_{\alpha}A_{\alpha k}f_{\alpha}=\lambda f_A(e_j).
    \end{equation}
    Donc \( \psi(\lambda A)=\lambda\psi(A)\).

    Si \( A,B\in \eM(n,\eK)\) nous avons de la même façon \( f_{A+B}=f_A+f_B\).
\end{proof}

\begin{proposition}     \label{PROPooCSJNooEqcmFm}
    Soient des espaces vectoriels \( E\), \( F\) et \( G\) de dimensions \( n\), \( m\) et \( p\) munis de bases\footnote{Avec trois, nous renonçons à utiliser des alphabets différents pour numéroter les éléments des bases.} \( \{ e_i \}\), \( \{ f_i \}\) et \( \{ g_i \}\). Nous considérons les deux applications
    \begin{equation}
        \psi\colon \eM(m\times n,\eK)\to \aL(E,F)
    \end{equation}
    et
    \begin{equation}
        \psi\colon \eM(p\times m,\eK)\to \aL(F,G).
    \end{equation}
    Nous avons 
    \begin{equation}
        f_A\circ f_B=f_{AB}
    \end{equation}
    pour toutes matrices \( A\in \eM(p\times m,\eK)\) et \( B\in \eM(m\times n,\eK)\).
\end{proposition}

\begin{proof}
    Nous considérons les applications linéaires associées à \( A\) et \( B\) : \( f_A\colon F\to G\) et \( f_B\colon E\to F\) et la composée \( f_A\circ f_B\colon E\to G\). Et puis c'est le calcul :
    \begin{subequations}
        \begin{align}
            (f_A\circ f_B)(e_k)&=f_A\big( \sum_{ij}B_{ij}(e_k)_jf_i \big)\\
            &=\sum_i B_{ik}f_A(f_i)\\
            &=\sum_iB_{ik}\sum_{rs}A_{rs}(f_i)_sg_r\\
            &=\sum_{ir}B_{ik}A_{ri}g_r\\
            &=\sum_r(AB)_{rk}g_r\\
            &=f_{AB}(e_k).
        \end{align}
    \end{subequations}
    Donc \( f_A\circ f_B=f_{AB}\) comme il se doit.
\end{proof}
    
Nous pouvons particulariser au cas où \( E=F=G\).
\begin{proposition}     \label{PROPooFMBFooEVCLKA}
    Si \( E\) est un espace vectoriel muni d'une base \( \{ e_i \}\), alors l'application
    \begin{equation}
        \psi\colon \eM(n,\eK)\to \End(E)
    \end{equation}
    est un isomorphisme d'algèbre\footnote{Définition \ref{DefAEbnJqI}.} et d'anneaux\footnote{Définition \ref{DEFooSPHPooCwjzuz}}.
\end{proposition}

\begin{proof}
    Le fait que \( \psi\) soit un isomorphisme d'algèbre est juste la combinaison entre les proposition \ref{PROPooIYVQooOiuRhX} et \ref{PROPooCSJNooEqcmFm}.

    En ce qui concerne l'isomorphisme d'anneaux, il faut en plus identifier les neutres. Le neutre pour la composition d'applications linéaires est l'application identité et le neutre pour la multiplication de matrices est la matrice identité. Nous devons donc montrer que \( \psi(\delta)=\id\). Juste un calcul :
    \begin{equation}
        f_{\delta}(x)=\sum_{ij}\delta_{ij}x_je_i=\sum_ix_ie_i=x.
    \end{equation}
    Donc oui, \( f_{\delta}\) est l'identité.
\end{proof}

Voila. Soyez bien conscient que l'application \( \psi\) dont nous avons beaucoup parlé est surtout intéressante dans le cas des espaces de la forme \( \eK^n\). Dans ce cas, nous avons une identification canonique entre \( \eM(n,\eK)\) et \( \End(\eK^n)\) qui est un isomorphisme d'anneaux et d'algèbres.

Nous verrons que ce \( \psi\) respecte encore les inverses\footnote{Proposition \ref{PROPooNPMCooPmaCwu}.} et les déterminants\footnote{Proposition \ref{PROPooFKDXooKMSolt}.}.

\begin{normaltext}
    Il convient de ne pas confondre matrice et application linéaire (bien que nous le ferons sans vergogne). Une matrice est un bête tableau de nombres, tandis qu'une application linéaire est une application entre deux espaces vectoriels vérifiant certains propriétés.

    Cependant si les espaces vectoriels \( E\) et \( F\) sont munis de bases, alors il y a une application
    \begin{equation}
        \psi\colon \eM(m\times n,\eK)\to \aL(E,F)
    \end{equation}
    qui a toutes les propriétés imaginables\footnote{Et elle en aura encore plus lorsque nous aurons vus les déterminants.}.

    Cette application dépend des bases choisies. Il n'y a donc pas de trucs comme «la matrice de telle application linéaire» ou comme «voici une matrice, nous considérons l'application linéaire associée». 

    Cependant, sur des espaces comme \( \eR^n\) ou plus généralement sur \( \eK^n\), nous avons une base canonique et toute personne raisonnable utilise toujours la base canonique (sauf mention du contraire). Dans ces cas il est sans danger de dire «la matrice associée à telle application linéaire» sans préciser les bases.

    Mais si un jour vous utilisez une base autre que la base canonique sur \( \eR^n\), précisez-le et plutôt deux fois qu'une\footnote{Au passage, non, les coordonnées polaires ne sont pas une base de \( \eR^2\). C'est un système de coordonnées, et ce n'est pas la même chose.}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYCKRooTrajdP}
    Si \( A\in\eM(n,\eK)\) nous définissons le \defe{déterminant}{déterminant!matrice} de \( A\) par la formule
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}(-1)^{\sigma}\prod_{i=1}^nA_{i\sigma(i)}
    \end{equation}
    où la somme est effectuée sur tous les éléments du groupe symétrique\footnote{Pour le groupe symétrique, c'est la définition \ref{DEFooJNPIooMuzIXd}, le fait que ce soit un groupe fini est le lemme \ref{LEMooSGWKooKFIDyT}, et pour la somme sur un groupe fini c'est la définition \ref{DEFooLNEXooYMQjRo}..} \( S_n\) et où \( (-1)^{\sigma}\) représente la parité de la permutation \( \sigma\).
\end{definition}
En se souvenant que \( | S_n |=n!\), nous sommes frappés de stupeur devant le fait que le nombre de termes dans la somme croît de façon factorielle (c'est plus qu'exponentiel, pour info) en la taille de la matrice. Cette formule est donc sans espoir pour une matrice plus grande que \( 3\times 3\) ou à la rigueur \( 4\times 4\) à la main. À l'ordinateur, il est possible de monter plus haut, mais pas tellement.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant en petite dimension}
%---------------------------------------------------------------------------------------------------------------------------

En dimension deux, le déterminant de la matrice
    $\begin{pmatrix}
        a    &   b    \\
        c    &   d
    \end{pmatrix}$
est le nombre
\begin{equation}        \label{EQooQRGVooChwRMd}
     \det\begin{pmatrix}
         a   &   b    \\
         c   &   d
     \end{pmatrix}=\begin{vmatrix}
          a  &   b    \\
        c    &   d
    \end{vmatrix}=ad-cb.
\end{equation}
Ce nombre détermine entre autres le nombre de solutions que va avoir le système d'équations linéaires associé à la matrice.

Pour une matrice $3\times 3$, nous avons le même concept, mais un peu plus compliqué; nous avons la formule
\begin{equation}
    \det
    \begin{pmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}
    \end{pmatrix}
    =
    \begin{vmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}
    \end{vmatrix}=
    a_{11}\begin{vmatrix}
        a_{22}  &   a_{23}    \\
        a_{32}    &   a_{33}
    \end{vmatrix}-
    a_{12}\begin{vmatrix}
        a_{21}  &   a_{23}    \\
        a_{31}    &   a_{33}
    \end{vmatrix}+
    a_{13}\begin{vmatrix}
        a_{21}  &   a_{22}    \\
        a_{31}    &   a_{32}
    \end{vmatrix}.
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Manipulations de lignes et de colonnes}
%---------------------------------------------------------------------------------------------------------------------------

Nous voudrions savoir ce qu'il se passe avec le déterminant d'une matrice lorsque nous substituons à une ligne ou une colonne une combinaison des autres lignes et colonnes. Lorsque une matrice est donnée, nous notons \( C_j\) sa \( j\)\ieme colonne.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooRSJTooQEoOtN}
    Si \( A\) et \( B\) sont des matrices, alors
    \begin{equation}
        (AB)^t=B^tA^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Il suffit de calculer les éléments de matrice :
    \begin{equation}
        (AB)^t_{ij}=(AB)_{ji}=\sum_k A_{jk}B_{ki}=\sum_kB^t_{ik}A^t_{kj}=(B^tA^t)_{ij}.
    \end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau,ooKYTYooJlzZMp}]        \label{LEMooCEQYooYAbctZ}
    Si \( A\) est une matrice, alors \( \det(A)=\det(A^t)\).
\end{lemma}

\begin{proof}
    Nous commençons par écrire la définition du déterminant :
    \begin{equation}
        \det(A^t)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n(A^t)_{i,\sigma(i)}=\sum_{\sigma}\epsilon(\epsilon)\prod_iA_{\sigma(i),i}.
    \end{equation}
    Pour chaque \( \sigma\) séparément, nous utilisant la proposition \ref{PROPooQMUDooQQVRIe} pour ré-indexer le produit :
    \begin{equation}
        \prod_i A_{\sigma(i),i}=\prod_iA_{i,\sigma^{-1}(i)}.
    \end{equation}
    Nous profitons du fait que l'application \( \varphi\colon S_n\to S_n\) donnée par \( \varphi(\sigma)=\sigma^{-1}\) soit une permutation de \( S_n\) pour appliquer la définition \ref{DEFooLNEXooYMQjRo} et faire la somme sur \( \sigma^{-1}\) :
    \begin{equation}
        \det(A^t)=\sum_{\sigma}\epsilon(\sigma)\prod_iA_{i,\sigma^{-1}(i)}=\sum_{\sigma}\epsilon(\sigma^{-1})\prod_iA_{i,\sigma(i)}=\det(A)
    \end{equation}
    où nous avons utilisé le fait que \(\epsilon(\sigma^{-1})=\epsilon(\sigma)\) (corollaire \ref{CORooZLUKooBOhUPG}).
\end{proof}

Le fait que \( \det(A)=\det(A^t)\) permet, dans toutes les propositions du type «ce qui arrive au déterminant si on change telle ligne ou colonnes» de ne donner qu'une preuve pour la partie «ligne» et déduire automatiquement le cas «colonne». Le lemme suivant donne un exemple d'utilisation.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooWMQWooGWFlmC}
    Soit une matrice \( A\). Nous considérons la matrice \( B\) obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\) ainsi que la matrice \( C\) obtenue à partir de \( A^t\) par la permutation de colonnes \( C_k\leftrightarrow C_l\).
    
    Alors \( C^t=B\).
\end{lemma}

\begin{proof}
    Calculons les éléments de matrice de \( C\) :
    \begin{equation}
        C_{ij}=\begin{cases}
            (A^t)_{ij}    &   \text{si }  j\neq k, j\neq l\\
            (A^t)_{ik}    &   \text{si } j=l\\
            (A^t)_{il}    &    \text{si }j=k
        \end{cases}=
        \begin{cases}
            A_{ji}    &   \text{si }  j\neq k, j\neq l\\
            A_{ki}    &   \text{si } j=l\\
            A_{li}    &    \text{si }j=k.
        \end{cases}
    \end{equation}
    Ensuite nous prouvons que \( C^t=B\) en écrivant les éléments de \( C^t\) :
    \begin{equation}
        (C^t)_{ij}=C_{ji}=\begin{cases}
            A_{ij}    &   \text{si } i\neq k, i\neq l\\
            A_{kj}    &   \text{si } i=l\\
            A_{lj}    &    \text{si }i=k.
        \end{cases}
    \end{equation}
    Cette dernière expression est la matrice \( A\) après permutation des lignes \( L_k\leftrightarrow L_l\), c'est-à-dire a matrice \( B\).
\end{proof}

Pour la suite nous écrivons \( \delta\) la matrice «identité», c'est-à-dire celle dont les entrées sont précisément les \( \delta_{ik}\).  Nous écrivons également \( E_{ij}\) la matrice contenant de zéros partout sauf en \( (i,j)\) où elle a un \( 1\), c'est-à-dire
\begin{equation}
    (E_{ij})_{kl}=\delta_{ik}\delta_{jl}.
\end{equation}

\begin{proposition}[Permuter des lignes ou des colonnes \( L_k\leftrightarrow L_l\)\cite{ooKBOMooSkKHvu,MonCerveau}]    \label{PROPooFQRDooRPfuxk}
    Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\). 
    \begin{enumerate}
        \item   \label{ITEMooAIHWooHXzeys}
            Si \( B\) est la matrice obtenue à partir de \( A\) en permutant deux lignes ou deux colonnes, alors
            \begin{equation}
                \det(A)=-\det(B).
            \end{equation}
        \item  \label{ITEMooDNHWooOMgmxa}
            Si \( B\) est la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\). Alors
            \begin{equation}
                B=SA
            \end{equation}
            avec \( S=\delta+E_{kl}+E_{lk}-E_{kk}-E_{ll}\).

            Autrement dit : la matrice \( S\) est une matrice de permutations de lignes.
        \item \label{ITEMooSHRQooQrqVdO}
            La matrice \( S\) vérifie \( \det(S)=-1\)
        \item       \label{ITEMooQXSEooMWiKbL}
            Nous avons 
            \begin{equation}
                \det(SA)=\det(S)\det(A).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point
    \begin{subproof}
    \item[\ref{ITEMooAIHWooHXzeys} pour les colonnes]

    Soient \( k\) et \( l\) fixés, et considérons la permutation des colonnes \( C_k\) et \( C_l\). Nous notons \( \alpha\) la permutation \( (kl)\) dans \( S_n\) (groupe symétrique, définition \ref{DEFooJNPIooMuzIXd}). Nous avons
    \begin{equation}
        B_{ij}=A_{i \alpha(j)},
    \end{equation}
    ou encore : \( A_{ij}=B_{i\alpha(j)}\). Par définition,
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_{n}}\epsilon(\sigma)\prod_{i=1}^nA_{i\sigma(i)}
    \end{equation}
    C'est le moment d'utiliser la proposition \ref{PROPooWJQQooFINSEc} à propos de somme sur des groupes avec \( G=S_n\), \( h=\alpha\) et 
    \begin{equation}
        f(\sigma)=\epsilon(\sigma)\prod_iA_{i,\sigma(i)}.
    \end{equation}
    Nous savons que \( \epsilon(\alpha)=-1\) et que \( \epsilon\) est un homomorphisme par la proposition \ref{ProphIuJrC}\ref{ITEMooBQKUooFTkvSu}, donc
    \begin{equation}
        f(\alpha \sigma)=\epsilon(\alpha\sigma)\prod_iA_{i,(\alpha\sigma)(i)}=-\epsilon(\sigma)\prod_iB_{i,\sigma(i)}.
    \end{equation}
    Avec ça, nous concluons :
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}f(\sigma)=\sum_{\sigma}f(\alpha \sigma)=-\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}=-\det(B).
    \end{equation}
    \item[\ref{ITEMooAIHWooHXzeys} pour les lignes]

    Que se passe-t-il si nous permutons les lignes \( L_k\) et \( L_{l}\) ?Si nous notons \( B'\) la matrice obtenue à partir de \( A\) par la permutation de lignes \( L_k\leftrightarrow L_l\), et \( C\) celle obtenue de \( A^t\) après permutation de colonnes \( C_k\leftrightarrow C_l\) alors nous avons \( C^t=B'\). Le lemme \ref{LEMooWMQWooGWFlmC} nous dit que \( C^t=B'\). En utilisant le lemme \ref{LEMooCEQYooYAbctZ} sur le déterminant de la transposée,
    \begin{equation}
        \det(B')=\det(C^t)=\det(C)=-\det(A^t)=-\det(A).
    \end{equation}
    Voila qui prouve le résultat pour les permutation de lignes.
        
\item[\ref{ITEMooDNHWooOMgmxa}]
    Si \( k=l\), il n'y a pas de permutations, et il est vite vu que la matrice \( S\) est l'identité parce qu'il y a quatre fois le terme \( E_{kk}\). Nous supposons donc que \( k\neq l\); en particulier \( \delta_{kl}=0\).

    Il s'agit surtout d'un beau calcul :
    \begin{subequations}
        \begin{align}
            (SA)_{ij}=\sum_{m}S_{im}A_{mj}&=A_{ij}+\sum_m(\delta_{ki}\delta_{lm}+\delta_{li}\delta_{lm}-\delta_{ki}\delta_{km}-\delta_{li}\delta_{lm})A_{mj}\\
            &=A_{ij}+\delta_{ki}A_{lj}+\delta_{li}A_{kj}-\delta_{ki}A_{kj}-\delta_{li}A_{lj}.
        \end{align}
    \end{subequations}
    Si \( i\neq j\) et \( i\neq l\), alors \( (SA)_{ij}=A_{ij}\). Si \( i=k\), alors 
    \begin{equation}
        (SA)_{kj}=A_{kj}+A_{lj}-A_{kj}=A_{lj},
    \end{equation}
    c'est-à-dire que la \( k\)\ieme ligne de \( SA\) est la \( l\)\ieme ligne de \( A\).

    Avec \( i=l\) nous obtenons la \( k\)\ieme ligne de \( A\).

    Tout cela montre que \( SA\) est la matrice \( A\) dans laquelle les lignes \( k\) et \( l\) ont été inversées, c'est-à-dire \( SA=B\).

\item[\ref{ITEMooSHRQooQrqVdO}]
            En utilisant la définition du déterminant,
            \begin{subequations}
                \begin{align}
                    \det(S)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nS_{i\sigma(i)}\\
                    &=\sum_{\sigma}\epsilon(\sigma)\prod_i\big( \delta_{i\sigma(i)}+\delta_{ki}\delta_{l\sigma(i)}+\delta_{li}\delta_{k\sigma(i)}-\delta_{ki}\delta_{k\sigma(i)}-\delta_{li}\delta_{l\sigma(i)} \big).
                \end{align}
            \end{subequations}
            Nous utilisons l'associativité et la commutativité du produit pour séparer les facteurs \( i=k\) et \( i=l\) des autres :
            \begin{equation}
                \det(S)=\sum_{\sigma}\epsilon(\sigma)\prod_{\substack{i\neq k\\i\neq l}}\delta_{i\sigma(i)}(\delta_{k\sigma(k)}+\delta_{l\sigma(k)}-\delta_{k\sigma(k)})(\delta_{l\sigma(l)}+\delta_{k\sigma(l)}-\delta_{l\sigma(l)}).
            \end{equation}
            À cause des facteurs \( i\neq k\) et \( i\neq l\), les \( \sigma\) pour lesquels le tout n'est pas nuls doivent vérifier \( \delta_{i\sigma(i)}=1\) pour tout \( i\) différent de \( k\) et \( l\). Les deux seuls sont donc \( \sigma=\id\) et la permutation \( \sigma=(k,l)\). Pour \( \sigma=\id\), nous avons
            \begin{equation}
                \prod_{\substack{i\neq k\\i\neq l}}\delta_{ii}(\delta_{kk}+\delta_{lk}-\delta_{kk})(\delta_{ll}+\delta_{kl}-\delta_{ll})=0.
            \end{equation}
            Dernier espoir : \( \sigma=(k,l)\). Pour ce terme nous avons \( \epsilon(\sigma)=-1\) et
            \begin{equation}
                \prod_{\substack{i\neq k\\i\neq l}}\delta_{ii}(\delta_{kl}+\delta_{ll}-\delta_{kl})(\delta_{lk}+\delta_{kk}-\delta_{lk})=1.
            \end{equation}
            Au final dans \( \det(S)\) il n'y a de non nul que le terms \( \sigma=(k,l)\) et il vaut \( -1\). Donc
            \begin{equation}
                \det(S)=-1.
            \end{equation}
        \item[\ref{ITEMooQXSEooMWiKbL}]
            Il s'agit de mettre bout à bout les points déjà prouvés :
            \begin{equation}
                \det(SA)=-\det(A)=\det(S)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

\begin{corollary}[\cite{ooKBOMooSkKHvu}]        \label{CORooAZFCooSYINvBl}
    Soit une matrice \( A\in \eM(n,\eK)\). Si deux lignes ou deux colonnes de \( A\) sont égales, alors \( \det(A)=0\).
\end{corollary}

\begin{proof}
    Si deux colonnes sont égales, la matrice ne change pas lorsqu'on les permute, alors que le déterminant change de signes. La seule possibilité est que \( \det(A)=-\det(A)\), ce qui signifie que \( \det(A)=0\).
\end{proof}
Notons que si pour \( k\neq l\) nous avons \( C_k=\lambda C_l\), alors nous avons aussi \( \det(A)=0\).

La réciproque n'est pas vraie : il existe des matrices dont le déterminant est nul et dont aucune entrée n'est nulle. Par exempe
\begin{equation}
    \begin{pmatrix}
        1    &   2    \\ 
        1    &   2    
    \end{pmatrix}.
\end{equation}


\begin{proposition}[\cite{ooKBOMooSkKHvu}]      \label{PROPooNGZJooHjtMyn}
    Soient \( A\in \eM(n,\eK)\), et \( v\in \eK^n\). Si \( B\) est la matrice \( A\) avec la substitution \( L_j\to L_j+v\) et \( C\) est la matrice \( A\) avec la substitution \( L_j\to v\), alors
    \begin{equation}
        \det(B)=\det(A)+\det(C).
    \end{equation}
\end{proposition}

\begin{proof}
    En utilisant l'associativité de la multiplication,
    \begin{subequations}
        \begin{align}
            \det(B)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}\\
            &=\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}B_{i\sigma(i)} \big)B_{j\sigma(j)}\\
            &=\sum_{\sigma}\epsilon(\sigma)\big( \prod_{i\neq j}A_{i\sigma(i)} \big)(A_{j\sigma(j)}+v_{\sigma(j)})\\
            &=\sum_{\sigma}\epsilon(\sigma)\prod_iA_{i\sigma(i)}+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i\sigma(i)}v_{\sigma(j)}         \label{SUBEQooKATCooVIbEpv}\\
            &=\det(A)+\sum_{\sigma}\epsilon(\sigma)\prod_{i\neq j}C_{i\sigma(i)}C_{j\sigma(j)}  \label{SUBEQooCOTDooPPrEYJ}\\    
            &=\det(A)+\det(C).
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item \ref{SUBEQooKATCooVIbEpv} parce que pour \( i\neq j\) nous avons \( A_{i\sigma(i)}=C_{i\sigma(i)}\)
        \item \ref{SUBEQooCOTDooPPrEYJ} parce que \( v_{\sigma(j)}=C_{j\sigma(j)}\).
    \end{itemize}
\end{proof}

\begin{proposition}[Combinaison de lignes ou colonnes \( L_k\to L_k+\lambda L_l\)\cite{ooKBOMooSkKHvu}]     \label{PROPooPYNHooLbeVhj}
    Soient une matrice \( A\in \eM(n,\eK)\), deux entiers \( k\neq l\) inférieurs ou égaux à \( n\).
    \begin{enumerate}
        \item       \label{ITEMooJSRDooTggEyO}
            Si \( B\) est la matrice obtenue à partir de \( A\) par la substitution \( L_k\to L_k+\lambda L_l\) ou \( C_k\to C_k+\lambda C_l\), alors
            \begin{equation}
                \det(A)=\det(B).
            \end{equation}
        \item   \label{ITEMooHKZWooVZDgnf}
            Si \( B\) est la matrice \( A\) dans laquelle nous avons fait la substitution \( L_k\to L_k+\lambda L_l\), alors
            \begin{equation}
                B=UA
            \end{equation}
            avec \( U=\delta+\lambda E_{kl}\), c'est-à-dire que \( U\) est une matrice de combinaison de lignes.
        \item           \label{ITEMooPGYJooWTTghT}
            La matrice \( U\) vérifie \( \det(U)=1\).
        \item       \label{ITEMooBBEAooZJVGNV}
            Nous avons 
            \begin{equation}
                \det(UA)=\det(U)\det(A).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
    \item[\ref{ITEMooJSRDooTggEyO}]
        Soit la matrice \( C\) obtenue à partir de \( A\) par \( L_k\to \lambda L_l\). En considérant le vecteur \( v=\lambda L_l\), nous sommes dans la situation de la proposition \ref{PROPooNGZJooHjtMyn}. Donc
        \begin{equation}
            \det(B)=\det(A)+\det(C).
        \end{equation}
        Mais dans la matrice \( C\), nous avons \( L_k=\lambda L_l\), ce qui implique \( \det(C)=0\) par le corollaire \ref{CORooAZFCooSYINvBl}. Donc \( \det(A)=\det(B)\) comme il se devait.
    \item[\ref{ITEMooHKZWooVZDgnf}]
        Encore un calcul :
        \begin{equation}
            (UA)_{ij}=\sum_m\big( \delta_{im}+\lambda(E_{kl})_{im} \big)A_{mj}=A_{ij}+\lambda\sum_m\delta_{ki}\delta_{lm}A_{mj}=A_{ij}+\lambda \delta_{li}A_{kj}.
        \end{equation}
        Cela donne, pour \( i=k\) la ligne
        \begin{equation}
            (UA)_{kj}=A_{kj}+\lambda A_{lj},
        \end{equation}
        ce qui correspond bien à \( L_k\to L_k+\lambda L_l\).

    \item[\ref{ITEMooPGYJooWTTghT}]
            Nous calculons le déterminant de \( U=\delta+\lambda E_{kl}\) avec \( k\neq l\). Nous avons dans un premier temps :
            \begin{equation}
                \det(U)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n(\delta_{i\sigma(i)}+\lambda \delta_{ki}\delta_{l\sigma(i)}).
            \end{equation}
            Vu que nous avons toujours \( \delta_{ki}\delta_{li}=0\), le terme \( \sigma=\id\) donne \( 1\).

            Pour les \( \sigma\neq \id\), le facteur \( \lambda\delta_{ki}\delta_{l\sigma(i)}\) ne s'annulle pas uniquement si \( i=k\) et \( \sigma(i)=k\). Donc le seul terme non nul autre que \( \sigma=\id\) peut provenir de \( \sigma=(k,l)\). Pour ce terme, nous isolons les termes \( i=l\) et \( i=k\) :
            \begin{equation}
                (\delta_{k\sigma(k)}+\lambda\delta_{kk}\delta_{k\sigma(k)})(\delta_{l\sigma(l)}+\lambda\delta_{kl}\delta_{k\sigma(l)}).
            \end{equation}
            Le dernier facteur est nul.
        \item[\ref{ITEMooBBEAooZJVGNV}]
            En mettant bout à bout les résultats prouvés,
            \begin{equation}
                \det(UA)=\det(A)=\det(U)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

\begin{proposition}[Multiplication par un scalaire d'une ligne ou colonne \( L_k\to \lambda L_k\)\cite{ooKBOMooSkKHvu}] \label{PROPooXUFKooOaPnna}
    Soient une matrice \( A\in \eM(n,\eK)\), un entier \( k\neq l\) inférieurs ou égal à \( n\). Soit la matrice \( B\) obtenue à partir de \( A\) en multipliant la ligne \( L_k\) par \( \lambda\in \eK\).
    \begin{enumerate}
        \item       \label{ITEMooBKIGooCDQEDt}
            \( \det(B)=\lambda\det(A)\)
        \item       \label{ITEMooWRRCooFXkRNW}
            En considérant la matrice \( T=\delta+(\lambda-1)E_{kk}\), nous avons
            \begin{equation}
                B=TA,
            \end{equation}
            c'est-à-dire que a matrice \( T\) est une matrice de multiplication de ligne par un scalaire.
        \item       \label{ITEMooOGGDooPVVRzk}
            Nous avons \( \det(T)=\lambda\).
        \item       \label{ITEMooIFRVooWQYgkK}
            Et aussi : \( \det(TA)=\det(T)\det(A)\)
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[\ref{ITEMooBKIGooCDQEDt}]
            La matrice \( B\) est donnée par les éléments
            \begin{equation}
                B_{ij}=\begin{cases}
                    A_{ij}    &   \text{si } j\neq k\\
                    \alpha A_{ij}    &    \text{si } j=kn
                \end{cases}
            \end{equation}
            c'est-à-dire \( B_{ij}=\big( 1+(\alpha-1)\delta_{jk} \big)A_{ij}\). Nous mettons cela dans la définition du déterminant de \( B\) :
            \begin{equation}        \label{EQooGVMTooPntKew}
                \det(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nB_{i\sigma(i)}=\sum_{\sigma}\prod_i\big( 1+(\alpha-1)\delta_{\sigma(i)k}A_{i\sigma(i)} \big).
            \end{equation}
            L'associativité du produit dans \( \eK\) nous permet de séparer le produit de la façon suivante :
            \begin{equation}
                \prod_{i=1}^n\big( 1+(\alpha-1)\delta_{\sigma(i)k} \big)A_{i\sigma(i)}=\prod_i\big( 1+(\lambda-1)\delta_{\sigma(i)k} \big)\prod_iA_{i\sigma(i)}=\lambda\prod_iA_{i\sigma(i)}.
            \end{equation}
            En remettant dans \eqref{EQooGVMTooPntKew}, nous trouvons \( \det(B)=\det(A)\).
        \item[\ref{ITEMooWRRCooFXkRNW}]
            C'est un cas particulier de la proposition \ref{PROPooPYNHooLbeVhj}\ref{ITEMooHKZWooVZDgnf} en prenant \( k=l\) et en adaptant le \( \lambda\).
        \item[\ref{ITEMooOGGDooPVVRzk}]
            Nous calculons le déterminant de la matrice \( T=\delta+(\lambda-1)E_{kk}\). La formule du déterminant donne
            \begin{equation}
                \det(T)=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^n\big( \delta_{i\sigma(i)}+(\lambda-1)\delta_{ki}\delta_{k\sigma(i)} \big).
            \end{equation}
            Si \( i\neq \sigma(i)\), alors non seulement \( \delta_{i\sigma(i)}=0\), mais en plus \( \delta_{ki}\delta_{k\sigma(i)}=0\). Donc suel \( \sigma=\id\) reste dans la somme sur \( \sigma\in S_n\). Il reste donc
            \begin{equation}
                \det(T)=\prod_{i=1}^n\big( 1+(\lambda-1)\delta_{ki} \big)=\left( \prod_{i\neq k}1 \right)(1+(\lambda-1))=\lambda
            \end{equation}
            où nous avons utilisé encore l'associativité pour isoler le facteur \( i=k\).
        \item[\ref{ITEMooIFRVooWQYgkK}]
            Il faut mettre bout à bout les résultats déjà faits :
            \begin{equation}
                \det(TA)=\lambda\det(A)=\det(T)\det(A).
            \end{equation}
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu les matrices d'opérations élémentaire sur les lignes et colonnes :
\begin{itemize}
    \item Permutation de lignes \( L_k\leftrightarrow L_l\)  : \( S(n;k,l)=\delta+E_{kl}+E_{lk}-E_{kk}-E_{ll}\), proposition \ref{PROPooFQRDooRPfuxk}.
    \item Combinaisons de lignes \( L_k\to L_k+\lambda L_l\) : \( U(n;k,l,\lambda)=\delta+\lambda E_{kl}\), proposition \ref{PROPooPYNHooLbeVhj}.
    \item Multiplication d'une ligne par un scalaire \( L_k\to \lambda L_k\) : \( T=\delta+(\lambda-1)E_{kk}\), proposition \ref{PROPooXUFKooOaPnna}.
\end{itemize}

Ces matrices seront dans la suite notées \( G\). Et elles vérifient la grosse propriété
\begin{equation}        \label{EQooLQTVooBYjVYl}
    \det(GA)=\det(G)\det(A)
\end{equation}
pour toute matrice \( A\).

\begin{proposition}[Réduction de Gauss\cite{MonCerveau}]        \label{PROPooJBTZooNLobpf}
    Soit une matrice \( A\in \eM(n,\eK)\) de déterminant non nul : \( \det(A)\neq 0\). Alors il existe des matrices \( G_1,\ldots, G_N\) toutes de type \( S\), \( U\) ou \( T\) telles que
    \begin{equation}
        G_1\ldots G_NA=\delta.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous faisons une récurrence sur \( n\). D'abord pour \( n=1\), la matrice \( A\) contient un seul élément \( A_{11}\) qui est non nul par hypothèse. Nous pouvons multiplier sa ligne par \( 1/A_{11}\) pour obtenir le résultat. Plus précisément, nous avons l'égalité
    \begin{equation}
        T(1;1,\frac{1}{ A_{11} })A=\delta
    \end{equation}
    dans \( \eM(1, \eK)\). Notons que \( \eK\) est un corps (donc \( A_{11}\) est inversible) commutatif, ce qui permet d'écrire \( 1/A_{11}\) sans ambiguïtés.

    Supposons le résultat prouvé pour \( n\), et voyons ce qu'il se passe pour \( n+1\). Vu que \( \det(A)\neq 0\), aucune de ses colonnes n'est nulle (corollaire \ref{CORooAZFCooSYINvBl}). Il existe donc un \( k\) tel que \( A_{k1}\neq 0\).

    Par la proposition \ref{PROPooFQRDooRPfuxk}, la matrice
    \begin{equation}
        B^{(1)}=S(n+1;k,1)A
    \end{equation}
    est une matrice telle que \( B^{(1)}_{11}=A_{k1}\neq 0\). Ensuite, par la proposition \ref{PROPooXUFKooOaPnna} la matrice
    \begin{equation}
        B^{(2)}=T(n+1;1,\frac{1}{ A_{k1} })B^{(1)}
    \end{equation}
    vérifie \( B^{(2)}_{11}=1\). 

    Vu que la multiplication par la matrice \( U(n+1;k;l;\lambda)\) fait par la proposition \ref{PROPooPYNHooLbeVhj} la substitution \( L_k\to L_{k}+\lambda L_l\), la matrice
    \begin{equation}
        B^{(3)}=\prod_{k=2}^{n+1}U(n+1;k,1,-B^{(1)}_{k1})B^{(1)}
    \end{equation}
    a toute sa première colonne nulle à l'exception de \( B^{(3)}_{11}=1\).

    Nous n'avons pas donné de nom ni démontré de théorèmes à propos de la substitution \( C_k\to C_k+\lambda C_l\). En passant éventuellement par les transposées et en utilisant les lemmes \ref{LEMooRSJTooQEoOtN} et \ref{LEMooCEQYooYAbctZ} nous obtenons une matrice \( U'(n+1;k,l,\lambda)\) ayant la propriété que la matrice
    \begin{equation}
        B^{(4)}=\prod_{k=2}^{n+1}U'(n+1;k,1,-B^{(3)}_{1k})B^{(3)}
    \end{equation}
    vérifie \( B^{(4)}_{1j}=B^{(4)}_{j1}=0\) pour tout \( j\) sauf \( j=1\). En d'autres termes, la matrice \( B^{(4)}\) est de la forme
    \begin{equation}
        B^{(4)}=\begin{pmatrix}
            1    &   \begin{matrix} 
                0    &   \ldots    &   0    
            \end{matrix}\\ 
            \begin{matrix}
                0    \\ 
                \vdots    \\ 
                0    
            \end{matrix}&   \begin{pmatrix}
                    &       &       \\
                    &   A'    &       \\
                    &       &   
            \end{pmatrix}
        \end{pmatrix}
    \end{equation}
    où \( A'\) est une matrice de taille \( n\).
    
    Voyons quelque propriétés de \( A'\). Nous savons que
    \begin{equation}
        B^{(4)}=\prod_i G_iA
    \end{equation}
    où les \( G_i\) sont de type \( S\), \( T\) ou \( U\). Vu que \( \det(SA)=\det(S)\det(A)\) (et idem pour \( T\) et \( U\)), nous avons
    \begin{equation}
        \det(B^{(4)})=\prod_i\det(G_i)\det(A),
    \end{equation}
    et comme aucun des \( \det(G_i)\) n'est nul, nous avons encore \( \det(B^{(4)})\neq 0\), ce qui implique \( \det(A')\neq 0\).

    La récurrence peut avoir lieu. Il existe des matrices \( G'_i\) telles que
    \begin{equation}
        G'_1\ldots G'_MA'=\delta
    \end{equation}
    où les \( G'_i\) sont de taille \( n\), ainsi que le \( \delta\). En remarquant que
    \begin{equation}
       S(n+1;k,l) =\begin{pmatrix}
            1    &   \begin{matrix} 
                0    &   \ldots    &   0    
            \end{matrix}\\ 
            \begin{matrix}
                0    \\ 
                \vdots    \\ 
                0    
            \end{matrix}& S(n;k-1,l-1)
        \end{pmatrix},
    \end{equation}
    et pareillement pour les matrices \( T\) et \( U\), nous voyons qu'en prenant
    \begin{equation}
       G_i =\begin{pmatrix}
            1    &   \begin{matrix} 
                0    &   \ldots    &   0    
            \end{matrix}\\ 
            \begin{matrix}
                0    \\ 
                \vdots    \\ 
                0    
            \end{matrix}& G'_i
        \end{pmatrix},
    \end{equation}
    nous avons
    \begin{equation}
        \prod_{i=1}^MG_iB^{(3)}=
      \begin{pmatrix}
            1    &   \begin{matrix} 
                0    &   \ldots    &   0    
            \end{matrix}\\ 
            \begin{matrix}
                0    \\ 
                \vdots    \\ 
                0    
            \end{matrix}& \prod_{i=1}^MG'_iA'
        \end{pmatrix}=\delta_{n+1}
    \end{equation}
    où nous avons mis un indice sur le dernier \( \delta\) pour être plus explicite.
\end{proof}

\begin{proposition} \label{PROPooPMYCooAAtHsB}
    Si \( A\in \eM(n,\eK)\) est telle que \( \det(A)=0\), alors il existe des matrices de manipulation de lignes et de colonnes \( G_1,\ldots, G_N\) telles que \( G_1\ldots G_NA\) ait une colonne de zéros.
\end{proposition}

\begin{proof}
    Si la matrice \( A\) elle-même n'a pas de colonnes de zéros, alors nous pouvons faire un pas de réduction de Gauss et obtenir des matrices \( G_1,\ldots,  G_{N_1}\) telles que
    \begin{equation}
        G_1\ldots G_{N_1}A=
      \begin{pmatrix}
            1    &   \begin{matrix} 
                0    &   \ldots    &   0    
            \end{matrix}\\ 
            \begin{matrix}
                0    \\ 
                \vdots    \\ 
                0    
            \end{matrix}& A^{(1)}
        \end{pmatrix}.
    \end{equation}
    Si \( A^{(1)}\) ne possède pas de colonnes de zéros, nous pouvons continuer. 

    Si nous parvenons à faire \( n\) pas de la sorte, alors nous aurions
    \begin{equation}
        G_1\ldots G_NA=\delta,
    \end{equation}
    et donc \( \det(G_1\ldots G_N)\det(A)=1\), ce qui est impossible lorsque \( \det(A)=0\). Nous en concluons que le processus doit s'arrêter et qu'une des matrices \( A^{(k)}\) doit avoir une colonne de zéros\footnote{En réalité, le processus tel que nous l'avons décrit ne s'arrête que lorsque la première colonne est remplie de zéros.}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrices inversibles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}      \label{PROPooMLWRooRWfZXE}
    Soit une matrice \( A\in \eM(n,\eK)\). Si les matrices \( B_1\) et \( B_2\) de \( \eM(n,\eK)\) vérifient
    \begin{equation}
        AB_1=B_1A=\delta
    \end{equation}
    et
    \begin{equation}
        AB_2=B_2A=\delta,
    \end{equation}
    alors \( B_1=B_2\). Dans ce cas, nous disons que \( A\) est inversible et nous notons \( A^{-1}\) l'unique matrice telle que \( AA^{-1}=A^{-1}A=\delta\).
\end{propositionDef}

\begin{proof}
    La preuve est réalisée dans le cas général par le lemme \ref{LEMooECDMooCkWxXf}. Mais si vous en voulez une preuve avec les notations d'ici, en voici une.

    Nous avons $AB_1=AB_2$. En multipliant à gauche par \( B_1\), nous trouvons \( B_1AB_1=B_1AB_2\). En remplaçant \( B_1A\) par \( \delta\) des deux côtés, il reste \( B_1=B_2\).
\end{proof}

\begin{lemma}[\cite{ooKBOMooSkKHvu}]        \label{LEMooGZCTooQigDvC}
    Si \( A\in \eM(n,\eK)\), alors il existe au plus une matrice \( B\in \eM(n,\eK)\) telle que \( AB=\delta\).
\end{lemma}

\begin{proof}
    Soient des matrices \( B,C\in \eM(n,\eK)\) telles que \( AB=AC=\delta\). Nous allons montrer que \( B=C\).

    Pour cela nous considérons les applications linéaires \( f_A, f_B, f_C\in \End(\eK^)\) associées par la proposition \ref{PROPooGXDBooHfKRrv}. Vu que \( AB=\delta\), par la proposition \ref{PROPooCSJNooEqcmFm}, nous avons \( f_A\circ f_B = f_{AB}=\id\). La proposition \ref{PROPooADESooATJSrH} nous dit alors que \( f_A\) et \( f_B\) sont bijectives. 

    En particulier, vu que \( \{e_i\}\) est une base, son image par \( f_B\) est une base par la proposition \ref{PROPooZFKZooBGLSex}. La proposition \ref{PROPooHLUYooNsDgbn} dit alors que \( \{f_B(e_i)\}\) est une base. Nous décomposons \( f_B(e_k)-f_C(e_k)\) dans cette base :
    \begin{equation}
        f_B(e_k)-f_C(e_k)=\sum_j\alpha_jf_B(e_j)
    \end{equation}
    où les \( \alpha_j\) dépendent a priori de \( k\). Vu que \( f_A\circ(f_B-f_C)=0\), nous avons 
    \begin{equation}
        0=f_A\big( f_B(e_k)-f_C(e_k) \big)=\sum_j(f_A\circ f_B)(e_j)=\sum_j\alpha_je_j.
    \end{equation}
    Donc les \( \alpha_j\) sont tous nuls.

    Nous en déduisons que \( f_B(e_k)=f_C(e_k)\), et donc \( f_B=f_C\). Cela implique que \( B=C\) par la proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.
\end{proof}

\begin{proposition}[\cite{ooKBOMooSkKHvu}]      \label{PROPooECIIooVMCIwz}
    Si \( A,B\in \eM(n,\eK)\) vérifient \( AB=\delta\), alors \( BA=\delta\).
\end{proposition}

\begin{proof}
    L'astuce est de poser \( C=BA-\delta+B\) et de montrer que \( C=B\). Pour cela, un rapide calcul commence par montrer que
    \begin{equation}
        AC=ABA-A+AB=AB=\delta.
    \end{equation}
    Donc \( C\) est également un inverse à droite de \( A\). Le lemme \ref{LEMooGZCTooQigDvC} donne alors \( C=B\).
\end{proof}

\begin{corollary}       \label{CORooBQLXooTeVfgb}
    Soit \( A\in \eM(n,\eK)\). Si il existe \( B\in \eM(n,\eK)\) tel que \( AB=\delta\), alors \( A\) est inversible et son inverse est \( B\).
\end{corollary}

\begin{proof}
    Il s'agit d'une paraphrase de la proposition \ref{PROPooECIIooVMCIwz} et de la définition \ref{PROPooMLWRooRWfZXE}.
\end{proof}

\begin{lemma}       \label{LEMooZDNVooArIXzC}
    Si une matrice \( A\) n'est pas inversible, alors le produit \( AB\) n'est inversible pour aucune matrice \( B\).
\end{lemma}

\begin{proof}
    Supposons que \( AB\) soit inversible. Alors
    \begin{equation}
        AB(AB^{-1})=\delta,
    \end{equation}
    ce qui dirait que \( B(AB^{-1})\) serait un inverse de \( A\).
\end{proof}

\begin{proposition}     \label{PROPooNPMCooPmaCwu}
    Une matrice est inversible si et seulement si son application linéaire associée est inversible. Dans ce cas, nous avons
    \begin{equation}
        f_A^{-1}=f_{A^{-1}}.
    \end{equation}
\end{proposition}

\begin{proof}
    Dans le sens direct, si \( A\) est inversible nous avons \( AA^{-1}=\delta\). Donc
    \begin{equation}        \label{EQooQQOSooBKVqXh}
        f_A\circ f_{A^{-1}}=f_{AA^{-1}}=f_{\delta}=\id
    \end{equation}
    où nous avons utilisé la proposition \ref{PROPooCSJNooEqcmFm} pour la composition et la proposition \ref{PROPooFMBFooEVCLKA} pour l'identité. L'égalité \eqref{EQooQQOSooBKVqXh} indique que \( f_A\) est inversible et que son inverse est \( f_{A^{-1}}\).

    Dans l'autre sens, l'application \( f_A^{-1}\) existe. Soit \( B\in \eM(n,\eK)\) sa matrice. Alors nous avons
    \begin{equation}
        f_{\delta}=\id=f_A\circ f_B=f_{AB}.
    \end{equation}
    Le fait que l'application \(\psi\colon A\to f_A\) soit une bijection\footnote{Proposition \ref{PROPooGXDBooHfKRrv}\ref{ITEMooHSMLooRJZref}.} implique que \( AB=\delta\), c'est-à-dire que \( A\) est inversible et que \( B=A^{-1}\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Inversibilité et déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooAVIXooMtVCet}
    Une matrice au déterminant non nul est inversible.
\end{proposition}

\begin{proof}
    Si \( A\) est une matrice telle que \( \det(A)\neq 0\), alors la proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices \( G_1,\ldots, G_N\) telles que
    \begin{equation}
        G_1\ldots G_NA=\delta.
    \end{equation}
    Donc la matrice \( G_1\ldots G_N\) est un inverse de \( A\) par le corollaire \ref{CORooBQLXooTeVfgb}.
\end{proof}

\begin{proposition}     \label{PROPooEOKBooKUROFg}
    Si une matrice \( A\) a une ligne ou une colonne de zéros, alors
    \begin{enumerate}
        \item
            \( \det(A)=0\),
        \item
            \( A\) n'est pas inversible.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Par définition nous avons
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^nA_{i\sigma(i)}.
    \end{equation}
    Si la \( k\)\ieme ligne est nulle, alors \( A_{k\sigma(k)}=0\) pour tout \( \sigma\). Donc tous les produits contiennent un facteur nul. Donc \( \det(A)=0\).

    Pour toute matrice \( B\) nous avons
    \begin{equation}
        (AB)_{kk}=\sum_lA_{kl}B_{lk}.
    \end{equation}
    Si la \( k\)\ieme ligne de \( A\) est nulle nous avons \( (AB)_{kk}=0\) et donc pas \( AB=\delta\). Donc \( A\) n'est pas inversible.
\end{proof}

\begin{proposition}     \label{PROPooVUDJooLWjmSI}
    Une matrice dont le déterminant est nul n'est pas inversible.
\end{proposition}

\begin{proof}
    Par la proposition \ref{PROPooPMYCooAAtHsB}, il existe des matrices de manipulation de lignes et de colonnes \( G_1,\ldots, G_N\) telles que la matrice \( G_1\ldots G_NA\) ait une colonne de zéros. De là, la proposition \ref{PROPooEOKBooKUROFg} implique que la matrice
    \begin{equation}        \label{EQooQGXBooXxFOtb}
        G_1\ldots G_NA
    \end{equation}
    n'est pas inversible. Vu les déterminants des matrices \( G_i\),  la proposition \ref{PROPooAVIXooMtVCet} implique que \( G_1\ldots G_N\) est inversible. Si \( A\) était inversible, nous aurions
    \begin{equation}
        G_1\dots G_NAA^{-1}(G_1\ldots G_N)^{-1}=\delta,
    \end{equation}
    c'est-à-dire que \( A^{-1}(G_1\ldots G_N)^{-1}\) serait un inverse de la matrice \eqref{EQooQGXBooXxFOtb}. Cette dernière n'ayant pas d'inverse, nous concluons que \( A\) n'en a pas non plus.
\end{proof}

\begin{theorem}     \label{THOooSNXWooSRjleb}
    Une matrice sur un corps commutatif est inversible si et seulement si son déterminant est non nul.
\end{theorem}

\begin{proof}
    Dans un sens c'est la proposition \ref{PROPooAVIXooMtVCet} et dans l'autre sens c'est la proposition \ref{PROPooVUDJooLWjmSI}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques ensembles de matrices particuliers}
%---------------------------------------------------------------------------------------------------------------------------
Certains ensembles de matrices ont une importance particulière, que nous développerons plus tard.

\begin{definition}[Groupe linéaire de matrices]
On note \( \GL(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont inversibles. En d'autres termes, \( \GL(n,\eA) = U (\eM (n,\eA) ) \).
\end{definition}

\begin{definition}[Groupe orthogonal de matrices]\label{DefMatriceOrthogonale}
    On dit qu'une matrice \( A \) est \defe{orthogonale}{matrice!orthogonale} si son inverse est sa transposée, c'est-à-dire si \( A^{-1} = A^t \). On note \( \gO(n,\eA) \) l'ensemble des matrices carrées d'ordre \( n \) à coefficients dans \( \eA \), qui sont orthogonales.
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Déterminant et combinaisons de lignes et colonnes}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooKMSVooBBHwkH}

\begin{proposition}     \label{PROPooUCZVooPkloQp}
    Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)\neq 0\neq \det(B)\). Alors
    \begin{equation}
        \det(AB)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    La proposition \ref{PROPooJBTZooNLobpf} nous donne des matrices de permutations de lignes et de colonnes \( G_1,\ldots, G_N\) et \( G'_1,\ldots, G'_N\) telles que\footnote{Les plus acharnés préciseront que pour avoir le même \( N\) des deux côtés, il a fallu compléter avec des matrices \( \delta\) là où il y en avait le moins.}
    \begin{subequations}        \label{EQooDNZUooHBhcZj}
        \begin{align}
            G_1\ldots G_NA&=\delta\\
            G'_1\ldots G'_NB&=\delta.
        \end{align}
    \end{subequations}
    Nous avons
    \begin{equation}
        (G'_1\ldots G'_N)\underbrace{(G_1\ldots G_N)A}_{=\delta}B=\delta.
    \end{equation}
    En prenant le déterminant des deux côtés et en tenant compte de \eqref{EQooLQTVooBYjVYl},
    \begin{equation}
        1=\det(\delta)=\det\big(  G'_1\ldots G'_NG_1\ldots G_NAB\big)=\det(G_1'\ldots G_N')\det(G_1\ldots G_N)\det(AB).
    \end{equation}
    Mais en même temps, les équations \ref{EQooDNZUooHBhcZj} donnent
    \begin{subequations}
        \begin{align}
            \det(G_1\ldots G_N)=\det(A)^{-1}\\
            \det(G_1'\ldots G'_N)=\det(B)^{-1}.
        \end{align}
    \end{subequations}
    Cela pour dire que
    \begin{equation}
        1=\det(A)^{-1}\det(B)^{-1}\det(AB),
    \end{equation}
    et donc ce qu'il nous fallait.
\end{proof}

\begin{proposition}     \label{PROPooWVJFooTmqoec}
    Soient des matrices \( A,B\in \eM(n,\eK)\) telles que \( \det(A)=0\) et \( \det(B)\neq0\). Alors
    \begin{equation}
        \det(AB)=\det(BA)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Il existe des matrices de manipulations de lignes et de colonnes \( G_1,\ldots, G_N\) telles que \( G_1\ldots G_NB=\delta\). Donc
    \begin{equation}
        0=\det(A)=\det(G_1\ldots G_NBA)=\det(G_1\ldots G_N)\det(BA).
    \end{equation}
    Donc \( \det(BA)=0\).
\end{proof}

\begin{proposition}     \label{PROPooHQNPooIfPEDH}
    Soient des matrices \( A\) et \( B\) sur un corps commutatif. Alors
    \begin{equation}
        \det(AB)=\det(A)\det(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Les propositions \ref{PROPooUCZVooPkloQp} et \ref{PROPooWVJFooTmqoec} ont déjà fait une grosse partie du travail. Il ne reste que le cas où \( \det(A)=\det(B)=0\).

    Dans ce cas, les matrices \( A\) et \( B\) ne sont pas inversibles (proposition \ref{THOooSNXWooSRjleb}). Le produit \( AB\) n'est alors pas inversible non plus\footnote{Citez le lemme \ref{LEMooZDNVooArIXzC} si vous voulez justifier ça.}. La proposition \ref{THOooSNXWooSRjleb}, utilisée dans le sens inverse, nous dit alors que \( \det(AB)=0\).

    Au final dans le cas \( \det(A)=\det(B)=0\) nous avons \( 0=\det(AB)=\det(A)\det(B)=0\).
\end{proof}

Faisons maintenant le cas général des manipulations de lignes et colonnes.

\begin{proposition}     \label{PROPooSLLGooSZjQrv}
    Soit une matrice carré \( A\in \eM(n,\eK)\). La matrice \( B\) obtenue par la substitution simultanée
    \begin{equation}
        C_j\to \sum_ka_{kj}C_k
    \end{equation}
    a pour déterminant
    \begin{equation}
        \det(B)=\det(a)\det(A).
    \end{equation}
\end{proposition}

\begin{proof}
    L'élément \( B_{ij}\) de la matrice \( B\) est une combinaison linéaire de tous les éléments de sa ligne :
    \begin{equation}
        B_{ij}=\sum_ka_{kj}A_{ik}=(Aa)_{ij}.
    \end{equation}
    Donc \( B=Aa\). La proposition \ref{PROPooHQNPooIfPEDH} nous dit alors que \( \det(B)=\det(a)\det(A)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transvections}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( E_{ij}\) la matrice remplie de zéros sauf à la case \( ij\) qui vaut \( 1\). Autrement dit
\begin{equation}
    (E_{ij})_{kl}=\delta_{ik}\delta_{jl}.
\end{equation}
\begin{definition}
    Une \defe{matrice de transvection}{transvection (matrice)}\index{matrice!de transvection} est une matrice de la forme
    \begin{equation}
        T_{ij}(\lambda)=\id+\lambda E_{ij}
    \end{equation}
    avec \( i\neq j\).

    Une \defe{matrice de dilatation}{matrice!de dilatation}\index{dilatation (matrice)} est une matrice de la forme
    \begin{equation}
        D_i(\lambda)=\id+(\lambda-1)E_{ii}.
    \end{equation}
    Ici le \( (\lambda-1)\) sert à avoir \( \lambda\) et non \( 1+\lambda\). C'est donc une matrice qui dilate d'un facteur \( \lambda\) la direction \( i\) tout en laissant le reste inchangé.

    Si \( \sigma\) est une permutation (un élément du groupe symétrique \( S_n\)) alors la \defe{matrice de permutation}{matrice!de permutation}\index{permutation!matrice} associée est la matrice d'entrées
    \begin{equation}
        (P_{\sigma})_{ij}=\delta_{i\sigma(j)}.
    \end{equation}
\end{definition}

\begin{lemma}   \label{LemyrAXQs}
    La matrice \( T_{ij}(\lambda)A=(\mtu+\lambda E_{ij})A\) est la matrice \( A\) à qui on a effectué la substitution
    \begin{equation}
        L_i\to L_i+\lambda L_j.
    \end{equation}
    La matrice \( AT_{ij}(\lambda)\) est la substitution
    \begin{equation}
        C_j\to C_j+\lambda C_i.
    \end{equation}

    La matrice \( AP_{\sigma}\) est la matrice \( A\) dans laquelle nous avons permuté les colonnes avec \( \sigma\).

    La matrice \( P_{\sigma}A\) est la matrice \( A\) dans laquelle nous avons permuté les lignes avec \( \sigma^{-1}\).
\end{lemma}

\begin{proof}
    Calculons la composante \( kl\) de la matrice \( E_{ij}A\) :
    \begin{subequations}
        \begin{align}
            (E_{ij}A)_{kl}&=\sum_m(E_{ij})_{km}A_{ml}\\
            &=\sum_m\delta_{ik}\delta_{jm}A_{ml}\\
            &=\delta_{ik}A_{jl}.
        \end{align}
    \end{subequations}
    C'est donc la matrice pleine de zéros, sauf la ligne \( i\) qui est donnée par la ligne \( j\) de \( A\). Donc effectivement la matrice
    \begin{equation}
        A+\lambda E_{ij}A
    \end{equation}
    est la matrice \( A\) à laquelle on a substitué la ligne \( i\) par la ligne \( i\) plus \( \lambda\) fois la ligne \( j\).

    En ce qui concerne l'autre assertion sur les transvections, le calcul est le même et nous obtenons
    \begin{equation}
        (AE_{ij})=A_{ki}\delta_{jl}.
    \end{equation}

    Pour les matrices de permutation, nous avons
    \begin{equation}
        (AP_{\sigma})_{kl}=A_{k\sigma(l)}
    \end{equation}
    et
    \begin{equation}
        (P_{\sigma}A)_{kl}=\sum_m\delta_{k\sigma(m)}A_{ml}=\sum_m\delta_{\sigma^{-1}(k)m}A_{ml}=A_{\sigma^{-1}(k)l}.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Mineur, rang}
%---------------------------------------------------------------------------------------------------------------------------

Pour la définition du rang d'une matrice, nous en donnons une qui est clairement inspirée de l'application linéaire associée.
\begin{definition}[\cite{ooKBOMooSkKHvu}]       \label{DEFooCSGXooFRzLRj}
    Le \defe{rang}{rang d'une matrice} d'une matrice de \( \eM(n,\eK)\) est la dimension de la partie de \( \eK^n\) engendrée par ses colonnes.
\end{definition}

Il est possible d'exprimer le rang d'une matrice de façon plus «intrinsèque» via le concept de mineur.
\begin{definition}[\cite{ooLFZTooJWJUed}]
    Les mineurs d'une matrice sont les déterminants de ses sous-matrices carrées.
\end{definition}
Dans la suite nous désignerons souvent par le mot «mineur» la sous-matrice carrée elle-même au lieu de son déterminant.

\begin{proposition}      \label{DEFooVVBYooJbliTi}
    Le rang d'une matrice est la taille de son plus grand mineur non nul.
\end{proposition}

\begin{lemma} \label{LEMVecsaRgFixe}
    Soit \( \eK \) un corps commutatif\footnote{Comme toujours.}. Si \( A \) est une matrice carrée d'ordre \( n \) et de rang \( r \) à coefficients dans \( \eK \), alors il existe des vecteurs \( (x_i)_{i=1,\dots,n} \) formant une base de \( \eK^n \) tels que 
    \begin{equation}
        f_A(x_i)\neq 0
    \end{equation}
    pour \( x\leq r\) et
    \begin{equation}
        f_A(x_i) = 0
    \end{equation}
    pour \( i > r \).

    Ici, \( f_A\) est l'application linéaire associée à la matrice \( A\) par l'application \eqref{EQooVZQWooMyFFeO}.
\end{lemma}

\begin{proof}
    Soit \( V\) le sous-espace de \( \eK^n\) engendré par les colonnes de \( A\). Nous considérons la base canonique \( \{ e_i \}\) de \( \eK^n\), ainsi que \( v_i\) le vecteur créé par la \( i\)\ieme colonne de \( A\). Nous avons
    \begin{equation}
        v_i=f_A(e_i).
    \end{equation}
    Les vecteurs \( v_i\) engendrent \( V\), donc nous pouvons en extraire une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooTZUDooFEgymQ}. Soit donc \( \{ v_j \}_{i\in J}\) une base de \( V\) avec \( J\subset\{ 1,\ldots, n \}\).

    La base de \( \eK^n\) que nous cherchons commence par les vecteurs \( \{ e_j \}_{j\in J}\). Ces vecteurs vérifient \( f_A(e_j)=v_j\neq 0\) parce que des vecteurs d'une base ne sont jamais nuls.

    % Note : toute la ligne suivante fait des références qui peuvent être vers le futur parce que ce sont des choses qui ne sont 
    %        pas utilisées dans la démonstration.
    Pour la suite de la base, nous pourrions penser au théorème de la base incomplète\footnote{Théorème \ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu}.}, mais les vecteurs ainsi complétant la base ne sont pas garantis de s'annuler par \( f_A\). Voir l'exemple \ref{EXooRKVQooZOGDEf}.

    L'idée est d'utiliser le noyau de \( f_A\) qui est un sous-espace vectoriel par la proposition \ref{PROPooRLLPooKYzsJp}. Soit une base\footnote{Cette base contient \( n-r\) éléments, mais ce n'est pas très important pour la suite.} \(  \{ z_k \}  \) de \( \ker(f)\). Les vecteurs \( \{ e_j \}_{j\in J}\) forment une base de \( \Image(f_A)\). Vu que les \( z_i\) forment une base de \( \ker(f_A)\), le théorème du rang \ref{ThoGkkffA} dit alors que \( \{ e_j \}_{j\in J}\cup \{ z_k \}\) est une base de \( \eK^n\).

    Il y a \( r\) éléments dans \( J\) parce que l'espace engendré par les colonnes de \( A\) est de dimension \( r\) par hypothèse. Donc il y a \( n-r\) éléments dans les \( z_k\) pour que le tout ait le bon nombre d'éléments.
\end{proof}

\begin{example}     \label{EXooRKVQooZOGDEf}
    Soit la matrice 
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\ 
            2    &   2    
        \end{pmatrix}.
    \end{equation}
    Elle est de rang \( 1\). En suivant l'idée de la démonstration, nous commençons la base de \( \eR^2\) par le vecteur \( e_1\) qui vérifie
    \begin{equation}
        f_A(e_1)=\begin{pmatrix}
            1    \\ 
            2    
        \end{pmatrix}.
    \end{equation}
    L'utilisation du théorème de la base incomplète ne permet pas de trouver un second vecteur de base \( v\) tel que \( f_A(v)=0\). En effet ce théorème donne juste l'existence d'une completion de la base, mais pas de propriétés particulières de la base obtenue. Elle pourrait donner \( v=e_2\) comme second vecteur de base. Mais alors
    \begin{equation}
        f_A(v)=f_A(e_2)=\begin{pmatrix}
            1    \\ 
            2    
        \end{pmatrix}\neq 0.
    \end{equation}

    Au contraire, le noyau de \( f_A\) est donné par le sous-espace engendré par \( \begin{pmatrix}
        1    \\ 
        -1    
    \end{pmatrix}\). Une base convenable est donc \( \{ e_1, e_1-e_2 \}\).
\end{example}

\begin{proposition}     \label{PROPooEGNBooIffJXc}
    Le rang d'une application linéaire est égal au rang de sa matrice dans n'importe quelle base.
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices équivalentes et semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefBLELooTvlHoB}
    Deux relations d'équivalence entre les matrices.
    \begin{enumerate}
        \item   \label{ItemPFXCooOUbSCt}
    Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) s'il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\).
\item
    Deux matrices sont \defe{semblables}{matrices!similitude} s'il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
    \end{enumerate}
\end{definition}

\begin{lemma}   \label{LemZMxxnfM}
    Une matrice de rang\footnote{Définition~\ref{DEFooVVBYooJbliTi}.} \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
    Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\), qui existe par le lemme~\ref{LEMVecsaRgFixe}.

    Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\); ses colonnes sont donc précisément les \( f_i \), si bien que
    \begin{equation}
        APe_i=Af_i=\begin{cases}
            0    &   \text{si } i>r\\
            \neq 0    &    \text{sinon}.
        \end{cases}
    \end{equation}
    La matrice \( AP\) se présente donc sous la forme
    \begin{equation}
        AP=\begin{pmatrix}
            M    &   0    \\
            *    &   0
        \end{pmatrix}
    \end{equation}
    où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
    \begin{equation}
        QAPe_i=\begin{cases}
            e_i    &   \text{si } i<r\\
            0    &    \text{sinon}.
        \end{cases}.
    \end{equation}
    Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{corollary}[Équivalence et rang]      \label{CorGOUYooErfOIe}
    Deux matrices sont équivalentes\footnote{Définition~\ref{DefBLELooTvlHoB}\ref{ItemPFXCooOUbSCt}.} si et seulement si elles sont de même rang.
\end{corollary}

\begin{proof}
    D'abord il y a des implicites dans l'énoncé. Vu que nous voulons soit par hypothèse soit par conclusion que les matrices \( A\) et \( B\) soient équivalentes, nous supposons qu'elles ont même dimension. Soient donc \( A\) et \( B\) deux matrices carrées d'ordre \( n \).

    Par le lemme~\ref{LemZMxxnfM}, deux matrices de même rang \( r\) sont équivalentes à \( J_r\). Elles sont donc équivalentes entre elles.

    Inversement, supposons que \( A\) et \( B\) soient deux matrices équivalentes : \( A=PBQ^{-1}\) avec \( P\) et \( Q\) inversibles. Alors
    \begin{subequations}
        \begin{align}
            \Image(PBQ^{-1})&=\{ PBQ^{-1}v\tq v\in \eK^n \}\\
            &=PB\underbrace{\{ Q^{-1}v\tq v\in \eK^n \}}_{=\eK^n}\\
            &=P\big( B(\eK^n) \big).
        \end{align}
    \end{subequations}
    L'ensemble \( B(\eK^n)\) est un sous-espace vectoriel de \( \eK^n\). Vu que le rang de \( P\) est maximum, la dimension de \( P\big( B(\eK^n) \big)\) est la même que celle de \( B(\eK^n)\). Par conséquent
    \begin{equation}
        \dim\Big( \Image(PBQ^{-1}) \Big)=\dim\big( B(\eK^n) \big)=\rang(B).
    \end{equation}
    Le membre de gauche de cela n'est autre que \( \rang(A)=\dim\big( \Image(PBQ^{-1}) \big)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithme des facteurs invariants}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Algorithme des facteurs invariants\cite{KXjFWKA}]   \label{PropPDfCqee}
    Soit \( (\eA,\delta)\) un anneau euclidien muni de son stathme  et \( U\in \eM(n \times m,\eA)\). Alors il existe \( d_1,\ldots, d_s\in \eA^*\) et des matrices \( P\in\GL(m,\eA)\), \( Q\in \GL(n,\eA)\) tels que nous ayons
    \begin{equation}
        U=P \begin{pmatrix}
            \begin{matrix}
                d_1    &       &       \\
                    &   \ddots    &       \\
                    &       &   d_s
            \end{matrix}&   0    \\
            0    &   0
        \end{pmatrix}Q
    \end{equation}
    avec \( d_i\divides d_{i+1}\) pour tout \( i\).
\end{proposition}
\index{anneau!euclidien!facteurs invariants}
\index{algorithme!facteurs invariants}

\begin{proof}
    Nous allons donner la preuve plus ou moins sous forme d'algorithme.

    D'abord si \( U=0\) c'est bon, on a la réponse. Sinon, nous prenons l'élément \( (i_0,j_0)\) dont le stathme est le plus petit et nous l'amenons en \( (1,1)\) par les permutations
    \begin{equation}
        \begin{aligned}[]
            C_1&\leftrightarrow C_{j_0}\\
            L_1&\leftrightarrow L_{i_0}
        \end{aligned}
    \end{equation}
    Ensuite nous traitons la première colonne jusqu'à amener des zéros partout en dessous de \( u_{11}\) de la façon suivante : pour chaque ligne successivement nous calculons la division euclidienne
    \begin{equation}
        u_{i1}=qu_{11}+r_i,
    \end{equation}
    et nous faisons
    \begin{equation}
        L_i\to L_i-qL_1,
    \end{equation}
    c'est-à-dire que nous enlevons le maximum possible et il reste seulement \( r_i\) en \( u_{i1}\). Vu que le but est de ne laisser que des zéros dans la première colonne, si le reste n'est pas zéro nous ne sommes pas content\footnote{S'il est zéro, nous passons à la ligne suivante}. Dans ce cas nous permutons \( L_1\leftrightarrow L_i\), ce qui aura pour effet de strictement diminuer le stathme de \( u_{11}\) parce qu'on va mettre en \( u_{11}\) le nombre \( r_i\) dont le stathme est strictement plus petit que celui de \( u_{11}\).

    En faisant ce jeu de division euclidienne puis échange, on diminue toujours le stathme de \( u_{11}\), donc ça finit par s'arrêter, c'est-à-dire qu'à un certain moment la division euclidienne de \( u_{i1}\) par \( u_{11}\) va donner un reste zéro et nous serons content.

    Une fois la première colonne ramenée à la forme
    \begin{equation}
        C_1=\begin{pmatrix}
            u_{11}    \\
            0    \\
            \vdots    \\
            0
        \end{pmatrix},
    \end{equation}
    nous faisons tout le même jeu avec la première ligne en faisant maintenant des sommes divisions et permutations de colonnes. Notons que ce faisant nous ne changeons plus la première colonne.

    En fin de compte nous trouvons une matrice\footnote{Nous nommons toujours par la même lettre \( U\) la matrice originale et la modifiée, comme il est d'usage en informatique.}
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Si l'élément \( u_{11}\) ne divise pas un des éléments de \( A\), disons \( a_{ij}\), alors nous faisons
    \begin{equation}
        C_1\to C_1-C_j.
    \end{equation}
    Cela nous détruit un peu la première colonne, mais ne change pas \( u_{11}\). Nous avons maintnant
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             *   &       &       &       \\
             u_{ij}   &       &   A    &       \\
             *   &       &       &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Et nous refaisons tout le jeu depuis le début. Cependant lorsque nous allons nous attaquer à la ligne \( i\), \( u_{11}\) ne divisera pas \( u_{ij}\), ce qui donnera lieu à une division euclidienne et un échange \( L_1\leftrightarrow L_i\). L'échange consistant à mettre \( r_i\) à la place de \( u_{11}\) et inversement  diminuera encore strictement le stathme. Encore une fois nous allons travailler jusqu'à avoir la matrice sous la forme
    \begin{equation}    \label{EqADcNVgI}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix},
    \end{equation}
    sauf que cette fois le stathme de \( u_{11}\) est strictement plus petit que la fois précédente. Si \( u_{11}\) ne divise toujours pas tous les éléments de \( A\), nous recommençons encore et encore. En fin de compte nous finissons par avoir une matrice de la forme \eqref{EqADcNVgI} avec \( u_{11}\) qui divise tous les éléments de \( A\).

    Une fois que cela est fait, il faut continuer en recommençant tout sur la matrice \( A\). Nous avons maintenant
    \begin{equation}
        U=\begin{pmatrix}
            \begin{matrix}
                u_{11}  &       \\
                &   u_{22}
            \end{matrix}&   0    \\
            0    &   B
        \end{pmatrix}.
    \end{equation}
    Sous cette forme nous avons \( u_{11}\divides u_{22}\) et \( u_{11}\) divise tous les éléments de \( B\). En effet \( u_{11}\) divisant tous les éléments de \( A\), il divise toutes les combinaisons de ces éléments. Or tout l'algorithme ne consiste qu'à prendre des combinaisons d'éléments.

    Nous finissons donc bien sûr une matrice comme annoncée. De plus n'ayant effectué que des combinaisons de lignes, nous avons seulement multiplié par des matrices inversibles (lemme~\ref{LemyrAXQs}).
\end{proof}



%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de polynômes}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecEspacePolynomes}

Attention : les polynômes en soi sont définis par la définition~\ref{DefRGOooGIVzkx}.

Pour chaque $k>0$ donné nous définissons
\begin{equation}
\mathcal{P}_\eR^k=\{p:\eR\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k, \, a_i\in\eR,\,\forall i=0,\ldots,k\}.
\end{equation}
Il est facile de se convaincre que la somme de deux polynômes de degré inférieur ou égal à $k$ est encore un polynôme de degré inférieur ou égal à $k$. En outre il est clair que la multiplication par un scalaire ne peut pas augmenter le degré d'un polynôme. L'ensemble $\mathcal{P}_\eR^k$ est donc un espace vectoriel muni des opérations héritées de $\mathcal{P}_{\eR}$.

La base canonique de l'espace $\mathcal{P}_\eR^k$ est donné par les monômes $\mathcal{B}=\{x\mapsto x^j \,|\, j=0, \ldots, k\}$. Le fait que cela soit une base est vraiment facile à démontrer et est un exercice très utile si vous ne l'avez pas encore vu dans un cours précédent.

Nous allons maintenant étudier trois applications linéaires de $\mathcal{P}_\eR^k$ vers des autres espaces vectoriels
\begin{description}
  \item[L'isomorphisme canonique  $\phi:\mathcal{P}_\eR^k \to\eR^{k+1}$] Nous définissons $\phi$ par les relations suivantes
\[
\phi(x^j)=e_{j+1}, \qquad \forall j\in\{0,\dots, k\}.
\]
Cela veut dire que pour tout $p$ dans $\mathcal{P}_\eR^k$, avec $p(x)=a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k$, l'image de $p$ par $\phi$ est
\[
\phi(p)=\phi\left(\sum_{j=0}^k a_j x^j\right)=\sum_{j=0}^k a_j e_{j+1}.
\]
\begin{example} Soit $k=5$ on a
  \begin{equation}
    \phi(-8-7x-4x^2+4x^3+2x^5)=
  \begin{pmatrix}
    -8\\
    -7\\
    -4\\
    4\\
    0\\
    2
  \end{pmatrix}.
  \end{equation}
\end{example}

Cette application est clairement bijective et respecte les opérations d'espace vectoriel, donc elle est un isomorphisme d'espaces vectoriels. L'existence d'un isomorphisme entre $\mathcal{P}_\eR^k$  et $\eR^{k+1}$ est un cas particulier du théorème qui dit que  pour chaque $m$ dans $\eN_0$ fixée, tous les espaces vectoriels sur $\eR$ de dimension $m$ sont isomorphes à $\eR^m$. Vous connaissez peut être déjà ce théorème depuis votre cours d'algèbre linéaire.
    \item[La dérivation $d: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k-1}$] L'application de dérivation $d$ fait exactement ce qu'on s'attend d'elle
\[
d(x^0)=d(1)=0, \qquad d(x^j)=j x^{j-1}, \quad \forall j\in\{1,\dots, k\}.
\]
Cette application n'est pas injective, parce que l'image de $p$ ne dépend pas de la valeur de $a_0$, donc si deux polynômes sont les mêmes à une constante près ils auront la même image par $d$.

\begin{example} Soit $k=3$ on a
  \begin{equation}
    d(-8-12x+4x^3)= -12 (1) + 4 (3x^2) = -12+12 x^2.
    \end{equation}

    Noter que $d(-30-12x+4x^3)=d(-8-12x+4x^3)$. Cela confirme, comme mentionné plus haut, que la dérivée n'est pas injective.
\end{example}
      \item[L'intégration $I: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k+1}$] Nous pouvons définir une application que est <<à une constante prés>> l'application inverse de la dérivation. Cette application est définie sur les éléments de base par
          \begin{equation}
                I(x^j)= \frac{x^{j+1}}{j+1}.
          \end{equation}
          Bien entendu la raison d'être et la motivation de cette définition apparaîtra lorsque nous développerons une théorie générale de l'intégration.

\begin{example}
   Soit $k=4$ on a
  \begin{equation}
    I(6+2x+x^2+x^4)= 6x+x^2+\frac{x^3}{3}+\frac{x^5}{5}.
    \end{equation}
\end{example}

Remarquez que, étant donné que dans la définition de $I$ nous avons décidé d'intégrer entre zéro et $x$, tous les polynômes dans $\mathcal{P}_\eR^{k+1}$ qui sont l'image par $I$ d'un polynôme de $\mathcal{P}_\eR^{k}$ ont $a_0=0$. Cela veut dire que nous pouvons générer toute l'image de $I$ en utilisant un sous-ensemble de la base canonique de $\mathcal{P}_\eR^{k+1}$,  en particulier $\mathcal{B}_1=\{x\mapsto x^j \,|\, j=1, \ldots, k\}\subset \mathcal{B}$ nous suffira. Cela n'est guère surprenant, parce que l'image par une application linéaire d'un espace vectoriel de dimension finie ne peut pas être un espace de dimension supérieure.
\end{description}

Les applications de dérivation et intégration correspondent évidemment à des applications linéaires de $\mathcal{P}_\eR$ dans lui-même.

L'espace de tous les polynômes étant de dimension infinie, il peut servir de contre-exemple assez simple. Dans la sous-section~\ref{SubSecPOlynomesCE}, nous verrons que toutes les normes ne sont pas équivalentes sur l'espace des polynômes.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Théorème de Sylvester}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% TODO : Il y a une démonstration sur wikipédia, à voir.
\begin{theorem}[de Sylvester]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature \( (p,q)\). Alors pour toute base orthonormée on a
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}\\
            q&=\Card\{ i\tq Q(e_i)<0 \}.
        \end{align}
    \end{subequations}
    Le rang de \( Q\) est \( p+q\).

    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}
\index{matrice!semblables}
\index{forme!quadratique}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropEJBZooTNFPRj}
    Si \( A\) est la matrice d'une application linéaire, alors le rang de cette application linéaire est égal au rang de \( A \), c'est-à-dire à la taille de la plus grande matrice carrée de déterminant non nul contenue dans \( A\).
\end{proposition}

\begin{definition}  \label{DefJPGSHpn}
    Soit \( E\) un espace vectoriel sur \( \eK\).

    Une \defe{forme linéaire}{forme linéaire} sur \( E \) est une application linéaire de \( E \) sur son corps de base \( \eK\).

    Le \defe{dual algébrique}{dual algébrique} de \( E\), noté \( E^*\), l'ensemble des formes linéaires sur \( E\). Ainsi, \( E^* = \GL(E,\eK)\).
\end{definition}

Nous verrons plus tard qu'en dimension infinie, les applications linéaires ne sont pas toujours continues. Nous définirons donc aussi un concept de dual topologique. Voir la proposition~\ref{PROPooQZYVooYJVlBd}, la remarque~\ref{RemOAXNooSMTDuN} et la définition~\ref{DEFooKSDFooGIBtrG}.

\begin{definition}      \label{DEFooTMSEooZFtsqa}
    Si \( E\) est un espace vectoriel et si \( \{ e_i \}\) est une base de \( E\), alors nous définissons la \defe{base duale}{base!duale} de \( E^*\) par
    \begin{equation}
        e_i^*(e_j)=\delta_{ij}
    \end{equation}
    est sa prolongation par linéarité.
\end{definition}
Notons que si \( v\in E\) est un vecteur, ça n'a aucun sens a priori de parler de \( v^*\). Il s'agit bien de définir \emph{toute} la base \( \{ e_i^* \}\) à partir de toute la base \( \{ e_i \}\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooEQSMooHVzbfz}
    Soit \( E\), un espace vectoriel, et \( F\) une sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
    \begin{equation}    \label{Eqiiyple}
        F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
    \end{equation}
\end{definition}

Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) munie du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition~\ref{DEFooEQSMooHVzbfz}, au contraire, est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
    B^o=\{ x\in E\tq \omega(x)=0\,\forall \omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual et on prend une notation précise pour dire qu'on remonte au pré-dual et non qu'on va au dual du dual.

\begin{proposition} \label{PropXrTDIi}
    Soient un espace vectoriel \( E\) et un sous-espace vectoriel \( F\). Nous avons
    \begin{equation}
        \dim F+\dim F^{\perp}=\dim E.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème~\ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\).

    Déjà c'est une partie libre en tant que partie d'une base.

    Ensuite ce sont des éléments de \( F^{\perp}\) parce que si \( i\leq p\) et si \( k\geq 1\), nous avons \( e^*_{p+k}(e_i)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

    Enfin \( F^{\perp}\subset\Span\{ e_{k}^*, k \in \{p+1, \dots, n\}\}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{k=p+1}^n\omega_ke^*_k\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : pas d'approche naïve}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooGPXVooEYwIiJ}

Il est légitime, si \( t\colon E\to E\) est une application linéaire, de dire que sa transposée soit l'application linéaire \( t^t\colon E\to E\) dont la matrice est la matrice transposée de celle de \( t\). Lorsque nous travaillons sur \( \eR^n\) muni de la base canonique, cela ne pose pas de problèmes et nous pouvons écrire des égalités du type \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).

Hélas nous allons voir que cette façon de définir une transposée est mauvaise.

Soit une application linéaire \( t\colon E\to E\) de matrice \( A\) dans la base \( \{ e_i \}_{i=1,\ldots, n}\) et de matrice \( B\) dans la base \( \{ f_{\alpha} \}_{\alpha=1,\ldots, n}\).

Nous nommons \( t_1\) l'application linéaire associée à \( A^t\) dans la base \( \{ e_i \}\) et \( t_2\) l'application linéaire associée à la matrice \( B^t\) dans la base \( \{ f_{\alpha} \}\). Définir la transposée d'une application linéaire comme étant l'application linéaire associée à la transposée de sa matrice ne sera une bonne définition que si \( t_1=t_2\).

La première chose facile à voir est
\begin{equation}        \label{EQooAMHPooUQEkJo}
    t_1(e_i)_j=\sum_k(A^t)_{jk}(e_i)_k=A^t_{ji}=A_{ij}.
\end{equation}
Pour calculer \( t_2(e_i)_j\), c'est un peu plus laborieux :
\begin{subequations}
    \begin{align}
        t_2(e_i)&=\sum_{\alpha}Q_{\alpha i}^{-1} t_2(f_\alpha)=\sum_{\beta\gamma\alpha}Q_{\alpha i}^{-1}B^t_{\gamma\beta}\underbrace{(t_{\alpha})_{\beta}}_{\delta_{\alpha\beta}}f_{\gamma}=\sum_{\beta\gamma}Q_{\beta i}^{-1}B^t_{\gamma\beta}f_{\gamma}\\
        &=(B^tQ^{-1})_{\gamma i}Q_{j\gamma}e_j\\
        &=\sum_j(QB^tQ^{-1})_{ji}e_j.
    \end{align}
\end{subequations}
Donc \( t_2(e_i)_j=(QB^tQ^{-1})_{ji}\). En tenant compte du fait que \( B=Q^{-1}AQ\) nous avons
\begin{equation}
    t_2(e_i)_j=(QQ^tA^t(Q^{-1})^tQ^{-1})_{ji}.
\end{equation}
Cela est égal à l'expression \eqref{EQooAMHPooUQEkJo} lorsque \( Q^t=Q^{-1}\). Nous voyons que confondre transposée d'une application linéaire avec transposée de la matrice associée n'est valable que si nous sommes certain de ne considérer que des changements de base par des matrices orthogonales.

Cela est la situation typique dans laquelle nous nous trouvons lorsque nous considérons des applications linéaires sur \( \eR^n\) muni de la base canonique et que nous n'avons aucune intention de changer de base, et encore moins de chercher une base non orthonormale. Cette situation est clairement la situation la plus courante.

\begin{example}[\cite{ooLIOMooBuCPUS}]
    Soit la base canonique \( \{ e_1,e_2 \}\) de \( \eR^2\). Nous considérons l'application linéaire \( t\colon \eR^2\to \eR^2\) définie par
    \begin{subequations}
        \begin{align}
            t(e_1)&=e_1\\
            t(e_2)&=0.
        \end{align}
    \end{subequations}
    La matrice de \( t\) dans cette base est
    \begin{equation}
        A=\begin{pmatrix}
            1    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Elle est symétrique : elle vérifie \( A^t=A\). Si nous comptions sur la transposée de matrice pour définir la transposée de \( t\), nous aurions \( t^t=t\).

    Soit maintenant la base \( f_1=e_1\), \( f_2=e_1+e_2\). Nous avons \( t(f_1)=f_1\) et
    \begin{equation}
        t(f_2)=t(e_1)+t(e_2)=e_1=f_1.
    \end{equation}
    Donc la matrice de \( t\) dans cette base est
    \begin{equation}
        B=\begin{pmatrix}
            1    &   1    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Et là, nous avons \( B^t\neq B\). Donc en comptant sur cette base pour définir la transposée de \( t\) nous aurions \( t^t\neq t\).
\end{example}

\begin{normaltext}      \label{NooMZVRooExWVKJ}
    Autrement dit, la façon «usuelle» de voir la transposée d'une application linéaire ne fonctionne dans les livres pour enfant uniquement parce qu'on n'y considère toujours \( \eR^n\) muni de la base canonique ou de bases orthonormées.

    Notons que nous avons tout de même les notions d'opérateur adjoint et autoadjoint pour parler d'application orthogonale sans passer par la transposée, voir~\ref{DEFooYKCSooURQDoS}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : la bonne approche}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooZLPAooKTITdd}
    Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
    \begin{equation}
        f^t(\omega)(x)=\omega\big( f(x) \big).
    \end{equation}
    pour tout \( \omega\in F^*\) et \( x\in E\).
\end{definition}

\begin{lemma}
    Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).
\end{lemma}

\begin{proof}
    Nous allons montrer que les formes \( f^t(g^*_i)\) et \( \sum_k(A^t)_{ik}g^*_k\) sont égales en les appliquant à un vecteur.

    Par définition de la matrice d'une application linéaire dans une base,
    \begin{equation}
        f^t(g_i^*)=\sum_j(f^t)_{ij}e^*_j,
    \end{equation}
    et
    \begin{equation}
        f(e_k)=\sum_lA_{kl}g_l.
    \end{equation}
    Du coup, si \( x=\sum_kx_ke_k\), nous avons
    \begin{equation}    \label{EqCzwftH}
        f^t(g_i^*)x=\sum_{kl}x_kg_i^*A_{kl}g_l=\sum_{kl}x_kA_{kl}\delta_{il}=\sum_k x_kA_{ki}=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    D'autre part,
    \begin{equation}    \label{EqWlQlrR}
        \sum_k(A^t)_{ik}g_k^*x=\sum_{kl}(A^t)_{ik}g^*_kx_le_l=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    Le fait que \eqref{EqCzwftH} et \eqref{EqWlQlrR} donnent le même résultat prouve le lemme.
\end{proof}
En corollaire, les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carrée de déterminant non nul. Nous prouvons cependant ce résultat de façon plus intrinsèque.

\begin{lemma}[\href{http://gilles.dubois10.free.fr/algebre_lineaire/dualite.html}{Gilles Dubois}]   \label{LemSEpTcW}
    Si \( f\colon E\to F\) est une application linéaire, alors
    \begin{equation}
        \rang(f)=\rang(f^t).
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons \( \dim\ker(f)=p\) et donc \( \rang(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
    \begin{equation}
        g_i=f(e_{p+i})
    \end{equation}
    pour \( i=1,\ldots, n-p\). C'est-à-dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
    \begin{equation}
        \sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
    \end{equation}
    alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), ce qui signifierait que \( \sum_ka_ke_{p+k}\) se trouve dans le noyau de \( f\), ce qui est impossible par construction de la base \( \{ e_i \}_{i=1,\ldots, n}\). Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous les complétons en une base
    \begin{equation}
        \{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}
    \end{equation}
    de \( F\).

    Nous prouvons maintenant que \( \rang(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) forment une partie libre (et donc l'espace image de \( f^t\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
    \begin{equation}
        f^t(g^*_i)e_k=g_i^*(fe_k),
    \end{equation}
    Si \( k=1,\ldots, p\), alors \( fe_k=0\) et donc \( g_i^*(fe_k)=0\); si \( k=p+l\) alors
    \begin{equation}
        f^t(g_i^*)e_k=g_i^*(fe_{k+l})=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
    \end{equation}
    Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
    \begin{equation}
        \rang(f^t)\geq n-p=\rang(f).
    \end{equation}
    En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
    \begin{equation}
        \rang\big( (f^t)^t \big)\geq \rang(f^t)
    \end{equation}
    et donc, vu que \( (f^t)^t=f\), nous obtenons \( \rang(f)=\rang(f^t)\).

\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]        \label{PropWOPIooBHFDdP}
    Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
    \begin{equation}
        \Image(f^t)=\ker(f)^{\perp}.
    \end{equation}
\end{proposition}

\begin{proof}
    Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est-à-dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est-à-dire que \( \omega\in (\ker f)^{\perp}\).

    Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
    \begin{subequations}
        \begin{align}
            \dim\big( \Image(f^t) \big)&=\rang(f^t)\\
            &=\rang(f)  &\text{lemme~\ref{LemSEpTcW}}\\
            &=\dim(E)-\dim\ker(f)   &\text{théorème~\ref{ThoGkkffA}}\\
            &=\dim\big( (\ker f)^{\perp} \big)  &\text{proposition~\ref{PropXrTDIi}}.
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}[\cite{ooEPEFooQiPESf}]
    Soit \( \eK\) un corps, \( E\) et \( F\) deux \( \eK\)-espaces vectoriels de dimension finie et une application linéaire \( f\colon E\to F\). L'application \( f\) est injective si et seulement si sa transposée\footnote{Définition~\ref{DefooZLPAooKTITdd}.} \( f^t\) est surjective.
\end{lemma}

\begin{proof}
    Supposons que \( f\) soit injective. Alors par le lemme~\ref{LEMooDAACooElDsYb}, il existe \( g\colon F\to E\) tel que \( g\circ f=\id|_E\). Nous avons alors aussi \( (g\circ f)^t=\id|_{E^*}\), mais \( (g\circ f)^t=f^t\circ g^t\), donc \( f^t\) est surjective.

    Inversement, nous supposons que \( f^t\colon F^*\to E^*\) est surjective. Alors en nous souvenant que \( E\) et \( F\) sont de dimension finie et en faisant jouer les identifications \( (f^t)^t=f\) et \( (E^*)^*=E\) nous savons qu'il existe \( s\colon E^*\to F^*\) tel que \( f^t\circ s=\id|_{E^*}\). En passant à la transposée,
    \begin{equation}
        s^t\circ f=\id|_{E},
    \end{equation}
    qui implique que \( f\) est injective.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E=\eR_n[X]\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient les \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in E^*\),
\begin{equation}
    f_i(P)=P(a_i).
\end{equation}
\begin{lemma}
    Ces formes forment une base de \( E^*\).
\end{lemma}

\begin{proof}
    Nous prouvons que l'orthogonal est réduit au nul :
    \begin{equation}
        \Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}
    \end{equation}
    pour que la proposition~\ref{PropXrTDIi} conclue. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme!Lagrange} sont les polynômes de la base (pré)duale de la base \( \{ f_i \}\).

\begin{proposition}
    Les polynômes de Lagrange sont donnés par
    \begin{equation}
        P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de vérifier que \( f_j(P_i)=\delta_{ij}\). Nous avons
    \begin{equation}
        f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
    \end{equation}
    Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dual de \texorpdfstring{$ \eM(n,\eK)$}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
    Soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
    \begin{equation}
        \begin{aligned}
            f_A\colon \eM(n,\eK)&\to \eK \\
            M&\mapsto \tr(AM).
        \end{aligned}
    \end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}


\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eK)&\to \eM(n,\eK)^* \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM(n,\eK)\) et \( \left(\eM(n,\eK)\right)^*\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{ik}\delta_{jl}\) :
    \begin{equation}
            0=f_A(E_{ij})
            =\sum_{k}(AE_{ij})_{kk}
            =\sum_{kl}A_{kl}(E_{ij})_{lk}
            =\sum_{kl}A_{kl}\delta_{il}\delta_{jk}
            =A_{ij}.
    \end{equation}
    Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soit \( \eK\) un corps et \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( M,N\in \eM(n,\eK)\) on ait
    \begin{equation}
        \phi(MN)=\phi(NM).
    \end{equation}
    Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
    La proposition~\ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(MN)=f_A(NM)\), c'est-à-dire
    \begin{equation}
        \Tr(AMN)=\Tr(ANM)
    \end{equation}
    pour toutes matrices \( M, N\in \eM(n,\eK)\). L'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.} appliqué au membre de droite nous donne \( \Tr(AMN)=\Tr(MAN)\), ce qui signifie que
    \begin{equation}
        \Tr\big( (AM-MA)N \big)=0
    \end{equation}
    ou encore que \( f_{AM-MA}=0\), et ce, pour toute matrice \( M\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
    \begin{equation}
        AM=MA
    \end{equation}
    pour tout \( M\in\eM(n,\eK)\). En particulier, en prenant pour \( M \) les fameuses matrices \( E_{ij}\) et en calculant un peu,
    \begin{equation}
        A_{li}\delta_{jm}=\delta_{il}A_{jm}
    \end{equation}
    pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
    \begin{equation}
        \phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
    \end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]       \label{CorICUOooPsZQrg}
    Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
    Soit \( \mH\) un hyperplan de \( \eM\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition~\ref{PropHOjJpCa} nous donne \( A\in \eM\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme~\ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Pour tout \( M\in \eM\) nous avons
    \begin{equation}
        \phi(M)=\Tr(AM)=\Tr(PJ_rQM)=\Tr(J_rQMP),
    \end{equation}
la dernière égalité découlant de l'invariance cyclique de la trace\footnote{Lemme~\ref{LEMooUXDRooWZbMVN}.}. Ce que nous cherchons est \( M\in \GL(n,\eK)\) telle que \( \phi(M)=0\). Nous commençons par trouver \( N\in\GL(n,\eK)\) telle que \( \Tr(J_rN)=0\). Celle-là est facile : c'est
    \begin{equation}
        N=\begin{pmatrix}
            0    &   1    \\
            \mtu_{n-1}    &   0
        \end{pmatrix}.
    \end{equation}
    Les éléments diagonaux de \( J_rN\) sont tous nuls. Par conséquent en posant \( M=Q^{-1}NP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Représentation de groupe}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Représentation]      \label{DEFooXVMSooXDIfZV}
    Soit un groupe \( G\) et un espace vectoriel \( V\). Nous disons qu'une application \( \rho\colon G\to \GL(V)\) est une \defe{représentation}{représentation} de \( G\) sur \( V\) si pour tout \( g,h\in G\) nous avons
    \begin{equation}
        \rho(g)\circ\rho(g)=\rho(gh).
    \end{equation}
    Très souvent, nous disons que la représentation est le couple \( (V,\rho)\).
\end{definition}

\begin{definition}
    Une représentation\footnote{Définition \ref{DEFooXVMSooXDIfZV}.} est \defe{fidèle}{représentation!fidèle} si elle est injective en tant que application \( G\to \GL(V)\). Ce ne sont pas chacun des \( \rho(g)\) qui doivent être injectifs. La dimension de \( V\) est le \defe{degré}{degré!d'une représentation} de la représentation \( (V,\rho)\).
\end{definition}

\begin{proposition}     \label{PROPooHNQOooSzeEFG}
    Soit un corps \( \eK\). Si \( G\) est un groupe dans \( \eM(n,\eK)\) (c'est à dire un groupe de matrices à coefficients dans \( \eK\)), alors l'application
    \begin{equation}
        \begin{aligned}
                \rho\colon G&\to \GL(\eK^n) \\
            A&\mapsto f_A 
        \end{aligned}
    \end{equation}
    où \( f_A\) est l'application linéaire associée à \( A\) est une représentation de \( G\).
\end{proposition}


