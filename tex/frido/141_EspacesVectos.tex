% This is part of Mes notes de mathématique
% Copyright (c) 2011-2021
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Hyperplans et formes linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}      \label{DEFooEWDTooQbUQws}
    Si \( E\) est un espace vectoriel de dimension \( n\), un \defe{hyperplan}{hyperplan} de \( E\) est un sous-espace vectoriel de dimension \( n-1\).
\end{definition}

\begin{proposition}[\cite{ooDYWYooBJkHuh}]      \label{PROPooVYJUooAWDQrZ}
    À propos d'hyperplans et de formes linéaires sur un espace vectoriel \( E\) sur le corps \( \eK\).
    \begin{enumerate}
        \item
            Si \( \varphi\) est une forme linéaire non nulle, alors \( \ker(\varphi)\) est un hyperplan.
        \item
            Si \( H\) est un hyperplan de \( E\), il existe une forme linéaire dont \( H\) est le noyau :
            \begin{equation}
                H=\ker(\varphi).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    En deux parties.
    \begin{enumerate}
        \item
            Soit un supplémentaire \( A\) de \( H\). Nous considérons la restriction \( \varphi_A\colon A\to \eK\). Vu que les éléments non nuls de \( A\) sont hors de \( H\), nous avons \( \varphi(x)\neq 0\) dès que \( x\) est non nul dans \( A\). Cela implique que \( \varphi_A\) est surjective.

            D'autre part, \( \varphi_A\) est également injective : si \( \varphi_A(x)=\varphi_A(y)\), alors \( \varphi_A(x-y)=0\), ce qui signifie que \( x-y=0\) ou encore que \( x=y\).

            Donc \( \varphi_A\) est un isomorphisme de \( \eK\)-espaces vectoriels; nous en déduisons par le corolaire \ref{CORooXIPKooWThOsr} que \( A\) est de dimension \( 1\) sur \( \eK\), parce que \( \eK\) est de dimension \( 1\).

        \item
            Nous utilisons le théorème de la base incomplète \ref{ThonmnWKs}\ref{ITEMooJIJSooGuJMdt} pour considérer une base \( \{ e_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \Span\{ e_1,\ldots, e_{n-1} \}=H\). Nous pouvons alors considérer la forme linéaire définie par
            \begin{equation}
                \varphi(e_i)=\begin{cases}
                    0    &   \text{si }  i=1,\ldots, n-1\\
                    1    &    \text{si } i=n.
                \end{cases}
            \end{equation}
            Cette forme vérifie \( \ker(\varphi)=H\).
    \end{enumerate}
\end{proof}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]
    Soit un espace vectoriel \( E\) de dimension finie \( n\geq 2\). Soit un sous-espace vectoriel \( V\) de \( E\) de dimension \( s\). Alors \( V\) est une intersection de \( n-s\) hyperplans de \( E\).
\end{proposition}

\begin{proof}
    Nous considérons une base de \( V\) que nous complétons\footnote{Théorème de la base incomplète, \ref{ThonmnWKs}\ref{ITEMooJIJSooGuJMdt}.} en une base de \( E\) : si \( x=\sum_{i=1}^nx_ie_i\), nous avons \( x\in V\) si et seulement si \( x_{s+1}=\ldots=x_n=0\). Nous considérons les formes linéaires
    \begin{equation}
        \begin{aligned}
            \varphi_i\colon E&\to \eR \\
            x&\mapsto x_i, 
        \end{aligned}
    \end{equation}
    et nous considérons les parties \( H_i=\ker(\varphi_i)\) qui sont de hyperplans par la proposition \ref{PROPooVYJUooAWDQrZ}. Les \( H_i\) avec \( s+1\leq i\leq n\) sont une famille de \( n-s\) hyperplans qui vérifient
    \begin{equation}
        V=\bigcap_{i=s+1}^n\ker(\varphi_i)
    \end{equation}
    parce que \( x\in \ker(\varphi_i)\) si et seulement si \( x_i=0\).

    Donc \( V\) peut être écrit comme intersection de \( n-s\) hyperplans de \( E\).
\end{proof}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]      \label{PROPooRCLNooJpIMMl}
    Soit un \( \eK\)-espace vectoriel \( E\) de dimension finie \( n\geq 2\). Si \( H_i\) sont des hyperplans de \( E\), alors
    \begin{equation}
        \dim\Big( \bigcup_{i=1}^mH_i \Big)\geq n-m.
    \end{equation}
\end{proposition}

\begin{proof}
    N'oubliez pas de prouver que \( \bigcap_{i=1}^mH_i\) est un espace vectoriel. À part ça, nous faisons une petite récurrence.
    \begin{subproof}
        \item[Pour \( m=2\)]
            Nous savons déjà par la proposition \ref{PROPooQCIXooHIyPPq} que
            \begin{equation}
                \dim(H_1\cap H_2)=\dim(H_1)+\dim(H_2)-\dim(H_1\cap H_2).
            \end{equation}
            De plus \( \dim(H_1+H_2)\leq n\). En remplaçant, par les valeurs,
            \begin{subequations}
                \begin{align}
                    \dim(H_1\cap H_2)&=\dim(H_1)+\dim(H_2)-\dim(H_1\cap H_2)\\
                    &=n-1+n-1-\dim(H_1+H_2)\\
                    &\geq 2n-2-n\\
                    &=n-2.
                \end{align}
            \end{subequations}
            Donc \( \dim(H_1\cap H_2)\geq n-2\).

        \item[La récurrence]
            Nous calculons \( \dim(H_1\cap\ldots\cap H_m\cap H_{m+1})\) en commençant encore par la proposition \ref{PROPooQCIXooHIyPPq} :
            \begin{subequations}
                \begin{align}
                    \dim(H_1\cap \ldots\cap H_m\cap H_{m+1})&=\underbrace{\dim(H_1\cap\ldots\cap H_m)}_{\leq n-m}+\dim(H_{m+1})\\
                        &\qquad -\underbrace{\dim\big( (H_1\cap\ldots H_m)+H_{m+1} \big)}_{\leq n}\\
                    &\geq n-m+(n-1)-n\\
                    &=n-m-1.
                \end{align}
            \end{subequations}
            C'est bon pour la récurrence.
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trouver la matrice d'une symétrie donnée}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMtrSym}

Les notions de déterminants, produit scalaire et vectoriels\footnote{Définitions~\ref{LEMooQTRVooAKzucd},~\ref{DefVJIeTFj} et~\ref{DEFooTNTNooRjhuJZ}.} donnent une bonne intuition géométrique des matrices. Nous pouvons alors chercher les matrices de quelques symétriques dans \( \eR^2\) ou \( \eR^3\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à un plan}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Comment trouver par exemple la matrice $A$ qui donne la symétrie autour du plan $z=0$ ? La définition d'une telle symétrie est que les vecteurs du plan $z=0$ ne bougent pas, tandis que les vecteurs perpendiculaires changent de signe. Ces informations vont permettre de trouver comment $A$ agit sur une base de $\eR^3$. En effet :
\begin{enumerate}

	\item
		Le vecteur $\begin{pmatrix}
			1	\\
			0	\\
			0
		\end{pmatrix}$ est dans le plan $z=0$, donc il ne bouge pas,

	\item
		le vecteur $\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix}$ est également dans le plan, donc il ne bouge pas non plus,

	\item
		et le vecteur $\begin{pmatrix}
			0	\\
			0	\\
			1
		\end{pmatrix}$ est perpendiculaire au plan $z=0$, donc il va changer de signe.

\end{enumerate}
Cela nous donne directement les valeurs de $A$ sur la base canonique et nous permet d'écrire
\begin{equation}
	A=\begin{pmatrix}
		1	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	0	&	-1
	\end{pmatrix}.
\end{equation}
Pour écrire cela, nous avons juste mit en colonne les images des vecteurs de base. Les deux premiers n'ont pas changé et le troisième a changé.

Et si maintenant on donne un plan moins facile que $z=0$ ? Le principe reste le même : il faudra trouver deux vecteurs qui sont dans le plan (et dire qu'ils ne bougent pas), et puis un vecteur qui est perpendiculaire au plan\footnote{Pour le trouver, penser au produit vectoriel.}, et dire qu'il change de signe.

Voyons ce qu'il en est pour le plan $x=-z$. Il faut trouver deux vecteurs linéairement indépendants dans ce plan. Prenons par exemple
\begin{equation}		\label{EqffudE}
	\begin{aligned}[]
		f_1&=\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix},&f_2&=\begin{pmatrix}
			1	\\
			0	\\
			-1
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons
\begin{equation}
	\begin{aligned}[]
		Af_1&=f_1\\
		Af_2&=f_2.
	\end{aligned}
\end{equation}
Afin de trouver un vecteur perpendiculaire au plan, calculons le produit vectoriel :
\begin{equation}
	f_3=f_1\times f_2=\begin{vmatrix}
		e_1	&	e_2	&	e_3	\\
		0	&	1	&	0	\\
		1	&	0	&	-1
	\end{vmatrix}=-e_1-e_3=\begin{pmatrix}
		-1	\\
		0	\\
		-1
	\end{pmatrix}.
\end{equation}
Nous avons
\begin{equation}
	Af_3=-f_3.
\end{equation}
Afin de trouver la matrice $A$, il faut trouver $Ae_1$, $Ae_2$ et $Ae_3$. Pour ce faire, il faut d'abord écrire $\{ e_1,e_2,e_3 \}$ en fonction de $\{ f_1,f_2,f_3 \}$. La première des équations \eqref{EqffudE} dit que
\begin{equation}
	f_1=e_2.
\end{equation}
Ensuite, nous avons
\begin{equation}
	\begin{aligned}[]
		f_2&=e_1-e_3\\
		f_3&=-e_1-e_3.
	\end{aligned}
\end{equation}
La somme de ces deux équations donne $-2e_3=f_2+f_3$, c'est-à-dire
\begin{equation}
	e_3=-\frac{ f_2+f_3 }{ 2 }
\end{equation}
Et enfin, nous avons
\begin{equation}
	e_1=\frac{ f_2-f_3 }{ 2 }.
\end{equation}

Maintenant nous pouvons calculer les images de $e_1$, $e_2$ et $e_3$ en faisant
\begin{equation}
	\begin{aligned}[]
		Ae_1&=\frac{ Af_2-Af_3 }{ 2 }=\frac{1 }{2}\begin{pmatrix}
			0	\\
			0	\\
			-2
		\end{pmatrix}=\begin{pmatrix}
			0	\\
			0	\\
			-1
		\end{pmatrix},\\
		Ae_2&=Af_1=f_1=\begin{pmatrix}
			0	\\
			1	\\
			0
		\end{pmatrix},\\
		Ae_3&=-\frac{ f_2-f_3 }{ 2 }=-\frac{ 1 }{2}\begin{pmatrix}
			2	\\
			0	\\
			0
		\end{pmatrix}=\begin{pmatrix}
			-1	\\
			0	\\
			0
		\end{pmatrix}.
	\end{aligned}
\end{equation}
La matrice $A$ s'écrit maintenant en mettant les trois images trouvées en colonnes :
\begin{equation}
	A=\begin{pmatrix}
		0	&	0	&	-1	\\
		0	&	1	&	0	\\
		-1	&	0	&	0
	\end{pmatrix}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à une droite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le principe est exactement le même : il faut trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Ensuite il faudra exprimer $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$ et $f_3$.

Le seul problème est de trouver les trois vecteurs $f_i$. Le premier est tout trouvé : c'est n'importe quel vecteur sur la droite. Pour les deux autres, il faut un peu ruser parce qu'il faut impérativement qu'ils soient perpendiculaire à la droite. Pour trouver $f_2$, on peut écrire
\begin{equation}
	f_2=\begin{pmatrix}
		1	\\
		0	\\
		x
	\end{pmatrix},
\end{equation}
et puis fixer le $x$ pour que le produit scalaire de $f_2$ avec $f_1$ soit nul. S'il n'y a pas moyen (genre si $f_1$ a sa troisième composante nulle), essayer avec $\begin{pmatrix}
	x	\\
	1	\\
	0
\end{pmatrix}$. Une fois que $f_2$ est trouvé (il y a des milliards de choix possibles), trouver $f_3$ est super facile : prendre le produit vectoriel entre $f_1$ et $f_2$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{En résumé}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
La marche à suivre est

\begin{enumerate}

	\item
		Trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Typiquement : des vecteurs qui sont sur l'axe ou le plan de symétrie, et puis des perpendiculaires. Pour la perpendiculaire, penser au produit scalaire et au produit vectoriel.

	\item
		Exprimer la base canonique $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$, $f_3$.

	\item
		Trouver $Ae_1$, $Ae_2$ et $Ae_3$ en utilisant leur expression en termes des $f_i$, et le fait que l'on connaisse l'action de $A$ sur les $f_i$.

	\item
		La matrice s'obtient en mettant les images des $e_i$ en colonnes.
\end{enumerate}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Théorème de Burnside}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}       \label{LemwXXzIt}
    Soit \( P\), un polynôme sur \( \eK\). Une racine de \( P\) est une racine simple si et seulement si elle n'est pas racine de \( P'\).
\end{lemma}

\begin{theorem}     \label{ThoBurnsideoPuCtS}
    Toute représentation\footnote{Définition \ref{DEFooXVMSooXDIfZV}.} d'un groupe abélien d'exposant fini sur \( \eC^n\) a une image finie.
\end{theorem}

\begin{proof}
    Étant donné que \( G\) est d'exposant fini, il existe \( \alpha\in \eN^*\) tel que \( g^{\alpha}=e\) pour tout \( g\in G\). Le polynôme \( P(X)=X^{\alpha}-1\) est scindé à racines simples. En effet tout polynôme sur \( \eC\) est scindé. Le fait qu'il soit à racines simples provient du lemme~\ref{LemwXXzIt} parce que si \( a^{\alpha}=1\), alors il n'est pas possible d'avoir \( \alpha a^{\alpha-1}=0\).

    Par ailleurs \( P(g)=0\). Le fait que nous ayons un polynôme annulateur de \( g\) scindé à racines simples implique que \( g\) est diagonalisable (théorème~\ref{ThoDigLEQEXR}). Le fait que \( G\) soit abélien montre qu'il existe une base de \( \eC^n\) dans laquelle tous les éléments de \( G\) sont diagonaux. Nous devons par conséquent montrer qu'il existe un nombre fini de matrices de la forme
    \begin{equation}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n
        \end{pmatrix}.
    \end{equation}
    Nous savons que \( \lambda_i^{\alpha}=1\) parce que \( g^{\alpha}=\mtu\), par conséquent chacun des \( \lambda_i\) est une racine de l'unité dont il n'existe qu'un nombre fini.
\end{proof}

\begin{theorem}[Burnside\cite{fJhCTE,ooFBZQooXyHIWK}]\label{ThooJLTit}
    Un sous-groupe de \( \GL(n,\eC)\) est fini si et seulement s'il est d'exposant\footnote{Définition~\ref{DefvtSAyb}.} fini.
\end{theorem}
\index{exposant}
\index{racine!de l'unité}
\index{endomorphisme!diagonalisable}

\begin{proof}
    Soit \( G\) un sous-groupe de \( \GL(n,\eC)\). Si \( G\) est fini, l'ordre de ses éléments divise \( | G |\) (corolaire \ref{CorpZItFX} au théorème de Lagrange) et l'exposant est le PPCM qui est donc fini également. Le théorème est déjà démontré dans un sens.

    Dans l'autre sens, nous notons \( e<\infty\) l'exposant de \( G\), et nous allons prouver que l'ensemble \( G\) est fini. Nous commençons par remarquer que tous les éléments de \( G\) sont des racines du polynôme \( X^e-1\), et ensuite nous nous lançons dans le travail.

    \begin{subproof}
        \item[Générateurs]

            Le groupe \( G\) est une partie de \( \eM(n,\eC)\) dont nous considérons l'algèbre engendrée\footnote{Définition \ref{DefkAXaWY}.} \( \mG\). Soit \( C_1,\ldots, C_r\) une famille génératrice de \( \mG\) constituée d'éléments de \( G\) et la fonction
            \begin{equation}
                \begin{aligned}
                    \tau\colon G&\to \eC^r \\
                    A&\mapsto \big( \tr(AC_1),\ldots, \tr(AC_r) \big).
                \end{aligned}
            \end{equation}

        \item[\( \tau\) est injective] Soient \( A,B\in G\) tels que \( \tau(A)=\tau(B)\). Si \( C_i\) est un générateur de \( G\), nous avons \( \tr(AC_i)=\tr(BC_i)\) et par la linéarité de la trace, nous avons
            \begin{equation}    \label{EqnCYmKW}
                \tr(AM)=\tr(BM)
            \end{equation}
            pour tout \( M\in G\). Notons par ailleurs
            \begin{equation}
                N=AB^{-1}-\mtu,
            \end{equation}
            qui est diagonalisable parce que \( AB^{-1}\in G\) et donc est annulé par le polynôme \( X^e-1\) qui est scindé à racines simples. Du coup \( AB^{-1}\) est diagonalisable; posons \( PAB^{-1}P^{-1}=D\), alors \( P\big( AB^{-1}-\mtu \big)P^{-1}=D-\mtu\) qui est encore diagonale. Donc \( N\) est diagonalisable.

            Par ailleurs nous avons
            \begin{subequations}
                \begin{align}
                    \tr\big( (AB^{-1})^p \big)&=\tr\big( AB^{-1}(AB^{-1})^{p-1} \big)\\
                    &=\tr\big( BB^{-1}(AB^{-1})^{p-1} \big) &\text{\eqref{EqnCYmKW}}\\
                    &=\tr\big( (AB^{-1})^{p-1} \big).
                \end{align}
            \end{subequations}
            En continuant nous obtenons
            \begin{equation}
                \tr\big(  (AB^{-1})^p \big)=\tr(\mtu)=n.
            \end{equation}

            D'autre part,
            \begin{equation}
                N^k=(AB^{-1}-\mtu)^k=\sum_{p=0}^k{p\choose k}(-1)^{k-p}(AB^{-1})^p
            \end{equation}
            En prenant la trace, et en tenant compte du fait que \( \tr\big( (AB^{-1})^p \big)=n\),
            \begin{equation}
                \tr(N^k)=\sum_{p=0}^k{p\choose k}(-1)^{k-p}n=n(1-1)^k=0.
            \end{equation}
            Donc la trace de \( N^k\) est nulle et le lemme~\ref{LemzgNOjY} nous enseigne que \( N\) est alors nilpotente. Étant donné qu'elle est aussi diagonalisable, elle est nulle. Nous en concluons que \( AB^{-1}=\mtu\) et donc que \( A=B\). La fonction \( \tau\) est donc injective.

        \item[Nombre fini de valeurs]

            Les éléments de \( G\) sont annulés par \( X^e-1\) qui est un polynôme scindé à racines simples. Dons le polynôme minimal d'un élément de \( G\) est (a fortiori) scindé à racines simples et le théorème~\ref{ThoDigLEQEXR} nous assure alors que ces éléments sont diagonalisables. Du coup les valeurs propres des matrices de \( G\) sont des racines \( e\)ièmes de l'unité. Par conséquent les traces des éléments de \( G\) ne peuvent prendre qu'un nombre fini de valeurs : toutes les sommes de \( n\) racines \( e\)ièmes de l'unité. Mais vu que les \( C_i\) sont dans \( G\), nous avons
            \begin{equation}
                \Image(\tau)=\{ \tr(A)\tq A\in G \}^r,
            \end{equation}
            qui est un ensemble fini. Par conséquent \( G\) est fini parce que \( \tau\) est injective.
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Lie-Kolchin}
%---------------------------------------------------------------------------------------------------------------------------

Contrairement à ce que l'on peut parfois croire, il n'est pas vrai que toute matrice à coefficient réel est diagonalisable, même pas sur \( \eC\). La raison est qu'une telle matrice peut très bien avoir des valeurs propres multiples.

\begin{example} \label{ExBRXUooIlUnSx}
    Le théorème~\ref{ThoDigLEQEXR} nous donne une façon simple de trouver des matrices non diagonalisables sur \( \eC\) : il suffit que le polynôme minimal ne soit pas scindé à racines simples. Par exemple
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\
            0    &   1
        \end{pmatrix},
    \end{equation}
    dont le polynôme caractéristique est \( \chi_A=(1-X)^2\). Ce polynôme n'a manifestement pas des racines simples. Nous pouvons faire le calcul explicite pour montrer que \( A\) n'est pas diagonalisable. D'abord l'unique valeur propre de \( A\) est \( 1\) et nous pouvons sans peine résoudre
    \begin{equation}
        \begin{pmatrix}
            1    &   1    \\
            0    &   1
        \end{pmatrix}\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            x    \\
            y
        \end{pmatrix}
    \end{equation}
    qui revient au système
    \begin{subequations}
        \begin{numcases}{}
            x+y=x\\
            y=y.
        \end{numcases}
    \end{subequations}
    La première équation donne directement \( y=0\). Le seul espace propre est de dimension \( 1\) et est engendré par \( \begin{pmatrix}
        1    \\
        0
    \end{pmatrix}\).
\end{example}

La remarque~\ref{RemBOGooCLMwyb} donne un exemple un peu plus avancé, qui montre la multiplicité algébrique et géométrique d'une racine d'un polynôme caractéristique.

\begin{lemma}[Trigonalisation simultanée]   \label{LemSLGPooIghEPI}
    Une famille de matrices de \( \GL(n,\eC)\) commutant deux à deux est simultanément trigonalisable.
\end{lemma}
\index{trigonalisation!simultanée}

\begin{proof}
    Commençons par enfoncer une porte ouverte par la proposition~\ref{PropKNVFooQflQsJ} : toutes les matrices de \( \GL(n,\eC)\) sont trigonalisables parce que tous les polynômes sont scindés.

    Nous effectuons la démonstration par récurrence sur la dimension. Si \( n=1\) alors toutes les matrices sont triangulaires et nous ne nous posons pas de questions. Nous supposons donc \( n>1\).

    Soit la famille \( (A_i)_{i\in I}\) dans \( \GL(n,\eC)\) et \( A_0\) un de ses éléments. Nous nommons \( \lambda_1,\ldots, \lambda_r\) les valeurs propres distinctes de \( A_0\). Le théorème de décomposition primaire~\ref{ThoSpectraluRMLok} nous donne la somme directe d'espaces caractéristiques\footnote{Définition~\ref{DefFBNIooCGbIix}.}
    \begin{equation}
        E=F_{\lambda_1}(A_0)\oplus\ldots\oplus F_{\lambda_r}(A_0).
    \end{equation}
    Nous pouvons supposer que cette somme n'est pas réduite à un seul terme. En effet si tel était le cas, \( A_0\) serait un multiple de l'identité parce que \( A_0\) n'aurait qu'une seule valeur propre et les sommes dans la décomposition de Dunford~\ref{ThoRURcpW}\ref{ItemThoRURcpWiii} se réduisent à un seul terme (et \( p_i=\id\)). En particulier les dimensions des espaces \( F_{\lambda}(A_0)\) sont strictement plus petites que \( n\).

    Vu que tous les \( A_i\) commutent avec \( A_0\), les espaces \( F_{\lambda}(A_0)\) sont stables par les \( A_i\) et nous pouvons trigonaliser les \( A_i\) simultanément sur chacun des \( F_{\lambda}(A_0)\) en utilisant l'hypothèse de récurrence.
\end{proof}

\begin{theorem}[Lie-Kolchin\cite{PAXrsMn}]  \label{ThoUWQBooCvutTO}
    Tout sous-groupe connexe et résoluble de \( \GL(n,\eC)\) est conjugué à un groupe de matrices triangulaires.
\end{theorem}
\index{trigonalisation!simultanée}
\index{théorème!Lie-Kolchin}

\begin{proof}
    Soit \( G\) un sous-groupe connexe et résoluble de \( \GL(n,\eC)\).

    \begin{subproof}
        \item[Si sous-espace non trivial stable par \( G\)]

    Nous commençons par voir ce qu'il se passe s'il existe un sous-espace vectoriel non trivial \( V\) de \( \eC^n\) stabilisé par \( G\). Pour cela nous considérons une base de \( \eC^n\) dont les premiers éléments forment une base de \( V\) (base incomplète, théorème~\ref{ThonmnWKs}). Les éléments de \( G\) s'écrivent, dans cette base,
    \begin{equation}    \label{EqGOKTooEaGACG}
        \begin{pmatrix}
            g_1    &   *    \\
            0    &   g_2
        \end{pmatrix}.
    \end{equation}
    Les matrices \( g_1\) et \( g_2\) sont carrés. Nous considérons alors l'application \( \psi\) définie par
    \begin{equation}
        \begin{aligned}
            \psi\colon G&\to \GL(V) \\
            g&\mapsto g_1.
        \end{aligned}
    \end{equation}
    Cela est un morphisme de groupes parce que
    \begin{equation}
        \begin{pmatrix}
            g_1    &   *    \\
            0    &   g_2
        \end{pmatrix}\begin{pmatrix}
            h_1    &   *    \\
            0    &   h_2
        \end{pmatrix}=
        \begin{pmatrix}
            g_1h_1    &   *    \\
            0    &   g_2h_2
        \end{pmatrix},
    \end{equation}
    de telle sorte que \( \psi(gh)=\psi(g)\psi(h)\).

    Le groupe \( \psi(G)\) est connexe et résoluble. En effet \( \psi(G)\) est connexe en tant qu'image d'un connexe par une application continue (proposition~\ref{PropGWMVzqb}). Et il est résoluble en tant qu'image d'un groupe résoluble par un homomorphisme par la proposition~\ref{PropBNEZooJMDFIB}. Vu que \( \psi(G)\) est un sous-groupe résoluble et connexe de \( \GL(V)\) et que la dimension de \( V\) est strictement plis petite que celle de \( \eC^n\), une récurrence sur la dimension indique que \( \psi(G)\) est conjugué à un groupe de matrices triangulaires. C'est-à-dire qu'il existe une base de \( V\) dans laquelle toutes les matrices \( g_1\) (avec \( g\in G\)) sont triangulaires supérieures.

    On fait de même avec l'application \( g\mapsto g_2\), ce qui donne une base du supplémentaire de \( V\) dans laquelle les matrices \( g_2\) sont triangulaires.

    En couplant ces deux bases, nous obtenons une base de \( \eC^n\) dans laquelle toutes les matrices \eqref{EqGOKTooEaGACG} (c'est-à-dire toutes les matrices de \( G\)) sont triangulaires supérieures.

    \item[Sinon]

    Nous supposons à présent que \( \eC^n\) n'a pas de sous-espaces non triviaux stables sous \( G\). Nous posons \( m=\min\{ k\tq D^k(G)=\{ e \} \}\), qui existe parce que \( G\) et résoluble et que sa suite dérivée termine sur \( {e}\) (proposition~\ref{PropRWYZooTarnmm}).

\item[Si \( m=1\)]

    Si \( m=1\) alors \( G\) est abélien et il existe une base de \( G\) dans laquelle toutes les matrices de \( G\) sont triangulaires (lemme~\ref{LemSLGPooIghEPI}). Le premier vecteur d'une telle base serait stable par \( G\), mais comme nous avons supposé qu'il n'y avait pas de sous-espaces non triviaux stabilisés par \( G\), il faut déduire que ce vecteur stable est à lui tout seul non trivial, c'est-à-dire que \( n=1\). Dans ce cas, le théorème est démontré.

\item[Si \( m>1\)]

    Nous devons maintenant traiter le cas où \( m>1\). Nous posons \( H=D^{m-1}(G)\); cela est un sous-groupe normal et abélien de \( G\). Encore une fois le résultat de trigonalisation simultanée~\ref{LemSLGPooIghEPI} donne une base dans laquelle tous les éléments de \( H\) sont triangulaires. En particulier le premier élément de cette base est un vecteur propre commun à toutes les matrices de \( H\).

    Soit \( V\) le sous-espace engendré par tous les vecteurs propres communs de \( H\). Nous venons de voir que \( V\) n'est pas vide. Nous allons montrer que \( V\) est stable par \( G\). Soient \( h\in H\), \( v\in V\) et \( g\in G\) :
    \begin{equation}    \label{EqPMOBooVLIhrJ}
        h\big( g(v) \big)=g\underbrace{g^{-1}hg}_{\in H}(v)=g(\lambda v)=\lambda g(v)
    \end{equation}
    parce que \( v\) est vecteur propre de \( g^{-1} hg\). Ce que le calcul \eqref{EqPMOBooVLIhrJ} montre est que \( g(v)\) est vecteur propre de \( h\) pour la valeur propre \( \lambda\). Donc \( g(v)\in V\) et \( V\) est stabilisé par \( G\). Mais comme il n'existe pas d'espaces non triviaux stabilisés par \( G\), nous en déduisons que \( V=\eC^n\). Donc tous les vecteurs de \( \eC^n\) sont vecteurs propres communs de \( H\). Autrement dit on a une base de diagonalisation simultanée de \( H\).

\item[\( H\) est dans le centre de \( G\)]

    Montrons à présent que \( H\) est dans le centre de \( G\), c'est-à-dire que pour tout \( g\in G\) et \( h\in H\) il faut \( ghg^{-1}=h\). D'abord \( ghg^{-1}\) est une matrice diagonale (parce que elle est dans \( H\)) ayant les mêmes valeurs propres que \( h\). En effet si \( \lambda\) est valeur propre de \( ghg^{-1}\) pour le vecteur propre \( v\), alors
    \begin{subequations}
        \begin{align}
            (ghg^{-1})(v)&=\lambda v\\
            h\big( g^{-1} v \big)&=\lambda \big( g^{-1}v \big),
        \end{align}
    \end{subequations}
    c'est-à-dire que \( \lambda\) est également valeur propre de \( h\), pour le vecteur propre \( g^{-1} v\). Mais comme \( h\) a un nombre fini de valeurs propres, il n'y a qu'un nombre fini de matrices diagonales ayant les mêmes valeurs propres que \( h\). L'ensemble \( \AD(G)h\) est donc un ensemble fini. D'autre part, l'application \( g\mapsto g^{-1}hg\) est continue, et \( G\) est connexe, donc l'ensemble \( \AD(G)h\) est connexe. Un ensemble fini et connexe dans \( \GL(n,\eC)\) est nécessairement réduit à un seul point. Cela prouve que \( ghg^{-1}=h\) pour tout \( g\in G\) et \( h\in H\).

\item[Espaces propres stables pour tout \( G\)]

        Soit \( h\in H\) et \( W\) un espace propre de \( h\) (ça existe non vide parce que \( H\) est triangularisé, voir plus haut). Alors nous allons prouver que \( W\) est stable pour tous les éléments de \( G\). En effet si \( w\in W\) avec \( h(w)=\lambda w\) alors en permutant \( g\) et \( h\),
        \begin{equation}
            hg(w)=g(hw)=\lambda g(w),
        \end{equation}
        donc \( g(w)\) est aussi vecteur propre de \( h\) pour la valeurs propre \( \lambda\), c'est-à-dire que \( g(w)\in W\). Vu que nous supposons que \( \eC^n\) n'a pas d'espaces invariants non triviaux, nous devons conclure que \( W=\eC^n\), c'est-à-dire que \( H\) est composé d'homothéties. C'est-à-dire que pour tout \( h\in H\) nous avons \( h=\lambda_h\mtu\).

    \item[Contradiction sur la minimalité de \( m\)]

        Les éléments d'un groupe dérivé sont de déterminant \( 1\) parce que \( \det(g_1g_2g_1^{-1}g_2^{-1})=1\). Par conséquent pour tout \( h\), le nombre \( \lambda_h\) est une racine \( n\)\ieme de l'unité. Vu qu'il n'y a qu'une quantité finie de racines \( n\)\ieme de l'unité, le groupe \( H\) est fini et connexe et donc une fois de plus réduit à un élément, c'est-à-dire \( H=\{ e \}\). Cela contredit la minimalité de \( m\) et donc produit une contradiction. Nous devons donc avoir \( m=1\).

    \item[Conclusion]

        Nous avons vu que si \( \eC^n\) avait un sous-espace non trivial fixé par \( G\) alors le théorème était démontré. Par ailleurs si \( \eC^n\) n'a pas un tel sous-espace, soit \( m=1\) (et alors le théorème est également prouvé), soit \( m>1\) et alors on a une contradiction.

        Bref, le théorème est prouvé sous peine de contradiction.
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

\begin{definition}[\cite{ooUQBZooCAKfrE}]      \label{DEFooEEQGooNiPjHz}
    Soient trois espaces vectoriels \( E,F\) et \( V\) sur le même corps commutatif \( \eK\). Une application \( b\colon E\times F\to V\) est \defe{bilinéaire}{application bilinéaire} si elle est séparément linéaire en ses deux variables, c'est-à-dire si
    \begin{enumerate}
        \item 
            \( b(u_1+u_2,v)=b(u_1,v)+b(u_2,v)\),
        \item
            \( b(u,v_1+v_2)=b(u,v_1)+b(u,v_2)\)
        \item
            \( b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,u_1,u_2\in E\), \( v,v_1,v_2\in F\) et pour tout \( \lambda\in \eK\).

    Dans le cas \( E=F\) et \( V=\eK\), nous parlons de \defe{forme bilinéaire}{forme!bilinéaire} sur \( E\).

    Nous parlons de forme bilinéaire \defe{symétrique}{forme bilinéaire symétrique} si de plus \( b(u,v)=b(v,u)\).
\end{definition}

\begin{normaltext}
    Une application bilinéaire \( E\times E\to \eK\) n'est pas une application linéaire; la distinction est importante. La linéarité est
    \begin{equation}
        b(\lambda u,\lambda v)= b\big( \lambda(u,v) \big)=\lambda b(u,v)
    \end{equation}
    et la bilinéarité est
    \begin{equation}
        b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v).
    \end{equation}
    En réalité la seule forme qui soit à la fois linéaire et bilinéaire est la forme identiquement nulle : la condition
    \begin{equation}
        b(\lambda u,\lambda v)=\lambda^2b(u,v)=\lambda b(u,v)
    \end{equation}
    pour tout \( \lambda\in \eK\) implique \( b(u,v)=0\).
\end{normaltext}

\begin{example}[\cite{BIBooJMSXooYUADgm}]
    L'application
    \begin{equation}
        \begin{aligned}
            b\colon \eM(n,\eK)\times \eM(n,\eK)&\to \eK \\
            (A,B)&\mapsto \trace(AB) 
        \end{aligned}
    \end{equation}
    est une forme bilinéaire symétrique.

    La vérification est un calcul :
    \begin{equation}
        \trace(BA)=\sum_{i}(BA)_{ii}=\sum_{ik}B_{ik}A_{ki}=\sum_{ik}A_{ki}A_{ik}=\sum_k(AB)_{kk}=\trace(AB).
    \end{equation}
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dégénérescence d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\), une forme bilinéaire symétrique non dégénérée  sur l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\) où \( \eK\) est un corps de caractéristique différente de \( 2\). Nous notons \( q\) la forme quadratique associée.

\begin{definition}      \label{DEFooNUBFooLfCqaK}
    Une forme bilinéaire est \defe{non dégénérée}{forme!bilinéaire!non dégénérée} \( b(x,z)=0\) pour tout \( z\) implique \( x=0\).
\end{definition}

\begin{lemma}   \label{LemyKJpVP}
    Soit \( b\) une forme bilinéaire non dégénérée. Si \( x\) et \( y\) sont tels que \( b(x,z)=b(y,z)\) pour tout \( z\), alors \( x=y\).
\end{lemma}

\begin{proof}
    C'est immédiat du fait de la linéarité en le premier argument et de la non-dégénérescence : si \( b(x,z)-b(y,z)=0\) alors
    \begin{equation}
        b(x-y,z)=0
    \end{equation}
    pour tout \( z\), ce qui implique \( x-y=0\).
\end{proof}

\begin{proposition}     \label{PROPooQHHPooSqpgcb}
    Une forme bilinéaire est non-dénénérée\footnote{Définition \ref{DEFooNUBFooLfCqaK}.} si et seulement si sa matrice associée est inversible.
\end{proposition}

\begin{proof}
    Nous savons que la matrice associée est symétrique et qu'elle peut donc être diagonalisée (théorème~\ref{ThoeTMXla}). En nous plaçant dans une base de diagonalisation, nous devons prouver que la forme est non-dégénérée si et seulement si les éléments diagonaux de la matrice sont tous non nuls.

    Écrivons \( b(x,z)\) en choisissant pour \( z\) le vecteur de base \( e_k\) de composantes \( (e_k)_j=\delta_{kj}\) :
    \begin{equation}
            b(x,e_k)=\sum_{ij}x_i(e_k)_j
            =\sum_i b_{ik}x_i
            =b_{kk}x_k.
    \end{equation}
    Si \( b\) est dégénérée et si \( x\) est un vecteur non nul (disons que la composante \( x_i\) est non nulle) de \( E\) tel que \( b(x,z)=0\) pour tout \( z\in E\), alors \( b_{ii}=0\), ce qui montre que la matrice de \( b\) n'est pas inversible.

    Réciproquement si la matrice de \( b\) est inversible, alors tous les \( b_{kk}\) sont différents de zéro, et le seul vecteur \( x\) tel que \( b_{kk}x_k=0\) pour tout \( k\) est le vecteur nul.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Topologie}
%---------------------------------------------------------------------------------------------------------------------------

La topologie considérée sur \( Q(E)\) est celle de la norme
\begin{equation}    \label{EqZYBooZysmVh}
    N(q)=\sup_{\| x \|_E=1}| q(x) |,
\end{equation}
qui du point de vue de \( S(n,\eR)\) est
\begin{equation}    \label{EQooJETQooIjxRWu}
    N(A)=\sup_{\| x \|_E=1}| x^tAx |.
\end{equation}
Notons que à droite, c'est la valeur absolue usuelle sur \( \eR\).

\begin{proposition} \label{PropFSXooRUMzdb}
    Soit \( \{ e_i \}\) une base de \( E\). L'application
    \begin{equation}
        \begin{aligned}
            \phi\colon Q(E)&\to S(n,\eR) \\
            q&\mapsto \big(   b(e_i,e_j)   \big)_{i,j}
        \end{aligned}
    \end{equation}
    où \( b\) est forme bilinéaire associée à \( q\) est une bijection linéaire et continue\footnote{Pour les topologies des normes \eqref{EqZYBooZysmVh} et \eqref{EQooJETQooIjxRWu}.}.
\end{proposition}

\begin{proof}
    Si \( \phi(q)=\phi(q')\); alors
    \begin{equation}
        q(x)=\sum_{i,j}\phi(q)_{ij}x_ix_j=\sum_{i,j}\phi(q')_{ij}x_ix_j=q'(x).
    \end{equation}
    Donc \( q=q'\). L'application \( \phi\) est donc injective

    De plus elle est surjective parce que si \( B\in S(n,\eR)\) alors la forme quadratique
    \begin{equation}
        q(x)=\sum_{i,j}B_{ij}x_ix_j
    \end{equation}
    a évidemment \( B\) comme matrice associée. L'application \( \phi\) est donc surjective.

    Notre application \( \phi\) est de plus linéaire parce que l'association d'une forme quadratique à la forme bilinéaire associée est linéaire.

    En ce qui concerne la continuité, nous la prouvons en zéro en considérant une suite convergente \( q_n\stackrel{Q(E)}{\longrightarrow}0\). C'est-à-dire que
    \begin{equation}
        \sup_{\| x \|=1}| q_n(x) |\to 0.
    \end{equation}
    Nous rappelons l'identité de polarisation :
    \begin{equation}
        b_n(x,y)=\frac{ 1 }{2}\big( q_n(x-y)-q(x)-q(y) \big).
    \end{equation}
    En ce qui concerne deux des trois termes, il n'y a pas de problèmes :
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|=\big| b_n(e_i,e_j) \big|\leq\frac{ 1 }{2}\big| b_n(e_i-e_j) \big|+\frac{ 1 }{2}\big| q_n(e_i) \big|+\frac{ 1 }{2}\big| q_n(e_j) \big|.
    \end{equation}
    Si \( n\) est assez grand, nous avons tout de suite
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\epsilon.
    \end{equation}
    Nous définissons \( e_{ij}\) et \( \alpha_{ij}\) de telle sorte que \( e_i-e_j=\alpha_{ij}e_{ij}\) avec \( \| e_{ij} \|=1\). Si \( \alpha=\max\{ \alpha_{ij},1 \}\) alors nous avons
    \begin{equation}
        q_n(e_i-e_j)=\alpha_{ij}^2q_n(e_{ij})\leq \alpha^2q_n(e_{ij}).
    \end{equation}
    Il suffit maintenant de prendre \( n\) assez grand pour avoir \( \sup_{\| x \|=1}| q_n(x) |\leq \frac{ \epsilon }{ \alpha^2 }\) pour avoir
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ \epsilon }{2}+\frac{ \epsilon }{ \alpha^2 }.
    \end{equation}
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isotropie}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Isotropie]   \label{DefVKMnUEM}
    Un vecteur est \defe{isotrope}{isotrope (vecteur)} pour \( b\) s'il est perpendiculaire à lui-même; en d'autres termes, \( x\) est isotrope si et seulement si \( b(x,x)=0\). Un sous-espace \( W\subset E\) est \defe{totalement isotrope}{isotrope!totalement} si pour tout \( x,y\in W\), nous avons \( b(x,y)=0\).

    Le \defe{cône isotrope}{isotrope!cône} de \( b\) est l'ensemble de ses vecteurs isotropes :
    \begin{equation}
        C(b)=\{ x\in E\tq b(x,x)=0 \}.
    \end{equation}
\end{definition}
Nous introduisons quelques notations. D'abord pour \( y\in E\) nous notons
\begin{equation}
    \begin{aligned}
        \Phi_y\colon E&\to \eR \\
        x&\mapsto b(x,y)
    \end{aligned}
\end{equation}
et ensuite
\begin{equation}
    \begin{aligned}
        \Phi\colon E&\to E^* \\
        y&\mapsto \Phi_y.
    \end{aligned}
\end{equation}
\begin{definition}
    Le fait pour une forme bilinéaire \( b\) d'être dégénérée signifie que l'application \( \Phi\) n'est pas injective. Le \defe{noyau}{noyau!d'une forme bilinéaire} de la forme bilinéaire est celui de \( \Phi\), c'est-à-dire
    \begin{equation}
        \ker(b)=\{ z\in E\tq b(z,y)=0\,\forall y\in E \}.
    \end{equation}
    Autrement dit, \( \ker(b)=E^{\perp}\) où le perpendiculaire est pris par rapport à \( b\).
\end{definition}
Notons tout de même que nous utilisons la notation \( \perp\) même si \( b\) est dégénérée et éventuellement pas positive; c'est-à-dire même si la formule \( (x,y)\mapsto b(x,y)\) ne fournit pas un produit scalaire.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Plus à propos de formes bilinéaires dans le thème \ref{THEMEooOAJKooEvcCVn}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{BIBooWVWZooZqliJt}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme!quadratique} sur \( E\) est une application \( q\colon E\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon E\times E\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in E\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{definition}[Application bilinéaire définie positive, thème~\ref{THEMEooYEVLooWotqMY}]      \label{DEFooJIAQooZkBtTy}
    Si $b$ est une application bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} sur un espace vectoriel \( E\) nous disons qu'elle est
    \begin{enumerate}
        \item
            \defe{définie positive}{application!définie positive} si $b(x,x)\geq 0$ pour tout $x\in E$ et $b(x,x)=0$ si et seulement si $x=0$.
        \item
            \defe{semi-définie positive}{application!semi-définie positive} si $b(x,x)\geq 0$ pour tout $x\in E$. Nous dirons aussi parfois qu'elle est simplement «positive».
        \end{enumerate}
\end{definition}
Cela est évidemment à lier à la définition~\ref{DefAWAooCMPuVM} et à la proposition \ref{PROPooUAAFooEGVDRC} : une application bilinéaire est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{proposition} \label{PROPooZLXVooOsXCcB}
    Soit une forme bilinéaire \( b\) et la forme quadratique associée \( q\). Alors nous avons l'\defe{identité de polarisation}{identité de polarisation} :
    \begin{equation}    \label{EqMrbsop}
        b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de substituer dans le membre de droite \( q(x)=b(x,y)\) et d'utiliser la bilinéarité :
    \begin{subequations}
        \begin{align}
            q(x)+q(y)-q(x-y)&=b(x,x)+b(y,y)-b(x-y,x-y)\\
            &=b(x,x)+b(y,y)-b(x)+b(x,y)+b(y,x)-b(y,y)\\
            &=2b(x,y)
        \end{align}
    \end{subequations}
    où nous avons utilisé le fait que \( b\) est symétrique : \( b(x,y)=b(y,x)\).
\end{proof}

\begin{lemma}       \label{LEMooLKNTooSfLSHt}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'identité de polarisation de la proposition \ref{PROPooZLXVooOsXCcB}. Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

\begin{definition}      \label{DEFooGECOooCCGVXG}
    Soit une forme quadratique \( q\) sur \( E\). Nous disons que \( v,w\in E\) sont \defe{\( q\)-orthogonaux}{\( q\)-orthogonal} si \( b(v,w)=0\) la forme bilinéaire \( b\) associée à \( q\) par le lemme \ref{LEMooLKNTooSfLSHt}.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice associée à une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooAOGPooXWXUcN}
    Soit une forme bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.} \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous définissons les nombres
    \begin{equation}    \label{EQooCUGFooRlKUtu}
        B_{\alpha\beta}=b(f_{\alpha},f_{\beta}),
    \end{equation}
    qui forment une matrice symétrique dans \( \eM(n,\eK)\). Cette matrice est la \defe{matrice associée}{matrice d'une forme bilinéaire} à la forme bilinéaire \( b\).

    La matrice d'une forme quadratique est celle associée à sa forme bilinéaire associée.
\end{definition}

\begin{lemma}       \label{LEMooDCIOooTlVZMR}
    Soit une forme bilinéaire \( b\colon E\times E\to \eK\) et une base quelconque \( \{ f_{\alpha} \}\) de \( E\). Nous notons \( B\) la matrice de \( b\) (definition \ref{DEFooAOGPooXWXUcN}) et \( q\) la forme quadratique associée.

    Alors nous avons
\begin{equation}        \label{EQooQFMWooVKVLMx}
    b(x,y)=\sum_{\alpha\beta}B_{\alpha\beta}x_{\alpha}y_{\beta}.
\end{equation}
et
\begin{equation}
    b(x,y)=x\cdot By.
\end{equation}
où le point est le produit scalaire usuel (composante par composante).
\end{lemma}

\begin{proof}
    Si \( x=\sum_{\alpha}x_{\alpha}f_{\alpha}\) et \( y=\sum_{\beta}y_{\beta}f_{\beta}\) :

    En utilisant la convention \eqref{EQooAXRJooUwHbjB} et les choses autour (voir aussi \ref{SECooBTTTooZZABWA}),
    \begin{equation}
        b(x,y)=\sum_{\alpha}x_{\alpha}\sum_{\beta}B_{\alpha\beta}y_{\beta}=\sum_{\alpha}x_{\alpha}(By)_{\alpha}=x\cdot By.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooUAAFooEGVDRC} \label{PROPooNQSXooVMFAtU}
    Une application bilinéaire est définie positive si et seulement si sa matrice symétrique associée l'est.
\end{proposition}

\begin{proof}
    La définition \ref{DEFooJIAQooZkBtTy} dit que \( b\) est strictement définie positive lorsque \( b(x,x)\geq 0\) et \( b(x,x)=0\) si et seulement si \( x=0\).

    D'autre part, le lemme \ref{LemWZFSooYvksjw} dit que la matrice \( B\) est strictement définie positive lorsque \( x\cdot Bx\geq 0\) et \( x\cdot Bx=0\) si et seulement si \( x=0\).

    Le lien entre les deux est que le lemme \ref{LEMooDCIOooTlVZMR} nous enseigne que pour tout \( x\) et \( y\),
    \begin{equation}
        b(x,y)=x\cdot By
    \end{equation}
    où \( B\) est la matrice de \( b\).
\end{proof}

\begin{proposition}     \label{PROPooCIEUooODqfwm}
    Soit une forme quadratique \( q\colon E\to \eK\) et sa matrice\footnote{Matrice associée à une forme quadratique, définition \ref{DEFooAOGPooXWXUcN}.} \( (q_{ij})\in \eM(n,\eK)\). Nous avons
    \begin{subequations}        \label{SUBEQSooEHVXooJjKLqyiB}
        \begin{align}
            q(x)&=\sum_{i=1}^n\sum_{j=1}^nq_{ij}x_ix_j\\
            &=\sum_{i=1}^nq_{ii}x_i^2+2\sum_{1\leq i <j\leq n}q_{ij}x_ix_j.
        \end{align}
    \end{subequations}
\end{proposition}

\begin{normaltext}
    De nombreux auteurs préfèrent écrire des choses comme \( x^tBy\) ou \( xB^ty\) ou \( xBy^t\) et se poser de longues questions sur qui est un «vecteur colonne» et qui est un «vecteur ligne», et si la matrice \( B\) soit être transposée ou non. Toutes ces notations servent(?) à cacher un bête produit scalaire.
\end{normaltext}

\begin{normaltext}
    Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Voir la section \ref{SECooBTTTooZZABWA}]     \label{PROPooLBIOooUpzxXA}
    Soit une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}} \( b\colon V\times V\to \eK\) dont la matrice\footnote{Définition~\ref{EQooCUGFooRlKUtu}.} dans la base \( \{ e_i \}\) est \( A\) et celle dans la base \( \{ f_{\alpha} \}\) est \( B\). Nous supposons que les bases sont liées par \( f_{\alpha}=\sum_{i}Q_{i\alpha}e_i\). Alors
\begin{equation}        \label{EQooZUVTooKjqnJj}
    B=Q^tAQ.
\end{equation}
\end{proposition}

\begin{proof}
    Soit \( x,x'\in V\) de coordonnées \( (x_i)\) et \( (x'_i)\) dans la base \( \{ e_i \}\) et \( (y_{\alpha})\), \( (y'_{\alpha})\) dans la base \( \{ f_{\alpha} \}\). Par définition de la matrice associée à une forme bilinéaire,
    \begin{equation}
        b(x,x')=\sum_{ij}A_{ij}x_ix'_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
    \end{equation}
    En remplaçant les \( x_i\) et \( x'_i\) par leurs valeurs en fonction de \( y_{\alpha}\) et \( y'_{\beta}\) données par la proposition \ref{PROPooNYYOooHqHryX}, nous trouvons
    \begin{subequations}
        \begin{align}
            b(x,x')&=\sum_{ij\alpha\beta}A_{ij}Q_{i\alpha}y_{\alpha}Q_{j\beta}y'_{\beta}\\
            &=\sum_{\alpha\beta}(Q^tAQ)_{\alpha\beta}y_{\alpha}y'_{\beta}
        \end{align}
    \end{subequations}
    où \( Q^t\) désigne la transposée de la matrice \( Q\) :  \( Q^t_{ij}=Q_{ji}\). Vu que les nombres \( y_{\alpha}\) et \( y'_{\beta}\) sont arbitraires nous déduisons\footnote{Lemme~\ref{LEMooLXAHooPRyHaF}.} que \( B=Q^tAQ\).
\end{proof}

\begin{remark}      \label{REMooNEJLooSqgeih}
    Notons que cette «loi de transformation» n'est pas la même que celle pour une application linéaire\footnote{Proposition \ref{PROPooNZBEooWyCXTw}.}. Ici nous avons \( Q^t\) alors que pour les applications linéaires nous avions \( Q^{-1}\).

    Pour cette raison, tant que nous travaillons avec des bases orthonormées, c'est-à-dire tant que \( Q\) est orthogonale\footnote{Définition~\ref{DefMatriceOrthogonale}.}, nous pouvons confondre une application linéaire avec une application bilinéaire en passant par la matrice. Mais cette identification n'est pas du tout canonique : elle repose sur le fait que les bases soient orthonormées.

    Il en découle que la réduction des endomorphismes et la réduction des formes bilinéaires ne sont pas tout à fait les mêmes théories. Par exemple la pseudo-diagonalisation simultanée (corolaire~\ref{CorNHKnLVA}) est un résultat de réduction de forme bilinéaire et non d'endomorphismes.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DEFooGVGGooWQEIET}
    Soit une forme quadratique \( q\) sur l'espace vectoriel \( V\) sur \( \eK\). Soit \( A\) la matrice de \( q\) dans la base \( \{ e_i \}\) et \( B\) sa matrice dans la base \( \{f_{\alpha}  \}\). Nous supposons que le changement de base est orthogonal.

    Alors les valeurs propres de \( A\) et \( B\) sont les mêmes.

    Ces valeurs sont les \defe{valeurs propres}{valeur propre!forme quadratique} de \( q\).
\end{lemmaDef}

\begin{proof}
    Nous nous rappelons de la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à \( Q\), et à la proposition \ref{PROPooLBIOooUpzxXA} qui parle de changement de base : \( B=Q^tAQ\) où \( Q\) est orthogonale.

    Soit un vecteur propre \( v\) de \(A \), de valeur propre \( \lambda\). Alors nous prouvons que \( Q^tv\) est un vecteur propre pour \( B\), de même valeur propre \( \lambda\). En effet,
    \begin{equation}
        BQ^tv=Q^tAQQ^tv=Q^tAv=\lambda Q^tv
    \end{equation}
    où nous avons utilisé \( QQ^t=\mtu\) et \( Av=\lambda v\).
\end{proof}

\begin{proposition}\label{PropFWYooQXfcVY}
    Dans la base de diagonalisation de sa matrice associée, une forme quadratique a la forme
    \begin{equation}
        q(x)=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de la matrice associée à \( q\).
\end{proposition}

\begin{proof}
    Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée. Si \( \{ f_i \}\) est une base de diagonalisation\footnote{Qui existe parce que la matrice est symétrique, théorème~\ref{ThoeTMXla}.} de la matrice de \( b\) alors dans cette base nous avons
\begin{equation}
    q(x)=b(x,x)=\sum_{ij}x_ix_jb(f_i,f_j)=\sum_i\lambda_ix_i^2
\end{equation}
où les \( \lambda_i\) sont les valeurs propres de la matrice de \( b\).
\end{proof}

Notons que si nous choisissons une autre base de diagonalisation, les \( \lambda_i\) ne changement pas (à part l'ordre éventuellement). 

Cela justifie la définition pour dire que nous nous permettrons de parler des \defe{valeurs propres}{valeur propre!d'une forme quadratique} d'une forme quadratique comme étant les valeurs propres de la matrice associée.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Réduction de Gauss\cite{BIBooNUUEooJUjLpy,BIBooUULNooUtlrar}]     \label{THOooOMMFooKxqICS}
    Soit une forme quadratique non nulle \( q\) sur l'espace vectoriel \( E\) sur le corps \( \eK\). Il existe une base  \(\{ l_i \}_{i=1,\ldots, n}\) de \( E^*\) et des coefficients \( \alpha_i\in \eK\) tels que 
        \begin{equation}
            q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
        \end{equation}
\end{theorem}

\begin{proof}
    Notre point de départ sont les formules \eqref{SUBEQSooEHVXooJjKLqyiB} pour la forme quadratique. Nous allons faire la preuve par récurrence sur la dimension de l'espace. Si \( n=1\), alors nous avons seulement
    \begin{equation}
        q(x)=\alpha x^2
    \end{equation}
    et donc le théorème est fait avec \( l(x)=x\).

    Nous supposons que le théorème est prouvé pour tout espace de dimension \( n\). Une forme quadratique pour un espace de dimension \( n+1\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^{n+1}m_{ii}x_i^2+2\sum_{1\leq i < j\leq n+1}m_{ij}x_ix_j.
    \end{equation}
    Vu que \( q\) est non nulle, un des \( m_{ij}\) est non nul. Nous allons diviser en plusieurs cas.
    \begin{itemize}
        \item
            \( m_{11}\neq 0\)
        \item
            \( m_{kk}\neq 0\) avec \( k\neq 1\)
        \item
            \( m_{12}\neq 0\) et \( m_{ii}=0\) pour tout \( i\).
        \item
            \( m_{kl}\neq 0\) avec \( (k,l)\neq (1,2)\) et \( m_{ii}=0\) pour tout \( i\).
    \end{itemize}
    Ces cas ne sont pas exclusifs, mais ils couvrent toutes les possibilités.

    \begin{subproof}
        \item[Si \( m_{11}\neq 0\)]
            Nous écrivons \( q\) sous la forme
            \begin{subequations}
                \begin{align}
                    q(x)&=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big)\\
                    &=m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{j=2}^{n+1}m_{1j}x_1x_j+2\sum_{i=2}^n\sum_{j=i+1}^{n+1}(m_{ij}x_ix_j)\\
                    &=m_{11}x_1^2+2x_1\sum_{j=2}^{n+1}m_{1j}x_k+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\left( x_1^2+2x_1\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j \right)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1^2+2x_1f(x_2,\ldots, x_{n+1}) \big)+R(x_2,\ldots, x_{n+1})\\
                    &=m_{11}\big( x_1+f(x_2,\ldots, x_{n+1}) \big)^2-f(x_2,\ldots, x_{n+1})+R(x_2,\ldots, x_{n+1})
                \end{align}
            \end{subequations}
            où 
            \begin{itemize}
                \item \( R\) est une forme quadratique de \( n-1\) variables;
                \item nous avons noté \( f(x_2,\ldots, x_{n+1})=\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j\).
            \end{itemize}
            Maintenant, toute la partie \( -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})\) est une forme quadratique de \( n\) variables. Par hypothèse de récurrence, il existe des coefficients \( \alpha_i\) et des formes linéairement indépendantes sur \( \eK^n\) \( l_i'(x_2,\ldots, x_{n+1})\) telles que
            \begin{equation}
                -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})=\sum_{i=2}^{n+1}\alpha_il_i'(x_2,\ldots, x_{n+1})^2.
            \end{equation}
            En posant ensuite \( l_j(x_1,\ldots, x_{n+1})=l'_j(x_2,\ldots, x_{n+1})\), ainsi que \( l_1(x_1,\ldots, x_{n+1})=x_1+f(x_2,\ldots, x_{n+1})\), nous avons
            \begin{equation}
                q(x)=m_{11}l_1(x)^2+\sum_{j=2}^{n+1}\alpha_jl_j(x)^2.
            \end{equation}
            
        \item[Si \( m_{kk}\neq 0\) avec \( k\neq 1\)]

            Nous nommons \( k\) le plus petit entier pour lequel \( m_{kk}\neq 0\), et nous supposons que \( k\neq 1\), parce que nous avons déjà couvert ce cas. Dans ce cas, nous avons
            \begin{equation}
                q(x)=m_{kk}x_k^2+\sum_{j=k+1}^{n+1}m_{jj}x_j^2  +2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big),
            \end{equation}
            et tout tourne comme dans le premier cas.
        \item[\( m_{ii}=0\) pour tout \( i\) et \( m_{12}\neq 0\)]
            Nous écrivons \( q\) en séparant les termes \( m_{1k}\) :
            \begin{subequations}
                \begin{align}
                    q(x)&=2\sum_{1\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2\sum_{2\leq j\leq n+1}m_{1j}x_1x_j+2\sum_{2\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+2x_1\sum_{2\leq j\leq n+1}m_{1j}x_j+2\sum_{3\leq j\leq n+1}m_{2j}x_2x_j+2\sum_{3\leq i<j\leq n+1}m_{ij}x_ix_j\\
                    &=2m_{12}x_1x_2+x_1f(x_2,\ldots, x_{n+1})+x_2g(x_3,\ldots, x_{n+1})+T(x_3,\ldots, x_{n+1})      \label{SUBEQooLBXBooXoLyuw}
                \end{align}
            \end{subequations}
            où \( f\) et \( g\) sont linéaires et \( T\) est multilinéaire.

            À ce moment, nous tentons de factoriser toute la partie concernant \( x_1\) et \( x_2\). L'idée est d'utiliser ceci :
            \begin{equation}
                (x_1+g)(x_2+f)=x_1x_2+x_1f+x_2g+fg,
            \end{equation}
            mais en mettant les bons coefficients pour reproduire ce que nous avons dans \eqref{SUBEQooLBXBooXoLyuw} : 
            \begin{equation}
                (2m_{12}+2g)(x_1+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }=2m_{12}x_1x_2+2x_1f+2x_2g.
            \end{equation}
            Cela pour dire que
            \begin{equation}
                q(x)=2(m_{12}x_1+g)(x_2+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }+T
            \end{equation}
            où \(-2fg/m_{12}+T\) est une forme quadratique de \( x_3,\ldots, x_{n+1}\), c'est à dire de \( n-1\) variables.

            L'hypothèse de récurrence nous donne des formes linéaires \( (l_i)_{i=3,\ldots, n+1}\) telles que
            \begin{equation}
                \frac{ 2fg }{ m_{12} }+T=\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
            Nous pouvons donc déjà écrire
            \begin{equation}
                q(x)=2l'_1(x)l'_2(x)+\sum_{i=3}^{n+1}\alpha_il_i(x)^2
            \end{equation}
            où
            \begin{itemize}
                \item Les forme \( l_i\) avec \( i\geq 3\) ne dépendent pas de \( x_1\) et \( x_2\), et sont donc indépendantes de \( l_1\) et \( l_2\).
                \item La forme \( l'_1\) ne dépend pas de \( x_2\),
                \item La forme \( l'_2\) ne dépend pas de \( x_1\).
            \end{itemize}
            Ce sont donc \( n+1\) formes linéaires indépendantes. Le seul problème résiduel est que les formes \( l'_1\) et \( l'_2\) arrivent en produit l'une de l'autre. Nous en définissons donc deux de plus :
            \begin{equation}
                \begin{aligned}[]
                    l_1(x)=\frac{ 1 }{2}(l'_1+l'_2)\\
                    l_2(x)=\frac{ 1 }{2}(l'_1-l'_2),
                \end{aligned}
            \end{equation}
            qui sont linéairement indépendantes l'une de l'autre et indépendantes des \( l_i\) (\( i\geq 3\)). Au final,
            \begin{equation}
                q(x)=l_1(x)^2+l_2(x)^2+\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
            \end{equation}
        \item[Si \( m_{ii}=0\) et \( m_{12}=0\) et \( m_{kl}\neq 0\) avec \( k<l\)]
            Nous considérons la permutation
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \} \\
                    i&\mapsto \begin{cases}
                         1   &   \text{si } i=k\\
                         2   &   \text{si } i=l\\
                         k   &   \text{si } i=1\\
                         l   &   \text{si } i=2\\
                        i    &    \text{sinon,}
                    \end{cases}
                \end{aligned}
            \end{equation}
            c'est à dire que \( \sigma\) permute \( 1\) et \( k\) ainsi que \( 2\) et \( l\). Ensuite nous posons
            \begin{equation}
                \begin{aligned}
                    s\colon \eR^{n+1}&\to \eR^{n+1} \\
                    e_i&\mapsto e_{\sigma(i)}. 
                \end{aligned}
            \end{equation}
            Nous allons un peu considérer \( q\circ s\), pour changer : 
            \begin{equation}        \label{EQooLVAWooAirEzP}
                (q\circ s)(x)=\sum_{i,j}m_{ij}s(x)_is(x)_j=\sum_{ij}x_{\sigma(i)}x_{\sigma(j)}.
            \end{equation}
            parce que \( s(x)_i=x_{\sigma(i)}\). 

            Utilisons un petit abus de notation pour considérer
            \begin{equation}
                \begin{aligned}
                    \sigma\colon \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \}&\to \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \} \\
                    (i,j)&\mapsto \big(\sigma(i), \sigma(j)\big). 
                \end{aligned}
            \end{equation}
            Cela est une bijection; nous pouvons utiliser le lemme \ref{DEFooLNEXooYMQjRo} pour permuter les termes dans \eqref{EQooLVAWooAirEzP} :      
            \begin{subequations}
                \begin{align}
                    (q\circ s)(x)&=\sum_{ij}m_{\sigma(i)\sigma(j)}x_{\sigma\sigma(i)}x_{\sigma\sigma(j)}\\
                    &=\sum_{ij}a_{ij}x_ix_j     \label{EQooPCTCooFnMWat}
                \end{align}
            \end{subequations}
            où nous avons posé \( a_{ij}=m_{\sigma(i)\sigma(j)}\) et utilisé le fait que \( \sigma=\sigma^{-1}\). Le point intéressant de l'histoire est que dans \eqref{EQooPCTCooFnMWat}, \( a_{12}=m_{kl}\neq 0\). La forme \( q\circ s\) est donc dans le cas déjà traité et il existe des formes linéaires \( l'_i\) telles que
            \begin{equation}
                (q\circ s)(x)=\sum_{i=1}^{n+1}\alpha_il'_i(x)^2.
            \end{equation}
            En évaluant cela en \( s(x)\), et en tenant compte de \( s=s^{-1}\), nous trouvons
            \begin{equation}
                q(x)=\sum_i\alpha_i(l_i\circ s)(x)^2,
            \end{equation}
            de telle sorte que \( l_i=l'_i\circ s\) soit la réponse à notre théorème.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooUULNooUtlrar}]       \label{PROPooYXMMooYIuGRd}
    Soient un espace vectoriel \( (E,\eK)\) et une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\). Une base de \( E\) est \( q\)-orthogonale\footnote{Définition \ref{DEFooGECOooCCGVXG}.} si et seulement si la matrice de \( q\) dans cette base est diagonale.
\end{proposition}

\begin{proof}
    La matrice de \( q\) est donnée par \( Q_{ij}=b(e_i,e_j)\). Donc oui, cette matrice est diagonale si et seulement si les \( e_i\) sont orthogonaux.
\end{proof}

\begin{proposition}
    Soit une forme quadratique \( q\). Si une base \( (e_i )\) de \( E\) est \( q\)-orthogonale, alors \( \mB=\{ e_i\tq q(e_i)=0 \}\) est une base de \( \ker(q)\).
\end{proposition}

\begin{proof}
    Nous considérons un vecteur de base \( e_j\), et nous montrons que \( q(e_j)=0\) si et seulement si \( e_j\in\ker(q)\). Nous savons par la proposition \ref{PROPooYXMMooYIuGRd} que la matrice de \( q\) dans la base \( (e_i)\) est diagonale et que les éléments diagonaux sont les \( q(e_i)\). Soit \( K=\{ i\tq q_(e_i)=0 \}\).
    \begin{subproof}
    \item[\( \Span\{ e_i \}_{i\in K}\subset\ker(q)\)]
        Si \( x=\sum_{i\in K}x_ie_i\), alors 
        \begin{equation}
            q(x)=b(x,x)=\sum_{i,j\in K}| x_i |^2b(e_i,e_j)=\sum_{i,j\in K}| x_i |^2\delta_{ij}q(e_i)=0
        \end{equation}
        parce que \( q(e_i)=0\) dès que \( i\in K\).
        \item{\( \ker(q)\subset\Span\{ e_i \}_{i\in K}\)}
            Soit \( x\in \ker(q)\) et écrivons-le sous la forme \( x=\sum_{i=1}^nx_ie_i\). Nous avons
            \begin{equation}
                0=q(x)=\sum_i| x_i |^2q(e_i).
            \end{equation}
            Mais \(    | x_i |^2\geq 0 \) et \( q(e_i)\geq 0\), donc si \( q(e_i)\neq 0\), alors \( x_i=0\). Donc les seules composantes non nulles de \( x\) sont celles sur lesquelles \( q\) s'annule. En d'autres termes \( x=\sum_ix_ie_i\in \Span\{ e_i \}_{i\in K}\).
    \end{subproof}
\end{proof}

\begin{theorem}[\cite{BIBooUULNooUtlrar,MonCerveau}]       \label{THOooIDMPooIMwkqB}
    Toute forme quadratique sur un espace vectoriel de dimension finie admet une base formée de vecteurs \( 2\) à \( 2\) orthogonaux (pour la forme considérée).
\end{theorem}

\begin{proof}
    Nous considérons la base \(  \{ l_i \}    \) de \( E^*\) donnée par la réduction de Gauss (théorème \ref{THOooOMMFooKxqICS}). La forme quadratique \( q\) s'écrit
    \begin{equation}
        q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
    \end{equation}
    La base préduale\footnote{Définition, existence, unicité dans la proposition \ref{PROPooDBPGooPagbEB}.} \( \{ e_i \}\) de \( \{ l_i \}\) répond aux conditions. Pour le vérifier, nous considérons la forme bilinéaire associée à \( q\) par l'identité de polarisation \ref{PROPooZLXVooOsXCcB} :
    \begin{equation}
        b(e_i,e_j)=\frac{ 1 }{2}\big( q(e_i)+q(e_j)-q(e_i-e_j) \big).
    \end{equation}
    Vu que \( l_k(e_i)=\delta_{ki}\), nous avons
    \begin{equation}
        q(e_i)=\sum_{k=1}^n\alpha_kl_k(e_i)^2=\alpha_i.
    \end{equation}
    En utilisant la linéarité,
    \begin{subequations}
        \begin{align}
            q(e_i-e_j)&=\sum_k\alpha_kl_k(e_i-e_j)^2\\
            &=\sum_k\alpha_k(\delta_{ki}-\delta_{kj})^2\\
            &=\sum_k\alpha_k(\delta_{ki}+\delta_{kj}-2\delta_{ki}\delta_{ki})\\
            &=\alpha_i+\alpha_j-2\delta_{ij}\alpha_i.
        \end{align}
    \end{subequations}
    Donc 
    \begin{equation}
        b(e_i,e_j)=\delta_{ij}\alpha_i.
    \end{equation}
    Les vecteurs \( \{ e_i \}\) sont donc bien deux à deux \( q\)-orthogonaux.
\end{proof}

Notons qu'en l'absence de notion de racine carrée sur \( \eK\), il n'est pas possible de considérer \( \sqrt{ \alpha_i }\) et donc de base \( q\)-orthonormée.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

Le théorème \ref{THOooIDMPooIMwkqB} a déjà donné une base orthogonale pour toute forme quadratique sur un espace vectoriel \( (E,\eK)\) de dimension finie. Dans le cas de \( \eR^n\), nous pouvons en donner une preuve basée sur le théorème spectral, c'est la proposition \ref{PROPooUKRUooGRIDHt}.

\begin{proposition}     \label{PROPooUKRUooGRIDHt}
    Soit une forme bilinéaire symétrique \( b\) sur un \( \eR^n\). Il existe une matrice orthogonale \( Q\) telle que 
    \begin{enumerate}
        \item
            \( D=Q^tbQ\) est diagonale
        \item
            \( D(x,y)=b(Qx,Qy)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Il existe une base \( (f_i)_{i=1,\ldots, n}\) qui est \( b\)-orthogonale.

    Dans cet énoncé, nous mélangeons sans vergogne les formes et les matrices, en supposant qu'une base soit fixée\footnote{Autrement dit, si vous avez en tête d'utiliser cette proposition pour \( \eR^n\) c'est bon; mais sinon vous devez choisir une base et considérer toutes les matrices dans cette base.}. Par exemple
    \begin{equation}
        D(x,y)=\sum_{ij}D_{ij}x_iy_j.
    \end{equation}
\end{proposition}

\begin{proof}
    Pour la matrice diagonale, c'est le théorème spectral \ref{ThoeTMXla}\ref{ITEMooMWWRooXxGONW} qui joue parce que la matrice d'une forme bilinéaire symétrique est symétrique (c'est vu de la définition \eqref{EQooCUGFooRlKUtu}).

    Pour le reste c'est un calcul :
    \begin{subequations}
        \begin{align}
            D(x,y)&=\sum_{ijkl}Q^t_{ik}b_{kl}Q_{lj}x_iy_j\\
            &=\sum_{ijkl}b_{kl}(Q_{ki}x_i)(Q_{lj}y_j)\\
            &=\sum_{kl}b_{kl}(Qx)_k(Qy)_l\\
            &=b(Qx,Qy).
        \end{align}
    \end{subequations}
    Nous avons utilisé le produit matrice fois vecteur donné par \eqref{EQooQFVTooMFfzol}.

    En ce qui concerne l'existence d'une base \( b\)-orthogonale, vu que \( D\) est diagonale, nous avons, pour \( i\neq j\) que \( D(e_i,e_j)=0\). Donc en posant \( f_i=Qe_i\), nous trouvons
    \begin{equation}
        0=D(e_i,e_j)=b(Qe_i,Qe_j)=b(f_i,f_j).
    \end{equation}
    La base \( (Qe_i)_{i=1,\ldots, n}\) est donc \( b\)-orthogonale.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométrie, forme quadratique et bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    La forme quadratique \( q(x)=x_1^2+x_2^2\) donne la norme euclidienne. La forme bilinéaire associée est \( b(x,y)=x_1y_1+x_2y_2\), qui est le produit scalaire usuel.
\end{example}

Il ne faudrait pas déduire trop vite que la formule \( \| x \|^2=q(x)\) donne une norme dès que \( q\) est non dégénérée. En effet \( q\) peut ne pas être définie positive. La forme \( q(x)=x_1^2-x_2^2\) prend des valeurs positives et négatives. A fortiori \( d(x,y)=q(x-y)\) ne donne pas toujours une distance.

\begin{definition}      \label{DEFooECTUooRxBhHf}
    Une \defe{isométrie}{isométrie!de forme quadratique} pour la forme quadratique \( q\) est une application bijective \( f\colon V\to V\) telle que 
    \begin{equation}
     q(x-y)=q\big( f(x)-f(y) \big).
    \end{equation}
     Dans les cas où \( q\) donne une distance, alors c'est une isométrie au sens usuel.
\end{definition}

\begin{definition}[Thème \ref{THMooVUCLooCrdbxm}]      \label{DEFooIQURooMeQuqX}
    Soit un espace vectoriel \( E\) muni d'une forme bilinéaire \( b\). Une \defe{isométrie}{isométrie (forme bilinéaire)} pour \( b\) est une bijection \( f\colon E\to E\) telle que
    \begin{equation}
        b\big( f(x),f(y) \big)=b(x,y)
    \end{equation}
    pour tout \( x,y\in E\).
\end{definition}

\begin{lemma}   \label{LemewGJmM}
    Soient \( q\) une forme quadratique et \( b\) la forme bilinéaire associée par le lemme~\ref{LEMooLKNTooSfLSHt}. Une application \( f\colon E\to E\) telle que \( f(0)=0\) est une isométrie pour \( b\) si et seulement si elle est une isométrie pour \( q\).
\end{lemma}

\begin{proof}
    Pour une application bijective \( f\colon E\to E\) telle que \( f(0)=0\), nous devons prouver l'équivalence des propriétés suivantes :
    \begin{enumerate}
        \item
            \( b\big( f(x),f(y) \big)=b(x,y)\) pour tout \( x,y\in E\);
        \item
            \( q\big( f(x)-f(y) \big)=q(x-y)\) pour tout \( x,y\in E\).
    \end{enumerate}

    Dans le sens direct, en posant \( x=y\) nous trouvons tout de suite \( q(f(x))=q(x)\); ensuite en utilisant la distributivité de \( b\),
    \begin{subequations}
        \begin{align}
            q\big( f(x)-f(y) \big)&=b\big( f(x)-f(y),f(x)-f(y) \big)\\
            &=q\big( f(x) \big)-2b\big( f(x),f(y) \big)+q\big( f(y) \big)\\
            &=q(x)+q(y)-2b(x,y)\\
            &=q(x-y).
        \end{align}
    \end{subequations}

    Dans l'autre sens, nous commençons par remarquer que l'hypothèse \( f(0)=0\) donne \( q(x)=q\big( f(x) \big)\). Ensuite nous utilisons l'identité de polarisation \eqref{EqMrbsop} :
    \begin{subequations}
        \begin{align}
            b\big( f(x),f(y) \big)&=\frac{ 1 }{2}\big[ q\big( f(x) \big)+q\big( f(y) \big)-q\big( f(x-y) \big) \big]\\
            &=\frac{ 1 }{2}\big[ q(x)+q(y)-q(x-y) \big]\\
            &=b(x,y).
        \end{align}
    \end{subequations}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Isométries}
%---------------------------------------------------------------------------------------------------------------------------

Voici un théorème pas toujours bien énoncé dans les cours de physique qui font de la relativité. Au moment de «prouver» les transformations de Lorentz\footnote{Théorème \ref{THOooYHDWooWxVovH}.}, beaucoup oublient de justifier pourquoi elles devraient être linéaires.
\begin{theorem}[\cite{ooQFKAooFnllQU}]     \label{ThoDsFErq}
    Une isométrie\footnote{Définition \ref{DEFooIQURooMeQuqX}.} d'une forme bilinéaire non dégénérée est linéaire.
\end{theorem}

\begin{proof}
    Soient une forme bilinéaire non-dégénérée \( b\) sur l'espace vectoriel \( E\) ainsi qu'une isométrie $f$ pour icelle. Soit \( z\in E\); étant donné que \( f\) est bijective nous pouvons considérer l'élément \( f^{-1}(z)\in E\) et calculer
    \begin{subequations}
        \begin{align}
            b\big( f(x+y),z \big)&=b\big( f(x+y),f(f^{-1}(z)) \big)\\
            &=b(x+y,f^{-1}(z))\\
            &=b(x,f^{-1}(z))+b(y,f^{-1}(z))\\
            &=b(f(x),z)+b(f(y),z)\\
            &=b\big( f(x)+f(y),z \big),
        \end{align}
    \end{subequations}
    donc \( f(x+y)=f(x)+f(y)\) par le lemme~\ref{LemyKJpVP}.

    De la même façon on trouve \( b\big( f(\lambda x),z \big)=b\big( \lambda f(x),z \big)\) qui prouve que \( f(\lambda x)=\lambda f(x)\) et donc que \( f\) est linéaire.
\end{proof}

\begin{example}
    Une isométrie peut ne pas être linéaire quand la forme bilinéaire est dégénérée. Par exemple pour la forme bilinéaire sur \( \eR^2\) donnée par
    \begin{equation}
        b\big( (a,b),(x,y) \big)=ax,
    \end{equation}
    nous pouvons faire
    \begin{equation}
        f(x,y)=\begin{pmatrix}
            x    \\ 
            \lambda(x,y)    
        \end{pmatrix}
    \end{equation}
    où \( \lambda\) est n'importe quoi.
\end{example}

\ifbool{isGiulietta}{
\begin{remark}
    Des preuves alternatives.
    \begin{enumerate}
        \item
            En utilisant un peut plus d'indices et un peu plus de mots comme «tenseurs», peut être trouvée dans \cite{BIBooMBAGooNCUaMT}. Le fait que la preuve donnée soit tensorielle me fait penser que le résultat peut encore être généralisé.
        \item
            Et encore une autre preuve, utilisant des techniques de groupes de Lie sera la proposition~\ref{PROPooDVIWooAFDNPy}.
    \end{enumerate}
\end{remark}
}
{}

\begin{theorem}
    Soit un espace vectoriel \( E\) muni d'une forme quadratique \( q\). Soit une isométrie \( f\colon E\to E\) pour \( q\). Alors
    \begin{enumerate}
        \item
            si \( f(0)=0\), alors \( f\) est linéaire;
        \item
            si \( f(0)\neq 0\) alors \( f\) est affine\footnote{Définition \ref{DEFooUAWZooXcMKve}.}.
            
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous considérons la forme bilinéaire associée \( b\). Si \( f(0)=0\), nous savons par le lemme~\ref{LemewGJmM} que \( b\big( f(x),f(y) \big)=b(x,y)\). La proposition \ref{ThoDsFErq} nous dit alors que \( f\) est linéaire.


    Si \( f(0)\neq 0\), alors nous posons \( g(x)=f(x)-f(0)\) qui vérifie \( g(0)=0\) et
    \begin{equation}
        q\big( g(x)-g(y) \big)=q\big( f(x)-f(0)-f(y)+f(0) \big)=q(x-y).
    \end{equation}
    Nous pouvons donc appliquer le premier point à \( g\), déduire que \( g\) est linéaire et donc que \( f\) est affine. C'est la caractérisation du lemme \ref{LEMooZZAIooOMiayy} des fonctions affines.
\end{proof}

Nous pouvons maintenant particulariser tout cela au cas de \( \eR^n\) muni du produit scalaire usuel et de la norme associée pour voir quel résultat nous avons à peine prouvé.

\begin{lemma}[\cite{ooYPVPooYGSlNU}]        \label{LEMooJPYZooHETCqt}
    Une isométrie d'un espace vectoriel normé de dimension finie est bijective.
\end{lemma}

\begin{proof}
    Si \( f\colon E\to E\) est une isométrie, elle est linéaire par le théorème~\ref{ThoDsFErq}. Elle vérifie également \( \| f(x) \|=\| x \|\), et donc \( f(x)=0\) si et seulement si \( x=0\), c'est-à-dire que \( f\) est injective. Elle est alors bijective par le corolaire~\ref{CORooCCXHooALmxKk} du théorème du rang.
\end{proof}

Nous notons ici \( T(n)\) le groupe des translations sur \( \eR^n\). Un élément de \( T(n)\) est une translation \( \tau_v\) donnée par un vecteur \( v\) et agissant sur \( \eR^n\) par
\begin{equation}
    \begin{aligned}
        \tau_v\colon \eR^n&\to \eR^{n} \\
        x&\mapsto x+v.
    \end{aligned}
\end{equation}
Ce groupe est isomorphe au groupe abélien \( (\eR^n,+)\), et nous allons souvent identifier \( \tau_v\) à \( v\).

Vous savez par culture générale que les isométries de \( \eR^n\) pour le produit scalaire usuel sont les matrices orthogonales. En voici une petite généralisation (pensez à \( \eta=\mtu\) dans le cas du produit scalaire usuel).
\begin{proposition}     \label{PROPooSYQMooEnZFdp}
    Soit une forme bilinéaire \( b\) sur \( \eR^n\) de matrice symétrique \( \eta\). Si \( A\) est la matrice d'une application linéaire \( \eR^n\to \eR^n\) telle que
    \begin{equation}
        b(Ax,Ay)=b(x,y)
    \end{equation}
    pour tout \( x,y\in\eR^n\), alors
    \begin{equation}
        A^t\eta A=\eta.
    \end{equation}
\end{proposition}

\begin{proof}
    En suivant la formule générale \eqref{EQooQFMWooVKVLMx},
    \begin{equation}
            b(Ax,Ay)=\sum_{ij} \eta_{ij} (Ax)_i(Ay)_j=\sum_{ijkl}\eta_{ij}A_{ik}A_{jl}x_ky_l.
    \end{equation}
    En imposant que ce soit égal à \( \sum_{kl}\eta_{kl}\eta_{kl}x_ky_l\) pour tout \( x,y\) nous avons la contrainte
    \begin{equation}
        \sum_{ij}\eta_{ij}A_{ik}A_{jl}=\eta_{kl}
    \end{equation}
    qui signifie exactement \( A^t\eta A=\eta\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Signature, théorème de Sylvester}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Signature\cite{BIBooXOWGooAPWTfT}]       \label{DEFooWDCLooDkRYLK}
    Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( Q\) sur un espace vectoriel \( E\) de dimension finie \( n\). L'\defe{indice d'inertie}{indice d'inertie} de \( q\) est le nombre
    \begin{equation}
        q=\max\{ \dim(F)\tq Q(v)<0\,\forall v\in F\setminus\{ 0 \} \}.
    \end{equation}
    Nous définissons aussi
    \begin{equation}
        p=\max\{ \dim(G)\tq Q(v)>0\,\forall v\in G\setminus\{ 0 \} \}.
    \end{equation}
    Le couple \( (p,q)\) est la \defe{signature}{signature!forme quadratique} de \( Q\).
\end{definition}

\begin{definition}[Rang d'une forme quadratique]        \label{DEFooVITQooQaMaTF}
    Si \( Q\colon E\times E\to \eK\) est une forme quadratique, nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f_Q\colon E&\to E^* \\
            x&\mapsto \big[ y\mapsto B(x,y) \big]. 
        \end{aligned}
    \end{equation}
    Le \defe{rang}{rang d'une forme quadratique} de \( Q\) est le rang de l'application linéaire \( f_Q\).
\end{definition}

\begin{proposition}     \label{PROPooLRZQooSfprff}
    Le rang d'une forme quadratique est le rang de sa matrice dans n'importe quelle base.
\end{proposition}

\begin{proof}
    Nous considérons une forme quadratique \( Q\) sur l'espace vectoriel \( E\). Sa trace est, par définition, la trace de l'application linéaire \( f_Q\) de la définition \ref{DEFooVITQooQaMaTF}. Or cette dernière trace ne dépend pas des bases choisies sur \( E\) et \( E^*\). Nous la calculons donc maintenant.

    Soit une base \( \{ e_i \}\) de \( E\) ainsi que sa base duale \( \{ e_i^* \}\) de \( E^*\). Si \( v=\sum_kv_ke_k\in E\), alors
    \begin{equation}
        f_Q(e_i)v=\sum_kv_kB(e_i,e_k)=\sum_kQ_{ik}v_k
    \end{equation}
    où nous avons noté \( B\) la forme bilinéaire associée à \( Q\) et utilisé la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à la forme quadratique \( Q\). Nous avons donc \( f_Q(ei)=\sum_kQ_{ik}e_k^*\) ou encore
    \begin{equation}
        f_Q(e_i)_k=Q_{ik},
    \end{equation}
    ce qui signifie, par \ref{ITEMooKZYYooZPTkpq} que la matrice associée à \( f_Q\) est la matrice \( Q^t\).

    Le rang de \( f_Q\) est donc celui de \( Q^t\), qui est le même que celui de la matrice \( Q\) (ici, nous avons noté \( Q\) la matrice de la forme quadratique \( Q\)). Le rang de \( f_Q\) est celui de sa matrice par la proposition \ref{PROPooCINLooFGNtwS}.
\end{proof}


\begin{lemma}[\cite{BIBooXOWGooAPWTfT}]     \label{LEMooISHCooVDJEKo}
    Soient une forme quadratique \( Q\) ainsi que deux bases \( Q\)-orthogonales \( \{ e_1,\ldots, e_n \}\) et \( \{ e'_1,\ldots, e'_n \}\). Nous posons
    \begin{subequations}
        \begin{align}
            r&=\Card\{ e_i\tq q(e_i)>0 \}\\
            r'&=\Card\{ e_i\tq q(e'_i)>0 \}\\
            s&=\Card\{ e_i\tq q(e_i)<0 \}\\
            s'&=\Card\{ e_i\tq q(e'_i)<0 \}
        \end{align}
    \end{subequations}
    Alors \( r=r'\) et \( s=s'\).
\end{lemma}

\begin{proof}
    Nous posons    
    \begin{subequations}
        \begin{align}
            I&=\{ i\tq Q(e_i)>0 \}\\
            J&=\{ j\tq Q(e_j)<0 \}
        \end{align}
    \end{subequations}
    Nous commençons par prouver que \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre. Supposons pour cela que
    \begin{equation}
        \sum_{i\in I}x_ie_i+\sum_{j\in J}y_je'_j=0,
    \end{equation}
    et posons \( z=\sum_{i\in I}x_ie_i\). Nous avons
    \begin{equation}        \label{EQooWGKAooElpETd}
        Q(z)=\sum_{i\in I}x_i^2Q(e_i)\geq 0.
    \end{equation}
    Mais nous avons aussi \( z=-\sum_{j\in J}y_je'_j\), donc
    \begin{equation}        \label{EQooJYOCooZPXmTf}
        Q(z)=\sum_{j\in J}y_j^2Q(e'_j)\leq 0.
    \end{equation}
    Donc \( Q(z)=0\). Vu \eqref{EQooWGKAooElpETd}, et le fait que \( Q(e_i)>0\), avoir \( Q(z)=0\) impose \( x_i=0\) pour tout \( i\). La relation \eqref{EQooJYOCooZPXmTf} nous donne aussi immédiatement que les \( y_j\) sont nuls. Donc la partie \( \{ e_i \}_{i\in I}\cup\{ e'_j \}_{j\in J}\) est libre.

    Le lemme \ref{LemytHnlD} nous indique qu'une partie libre est toujours de cardinal plus petit ou égal à la dimension de l'espace\footnote{Ici nous utilisons l'hypothèse que \( V\) est de dimension finie.}. Tout ça pour dire que
    \begin{equation}
        \underbrace{\Card(I)}_{=r}+\underbrace{\Card(J)}_{=n-r'}\leq n,
    \end{equation}
    et donc \( r\leq r'\). 

    Le même raisonnement, en partant de \( I=\{ i\tq Q(e_i)\leq 0 \}\) et de \( J=\{ j\tq Q(e'_j)>0 \}\), prouve que \( r'\leq r\).

    La preuve de \( s=s'\) est du même tonneau.
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooOQIDooPSOeXL}
    Soit une forme quadratique \( Q\) sur \( E\). Si \( F\) est un sous-espace de \( E\), alors
    \begin{equation}
        \dim(F)+\dim(F^{\perp})\geq n
    \end{equation}
    où \( F^{\perp}\) est l'orthogonal par rapport à \( Q\).
\end{lemma}

\begin{proof}
    Nous posons \( p=\dim(F)\). Nous considérons une base \( \{ f_i \}_{i=1,\ldots, n}\) de \( E\) telle que \( \{ f_i \}_{i=1,\ldots, p}\) est une base de \( F\)\footnote{Théorème de la base incomplète, \ref{THOooOQLQooHqEeDK}.}. Nous posons
    \begin{equation}
        \begin{aligned}
            \phi\colon E&\to F \\
            x&\mapsto \sum_{i=1}^pB(x,f_i)f_i 
        \end{aligned}
    \end{equation}
    où \( B\) est la forme bilinéaire associée à \( Q\). Ce \( \phi\) est une application linéaire à qui nous appliquons le théorème du rang \eqref{EQooUEOQooLySRiE} :
    \begin{equation}        \label{EQooCLWLooCFxVDq}
        \dim(E)=\rang(\phi)+\dim\big( \ker(\phi) \big).
    \end{equation}
    Mais vu que l'image de \( \phi\) est dans \( F\), nous avons \( \rang(\phi)\leq \dim(F)\). De plus, \( \ker(\phi)=F^{\perp}\). Donc \eqref{EQooCLWLooCFxVDq} devient
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooTGBVooObvIHq}]     \label{LEMooUOZOooYvEcji}
    Soit un espace vectoriel \( E\) de dimension finie et un sous-espace \( F\) sur lequel la forme quadratique \( Q\) est strictement définie positive ou négative. Alors
    \begin{equation}
        E=F\oplus F^{\perp}.
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord nous montrons que \( F\cap F^{\perp}=\{ 0 \}\). Si \( v\neq 0\) est dans \( F\), alors \( Q(v)>0\), et donc \( v\) n'est pas dans \( F^{\perp}\). Donc \( F\cap F^{\perp}\subset \{ 0 \}\). L'inclusion inverse est immédiate.

    Nous avons vu dans le lemme \ref{LEMooOQIDooPSOeXL} que
    \begin{equation}
        \dim(E)\leq \dim(F)+\dim(F^{\perp}).
    \end{equation}
    Vu que \( F\) et \( F^{\perp}\) n'ont pas d'intersection autre que \( \{ 0 \}\), nous avons
    \begin{equation}
        \dim(E)\geq\dim(F\oplus F^{\perp}) = \dim(F)+\dim(F^{\perp}) \geq\dim(E).
    \end{equation}
    Toutes ces inégalités sont donc des égalités et \( \dim(E)=\dim(F)+\dim(F^{\perp})\).
\end{proof}

\begin{theorem}[de Sylvester\cite{BIBooXOWGooAPWTfT}]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature\footnote{Définition \ref{DEFooWDCLooDkRYLK}.} \( (p,q)\). Alors pour toute base \( Q\)-orthogonale \( \{ e_i \}\) de \( \eR^{p+q}\) nous avons les propriétés suivantes.
    \begin{enumerate}
        \item       \label{ITEMooCFQHooRWfmpT}
            Les nombres \( p\) et \( q\) sont donnée par 
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}             \label{SUBEQooONWLooNsgmQY}   \\
            q&=\Card\{ i\tq Q(e_i)<0 \}.        \label{SUBEQooFKXMooOVwvKR}
        \end{align}
    \end{subequations}
\item       \label{ITEMooWLPVooSTOOjL}
    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\item       \label{ITEMooGOHCooPrNQwm}
    Le rang de \( Q\) est \( p+q\).
    \end{enumerate}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}

\begin{proof}
    Soit  $F$ un sous-espace de dimension maximale $q$ sur lequel $Q$ est définie négative. Le fait que la dimension de \( F\) soit \( q\) est la définition \ref{DEFooWDCLooDkRYLK} de la signature. Nous notons \( F^{\perp}\) sont \( Q\)-orthogonal, c'est à dire que
    \begin{equation}
        F^{\perp}=\{ v\in E\tq B(v,x)=0\,\forall x\in F \}.
    \end{equation}
    Le lemme \ref{LEMooUOZOooYvEcji} nous assure que \( E=F\oplus F^{\perp}\).

    Le théorème \ref{THOooIDMPooIMwkqB} sur l'existence de bases \( Q\)-orthogonales nous permet de considérer une base \( Q\)-orthogonale de \( F\) et une de \( F^{\perp}\). En réunissant les deux, nous avons une base de \( E\). Nous la notons \( \{ f_1,\ldots, f_n \}\) avec
    \begin{itemize}
        \item La partie \( \{ f_1,\ldots, f_q \}\) est une base de \( F\),
        \item La partie \( \{ f_{q+1},\ldots, f_n \}\) est une base de \( F^{\perp}\),
        \item Remarquez cependant qu'il n'est pas dit que \( n=q+p\).
    \end{itemize}
    Notons que pour \( i>q\), nous avons \( Q(f_i)\geq 0\), sinon la maximalité de \( F\) serait contredite par \( \Span\{ f_1,\ldots, f_q,f_i \}\).

    Cela prouve que 
    \begin{equation}
        \Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    Le lemme \ref{LEMooISHCooVDJEKo} nous dit alors que
    \begin{equation}
        \Card\{ i\tq Q(e_i)>0 \}=\Card\{ i\tq Q(f_i) >0\}=p.
    \end{equation}
    C'est l'égalité \eqref{SUBEQooFKXMooOVwvKR}. L'égalité \eqref{SUBEQooONWLooNsgmQY} se prouve de la même façon, en prenant \( F\) maximal pour la propriété que \( Q\) y est strictement définie positive.

    Le point \ref{ITEMooCFQHooRWfmpT} est prouvé.

    Dans une base \( Q\)-orthogonale, la matrice de \( Q\) est diagonale, et contient sur la diagonale les valeurs de \( Q(e_i)\). Parmi celles-ci, on en a \( p\) strictement positives et \( q\) strictement négatives. Les \( n-p-q\) autres sont nulles. Vu que \( Q\) est à valeur réelle, nous avons une notion de racine carré, et nous pouvons considérer \( e_i/\sqrt{ | Q(e_i) | }\) au lieu de \( e_i\). De cette façon, \( Q(e_i)\) est normalisé. Avec ça, la matrice de \( Q\) est
    \begin{equation}        \label{EQooLQNRooCsgKVF}
        D=\begin{pmatrix}
            \mtu_p    &       &       \\
                &   -\mtu_q    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
    Nous venons de prouver qu'il existe une base \( \{ e_i \}\) dans laquelle la matrice de \( Q\) est \eqref{EQooLQNRooCsgKVF}. Si \( A\) est la matrice de \( Q\) dans une base quelconque \( \{ f_i \}\) et si \( P\) est la matrice de changement de base \( f_j=\sum_iP_{ij}e_i\), la proposition \ref{PROPooLBIOooUpzxXA} donne \(D= P^tAP\).

    Le point \ref{ITEMooWLPVooSTOOjL} est prouvé.

    Pour \ref{ITEMooGOHCooPrNQwm}, la proposition \ref{PROPooLRZQooSfprff} nous permet de calculer le rang de \( Q\) par le rang de sa matrice dans n'importe quelle base. Nous choisissons la base qui donne la matrice \eqref{EQooLQNRooCsgKVF}. Le rang est alors bien \( p+q\).
\end{proof}


\begin{definition}[Équivalence de forme quadratique\cite{BIBooWVWZooZqliJt}]        \label{DEFooOLWYooMwhMJp}
    Deux formes quadratiques $Q$ et $Q'$ sont \defe{équivalentes}{équivalence de forme quadratiques} si il existe une application linéaire inversible \( \phi\) telle que \( Q'=Q\circ \phi\).
\end{definition}

\begin{proposition}[\cite{BIBooXOWGooAPWTfT}]       \label{PROPooBWXMooLsgyKm}
    Deux formes quadratiques sont équivalentes\footnote{Définition \ref{DEFooOLWYooMwhMJp}.} si et seulement si elles ont même signature.
\end{proposition}

\index{matrice!semblables}
\index{forme!quadratique}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Invariance de la trace}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRMYQooWkEpJJ}
    Soit une application linéaire \( f\). Si la matrice de \( f\) dans une base est \( A\) et est \( B\) dans une autre base, alors
    \begin{equation}
        \trace(A)=\trace(B).
    \end{equation}
\end{proposition}

\begin{proof}
    Les matrices \( A\) et \( B\) sont liées par la proposition \ref{PROPooNZBEooWyCXTw} : \( B=Q^{-1}AQ\) où \( Q\) est la matrice qui lie les vecteurs des deux bases. L'invariance cyclique de la trace donnée en le lemme \ref{LEMooUXDRooWZbMVN} implique que
    \begin{equation}
        \trace(B)=\trace(Q^{-1}AQ)=\trace(QQ^{-1}A)=\trace(A).
    \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit scalaire, produit hermitien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}\label{DefVJIeTFj}
    Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel réel est une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition~\ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

La définition suivante est utile pour celles qui veulent faire de la relativité\footnote{Voir le théorème \ref{THOooYHDWooWxVovH} qui établit les transformations de Lorentz.}.
\begin{definition}      \label{DEFooLPBGooXLxubc}
    Un \defe{produit pseudo-scalaire}{produit pseudo-scalaire} sur un espace vectoriel réel est une forme bilinéaire et symétrique.
\end{definition}

Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj} 
    Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel de dimension finie muni d'un produit scalaire (définition~\ref{DefVJIeTFj}).
\end{definition}
Avouez que c'est drôle qu'un espace vectoriel est euclidien lorsqu'il possède une \emph{multiplication} alors qu'un anneau est euclidien lorsqu'il possède une \emph{division} (voir la définition~\ref{DefAXitWRL}). C'est pas très profond, mais si ça peut vous servir de moyen mnémotechnique\ldots

\begin{definition}[\cite{ooJUXBooVrwvfP}]  \label{DefMZQxmQ}
    Soit \( E\) est un espace vectoriel sur \( \eC\). Une application \( \langle ., .\rangle \colon E\times E\to \eC\) est \defe{sesquilinéaire à droite}{sesquilinéaire} si pour tout \( x,y\in E\) et pour tout \( \lambda\in \eC\),
    \begin{enumerate}
        \item
            \( \langle \lambda x, y\rangle =\lambda\langle x,y, \rangle =\langle x, \bar\lambda y\rangle \),
        \item
            \( \langle x+y, z\rangle =\langle x, y\rangle+\langle y, z\rangle  \),
        \item
            \( \langle x, y+z\rangle =\langle x, y\rangle +\langle x, z\rangle \).
    \end{enumerate}
    Cette forme est \defe{hermitienne}{hermitienne} si de plus
    \begin{equation}
        \langle x, y\rangle =\overline{ \langle y, x\rangle  }.
    \end{equation}
    Un \defe{produit hermitien}{produit hermitien} est une forme hermitienne strictement définie positive, c'est-à-dire telle que \( \langle x, x\rangle \geq 0\) pour tout \( x\in E\) et \( \langle x, x\rangle =0\) si et seulement si \( x=0\).
\end{definition}

\begin{example}
    L'ensemble \( E=\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\)  est muni d'une forme sesquilinéaire
    \begin{equation}    \label{EqFormSesqQrjyPH}
        \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
    \end{equation}
    pour tout \( x,y\in\eC^n\). Cela est un espace vectoriel hermitien.
\end{example}


La proposition suivante est une version plus «pragmatique» de la proposition \ref{PropXrTDIi}.
\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooNITTooCYcrrT}
    Soient un espace euclidien\footnote{Qui possède un produit scalaire, définition \ref{DefLZMcvfj}.} de dimension finie \( V\) ainsi qu'un sous-espace \( M\). Nous posons
    \begin{equation}
        M^{\perp}=\{ x\in V\tq x\cdot y=0\forall y\in M \}.
    \end{equation}
    Alors \( M\oplus M^{\perp}=V\).
\end{proposition}

\begin{proof}
    D'abord si \( x\in M\cap M^{\perp}\), alors \( x\cdot x=0\) et donc \( x=0\). Donc nous avons déjà \( M\cap M^{\perp}=\{ 0 \}\). Nous considérons une base \( \{b_1,\ldots, b_k\}\) de \( M\), et nous définissons l'application linéaire
    \begin{equation}
        \begin{aligned}
            f\colon V&\to \eR^k \\
            x&\mapsto (x\cdot b_1,\ldots, x\cdot b_k). 
        \end{aligned}
    \end{equation}
    Nous avons que \( M^{\perp}=\ker(f)\). Le théorème du rang \ref{ThoGkkffA} nous indique que
    \begin{equation}
        \dim(V)=\dim\big( \ker(f) \big)+\dim\big( \Image(f) \big)\leq \dim(M^{\perp})+k=\dim(M^{\perp})+\dim(M).
    \end{equation}
    Une justification : vu que \( f\) prend ses valeurs dans \( \eR^k\), la dimension de son image est majorée par \( k\).

    Nous en déduisons que 
    \begin{equation}
        \dim(M)+\dim(M^{\perp})\geq\dim(V),
    \end{equation}
    et la proposition \ref{PROPooCASNooEqisqa} nous permet de conclure que \( M\oplus M^{\perp}=V\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ellipsoïde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemYVWoohcjIX}
    Toute matrice peut être décomposée de façon unique en une partie symétrique et une partie antisymétrique. Cette décomposition est donnée par
\begin{equation}\label{subEqHIQooyhiWM}
    \begin{aligned}[]
            S&=\frac{ M+M^t }{ 2 },&A&=\frac{ M-M^t }{ 2 }
    \end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
    L'existence est une vérification immédiate de \( S+A=M\) en utilisant \eqref{subEqHIQooyhiWM}. Pour l'unicité, si \( S+A=S'+A'\) alors \( S-S'=A-A'\). Mais \( S-S'\) est symétrique et \( A-A'\) est antisymétrique; l'égalité implique l'annulation des deux membres, c'est-à-dire \( S=S'\) et \( A=A'\).
\end{proof}

\begin{definition}  \label{DefOEPooqfXsE}
    Un \defe{ellipsoïde}{ellipsoïde} dans \( \eR^n\) centré en \( v\) est le lieu des points \( x\) vérifiant l'équation
    \begin{equation}\label{EqSNWooXfbTH}
        \langle x-v, M(x-v)\rangle =1
    \end{equation}
    où \( M\) est une matrice symétrique strictement définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.}.

    Lorsque nous parlons d'ellipsoïde \emph{plein}, il suffit de changer l'égalité en une inégalité.
\end{definition}

\begin{remark}
    Le fait que \( M\) soit symétrique n'est pas tout à fait obligatoire; la chose important est que toutes les valeurs propres soient strictement positives. En effet si \( M\) a toutes ses valeurs propres strictement positives, nous nommons \( S\) la partie symétrique de \( M\) et \( A\) la partie antisymétrique (lemme~\ref{LemYVWoohcjIX}). Alors pour tout \( x\in \eR^n\) nous avons
    \begin{equation}
        x^tAx=\langle x, Ax\rangle =\langle A^tx,x \rangle =-\langle Ax, x\rangle =-\langle x,Ax\rangle ,
    \end{equation}
    donc \( x^tAx=0\). L'équation \( x^tMx=1\) est donc équivalente à \( x^tSx=1\) (elles ont les mêmes solutions).

    De plus \( S\) reste strictement définie positive parce que pour tout \( x\in \eR^n\) nous avons
    \begin{equation}
        0<x^tMx=x^tSx.
    \end{equation}
\end{remark}

\begin{proposition}\label{PropWDRooQdJiIr}
    Si \( \ellE\) est un ellipsoïde centrée à l'origine, il existe une base de \( \eR^n\) dans laquelle son équation est :
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }=1.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous avons une matrice symétrique strictement définie positive \( S\) telle que l'équation soit \( \langle x, Sx\rangle =1\). Le théorème spectral~\ref{ThoeTMXla} nous fournit une base orthonormale \( \{ e_i \}\) dans laquelle \( Se_i=\lambda_ie_i\) avec \( \lambda_i>0\). En substituant dans l'équation \( \langle x, Sx\rangle =1\) nous trouvons l'équation
    \begin{equation}
        \sum_i\lambda_ix_i^2=1.
    \end{equation}
    En posant \( a_i=\frac{1}{ \sqrt{\lambda_i} }\), nous trouvons le résultat.  Cette définition des \( a_i\) est toujours possible parce que \( \lambda_i>0\).
\end{proof}

\begin{corollary}   \label{CorKGJooOmcBzh}
    Un ellipsoïde plein centré en l'origine admet une équation de la forme \( q(x)\leq 1\) où \( q\) est une forme quadratique strictement définie positive.
\end{corollary}
Pour rappel de notation, l'ensemble des formes quadratiques strictement définies positives sur l'espace vectoriel \( E\) est noté \( Q^{++}(E)\).

\begin{proof}
    Soit \( \{ e_i \}\) une base de \( \eR^n\) telle que l'ellipsoïde \( \ellE\) ait pour équation
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }\leq 1.
    \end{equation}
    Nous considérons la forme quadratique
    \begin{equation}
        \begin{aligned}
            q\colon \eR^n&\to \eR \\
            x&\mapsto \sum_{i=1}^n\frac{ \langle x, e_i\rangle^2 }{ a_i^2 }.
        \end{aligned}
    \end{equation}
    Nous avons évidemment \( \ellE=\{ x\in \eR^n\tq q(x)\leq 1 \}\). De plus la forme \( q\) est strictement définie positive parce que dès que \( x\neq 0\), au moins un des produits scalaires \( \langle x, e_i\rangle \) est non nul et \( q(x)> 0\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Système d'équations linéaires : méthode de Gauss}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% TODO: Ajouter un texte sur les équations de plan, et pourquoi ax+by+cz+d=0 est perpendiculaire au vecteur (a,b,c).

Pour résoudre un système d'équations linéaires, on procède comme suit:
\begin{enumerate}
\item Écrire le système sous forme matricielle. \[\text{p.ex. } \begin{cases} 2x+3y &= 5 \\ x+2y &= 4 \end{cases} \Leftrightarrow \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right) \]
\item Se ramener à une matrice avec un maximum de $0$ dans la partie de gauche en utilisant les transformations admissibles:
\begin{enumerate}
\item Remplacer une ligne par elle-même + un multiple d'une autre;
\[\text{p.ex. } \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{L_1  - 2. L_2 \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Remplacer une ligne par un multiple d'elle-même;
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{-L_1  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Permuter des lignes.
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 0 & -2 \end{array}\right)  \stackrel{L_1  \mapsto L_2' \text{ et } L_2  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \]
\end{enumerate}
\item Retransformer la matrice obtenue en système d'équations.
\[\text{p.ex. }  \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \Leftrightarrow \begin{cases} x &= -2 \\ y &= 3 \end{cases}  \]
\end{enumerate}

\begin{remark}
\begin{itemize}
\item Si on obtient une ligne de zéros, on peut l'enlever:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \Leftrightarrow  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \end{array}\right) \]
\item Si on obtient une ligne de zéros suivie d'un nombre non-nul, le système d'équations n'a pas de solution:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 7 \end{array}\right) \Leftrightarrow  \begin{cases} \cdots \\ \cdots \\ 0x + 0y + 0z = 7 \end{cases} \Rightarrow \textbf{Impossible} \]
\item Si on a moins d'équations que d'inconnues, alors il y a une infinité de solutions qui dépendent d'un ou plusieurs paramètres:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 1 & 0 & -2 & 2 \\ 0 & 1 & 3 & 0 \end{array}\right) \Leftrightarrow  \begin{cases} x - 2z = 2 \\ y + 3z = 0 \end{cases} \Leftrightarrow  \begin{cases} x = 2 + 2\lambda \\ y = -3\lambda \\ z = \lambda \end{cases} \]
\end{itemize}
\end{remark}
