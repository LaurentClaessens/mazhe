% This is part of Mes notes de mathématique
% Copyright (c) 2011-2022, 2025
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Hyperplans et formes linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{propositionDef}[\cite{MonCerveau,BIBooXYPGooIYVgoZ,BIBooXKSMooJHzoRo}]      \label{DEFooEWDTooQbUQws}
	Soient un espace vectoriel \( E\) sur \( \eK\), et un sous-espace vectoriel \( H\). Il y a équivalence entre
	\begin{enumerate}
		\item		\label{ITEMooUCWQooToKrop}
		      \( E\) possède une droite vectorielle supplémentaire\footnote{Définition \ref{PROPooRKOVooBFRCKq}.} à \( H\),
		\item		\label{ITEMooXVIPooNxijzA}
		      Il existe une forme linéaire non nulle \(f \colon E\to \eK  \) telle que \( H=\ker(f)\).
	\end{enumerate}
	Quand ces conditions sont vérifiées, nous disons que \( H\) est un \defe{hyperplan}{hyperplan} de \( E\).
\end{propositionDef}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\ref{ITEMooUCWQooToKrop} \( \Rightarrow\) \ref{ITEMooXVIPooNxijzA}]
		%-----------------------------------------------------------
		Soit \( V\) une droite vectorielle supplémentaire de \( H\) : pour tout \( x\in E\), il existe des uniques \( x_H\in H\) et \( x_V\in V\) tels que \( x=x_H+x_V\). Si \( \{ e_i \}_{i\in I}\) est une base de \( H\), nous prouvons que \( B=\{ e_i \}_{i\in I}\cup \{ v \}\) est une base de \( E\). Le fait que \( B\) soit générateur est immédiat. Pour que ce soit libre, nous supposons que \( \sum_{i\in I}a_ie_i+bv=0\) (\( a_i\neq 0\) que pour une partie finie de \( I\)).

		Le vecteur \( 0\) s'écrit dans la décomposition \( E=H\oplus V\) de façon unique \( 0=0+0\). Nous avons donc \( \sum_ia_ie_i=0\) et \( b=0\). Vu que \( \{ e_i \}_{i\in I}\) est libre, nous en déduisons également \( a_i=0\) pour tout \( i\).

		Maintenant nous posons
		\begin{equation}
			\begin{aligned}
				f\colon E & \to \eK    \\
				x_H+bv    & \mapsto b.
			\end{aligned}
		\end{equation}
		C'est une application linéaire et \( H=\ker(f)\).
		\spitem[\ref{ITEMooXVIPooNxijzA} \( \Rightarrow\) \ref{ITEMooUCWQooToKrop}]
		%-----------------------------------------------------------
		Nous considérons une application linéaire \(f \colon E\to \eK  \) et \( H=\ker(f)\). Vu que \( f\neq 0\), nous pouvons considérer un élément \( v\in E\) tel que \( f(v)\neq 0\). Par linéarité nous avons aussi \( \Image(f)=\eK\). Donc \( \{ f(v) \}\) est une base de \( \Image(f)\).

		Nous sortons maintenant le théorème du rang \ref{ThoGkkffA}. Si \( B\) est une base de \( H\), alors \( B\cup\{ v \}\) est une base de \( E\). Donc \( V=\eK v\) est un supplémentaire de \( H\) dans \( E\).
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooYPBYooXJaYwX}]	\label{PROPooUZZDooPAKmvN}
	Soit un espace vectoriel de dimension finie \( n\). Une partie est un hyperplan si et seulement si c'est un sous-espace vectoriel de dimension \( n-1\).
	%TODOooAZPVooSDwoNk. Prouver ça.
\end{proposition}


\begin{proposition}[\cite{ooDSTAooKgSyCN}]
	Soit un espace vectoriel \( E\) de dimension finie \( n\geq 2\). Soit un sous-espace vectoriel \( V\) de \( E\) de dimension \( s\). Alors \( V\) est une intersection de \( n-s\) hyperplans de \( E\).
\end{proposition}

\begin{proof}
	Nous considérons une base de \( V\) que nous complétons\footnote{Théorème de la base incomplète, \ref{ThonmnWKs}\ref{ITEMooJIJSooGuJMdt}.} en une base de \( E\) : si \( x=\sum_{i=1}^nx_ie_i\), nous avons \( x\in V\) si et seulement si \( x_{s+1}=\ldots=x_n=0\). Nous considérons les formes linéaires
	\begin{equation}
		\begin{aligned}
			\varphi_i\colon E & \to \eR      \\
			x                 & \mapsto x_i,
		\end{aligned}
	\end{equation}
	et nous considérons les parties \( H_i=\ker(\varphi_i)\) qui sont de hyperplans (définition \ref{DEFooEWDTooQbUQws}). Les \( H_i\) avec \( s+1\leq i\leq n\) sont une famille de \( n-s\) hyperplans qui vérifient
	\begin{equation}
		V=\bigcap_{i=s+1}^n\ker(\varphi_i)
	\end{equation}
	parce que \( x\in \ker(\varphi_i)\) si et seulement si \( x_i=0\).

	Donc \( V\) peut être écrit comme intersection de \( n-s\) hyperplans de \( E\).
\end{proof}

\begin{proposition}[\cite{ooDSTAooKgSyCN}]      \label{PROPooRCLNooJpIMMl}
	Soit un \( \eK\)-espace vectoriel \( E\) de dimension finie \( n\geq 2\). Si \( H_i\) sont des hyperplans de \( E\), alors
	\begin{equation}
		\dim\Big( \bigcap_{i=1}^mH_i \Big)\geq n-m.
	\end{equation}
\end{proposition}

\begin{proof}
	N'oubliez pas de prouver que \( \bigcap_{i=1}^mH_i\) est un espace vectoriel. À part ça, nous faisons une petite récurrence.
	\begin{subproof}
		\spitem[Pour \( m=2\)]
		Nous savons déjà par la proposition \ref{PROPooQCIXooHIyPPq} que
		\begin{equation}
			\dim(H_1 + H_2)=\dim(H_1)+\dim(H_2)-\dim(H_1\cap H_2).
		\end{equation}
		De plus \( \dim(H_1+H_2)\leq n\). En remplaçant, par les valeurs,
		\begin{subequations}
			\begin{align}
				\dim(H_1\cap H_2) & =\dim(H_1)+\dim(H_2)-\dim(H_1+ H_2) \\
				                  & =n-1+n-1-\dim(H_1+H_2)              \\
				                  & \geq 2n-2-n                         \\
				                  & =n-2.
			\end{align}
		\end{subequations}
		Donc \( \dim(H_1\cap H_2)\geq n-2\).

		\spitem[La récurrence]
		Nous calculons \( \dim(H_1\cap\ldots\cap H_m\cap H_{m+1})\) en commençant encore par la proposition \ref{PROPooQCIXooHIyPPq} :
		\begin{subequations}
			\begin{align}
				\dim(H_1\cap \ldots\cap H_m\cap H_{m+1}) & =\underbrace{\dim(H_1\cap\ldots\cap H_m)}_{\leq n-m}+\dim(H_{m+1})        \\
				                                         & \qquad -\underbrace{\dim\big( (H_1\cap\ldots H_m)+H_{m+1} \big)}_{\leq n} \\
				                                         & \geq n-m+(n-1)-n                                                          \\
				                                         & =n-m-1.
			\end{align}
		\end{subequations}
		C'est bon pour la récurrence.
	\end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Trouver la matrice d'une symétrie donnée}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMtrSym}

Les notions de déterminants, produit scalaire et vectoriels\footnote{Définitions~\ref{LEMooQTRVooAKzucd},~\ref{DefVJIeTFj} et~\ref{DEFooTNTNooRjhuJZ}.} donnent une bonne intuition géométrique des matrices. Nous pouvons alors chercher les matrices de quelques symétries dans \( \eR^2\) ou \( \eR^3\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à un plan}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Comment trouver par exemple la matrice \( A\) qui donne la symétrie autour du plan \( z=0\) ? La définition d'une telle symétrie est que les vecteurs du plan \( z=0\) ne bougent pas, tandis que les vecteurs perpendiculaires changent de signe. Ces informations vont permettre de trouver comment \( A\) agit sur une base de \( \eR^3\). En effet :
\begin{enumerate}

	\item
	      Le vecteur \( \begin{pmatrix}
		      1 \\
		      0 \\
		      0
	      \end{pmatrix}\) est dans le plan \( z=0\), donc il ne bouge pas,

	\item
	      le vecteur \( \begin{pmatrix}
		      0 \\
		      1 \\
		      0
	      \end{pmatrix}\) est également dans le plan, donc il ne bouge pas non plus,

	\item
	      et le vecteur \( \begin{pmatrix}
		      0 \\
		      0 \\
		      1
	      \end{pmatrix}\) est perpendiculaire au plan \( z=0\), donc il va changer de signe.

\end{enumerate}
Cela nous donne directement les valeurs de \( A\) sur la base canonique et nous permet d'écrire
\begin{equation}
	A=\begin{pmatrix}
		1 & 0 & 0  \\
		0 & 1 & 0  \\
		0 & 0 & -1
	\end{pmatrix}.
\end{equation}
Pour écrire cela, nous avons juste mis en colonne les images des vecteurs de base. Les deux premiers n'ont pas changé et le troisième a changé.

Et si maintenant on donne un plan moins facile que \( z=0\) ? Le principe reste le même : il faudra trouver deux vecteurs qui sont dans le plan (et dire qu'ils ne bougent pas), et puis un vecteur qui est perpendiculaire au plan\footnote{Pour le trouver, penser au produit vectoriel.}, et dire qu'il change de signe.

Voyons ce qu'il en est pour le plan \( x=-z\). Il faut trouver deux vecteurs linéairement indépendants dans ce plan. Prenons par exemple
\begin{equation}		\label{EqffudE}
	\begin{aligned}[]
		f_1 & =\begin{pmatrix}
			       0 \\
			       1 \\
			       0
		       \end{pmatrix}, & f_2 & =\begin{pmatrix}
			                               1 \\
			                               0 \\
			                               -1
		                               \end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons
\begin{equation}
	\begin{aligned}[]
		Af_1 & =f_1  \\
		Af_2 & =f_2.
	\end{aligned}
\end{equation}
Afin de trouver un vecteur perpendiculaire au plan, calculons le produit vectoriel :
\begin{equation}
	f_3=f_1\times f_2=\begin{vmatrix}
		e_1 & e_2 & e_3 \\
		0   & 1   & 0   \\
		1   & 0   & -1
	\end{vmatrix}=-e_1-e_3=\begin{pmatrix}
		-1 \\
		0  \\
		-1
	\end{pmatrix}.
\end{equation}
Nous avons
\begin{equation}
	Af_3=-f_3.
\end{equation}
Afin de trouver la matrice \( A\), il faut trouver \( Ae_1\), \( Ae_2\) et \( Ae_3\). Pour ce faire, il faut d'abord écrire \( \{ e_1,e_2,e_3 \}\) en fonction de \( \{ f_1,f_2,f_3 \}\). La première des équations \eqref{EqffudE} dit que
\begin{equation}
	f_1=e_2.
\end{equation}
Ensuite, nous avons
\begin{equation}
	\begin{aligned}[]
		f_2 & =e_1-e_3   \\
		f_3 & =-e_1-e_3.
	\end{aligned}
\end{equation}
La somme de ces deux équations donne \( -2e_3=f_2+f_3\), c'est-à-dire
\begin{equation}
	e_3=-\frac{ f_2+f_3 }{ 2 }
\end{equation}
Et enfin, nous avons
\begin{equation}
	e_1=\frac{ f_2-f_3 }{ 2 }.
\end{equation}

Maintenant nous pouvons calculer les images de \( e_1\), \( e_2\) et \( e_3\) en faisant
\begin{equation}
	\begin{aligned}[]
		Ae_1 & =\frac{ Af_2-Af_3 }{ 2 }=\frac{1 }{2}\begin{pmatrix}
			                                            0 \\
			                                            0 \\
			                                            -2
		                                            \end{pmatrix}=\begin{pmatrix}
			                                                          0 \\
			                                                          0 \\
			                                                          -1
		                                                          \end{pmatrix},  \\
		Ae_2 & =Af_1=f_1=\begin{pmatrix}
			                 0 \\
			                 1 \\
			                 0
		                 \end{pmatrix},                                           \\
		Ae_3 & =-\frac{ f_2-f_3 }{ 2 }=-\frac{ 1 }{2}\begin{pmatrix}
			                                             2 \\
			                                             0 \\
			                                             0
		                                             \end{pmatrix}=\begin{pmatrix}
			                                                           -1 \\
			                                                           0  \\
			                                                           0
		                                                           \end{pmatrix}.
	\end{aligned}
\end{equation}
La matrice \( A\) s'écrit maintenant en mettant les trois images trouvées en colonnes :
\begin{equation}
	A=\begin{pmatrix}
		0  & 0 & -1 \\
		0  & 1 & 0  \\
		-1 & 0 & 0
	\end{pmatrix}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à une droite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le principe est exactement le même : il faut trouver trois vecteurs \( f_1\), \( f_2\) et \( f_3\) sur lesquels on connaît l'action de la symétrie. Ensuite il faudra exprimer \( e_1\), \( e_2\) et \( e_3\) en termes de \( f_1\), \( f_2\) et \( f_3\).

Le seul problème est de trouver les trois vecteurs \( f_i\). Le premier est tout trouvé : c'est n'importe quel vecteur sur la droite. Pour les deux autres, il faut un peu ruser parce qu'il faut impérativement qu'ils soient perpendiculaire à la droite. Pour trouver \( f_2\), on peut écrire
\begin{equation}
	f_2=\begin{pmatrix}
		1 \\
		0 \\
		x
	\end{pmatrix},
\end{equation}
et puis fixer le \( x\) pour que le produit scalaire de \( f_2\) avec \( f_1\) soit nul. Si il n'y a pas moyen (genre si \( f_1\) a sa troisième composante nulle), essayer avec \( \begin{pmatrix}
	x \\
	1 \\
	0
\end{pmatrix}\). Une fois que \( f_2\) est trouvé (il y a des milliards de choix possibles), trouver \( f_3\) est super facile : prendre le produit vectoriel entre \( f_1\) et \( f_2\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{En résumé}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
La marche à suivre est

\begin{enumerate}

	\item
	      Trouver trois vecteurs \( f_1\), \( f_2\) et \( f_3\) sur lesquels on connaît l'action de la symétrie. Typiquement : des vecteurs qui sont sur l'axe ou le plan de symétrie, et puis des perpendiculaires. Pour la perpendiculaire, penser au produit scalaire et au produit vectoriel.

	\item
	      Exprimer la base canonique \( e_1\), \( e_2\) et \( e_3\) en termes de \( f_1\), \( f_2\), \( f_3\).

	\item
	      Trouver \( Ae_1\), \( Ae_2\) et \( Ae_3\) en utilisant leur expression en termes des \( f_i\), et le fait que l'on connaisse l'action de \( A\) sur les \( f_i\).

	\item
	      La matrice s'obtient en mettant les images des \( e_i\) en colonnes.
\end{enumerate}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Théorème de Burnside}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}       \label{LemwXXzIt}
	Soit \( P\), un polynôme sur \( \eK\). Une racine de \( P\) est une racine simple si et seulement si elle n'est pas racine de \( P'\).
\end{lemma}

\begin{theorem}     \label{ThoBurnsideoPuCtS}
	Toute représentation\footnote{Définition \ref{DEFooXVMSooXDIfZV}.} d'un groupe abélien d'exposant fini sur \( \eC^n\) a une image finie.
\end{theorem}

\begin{proof}
	Étant donné que \( G\) est d'exposant fini, il existe \( \alpha\in \eN^*\) tel que \( g^{\alpha}=e\) pour tout \( g\in G\). Le polynôme \( P(X)=X^{\alpha}-1\) est scindé à racines simples. En effet tout polynôme sur \( \eC\) est scindé. Le fait qu'il soit à racines simples provient du lemme~\ref{LemwXXzIt} parce que si \( a^{\alpha}=1\), alors il n'est pas possible d'avoir \( \alpha a^{\alpha-1}=0\).

	Par ailleurs \( P(g)=0\). Le fait que nous ayons un polynôme annulateur de \( g\) scindé à racines simples implique que \( g\) est diagonalisable (théorème~\ref{ThoDigLEQEXR}). Le fait que \( G\) soit abélien montre qu'il existe une base de \( \eC^n\) dans laquelle tous les éléments de \( G\) sont diagonaux. Nous devons par conséquent montrer qu'il existe un nombre fini de matrices de la forme
	\begin{equation}
		\begin{pmatrix}
			\lambda_1 &        &           \\
			          & \ddots &           \\
			          &        & \lambda_n
		\end{pmatrix}.
	\end{equation}
	Nous savons que \( \lambda_i^{\alpha}=1\) parce que \( g^{\alpha}=\mtu\), par conséquent chacun des \( \lambda_i\) est une racine de l'unité dont il n'existe qu'un nombre fini.
\end{proof}

\begin{theorem}[Burnside\cite{fJhCTE,ooFBZQooXyHIWK}]\label{ThooJLTit}
	Un sous-groupe de \( \GL(n,\eC)\) est fini si et seulement si il est d'exposant\footnote{Définition~\ref{DefvtSAyb}.} fini.
\end{theorem}
\index{exposant}
\index{racine!de l'unité}
\index{endomorphisme!diagonalisable}

\begin{proof}
	Soit \( G\) un sous-groupe de \( \GL(n,\eC)\). Si \( G\) est fini, l'ordre de ses éléments divise \( | G |\) (corolaire \ref{CorpZItFX} au théorème de Lagrange) et l'exposant est le PPCM qui est donc fini également. Le théorème est déjà démontré dans un sens.

	Dans l'autre sens, nous notons \( e<\infty\) l'exposant de \( G\), et nous allons prouver que l'ensemble \( G\) est fini. Nous commençons par remarquer que tous les éléments de \( G\) sont des racines du polynôme \( X^e-1\), et ensuite nous nous lançons dans le travail.

	\begin{subproof}
		\spitem[Générateurs]

		Le groupe \( G\) est une partie de \( \eM(n,\eC)\) dont nous considérons l'algèbre engendrée\footnote{Définition \ref{DefkAXaWY}.} \( \mG\). Soit \( C_1,\ldots, C_r\) une famille génératrice de \( \mG\) constituée d'éléments de \( G\) et la fonction
		\begin{equation}
			\begin{aligned}
				\tau\colon G & \to \eC^r                                        \\
				A            & \mapsto \big( \tr(AC_1),\ldots, \tr(AC_r) \big).
			\end{aligned}
		\end{equation}

		\spitem[\( \tau\) est injective] Soient \( A,B\in G\) tels que \( \tau(A)=\tau(B)\). Si \( C_i\) est un générateur de \( G\), nous avons \( \tr(AC_i)=\tr(BC_i)\) et par la linéarité de la trace, nous avons
		\begin{equation}    \label{EqnCYmKW}
			\tr(AM)=\tr(BM)
		\end{equation}
		pour tout \( M\in G\). Notons par ailleurs
		\begin{equation}
			N=AB^{-1}-\mtu,
		\end{equation}
		qui est diagonalisable parce que \( AB^{-1}\in G\) et donc est annulé par le polynôme \( X^e-1\) qui est scindé à racines simples. Du coup \( AB^{-1}\) est diagonalisable; posons \( PAB^{-1}P^{-1}=D\), alors \( P\big( AB^{-1}-\mtu \big)P^{-1}=D-\mtu\) qui est encore diagonale. Donc \( N\) est diagonalisable.

		Par ailleurs nous avons
		\begin{subequations}
			\begin{align}
				\tr\big( (AB^{-1})^p \big) & =\tr\big( AB^{-1}(AB^{-1})^{p-1} \big)                           \\
				                           & =\tr\big( BB^{-1}(AB^{-1})^{p-1} \big) & \text{\eqref{EqnCYmKW}} \\
				                           & =\tr\big( (AB^{-1})^{p-1} \big).
			\end{align}
		\end{subequations}
		En continuant nous obtenons
		\begin{equation}
			\tr\big(  (AB^{-1})^p \big)=\tr(\mtu)=n.
		\end{equation}

		D'autre part,
		\begin{equation}
			N^k=(AB^{-1}-\mtu)^k=\sum_{p=0}^k{p\choose k}(-1)^{k-p}(AB^{-1})^p
		\end{equation}
		En prenant la trace, et en tenant compte du fait que \( \tr\big( (AB^{-1})^p \big)=n\),
		\begin{equation}
			\tr(N^k)=\sum_{p=0}^k{p\choose k}(-1)^{k-p}n=n(1-1)^k=0.
		\end{equation}
		Donc la trace de \( N^k\) est nulle et le lemme~\ref{LemzgNOjY} nous enseigne que \( N\) est alors nilpotente. Étant donné qu'elle est aussi diagonalisable, elle est nulle. Nous en concluons que \( AB^{-1}=\mtu\) et donc que \( A=B\). La fonction \( \tau\) est donc injective.

		\spitem[Nombre fini de valeurs]

		Les éléments de \( G\) sont annulés par \( X^e-1\) qui est un polynôme scindé à racines simples. Dons le polynôme minimal d'un élément de \( G\) est (a fortiori) scindé à racines simples et le théorème~\ref{ThoDigLEQEXR} nous assure alors que ces éléments sont diagonalisables. Du coup les valeurs propres des matrices de \( G\) sont des racines \( e\)ièmes de l'unité. Par conséquent les traces des éléments de \( G\) ne peuvent prendre qu'un nombre fini de valeurs : toutes les sommes de \( n\) racines \( e\)ièmes de l'unité. Mais vu que les \( C_i\) sont dans \( G\), nous avons
		\begin{equation}
			\Image(\tau)=\{ \tr(A)\tq A\in G \}^r,
		\end{equation}
		qui est un ensemble fini. Par conséquent \( G\) est fini parce que \( \tau\) est injective.
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Ellipsoïde}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}   \label{LemYVWoohcjIX}
	Toute matrice peut être décomposée de façon unique en une partie symétrique et une partie antisymétrique. Cette décomposition est donnée par
	\begin{equation}\label{subEqHIQooyhiWM}
		\begin{aligned}[]
			S & =\frac{ M+M^t }{ 2 }, & A & =\frac{ M-M^t }{ 2 }
		\end{aligned}
	\end{equation}
\end{lemma}

\begin{proof}
	L'existence est une vérification immédiate de \( S+A=M\) en utilisant \eqref{subEqHIQooyhiWM}. Pour l'unicité, si \( S+A=S'+A'\) alors \( S-S'=A-A'\). Mais \( S-S'\) est symétrique et \( A-A'\) est antisymétrique; l'égalité implique l'annulation des deux membres, c'est-à-dire \( S=S'\) et \( A=A'\).
\end{proof}

\begin{definition}  \label{DefOEPooqfXsE}
	Un \defe{ellipsoïde}{ellipsoïde} dans \( \eR^n\) centré en \( v\) est le lieu des points \( x\) vérifiant l'équation
	\begin{equation}\label{EqSNWooXfbTH}
		\langle x-v, M(x-v)\rangle =1
	\end{equation}
	où \( M\) est une matrice symétrique strictement définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.}.

	Lorsque nous parlons d'ellipsoïde \emph{plein}, il suffit de changer l'égalité en une inégalité.
\end{definition}

\begin{remark}
	Le fait que \( M\) soit symétrique n'est pas tout à fait obligatoire; la chose important est que toutes les valeurs propres soient strictement positives. En effet si \( M\) a toutes ses valeurs propres strictement positives, nous nommons \( S\) la partie symétrique de \( M\) et \( A\) la partie antisymétrique (lemme~\ref{LemYVWoohcjIX}). Alors pour tout \( x\in \eR^n\) nous avons
	\begin{equation}
		x^tAx=\langle x, Ax\rangle =\langle A^tx,x \rangle =-\langle Ax, x\rangle =-\langle x,Ax\rangle ,
	\end{equation}
	donc \( x^tAx=0\). L'équation \( x^tMx=1\) est donc équivalente à \( x^tSx=1\) (elles ont les mêmes solutions).

	De plus \( S\) reste strictement définie positive parce que pour tout \( x\in \eR^n\) nous avons
	\begin{equation}
		0<x^tMx=x^tSx.
	\end{equation}
\end{remark}

\begin{proposition}\label{PropWDRooQdJiIr}
	Si \( \ellE\) est un ellipsoïde centrée à l'origine, il existe une base de \( \eR^n\) dans laquelle son équation est :
	\begin{equation}
		\sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }=1.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous avons une matrice symétrique strictement définie positive \( S\) telle que l'équation soit \( \langle x, Sx\rangle =1\). Le théorème spectral~\ref{ThoeTMXla} nous fournit une base orthonormale \( \{ e_i \}\) dans laquelle \( Se_i=\lambda_ie_i\) avec \( \lambda_i>0\). En substituant dans l'équation \( \langle x, Sx\rangle =1\) nous trouvons l'équation
	\begin{equation}
		\sum_i\lambda_ix_i^2=1.
	\end{equation}
	En posant \( a_i=\frac{1}{ \sqrt{\lambda_i} }\), nous trouvons le résultat.  Cette définition des \( a_i\) est toujours possible parce que \( \lambda_i>0\).
\end{proof}

\begin{corollary}   \label{CorKGJooOmcBzh}
	Un ellipsoïde plein centré en l'origine admet une équation de la forme \( q(x)\leq 1\) où \( q\) est une forme quadratique strictement définie positive.
\end{corollary}
Pour rappel de notation, l'ensemble des formes quadratiques strictement définies positives sur l'espace vectoriel \( E\) est noté \( Q^{++}(E)\).

\begin{proof}
	Soit \( \{ e_i \}\) une base de \( \eR^n\) telle que l'ellipsoïde \( \ellE\) ait pour équation
	\begin{equation}
		\sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }\leq 1.
	\end{equation}
	Nous considérons la forme quadratique
	\begin{equation}
		\begin{aligned}
			q\colon \eR^n & \to \eR                                                        \\
			x             & \mapsto \sum_{i=1}^n\frac{ \langle x, e_i\rangle^2 }{ a_i^2 }.
		\end{aligned}
	\end{equation}
	Nous avons évidemment \( \ellE=\{ x\in \eR^n\tq q(x)\leq 1 \}\). De plus la forme \( q\) est strictement définie positive parce que dès que \( x\neq 0\), au moins un des produits scalaires \( \langle x, e_i\rangle \) est non nul et \( q(x)> 0\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Système d'équations linéaires : méthode de Gauss}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% TODO: Ajouter un texte sur les équations de plan, et pourquoi ax+by+cz+d=0 est perpendiculaire au vecteur (a,b,c).

Pour résoudre un système d'équations linéaires, on procède comme suit:
\begin{enumerate}
	\item Écrire le système sous forme matricielle. \[\text{p.ex. } \begin{cases} 2x+3y &= 5 \\ x+2y &= 4 \end{cases} \Leftrightarrow \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right) \]
	\item Se ramener à une matrice avec un maximum de \( 0\) dans la partie de gauche en utilisant les transformations admissibles:
	      \begin{enumerate}
		      \item Remplacer une ligne par elle-même + un multiple d'une autre;
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{L_1  - 2. L_2 \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right) \]
		      \item Remplacer une ligne par un multiple d'elle-même;
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{-L_1  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 2 & 4 \end{array}\right) \]
		      \item Permuter des lignes.
		            \[\text{p.ex. } \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 0 & -2 \end{array}\right)  \stackrel{L_1  \mapsto L_2' \text{ et } L_2  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \]
	      \end{enumerate}
	\item Retransformer la matrice obtenue en système d'équations.
	      \[\text{p.ex. }  \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \Leftrightarrow \begin{cases} x &= -2 \\ y &= 3 \end{cases}  \]
\end{enumerate}

\begin{remark}
	\begin{itemize}
		\item Si on obtient une ligne de zéros, on peut l'enlever:
		      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \Leftrightarrow  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \end{array}\right) \]
		\item Si on obtient une ligne de zéros suivie d'un nombre non-nul, le système d'équations n'a pas de solution:
		      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 7 \end{array}\right) \Leftrightarrow  \begin{cases} \cdots \\ \cdots \\ 0x + 0y + 0z = 7 \end{cases} \Rightarrow \textbf{Impossible} \]
		\item Si on a moins d'équations que d'inconnues, alors il y a une infinité de solutions qui dépendent d'un ou plusieurs paramètres:
		      \[\text{p.ex. }  \left(\begin{array}{ccc|c} 1 & 0 & -2 & 2 \\ 0 & 1 & 3 & 0 \end{array}\right) \Leftrightarrow  \begin{cases} x - 2z = 2 \\ y + 3z = 0 \end{cases} \Leftrightarrow  \begin{cases} x = 2 + 2\lambda \\ y = -3\lambda \\ z = \lambda \end{cases} \]
	\end{itemize}
\end{remark}
