% This is part of Mes notes de mathématique
% Copyright (c) 2012-2013, 2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Processus de Poisson}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecHxbtzQ}

\begin{definition}
    Une famille de variables aléatoires \( (N_t)_{t\geq 0}\) est une \defe{processus de Poisson}{processus!Poisson}\index{Poisson!processus} d'intensité \( \lambda\) s'il existe une suite de variables aléatoires indépendantes et identiquement distribuées \( (T_k)_{k\in \eN}\) de loi \( \dE(\lambda)\) telles que
    \begin{equation}
        N_t=\sup\{ n\geq 0\tq \sum_{k=1}^nT_k\leq t \}.
    \end{equation}
\end{definition}
Si nous posons \( S_n=\sum_{k=1}^nT_k\), alors nous avons une expression plus pratique pour \( N_t\) :
\begin{equation}
    N_t=\sum_{n=1}^{\infty}\mtu_{\{ S_n\leq t \}}.
\end{equation}
Nous avons par la proposition~\ref{PropGMntiy} vu que \( N_t\sim\dP(\lambda t)\).


Pour chaque \( \omega\in \Omega\), la fonction \( t\mapsto N_t(\omega)\) est une fonction (pas du tout strictement) croissante à valeurs dans $\eN$. Cette fonction part de \( 0\) et fait un saut de taille \( 1\) après des intervalles de temps \( T_1(\infty)\), \( T_2(\omega)\), etc. Elle est continue à droite.

Nous avons les égalités d'événements suivantes qui sont pratiques :
\begin{subequations}
    \begin{align}
        \{ s<S_n\leq t \}=\{ N_t\geq n>N_s \}\\
        \{ N_t=n \}=\{ S_n\leq t\leq S_{n+1} \}.
    \end{align}
\end{subequations}

\begin{theorem}
    Les variables aléatoires \( (N_t)_{t\geq 0}\) est un processus de Poisson d'intensité \( \lambda\) si et seulement si elles vérifient les trois propriétés suivantes.
    \begin{description}
        \item[Accroissements indépendants] Pour tout choix \( 0<t_0<t_1<\ldots t_n\), les variables aléatoires \( N_{t_{i+1}}-N_{t_i}\) sont indépendantes.
        \item[Accroissements stationnaires] Si \( 0<s<t\) et \( h>0\) alors
            \begin{equation}
                N_{t+h}-N_{s+h}\stackrel{\mL}{=}N_t-N_s,
            \end{equation}
            c'est-à-dire que les accroissements décalés suivent les mêmes lois.
        \item[Poisson] Pour tout \( t\) nous avons \( N_t\sim\dP(\lambda t)\).
    \end{description}
\end{theorem}
Une conséquence des accroissements stationnaires est que \( N_t-N_s\stackrel{\mL}{=}N_{t-s}-N_0=N_{t-s}\) parce que \( N_0=0\).

\begin{proposition}
    Si \( (N_t)\) est un processus de Poisson d'intensité \( \lambda\), alors
    \begin{equation}
        \lim_{t\to \infty} N_t=+\infty
    \end{equation}
    presque surement. De plus
    \begin{equation}        \label{EqvaVYAs}
        \lim_{t\to \infty} \frac{ N_t }{ t }=\lambda
    \end{equation}
    presque surement.
\end{proposition}
La relation \eqref{EqvaVYAs} est appelée \defe{loi des grands nombres}{loi!des grands nombres!processus de Poisson}.

\begin{proof}
    Par définition nous savons que
    \begin{equation}
        N_t=\sup\{ n\geq 0\tq S_n\leq t \}.
    \end{equation}
    Évidemment la fonction \( t\mapsto N_t\) est croissante, donc la limite
    \begin{equation}
        \lim_{t\to \infty} N_t(\omega)
    \end{equation}
    existe dans \( \mathopen[ 0 , \infty \mathclose]\). Nous pouvons nous restreindre à \( t\in \eN\) et considérer \( L(\omega)=\lim_{n\to \infty} N_n(\omega)\). Par somme télescopique avec \( N_0=0\),
    \begin{equation}    \label{EqfRpnfF}
        \frac{ N_n }{ n }=\frac{ \sum_{k=1}^n(N_k-N_{k-1}) }{ n }.
    \end{equation}
    Étant donné que le processus est de Poisson, les variables aléatoires \( (N_k-N_{k-1})_{k=1,\ldots, n}\) sont indépendantes et suivent toutes la loi de \( N_1-N_0\), c'est-à-dire la loi de \( N_1\). Encore par le fait que \( N_t\) soit de Poisson nous savons que \( N_1\sim\dP(\lambda)\). La loi des grands nombres (\ref{ThoefQyKZ}) appliquée aux variables aléatoires \( N_k-N_{k-1}\) nous dit que
    \begin{equation}
        \frac{ N_n }{ n }\stackrel{p.s.}{\longrightarrow}E(N_1)=\lambda>0.
    \end{equation}
    Du coup \( N_n\to \infty\) et \( L(\omega)=\infty\).

    Nous démontrons maintenant la loi des grands nombres pour les processus de Poisson. Étant donné que pour les entiers \( N_n/n\to \lambda\), pour les réels, si la limite existe, ça ne peut pas être autre chose. Si nous notons \( \bar t\) la partie entière de \( t\in \eR^+\),
    \begin{equation}
        \frac{ N_t }{ t }=\frac{ N_t-N_{\bar t} }{ t }+\frac{ N_{\bar t} }{ t }.
    \end{equation}
    Le second terme est relativement simple à traiter :
    \begin{equation}
        \frac{ N_{\bar t} }{ t }=\underbrace{\frac{ N_{\bar t} }{ \bar t }}_{\to \lambda}\cdot\underbrace{\frac{ \bar t }{ t }}_{\to 1}.
    \end{equation}
    où nous avons utilisé le premier point, \( \bar t\) étant entier. Pour le premier terme nous savons que \( t\mapsto N_t\) est croissante et donc que
    \begin{equation}
        \frac{ N_t-N_{\bar t} }{ t }\leq \frac{ N_{\bar t+1}-N_{\bar t}}{ t }=\frac{ N_{\bar t+1}-N_{\bar t} }{ \bar t+1 }\frac{ \bar t+1 }{ t }.
    \end{equation}
    Le second facteur tend vers \( 1\) lorsque \( t\to \infty\). Le premier s'écrit
    \begin{equation}    \label{eqtPgPpJ}
        \frac{ N_n-N_{n-1} }{ n }
    \end{equation}
    et tend vers zéro en tant que terme général de la série \eqref{EqfRpnfF} qui converge.
\end{proof}

\begin{proposition}
    La variable aléatoire \( N_t/t\) est un estimateur sans biais de \( \lambda\). De plus il converge vers \( \lambda\) en moyenne quadratique.
\end{proposition}

\begin{proof}
    Vu que \( N_t/t\to\lambda\) presque surement, la variable aléatoire \( N_t/t\) est un estimateur de \( \lambda\). Le fait qu'il soit sans biais a été fait dans l'exemple~\ref{ExytNlTq}.

    D'autre part nous avons (voir théorème~\ref{ThojDZjuj})
    \begin{equation}
        \Var\left( \frac{ N_t }{ t } \right)=\frac{1}{ t^2 }\Var(N_t)=\frac{ \lambda }{ t }.
    \end{equation}
    En appliquant la formule \( \Var(X)=E(X^2)-E(X)^2\) à \( X=N_t/t\) nous trouvons
    \begin{equation}
        E\left( \frac{ N_t^2 }{ t^2 } \right)=\frac{ \lambda }{ t }+\lambda^2.
    \end{equation}
    Cela montre que \( \frac{ N_t }{ t }\stackrel{L^2}{\longrightarrow}\lambda\).
\end{proof}

Pour le théorème central limite d'un processus de Poisson, nous visons un résultat du style de
\begin{equation}
    \frac{ \frac{1}{ n }\sum_iX_i-mn }{ \sigma\sqrt{n} }\stackrel{\hL}{\longrightarrow}\dN(0,1).
\end{equation}
Nous écrivons le théorème central limite pour le nombre de sauts que le processus de Poisson a connu en un temps \( t\). Le rôle de la moyenne empirique est joué par \( N_t\). Nous considérons avoir fait \emph{une seule expérience} qui a duré un temps \( t\). Donc le rôle de \( n\) est joué par \( 1\) (et non \( t\) comme on pourrait le croire). Pour le reste, le nombre de succès en un temps \( t\) d'une variable aléatoire exponentielle de paramètre \( \lambda\) est une variable aléatoire de Poisson de paramètre \( \lambda t\), en vertu de ce qui est raconté au point~\ref{subsecPoissonetexpo}. C'est cela qui motive l'énoncé suivant.

\begin{theorem}[Théorème central limite pour les processus de Poisson]\index{théorème!central limite!processus de Poisson}  \label{ThoCSuLLo}
    Si \( (N_t)_{t>0}\) est un processus de Poisson de paramètre \( \lambda\), alors nous avons
    \begin{equation}
        \frac{ N_t-\lambda t }{ \sqrt{\lambda t} }\stackrel{\hL}{\longrightarrow}\dN(0,1).
    \end{equation}
\end{theorem}

\begin{remark}
    Avant de nous lancer dans la démonstration, remarquons que si nous nous limitons à \( t\in \eN\), alors nous avons
    \begin{equation}
        \frac{ N_n-\lambda n }{ \sqrt{\lambda n} }=\frac{ \sum_{k=1}^n(N_k-N_{k-1})-\lambda n }{ \sqrt{\lambda n} }
    \end{equation}
    or par définition nous avons les égalités de lois
    \begin{equation}
        N_k-N_{k-1}\sim N_1\sim \dP(\lambda),
    \end{equation}
    donc
    \begin{equation}
        \frac{ S_n-\lambda n }{ \sqrt{\lambda n} }=\frac{ \frac{1}{ n }S_n-\lambda }{ \frac{ \sqrt{\lambda n} }{ n } }=\frac{ \frac{1}{ n }S_n-\lambda }{ \sqrt{\lambda}/\sqrt{n} },
    \end{equation}
    ce qui est exactement le théorème central limite pour une suite de lois de Poisson\footnote{Au fait près que nous devrions encore montrer que \( S_n\) est de carré intégrable.}.
\end{remark}

\begin{proof}
    Nous écrivons \( \bar t\) la partie entière de \( \bar t\) et nous décomposons :
    \begin{equation}
        \frac{ N_t-\lambda t }{ \sqrt{\lambda t} }=\underbrace{\frac{ N_t-N_{\bar t} }{ \sqrt{\lambda t} }}_A+\underbrace{\frac{ N_{\bar t}-\lambda \bar t }{ \sqrt{\lambda t} }}_B+\underbrace{\frac{ \lambda \bar t-\lambda t }{ \sqrt{\lambda t} }}_C.
    \end{equation}
    En ce qui concerne le terme \( B\), nous avons
    \begin{equation}
        B=\sqrt{\frac{ \bar t }{ t }}\frac{ N_{\bar t}-\lambda \bar t }{ \sqrt{\lambda \bar t} }\to\dN(0,1).
    \end{equation}
    Notons que nous utilisons le fait que si \( a_n\to 1\) (en tant que suite de nombre) et si \( X_n\to\dN(0,1)\) (limite en loi), alors \( a_nX_n\to \dN(0,1)\) en loi.

    Le terme \( C\) est également facile parce que \( \lambda \bar t-\lambda t\) est majoré en norme par \( \lambda\). Du coup
    \begin{equation}
        -\frac{ \lambda }{ \sqrt{\lambda t} }\leq C\leq \frac{ \lambda }{ \sqrt{\lambda t} }.
    \end{equation}
    Donc $\lim_{t\to \infty} C=0$.

    Reste à travailler sur \( A\). Vu que \( t\mapsto N_t\) est croissante, la différence \( N_t-N_{\bar t}\) est positive. Soit \( \eta>0\), nous avons
    \begin{equation}
        P(| A |>\eta)=P(N_t-N_{\bar t}>\sqrt{\lambda t}\eta)\leq P\big( N_{\bar t+1}-N_{\bar t}\geq \sqrt{\lambda t}\eta \big)=P(N_1\geq \eta\sqrt{\lambda t})
    \end{equation}
    parce que nous savons que $N_{\bar t+1}-N_{\bar t}\sim N_1\sim\dP(\lambda)$. En vertu des propriétés de la loi de Poisson,
    \begin{equation}
        \lim_{t\to \infty}P(N_1\geq \eta\sqrt{\lambda t})=0.
    \end{equation}
    En effet si \( Z\) est une variable aléatoire de Poisson de paramètre \( \lambda\) nous avons
    \begin{equation}
        P(Z>l)=\sum_{k=l}^{\infty}P(Z=k)= e^{-\lambda}\sum_{k=l}^{\infty}\frac{ \lambda^k }{ k! }.
    \end{equation}
    Nous reconnaissons la queue de série de \(  e^{\lambda}\), qui tend donc vers zéro lorsque \( l\to \infty\). Nous avons donc prouvé que
    \begin{equation}
        \lim_{t\to \infty} P\big( | A |>\eta \big)=0,
    \end{equation}
    c'est-à-dire la convergence en probabilité de \( A\) vers zéro.

    Nous avons montré que
    \begin{subequations}
        \begin{align}
            B+C&\stackrel{\hL}{\longrightarrow} U\sim\dN(0,1)\\
            A\stackrel{P}{\longrightarrow}0.
        \end{align}
    \end{subequations}
    Le lemme de Slutsky (\ref{LemgXDlhs}) nous avons une convergence du couple
    \begin{equation}
        (A,B+C)\stackrel{\hL}{\longrightarrow}(0,U).
    \end{equation}
    Utilisant le corollaire~\ref{CorINgTPH}, nous trouvons la convergence en loi
    \begin{equation}
        A+(B+C)\stackrel{\hL}{\longrightarrow}0+U,
    \end{equation}
    ce qu'il fallait.
\end{proof}

\section{Quelques trucs sur la simulation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Le théorème ergodique dit que
\begin{equation}
    \pi(x)=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\mtu_{X_k=x}.
\end{equation}
C'est avec cela qu'on calcule \( \pi(x)\) à partir d'une simulation de chaîne de Markov.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le théorème central limite pour Markov}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Version allégée]
    Si \( (X_n)\) est irréductible et positive récurrente, alors pour toute fonction \( f\),
    \begin{equation}
        \frac{1}{ \sqrt{N} }\left[ \sum_{k=1}^N-N\int fd\pi \right]\stackrel{\hL}{\longrightarrow}\dN(0,\sigma^2)
    \end{equation}
    où \( \sigma^2\) dépend de la fonction \( f\) et de la chaîne de Markov.

   Ici, \( \int fd\pi=\sum_{x\in E}f(x)\pi(x)\).
\end{theorem}

Nous allons simuler la variable aléatoire
\begin{equation}
    Z=\frac{1}{ \sqrt{N} }\left[ \sum f(X_k)-N\sum_{x\in E} f(x)\pi(x) \right]
\end{equation}
et puis on va mettre sa réalisation dans un histogramme. Dans le cas où on prend \( f(i)=\mtu_{i=i_0}\), il y a de la simplification dans l'intégrale qui devient
\begin{equation}
    Z=\frac{1}{ \sqrt{N} }\left[ \sum_{i=1}^N\mtu_{X_k=i_0}-N\pi(i_0) \right].
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 5}
%---------------------------------------------------------------------------------------------------------------------------

On pose
\begin{equation}
    D_n=\sqrt{n}\sup_{x\in\eR}| F_n(x)-F(x) |.
\end{equation}
On en génère un milliers de fois \( D_n\), on note \( D_n^{(k)}\) ces réalisations, et on regarde ce que vaut
\begin{equation}
    \frac{1}{ 1000 }\sum_{k=1}^{1000}\mtu_{D_n^{(k)\geq c}}.
\end{equation}
Cela nous donne une approximation de
\begin{equation}
    P\big( \sqrt{n}\sup_{x\in\eR}| F_n(x)-F(x) |\geq c \big).
\end{equation}

Note que chacun des \( D_n^{(k)}\) demande de créer un nouveau vecteur \( Y_i\) de lois qu'on veut regarder. Par exemple de loi exponentielle.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 6}
%---------------------------------------------------------------------------------------------------------------------------

Pour créer une fonction qui renvoie \( i\) avec probabilité \( p_i\) pour \( i=1,2,3\), on peut faire
\begin{equation}
    U\sim\dU[0,1]
\end{equation}
et puis on a
\begin{subequations}
    \begin{align}
        P(U<p_0)&=p_0\\
        P(p_0<U<p_0+p_1)&=p_1\\
        P(p_0+p_1<U<p_2)&=p_2.
    \end{align}
\end{subequations}
Une façon de faire une loi uniforme \( [0,1]\) est de faire \info{rand}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 7}
%---------------------------------------------------------------------------------------------------------------------------

L'échantillon est \( (Y_1,\ldots, Y_n) \) et nous écrivons le vecteur
\begin{equation}
    Y=X\beta+\epsilon
\end{equation}
où \( Y\sim\dN(X\beta,\sigma^2\id)\) et \( \epsilon\sim\dN(0,\sigma^2\id)\). Nous utilisons le principe de maximum de vraisemblance. Soit \( (y_1,\ldots, y_n)\) un échantillon et
\begin{equation}
    P_{\theta}(y_1,\ldots, y_n)=\prod_i\frac{1}{ \sigma\sqrt{2\pi }}\exp\left[ -\frac{ 1 }{2}\left( \frac{ y_i-X_i^t\beta }{ \sigma } \right)^2 \right].
\end{equation}
L'astuce est de faire que \( y_i-X_i^t\beta\) est la \( i\)ième composante du vecteur \( Y-X\beta\) et donc la somme qui est dans l'exponentielle devient la norme de \( Y-X\beta\) :
\begin{equation}
    f_{\theta}(y_1,\ldots, y_n)=\left( \frac{1}{ \sigma\sqrt{2\pi} } \right)^n\exp\left[ -\frac{ 1 }{2}\| Y-X\beta \|^2 \right].
\end{equation}
On passe au logarithme et on dérive par rapport à \( \sigma^2\). Attention : la variable est \( \sigma^2\), donc la dérivée de \( \sigma^2\) est \( 1\) et non \( 2\sigma\). Bref, on trouve
\begin{equation}
    \sigma^2=\frac{1}{ 2n }\| U+X\beta \|.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Simuler des lois conditionnelles}
%---------------------------------------------------------------------------------------------------------------------------

Nous voulons générer des couples \( (X,Y)\) tels que \( Y\) prend les valeurs \( 0\) ou \( 1\) et tels que
\begin{subequations}
    \begin{numcases}{}
        P(X|Y=0)\sim\dE(\lambda_0)\\
        P(X|Y=1)\sim\dE(\lambda_1).
    \end{numcases}
\end{subequations}
Le plus simple est de générer une liste
\begin{subequations}
    \begin{align}
        (X_1,0)&&(X_4,1)\\
        (X_2,0)&&(X_5,1)\\
        (X_3,0)&&(X_6,1)
    \end{align}
\end{subequations}
avec \( X_1,X_2,X_3\sim\dE(\lambda_0)\) et \( X_4,X_5,X_6\sim\dE(\lambda_1)\).

Avec cette méthode cependant la liste est triée et en plus on a autant de \( 1\) que de \( 0\). On peut faire un peu plus technologique pour corriger cela. Pour créer un couple, on commence par \( Y\sim\dB(p)\) et puis suivant que \( Y=0\) ou \( y=1\), on génère \( X\sim\dE(\lambda_0)\) ou \( X\sim\dE(\lambda_1)\).
