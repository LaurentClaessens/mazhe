% This is part of Le Frido
% Copyright (c) 2008-2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Plusieurs notions sur les espaces vectoriels normés (dont la définition \ref{DefNorme}) ont déjà été abordées dans la section \ref{SECooWKJNooKOqpsx}. Voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme, produit scalaire et Cauchy-Schwarz (cas réel)}
%---------------------------------------------------------------------------------------------------------------------------

Dans la suite, le produit scalaire de \( x\) et \( y\) pourra être noté indifféremment par \( x\cdot y\), \( \langle x, y\rangle \) ou \( b(x,y)\) lorsque une forme bilinéaire est donnée.

Nous rappelons au passage que les espaces vectoriels réels sont susceptibles de recevoir un produit scalaire, alors que les espaces vectoriels complexes sont susceptibles de recevoir un produit hermitien. Bien que de nombreux résultats soient identiques ou très similaires, ces deux notions sont à ne pas confondre.

Nous commençons par prouver qu'un produit scalaire étant donné, nous pouvons définir une norme par la formule \( \| x \|^2=\langle x, x\rangle \). Pour cela nous aurons besoin de l'inégalité de Cauchy-Schwarz.

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas réel]      \label{ThoAYfEHG}
	Soit un espace vectoriel réel \( E\) muni d'un produit scalaire\footnote{Produit scalaire, définition \ref{DefVJIeTFj}.} \( (x,y)\mapsto x\cdot y\). Nous posons\footnote{Attention à la notation : pour l'instant nous ne savons pas que c'est une norme; c'est justement un des points de ce théorème. Par ailleurs, la racine carrée est définie par \ref{DEFooGQTYooORuvQb}.}
	\begin{equation}
		\| x \|=\sqrt{ x\cdot x }.
	\end{equation}
	Alors :
	\begin{enumerate}
		\item
		      Il y a l'inégalité
		      \begin{equation}        \label{EQooZDSHooWPcryG}
			      | x\cdot y |\leq \| x \|\| y \|.
		      \end{equation}
		      pour tout \( x,y\in E\).
		\item
		      Il y a égalité \( | x\cdot y | = \| x \|\| y \|.\) si et seulement si \( x\) et \( y\) sont multiples l'un de l'autre.
		\item
		      L'opération \( \| . \|\) est une norme\footnote{Définition \ref{DefNorme}.}.
		\item
		      Cette norme vérifie l'identité du parallélogramme :
		      \begin{equation}        \label{EqYCLtWfJ}
			      \| x-y \|^2+\| x+y \|^2=2\| x \|^2+2\| y \|^2.
		      \end{equation}
	\end{enumerate}
\end{theorem}
\index{Cauchy-Schwarz}
\index{inégalité!Cauchy-Schwarz}

\begin{proof}
	Étant donné que les deux membres de l'inéquation sont positifs, nous allons travailler en passant au carré afin d'éviter les racines carrés dans le second membre.

	Nous considérons l'application
	\begin{equation}
		\begin{aligned}
			P\colon \eR & \to \eR^+           \\
			t           & \mapsto \| x+ty \|,
		\end{aligned}
	\end{equation}
	et nous calculons un peu :
	\begin{subequations}
		\begin{align}
			0 & \leq \| x+ty \|^2                         \\
			  & =(x+ty)\cdot(x+ty)                        \\
			  & =x\cdot x+x\cdot ty+ty\cdot x+t^2y\cdot y \\
			  & =\| y \|^2t^2+2(x\cdot y)t+\| x \|^2.
		\end{align}
	\end{subequations}
	Nous avons utilisé la bilinéarité (pour sortir les \( t\)) et la symétrique du produit scalaire.

	Nous voyons que \( P\) est un polynôme du second degré en \( t\) à valeurs dans \( \mathopen[ 0 , \infty \mathclose[\). Par la proposition \ref{PROPooEZIKooKjJroH} nous en déduisons que le fameux \( b^2-4ac\) doit être négatif ou nul. Nous avons donc
	\begin{equation}
		\Delta=4(x\cdot y)^2-4\| x \|^2\| y \|^2\leq 0,
	\end{equation}
	ce qui donne immédiatement
	\begin{equation}
		(x\cdot y)^2\leq\| x \|^2\| y \|^2.
	\end{equation}

	En ce qui concerne le cas d'égalité, si nous avons \( x\cdot y=\| x \|\| y \|\), alors le discriminant \( \Delta\) ci-dessus est nul et le polynôme \( P\) admet une racine double \( t_0\). Pour cette valeur nous avons
	\begin{equation}
		P(t_0)=| x+t_0y |=0,
	\end{equation}
	ce qui implique \( x+t_0y=0\) et donc que \( x\) et \( y\) sont liés.

	\begin{subproof}
		\spitem[C'est une norme]
		Nous allons nous contenter de prouver l'inégalité triangulaire. Si \( x,y\in E\),  nous avons
		\begin{subequations}
			\begin{align}
				\| x+y \| & =\sqrt{ \| x \|^2+\| y \|^2+2x\cdot y }                                            \\
				          & \leq\sqrt{ \| x \|^2+\| y \|^2+2| x\cdot y | } \label{SUBEQooRISGooRUqBzj}         \\
				          & \leq\sqrt{ \| x \|^2+\| y \|^2+2\| x \|\| y \| }       \label{SUBEQooPQPYooBRRRzK} \\
				          & =\sqrt{ \big( \| x \|+\| y \| \big)^2 }                                            \\
				          & =\| x \|+\| y \|.
			\end{align}
		\end{subequations}
		Justifications.
		\begin{itemize}
			\item Pour \eqref{SUBEQooRISGooRUqBzj}. La fonction racine carrée est croissante, lemme \ref{LEMooSBOAooOOIotR}.
			\item Pour \eqref{SUBEQooPQPYooBRRRzK}. Inégalité de Cauchy-Schwarz \ref{ThoAYfEHG}.
		\end{itemize}

		\spitem[Inégalité du parallélogramme]
		Cette assertion est seulement un calcul :
		\begin{equation}
			\begin{aligned}[]
				\| x-y \|^2+\| x+y \|^2 & =(x-y)\cdot (x-y)+(x+y)\cdot(x+y)          \\
				                        & =x\cdot x-x\cdot y-y\cdot x+y\cdot y       \\
				                        & \quad +x\cdot x+x\cdot y+y\cdot x+y\cdot y \\
				                        & =2x\cdot x+2y\cdot y                       \\
				                        & =2\| x \|^2+2\| y \|^2.
			\end{aligned}
		\end{equation}
	\end{subproof}
\end{proof}

Toute norme dérivant d'un produit scalaire vérifie l'identité du parallélogramme. Ce résultat sert souvent à prouver que des normes ne dérivent pas d'un produit scalaire. C'est le cas de la norme \( N(x,y)=| x |+| y |\) du lemme \ref{LEMooRWJYooOIJkZc} ainsi que du théorème de Weinersmith \ref{THOooCCMBooGulxkQ}.

\begin{proposition}     \label{PROPooVSVMooZrqxdc}
	La norme euclidienne\footnote{Un espace euclidien est un espace vectoriel muni d'un produit scalaire, définition \ref{DefLZMcvfj}. Ici nous considérons la norme associée par le théorème \ref{ThoAYfEHG}.} a les propriétés suivantes :
	\begin{enumerate}
		\item
		      Pour tout vecteur \( x\) et réel \( \lambda\),
		      \begin{equation}
			      \| \lambda x \|=| \lambda |\| x \|.
		      \end{equation}
		\item
		      Pour tout vecteurs \( x\) et \( y\),
		      \begin{equation}
			      \| x+y \|\leq \| x \|+\| y \|
		      \end{equation}
		\item		\label{ITEMooXXDMooDMVlak}
		      Si \( \| x-y \|\leq \lambda\), alors \( \| x \|\leq \|y \|+\lambda\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Un produit scalaire est en particulier une forme bilinéaire, et vérifie les conditions de la définition \ref{DEFooEEQGooNiPjHz}.

	Pour le premier point nous avons
	\begin{equation}
		\| \lambda x\|=\sqrt{ (\lambda x)\cdot (\lambda x) }=\sqrt{ \lambda^2 x\cdot x }=| \lambda |\| x \|.
	\end{equation}
	Nous avons utilisé la formule du lemme \ref{LEMooWSVNooKsymDy}\ref{ITEMooEPHBooCEeJOD}.

	Pour le second point, nous avons les inégalités suivantes :
	\begin{subequations}
		\begin{align}
			\| x+y \|^2 & =\| x \|^2+\| y \|^2+2x\cdot y                                         \\
			            & \leq\| x \|^2+\| y \|^2+2|x\cdot y|                                    \\
			            & \leq\| x \|^2+\| y \|^2+2\| x \|\| y \| =\big( \| x \|+\| y \| \big)^2
		\end{align}
	\end{subequations}
	Nous avons utilisé d'abord la majoration \( | x |\geq x\) qui est évidente pour tout nombre \( x\); et ensuite l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG}.

	Pour \ref{ITEMooXXDMooDMVlak}, nous avons
	\begin{equation}
		\| x \|=\| (x-y)+y \|\leq\| x-y \|+\| y \|\leq \lambda+\| y \|.
	\end{equation}
\end{proof}

\begin{definition}[Norme sur \( \eR^n\)]        \label{DEFooJAGXooMgaUsR}
	Sauf mention du contraire, nous considérons toujours sur \( \eR^n\) la norme (et donc la topologie) associée au produit scalaire de la proposition \ref{PROPooSKVRooDGVCYj} par le théorème \ref{ThoAYfEHG}, c'est-à-dire
	\begin{equation}
		\| x \|=\sqrt{ \sum_{i=1}^nx_i^2 }.
	\end{equation}
\end{definition}

\begin{proposition}[\cite{RTzQrdx}]     \label{PropHIWjdMX}
	Soit \( b\) une forme bilinéaire et symétrique. Alors
	\begin{enumerate}
		\item
		      \( \ker(b)\subset C(b)\) (cône d'isotropie, définition~\ref{DefVKMnUEM})
		\item
		      si \( b\) est positive alors \( \ker(b)=C(b)\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	En deux parties.
	\begin{enumerate}
		\item
		      Si \( z\in\ker(b)\) alors pour tout \( y\in E\) nous avons \( b(z,y)=0\). En particulier pour \( y=z\) nous avons \( b(z,z)=0\) et donc \( z\in C(b)\).
		\item
		      Soit \( b\) positive et \( x\in C(b)\). Par l'inégalité de Cauchy-Schwarz (proposition~\ref{ThoAYfEHG}) nous avons
		      \begin{equation}
			      | b(x,y) |\leq \sqrt{   b(x,x)b(y,y) }=0.
		      \end{equation}
		      Donc pour tout \( y\) nous avons \( b(x,y)=0\).
	\end{enumerate}
\end{proof}


\begin{lemma}       \label{LEMooEZFIooXyYybe}
	Soit un espace vectoriel euclidien\footnote{C'est-à-dire qu'il possède un produit scalaire, voir la définition \ref{DefLZMcvfj}.} \( E\). Si \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( E\) et si \( f\colon E\to E\) est un endomorphisme, alors
	\begin{equation}        \label{EQooQAZLooZutFUz}
		\det(f)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\langle e_{\sigma(i)}, f(e_i)\rangle.
	\end{equation}
\end{lemma}

\begin{proof}
	Nous utilisons la définition \ref{LEMooQTRVooAKzucd} du déterminant d'un endomorphisme \( \det(f)=\det_B\big( f(B) \big)\) en prenant la liste des vecteurs \( \{ e_i \}\) comme \( B\). En l'occurrence, le \( i\)\ieme\ vecteur de la famille \( f(B)\) est \( f(e_i)\).

	Puisque la base est orthonormée, nous avons \( e^*_k(v)=\langle e_k, v\rangle \) et donc aussi
	\begin{equation}
		e^*_{\sigma(i)}(v_i)=\langle e_{\sigma(i)}^*, f(e_i)\rangle.
	\end{equation}
\end{proof}

Et si vous avez tout suivi, vous aurez remarqué que les produits scalaires impliqués dans la formule \eqref{EQooQAZLooZutFUz} sont les éléments de la matrice de \( f\) dans la base \( \{ e_i \}\) parce que \( \langle e_i, f(e_j)\rangle \) est la composante \( i\) de l'image de \( e_j\) par \( f\). Si la matrice est composée en mettant en colonne les images des vecteurs de base, le compte est bon.

%-------------------------------------------------------
\subsection{Topologie}
%----------------------------------------------------

\begin{propositionDef}[\cite{MonCerveau}]	\label{PROPooEKTIooEKmWfn}
	Si \( (E,\| . \|)\) est un espace vectoriel normé, alors
	\begin{equation}
		\begin{aligned}
			d\colon E\times E & \to \eR           \\
			(x,y)             & \mapsto \| x-y \|
		\end{aligned}
	\end{equation}
	est une distance\footnote{Définition \ref{DefMVNVFsX}.}.
	%TODOooIYDZooJKtIXp. Prouver ça

	La \defe{topologie}{topologie d'une norme} associée à la norme \( \| . \|\) est la topologie métrique de \( d\)\footnote{Définition \ref{ThoORdLYUu}.}.
\end{propositionDef}


\begin{proposition}[\cite{MonCerveau}]	\label{PROPooWIGWooXPyDjp}
	Soit un espace vectoriel normé \( (E,\| . \|)\) de dimension finie. Soit un sous-espace vectoriel \( F\subset E\). Nous posons
	\begin{equation}
		\begin{aligned}
			n\colon F & \to \eR          \\
			x         & \mapsto \| x \|.
		\end{aligned}
	\end{equation}
	Alors
	\begin{enumerate}
		\item
		      \( (F,n)\) est un espace vectoriel normé.
		\item
		      La topologie métrique\footnote{Voir la définition \ref{ThoORdLYUu}.} de \( (F,n) \) est la topologie induite de \( E\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	L'application \( n\) est une norme parce que toutes les propriétés de la définition \ref{DefNorme} passent directement de \( \| . \|\) à \( n\). Pour le reste, remarquons d'abord que
	\begin{equation}
		B_F(x,r)=B_E(x,r)\cap F.
	\end{equation}
	Nous notons \( \tau_i\) la topologie induite de \( E\) vers \( F\), et \( \tau_n\) la topologie sur \( F\) pour la norme \( n\). Nous allons prouver que \( \tau_i=\tau_n\).
	\begin{subproof}
		\spitem[\( \tau_i\subset\tau_n\)]
		%-----------------------------------------------------------
		Soit \( A\in\tau_i\). Pour prouver que \( A\in \tau_n\), nous prenons \( x\in A\) et nous prouvons qu'il existe \( r>0\) tel que \( B_F(x,r)\subset A\). Par définition de la topologie induite, il existe un ouvert \( A'\) de \( E\) tel que \( A=F\cap A'\). Vu que \( A'\) est ouvert pour la topologie métrique, il existe \( r>0\) tel que \( B_E(x,r)\subset A'\). Pour ce \( r\) nous avons
		\begin{equation}
			B_F(x,r)=B_E(x,r)\cap F\subset A'\cap F=A.
		\end{equation}
		\spitem[\( \tau_n\subset\tau_i\)]
		%-----------------------------------------------------------
		Soient \( A\in \tau_n\) et \( x\in A\). Il existe \( r>0\) tel que \( x\in B_F(x,r)\subset A\), et donc aussi
		\begin{equation}
			x\in B_E(x,r)\cap F\in \tau_i.
		\end{equation}
	\end{subproof}
\end{proof}


%-------------------------------------------------------
\subsection{Le tore}
%----------------------------------------------------

%-----------------------------------
\subsubsection{Tore sur \( \eR\)}

Pour rappel, nous avons la relation d'équivalence \( \sim\) définie sur \( \eR^n\) en \ref{DEFooRWLLooHkdFDN} : \( a\sim b\) lorsque \( a_i-b_i\in \eZ\) pour tout \( i\).

Pour définir la distance entre \( a\) et \( b\) sur le tore, nous allons d'abord construire une application \(x\mapsto \tau_a(x)\) qui aura pour effet de «rapprocher» \( x\) au maximum de \( a\) tout en restant dans \( [x]\). Ensuite, ce sera la distance entre \( a\) et \( x\).

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooJRQZooLCXOWQ}
	Soient \( a,b\in \eR\). Nous avons
	\begin{enumerate}
		\item		\label{ITEMooBDAQooSYWkMX}
		      \( \overline{B(a,1/2)}\cap[b]\neq \emptyset\)
		\item		\label{ITEMooRSAKooLzvllq}
		      Si \( a-1/2\in[b]\), alors \( \overline{B(a,1/2)}\cap[b]=\{ a-\frac{ 1 }{2},a+\frac{ 1 }{2} \}\).
		\item		\label{ITEMooANPJooAdrPTK}
		      Si \( a-1/2\not\in[b]\), alors \( \overline{B(a,1/2)}\cap[b]\) contient un unique élément et il est dans \( B(a,1/2)\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooBDAQooSYWkMX}]
		%-----------------------------------------------------------
		Nous posons
		\begin{subequations}
			\begin{align}
				b_{max} & =\max\{ x\in[b]\tq x\leq a-1/2 \} \\
				b_{min} & =\min\{ x\in[b]\tq x\geq a+1/2 \}
			\end{align}
		\end{subequations}
		Si \( b_{max}=a-1/2\) alors on est bon. Si au contraire \( b_{max}\neq a-1/2\) alors \( b_{max}<a-1/2\). Dans ce cas \( b_{max}+1>a-1/2\) (sinon ça aurait contredit la maximalité de \( b_{max}\)). Mais
		\begin{equation}
			b_{max}+1<a-1/2+1=a+1/2.
		\end{equation}
		Donc \( b_{max}+1\in B(a,1/2)\).
		\spitem[Pour \ref{ITEMooRSAKooLzvllq}]
		%-----------------------------------------------------------
		Nous supposons que \( a-1/2\in[b]\). Dans ce cas nous avons aussi \( a+1/2\in[b]\). Si \( x\in B(a,1/2)\) alors
		\begin{equation}
			| x-(a-1/2) |\leq | x-a |+\frac{ 1 }{2}<\frac{ 1 }{2}+\frac{ 1 }{2}=1.
		\end{equation}
		Or il n'y a pas deux éléments dans \( [b]\) ayant une distance plus petite que \( 1\). Donc \( x\) n'est pas dans \( [b]\).
		\spitem[Pour \ref{ITEMooANPJooAdrPTK}]
		%-----------------------------------------------------------
		Si \( a-1/2\not\in [b]\). Vu que \( [b]\cap \overline{B(a,1/2)}\) est non vide, et que ni \( a-1/2\) ni \( a+1/2\) ne sont dans \( [b]\), nous en déduisons que \( [b]\cap \overline{B(a,1/2)}\subset B(a,1/2) \). Dans ce cas l'intersection contient un seul élément parce que si \( x,y\in B(a,1/2)\) nous avons \( | x-y |<1\).
	\end{subproof}
\end{proof}

Pour \( a\in \eR\) nous considérons les applications
\begin{equation}
	\begin{aligned}
		d_a\colon \eR & \to \eR                         \\
		b             & \mapsto \min_{x\in [b]}| a-x |,
	\end{aligned}
\end{equation}
et
\begin{equation}
	\begin{aligned}
		\tau_a\colon \eR & \to \eR                                    \\
		x                & \mapsto \min \Big( [x]\cap B(a,1/2) \Big).
	\end{aligned}
\end{equation}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooRMLSooNOKXkQ}
	Soient \( a,b\in \eR\). Nous avons
	\begin{enumerate}
		\item		\label{ITEMooHRISooLsqKqd}
		      Si \( a-1/2\in[b]\), alors \( d_a(b)=\frac{ 1 }{2}\).
		\item	\label{ITEMooXMNYooKQYaee}
		      Si \( a-1/2\not\in[b]\), alors \( d_a(b)<\frac{ 1 }{2}\).
		\item	\label{ITEMooMUCXooBWrRMz}
		      \( d_a(b)=| a-\tau_a(b) |\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooHRISooLsqKqd}]
		%-----------------------------------------------------------
		Si \( a-1/2\in[b]\), nous savons par le lemme \ref{LEMooJRQZooLCXOWQ} que \( \overline{B(a,1/2)}\cap [b]=\{ a-\frac{ 1 }{2},a+\frac{ 1 }{2} \}\). Donc
		\begin{equation}
			d_a(b)=\min_{x\in [b]}| a-x |\leq | a-(a-\frac{1  }{2}) |=\frac{ 1 }{2}.
		\end{equation}
		Mais en même temps, \( [b]\cap B(0,\frac{ 1 }{2})\) est vide, donc pour tout \( x\in [b]\) nous avons \( | a-x |\geq \frac{ 1 }{2}\). Nous en déduisons que \( d_a(b)=\frac{ 1 }{2}\).
		\spitem[Pour \ref{ITEMooXMNYooKQYaee}]
		%-----------------------------------------------------------
		Si \( a-\frac{ 1 }{2}\not\in[b]\), alors l'ensemble \( \overline{B(a,\frac{ 1 }{2})}\cap [b]\) contient un seul élément \( x_0\), qui sera dans \( B(a,\frac{ 1 }{2})\). Donc
		\begin{equation}
			d_a(b)=\min_{x\in[b]}| a-x |\leq | a-x_0 |<\frac{ 1 }{2}.
		\end{equation}
		\spitem[Pour \ref{ITEMooMUCXooBWrRMz}]
		%-----------------------------------------------------------
		Il y a deux possibilités : \( a-\frac{ 1 }{2}\) appartient à \( [b]\) ou non.

		Si \( a-\frac{ 1 }{2}\in[b]\), alors nous avons déjà vu que \( \tau_a(b)=a-\frac{ 1 }{2}\) et \( d_a(b)=\frac{ 1 }{2}\). Dans ce cas nous avons bien
		\begin{equation}
			| a-\tau_a(b) |=| a-(a-1/2) |=1/2=d_a(b).
		\end{equation}

		Si par contre \( a-\frac{ 1 }{2}\not\in[b]\), alors nous notons \( x_0\) l'unique élément de \( [b]\cap B(a,\frac{ 1 }{2})\). Notez que \( x_0=\tau_a(b)\).

		Nous avons \( d_a(b)\leq | a-x_0 |\). Mais nous ne pouvons pas avoir \( d_a(b)<| a-x_0 |\) parce que ça voudrait dire qu'il existe un élément \( x_1\in B(a,1/2)\cap [b]\) autre que \( x_0\), ce qui n'est pas possible. Donc \( d_a(b)=| a-x_0 |\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooAXVLooCmtsPb}
	Pour tout \( a,b\in \eR\) et \( k\in \eZ\) nous avons
	\begin{enumerate}
		\item		\label{ITEMooTELUooJoLYEF}
		      \( \tau_{a+k}(b)=\tau_a(b)+k\),
		\item		\label{ITEMooUWLDooLsDhmI}
		      \( \tau_a(b)=\tau_a(b+k)\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Vu que \( \tau_a(b)\in [b]\), il existe \( l\in \eZ\) tel que \( \tau_a(b)=b+l\).
	\begin{subproof}
		\spitem[\( a+k-\frac{ 1 }{2}\in [b]\)]
		%-----------------------------------------------------------
		Dans ce cas, \( \tau_{a+k}(b)=a+k-\frac{ 1 }{2}\). Mais dans ce cas nous avons aussi \( a-\frac{ 1 }{2}\in [b]\) et donc aussi \( \tau_a(b)=a-\frac{ 1 }{2}\). Au final,
		\begin{equation}
			\tau_{a+k}(b)=a+k-\frac{ 1 }{2}=\tau_a(b)+k.
		\end{equation}
		\spitem[Si \( a+k-1/2\) n'est pas dans \( [b]\)]
		%-----------------------------------------------------------
		Nous avons alors que \( \tau_{a+k}(b)\) est l'unique élément de \( [b]\cap B(a+k,\frac{ 1 }{2})\). Dans ce cas, \( a-\frac{ 1 }{2}\) n'est pas non plus dans \( [b]\), et donc \( \tau_a(b)\) est l'unique élément de \( [b]\cap B(a,\frac{ 1 }{2})\). Donc nous avons aussi \( \tau_a(b)+k\in [b]+B(a+k,\frac{ 1 }{2})\), et comme \( [b]\cap B(a+k,1/2)\) contient un seul élément qui est \( \tau_{a+k}(b)\), nous avons \( \tau_a(b)+k=\tau_{a+k}(b)\).
	\end{subproof}

	Pour voir que \ref{ITEMooUWLDooLsDhmI} est vraie, il suffit de remarquer que \( [b]=[b+k]\).
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooSCXBooVXlqpp}
	Les applications \( \tau_a\) et \( d_a\) sont assez bien adaptées aux classes :
	\begin{enumerate}
		\item		\label{ITEMooLYTMooUdTFWw}
		      \( d_a(b)=d_b(a)\),
		\item	\label{ITEMooEIGSooQXMNYq}
		      Si \( a'\in [a]\), alors \( d_a(b)=d_{a'}(b)\).
		\item		\label{ITEMooVLHWooVGQSKU}
		      Si \( b'\in [b]\), alors \( d_a(b)=d_a(b')\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooLYTMooUdTFWw}]
		%-----------------------------------------------------------
		Il existe un \( k\in \eZ\) tel que \( \tau_a(b)=b+k\). Pour ce \( k\), nous avons \( d_a(b)=| a-b-k |\). Nous avons :
		\begin{equation}
			\tau_b(a)=\min_{x\in [a]}| b-x |\leq | b-(a-k) |=| a-b-k |=\tau_a(b).
		\end{equation}
		Le même type raisonnement montre que \( \tau_a(b)\leq \tau_b(a)\).
		\spitem[Pour \ref{ITEMooEIGSooQXMNYq}]
		%-----------------------------------------------------------
		Soit \( k\in \eZ\) tel que \( a'=a+k\). Un petit calcul utilisant les lemmes précédents :
		\begin{subequations}
			\begin{align}
				d_{a'}(b) & =d_{a+k}(b)                                                                          \\
				          & =| a+k-\tau_{a+k}(b) | & \text{lem. \ref{LEMooRMLSooNOKXkQ}\ref{ITEMooMUCXooBWrRMz}} \\
				          & = | a+k-\tau_a(b)-k |  & \text{lem. \ref{LEMooAXVLooCmtsPb}\ref{ITEMooTELUooJoLYEF}} \\
				          & =| a-\tau_a(b) |                                                                     \\
				          & =d_a(b).
			\end{align}
		\end{subequations}
		\spitem[Pour \ref{ITEMooVLHWooVGQSKU}]
		%-----------------------------------------------------------
		Si \( b'\in [b]\), alors en utilisant le lemme \ref{LEMooAXVLooCmtsPb}\ref{ITEMooUWLDooLsDhmI},
		\begin{equation}
			d_a(b')=| a-\tau_a(b') |=| a-\tau_a(b) |=d_a(b)
		\end{equation}
	\end{subproof}
\end{proof}

\begin{theorem}[\cite{MonCerveau}]	\label{THOooLUOHooCBuOPX}
	La formule
	\begin{equation}
		d\big( [a],[b] \big)=d_a(b)
	\end{equation}
	\begin{enumerate}
		\item
		      est bien définie.
		\item
		      est une distance sur \( \eR/\sim\).
	\end{enumerate}
\end{theorem}

\begin{proof}
	Le fait que la formule soit correcte est l'indépendance des classes du lemme \ref{LEMooSCXBooVXlqpp}\ref{ITEMooEIGSooQXMNYq}\ref{ITEMooVLHWooVGQSKU}. Il nous reste à montrer que c'est une distance en vérifiant les propriétés de la définition \ref{DefMVNVFsX}. Commençons par les points faciles.
	\begin{enumerate}
		\item
		      Nous avons \( d\big( [a],[b] \big)=d\big( [b],[a] \big)\) par le lemme \ref{LEMooSCXBooVXlqpp}\ref{ITEMooLYTMooUdTFWw}.
		\item
		      Nous avons \( d\big( [a],[b] \big)=| \tau_a(b)-a |\geq 0\).
		\item
		      Si \( d\big( [a],[b] \big)=0\), alors \( | \tau_a(b)-a |=0\). Mais cela signifie que \( \tau_a(b)=a\). Et comme \( \tau_a(b)\in [b]\), nous avons \( a\in [b]\) et donc \( [a]=[b]\).
	\end{enumerate}
	Et maintenant nous faisons l'inégalité triangulaire.

	L'astuce est de noter que \( \tau_a(x)\in[x]\), de telle sorte que \( [\tau_a(x)]=[x]\) et que
	\begin{equation}
		d\big( [x],[b] \big)=d\big( [\tau_a(x)],[b] \big).
	\end{equation}
	Nous avons alors ceci pour l'inégalité triangulaire :
	\begin{subequations}
		\begin{align}
			d\big( [a],[x] \big)+d\big( [x],[b] \big) & =| \tau_a(x)-a |+| \tau_a(x)-\tau_{\tau_a(x)}(b) |                                  \\
			                                          & \geq | \tau_{\tau_a(x)}-a |                        & \text{ineg trig dans \( \eR\)} \\
			                                          & \geq\min_{y\in[b]}| y-a |                                                           \\
			                                          & =d_a(b)                                                                             \\
			                                          & =d\big( [a],[b] \big)
		\end{align}
	\end{subequations}
\end{proof}

%-------------------------------------------------------
\subsection{Le tore}
%----------------------------------------------------
\label{SUBSECooZVJPooCBqBuk}


Pour \( a\in \eR^2\) et \( r>0\) nous posons
\begin{equation}
	Q(a,r)=\{ x\in \eR^2\tq | x_i-a_i |<r,\,i=1,2 \}.
\end{equation}
Et en nous servant de la fonction \( \tau\) déjà définie dans le cas réel, nous posons
\begin{equation}
	\begin{aligned}
		\tau_a\colon \eR^2 & \to \overline{Q(a,1/2)}                              \\
		x                  & \mapsto \big( \tau_{a_1}(x_1),\tau_{a_2}(x_2) \big).
	\end{aligned}
\end{equation}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooWYCAooYVhcoD}
	Pour \( a,b\in \eR^2\) nous avons
	\begin{equation}
		\| \tau_a(b)-a \|=\min_{x\in[b]}\| a-x \|.
	\end{equation}
\end{lemma}

\begin{proof}
	Soit \( y\in [b]\), c'est-à-dire \( y_1\in[b_1]\) et \( y_2\in [b_2]\). Donc nous avons \( | a_i-y-2 |\geq | a_1-\tau_{a_1}(y_1) |\) et donc
	\begin{equation}
		\| a-y \|=\sqrt{ (a_1-y_1)^2+(a_2-y_2)^2 }\geq \sqrt{\big( a_1-\tau_{a_1}(y_1) \big)^2+\big( a_2-\tau_{a_2}(y_2) \big)^2}=\| a-\tau_a(y) \|.
	\end{equation}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooTDKOooXBbqzK}
	Pour \( a,b\in \eR^2\) et \( k\in \eZ^2\) nous avons
	\begin{equation}
		\tau_{a+k}(b)=\tau_a(b)+k.
	\end{equation}
	Nous avons aussi \( \tau_a(b+l)=\tau_a(b)\) pour tout \( a,b\in \eR^2\) et \( l\in \eZ^2\).
\end{lemma}

\begin{proof}
	Il s'agit d'un calcul tenant compte de la propriété correspondante dans \( \eR\) (lemme \ref{ITEMooTELUooJoLYEF}\ref{ITEMooTELUooJoLYEF}) :
	\begin{subequations}
		\begin{align}
			\tau_{a+k}(b) & =\big( \tau_{a_1+k_1}(b_1),\tau_{a_2+k_2}(b_2) \big)    \\
			              & =\big( \tau_{a_1}(b_1)+k_1,\tau_{a_2}(b_2)+k_2 \big)    \\
			              & =\big( \tau_{a_1}(b_1),\tau_{a_2}(b_2) \big)+(k_1,k_2).
		\end{align}
	\end{subequations}
	En ce qui concerne la second égalité, c'est juste le fait que \( [b+l]=[b]\).
\end{proof}


\begin{proposition}[\cite{MonCerveau}]	\label{PROPooUVTXooYxwdtb}
	La formule
	\begin{equation}
		d\big( [a],[b] \big)=\| \tau_a(b)-a \|
	\end{equation}
	est bien définie et est une distance sur \( \eR^2/\sim\).
\end{proposition}

\begin{proof}
	Pour montrer que la formule est bien définie, nous utilisons les deux points du lemme \ref{LEMooTDKOooXBbqzK}. Si \( a,b\in \eR^2\) et si \( k,l\in \eZ^2\) nous avons
	\begin{equation}
		\| \tau_{a+k}(b+l)-(a+k) \|=\| \tau_{a}(b+l)+k-a-k \|=\| \tau_a(b)-a \|.
	\end{equation}

	Nous montrons maintenant l'inégalité triangulaire\quext{Je n'ai pas vérifié les autres conditions. Faites-les.}. Comme avant, nous avons
	\begin{equation}
		d\big( [x],[b] \big)=d\big( [\tau_a(x)],[b] \big)=\| \tau_a(x)-\tau_{\tau_a(x)}(b) \|,
	\end{equation}
	et donc
	\begin{subequations}
		\begin{align}
			d\big( [a],[x] \big)+d\big( [x],[b] \big) & = \| \tau_a(x)-a \|+\| \tau_a(x)-\tau_{\tau_a(x)}(b) \| \\
			                                          & \geq\| \tau_{\tau_a(x)}(b)-a \|                         \\
			                                          & \geq \min_{y\in[b]}\| y-a \|                            \\
			                                          & =d\big( [b],[a] \big).
		\end{align}
	\end{subequations}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooDKGCooBIPJqn}
	Nous considérons la projection \(p \colon \eR^2\to T  \). Si \( \alpha\in T\) nous avons
	\begin{equation}
		p^{-1}\big( B(\alpha,r) \big)=\bigcup_{s\in \alpha}B(s,r).
	\end{equation}
\end{lemma}

\begin{proof}
	En deux inclusions.
	\begin{subproof}
		\spitem[Première inclusion]
		%-----------------------------------------------------------
		Soit \( x\in p^{-1}\big( B(\alpha,r) \big)\). La condition \( p(x)\in B(\alpha,r)\) signifie \( d\big( p(x),\alpha \big)<r\). En choissisant \( a\in \alpha\) nous avons
		\begin{equation}
			r>d\big( p(x),\alpha \big)=d\big( [x],[a] \big)=\| \tau_x(a)-x \|.
		\end{equation}
		Donc \( x\in B\big( \tau_x(a),r \big)\). Mais \( \tau_x(a)\in \alpha\), donc
		\begin{equation}
			x\in B\big( \tau_x(a),r \big)\subset\bigcup_{s\in \alpha}B(s,r).
		\end{equation}

		\spitem[L'autre inclusion]
		%-----------------------------------------------------------
		Soient \( s\in \alpha\), \( r>0\) et \( x\in B(s,r)\). Nous allons prouver que \( p(x)\in B(\alpha,r)\). Soit \( a\in \alpha\). En tenant compte du fait que \( s,a\in \alpha\) nous avons \( \tau_x(a)=\tau_x(s)\) et le calcul suivant :Nous avons
		\begin{subequations}
			\begin{align}
				d\big( p(x),\alpha \big) & =d\big( [x],[a] \big)                                           \\
				                         & =\| \tau_x(a)-x \|                                              \\
				                         & =\| \tau_x(s)-x \|        & \text{pcq. } \tau_x(a)=\tau_x(s)    \\
				                         & =\min_{y\in [s]}\| x-y \| & \text{lem. \ref{LEMooWYCAooYVhcoD}} \\
				                         & \leq \| x-s \|                                                  \\
				                         & <r
			\end{align}
		\end{subequations}
		Cela prouve que \( p(x)\in B(\alpha,r)\).
	\end{subproof}
\end{proof}

\begin{lemma}[\cite{MonCerveau}]	\label{LEMooQQZMooMpdZTU}
	La projection canonique \(p \colon \eR^2\to (T,\tau_d)  \) est continue.
\end{lemma}

\begin{proof}
	Soient \( A\in \tau_d\), et \( a\in p^{-1}(A)\). Nous allons trouver \( \epsilon>0\) tel que \( B(a,\epsilon)\subset p^{-1}(A)\). Soit \( \alpha\in p(a)\). Il existe \( r>0\) tel que \( B(\alpha,r)\subset A\). Par le lemme \ref{LEMooDKGCooBIPJqn},
	\begin{equation}
		p^{-1}\big( B(\alpha,r) \big)=\bigcup_{s\in \alpha}B(s,r).
	\end{equation}
	En particulier \( a\in\alpha\) et donc
	\begin{equation}
		B(a,r)\subset\bigcup_{s\in \alpha}B(s,r)=p^{-1}\big( B(\alpha,r) \big)\subset p^{-1}(A).
	\end{equation}
	Donc \( \epsilon=r\) fonctionne.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooFUQHooKZMoMm}
	Sur le tore \( T\), la topologie quotient\footnote{Définition \ref{DEFooHWSYooZZLXQU}.} \( \tau_Q\) est la même que la topologie métrique \( \tau_d\).
\end{proposition}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\( \tau_Q\subset\tau_d\)]
		%-----------------------------------------------------------
		Le lemme \ref{LEMooQQZMooMpdZTU} dit que \( p\) est continue pour \( \tau_d\); donc la proposition \ref{PROPooGOEVooZBAOQh}\ref{ITEMooJLXPooMokQuP} implique que \( \tau_Q\subset\tau_d\).

		\spitem[\( \tau_d\subset\tau_Q\)]
		%-----------------------------------------------------------
		Soit \( A\in \tau_d\). Nous allons prouver que \( S=p^{-1}(A)\) est un ouvert de \( \eR^2\); de cette façon \( A\) sera l'image par \( p\) d'un ouvert de \( \eR^2\). Notez que par rapport à l'énoncé de la proposition \ref{PROPooGOEVooZBAOQh}, ici \( p\) joue le rôle de \( \varphi_i^{-1}\).

		Soit \( s\in p^{-1}(A)\). Vu que \( p(s)\in A\), il existe \( r>0\) tel que \( B_d\big( p(s),r \big)\subset A\). En prenant \( p^{-1}\) des deux côtés,
		\begin{equation}
			p^{-1}\Big(  B\big( p(s),r \big) \Big)\subset p^{-1}(A)=S.
		\end{equation}
		Mais le lemme \ref{LEMooDKGCooBIPJqn} dit que
		\begin{equation}
			p^{-1}\Big( B\big( p(s),r \big) \Big)=\bigcup_{x\in [s]}B(x,r),
		\end{equation}
		donc \( \bigcup_{x\in [s]}B(x,r)\subset S\) et à fortiori \( B(s,r)\subset S\). Nous avons prouvé que \( S\) est ouvert.
	\end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Théorème spectral autoadjoint}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{theorem}[Théorème spectral autoadjoint] \label{ThoRSBahHH}
	Un endomorphisme autoadjoint d'un espace euclidien
	\begin{enumerate}
		\item
		      est diagonalisable dans une base orthonormée,
		\item
		      a son spectre réel.
	\end{enumerate}
\end{theorem}
\index{théorème!spectral!autoadjoint}
\index{diagonalisation!endomorphisme autoadjoint}

\begin{proof}
	Nous procédons par récurrence sur la dimension de \( E\), et nous commençons par \( n=1\)\footnote{Dans \cite{KXjFWKA}, l'auteur commence avec \( n=0\) mais moi je n'en ai \wikipedia{en}{Vacuous_truth}{pas le courage.}.}. Soit donc \( f\colon E\to E\) avec \( \langle f(x), y\rangle =\langle x, f(y)\rangle \). Étant donné que \( f\) est également linéaire, il existe \( \lambda\in \eR\) tel que \( f(x)=\lambda x\) pour tout \( x\in E\). Tous les vecteurs de \( E\) sont donc vecteurs propres de \( f\).

	Passons à la récurrence. Nous considérons \( \dim(E)=n+1\) et \( f\in\gS(E)\). Nous considérons la forme bilinéaire symétrique \( \Phi_f\) et la forme quadratique associée \( \phi_f\). Pour rappel,
	\begin{subequations}
		\begin{align}
			\Phi_f(x,y)=\langle x, f(y)\rangle \\
			\phi_f(x)=\Phi_f(x,x).
		\end{align}
	\end{subequations}
	Et nous allons laisser tomber les indices \( f\) pour noter simplement \( \Phi\) et \( \phi\). Étant donné que \( \overline{ B(0,1) }\) est compacte et que \( \phi\) est continue, il existe \( x_0\in\overline{ B(0,1) }\) tel que
	\begin{equation}
		\lambda=\phi(x_0)=\sup_{x\in\overline{ B(0,1) }}\phi(x).
	\end{equation}
	Notons aussi que \( \| x_0 \|=1\) : le maximum est pris sur le bord. Nous posons
	\begin{equation}
		g=\lambda\id-f
	\end{equation}
	ainsi que
	\begin{equation}
		\Phi_1(x,y)=\langle x, g(y)\rangle .
	\end{equation}
	C'est une forme bilinéaire et symétrique parce que
	\begin{equation}
		\Phi_1(y,x)=\langle y, g(x)\rangle =\langle g(y), x\rangle =\langle x, g(y)\rangle =\Phi_1(x,y)
	\end{equation}
	où nous avons utilisé le fait que \( g\) était autoadjoint et la symétrie du produit scalaire. De plus \( \Phi_1\) est semi-définie positive parce que
	\begin{equation}
		\Phi_1(x,x)=\langle x, \lambda x-f(x)\rangle =\lambda\| x \|^2-\phi(x).
	\end{equation}
	Puisque \( \lambda\) est le maximum, nous avons tout de suite \( \Phi_1(x)\geq 0\) tant que \( \| x \|=1\). Et si \( x\) n'est pas de norme \( 1\), c'est le même prix parce qu'on se ramène à \( \| x \|=1\) en multipliant par un nombre positif. Attention cependant :
	\begin{equation}
		\Phi_1(x_0,x_0)=\lambda\| x_0 \|^2-\phi(x_0)=0.
	\end{equation}
	Donc \( \Phi_1\) a un noyau contenant \( x_0\) par la proposition~\ref{PropHIWjdMX}. Nous en déduisons que \( \Image(g)\neq E\) en effet, \( x_0\in\Image(g)^{\perp}\), mais nous avons la proposition~\ref{PropXrTDIi} sur les dimensions :
	\begin{equation}
		\dim E=\dim(\Image(g))+\dim( \Image(g)^{\perp}).
	\end{equation}
	Comme \( \Image(g)^{\perp}\) est un espace vectoriel non réduit à \( \{ 0 \}\), la dimension de \( \Image(g)\) ne peut pas être celle de \( E\). L'endomorphisme \( g\) n'étant pas surjectif, il ne peut pas être injectif non plus parce que nous sommes en dimension finie; il existe donc \( e_1\in E\) tel que \( g(e_1)=0\) et tant qu'à faire nous choisissons \( \| e_1 \|=1\) (ici la norme est bien celle de l'espace euclidien considéré). Par définition,
	\begin{equation}
		f(e_1)=\lambda e_1,
	\end{equation}
	c'est-à-dire que \( \lambda\in\Spec(f)\). Et \( \phi\) étant une forme quadratique réelle nous avons \( \lambda\in \eR\).

	Nous posons à présent \( H=\Span\{ e_1 \}^{\perp}\). C'est un sous-espace stable par \( f\) parce que si \( x\in H\) alors
	\begin{equation}
		\langle e_1, f(x)\rangle =\langle f(e_1),x\rangle =\lambda\langle e_1, x\rangle =0.
	\end{equation}
	Nous pouvons donc considérer la restriction de \( f\) à \( H\) : \( f_H\colon H\to H\). Cet endomorphisme est bilinéaire et symétrique sur l'espace \( H\) de dimension inférieure à celle de \( E\), donc la récurrence nous donne une base orthonormée
	\begin{equation}
		\{ e_2,\ldots, e_n \}
	\end{equation}
	de vecteurs propres de \( f_H\). De plus les valeurs propres sont réelles, toujours par récurrence. Donc
	\begin{equation}
		\Spec(f)=\{ \lambda \}\cup\Spec(f_H)\subset \eR.
	\end{equation}
	Notons pour être complet que si \( i\geq 2\) alors
	\begin{equation}
		\langle e_1, e_i\rangle =0
	\end{equation}
	parce que le vecteur \( e_i\) est par construction choisi dans l'espace \( H=e_1^{\perp}\). Nous avons donc bien une base orthonormée de \( E\) construite sur des vecteurs propres de \( f\).
\end{proof}

\begin{corollary}   \label{CorSMHpoVK}
	Soit \( E\) un espace vectoriel ainsi que des formes bilinéaires \( \phi\) et \( \psi\) sur \( E\). Si \( \psi\) est définie positive, alors il existe une base \( \psi\)-orthonormale dans laquelle \( \phi\) est diagonale.
\end{corollary}

\begin{proof}
	Il suffit de considérer l'espace euclidien \( E\) muni du produit scalaire \( \langle x, y\rangle =\psi(x,y)\). Ensuite nous diagonalisons la matrice (symétrique) de \( \phi\) pour ce produit scalaire à l'aide du théorème~\ref{ThoRSBahHH}.
\end{proof}

\begin{definition}      \label{DefNormeEucleApp}
	La \defe{norme euclidienne}{norme!euclidienne!dans \( \eR^m\)} d'un élément de \( \eR^m\) est définie par \( \| u \|=\sqrt{u\cdot u}\)\footnote{La racine carrée est définie en \ref{DEFooGQTYooORuvQb}.}.
\end{definition}

Cette définition est motivée par le fait que le produit scalaire \( u\cdot u\) donne exactement la norme usuelle donnée par le théorème de Pythagore :
\begin{equation}
	u\cdot u=\sum_{i=1}^mu_iu_i=\sum_{i=1}^m u_i^2=u_1^2+u_2^2+\cdots+u_m^2.
\end{equation}

Le fait que \( e_i\cdot e_j=\delta_{ij}\) signifie que la base canonique est \defe{orthonormée}{orthonormé}, c'est-à-dire que les vecteurs de la base canonique sont orthogonaux deux à deux et qu'ils ont tout \( 1\) comme norme.

\begin{lemma}\label{LemSclNormeXi}
	Pour tout \( u\in\eR^m\), il existe un \( \xi\in\eR^m\) tel que \( \| u \|=\xi\cdot u\) et \( \| \xi \|=1\).
\end{lemma}

\begin{proof}
	Vérifions que le vecteur \( \xi=u/\| u \|\) ait les propriétés requises. D'abord \( \| \xi \|=1\) parce que \( u\cdot u=\| u \|^2\). Ensuite
	\begin{equation}
		\xi\cdot u=\frac{ u\cdot u }{ \| u \| }=\frac{ \| u \|^2 }{ \| u \| }=\| u \|.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Inégalité de Minkowski}
%---------------------------------------------------------------------------------------------------------------------------

Ce qui est couramment nommé «inégalité de Minkowski» est la proposition~\ref{PropInegMinkKUpRHg} dans les espaces \( L^p\). Nous allons en donner ici un cas très particulier.

\begin{proposition} \label{PropACHooLtsMUL}
	Si \( q\) est une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} sur \( \eR^n\) et si \( x,y\in \eR^n\) alors
	\begin{equation}
		\sqrt{q(x+y)}\leq\sqrt{q(x)}+\sqrt{q(y)}.
	\end{equation}
\end{proposition}

\begin{proof}
	La proposition~\ref{PropFWYooQXfcVY} nous permet de «diagonaliser» la forme quadratique \( q\). Quitte à ne plus avoir une base orthonormale, nous pouvons renormaliser les vecteurs de base pour avoir
	\begin{equation}
		q(x)=\sum_ix_i^2.
	\end{equation}
	Le résultat n'est donc rien d'autre que l'inégalité triangulaire pour la norme euclidienne usuelle, laquelle est démontrée dans le théorème \ref{ThoAYfEHG}.
\end{proof}


\begin{normaltext}
	Un produit scalaire fournit donc toujours une norme et donc une topologie. Il ne faudrait cependant pas croire que toute norme dérive d'un produit scalaire, même pas en dimension finie. Et ce, malgré l'équivalence de toutes les normes du théorème~\ref{ThoNormesEquiv} dont vous avez déjà peut-être entendu parler.
\end{normaltext}


L'intérêt du lemme suivant sera apparent en \ref{NORMooNKBCooKziIjx}.
\begin{lemma}   \label{LEMooRWJYooOIJkZc}
	Sur \( \eR^2\), l'application \( N(x,y)=| x |+| y |\) est une norme\footnote{Définition \ref{DefNorme}.} qui ne dérive pas d'un produit scalaire\footnote{La norme d'un produit scalaire est le théorème  \ref{ThoAYfEHG}.}.
\end{lemma}

\begin{proof}
	Nous commençons par montrer que \( N\) est une norme. Il faut vérifier les trois conditions de la définition \ref{DefNorme}.
	\begin{enumerate}
		\item
		      Il faut utiliser le lemme \ref{LemooANTJooYxQZDw}\ref{ITEMooLQLTooTJTPVM} dans les deux sens. Si \( (x,y)=(0,0)\), alors \( N(x,y)=| 0 |+| 0 |=0+0=0\). Dans l'autre sens, si \( N(x,y)=0\) nous avons
		      \begin{equation}
			      0=| x |+| y |\geq | x |.
		      \end{equation}
		      Donc \( | x |\leq 0\), mais comme \( | x |\geq 0\), nous avons \( | x |=0\) et donc \( x=0\). Le même raisonnement tient pour \( y\).
		\item
		      En tenant compte du fait que \( | \lambda x |=| \lambda | |x |\), nous avons
		      \begin{equation}
			      N\big( \lambda(x,y) \big)=N(\lambda x,\lambda y)=| \lambda | |x |+| \lambda | |y |=| \lambda |(| x |+| y |)=| \lambda |N(x,y).
		      \end{equation}
		\item
		      Nous avons le calcul
		      \begin{subequations}
			      \begin{align}
				      N\big( (x,y)+(a,b) \big) & =N(x+a,y+b)                                                                 \\
				                               & =| x+a |+| y+b |                                                            \\
				                               & \leq | x |+| a |+| y |+| b |     \label{SUBEQooIXKWooTNQFnu} =N(x,y)+N(a,b)
			      \end{align}
		      \end{subequations}
		      Justification : pour \eqref{SUBEQooIXKWooTNQFnu} nous avons utilisé \( | a+b |\leq | a |+| b |\), du lemme \ref{LemooANTJooYxQZDw}.
	\end{enumerate}
	Pour voir qu'elle ne dérive pas d'un produit scalaire, nous montrons qu'elle ne vérifie pas l'identité du parallélogramme du théorème \ref{ThoAYfEHG}.

	Voici un petit bout de code qui nous permet de ne pas faire de recherches à la main :
	\lstinputlisting{tex/sage/sageSnip018.sage}

	Il est vite vu qu'avec \( v=(-1,1)\) et \( w=(1,1)\), l'identité du parallélogramme n'est pas vérifiée.
\end{proof}

\begin{lemma}[\cite{KXjFWKA}]   \label{LemLPOHUme}
	Soit \( V\) un espace vectoriel muni d'un produit scalaire\footnote{Définition \ref{DefVJIeTFj}.} et de la norme associée. Si \( x,y\in V\) satisfont à \( \| x+y \|=\| x \|+\| y \|\), alors il existe \( \lambda\geq 0\) tel que \( x=\lambda y\).
\end{lemma}

\begin{proof}
	Quitte à raisonner avec \( x/\| x \|\) et \( y/\| y \|\), nous supposons que \( \| x \|=\| y \|=1\). Dans ce cas l'hypothèse signifie que \( \| x+y \|^2=4\). D'autre part en écrivant la norme en termes de produit scalaire,
	\begin{equation}
		\| x+y \|^2=\| x \|^2+\| y \|^2+2\langle x, y\rangle ,
	\end{equation}
	ce qui nous mène à affirmer que \( \langle x, y\rangle =1=\| x \|\| y \|\). Nous sommes donc dans le cas d'égalité de l'inégalité de Cauchy-Schwarz\footnote{Théorème~\ref{ThoAYfEHG}.}, ce qui nous donne un \( \lambda\) tel que \( x=\lambda y\). Étant donné que \( \| x \|=\| y \|=1\) nous avons obligatoirement \( \lambda=\pm 1\), mais si \( \lambda=-1\) alors \( \langle x, y\rangle =-1\), ce qui est le contraire de ce qu'on a prétendu plus haut. Par souci de cohérence, nous allons donc croire que \( \lambda=1\).
\end{proof}

\begin{proposition}[\cite{BIBooFJROooLckBUN}]     \label{PropVectsOrthLibres}
	si \( v_1,\cdots,v_k\) sont des vecteurs non nuls, orthogonaux deux à deux, alors ces vecteurs forment une famille libre.
\end{proposition}

\begin{proof}
	Soit une combinaison linéaire nulle des \( v_i\) : \( \sum_{i=1}^k\lambda_iv_i=0\). Nous multiplions scalairement par \( v_k\) :
	\begin{equation}
		0=\sum_i\lambda_iv_k\cdot v_i=\sum_i\lambda_i\delta_{ki}\| v_i \|^2=\lambda_k\| v_k \|^2.
	\end{equation}
	Donc \( \lambda_k=0\).
\end{proof}

\begin{lemma}       \label{LEMooYXJZooWKRFRu}
	Une isométrie d'un espace euclidien fixe l'origine.
\end{lemma}

\begin{proof}
	Soit une isométrie \( f\) d'un espace euclidien : \( f(x)\cdot f(y)=x\cdot y\) pour tout \( x,y\in E\). En particulier pour \( x=0\) nous avons
	\begin{equation}
		f(0)\cdot f(y)=0
	\end{equation}
	pour tout \( y\). Parce que \( f\) est une bijection, nous avons \( f(0)\cdot x=0\) pour tout \( x\). Comme le produit scalaire est non dégénéré\footnote{Lemme \ref{LEMooLPUFooVCvnwW}.} cela implique que \( f(0)=0\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Cauchy-Schwarz etc. cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas complexe\cite{HilbertLi}]      \label{THOooSUCBooFnpkaF}
	Soit un espace vectoriel complexe muni d'un produit hermitien \( \langle ., .\rangle \). Alors pour tous vecteurs \( x,y\) nous avons
	\begin{equation}
		| \langle x, y\rangle  |\leq \| x \|\| y \|
	\end{equation}
	où nous avons posé \( \| x \|=\sqrt{ \langle x, x\rangle  }\).
\end{theorem}

\begin{proof}
	Si \( \langle x, y\rangle =0\), le résultat est évident. Nous nous concentrons donc sur le cas où \( \langle x, y\rangle \neq0\). Nous posons
	\begin{equation}
		\theta=\frac{ \langle x, y\rangle  }{ | \langle x, y\rangle  | }.
	\end{equation}
	C'est un élément de \( \eC\) de norme \( 1\). Nous avons
	\begin{equation}
		\langle \frac{1}{ \theta }x, y\rangle =\frac{ | \langle x, y\rangle  | }{ \langle x, y\rangle  }\langle x, y\rangle =| \langle x, y\rangle  |\geq 0
	\end{equation}
	où le symbole «\( \geq\)» signifie «est réel et positif». Nous posons \( x'=\frac{1}{ \theta }x\) et nous considérons \( t\in \eR\). Remarquons que \( \| x' \|^2=\| x \|^2\) :
	\begin{equation}
		\| x' \|^2=\langle x', x'\rangle =\frac{1}{ \theta\bar\theta }\langle x, x\rangle =\| x \|^2
	\end{equation}
	parce que \( | \theta |=1\).

	En utilisant le fait que \( \langle a, b\rangle +\langle b, a\rangle =\real(\langle a, b\rangle )\) nous avons :
	\begin{subequations}
		\begin{align}
			0\leq \| x'+ty \|^2 & =\| x' \|^2+t\langle x', y\rangle +t\langle y, x'\rangle +t^2\| y \|^2 \\
			                    & =\| y \|^2t^2+2\real(\langle x', y\rangle )t+\| x' \|^2.
		\end{align}
	\end{subequations}
	C'est un polynôme de degré \( 2\) en \( t\) qui n'est jamais strictement négatif. Autrement dit, il a au maximum une seule racine, ce qui signifie que son discriminant est négatif ou nul :
	\begin{equation}
		\real(\langle x', y\rangle )^2-\| y \|^2\| x' \|^2\leq 0.
	\end{equation}
	Mais nous avons choisi \( x'\) de telle sorte que \( \langle x', y\rangle =| \langle x, y\rangle  |\in \eR\) et \( \| x' \|^2=\| x \|^2\); nous avons donc
	\begin{equation}
		| \langle x, y\rangle  |^2\leq \| x \|^2\| y \|^2,
	\end{equation}
	comme il se devait.
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Norme à partir d'un produit scalaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


\begin{proposition}[Identité du parallélogramme\cite{BIBooXLLGooAFwpyU}]        \label{PROPooSSYJooHAXAnC}
	Soit un espace vectoriel complexe \( E\) muni d'un produit hermitien \( \langle ., .\rangle \). Nous posons \( \| x \|=\sqrt{ \langle x, x\rangle  }\). Nous avons
	\begin{enumerate}
		\item
		      \( \| . \|\) est une norme.
		\item
		      Elle vérifie l'identité du parallélogramme :
		      \begin{equation}
			      \| x+y \|^2+\| x-y \|^2=2\| x \|^2+2\| y \|^2
		      \end{equation}
		      pour tout \( x,y\in E\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	En ce qui concerne le fait que \( \| . \|\) soit une norme, tout est essentiellement dans la définition \ref{DefMZQxmQ} d'un produit hermitien. Voyons tout de même l'inégalité triangulaire. Nous avons :
	\begin{subequations}
		\begin{align}
			\| x+y \|^2 & =\langle x+y, x+y\rangle                                            \\
			            & =\| x \|^2+\| y \|^2+\langle x, y\rangle +\langle y, x\rangle       \\
			            & =\| x \|^2+\| y \|^2+2\Re\big( \langle x, y\rangle  \big)           \\
			            & \leq\| x \|^2+\| y \|^2+2|\Re\big( \langle x, y\rangle  \big)|      \\
			            & \leq\| x \|^2+\| y \|^2+2| \langle x, y\rangle  |                   \\
			            & \leq \| x \|^2+\| y \|^2+2\| x \|\| y \|\label{SUBEQooQGQBooMRJcUc} \\
			            & =\big( \| x \|+\| y \| \big)^2.
		\end{align}
	\end{subequations}
	Pour \eqref{SUBEQooQGQBooMRJcUc} nous avons utilisé Cauchy-Schwarz \ref{THOooSUCBooFnpkaF}.
\end{proof}

\begin{definition}      \label{DEFooGUXNooXwCsrq}
	Dans le cas de \( \eC^n\), nous considérons toujours la norme associée à la forme \eqref{EqFormSesqQrjyPH} par la proposition \ref{PROPooSSYJooHAXAnC}, c'est-à-dire, pour rappel :
	\begin{equation}		\label{EQooRRVCooVmcRrp}
		\langle x, y\rangle =\sum_{k=1}^nx_k\overline {y_k},
	\end{equation}
	et
	\begin{equation}
		\| x \|=  \sqrt{ \langle x, x\rangle }
	\end{equation}
	pour tout \( x\in \eC^n\).
	%TODOooULEVooVipGnA. Prouver ça. Prouver que EQooRRVCooVmcRrp est bien un produit hermitien.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, ce qu'on a}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Théorème spectral hermitien]      \label{LEMooVCEOooIXnTpp}
	Nous considérons un espace vectoriel complexe hermitien. Pour un opérateur hermitien\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.},
	\begin{enumerate}
		\item
		      le spectre est réel,
		\item
		      deux vecteurs propres pour des valeurs propres distinctes sont orthogonaux\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
	\end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
	Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part
	\begin{equation}
		\langle Av, v\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
	\end{equation}
	et d'autre part, en utilisant le fait que \( A\) est hermitien,
	\begin{equation}
		\langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
	\end{equation}
	par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

	Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
	\begin{equation}
		\langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
	\end{equation}
	et d'autre part
	\begin{equation}
		\langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
	\end{equation}
	Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

\begin{remark}      \label{REMooMLBCooTuKFmz}
	Un opérateur de la forme \( A^*A\) est évidemment hermitien. De plus ses valeurs propres sont toutes positives parce que si \( A^*Av=\lambda v\) alors
	\begin{equation}
		0\leq \langle Av, Av\rangle =\langle A^*Av, v\rangle =\lambda\langle v, v\rangle .
	\end{equation}
	Donc \( \lambda\geq 0\).
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

La définition du produit scalaire dans \( \eR^n\) est \ref{PROPooSKVRooDGVCYj} et le lien avec la matrice d'une application linéaire est la proposition \ref{PROPooZKWXooWmEzoA}.

\begin{remark}
	Outre l'orthogonalité, le produit scalaire permet de connaître l'angle entre deux vecteurs à travers la définition~\ref{DEFooSVDZooPWHwFQ}. D'autres interprétations géométriques du déterminant sont listées dans le thème~\ref{THMooUXJMooOroxbI}.
\end{remark}

Nous sommes maintenant en mesure de déterminer, pour deux vecteurs quelconques \( u\) et \( v\), la projection orthogonale de \( u\) sur \( v\). Ce sera le vecteur \( \bar u\) parallèle à \( v\) tel que \( u-\bar u\) est orthogonal à \( v\). Nous avons donc
\begin{equation}
	\bar u=\lambda v
\end{equation}
et
\begin{equation}
	(u-\lambda v)\cdot v=0.
\end{equation}
La seconde équation donne \( u\cdot v-\lambda v\cdot v=0\), ce qui fournit \( \lambda\) en fonction de \( u\) et \( v\) :
\begin{equation}
	\lambda=\frac{ u\cdot v }{ \| v \|^2 }.
\end{equation}
Nous avons par conséquent
\begin{equation}
	\bar u=\frac{ u\cdot v }{ \| v \|^2 }v.
\end{equation}
Armés de cette interprétation graphique du produit scalaire, nous comprenons pourquoi nous disons que deux vecteurs sont orthogonaux lorsque leur produit scalaire est nul.

Nous pouvons maintenant savoir quel est le coefficient directeur d'une droite orthogonale à une droite donnée. En effet, supposons que la première droite soit parallèle au vecteur \( X\) et la seconde au vecteur \( Y\). Les droites seront perpendiculaires si \( X\cdot Y=0\), c'est-à-dire si
\begin{equation}
	\begin{pmatrix}
		x_1 \\
		x_2
	\end{pmatrix}\cdot\begin{pmatrix}
		y_1 \\
		y_2
	\end{pmatrix}=0.
\end{equation}
Cette équation se développe en
\begin{equation}    \label{Eqxuyukljsca}
	x_1y_1=-x_2y_2.
\end{equation}
Le coefficient directeur de la première droite est \( \frac{ x_2 }{ x_1 }\). Isolons cette quantité dans l'équation \eqref{Eqxuyukljsca} :
\begin{equation}
	\frac{ x_2 }{ x_1 }=-\frac{ y_1 }{ y_2 }.
\end{equation}
Donc le coefficient directeur de la première est l'inverse et l'opposé du coefficient directeur de la seconde.

\begin{example}
	Soit la droite \( d\equiv y=2x+3\). Le coefficient directeur de cette droite est \( 2\). Donc le coefficient directeur d'une droite perpendiculaires doit être \( -\frac{ 1 }{ 2 }\).
\end{example}


\begin{probleme}		\label{PROBooLOEKooZxNpJZ}
	La «preuve alternative» qui suit \ldots{} je ne sais plus du tout à quoi elle faisait référence. Si vous avez une idée de l'énoncé, faites-le moi savoir.

	Sinon, vous pouvez certainement ne pas la lire.
\end{probleme}

\begin{proof}[Preuve alternative]
	La preuve peut également être donnée en ne faisant pas référence au produit scalaire. Il suffit d'écrire toutes les quantités en termes des coordonnées de \( X\) et \( Y\). Si nous posons
	\begin{equation}
		\begin{aligned}[]
			X & =\begin{pmatrix}
				     x_1 \\
				     x_2 \\
				     x_2
			     \end{pmatrix},
			  & Y                & =\begin{pmatrix}
				                        y_1 \\
				                        y_2 \\
				                        y_3
			                        \end{pmatrix},
		\end{aligned}
	\end{equation}
	l'inégalité à prouver devient
	\begin{equation}
		(x_1y_1+x_2y_2+x_3y_3)^2\leq (x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2).
	\end{equation}
	Nous considérons la fonction
	\begin{equation}
		\varphi(t)=(x_1+ty_1)^2+(x_2+ty_2)^2+(x_3+ty_3)^2
	\end{equation}
	En tant que norme, cette fonction est évidemment positive pour tout \( t\). En regroupant les termes de chaque puissance de \( t\), nous avons
	\begin{equation}
		\varphi(t)=(y_1^2+y_2^2+y_3^2)t^2+2(x_1y_1+x_2y_2+x_3y_3)t+(x_1^2+x_2^2+x_3^2).
	\end{equation}
	C'est un polynôme du second degré en \( t\). Par conséquent le discriminant doit être négatif\footnote{Proposition \ref{PROPooEZIKooKjJroH}.}. Nous avons donc
	\begin{equation}
		4(x_1y_1+x_2y_2+x_3y_3)^2-(x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2)\leq 0.
	\end{equation}
	La thèse en découle aussitôt.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Pythagore}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Pythagore\cite{MonCerveau}]     \label{THOooHXHWooCpcDan}
	Soient un espace euclidien\footnote{Définition \ref{DefLZMcvfj}.} \( E\) ainsi que trois points \( a,b,c\in E\) formant un triangle rectangle en \( a\), c'est-à-dire tel que
	\begin{equation}        \label{EQooRAWAooBxlBcZ}
		(b-a)\cdot (a-c)=0
	\end{equation}
	Alors
	\begin{equation}
		\| b-c \|^2=\| b-a \|^2+\| a-c \|^2.
	\end{equation}
\end{theorem}

\begin{proof}
	D'abord pour développons l'hypothèse \eqref{EQooRAWAooBxlBcZ}:
	\begin{equation}
		b\cdot a-b\cdot c-\| a \|^2+a\cdot c=0,
	\end{equation}
	et nous isolons un bout qui va nous servir plus tard:
	\begin{equation}        \label{EQooWPWZooLjlVJk}
		b\cdot a+a\cdot c=b\cdot c+\| a \|^2.
	\end{equation}

	Maintenant nous calculons un peu :
	\begin{subequations}
		\begin{align}
			\| b-a \|^2+\| a-c \|^2 & =\| b \|^2-2b\cdot a+\| a \|^2+\| a \|^2-2a\cdot c+\| c \|^2                           \\
			                        & =2\| a \|^2+\| b \|^2+\| c \|^2-2(b\cdot c+\| a \|^2)      \label{SUBEQooHCWXooQHpGTO} \\
			                        & =\| b \|^2+\| c \|^2-2b\cdot c                                                         \\
			                        & =\| b-c \|^2.
		\end{align}
	\end{subequations}
	Pour \eqref{SUBEQooHCWXooQHpGTO}, nous avons substitué \eqref{EQooWPWZooLjlVJk}.
\end{proof}

\begin{normaltext}
	Je profite de l'occasion pour montrer mon scepticisme quant aux preuves de Pythagore basées sur différents pliages et découpages des carrés construits sur les côtés du triangle.

	Si, comme ici, nous considérons la géométrie dans \( \eR^2\) muni de son produit scalaire, alors le théorème \ref{THOooHXHWooCpcDan} est le théorème de Pythagore et il n'est pas loin d'être la définition de la distance entre deux points. Ce serait exactement la définition pour le triangle \( A=(0,0)\), \( B=(a,0)\), \( C=(a,b)\).

	Pour autant que je le sache, la géométrie dans «le plan» (celle du collège) ne définit pas «longueur» et «aire». Donc bon \ldots{}  Il y a peut-être un moyen de s'en sortir, mais je ne le connais pas.

	Bref, soit on se met d'accord sur les définitions (et dans ce cas je serais étonné qu'il existe une démonstration de Pythagore très différente de ce qu'on a ici), soit il faudrait se calmer avec les soi-disant preuves du théorème de Pythagore.
\end{normaltext}

\begin{lemma}       \label{LEMooKDGRooMmJqnn}
	Soit un carré de côté \( c\). Alors deux points du carré sont au maximum éloignés d'une distance égale à \( c\sqrt{ 2 }\).
\end{lemma}
% Juste pour donner le vertige, cela est une dépendance du théorème de Jordan.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooTNTNooRjhuJZ}
	Soient \( u\) et \( v\), deux vecteurs de \( \eR^3\). Le \defe{produit vectoriel}{produit!vectoriel} de \( u\) et \( v\) est le vecteur \( u\times v\) défini par
	\begin{equation}    \label{EQooCUJRooFuFPaZ}
		u\times v=\det\begin{pmatrix}
			e_1 & e_2 & e_3 \\
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3
		\end{pmatrix}
	\end{equation}
	où les vecteurs \( e_1\), \( e_2\) et \( e_3\) sont les vecteurs de la base canonique de \( \eR^3\).
\end{definition}

\begin{lemma}       \label{LEMooYKNPooGVMuse}
	Le produit vectoriel \( u\times v\) est également exprimé par
	\begin{subequations}        \label{EQSooOWGZooNYruoy}
		\begin{align}
			u\times v & =(u_2v_3-u_3v_2)e_1+(u_3v_1-u_1v_3)e_2+(u_1v_2-u_2v_1)e_3     \label{SEBEQooVROKooRpUOIr} \\
			          & =\sum_{i,j,k}\epsilon_{ijk}v_iw_je_k
		\end{align}
	\end{subequations}
	où \( \epsilon_{ijk}\) est défini par \( \epsilon_{xyz}=1\) et ensuite \( \epsilon_{ijk}\) est \( 1\) ou \( -1\) suivant que la permutation des \( x\), \( y\) et \( z\) est paire ou impaire. C'est-à-dire que \( \epsilon_{ijk}\) est la signature de la permutation qui amène \( (1,2,3)\) sur \( (i,j,k)\).
\end{lemma}

\begin{proof}
	Il s'agit seulement de développer explicitement le déterminant \eqref{EQooCUJRooFuFPaZ}.
\end{proof}

\begin{normaltext}      \label{NORMooARQCooCAapiy}
	Admettons que \( a\times b=v\). En calculant le même produit vectoriel dans la base \( f_i=-e_i\), les composantes de \( a\) et \( b\) changent de signe et la formule \eqref{EQSooOWGZooNYruoy} dit que le produit vectoriel ne change pas. On serait tenté d'écrire, dans la base \( \{ f_i \}\)
	\begin{equation}
		(-a)\times (-b)=v,
	\end{equation}
	tout en pleurant parce que dans la base des \( f_i\), le vecteur \( v\) devient \( -v\).

	Il y a des personnes que cela tracasse tellement qu'on entend parler de «le produit vectoriel est un pseudo-vecteur sous \( \SO(2)\)». Les physiciens en théorie quantique des champs sont terribles sur ce sujet\footnote{D'autant plus que, au lieu d'écrire un produit scalaire \( a\cdot b\), il écrivent \( a_ib^i\) en disant que \( b^i\) est un covecteur. Avec toutes les complications que ça implique sur les «lois de transformation» d'un covecteur sous tel ou tel groupe.}.

	Il suffit d'être clair. Le produit vectoriel n'est défini que sur \( \eR^3\), et est défini par sa formule dans la base canonique, point barre. Si vous avez des vecteurs \( a\) et \( b\) dont vous connaissez les composantes dans une autre base, vous devez calculer les composantes dans la base canonique, utiliser la formule pour trouver les composantes de \( a\times b\) dans la base canonique. Ensuite, si ça vous chante, vous pouvez calculer à nouveau les composantes de \( a\times b\) dans une autre base.

	Tout cela pour dire que le produit vectoriel n'est pas une opération très généralisable. Il est possible, pour sembler plus intrinsèque, de tenter cette définition : le produit vectoriel \( a\times b\) est le vecteur perpendiculaire à \( a\) et \( b\), de longueur égale à l'aire du parallélogramme construit sur \( a\) et \( b\).

	Cette «définition» a plusieurs inconvénients.
	\begin{itemize}
		\item Elle demande quand même un produit scalaire et des aires; bref, elle demande une structure métrique,
		\item Elle ne donne pas le sens. En effet, dans \( \eR^3\), il y a deux vecteurs de longueur donnée perpendiculaires à \( a\) et \( b\). Il faut donc préciser le sens. Cela revient à donner une orientation et donc, fondamentalement, à choisir une base.
	\end{itemize}

	Bref, on retiendra que le produit vectoriel est une opération accrochée à \( \eR^3\) et à sa base canonique.
\end{normaltext}

\begin{lemmaDef}
	Nous avons l'égalité suivante pour tout \( u,v,w\in \eR^3\) :
	\begin{equation}        \label{EQooKJYUooSQgfXU}
		(u\times v)\cdot w=\det\begin{pmatrix}
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3 \\
			w_1 & w_2 & w_3
		\end{pmatrix}.
	\end{equation}
	Le résultat est nommé le \defe{produit mixte}{produit!mixte} de trois vecteurs de \( \eR^3\).
\end{lemmaDef}

\begin{normaltext}
	Nous avons donné un nom à la combinaison \( (u\times v)\cdot w\). J'imagine que vous voyez pourquoi nous ne considérons pas la combinaison \( (u\cdot v)\times w\).
\end{normaltext}

Le lemme suivant donne un moyen compliqué et peu pratique de calculer la valeur absolue du produit mixte. La formule \eqref{EQooWZUQooYydphW} ne sera utilisée que pour faire le lien entre un jacobien et un élément de volume en dimension trois lorsque nous verrons les intégrales sur des variétés. Voir l'équation \eqref{EQooYIJSooHtkXfu}.

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooSMWNooCmEZeY}
	Le produit mixte peut également être exprimé par
	\begin{equation}        \label{EQooWZUQooYydphW}
		|(u\times v)\cdot w|^2=\det\begin{pmatrix}
			\| u \|^2 & u\cdot v  & u\cdot w  \\
			v\cdot u  & \| v \|^2 & v\cdot w  \\
			w\cdot u  & w\cdot v  & \| w \|^2
		\end{pmatrix}.
	\end{equation}
\end{lemma}

\begin{proof}
	Si nous notons
	\begin{equation}
		a= \begin{pmatrix}
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3 \\
			w_1 & w_2 & w_3
		\end{pmatrix},
	\end{equation}
	il faut simplement remarquer que
	\begin{equation}
		\begin{pmatrix}
			\| u \|^2 & u\cdot v  & u\cdot w  \\
			v\cdot u  & \| v \|^2 & v\cdot w  \\
			w\cdot u  & w\cdot v  & \| w \|^2
		\end{pmatrix}=aa^t.
	\end{equation}
	Donc au niveau des déterminants, en utilisant les propositions \ref{PROPooHQNPooIfPEDH} et le lemme \ref{LEMooCEQYooYAbctZ} nous avons
	\begin{equation}
		\det\begin{pmatrix}
			\| u \|^2 & u\cdot v  & u\cdot w  \\
			v\cdot u  & \| v \|^2 & v\cdot w  \\
			w\cdot u  & w\cdot v  & \| w \|^2
		\end{pmatrix}=\det(aa^t)=\det(a)\det(a^t)=\det(a)^2.
	\end{equation}
	Et maintenant, par définition, \( \det(a)=(u\times w)\cdot w\). Donc le résultat annoncé.
\end{proof}

\begin{proposition}     \label{PropScalMixtLin}
	Les applications produit scalaire, vectoriel et mixte sont multilinéaires. Spécifiquement, nous avons les propriétés suivantes.
	\begin{enumerate}
		\item
		      Les applications produit scalaire et vectoriel sont bilinéaires. C'est-à-dire que pour tout vecteurs \( a\), \( b\), \( c\) et pour tout nombre \( \alpha\) et \( \beta\) nous avons
		      \begin{equation}
			      \begin{aligned}[]
				      a\times (\alpha b +\beta c) & =\alpha(a\times b)+\beta(a\times c)  \\
				      (\alpha a+\beta b)\times c  & =\alpha(a\times c)+\beta(b\times c).
			      \end{aligned}
		      \end{equation}

		\item
		      Le produit mixte est trilinéaire.
		\item
		      Le produit vectoriel est antisymétrique, c'est-à-dire \( u\times v=-v\times u\).
		\item
		      Nous avons \( u\times v=0\) si et seulement si \( u\) et \( v\) sont colinéaires, c'est-à-dire si et seulement si l'équation \( \alpha u+\beta v=0\) a une solution différente de la solution triviale \( (\alpha,\beta)=(0,0)\).
	\end{enumerate}
\end{proposition}

\begin{proposition}[Identité de Lagrange\cite{ooHFUZooGakvHi}]     \label{PROPooMXAIooJureOD}
	Si \( x,y\in \eR^n\), alors
	\begin{equation}
		\| x \|^2\| y \|^2-(x\cdot y)^2=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
	\end{equation}
	Et si \( n=3\) alors
	\begin{equation}
		\| x\times y \|^2=\| x \|^2\| y \|^2-(x\cdot y)^2.
	\end{equation}
\end{proposition}

\begin{proof}
	C'est un calcul. D'abord nous avons
	\begin{equation}
		\| x \|^2\| y \|^2-(x\cdot y)^2=\sum_ix_i^2\sum_jy_j^2-\big( \sum_k x_ky_k  \big)^2=\sum_{ij}x_i^2y_j^2-\sum_{kl}x_ky_kx_ly_l.
	\end{equation}
	Ensuite nous coupons les sommes de la façon suivante
	\begin{equation}
		\sum_{ij}=\sum_j\sum_{i<j}+\sum_j(i=j)+\sum_j\sum_{i>j}
	\end{equation}
	pour obtenir
	\begin{equation}
		\begin{aligned}[]
			\| x \|^2\| y \|^2-(x\cdot y)^2 & =\sum_j\sum_{i<j}x_i^2y_j^2+\sum_jx_j^2y_j^2+\sum_j\sum_{i>j}x_i^2y_j^2           \\
			                                & \quad-\sum_l\sum_{k<l}x_ky_kx_ly_l-\sum_kx_k^2y_k^2-\sum_l\sum_{k>l}x_ky_kx_ly_l.
		\end{aligned}
	\end{equation}
	Il y a deux termes qui se simplifient. Notez que si \( A_{kl}\) est symétrique en \( kl\) nous avons
	\begin{equation}
		\sum_l\sum_{k<l}A_{kl}=\sum_k\sum_{l<k}A_{lk}=\sum_k\sum_{l<k}A_{kl}.
	\end{equation}
	La première égalité était seulement un renommage des indices. Le coup des indices symétriques est justement ce qu'il se passe dans les deux termes en\( x_ky_kx_ly_l\), donc nous les regroupons :
	\begin{subequations}
		\begin{align}
			\| x \|^2\| y \|^2-(x\cdot y)^2 & =\sum_j\big( \sum_{i<j}x_i^2x_j^2+\sum_{i>j}x_i^2y_j^2-2\sum_{i>j}x_iy_ix_jy_j \big) \\
			                                & =\sum_j\sum_{i<j}(x_i^2y_j^2+x_j^2y_i^2-2x_iy_ix_jy_j)                               \\
			                                & =\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
		\end{align}
	\end{subequations}
	Voilà qui prouve la première formule. Pour la seconde, il faut seulement poser \( n=3\) et écrire les sommes explicitement.

	\begin{itemize}
		\item
		      Pour \( j=1\), la somme sur \( i\) est \( \sum_{i<1}\), c'est-à-dire aucun termes.
		\item
		      Pour \( j=2\), il y a seulement \( i=1\), donc le terme \( (x_1y_2-x_2y_1)^2\).
		\item
		      Pour \( j=3\), il y a les termes \( i=1\) et \( i=2\), donc les termes \( (x_1y_3-x_3y_1)^2+(x_2y_3-x_3y_2)^2\).
	\end{itemize}
	Ces trois termes collectés sont justement les composants (au carré) de \( x\times y\) données dans la formule \eqref{SEBEQooVROKooRpUOIr}.
\end{proof}

Les trois vecteurs de base \( e_x\), \( e_y\) et \( e_y\) ont des produits vectoriels faciles à retenir :
\begin{equation}
	\begin{aligned}[]
		e_x\times e_y & =e_z \\
		e_y\times e_z & =e_x \\
		e_z\times e_x & =e_y
	\end{aligned}
\end{equation}

Les deux formules suivantes, qui mêlent le produit scalaire et le produit vectoriel, sont souvent utiles en analyse vectorielle :
\begin{equation}
	\begin{aligned}[]
		(u\times v)\cdot w  & =u\cdot(v\times w)                                   \\
		(u\times v)\times w & =-(v\cdot w)u+(u\cdot w)v     \label{EqFormExpluxxx}
	\end{aligned}
\end{equation}
pour tout vecteurs \( u\), \( v\) et \( w\) dans \( \eR^3\). Nous les admettons sans démonstration. La seconde formule est parfois appelée \defe{formule d'expulsion}{formule!d'expulsion (produit vectoriel)}.

\begin{example}
	Calculons le produit vectoriel \( v\times w\) avec
	\begin{equation}
		\begin{aligned}[]
			v & =\begin{pmatrix}
				     3  \\
				     -1 \\
				     1
			     \end{pmatrix} & w=\begin{pmatrix}
				                       1 \\
				                       2 \\
				                       -1
			                       \end{pmatrix}.
		\end{aligned}
	\end{equation}
	Les vecteurs s'écrivent sous la forme \( v=3e_x-e_y+e_z\) et \( w=e_x+2e_y-e_z\). Le produit vectoriel s'écrit
	\begin{equation}
		\begin{aligned}[]
			(3e_x-e_y+e_z)\times (e_x+2e_y-e_z) & =6e_x\times e_y-3e_x\times e_z         \\
			                                    & \quad -e_y\times e_x + e_y\times e_z   \\
			                                    & \quad + e_z\times e_x + 2e_z\times e_y \\
			                                    & =6e_z+3e_y+e_z+e_x+e_y-2e_x            \\
			                                    & =-e_x+4e_y+7e_z.
		\end{aligned}
	\end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si \( a\), \( b\) et \( c\) sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre \( a\cdot(b\times c)\). En écrivant le produit vectoriel sous forme de somme de trois déterminants \( 2\times 2\), nous avons
\begin{equation}
	\begin{aligned}[]
		a\cdot & (b\times c)                                          \\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
		\begin{vmatrix}
				b_2 & b_3 \\
				c_2 & c_3
			\end{vmatrix}e_x-\begin{vmatrix}
				                 b_1 & b_3 \\
				                 c_1 & c_3
			                 \end{vmatrix}e_y+\begin{vmatrix}
				                                  b_1 & b_2 \\
				                                  c_1 & c_2
			                                  \end{vmatrix}\right)        \\
		       & =a_1\begin{vmatrix}
			             b_2 & b_3 \\
			             c_2 & c_3
		             \end{vmatrix}-a_2\begin{vmatrix}
			                              b_1 & b_3 \\
			                              c_1 & c_3
		                              \end{vmatrix}+a_3\begin{vmatrix}
			                                               b_1 & b_2 \\
			                                               c_1 & c_2
		                                               \end{vmatrix} \\
		       & =\begin{vmatrix}
			          a_1 & a_2 & a_3 \\
			          b_1 & b_2 & b_3 \\
			          c_1 & c_2 & c_3
		          \end{vmatrix}.
	\end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
	a\cdot (b\times c)=\begin{vmatrix}
		a_1 & a_2 & a_3 \\
		b_1 & b_2 & b_3 \\
		c_1 & c_2 & c_3
	\end{vmatrix}.
\end{equation}

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}     \label{PROPooTUVKooOQXKKl}
	Le produit vectoriel\footnote{Définition \ref{DEFooTNTNooRjhuJZ}.} \( a\times b\) est un vecteur orthogonal à \( a\) et \( b\).
\end{proposition}

\begin{proof}
	Vérifions que \( a\perp (a\times b)\). Pour cela, nous calculons \( a\cdot (a\times b)\), c'est-à-dire le produit mixte
	\begin{equation}
		a\cdot(a\times b)=\begin{vmatrix}
			a_1 & a_2 & a_3 \\
			a_1 & a_2 & a_3 \\
			b_1 & b_2 & b_3
		\end{vmatrix}=0.
	\end{equation}
	L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

Ces résultats admettent une intéressante généralisation.
\begin{lemma}       \label{LEMooFRWKooVloCSM}
	Soit \( X\in \eR^n\) ainsi que \( v_1,\ldots, v_{n-1}\in \eR^n\). Alors
	\begin{enumerate}
		\item
		      Nous avons
		      \begin{equation}        \label{EQooMQNPooRHHBjz}
			      \det(X,v_1,\ldots, v_{n-1})=X\cdot
			      \det\begin{pmatrix}
				      e_1 & \ldots  & e_n \\
				          & v_1     &     \\
				          & \vdots  &     \\
				          & v_{n-1} &
			      \end{pmatrix}
		      \end{equation}
		\item
		      Le vecteur
		      \begin{equation}
			      \det\begin{pmatrix}
				      e_1 & \ldots  & e_n \\
				          & v_1     &     \\
				          & \vdots  &     \\
				          & v_{n-1} &
			      \end{pmatrix}
		      \end{equation}
		      est orthogonal à tous les \( v_i\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	Puisque les deux côtés de \eqref{EQooMQNPooRHHBjz} vus comme fonctions de \( X\), sont des applications linéaires de \( \eR^n\) dans \( \eR\), il suffit de vérifier l'égalité sur une base.

	Nous posons \( \tau_i\colon \eR^n\to \eR^{n-1}\),
	\begin{equation}
		\tau_i(v)_k=\begin{cases}
			v_k     & \text{si } k<i             \\
			v_{k+1} & \text{si } k\geq i\text{.}
		\end{cases}
	\end{equation}
	et nous avons d'une part
	\begin{equation}
		e_k\cdot
		\det
		\begin{pmatrix}
			e_1 & \ldots  & e_n \\
			    & v_1     &     \\
			    & \vdots  &     \\
			    & v_{n-1} &
		\end{pmatrix}
		=\det\begin{pmatrix}
			\tau_kv_1 \\
			\vdots    \\
			\tau_kv_{n-1}
		\end{pmatrix}
	\end{equation}
	et d'autre part,
	\begin{equation}
		\det(e_k,v_1,\ldots, v_{n-1})=\det
		\begin{pmatrix}
			0      &     &        &         \\
			\vdots &     &        &         \\
			1      & v_1 & \cdots & v_{n-1} \\
			\vdots &     &        &         \\
			0      &     &        &
		\end{pmatrix}=\det(\tau_k v_1,\ldots, \tau_k v_{n-1}).
	\end{equation}
	La première assertion est démontrée.

	En ce qui concerne la seconde, il suffit d'appliquer la première et se souvenir qu'un déterminant est nul lorsque deux lignes sont égales\footnote{Corolaire \ref{CORooAZFCooSYINvBl}.}. En effet :
	\begin{equation}
		v_k\cdot \det
		\begin{pmatrix}
			e_1 & \ldots  & e_n \\
			    & v_1     &     \\
			    & \vdots  &     \\
			    & v_{n-1} &
		\end{pmatrix}
		= \det(v_k,v_1,\ldots, v_n)=0.
	\end{equation}
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Procédé de Gram-Schmidt}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Procédé de Gram-Schmidt]    \label{PropUMtEqkb}
	Un espace euclidien possède une base orthonormée.
\end{proposition}
\index{espace!euclidien}
\index{Gram-Schmidt}

\begin{proof}
	Soit \( E\) un espace euclidien et \( \{ v_1,\ldots, v_n \}\), une base quelconque de \( E\). Nous posons d'abord
	\begin{equation}
		\begin{aligned}[]
			f_1 & =v_1, & e_1 & =\frac{ f_1 }{ \| f_1 \| }.
		\end{aligned}
	\end{equation}
	Ensuite
	\begin{equation}
		\begin{aligned}[]
			f_2 & =v_2-\langle v_2, e_1\rangle e_1, & e_2 & =\frac{ f_2 }{ \| f_2 \| }.
		\end{aligned}
	\end{equation}
	Notons que \( \{ e_1,e_2 \}\) est une base de \( \Span\{ v_1,v_2 \}\). De plus elle est orthogonale :
	\begin{equation}
		\langle e_1, f_2\rangle =\langle e_1, v_2\rangle -\langle v_2, e_1\rangle \underbrace{\langle e_1, e_1\rangle}_{=1} =0.
	\end{equation}
	Le fait que \( \| e_1 \|=\| e_2 \|=1\) est par construction. Nous avons donc donné une base orthonormée de \( \Span\{ v_1,v_2 \}\).

	Nous continuons par récurrence en posant
	\begin{equation}
		\begin{aligned}[]
			f_k & =v_k-\sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i, & e_k & =\frac{ f_k }{ \| f_k \| }.
		\end{aligned}
	\end{equation}
	Pour tout \( j<k\) nous avons
	\begin{equation}
		\langle e_j, f_k\rangle =\langle e_j, v_k\rangle -\sum_{i=1}^{k-1}\langle v_k, e_i\rangle \underbrace{\langle e_i, e_j\rangle}_{=\delta_{ij}} =0
	\end{equation}
\end{proof}
Cet algorithme de Gram-Schmidt nous donne non seulement l'existence de bases orthonormée pour tout espace euclidien, mais aussi le moyen d'en construire à partir de n'importe quelle base.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Pseudo-réduction simultanée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{corollary}[Pseudo-réduction simultanée\cite{JMYQgLO}]  \label{CorNHKnLVA}
	Soient \( A,B\in \gS(n,\eR)\) avec \( A\) définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.}. Alors il existe \( Q\in \GL(n,\eR)\) telle que \( Q^tBQ\) soit diagonale et \( Q^tAQ=\mtu\).
\end{corollary}

\begin{proof}
	Nous allons noter \( x\cdot y\) le produit scalaire usuel de \( \eR^n\) et \( \{ e_i \}_{i=1,\ldots, n}\) sa base canonique.

	Comme \( A\) est définie positive, l'expression \( \langle x, y\rangle =x\cdot Ay\) donne un produit scalaire sur \( \eR^n\). Nous avons donc deux produits scalaires sur \( \eR^n\), et nous allons travailler avec les deux.

	La proposition \ref{PropUMtEqkb} appliquée à l'espace euclidien \( (\eR^n,\langle ., .\rangle )\) dit qu'il existe une base de \( \eR^n\) orthonormée \( (f_i )_{i=1,\ldots, n}\) pour ce produit scalaire. Nous considérons l'application linéaire \( P\) définie par
	\begin{equation}
		Pe_i=f_i.
	\end{equation}

	Nous démontrons à présent que \( P^tAP=\mtu\). Pour cela, nous calculons
	\begin{subequations}
		\begin{align}
			\delta_{ij} & =\langle f_i, f_j\rangle    \label{SUBEQooGZDJooVMuWNn} \\
			            & =f_i\cdot Af_j                                          \\
			            & =Pe_i\cdot APe_j                                        \\
			            & =e_i\cdot P^tAPe_j      \label{SUBEQooQNVUooNbyIzM}     \\
			            & =(P^tAP)_{ij}.          \label{SUBEQooITBKooCEmqxx}
		\end{align}
	\end{subequations}
	Justifications :
	\begin{itemize}
		\item Pour \eqref{SUBEQooGZDJooVMuWNn}, la base \( (f_j)\) est orthonormée pour le produit scalaire \( \langle ., .\rangle \).
		\item Pour \eqref{SUBEQooQNVUooNbyIzM}, la proposition \ref{PROPooNARVooEuhweD} sur la transposée.
		\item Pour \eqref{SUBEQooITBKooCEmqxx}, la formule du produit scalaire usuel pour avoir les éléments de matrice, proposition \ref{PROPooZKWXooWmEzoA}.
	\end{itemize}
	La matrice \( P^tBP\) est une matrice symétrique, donc le théorème spectral~\ref{ThoeTMXla} nous donne une matrice \( R\in \gO(n,\eR)\) telle que \( R^tP^tBPR\) soit diagonale. En posant maintenant \( Q=PR\) nous avons la matrice cherchée.
\end{proof}

\begin{remark}      \label{REMooDDUEooJXZIfE}
	Plusieurs remarques :
	\begin{enumerate}
		\item
		      Nous n'avons pas prouvé l'existence d'une matrice \( P\) telle que \( P^{-1}BP\) et \( P^{-1}AP\) soient diagonales. Au contraire, nous avons \( Q^tBQ\) et \( Q^tAQ\) qui sont diagonales. Tant que \( Q\) n'est pas orthogonale, ce n'est pas la même chose.

		      Autrement dit, nous n'avons pas ici une vraie diagonalisation, parce que les matrices \( A\) et \( B\) ne sont pas semblables à des matrices diagonales. Voir les définitions~\ref{DefCNJqsmo} (diagonalisable) et~\ref{DefCQNFooSDhDpB} (semblable).

		      C'est pour cela que nous parlons de \emph{pseudo}-diagonalisation.
		\item
		      Dans le même ordre d'idée, la démonstration de la pseudo-diagonalisation simultanée parle clairement de formes bilinéaires, et non d'endomorphismes. Or en comparant les lois de transformations \eqref{ooWKTYooOJfclT} et \eqref{EQooZUVTooKjqnJj}, nous voyons bien que la réduction en passant par \( Q^tAQ\) est bien une réduction de forme bilinéaire et non une réduction d'endomorphismes.
		\item
		      Nous avons prouvé la pseudo-réduction simultanée comme corolaire du théorème de diagonalisation des matrices symétriques~\ref{ThoeTMXla}. Il aurait aussi pu être vu comme un corolaire du théorème spectral~\ref{ThoRSBahHH} sur les opérateurs autoadjoints via son corolaire~\ref{CorSMHpoVK}.
	\end{enumerate}
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Approximations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Le lemme suivant est surtout intéressant en dimension infinie.
\begin{lemma}
	Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); il existe une suite \( (v_n)\) dans \( A\) telle que \( v_n\stackrel{V}{\longrightarrow}v\) et \( \| v_n \|\leq \| v \|\) pour tout \( n\).
\end{lemma}

\begin{proof}
	Puisque \( A\) est dense, il existe une suite \( a_n\) dans \( A\) telle que \( a_n\to v\). Ensuite il suffit de poser
	\begin{equation}
		v_n=\frac{ n }{ n+1 }\frac{ \| v \| }{ \| a_n \| }a_n.
	\end{equation}
	Par construction nous avons toujours
	\begin{equation}
		\| v_n \|=\frac{ n }{ n+1 }\| v \|\leq \| v \|.
	\end{equation}
	Et de plus, la norme étant continue\footnote{Où dans le calcul suivant utilisons-nous la continuité de la norme ? Posez-vous la question.},
	\begin{equation}
		\lim_{n\to \infty} v_n=\lim_{n\to \infty} \frac{ n }{ n+1 }\lim_{n\to \infty} \frac{ \| v \| }{ \| a_n \| }\lim_{n\to \infty} a_n=v.
	\end{equation}

	Le fait que \( v_n\) soit dans \( A\) est dû au fait que \( A\) soit vectoriel.
\end{proof}

\begin{proposition}     \label{PROPooVEMGooYKhMFy}
	Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\). Nous avons
	\begin{equation}
		\sup\{ | v\cdot a |\tq a\in A\text{ et }\| a \|\leq \lambda \}=\lambda\| v \|.
	\end{equation}
\end{proposition}

\begin{proof}
	D'abord pour tout \( a\in A\) vérifiant \( \| a \|\leq \lambda\) l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG} donne
	\begin{equation}
		| v\cdot a |\leq \| v \|\| a \|\leq \lambda\| v \|.
	\end{equation}
	Donc le supremum dont on parle est majoré par \( \lambda\| v \|\).

	Il nous faut l'inégalité dans l'autre sens. Par densité nous pouvons choisir une suite \( v_n\in A\) tel que \( v_n\to v\). Ensuite nous posons
	\begin{equation}
		a_n=\frac{ \lambda }{ \| v_n \| }v_n.
	\end{equation}
	Nous avons \( \| a_n \|=\lambda\) pour tout \( n\) et
	\begin{equation}
		| v\cdot a_n |=\frac{ \lambda }{ \| v_n \| }| v\cdot v_n |,
	\end{equation}
	et en passant à la limite,
	\begin{equation}
		\lim_{n\to \infty} | v\cdot a_n |=\frac{ \lambda }{ \| v \| }\| v\cdot v \|=\lambda\| v \|.
	\end{equation}
	Donc l'ensemble sur lequel nous prenons le supremum contient une suite convergente vers \( \lambda\| v \|\). Le supremum est donc au moins aussi grand que cela.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques exemples de normes sur \texorpdfstring{\( \eR^n\)}{Rn}}
%---------------------------------------------------------------------------------------------------------------------------

Il est possible de définir de nombreuses normes sur \( \eR^n\). Citons-en quelques-unes parmi les normes \( \| . \|_p\). Le cas général \( p\geq 1\) sera fait dans \ref{PROPooUDFTooQyhAtq}.

\begin{propositionDef}[\cite{BIBooHVNIooHdTaQN,BIBooXHRKooZrKDcs}]      \label{PROPooCLZRooIRxCnZ}
	Les formules suivantes définissent des normes\footnote{Définition \ref{DefNorme}.} sur \( \eR^n\).
	\begin{enumerate}
		\item       \label{ITEMooQBLGooPQKSev}
		      \( \| x \|_1=\sum_{i=1}^n| x_i |\),
		\item       \label{ITEMooXQUFooLHrITI}
		      \( \| x \|_2=\sqrt{ \sum_{i=1}^nx_i^2 }\),
		\item       \label{ITEMooSOVDooTuhEik}
		      \( \| x \|_{\infty}=\max_i| x_i |\).
	\end{enumerate}
	La norme \( \| . \|_{\infty}\) est nommée \defe{norme supremum}{norme!supremum}.
\end{propositionDef}

\begin{proof}
	Point par point.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooQBLGooPQKSev}]
		Déjà fait dans le lemme \ref{LEMooRWJYooOIJkZc}.
		\spitem[Pour \ref{ITEMooXQUFooLHrITI}]
		Le cas \( p=2\) provient de l'inégalité
		\begin{equation}
			\sqrt{ (a+b)^2 }\leq \sqrt{ a^2 }+\sqrt{ b^2 },
		\end{equation}
		laquelle se démontre en passant au carré :
		\begin{equation}        \label{EQooRYNYooTzZpPz}
			(a+b)^2=a^2+b^2+2ab\leq a^2+b^2+2| ab |=\big( \sqrt{ a^2 }+\sqrt{ b^2 } \big)^2.
		\end{equation}
		\spitem[Pour \ref{ITEMooSOVDooTuhEik}]
		Pour l'inégalité triangulaire, nous faisons
		\begin{equation}
			\| x+y \|_{\infty}=\max_i| x_i+y_i |\leq \max_i\big( | x_i |+| y_i | \big)\leq\max_i| x_i |+\max_i| y_i |=\| x \|_{\infty}+\| y \|_{\infty}.
		\end{equation}
		Les autres points de la définition \ref{DefNorme} sont faciles quand on se rappelle que \( x=0\) si et seulement si \( x_i=0\) pour tout \( i\).
	\end{subproof}
\end{proof}

Parmi ces normes, celles qui seront le plus souvent utilisées sont les normes \( \| . \|_1\) et \( \| . \|_2\) qui, pour des raisons que nous verrons beaucoup plus tard\footnote{La proposition \ref{PROPooTYCYooAKJWOX}, par exemple.} sont souvent notées \( \|.  \|_{L^p}\) :
\begin{equation}
	\begin{aligned}[]
		\| x \|_{L^1} & =\sum_{i=1}^n| x_i |,                     \\
		\| x \|_{L^2} & =\Big( \sum_{i=1}^n| x_i |^2 \Big)^{1/2}.
	\end{aligned}
\end{equation}

\newcommand{\CaptionFigDistanceEuclide}{La \emph{norme} euclidienne induit la \emph{distance} euclidienne. D'où son nom. Le point \( C\) est construit aux coordonnées \( (A_x,B_y)\).}
\input{auto/pictures_tex/Fig_DistanceEuclide.pstricks}

Soient \( A=(A_x,A_y)\) et \( B=(B_x,B_y)\) deux éléments de \( \eR^2\). La distance\footnote{Ne pas confondre «distance» et «norme».} euclidienne entre \( A\) et \( B\) est donnée par \( \| A-B \|_2\). En effet, sur la figure~\ref{LabelFigDistanceEuclide}, la distance entre les points \( A\) et \( B\) est donnée par
\begin{equation}
	| AB |^2=| AC |^2+| CB |^2=| A_x-B_x |^2+| A_y-B_y |^2,
\end{equation}
par conséquent,
\begin{equation}
	| AB |=\sqrt{| A_x-B_x |^2+| A_y-B_y |^2}=\| A-B \|_2.
\end{equation}

\begin{remark}
	Si \( A\), \( B\) et \( C\) sont trois points dans le plan \( \eR^2\), alors l'inégalité triangulaire \( | AB |\leq| AC |+| CB |\) est précisément la propriété~\ref{ItemDefNormeiii} de la norme (définition~\ref{DefNorme}). En effet l'inégalité triangulaire s'exprime de la façon suivante en termes de la norme \( \| . \|_2\) :
	\begin{equation}    \label{EqNDeuxAmBNNdd}
		\| A-B \|_2\leq \| A-C \|_2+\| C-B \|_2.
	\end{equation}
	En notant \( u=A-C\) et \( v=C-B\), l'équation \eqref{EqNDeuxAmBNNdd} devient exactement la propriété de définition de la norme :
	\begin{equation}
		\| u+v \|_2\leq \| u \|_2+\| v \|_2.
	\end{equation}
	Ceci explique pourquoi cette propriété des normes est appelée «inégalité triangulaire».
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équivalence des normes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{normes_equiv}

Au premier coup d'œil, les notions dont nous parlons dans ce chapitre ont l'air très générales. Nous prenons en effet n'importe quel espace vectoriel \( V\) de dimension finie, et nous le munissons de n'importe quelle norme. Rien que dans \( \eR^m\) nous allons en définir une infinité par l'équation \eqref{EqDeformeLp}). À partir de ces données, nous définissons les boules, la topologie, l'adhérence, etc.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{En dimension finie}
%---------------------------------------------------------------------------------------------------------------------------

Dans \( \eR^n\), les normes \( \| . \|_{L^1}\), \( \| . \|_{L^2}\) et \( \| . \|_{\infty}\) ne sont pas égales. Cependant elles ne sont pas complètement indépendantes au sens où l'on sent bien que si un vecteur sera grand pour une norme, il sera également grand pour les autres normes; les normes «vont dans le même sens». Cette notion est précisée par le concept de norme équivalente.

\begin{propositionDef}[\cite{BIBooTTHLooHrHjvc}]      \label{DefEquivNorm}\label{LEMooHAITooWdtLAN}
	Soient deux normes \( N_1\) et \( N_2\) sur l'espace vectoriel \( E\) de dimension finie. Nous disons que \( N_1\sim N_2\) si et seulement si il existe des réels strictement positifs \( k_1\), \( k_2\) tels que
	\begin{equation}
		k_1N_1(x)\leq N_2(x)\leq k_2 N_1(x).
	\end{equation}

	La relation \( \sim\) est une relation d'équivalence\footnote{Définition \ref{DefHoJzMp}.} sur l'ensemble des normes de \( E\).


	Si \( N_1\sim N_2\) nous disons que les normes \( N_1\) et \( N_2\) sur \( \eR^m\) sont \defe{équivalentes}{normes équivalentes}\index{équivalence de normes}.
\end{propositionDef}


\begin{proof}
	Vu que tous les nombres sont strictement positifs, nous pouvons multiplier et diviser sans changer le sens des inégalités. Gardant cela en tête, nous faisons les trois vérifications.
	\begin{subproof}
		\spitem[réflexive]
		Nous avons \( N_1\sim N_1\) avec \( k_1=k_2=1\).
		\spitem[Symétrique]
		Supposons avoir \( k_1,k_2\) tels que \( k_1N_1(x)\leq N_2(x)\leq k_2N_1(x)\). Alors nous avons aussi
		\begin{equation}
			\frac{1}{ k_2 }N_2(x)\leq N_1(x)\leq \frac{ 1 }{ k_1 }N_2(x).
		\end{equation}
		\spitem[Transitive]
		% -------------------------------------------------------------------------------------------- 
		Supposons \( N_1\sim N_2\) et \( N_2\sim N_3\). Nous avons donc des nombres \( k_1\), \( k_2\), \( l_1\) et \( l_2\) tels que
		\begin{equation}
			\begin{aligned}[]
				k_1N_1(x) & \leq N_2(x)     \\
				N_2(x)    & \leq k_2N_1(x)  \\
				l_1N_2(x) & \leq N_3(x)     \\
				N_3(x)    & \leq l_2N_2(x).
			\end{aligned}
		\end{equation}
	\end{subproof}
	En combinant,
	\begin{equation}
		k_1l_1N_1(x)\leq l_1N_2(x)\leq N_3(x)\leq l_2N_2(x)\leq l_2k_2N_1(x).
	\end{equation}
	En particulier, $k_1l_1N_1(x) \leq N_3(x)\leq l_2k_2N_1(x)$.
\end{proof}

\begin{proposition} \label{PropLJEJooMOWPNi}
	Pour \( \eR^n\), nous avons les équivalences de normes \( \| . \|_{L^1}\sim\| . \|_{L^2}\), \( \| . \|_{L^1}\sim\| . \|_{\infty}\) et \( \| . \|_{L^2}\sim\| . \|_{\infty}\). Plus précisément nous avons les inégalités\footnote{Les racines carrés sont définies en \ref{DEFooGQTYooORuvQb}.}
	\begin{enumerate}
		\item\label{ItemABSGooQODmLNi}
		      \(  \| x \|_2\leq \| x \|_1\leq\sqrt{n}\| x \|_2\)
		\item\label{ItemABSGooQODmLNii}
		      \( \| x \|_{\infty}\leq \| x \|_1\leq n \| x \|_{\infty}\)
		\item\label{ItemABSGooQODmLNiii}
		      \( \| x \|_{\infty}\leq \| x \|_2\leq \sqrt{n}\| x \|_{\infty}\)
	\end{enumerate}
\end{proposition}

\begin{proof}
	En mettant au carré la première inégalité nous voyons que nous devons vérifier l'inégalité
	\begin{equation}
		| x_1 |^2+\cdots+| x_n |^2\leq\big( | x_1 |+\cdots+| x_n | \big)^2
	\end{equation}
	qui est vraie parce que le membre de droite est égal au carré de chaque terme plus les doubles produits. La seconde inégalité provient de l'inégalité de Cauchy-Schwarz (théorème~\ref{ThoAYfEHG}) sur les vecteurs
	\begin{equation}
		\begin{aligned}[]
			v & =\begin{pmatrix}
				     1/n    \\
				     \vdots \\
				     1/n
			     \end{pmatrix},
			  & w                & =\begin{pmatrix}
				                        | x_1 | \\
				                        \vdots  \\
				                        | x_n |
			                        \end{pmatrix}.
		\end{aligned}
	\end{equation}
	Nous trouvons
	\begin{equation}
		\frac{1}{ n }\sum_i| x_i |\leq\sqrt{n\cdot\frac{1}{ n^2 }}\sqrt{\sum_i| x_i |^2},
	\end{equation}
	et par conséquent
	\begin{equation}
		\sum_i| x_i |\leq\sqrt{n}\| x \|_2.
	\end{equation}

	La première inégalité de~\ref{ItemABSGooQODmLNiii} se démontre en remarquant que si \( a\) et \( b\) sont positifs, \( a\leq\sqrt{a^2+b}\). En appliquant cela à \( a=\max_i| x_i |\), nous avons
	\begin{equation}
		\max_i| x_i |\leq\sqrt{ | x_1 |^2+\cdots+| x_n |^2  }
	\end{equation}
	parce que \( \max_i| x_i |\) est évidemment un des termes de la somme. Pour la seconde inégalité de~\ref{ItemABSGooQODmLNiii}, nous avons
	\begin{equation}
		\sqrt{\sum_k| x_k |^2}\leq\left( \sum_k\max_i| x_i |^2 \right)^{1/2}=\sqrt{n}\| x \|_{\infty}.
	\end{equation}
	Pour obtenir cette inégalité, nous avons remplacé tous les termes \( | x_k |\) par le maximum.
\end{proof}

Pour les autres normes \( \| . \|_p\), il y a des inégalités dans \ref{THOooPPDPooJxTYIy} et \ref{CORooMBQMooWBAIIH}; voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.

Une dernière avant l'équivalence de toutes les normes.
\begin{propositionDef}
	Les topologies suivantes sont égales sur \( \eR^n\).
	\begin{enumerate}
		\item       \label{ITEMooWACPooFBAWhx}
		      La topologie produit \( \eR\times \ldots\times \eR\) des espaces topologiques \( (\eR,| . |)\),
		\item       \label{ITEMooJPJHooGTuLen}
		      La topologie de la norme
		      \begin{equation}
			      \| (x_1,\ldots, x_n) \|_{\infty}=\max_i\{ | x_i | \},
		      \end{equation}
		\item       \label{ITEMooEBYQooXiOOtb}
		      La topologie de la norme
		      \begin{equation}
			      \| (x_1,\ldots, x_n) \|_2=\sqrt{ \sum_{i=1}^nx_i^2 }.
		      \end{equation}
	\end{enumerate}
	Elle est la topologie que nous allons toujours considérer sur \( \eR^n\) (sauf mention très explicite du contraire).
\end{propositionDef}

\begin{proof}
	Les topologies \ref{ITEMooWACPooFBAWhx} et \ref{ITEMooJPJHooGTuLen} sont identiques par le lemme \ref{LEMooFQMSooLmdIvD}. Les topologies \ref{ITEMooJPJHooGTuLen} et \ref{ITEMooEBYQooXiOOtb} sont identiques par la proposition \ref{PropLJEJooMOWPNi}.
\end{proof}

En réalité, toutes les normes \( \| . \|_{L^p}\) et \( \| . \|_{\infty}\) sont équivalentes et, plus généralement, en dimension finie, toutes les normes sont équivalentes.
\begin{theorem}[\cite{TrenchRealAnalisys,BIBooBEINooZqnSSN}]    \label{ThoNormesEquiv}
	Sur un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
\end{theorem}

\begin{proof}
	Soit un espace vectoriel \( V\) de dimension finie. Nous allons montrer que toutes les normes sont équivalentes à la norme \( \| . \|_{\infty}\). Soit donc une norme \( N\) sur \( V\).

	\begin{subproof}
		\spitem[Inégalité dans un sens]
		% -------------------------------------------------------------------------------------------- 
		Vu que \( V\) est de dimension finie, il accepte une base \( \{ e_1,\ldots, e_n \}\). En posant \( D=\sum_{i=1}^nN(e_i)\), nous avons
		\begin{equation}
			N(x)=N(\sum_ix_ie_i)\leq \sum_i|x_i|N(e_i)\leq n\| x \|_{\infty}D.
		\end{equation}
		Nous avons donc l'inégalité\quext{Dans \ref{ThoNormesEquiv}, le \( n\) n'apparaît pas dans la majoration. C'est lui ou moi qui fait une erreur ? Pourquoi ?}
		\begin{equation}
			N(x)\leq nD\| x \|_{\infty}.
		\end{equation}
		\spitem[Dans l'autre sens]
		% -------------------------------------------------------------------------------------------- 
		La proposition \ref{PropNmNNm} donne
		\begin{equation}
			\big| N(x)-N(y) \big|\leq N(x-y)\leq Dn\| x-y \|_{\infty}.
		\end{equation}
		Donc l'application \( N\colon (V,\| . \|_{\infty})\to \eR \) est continue. Vu que la sphère
		\begin{equation}
			S=\{ x\in V\tq \| x \|_{\infty}=1 \}
		\end{equation}
		est compacte\footnote{La partie \( S\) est fermée et bornée de \( (V,\| . \|_{\infty})\), voir le théorème \ref{ThoXTEooxFmdI}.}, la fonction continue \( N\) y prend un minimum\footnote{Théorème de Weierstrass \ref{ThoWeirstrassRn}.}. Il existe donc \( m>0\) tel que \( m\leq N(x)\) pour tout \( x\in S\).

		Soit \( x\in V\). Nous avons \( x/\| x \|_{\infty}\in S\), de telle sorte que
		\begin{equation}
			m\leq N(\frac{ x }{ \| x \|_{î} })=\frac{1}{ \| x \|_{\infty} }N(x),
		\end{equation}
		ou encore \( N(x)\geq m\| x \|_{\infty}\).
		\spitem[Conclusion]
		% -------------------------------------------------------------------------------------------- 
		Pour tout \( x\in V\) nous avons les inégalités
		\begin{equation}
			m\| x \|_{\infty}\leq N(x)\leq nD\| x \|_{\infty}.
		\end{equation}
		Donc toutes les normes sont équivalentes à la norme \( \| . \|_{\infty}\).

		Comme l'équivalence de norme est transitive, toutes les normes sont équivalentes.
	\end{subproof}
\end{proof}

\begin{corollary}       \label{CORooBRDYooLmGJDE}
	Soit \( V\) un espace vectoriel de dimension finie et \( \| . \|_1\), \( \| . \|_2\) deux normes sur \( V\). Alors l'identité \( i\colon V\to V\) est un isomorphisme d'espace topologique \( (V,\| . \|_1)\to (V,\| . \|_2)\).

	De plus les ouverts sont les mêmes : une partie de \( V\) est ouverte dans \( (V,\| . \|_1)\) si et seulement si elle est ouverte dans \( (V,\| . \|_2)\).
\end{corollary}

\begin{proof}
	Le théorème \ref{ThoNormesEquiv} nous dit qu'il existe des nombres \( \alpha,\beta>0\) tels que
	\begin{equation}
		\alpha\| x \|_1\leq \| x \|_2\leq \beta\| x \|_1
	\end{equation}
	pour tout \( x\in V\). Soit un 2-ouvert \( \mO\). Nous prouvons que \( i^{-1}(\mO)=\mO\) est un 1-ouvert. Pour cela, soit \( a\in\mO\).  Vu que \( \mO\) est un \( 2\)-ouvert, il existe \( r>0\) tel que \( B_2(a,r)\subset\mO\). Prouvons que \( B_1(a,r/\beta)\subset B_2(a,r)\). En effet si \( y\in B_1(a,r/\beta)\), alors
	\begin{equation}
		\| y-a \|_2\leq \beta\| y-a \|_1\leq r.
	\end{equation}
	Nous avons donc
	\begin{equation}
		B_1(a,r/\beta)\subset B_2(a,r)\subset\mO=i^{-1}(\mO).
	\end{equation}
	Donc \( i^{-1}(\mO)\) contient un 1-ouvert autour de chacun de ses points. Il est donc 1-ouvert.

	Pour prouver que \( i^{-1}\) est continue, c'est la même chose.
\end{proof}


\begin{normaltext}      \label{NORMooNKBCooKziIjx}
	Le lemme \ref{LEMooRWJYooOIJkZc} donnera une norme sur \( \eR^2\) qui ne dérive pas d'un produit scalaire. Vu que toutes les normes sur \( \eR^2\) produisent la même topologie (c'est le corolaire~\ref{CORooBRDYooLmGJDE}), il y a parfaitement moyen pour deux espaces vectoriels topologiques d'être isomorphes alors que l'un a une norme dérivant d'un produit scalaire et l'autre non.
\end{normaltext}

\begin{normaltext}
	Le théorème d'équivalence de norme sera utilisé pour montrer que l'ensemble des formes quadratiques non dégénérées de signature \( (p,q)\) est ouvert dans l'ensemble des formes quadratiques, proposition~\ref{PropNPbnsMd}. Plus généralement il est utilisé à chaque fois que l'on fait de la topologie sur les espaces de matrices en identifiant \( \eM(n,\eR)\) à \( \eR^{n^2}\), pour se rassurer en se disant que ce qu'on fait ne dépend pas de la norme choisie.

	Voir aussi ce qu'on en fait en \ref{NORMooDAZZooDiGFoW} pour démontrer la différentiabilité à partir des dérivées partielles.
\end{normaltext}

\begin{proposition}[\cite{MonCerveau}] \label{PROPooNTCFooEcwZwt}
	Soit un espace vectoriel \( V\) de dimension finie sur \( \eC\). Pour une base \( B= \{ e_i \}\) de \( V\) nous définissons
	\begin{equation}        \label{EQooEGXVooLASQIC}
		\| \sum_kv_ke_k \|_B= \sqrt{ \sum_k| v_k |^2 }.
	\end{equation}
	\begin{enumerate}
		\item
		      La formule \eqref{EQooEGXVooLASQIC} définit une norme sur \( V\).
		\item
		      Si \( B\) et \( B'\) sont des bases de \( V\), alors les topologies induites par le norme \( \| . \|_B\) et \( \| . \|_{B'}\) sont égales.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous commençons par fixer une base \( B=\{ e_i \}_{i=1,\ldots, n}\) de \( V\). Cette base nous permet de définir
	\begin{equation}
		\begin{aligned}
			\varphi\colon V & \to \eC^n                  \\
			\sum_kv_ke_k    & \mapsto (v_1,\ldots, v_n).
		\end{aligned}
	\end{equation}
	Cette application linéaire permet d'écrire
	\begin{equation}
		\| v \|_V=\| \varphi(v) \|_{\eC^n}.
	\end{equation}
	À partir de là, la vérification des propriétés de la définition \ref{DefNorme} est immédiate. Par exemple :
	\begin{equation}
		\| v+w \|=\| \varphi(v+w) \|=\| \varphi(v)+\varphi(w) \|\leq \| \varphi(v) \|+\| \varphi(w) \|=\| v \|+\| w \|.
	\end{equation}

	En ce qui concerne la seconde assertion, c'est le théorème \ref{ThoNormesEquiv}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Contre-exemple en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecPOlynomesCE}

Lorsque nous considérons des espaces vectoriels de dimension infinie, les choses ne sont plus aussi simples. Nous voyons ici sur l'exemple de l'espace des polynômes que le théorème~\ref{ThoNormesEquiv} n'est plus valable si on enlève l'hypothèse de dimension finie.

On considère l'ensemble des fonctions polynomiales à coefficients réels sur  l'intervalle \( [0,1]\).
\begin{equation}
	\mathcal{P}_\eR([0,1])=\{p:[0,1]\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots, \, a_i\in\eR,\,\forall i\in \eN\}.
\end{equation}
Cet ensemble, muni des opérations usuelles de somme entre polynômes et multiplications par les scalaires, est un espace vectoriel.

Sur \( \mathcal{P}(\eR)\) on définit les normes suivantes
\begin{equation}
	\begin{aligned}
		 & \|p\|_\infty=\sup_{x\in[0,1]}\{p(x)\},             \\
		 & \|p\|_1 =\int_0^1|p(x)|\, dx,                      \\
		 & \|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}. \\
	\end{aligned}
\end{equation}
Les inégalités suivantes sont  immédiates
\begin{equation}
	\begin{aligned}
		 & \|p\|_1 =\int_0^1|p(x)|\, dx\leq \|p\|_\infty,                      \\
		 & \|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}\leq \|p\|_\infty, \\
	\end{aligned}
\end{equation}
mais la norme \( \|\cdot\|_\infty\) n'est  équivalente ni à \( \|\cdot\|_1\), ni à \( \|\cdot\|_2\). Soit \( p_k(x)= x^k\). Alors
\begin{equation}
	\begin{aligned}
		 & \|p_k\|_\infty=1,                                                        \\
		 & \|p_k\|_1 =\int_0^1x^k\, dx=  \frac{1}{k+1},                             \\
		 & \|p_k\|_2 =\left(\int_0^1x^{2k}\, dx\right)^{1/2}=\sqrt{\frac{1}{2k+1}}.
	\end{aligned}
\end{equation}
Pour \( k\to \infty\) les normes \( \|p_k\|_1\), \( \|p_k\|_2\) tendent vers zéro, alors que la norme \( \|p_k\|_\infty\) est constante, donc les normes ne sont pas équivalentes parce que il n'existe pas un nombre positif \( m\) tel que
\begin{equation}
	\begin{aligned}
		 & m \|p_k\|_\infty\leq \|p_k\|_1 , \\
		 & m \|p_k\|_\infty\leq \|p_k\|_2 , \\
	\end{aligned}
\end{equation}
uniformément pour tout \( k\) dans \( \eN\).

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooILSOooPdUyFu}
	Soit un espace vectoriel normé \( E\) de dimension \( n\) ainsi qu'un ouvert \( U\subset  E\). Il existe un ouvert \( V\) de \( \eR^n\) et une bijection isométrique \(g \colon V\to U  \).
	%TODOooNRMJooYKKREE. Prouver ça.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Norme opérateur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La proposition suivante donne une norme (au sens de la définition~\ref{DefNorme}) sur \( \aL(V,W)\) dès que \( V\) et \( W\) sont des espaces vectoriels normés.
\begin{definition}[Norme opérateur\cite{ooTZRDooWmjBJi}, thème \ref{THEMEooOJJFooWMSAtL}]          \label{DefNFYUooBZCPTr}
	Soient des espaces vectoriels normés \( (V,\| . \|_V)\) et \( (W,\| . \|_W)\). Soient une application linéaire \( T\colon V\to W\), et le nombre
	\begin{equation}
		\|T\|_{\aL}=\sup_{\substack{x\in V\\x\neq 0}}\frac{\|T(x)\|_{W}}{\|x\|_{V}}.
	\end{equation}

	Le nombre \( \| T \|_{\aL}\) est la \defe{norme opérateur}{norme!d'application linéaire} de \( T\). Nous disons que cette norme est \defe{subordonnée}{subordonnée!norme} aux normes choisies sur \( V\) et \( W\).

	Si \( \| T \|<\infty\), nous disons que \( T\) est borné\footnote{Nous verrons plus tard que si \( V\) est de dimension infinie, ce n'est pas garanti.}. Nous notons
	\begin{equation}
		\cL(V,W)=\{ T\in\aL(V,W)\tq \| T \|<\infty \}.
	\end{equation}
\end{definition}
\index{norme!d'une application linéaire}

\begin{proposition}	\label{PROPooQMZLooOxBrmt}
	Si \( V\) est de dimension finie, alors \( \| T \|_{\aL}<\infty\) pour tout \( T\in\aL(V,W)\).
\end{proposition}

\begin{proof}
	Si \( V\) est de dimension finie alors l'ensemble \( \{ x\in V \tq \| x \|= 1 \}\) est compact par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}. Alors la fonction
	\begin{equation}
		x\mapsto \frac{ \| T(x) \| }{ \| x \| }
	\end{equation}
	est une fonction continue sur un compact. Le corolaire~\ref{CorFnContinueCompactBorne} nous dit alors qu'elle est bornée. Le supremum est donc un nombre réel fini.
\end{proof}

\begin{proposition}	\label{PROPooZBJXooNDsqax}
	À propos d'opérateurs bornés.
	\begin{enumerate}
		\item
		      La partie \( \cL(V,W)\) des applications linéaires bornées est un espace vectoriel.
		      %TODOooLQWHooPTaZAW. Ce point-ci est encore à prouver
		\item
		      Le couple \( \big( \cL(V,W),\| . \| \big)\) est un espace vectoriel normé\footnote{Espace vectoriel normé, définition \ref{DefNorme}.}.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous vérifions que l'application \( \| . \|\) de \( \aL(V,W)\) dans \( \eR\) ainsi définie est effectivement une norme.
	\begin{enumerate}
		\item
		      \( \|T\|_{\aL}=0\) signifie que \( \|T(x)\|=0\) pour tout \( x\) dans \( V\). Comme  \( \|\cdot\|_W\) est une norme nous concluons que \( T(x)=0\) pour tout \( x\) dans \( V\), donc \( T\) est l'application nulle.
		\item
		      Pour tout \( \lambda\in \eR\) et tout  \( T\) dans \( \aL(V,W)\) nous avons
		      \begin{equation}
			      \| \lambda T \|_{\aL}=\sup_{\substack{x\in V\\x\neq 0}}\frac{ \| (\lambda T)(x) \|_W }{ \| x \|_V }=\sup\frac{ | \lambda |\| T(x) \|_W }{ \| x \| }=| \lambda |\| T \|_{\aL}.
		      \end{equation}
		\item
		      Pour tous \( T_1\) et \( T_2\) dans \( \aL(V,W)\) nous avons
		      \begin{subequations}
			      \begin{align}
				      \|T_1+ T_2\|_{\mathcal{L}} & =\sup_{\substack{x\in V                                                               \\x\neq 0}}\frac{ \| T_1(x)+T_2(x) \| }{ \| x \| }\\
				                                 & \leq \sup\frac{ \| T_1(x)+T_2(x) \| }{ \| x \| }                                      \\
				                                 & =\sup\left( \frac{ \| T_1(x) \| }{ \| x \| }+\frac{ \| T_2(x) \| }{ \| x \| } \right) \\
				                                 & =\leq\sup\frac{ \| T_1(x) \| }{ \| x \| }+\sup\frac{ \| T_2(x) \| }{ \| x \| }        \\
				                                 & =\| T_1 \|+\| T_2 \|.
			      \end{align}
		      \end{subequations}
	\end{enumerate}
\end{proof}

\begin{proposition}	\label{PROPooWHJCooXxCSWp}
	Nous avons la formule
	\begin{equation}    \label{EqFZPooIoecGH}
		\| T \|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}} =\sup_{\|x\|_{V}=1}\|T(x)\|_{W}
	\end{equation}
\end{proposition}

\begin{proof}
	Nous allons montrer que les ensembles sur lesquels ont prend le supremum sont en réalité les mêmes :
	\begin{equation}
		\underbrace{\left\{ \frac{ \| Ax \| }{ \| x \| }\right\}_{x\neq 0}}_{A}=\underbrace{\left\{ \| Ax \|\tq \| x \|=1 \right\}}_{B}.
	\end{equation}
	Attention : ce sont des sous-ensembles de réels; pas de sous-ensembles de \( \eM(\eR)\) ou des sous-ensembles de \( \eR^n\).

	Pour la première inclusion, prenons un élément de \( A\), et prouvons qu'il est dans \( B\). C'est-à-dire que nous prenons \( x\in V\) et nous considérons le nombre \( \| Ax \|/\| x \|\). Le vecteur \( y=x/\| x \|\) est un vecteur de norme \( 1\), donc la norme de \( Ay\) est un élément de \( B\), mais
	\begin{equation}
		\| Ay \|=\frac{ \| Ax \| }{ \| x \| }.
	\end{equation}
	Nous avons donc \( A\subset B\).

	L'inclusion \( B\subset A\) est immédiate.
\end{proof}

%-------------------------------------------------------
\subsection{Matrice bloc-diagonale}
%----------------------------------------------------

Avec un dessin, on comprend bien ce que c'est. À faire réellement, c'est beaucoup de soucis. Mais bon. Nous sommes \randomGender{courageux}{courageuses} et nous nous y collons.


\begin{definition}[Matrice bloc-diagonale\cite{MonCerveau}]	\label{DEFooBPPGooFFMXny}
	Soient des matrices carrées \( B_i\in \eM(n_i,\eK)\). Nous posons : \( n=\sum_in_i\),
	\begin{equation}
		\sigma_k=1+\sum_{i=1}^{k-1}n_i,
	\end{equation}
	et
	\begin{equation}
		s(\alpha)=\max\{ l\in \eN\tq \sigma_l\leq \alpha \}.
	\end{equation}
	En ce qui concerne vos dessins, \( \sigma_k\) est l'endroit dans \( A\) où la matrice \( B_k\) commence et \( s(\alpha)\) est le numéro \( i\) de la matrice \( B_i\) qui se trouve à la position \( \alpha\).

	La matrice \defe{bloc-diagonale}{matrice bloc-diagonale} \( A\) de blocs \( B_i\) est la matrice \( n\times n\) donnée par
	\begin{equation}
		A_{\alpha,\beta}= (B_{s(\alpha)})_{\alpha-\sigma_{s(\alpha)}+1,\beta-\sigma_{s(\alpha)}+1}
	\end{equation}
	si \( 1\leq \alpha-\sigma_{s(\alpha)}+1\leq n_i\) et \( 1\leq \beta-\sigma_{s(\alpha)}+1\leq n_i\) et
	\begin{equation}
		A_{\alpha,\beta}=0
	\end{equation}
	sinon.
\end{definition}

\begin{normaltext}
	Au vu de cette longue définition, je crois qu'il vaut en réalité mieux définie une matrice bloc-diagonale comme étant la matrice d'une application bloc-diagonale.
\end{normaltext}

\begin{definition}[Application bloc-diagonale\cite{MonCerveau}]	\label{DEFooSIAIooFOAPvu}
	Soient des espaces vectoriels \( V_i\). Nous supposons que tous ces espaces sont différents. Nous posons \( V=\bigoplus_iV_i\). Une application linéaire \(f \colon V\to V  \) est \defe{bloc-diagonale}{application bloc-diagonale} si elle peut être écrite sous la forme
	\begin{equation}
		f=\sum_is_i\circ f_i\circ \pr_i
	\end{equation}
	où
	\begin{enumerate}
		\item
		      \(f_i \colon V_i\to V_i  \) est une application linéaire,
		\item
		      \(\pr_i \colon V\to V_i  \) est la projection canonique.
		\item
		      \(s_i \colon V_i\to V  \) est l'inclusion canonique.
	\end{enumerate}
\end{definition}


\begin{lemma}       \label{LEMooHGCKooBzfAtg}
	Si \( f\) est une application bloc-diagonale\footnote{Définition \ref{DEFooSIAIooFOAPvu}.} formée des sous-applications \( (f_i)\), alors \( f^k\) (\( k\in \eZ\)) est une application bloc-diagonale formée des sous-applications \( f_i^k\).
\end{lemma}

\begin{proof}
	Nous avons
	\begin{equation}
		f^2=\sum_{ij}s_i\circ f_i\circ \pr_i\circ s_j\circ f_j\circ \pr_j.
	\end{equation}
	Notez que si \( i\neq j\), alors \( \pr_i\circ s_j=0\). Nous pouvons donc enlever une somme et obtenir
	\begin{equation}
		f^2=\sum_is_i\circ f_i^2\circ\pr_i,
	\end{equation}
	qui est la forme bloc-diagonale que nous voulions.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooJUYCooHnlFef}
	Si \( A\) est une matrice bloc-diagonale, alors la norme de \( A\) majore la norme de chacun de ses blocs.
\end{proposition}

\begin{proof}
	Soient des matrices carrées \( B_i\in \eM(n_i,\eR)\). La matrice \( A\) est celle dont les blocs sont les \( B_i\). Nous fixons \( i\) et nous considérons un vecteur \( u\in \eR^{n_i}\) tel que \( \| u \|=1\) et \( \| B_i \|=\| B_iu \|\).

	Nous considérons aussi l'application \(f_k \colon \eR^{n_k}\to \eR^n  \) qui consiste à ajouter des zéros au-dessus et en-dessous de telle façon à avoir \( A\circ f_k=f_k\circ B_k\). Nous avons alors
	\begin{equation}
		\| A \|\geq \| (A\circ f_i)(u) \|=\| (f_i\circ B_i)(u) \|=\| B_i(u) \|=\| B_i \|.
	\end{equation}
\end{proof}

En d'autres termes, il y a autant de normes opérateur sur \( \aL(E,F)\) qu'il y a de paires de choix de normes sur \( E\) et \( F\). En particulier, cela donne lieu à toutes les normes \( \| A \|_p\) qui correspondent aux normes \( \| . \|_p\) sur \( \eR^n\).

\begin{example}     \label{EXooXPXAooYyBwMX}
	Voyons la norme opérateur subordonnée à la norme \( \| x \|_{\infty}=\max_i| x_i |\) sur \( \eC^n\). Par la proposition \ref{PROPooWHJCooXxCSWp},
	\begin{equation}
		\| A \|_{\infty}=\sup_{\| x \|_{\infty}=1}\| Ax \|_{\infty}.
	\end{equation}
	Vu que \( (Ax)_i=\sum_kA_{ik}x_k\), lorsque \( \| x \|_{\infty}\leq 1\) nous avons \( | (Ax)_i |\leq \sum_k| A_{ik} |\). Donc nous avons toujours
	\begin{equation}        \label{EQooPLCIooVghasD}
		\| A \|_{\infty}\leq \max_i\sum_{k}| A_{ik} |.
	\end{equation}
\end{example}

\begin{definition}
	La \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs est la topologie de la norme opérateur.
\end{definition}
Lorsque nous considérons un espace vectoriel d'applications linéaires, nous considérons toujours\footnote{Sauf lorsque les événements nous forceront à trahir.} dessus la topologie liée à cette norme.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme d'algèbre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Norme d'algèbre\cite{ooTZRDooWmjBJi}]  \label{DefJWRWQue}
	Si \( A\) est une algèbre\footnote{Définition~\ref{DefAEbnJqI}.}, une \defe{norme d'algèbre}{norme!d'algèbre} sur \( A\) est une norme telle que pour toute \( x,y\in A\),
	\begin{equation}
		\| xy \|\leq \| x \|\| y \|.
	\end{equation}
\end{definition}
La norme opérateur est une norme d'algèbre, comme nous le verrons dans le lemme \ref{LEMooFITMooBBBWGI}.

Un des intérêts d'utiliser une norme d'algèbre est que l'on a l'inégalité \( \| x^k \|\leq \| x \|^k \). Cela sera particulièrement utile lors de l'étude des séries entières, voir par exemple~\ref{secEVnZXgf}.

\begin{definition}[\cite{ooYLHAooCzQvoa}]      \label{DEFooEAUKooSsjqaL}
	Le \defe{rayon spectral}{rayon!spectral} d'une matrice carrée \( A\), noté \( \rho(A)\), est défini de la manière suivante :
	\begin{equation}    \label{EQooNVNOooNjJhSS}
		\rho(A)=\max_i|\lambda_i|
	\end{equation}
	où les \( \lambda_i\) sont les valeurs propres de \( A\).
\end{definition}

\begin{normaltext}
	Quelques remarques sur la définition du rayon spectral.
	\begin{itemize}
		\item
		      Même si \( A\) est une matrice réelle, les valeurs propres sont dans \( \eC\). Donc dans \eqref{EQooNVNOooNjJhSS}, \( | \lambda_i |\) est le module dans \( \eC\) de \( \lambda_i\).
		\item
		      Puisque les valeurs propres de \( A\) sont les racines de son polynôme caractéristique (théorème~\ref{ThoWDGooQUGSTL}), il y en a un nombre fini et le maximum est bien défini.
		\item
		      La définition s'applique uniquement pour les espaces de dimension finie.
	\end{itemize}
\end{normaltext}

\begin{lemma}       \label{LEMooIBLEooLJczmu}
	Soient des espaces vectoriels normés \( E\) et \( F\), sur les corps \( \eR\) ou \( \eC\). Pour tout \( A\in \aL(E,F)\), et pour tout \( u\in E\) nous avons la majoration
	\begin{equation}
		\| Au \|\leq \| A \|\| u \|
	\end{equation}
	où la norme sur \( A\) est la norme opérateur subordonnée à la norme sur \( u\).
\end{lemma}

\begin{proof}
	Si \( u\in E\) alors, étant donné que le supremum d'un ensemble est plus grand ou égal à chacun des éléments qui le compose,
	\begin{equation}
		\| A \|=\sup_{x\in E}\frac{ \| Ax \| }{ \| x \| }\geq \frac{ \| Au \| }{ \| u \| },
	\end{equation}
	donc le résultat annoncé : \( \| Au \|\leq \| A \|\| u \|\).
\end{proof}

Le lemme suivant est valable en dimension infinie. Nous en toucherons un mot dans l'exemple \ref{EXooTQPEooRRdddt}.
\begin{lemma}       \label{LEMooWFNXooLyTyyX}
	Soient des espaces vectoriels normés \( E\) et \( F\). Soit \( x\in E\). Alors l'application d'évaluation
	\begin{equation}
		\begin{aligned}
			ev_x\colon \aL(E,F) & \to F        \\
			f                   & \mapsto f(x)
		\end{aligned}
	\end{equation}
	est continue.
\end{lemma}

\begin{proof}
	Si \( x=0\), alors par linéarité de \( f\) nous avons \( ev_0(f)=0\) pour tout \( f\). Donc d'accord pour la continuité.

	Soit une suite convergente \( f_k\stackrel{\aL(E,F)}{\longrightarrow}f\). Nous voulons prouver que \( ev_x(f_k)\stackrel{F}{\longrightarrow}ev_x(f)\), c'est-à-dire que
	\begin{equation}
		\lim_{k\to \infty} \| f_k(x)-f(x) \|=0.
	\end{equation}
	Par hypothèse si \( k\) est grand, alors \( \| f_k-f  \|_{\aL(E,F)}\leq \epsilon\), c'est-à-dire que\footnote{Définition \ref{DefNFYUooBZCPTr} de la norme sur \( \aL(E,F)\).}
	\begin{equation}
		\sup_{y\in E}\frac{ \| f_k(y)-f(y) \| }{ \| y \| }\leq \epsilon.
	\end{equation}
	En particulier pour notre \( x\) nous avons
	\begin{equation}
		\frac{ \| f_k(x)-f(x) \| }{ \| x \| }\leq \epsilon,
	\end{equation}
	c'est-à-dire \( \| f_k(x)-f(x) \|\leq \| x \|\epsilon\). Vu que \( \| x \|\) est une simple constante et que \( \epsilon\) est arbitraire, cela implique \( f_k(x)\to f(x)\).
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Application linéaire continue et bornée}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{normaltext}		\label{NORMooUKURooIkcRoS}
	Quelque notations. Par analogie avec \( C^0\) qui est l'ensemble des applications continues, nous notons \( \cL(E,F)\) l'ensemble des applications linéaires continues de \( E\) vers \( F\). De même, \( \End^0(E)\) est l'ensemble des endomorphismes continus de \( E\).
\end{normaltext}


\begin{lemma}[La norme opérateur est une norme d'algèbre\cite{MonCerveau}]   \label{LEMooFITMooBBBWGI}
	Soient des espaces vectoriels normés \( E\), \( F\) et \( G\). Soient des opérateurs linéaires bornés \( B\colon E\to F\), \( A\colon F\to G\). Alors
	\begin{equation}
		\| A\circ B \|\leq \| A \|\| B \|.
	\end{equation}
	C'est-à-dire que la norme opérateur est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}.}.
\end{lemma}

\begin{proof}
	Nous avons les (in)égalités suivantes :
	\begin{subequations}
		\begin{align}
			\| AB \| & =\sup_{x\in E}\frac{ \| ABx \|_G }{ \| x \|_E } \\
			         & =\sup_{\substack{x\in E                         \\Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\frac{ \| Bx \|_F }{ \| Bx \|_F }\\
			         & =\sup_{\substack{x\in E                         \\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }\frac{ \| Bx \| }{ \| x \| }\\
			         & \leq\underbrace{\sup_{\substack{x\in E          \\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }}_{\leq\| A \|}\underbrace{\sup_{\substack{y\in E\\By\neq 0}}\frac{ \| Bx \| }{ \| y \| }}_{=\| B \|}\\
			         & \leq \| A \|\| B \|.
		\end{align}
	\end{subequations}
	La dernière inégalité provient que dans \( \sup_{\substack{x\in E\\Bx\neq 0}}\| ABx \|/\| x \|\), le supremum est pris sur un ensemble plus petit que celui sur lequel porte la définition de la norme de \( A\) : seulement l'image de \( B\) au lieu de tout l'espace de départ de \( A\).
\end{proof}

La proposition suivante dit qu'une application linéaire est continue si et seulement si elle est bornée. Un résultat similaire pour les applications multilinéaires est dans la proposition \ref{PROPooDQBOooByBvmj}.

\begin{proposition}[Bornée si et seulement si continue\cite{GKPYTMb}]       \label{PROPooQZYVooYJVlBd}
	Soient \( E\) et \( F\) des espaces vectoriels normés. Une application linéaire \( E\to F\) est bornée si et seulement si elle est continue.
\end{proposition}

\begin{proof}
	Nous commençons par supposer que \( A\) est bornée. Par le lemme~\ref{LEMooFITMooBBBWGI}, pour tout \( x,y\in E\), nous avons
	\begin{equation}
		\| A(x)-A(y) \|=\| A(x-y) \|\leq \| A \|\| x-y \|.
	\end{equation}
	En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
	\begin{equation}
		0\leq \| A(x_n)-A(x) \|\leq \| A \|\| x_n-x \|\to 0
	\end{equation}
	et \( A\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition~\ref{PropFnContParSuite}.

	Nous supposons maintenant que \( \| A \|\) n'est pas borné : l'ensemble \( \{ \| A(x) \|\tq \| x \|=1 \}\) contient des valeurs arbitrairement grandes. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| A(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| A(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( A\) n'est pas continue.
\end{proof}


Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que pour une application linéaire, être bornée est équivalent à être continue. Nous allons maintenant voir un certain nombre d'exemples illustrant ce fait.

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
	Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle \( \| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}\) où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]      \label{EXooDMVJooAJywMU}
	En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

	Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

	Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

	Nous avons utilisé le critère séquentiel de la continuité, voir la définition~\ref{DefENioICV} et la proposition~\ref{PropFnContParSuite}.
\end{example}

\begin{remark}  \label{RemOAXNooSMTDuN}
	Cette proposition permet de retrouver l'exemple~\ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{example}     \label{EXooTQPEooRRdddt}
	Nous avons vu dans le lemme \ref{LEMooWFNXooLyTyyX} que pour un \( x\in E\) donné, l'application
	\begin{equation}
		\begin{aligned}
			ev_x\colon \aL(E,F) & \to F        \\
			f                   & \mapsto f(x)
		\end{aligned}
	\end{equation}
	est continue. Puisque \( ev_x\) est linéaire, la proposition \ref{PROPooQZYVooYJVlBd} nous indique que \( ev_x\) est bornée. Vérifions-le directement. Le calcul n'est pas très compliqué :
	\begin{equation}
		\| ev_x \|=\sup_{\| f \|=1}\| ev_x(f) \|=\sup_{\| f \|=1}\| f(x) \|\leq \sup_{\| f \|=1}\| x \|\| f \|=\| x \|
	\end{equation}
	où nous avons utilisé le lemme \ref{LEMooIBLEooLJczmu} en passant. Donc la norme de \( ev_x\) est majorée par \( \| x \|\).

	Elle est même égale à \( \| x \|\). En effet, pour chaque \( f\in \aL(E,F)\) tel que \(  \| f \|=1\), nous avons
	\begin{equation}
		\| ev_x \|\geq \| ev_x(f) \|=\| f(x) \|.
	\end{equation}
	En prenant \( f=\id\) nous trouvons \(  \| ev_x \|\geq \| x \|  \).
\end{example}


%-------------------------------------------------------
\subsection{Déterminant entre espaces différents}
%----------------------------------------------------

Il n'existe à priori pas de façons naturelles de définir le déterminant d'une application linéaire \(f \colon E\to F  \) lorsque \( E\) et \( F\) sont des espaces vectoriels différents\cite{BIBooJLVVooEdGshG}.

\begin{definition}[\cite{MonCerveau}]	\label{DEFooJEOMooEXvnUU}
	Si \(f \colon \eK^n\to \eL^n  \) alors le déterminant de \( f\) est celui de la matrice dans la base canonique.
\end{definition}


\begin{proposition}[\cite{BIBooJLVVooEdGshG, MonCerveau}]	\label{PROPooQCPJooFSzaPc}
	Soient \( E\) et \( F\), deux espaces vectoriels de même dimension. Nous considérons une bijection linéaire \(\varphi \colon E\to F  \), et nous définissons
	\begin{equation}
		\begin{aligned}
			\det_{\varphi}\colon \aL(E,F) & \to \eR                            \\
			f                             & \mapsto \det(\varphi^{-1}\circ f).
		\end{aligned}
	\end{equation}
	\begin{enumerate}
		\item		\label{ITEMooERPIooBvkAPo}
		      L'application \(f \colon E\to F  \) est inversible si et seulement si \( \det_{\varphi}(f)\neq 0\).
		\item		\label{ITEMooTLJEooKyPOVs}
		      Si \( E\) et \( F\) sont des espaces vectoriels normés, alors l'application \(\det_{\varphi} \colon \aL(E,F)\to \eR  \) est continue.
	\end{enumerate}
\end{proposition}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooERPIooBvkAPo}]
		%-----------------------------------------------------------
		Vu que \( \varphi\) est une bijection, nous avons les équivalences
		\begin{subequations}
			\begin{align}
				\text{\( f\) est inversible} & \Leftrightarrow \text{\( \varphi^{-1}\circ f\) est inversible}                                                               \\
				                             & \Leftrightarrow \det(\varphi^{-1}\circ f)\neq 0                & \text{prop. \ref{PropYQNMooZjlYlA}\ref{ITEMooNZNLooODdXeH}} \\
				                             & \Leftrightarrow \det_{\varphi}(f)\neq 0.
			\end{align}
			\spitem[Pour \ref{ITEMooTLJEooKyPOVs}]
			%-----------------------------------------------------------
			Nous posons
			\begin{equation}
				\begin{aligned}
					s\colon \aL(E,F) & \to \aL(E,E)                    \\
					f                & \mapsto \varphi^{-1}\circ f   ,
				\end{aligned}
			\end{equation}
		\end{subequations}
		et nous commençons par prouver que \( s\) est continue. Le lemme \ref{LEMooFITMooBBBWGI} dit que
		\begin{equation}
			\| \varphi^{-1}\circ f \|\leq \| \varphi^{-1} \|\| f \|,
		\end{equation}
		de telle sorte que
		\begin{subequations}
			\begin{align}
				\| s \| & =\sup_{\| f \|=1}\| s(f) \|                    \\
				        & \leq \sup_{\| f \|=1}\| \varphi^{-1} \|\| f \| \\
				        & =\| \varphi^{-1} \|.
			\end{align}
		\end{subequations}
		Donc \( s\) est bornée, et donc elle est continue (proposition \ref{PROPooQZYVooYJVlBd}). Finalement l'application \( \det_{\varphi}=\det\circ s\) est continue comme composée d'applications continues.
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooPNMXooLaJDGc}
	Soient deux espaces vectoriels normés \( E\) et \( F\) de même dimension finie. La partie \( \GL(E,F)\) des applications linéaires inversibles est ouverte dans \( \aL(E,F)\).
	%TODOooCTMKooURnyJN, prouver ça.
\end{proposition}
Pour la preuve, il faudra utiliser la proposition \ref{PROPooQCPJooFSzaPc}.


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Mini bonus}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{normaltext}[\cite{BIBooQLAKooGLbfZf}]        \label{NORMooUDZOooKxAPit}
	Vous vous souvenez de la définition \ref{PropKZDqTR} d'une somme directe topologique ?  Si \( V\) est normé, il existe une façon plus directe (mais pas spécialement plus simple) de prouver l'implication \ref{ITEMooMUELooWdJQeW} \( \Rightarrow\) \ref{ITEMooDKOYooUpEfOR}, et en particulier la continuité de \( s^{-1}\).
	Rappelons que dans le cas normé, nous avons plusieurs façons équivalentes de décrire les topologies\footnote{Voir le thème \ref{THEMEooYRIWooDXZnhX}.}.
	\begin{itemize}
		\item Sur \( V_1\) et \( V_2\) nous avons la topologie induite, définition \ref{DefVLrgWDB}, qui est la même que celle de la restriction de la norme de \( V\) (lemme \ref{LEMooKDMYooMIcFRI}).
		\item La topologie sur \( V_1\times V_2\) est la définition \ref{DefIINHooAAjTdY}. C'est la même que celle de la norme produit par le lemme \ref{LEMooFQMSooLmdIvD}.
		\item La topologie sur \( V/V_1\) est la topologie quotient \ref{DEFooHWSYooZZLXQU}. La proposition \ref{PROPooKLXSooSOUZkc} dit que la norme induite donne la même topologie.
	\end{itemize}

	L'espace \( V_1\times V_2\) est muni de sa norme produit de la définition \ref{LEMooFQMSooLmdIvD} : \( \| (v_1,v_2) \|=\max\{ \| v_1 \|, \| v_2 \| \}\). Par hypothèse \( \psi^{-1}\colon V_1\oplus V_2\to V_1\times V_2\) est linéaire et continue. La proposition \ref{PROPooQZYVooYJVlBd} nous indique qu'elle est alors bornée. Il existe donc un nombre \( C\in \eR^+\) tel que
	\begin{equation}
		\| \psi^{-1}(v_1+v_2) \|=\max\{ \| v_1 \|,\| v_2 \| \}\leq C\| v_1+v_2 \|.
	\end{equation}

	Nous allons montrer que \( s^{-1}\) est séquentiellement continue\footnote{Définition \ref{DefENioICV}. La continuité séquentielle est équivalente à la continuité par la proposition \ref{PropXIAQSXr}.} en partant d'une suite \( \alpha_n\stackrel{V/V_1}{\longrightarrow}0\). En vertu de la proposition \ref{PROPooKLXSooSOUZkc}, nous avons \( d(\alpha_n,V_1)\stackrel{\eR}{\longrightarrow}0\).

	Chacun des \( \alpha_n\) contient un unique élément \( x_n\) de \( V_1\), qui est donné par \( s^{-1}\) : \( x_n= s^{-1}(\alpha_n)\). Nous avons
	\begin{equation}
		\| \alpha_n \|=\inf_{u\in \alpha_n}\| u \|=\inf_{v\in V_1}\| x_n+v \|.
	\end{equation}
	Vu que \( \| \alpha_n \|\) est donné par un infimum, nous pouvons, pour chaque \( n\), choisir \( v\in V_1\) de telle sorte que \( x_n+v_n\) ne soit pas trop loin d'être l'infimum :
	\begin{equation}
		\big| \| x_n+v_n \|-\| \alpha_n \| \big|\leq \frac{1}{ n }.
	\end{equation}
	Un tel choix nous donne une suite \( (v_n)\) dans \( V_1\) telle que
	\begin{equation}
		\big| \| x_n+v_n \|-\| \alpha_n \| \big|\to 0.
	\end{equation}
	Vu que \( \| \alpha_n \|\to 0\), cela implique \( \| x_n+v_n \|\to 0\).

	En remettant tout ensemble,
	\begin{equation}
		\| x_n \|\leq \max\{ \| x_n \|,\| v_n \| \}\leq C\| x_n+v_n \|\to 0.
	\end{equation}
	Donc \( s^{-1}(\alpha_n)=x_n\to 0\), et \( s^{-1}\) est séquentiellement continue.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Suites}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons maintenant parler de suites dans \( V\times W\). Nous noterons \( (v_n,w_n)\) la suite dans \( V\times W\) dont l'élément numéro \( n\) est le couple \( (v_n,w_n)\) avec \( v_n\in V\) et \( w_n\in W\). La notion de convergence de suite découle de la définition de la norme via la proposition \ref{PROPooOSXCooJWXkWH}. Il se fait que dans le cas des produits d'espaces, la convergence d'une suite est équivalente à la convergence des composantes. Plus précisément, nous avons le lemme suivant.
\begin{lemma}       \label{LemCvVxWcvVW}
	La suite \( (v_n,w_n)\) converge vers \( (v,w)\) dans \( V\times W\) si et seulement les suites \( (v_n)\) et \( (w_n)\) convergent séparément vers \( v\) et \( w\) respectivement dans \( V\) et \( W\).
\end{lemma}

\begin{proof}
	Pour le sens direct, nous devons étudier le comportement de la norme de \( (v_n,w_n)-(v,w)\) lorsque \( n\) devient grand. En vertu de la définition de la norme dans \( V\times W\) nous avons
	\begin{equation}
		\Big\| (v_n,w_n)-(v,w) \Big\|_{V\times W}=\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}.
	\end{equation}
	Soit \( \varepsilon>0\). Par définition de la convergence de la suite \( (v_n,w_n)\), il existe un \( N\in\eN\) tel que \( n>N\) implique
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	et donc en particulier les deux inéquations
	\begin{subequations}
		\begin{align}
			\| v_n-v \| & <\varepsilon  \\
			\| w_n-w \| & <\varepsilon.
		\end{align}
	\end{subequations}
	De la première, il ressort que \( (v_n)\to v\), et de la seconde que \( (w_n)\to w\).

	Pour le sens inverse, nous avons pour tout \( \varepsilon\) un \( N_1\) tel que \( \| v_n-v \|_V\leq\varepsilon\) pour tout \( n>N_1\) et un \( N_2\) tel que \( \| w_n-w \|_W\leq\varepsilon\) pour tout \( n>N_2\). Si nous posons \( N=\max\{ N_1,N_2 \}\) nous avons les deux inégalités simultanément, et donc
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	ce qui signifie que la suite \( (v_n,w_n)\) converge vers \( (v,w)\) dans \( V\times W\).
\end{proof}

\begin{proposition}	\label{PROPooPESUooGAmqCW}
	Si $A$ est une partie d'un espace vectoriel normé $V$ et si $B$ est une partie d'un espace vectoriel normé $W$, alors $\overline{ A\times B }=\bar A\times \bar B$.
\end{proposition}

\begin{proof}
	Nous commençons par prouver que $\overline{ A\times B }\subset\bar A\times \bar B$. Un élément $(a,b)$ dans $\overline{ A\times B }$ peut toujours être vu comme la limite d'une suite dans $A\times B$. Soit donc $(a_n,b_n)$ une suite telle que $\lim(a_n,b_n)=(a,b)$ avec $a_n\in A$ et $b_n\in B$. Par le lemme \ref{LemCvVxWcvVW}, nous avons une convergence «composante par composante» : $a_n\to a$ et $b_n\to b$. Mais la suite $(a_n)$ est contenue dans $A$, donc sa limite, $a$, est dans $\bar A$. De la même manière, $b\in\bar B$. Par conséquent $(a,b)\in \bar A\times\bar B$.

	Il faut maintenant prouver l'inclusion inverse. Soit $(a,b)\in\bar A\times \bar B$. Nous avons des suites $(a_n)$ et $(b_n)$ dans $A$ et $B$ respectivement qui convergent vers $a$ et $b$. En utilisant à nouveau le lemme \ref{LemCvVxWcvVW} (mais cette fois dans le sens inverse), nous avons
	\begin{equation}
		\lim\underbrace{(a_n,b_n)}_{\in A\times B}=(a,b)\in\overline{ A\times B }.
	\end{equation}
	La dernière appartenance est simplement le fait qu'une suite convergente contenue dans $A\times B$ converge dans $\overline{ A\times B }$.
\end{proof}



\begin{proposition}[\cite{MonCerveau}]          \label{PROPooKDGOooDjWQct}
	Soit un espace \( V\) muni d'un produit scalaire à valeurs dans \( \eK\) (si \( \eK=\eC\) nous supposons le produit hermitien, mais ce n'est pas très important ici). Alors l'application
	\begin{equation}
		\begin{aligned}
			a\colon V\times V & \to \eK                     \\
			(x,y)             & \mapsto \langle x, y\rangle
		\end{aligned}
	\end{equation}
	est continue.
\end{proposition}

\begin{proof}
	Nous ne disons pas que l'espace \( V\times V\) est muni d'un produit scalaire. Mais en tout cas c'est un espace métrique, et \( \eK\) l'est aussi. Donc \( a\) est une application entre deux espaces métriques et elle sera continue si et seulement si elle est séquentiellement continue (propositions \ref{PropFnContParSuite} et \ref{PROPooBHRBooJMZYSg}).

	Soit donc une suite convergente dans \( V\times V\), c'est-à-dire \( (x_k,y_k)\stackrel{V\times V}{\longrightarrow}(x,y)\). Nous devons démontrer que \( \langle x_k, y_k\rangle \stackrel{\eR}{\longrightarrow}\langle x, y\rangle \). Les majorations usuelles donnent
	\begin{subequations}
		\begin{align}
			\big| \langle x_k, y_k\rangle -\langle x, y\rangle  \big| & \leq \big| \langle x_k, y_k\rangle -\langle x, y_k\rangle  \big|+\big| \langle x, y_k\rangle -\langle x, y\rangle  \big| \\
			                                                          & =\big| \langle x_k-x, y_k\rangle  \big|+\big| \langle x, y_k-y\rangle  \big|.
		\end{align}
	\end{subequations}
	Nous savons du lemme~\ref{LemCvVxWcvVW} que les suites \( (x_k)\) et \( (y_k)\) sont séparément convergentes : \( x_k\stackrel{V}{\longrightarrow}x\) et \( y_k\stackrel{V}{\longrightarrow}y\). En utilisant l'inégalité de Cauchy-Schwarz~\ref{EQooZDSHooWPcryG} nous trouvons
	\begin{equation}
		\big| \langle x_k-x, y_k\rangle  \big|\leq \| x_k-x \|\| y_k \|.
	\end{equation}
	Nous avons \( \| x_k-x \|\to 0\) et \( \| y_k \|\to \| y \|\), et par la règle du produit de limites dans \( \eR\) nous avons que \( \big| \langle x_k-x, y_k\rangle  \big|\to 0\).
\end{proof}

\begin{remark}      \label{RemTopoProdPasRm}
	Il faut remarquer que la norme \eqref{EqNormeVxWmax} est une norme \emph{par défaut}. C'est la norme qu'on met quand on ne sait pas quoi mettre. Or il y a au moins un cas d'espace produit dans lequel on sait très bien quelle norme prendre : les espaces \( \eR^m\). La norme qu'on met sur \( \eR^2\) est
	\begin{equation}
		\| (x,y) \|=\sqrt{x^2+y^2},
	\end{equation}
	et non la norme «par défaut» de \( \eR^2=\eR\times\eR\) qui serait
	\begin{equation}
		\| (x,y) \|=\max\{ | x |,| y | \}.
	\end{equation}
	Les théorèmes que nous avons donc démontré à propos de \( V\times W\) ne sont donc pas immédiatement applicables au cas de \( \eR^2\).

	Cette remarque est valable pour tous les espaces \( \eR^m\). À moins de mention contraire explicite, nous ne considérons jamais la norme par défaut \eqref{EqNormeVxWmax} sur un espace \( \eR^m\).
\end{remark}

Étant donné la remarque~\ref{RemTopoProdPasRm}, nous ne savons pas comment calculer par exemple la fermeture du produit d'intervalle \( \mathopen] 0,1 ,  \mathclose[\times\mathopen[ 4 , 5 [\). Il se fait que, dans \( \eR^m\), les fermetures de produits sont quand même les produits de fermetures.

\begin{proposition}     \label{PropovlAxBbarAbraB}
	Soient des espaces vectoriels normés \( V\) et \( W\). Soient \( A\subset V\) et \( B\subset W\). Alors
	\begin{equation}
		\overline{ A\times B }=\bar A\times \bar B
	\end{equation}
	pour la norme produit\footnote{Définition \ref{LEMooFQMSooLmdIvD}.} sur \( V\times W\).
\end{proposition}

\begin{proof}
	Nous commençons par prouver que \( \overline{ A\times B }\subset \bar A\times \bar B\). Soit donc \( (x,y)\in\overline{ A\times B }\). La proposition \ref{PropLFBXIjt} nous assure de l'existence d'une suite dans \( A\times B\) convergent vers \( (x,y)\). Nous considérons donc une suite \( (x_n,y_n)\) dans \( A\times B\) telle que
	\begin{equation}
		(x_n,y_n)\stackrel{V\times W}{\longrightarrow}(x,y).
	\end{equation}
	Soit \( \epsilon>0\). Si \( n\) est assez grand nous avons
	\begin{equation}
		\| (x_n,y_n)-(x,y) \|\leq \epsilon.
	\end{equation}
	Mais
	\begin{equation}
		\| (x_n,y_n)-(x,y) \|=\| (x_n-x,y_n-y) \|=\max\big( \| x_n-x \|_V,\| y_n-y \|_W \big).
	\end{equation}
	Donc pour ce \( n\) nous avons \( \| x_n-x \|\leq \epsilon\) et \( \| y_n-y \|\leq \epsilon\). Nous en déduisons que \( x_n\stackrel{V}{\longrightarrow}x\) et \( y_n\stackrel{W}{\longrightarrow}y\). Donc \( x\in \bar A\) et \( y\in\bar B\).

	Dans l'autre sens maintenant. Soit \( (x,y)\in\bar A\times \bar B\). Nous considérons deux suites \( (x_n)\) dans \( A\) et \( (y_n)\) dans \( B\) telles que \( x_n\stackrel{V}{\longrightarrow}x\) et \( y_n\stackrel{W}{\longrightarrow}y\). Soit \( n\) suffisamment grand pour que \( \| x_n-x \|\leq \epsilon\) et \( \| y_n-y \|\leq \epsilon\). Nous avons alors
	\begin{equation}
		\| (x_n,y_n)-(x,y) \|=\max\{ \| x_n-x \|,\| y_n-y \| \}\leq \epsilon.
	\end{equation}
\end{proof}


\begin{proposition}\label{exoEspVectoNorme0002}
	Soit $ A = \{( x, y) \in \eR^2 \; | \; x>0, y > 0 \}. $
	\begin{enumerate}
		\item
		      La partie $A$ est ouverte dans $\eR^2$.
		\item
		      L'adhérence de \( A\) est donnée par
		      \begin{equation}
			      \bar A=A\cup\{ (x,0)\tqs x\geq 0 \}\cup\{ (0,y)\tqs y\geq 0\}.
		      \end{equation}
		\item
		      La frontière de \( A\) est donnée par
		      \begin{equation}
			      \partial A=\{ (x,0)\tqs x\geq 0 \}\cup\{ (0,y)\tqs y\geq 0\}.
		      \end{equation}
	\end{enumerate}
\end{proposition}


\begin{proof}
	Point par point.
	\begin{enumerate}
		\item

		      Prenons $(x,y)\in A$, et tâchons de trouver une boule autour de $(x,y)$ qui soit contenue dans $A$. Par définition, $x>0$ et $y>0$. Donc si nous prenons $r=\min\{ x,y \}/2$, la boule $B\big( (x,y),r \big)$ est encore contenue dans $A$.

		      Notez que l'ensemble des boules du type $\mO_{(x,y)}=B\big( (x,y),\frac{ \min\{ x,y \} }{ 2 } \big)$ est un recouvrement de $A$ par des ouverts.

		\item
		      L'adhérence est constituée des points qui «touchent» presque l'ensemble. Intuitivement, nous devinons que l'adhérence de $A$ va être l'ensemble des points $(x,y)$ tels que $x\geq 0$ et $y\geq 0$. D'abord, un point $(a,b)$ avec $a<0$ n'est pas dans $\bar A$ parce qu'il existe une boule autour de $(a,b)$ telle que $x<0$ pour tout $(x,y)$ dans la boule (même chose pour les points avec $y<0$).

		      Ensuite, prouvons que les points de la forme $(0,y)$ et $(x,0)$ avec $x,y\geq 0$ sont dans $\bar A$. Pour cela, rien de tel qu'une suite. La suite $(\frac{1}{ n },y)$ avec $y\geq 0$ est contenue dans $A$, et sa limite est clairement le point $(0,y)$. Nous en concluons que $(0,y)$ est un point de $\bar A$.

		      De la même façon, la suite $(x,\frac{1}{ n })$ montre que le point $(x,0)$ est dans $\bar A$. Donc
		      \begin{equation}
			      \bar A=A\cup\{ (x,0)\tqs x\geq 0 \}\cup\{ (0,y)\tqs y\geq 0\}.
		      \end{equation}
		      En particulier, le point $(0,0)$ est dans $\bar A$.

		\item
		      En ce qui concerne la frontière, nous utilisons la caractérisation $\partial A=\bar A\setminus\Int(A)$. Étant donné que $A$ est ouvert, $\Int(A)=A$. Les points qui sont dans $\bar A$ et pas dans $A$ sont les points avec $x=0$ ou $y=0$. Donc
		      \begin{equation}
			      \partial A=\{ (x,0)\tqs x\geq 0 \}\cup\{ (0,y)\tqs y\geq 0\}.
		      \end{equation}

	\end{enumerate}

	L'adhérence peut aussi être trouvée en utilisant la proposition \ref{PropovlAxBbarAbraB}. Nous avons $A=\mathopen] 0 , \infty \mathclose[\times\mathopen] 0 , \infty \mathclose[$, et par conséquent, la fermeture de $A$ est la produit des fermetures :
				\begin{equation}
					\bar A=\mathopen[ 0 , \infty [\times\mathopen[ 0 , \infty [.
				\end{equation}
				Nous insistons sur le fait que la fermeture de $\mathopen] 0 , \infty \mathclose[$ n'est pas $\mathopen[ 0 , \infty \mathclose]$. Ce dernier ensemble n'est pas une partie de $\eR$.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Continuité du produit de matrices}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooOAWAooFcyUfI}

Nous avons introduit des normes sur \( \eM(n,\eK)\), entre autres la norme opérateur de la définition~\ref{DefNFYUooBZCPTr}. Qui dit norme dit topologie. Il advient alors la question évidente : est-ce que des opérations aussi élémentaires que le produit de matrices sont continues pour ces topologies ?

Une façon simple de répondre à cela est d'introduire sur \( \eM(n,\eK)\) une nouvelle norme très simple : celle de \( \eK^n\). C'est la topologie par composante. Pour cette topologie, il est simple de voir que le produit matriciel est continu parce que les éléments de \( AB\) sont des polynômes en les éléments de \( A\) \( B\). Ensuite il suffit d'invoquer l'équivalence de toutes les normes (théorème~\ref{ThoNormesEquiv}).

Voyons comment montrer cela de façon plus directe (bien que le raisonnement précédent soit une démonstration qui devrait déjà avoir convaincu les plus sceptiques). La preuve suivante va donc s'amuser à bien préciser les topologies et caractérisations utilisées.

\begin{lemma}       \label{LEMooRGNRooPovBQw}
	Si \( \| . \|\) est une norme algébrique sur \( \eM(n,\eK)\) (\( \eK\) est \( \eR\) ou \( \eC\)) alors l'application
	\begin{equation}
		\begin{aligned}
			p\colon \eM(n,\eK)\times \eM(n,\eK) & \to \eM(n,\eK) \\
			(A,B)                               & \mapsto AB
		\end{aligned}
	\end{equation}
	est continue.
\end{lemma}

\begin{proof}
	L'espace \( \eM(n,\eK)\times \eM(n,\eK)\) est métrique (définition~\ref{DefFAJgTCE}), donc la caractérisation séquentielle de la continuité (proposition~\ref{PropXIAQSXr}) s'applique. Nous considérons donc une suite \( (A_k,B_k)\) dans \( \eM(n,\eK)\times \eM(n,\eK)\) convergente vers \( (A,B)\), et nous allons prouver que \( p(A_k,B_k)\to p(A,B)\).

	Nous savons que la topologie sur \( \eM(n,\eK)\times \eM(n,\eK)\) est la topologie produit (lemme~\ref{DefFAJgTCE}) et que celle-ci donne la convergence composante par composante dès que nous avons convergence d'une suite; c'est la proposition~\ref{PROPooNRRIooCPesgO}. Nous avons donc \( A_k\stackrel{\eM(n,\eK)}{\longrightarrow}A\) et \( B_k\stackrel{\eM(n,\eK)}{\longrightarrow}B\).

	Voilà pour le contexte. Maintenant, la preuve de la continuité. Nous effectuons les majorations suivantes :
	\begin{subequations}
		\begin{align}
			\| p(A_k,B_K)-AB \| & \leq \| p(A_k,B_k)-p(A_k,B) \|+\| p(A_k,B)-AB \|                                                                   \\
			                    & =\| A_kB_k-A_kB \|+\| A_kB-AB \|                                                                                   \\
			                    & =\| A_k(B_k-B) \|+\| (A_k-A)B \|                                                                                   \\
			                    & \leq \underbrace{\| A_k \|}_{\to \| A \|}\underbrace{\| B_k-B \|}_{\to 0}+\underbrace{\| A_k-A \|}_{\to 0}\| B \|.
		\end{align}
	\end{subequations}
\end{proof}



%+++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications multilinéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooBNZHooGfhPRK}

%-------------------------------------------------------
\subsection{Norme}
%----------------------------------------------------


\begin{lemma}[\cite{MonCerveau,BIBChatGPT}]	\label{LEMooIEXNooPOHokX}
	Soit un espace de Banach \( F\). Nous considérons une application \( k\)-multilinéaire \(A \colon (\eR^n)^k\to \eR  \). Nous posons
	\begin{equation}
		\begin{aligned}
			a\colon \eR^n & \to \eR                \\
			x             & \mapsto A(x,\ldots,x).
		\end{aligned}
	\end{equation}
	Alors
	\begin{enumerate}
		\item		\label{ITEMooHELXooHqYioG}
		      Il existe un unique\quext{L'unicité n'est pas démontrée. Écrivez une démonstration sur une feuille de papier, et envoyez-moi une photo de la feuille.} choix de \( \{ a_{\alpha} \}_{\alpha\in\Lambda_k}\) tel que
		      \begin{equation}
			      a(x)=\sum_{| \alpha |=k}a_{\alpha}x^{\alpha}.
		      \end{equation}
		      %TODOooIFSUooCbVCxg. Prouver la partie unicité.
		      Ce choix est donné par
		      \begin{equation}
			      a_{\alpha}=\frac{ k! }{ \alpha! }A(e_1^{\alpha_1},\ldots,a_n^{\alpha_n}).
		      \end{equation}
		\item		\label{ITEMooHMZUooGjtmEB}
		      Nous avons l'inégalité, pour tout \( x\in \eR^n\) :
		      \begin{equation}
			      \sum_{| \alpha |=k}\| a_{\alpha}x^{\alpha} \|\leq \| A \|\big( | x_1 |+\ldots+| x_n | \big)^k\leq n^{k/2}\| A \|\| x \|^k.
		      \end{equation}
	\end{enumerate}
\end{lemma}

\begin{proof}
	Nous commençons par le point \ref{ITEMooHELXooHqYioG}. En plusieurs parties.
	\begin{subproof}
		\spitem[Plein de Définitions]
		%-----------------------------------------------------------
		Soit \( x\in \eR^n\). Nous avons
		\begin{equation}		\label{EQooAJCPooQnXdyI}
			a(x)=A\big( \sum_{i_1}x_{i_1}e_{i_1},\ldots,\sum_{i_k}x_{i_k}e_{i_k} \big)=\sum_{i_1,\ldots,i_k=1}^nA(e_{i_1},\ldots,e_{i_k})x_{i_1}\ldots x_{i_k}.
		\end{equation}
		Pour les besoins de la preuve nous posons
		\begin{equation}
			M=\big\{ (i_1,\ldots,i_k)\in \{ 1,\ldots,n \}^k \big\}
		\end{equation}
		et
		\begin{equation}
			\Lambda_k=\big\{  (\alpha_1,\ldots,\alpha_n)\in \eN^n\tq\sum_{i=1}^n\alpha_i=k  \big\}.
		\end{equation}
		Pour chaque \( s\in M\) et \( i\in\{ 1,\ldots,n \}\) considérons les indices de \( s\) égaux à \( i\), c'est-à-dire
		\begin{equation}
			P_i(s)=\big\{ j\in \{ 1,\ldots,k \}\tq s_j=i  \big\},
		\end{equation}
		et le nombre de fois que \( i\) apparaît dans \( s\), c'est-à-dire
		\begin{equation}
			f_i(s)=\Card\big( P_i(s) \big),
		\end{equation}
		et enfin nous posons
		\begin{equation}
			f(s)=\big( f_1(s),\ldots,f_n(s) \big).
		\end{equation}
		\spitem[Les \( P_i(s)\) forment une partition de \( \{ 1,\ldots,k \}\)]
		%-----------------------------------------------------------
		D'abord si \( l\in P_j(s)\cap P_i(s)\) alors \( s_l=i\) et \( s_l=j\), ce qui donne \( i=j\). Donc \( P_i(s)\cap P_j(s)=\emptyset\) dès que \( i\neq j\). Ensuite si \( l\in \{ 1,\ldots,k \}\), en posant \( j=s_l\) nous avons \( l\in P_j(s)\subset\bigcup_{i=1}^nP_i(s)\).

		Bref nous avons bien
		\begin{equation}		\label{EQooNLPRooFFwhYb}
			\{ 1,\ldots,k \}=\bigcup_{i=1}^nP_i(s)
		\end{equation}
		et l'union est disjointe.

		\spitem[\( f(s)\in \Lambda_k\)]
		%-----------------------------------------------------------
		Étant donné que \( f_i(s)=\Card\big( P_i(s) \big)\), et que l'union \eqref{EQooNLPRooFFwhYb} est disjointe\footnote{Nous utilisons le lemme \ref{LEMooVFPNooVmdUXY}\ref{ITEMooBUCZooYLCuIe}.} nous avons
		\begin{equation}
			\sum_{i=1}^nf_i(s)=\sum_{i=1}^n\Card(P_i(s))=\Card\big( \bigcup_{i=1}^nP_i(s) \big)=\Card\{ 1,\ldots,k \}=k.
		\end{equation}

		\spitem[\(f \colon M\to \Lambda_k  \) est surjective]
		%-----------------------------------------------------------
		Pour \(\alpha\in\Lambda_k\) nous posons
		\begin{equation}
			s=(\underbrace{1,\ldots,1}_{\text{\( \alpha_1\) fois}}, \underbrace{2,\ldots,2}_{\text{\( \alpha_2\) fois}},\ldots,\underbrace{n,\ldots,n}_{\text{\( \alpha_n\) fois}}).
		\end{equation}
		Vu que \( \alpha_1+\ldots+\alpha_n=k\), ce \( s\) est bien dans \( M=\{ 1,\ldots,n \}^k\). Avec ça, l'ensemble des positions dans \( s\) où apparaît le \( 1\) est \( \{ 1,\ldots,\alpha_1 \}\), de telle sorte que \( P_1(s)=\{ 1,\ldots,\alpha_1 \}\) et donc \( f_1(s)=\alpha_1\). De même pour les autres \( i=1,\ldots,n\). Donc \( \Card\big( P_i(s) \big)=\alpha_i\), et
		\begin{equation}
			f(s)=\alpha.
		\end{equation}

		\spitem[Stabilisateur]
		%-----------------------------------------------------------
		Le groupe symétrique \( S_k\) agit sur \( M\) par \( \sigma(s)_i=s_{\sigma(i)}\). Soient \( \alpha\in \Lambda_k\) et \( s\in f^{-1}(\alpha)\). Question : quel est le stabilisateur\footnote{Définition \ref{DEFooMDYGooLrOERP}.} de \( s\) dans \( S_k\) ?

		Étant donne que \( s\) contient \( \alpha_i\) fois le nombre \( i\) aux positions \( P_i(s)\), le groupe \( S_{\alpha_i}\) peut agir sur \( P_i(s)\) sans changer \( s\). Le stabilisateur de \( s\) est donc
		\begin{equation}
			\Stab(s)= S_{\alpha_1}\times\ldots\times S_{\alpha_n}.
		\end{equation}

		\spitem[Orbite]
		%-----------------------------------------------------------
		Soit encore \( s\in f^{-1}(\alpha)\). Quelle est l'orbite de \( s\) sous \( S_k\) ? Si \( S_k\) agit sur \( s\), alors le nombre de \( 1,\ldots,n\) dans \( \sigma(s)\) sera le même que dans \( s\); ils auront juste changé de place. Autrement dit, les ensembles \( P_i(s)\) et \( P_i\big( \sigma(s) \big)\) ne sont pas identiques, mais leur cardinal sont égaux. Donc \( \sigma(s)\) est encore dans \( f^{-1}(\alpha)\). Autrement dit, pour l'orbite de \( s\) nous avons
		\begin{equation}		\label{EQooFIBBooVcZiOB}
			\mO_s=f^{-1}(\alpha).
		\end{equation}

		\spitem[Quelques factorielles]
		%-----------------------------------------------------------
		C'est le moment d'utiliser le théorème orbite-stabilisateur \ref{Propszymlr} :
		\begin{equation}
			\Card(\mO_s)=\frac{ \Card(S_k) }{ \Card\big( \Stab(s) \big)}=\frac{ k! }{ \alpha_1!\ldots \alpha_n! }.
		\end{equation}
		En utilisant \eqref{EQooFIBBooVcZiOB} et les notations \ref{EQooMZAMooMJuLKD},
		\begin{equation}		\label{EQooFQONooJhLOER}
			\Card\big( f^{-1}(\alpha) \big)=\frac{ k! }{ \alpha! }.
		\end{equation}

		\spitem[\( \{ f^{-1}(\alpha) \}_{\alpha\in \Lambda_k}\) est une partition de \( M\)]
		%-----------------------------------------------------------
		D'une part si \( s\in f^{-1}(\alpha)\cap f^{-1}(\beta)\), alors \( f(s)=\alpha\) et \( f(s)=\beta\) de telle sorte que \( \alpha=\beta\). D'autre part si \( s\in M\), en posant \( \alpha=f(s)\) nous avons bien \( s\in f^{-1}(\alpha)\). Bref
		\begin{equation}
			M=\bigcup_{\alpha\in \Lambda_k}f^{-1}(\alpha)
		\end{equation}
		et l'union est disjointe.

		\spitem[Retour à la fonction \( a\)]
		%-----------------------------------------------------------
		Nous pouvons maintenant décomposer la somme \eqref{EQooAJCPooQnXdyI} en
		\begin{subequations}
			\begin{align}
				a(x) & =\sum_{s\in M}A(e_{s_1},\ldots,e_{s_k})x_{s_1}\ldots x_{s_k}                                          \\
				     & =\sum_{\alpha\in\Lambda_k}\sum_{s\in f^{-1}(\alpha)}  A(e_{s_1},\ldots,e_{s_k})x_{s_1}\ldots x_{s_k}.
			\end{align}
		\end{subequations}
		Notez que si \( f(s)=f(t)\) alors \( \Card(P_i(s))=\Card\big( P_i(t) \big)\), c'est-à-dire que \( A(e_{s_1},\ldots,e_{s_k}) \) et \( A(e_{t_1},\ldots,e_{t_k})\) contiennent autant de \( e_i\) l'un que l'autre. Étant donné que \( A\) est symétrique, ça donne \( A(e_{s_1},\ldots, e_{s_k})=A(e_{t_1},\ldots,e_{t_k})\). Idem pour \( x_{s_1}\ldots x_{s_k}=x_{t_1}\ldots x_{t_k}\).

		Donc tous les termes de
		\begin{equation}
			\sum_{s\in f^{-1}(\alpha)}A(e_{s_1},\ldots s_k)x_{s_1}\ldots x_{s_k}
		\end{equation}
		sont égaux et valent
		\begin{equation}		\label{EQooCUAOooWhhhAY}
			A(\underbrace{e_1,\ldots,e_1}_{\text{\( \alpha_1\) fois}},\ldots,\underbrace{e_n,\ldots,e_n}_{\text{\( \alpha_n\) fois}})x_1^{\alpha_1}\ldots x_n^{\alpha_n}=A(e_1^{\alpha_1},\ldots,e_n^{\alpha_n})x^{\alpha}.
		\end{equation}
		La seconde égalité est juste une notation plus compacte.

		\spitem[Et enfin]
		%-----------------------------------------------------------
		En utilisant \eqref{EQooFQONooJhLOER} et \eqref{EQooCUAOooWhhhAY} nous trouvons bien
		\begin{equation}
			a(x)=\sum_{\alpha\in \Lambda_k}a_{\alpha}x^{\alpha}
		\end{equation}
		avec
		\begin{equation}
			a_{\alpha}=\frac{ k! }{ \alpha! }A(e_1^{\alpha_1},\ldots,e_n^{\alpha_n}).
		\end{equation}
		Cela termine la preuve de \ref{ITEMooHELXooHqYioG}.
	\end{subproof}
	Nous passons à la preuve de \ref{ITEMooHMZUooGjtmEB}. Pour chaque \( \alpha\in\Lambda_k\) nous avons
	\begin{equation}
		\| a_{\alpha}x^{\alpha} \|=\| \frac{ k! }{ \alpha! }A(e_1^{\alpha_1},\ldots,e_n^{\alpha_n})x^{\alpha} \|\leq \frac{ k! }{ \alpha! }\| A \|| x |^{\alpha}
	\end{equation}
	où \( | x |^{\alpha}=| x_1 |^{\alpha_1}\ldots | x_n |^{\alpha_n}\). En posant \( y=\big( | x_1 |,\ldots,| x_n | \big)\) nous avons donc
	\begin{equation}
		\| a_{\alpha}x^{\alpha} \|\leq \frac{ k! }{ \alpha! }\| A \|y^{\alpha}.
	\end{equation}
	En sommant et en utilisant le théorème multinomial \ref{THOooNHAUooQvuytn}, il vient
	\begin{subequations}
		\begin{align}
			\sum_{\alpha\in \Lambda_k}\| a_{\alpha}x^{\alpha} \| & \leq \sum_{\alpha\in\Lambda_k}\frac{ k! }{ \alpha! }\| A \|y^{\alpha} \\
			                                                     & =\| A \|(y_1+\ldots+y_n)^k                                            \\
			                                                     & =\| A \|\big( | x_1 |+\ldots+| x_n | \big)^k.
		\end{align}
	\end{subequations}
	Cela est la première inégalité de \ref{ITEMooHMZUooGjtmEB}. Pour la seconde, il suffit d'utiliser l'équivalence de norme de la proposition \ref{PropLJEJooMOWPNi}\ref{ItemABSGooQODmLNi}.
\end{proof}


%-------------------------------------------------------
\subsection{Continuité}
%----------------------------------------------------


\begin{lemma}[\cite{BIBooFWFPooGfRwtt}]	\label{LEMooGQWQooUfqURv}
	Soit une forme \( n\)-multilinéaire \( \alpha\in\aL_n(E_1,\ldots,E_n;F)\). Nous avons la formule
	\begin{equation}
		\alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n)=\sum_{i=1}^n\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n).
	\end{equation}
\end{lemma}

\begin{proof}
	Nous y allons par récurrence.
	\begin{subproof}
		\spitem[\( n=2\)]
		%-----------------------------------------------------------
		Vu que \( \alpha\) est multilinéaire, nous avons \( \alpha(x_1,x_2)=\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2)\). Avec ça, nous avons
		\begin{subequations}
			\begin{align}
				\alpha(x_1,x_2)-\alpha(a_1,a_2) & =\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2)-\alpha(a_1,a_2) \\
				                                & =\alpha(x_1-a_1,x_2)+\alpha(a_1,x_2-a_2).
			\end{align}
		\end{subequations}
		\spitem[La récurrence]
		%-----------------------------------------------------------
		Nous supposons que la formule est valable pour toute application \( n\)-multilinéaire, et nous considérons une application \( n+1\)-multilinéaire \( \alpha\in\aL_{n+1}(E,F)\). Nous considérons aussi des éléments \( x_i, a_i\in E_i\) pour \( i=1,\ldots,n+1\). Nous posons
		\begin{equation}
			\begin{aligned}
				\alpha'\colon E_1\times\ldots\times E_n & \to F                                   \\
				y_1,\ldots,y_n                          & \mapsto \alpha(y_1,\ldots,y_n,x_{n+1}).
			\end{aligned}
		\end{equation}
		Cela est une application \( n\)-multilinéaire. Nous avons ce calcul :
		\begin{subequations}
			\begin{align}
				 & \alpha(x_1,  \ldots,x_{n+1})  -\alpha(a_1,\ldots,a_{n+1})      \nonumber                                                      \\& =\alpha'(x_1,\ldots,x_n)-\alpha'(a_1,\ldots,a_n)+\alpha'(a_1,\ldots,a_n)-\alpha(a_1,\ldots,a_{n+1}) \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha'(a_1,\ldots,a_n)-\alpha(a_1,\ldots,a_{n+1})        \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha(a_1,\ldots,a_n,x_{n+1})-\alpha(a_1,\ldots,a_{n+1}) \\
				 & =\sum_{i=1}^n\alpha'(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)+\alpha(a_1,\ldots,a_n,x_{n+1}-a_{n+1})                    \\
				 & =\sum_{i=1}^{n+1}\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n)
			\end{align}
		\end{subequations}
	\end{subproof}
\end{proof}


\begin{proposition}[\cite{BIBooINSNooUzJOru}]	\label{PROPooDQBOooByBvmj}
	Soient des espaces vectoriels normés \( E_1,\ldots, E_n\) et \( F\). Une application \( n\)-multilinéaire \(\alpha \colon E_1\times \ldots E_n\to F  \) est continue\footnote{Pour la topologie sur \( E_1\times\ldots\times E_n\), c'est celle de la norme produit \ref{LEMooFQMSooLmdIvD}.} si et seulement si il existe \( \lambda>0\) tel que
	\begin{equation}		\label{EQooGTVEooZsvzAM}
		\| \alpha(x_1,\ldots,x_n) \|\leq \lambda \| x_1 \|\ldots \| x_n \|.
	\end{equation}
\end{proposition}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\( \Rightarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( \alpha\) est continue. En particulier, elle est continue en \( (0,\ldots,0)\in E_1\times \ldots\times E_n\). Nous considérons \( r>0\) tel que
		\begin{equation}
			\alpha\big( B(0,2r) \big)\subset B(0,1).
		\end{equation}
		Nous posons \( \lambda=1/r^{n}\), et nous allons voir que ça fait le boulot. Soit \( (x_1,\ldots,x_n)\in E_1\times \ldots \times E_n\) avec \( x_i\neq 0\) pour tout \( i\). Nous posons \( z_i=rx_i/\| x_i \|\), de telle sorte que \( \| z_1 \|=\ldots =\| z_n \|=r\) et donc que \( (z_1,\ldots,z_n)\in B(0,2r)\) et donc
		\begin{equation}
			\| \alpha(z_1,\ldots,z_n) \|<1.
		\end{equation}
		Mais \( \alpha(z_1,\ldots,z_n)=r^n/\prod_i\| x_i \|\). Donc avec \( \lambda=1/r^n\) nous trouvons
		\begin{equation}
			\| \alpha(x_1,\ldots,x_n) \|\leq \lambda \prod_{i}\| x_i \|.
		\end{equation}

		Si \( x_i=0\), alors c'est encore plus simple parce que la relation \eqref{EQooGTVEooZsvzAM} se réduit à \( 0\leq 0\).
		\spitem[\( \Leftarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( \lambda>0\) vérifie \( \| \alpha(x_1,\ldots,x_n) \|\leq \lambda \prod_i\| x_i \|\) pour tout \( (x_1,\ldots,x_n)\in E_1\times \ldots\times E_n\). En utilisant le lemme \ref{LEMooGQWQooUfqURv}, nous avons, pour tout \( x_i\) et \( a_i\) :
		\begin{subequations}
			\begin{align}
				\| \alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n) \| & =\| \sum_{i=1}^n\alpha(a_1,\ldots,a_{i-1},x_i-a_i,x_{i+1},\ldots,x_n) \|                                                             \\
				                                                    & \leq \sum_{i=1}^n  \lambda  \| x_i-a_i \|\cdot \prod_{j=1}^{i-1}\| a_j \|\cdot\prod_{j=i+1}^n\| x_j \|.		\label{SUBEQooYDGCooJFBaxN}
			\end{align}
		\end{subequations}
		Et maintenant nous prenons \( 0<\epsilon<1\) et \( (x_1,\ldots,x_n)\in B\big( (a_1,\ldots,a_n),\epsilon \big)\), de telle sorte que \( \| x_i-a_i \|<\epsilon\) pour tout \( i\) et que
		\begin{equation}
			\| x_i \|<\| a_i \|+\epsilon\leq \| a_i \|+1.
		\end{equation}
		Nous posons \( M=\max_{j=1,\ldots,n}\| a_j \|\) : \( \| x_j \|\leq \| a_i \|+1\leq M+1\) et nous majorons sauvagement chacun des produits de \eqref{SUBEQooYDGCooJFBaxN} par \( (M+1)^n\) :
		\begin{subequations}
			\begin{align}
				\| \alpha(x_1,\ldots,x_n)-\alpha(a_1,\ldots,a_n) \| & \leq \sum_{i=1}^n\lambda \| x_i-a_i \|(M+1)^{2n} \\
				                                                    & \leq n\lambda \epsilon (M+1)^{2n}.
			\end{align}
		\end{subequations}
		Nous avons donc bien prouvé que \( \alpha(x_1,\ldots,x_n)\stackrel{F}{\longrightarrow} \alpha(a_1,\ldots,a_n) \) lorsque \( (x_1,\ldots,x_n)\stackrel{ E_1\times \ldots\times E_n}{\longrightarrow} (a_1,\ldots,a_n) \).

	\end{subproof}
\end{proof}



%-------------------------------------------------------
\subsection{Continuité de la composition}
%----------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]	\label{PROPooIPILooZMtYSP}
	Soit un espace vectoriel normé \( E\). L'application
	\begin{equation}
		\begin{aligned}
			\alpha\colon \End(E)^n & \to \End(E)                      \\
			(A_1,\ldots,A_n)       & \mapsto A_1\circ\ldots \circ A_n
		\end{aligned}
	\end{equation}
	est une application multilinéaire continue.
\end{proposition}

\begin{proof}
	Le fait que \( \alpha\) soit multilinéaire est une simple vérification\quext{Que je n'ai pas faite, alors faites-la.}. Pour la continuité, nous utilisons le lemme \ref{LEMooFITMooBBBWGI} qui donne
	\begin{equation}
		\| \alpha(A_1,\ldots,A_n) \|\leq \| A_1 \|\ldots \| A_n \|.
	\end{equation}
	Donc \( \lambda=1\) fait fonctionner la proposition \ref{PROPooDQBOooByBvmj}.
\end{proof}


%-------------------------------------------------------
\subsection{Norme}
%----------------------------------------------------

La proposition suivante\quext{Que je n'ai pas vérifiée. Démontrez-la par vous-même avant de me croire sur parole.} doit surement beaucoup à la proposition \ref{PROPooDQBOooByBvmj}.

\begin{propositionDef}[\cite{MonCerveau}]	\label{DEFooTAUWooNDJJEO}
	Soient des espaces vectoriels normés \( E_1,\ldots,E_n\) et \( F\). Si \(\alpha \colon E_1\times\ldots \times E_n\to F  \) est \( n\)-multilinéaire\footnote{Définition \ref{DefFRHooKnPCT}.}, alors
	\begin{enumerate}
		\item
		      Nous avons
		      \begin{equation}
			      \begin{aligned}[]
				      \inf & \{ \lambda\in \eR\tq\,\forall (x_1,\ldots,x_n)\in E_1\times \ldots E_n,\, \| \alpha(x_1,\ldots,x_n) \|\leq \lambda \| x_1 \|\ldots\| x_n \| \} \\
				           & =\sup\{ \| \alpha(x_1,\ldots,x_n) \|\tq\| x_i \|\leq 1 \}
			      \end{aligned}
		      \end{equation}
		\item
		      Ce nombre est définit une norme sur l'espace des applications \( n\)-linéaires.
	\end{enumerate}
	%TODOooVGCXooCWtuGG. Prouver ça.
	% À mon avis c'est un duplicat de DefKPBYeyG.
\end{propositionDef}
Voir aussi la définition \ref{DefKPBYeyG} et \ref{LEMooORANooRLJYWw}.

\begin{lemmaDef}[\cite{MonCerveau}]	\label{LEMooORANooRLJYWw}
	Soit une forme \( k\)-multilinéaire \(A \colon (\eR^n)^k\to \eR  \) ainsi que
	\begin{equation}
		\begin{aligned}
			a\colon \eR^n & \to \eR                \\
			x             & \mapsto A(x,\ldots,x).
		\end{aligned}
	\end{equation}
	Si nous notons
	\begin{equation}
		a(x)=\sum_{\alpha\in\Lambda_k}a_{\alpha}x^{\alpha},
	\end{equation}
	alors l'opération\footnote{Qui est bien définie par le lemme \ref{LEMooIEXNooPOHokX}\ref{ITEMooHELXooHqYioG}.}
	\begin{equation}
		\begin{aligned}
			N\colon \aL_k(\eR^n) & \to \eR                                            \\
			A                    & \mapsto    \max_{\alpha\in\Lambda_k}| a_{\alpha} |
		\end{aligned}
	\end{equation}
	est une norme.
\end{lemmaDef}

\begin{proof}
	Il faut vérifier les conditions de la définition \ref{DefNorme} parmi lesquelles nous n'allons faire que l'inégalité triangulaire. Soient les applications \( k\)-multilinéaires \( A\) et \( B\) donnant
	\begin{equation}
		\begin{aligned}[]
			a(x) & =\sum_{\alpha\in\Lambda_k}a_{\alpha}x^{\alpha}  \\
			b(x) & =\sum_{\alpha\in\Lambda_k}b_{\alpha}x^{\alpha}.
		\end{aligned}
	\end{equation}
	Étant donnée l'associativité des sommes finies, nous avons
	\begin{equation}
		(a+b)(x)=\sum_{\alpha\in\Lambda_k}(a_{\alpha}+b_{\alpha})x^{\alpha},
	\end{equation}
	et donc le calcul suivant :
	\begin{subequations}
		\begin{align}
			N(A+B) & =\max_{\alpha\in\Lambda_k}| a_{\alpha}+b_{\alpha} |                                  \\
			       & \leq\max_{\alpha\in\Lambda_k}\big( | a_{\alpha} |+| b_{\alpha} | \big)               \\
			       & \leq \max_{\alpha\in\Lambda_k}| a_{\alpha} |+\max_{\alpha\in\Lambda_k}| b_{\alpha} | \\
			       & =\| A \|+\| B \|.
		\end{align}
	\end{subequations}
\end{proof}
