% This is part of Le Frido
% Copyright (c) 2008-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Plusieurs choses sur les espaces vectoriels normés (dont la définition \ref{DefNorme}) ont déjà été vues dans la section \ref{SECooWKJNooKOqpsx}. Voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.

On fixe maintenant une définition largement utilisée dans la suite.
\begin{definition}      \label{DefAQIQooYqZdya}
	 Soient $U$ et $V$, deux ouverts d'un espace vectoriel normé. Une application $f$ de $U$ dans $V$ est un \defe{difféomorphisme}{difféomorphisme} si elle est bijective, différentiable et dont l'inverse $f^{-1}:V\to U $ est aussi différentiable.
\end{definition}

\begin{remark}
	Il n'est pas possible d'avoir une application inversible d'un ouvert de $\eR^m$ vers un ouvert de $\eR^n$ si $m\neq n$. Il n'y a donc pas de notion de difféomorphismes entre ouverts de dimensions différentes.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équivalence des normes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{normes_equiv}

Au premier coup d'œil, les notions dont nous parlons dans ce chapitre ont l'air très générales. Nous prenons en effet n'importe quel espace vectoriel $V$ de dimension finie, et nous le munissons de n'importe quelle norme (rien que dans $\eR^m$ nous en avons défini une infinité par l'équation \eqref{EqDeformeLp}). À partir de ces données, nous définissons les boules, la topologie, l'adhérence, etc.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{En dimension finie}
%---------------------------------------------------------------------------------------------------------------------------

Dans $\eR^n$, les normes $\| . \|_{L^1}$, $\| . \|_{L^2}$ et $\| . \|_{\infty}$ ne sont pas égales. Cependant elles ne sont pas complètement indépendantes au sens où l'on sent bien que si un vecteur sera grand pour une norme, il sera également grand pour les autres normes; les normes «vont dans le même sens». Cette notion est précisée par le concept de norme équivalente.

\begin{definition}		\label{DefEquivNorm}
    Deux normes $N_1$ et $N_2$ sur $\eR^m$ sont \defe{\wikipedia{fr}{Norme_équivalente}{équivalentes}}{equivalence@équivalence!norme}\index{norme!équivalence}\index{équivalence!de norme} s'il existe deux nombres réels strictement positifs $k_1$ et $k_2$ tels que
	\begin{equation}
		k_1N_1(x)\leq N_2(x)\leq k_2 N_1(x),
	\end{equation}
	pour tout $x$ dans $\eR^m$. Dans ce cas nous écrivons que $N_1\sim N_2$.
\end{definition}

\begin{lemma}       \label{LEMooHAITooWdtLAN}
    La définition de norme équivalentes donne une relation d'équivalence (définition~\ref{DefHoJzMp}) sur l'ensemble des normes existantes sur $\eR^m$.
\end{lemma}

\begin{proposition} \label{PropLJEJooMOWPNi}
    Pour \( \eR^N\), nous avons les équivalences de normes $\| . \|_{L^1}\sim\| . \|_{L^2}$, $\| . \|_{L^1}\sim\| . \|_{\infty}$ et $\| . \|_{L^2}\sim\| . \|_{\infty}$. Plus précisément nous avons les inégalités
    \begin{enumerate}
        \item\label{ItemABSGooQODmLNi}
           $ \| x \|_2\leq \| x \|_1\leq\sqrt{n}\| x \|_2$
        \item\label{ItemABSGooQODmLNii}
            $\| x \|_{\infty}\leq \| x \|_1\leq n \| x \|_{\infty}$
        \item\label{ItemABSGooQODmLNiii}
            $\| x \|_{\infty}\leq \| x \|_2\leq \sqrt{n}\| x \|_{\infty}$
    \end{enumerate}
\end{proposition}


\begin{proof}
    En mettant au carré la première inégalité nous voyons que nous devons vérifier l'inégalité
    \begin{equation}
        | x_1 |^2+\cdots+| x_n |^2\leq\big( | x_1 |+\cdots+| x_n | \big)^2
    \end{equation}
    qui est vraie parce que le membre de droite est égal au carré de chaque terme plus les double produits. La seconde inégalité provient de l'inégalité de Cauchy-Schwarz (théorème~\ref{ThoAYfEHG}) sur les vecteurs
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                1/n    \\
                \vdots    \\
                1/n
            \end{pmatrix},
            &w&=\begin{pmatrix}
                | x_1 |    \\
                \vdots    \\
                | x_n |
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous trouvons
    \begin{equation}
        \frac{1}{ n }\sum_i| x_i |\leq\sqrt{n\cdot\frac{1}{ n^2 }}\sqrt{\sum_i| x_i |^2},
    \end{equation}
    et par conséquent
    \begin{equation}
        \sum_i| x_i |\leq\sqrt{n}\| x \|_2.
    \end{equation}

    La première inégalité de~\ref{ItemABSGooQODmLNiii} se démontre en remarquant que si \( a\) et \( b\) sont positifs, \( a\leq\sqrt{a^2+b}\). En appliquant cela à \( a=\max_i| x_i |\), nous avons
    \begin{equation}
        \max_i| x_i |\leq\sqrt{ | x_1 |^2+\cdots+| x_n |^2  }
    \end{equation}
    parce que \( \max_i| x_i |\) est évidemment un des termes de la somme. Pour la seconde inégalité de~\ref{ItemABSGooQODmLNiii}, nous avons
    \begin{equation}
        \sqrt{\sum_k| x_k |^2}\leq\left( \sum_k\max_i| x_i |^2 \right)^{1/2}=\sqrt{n}\| x \|_{\infty}.
    \end{equation}
    Pour obtenir cette inégalité, nous avons remplacé tous les termes \( | x_k |\) par le maximum.
\end{proof}

Pour les autres normes \( \| . \|_p\), il y a des inégalités dans \ref{THOooPPDPooJxTYIy} et \ref{CORooMBQMooWBAIIH}; voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.

En réalité, toutes les normes \( \| . \|_{L^p}\) et \( \| . \|_{\infty}\) sont équivalentes et, plus généralement, nous avons le résultat suivant, très étonnant à première vue, et en réalité assez difficile à prouver :
\begin{theorem}[\cite{TrenchRealAnalisys}]		\label{ThoNormesEquiv}
	Sur un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
\end{theorem}
% TODO : la preuve est à la page 583 de Trench.

\begin{corollary}       \label{CORooBRDYooLmGJDE}
    Soit \( V\) un espace vectoriel de dimension finie et \( \| . \|_1\), \( \| . \|_2\) deux normes sur \( V\). Alors l'identité \( \id\colon V\to V\) est un isomorphisme d'espace topologique \( (V,\| . \|_1)\to (V,\| . \|_2)\).

    De plus les ouverts sont les mêmes : une partie de \( V\) est ouverte dans \( (V,\| . \|_1)\) si et seulement si elle est ouverte dans \( (V,\| . \|_2)\).
\end{corollary}

\begin{normaltext}
    L'exemple~\ref{EXooCAPYooMgOSyH} donne une norme sur \( \eR^2\) qui ne dérive pas d'un produit scalaire. Vu que toutes les normes sur \( \eR^2\) produisent la même topologie (c'est le corollaire~\ref{CORooBRDYooLmGJDE}), il y a parfaitement moyen pour deux espaces vectoriels topologiques d'être isomorphes alors que l'un a une norme dérivant d'un produit scalaire et l'autre non.
\end{normaltext}

Le théorème d'équivalence de norme sera utilisé pour montrer que l'ensemble des formes quadratiques non dégénérées de signature \( (p,q)\) est ouvert dans l'ensemble des formes quadratiques, proposition~\ref{PropNPbnsMd}. Plus généralement il est utilisé à chaque fois que l'on fait de la topologie sur les espaces de matrices en identifiant \( \eM(n,\eR)\) à \( \eR^{n^2}\), pour se rassurer en se disant que ce qu'on fait ne dépend pas de la norme choisie.

\begin{proposition}[\cite{MonCerveau}] \label{PROPooNTCFooEcwZwt}
    Let \( V\) be a finite dimensional complex vector space. For a basis \( B=\{ e_1,\ldots, e_n \}\) of \( V\) we define
    Soit un espace vectoriel \( V\) de dimension finie sur \( \eC\). Pour une base \( B= \{ e_i \}\) de \( V\) nous définissons
    \begin{equation}        \label{EQooEGXVooLASQIC}
        \| \sum_kv_ke_k \|_B= \sqrt{ \sum_k| v_k |^2 }.
    \end{equation}
    \begin{enumerate}
        \item
            La formule \eqref{EQooEGXVooLASQIC} définit une norme sur \( V\).
        \item
            Si \( B\) et \( B'\) sont des bases de \( V\), alors les topologies induites par le norme \( \| . \|_B\) et \( \| . \|_{B'}\) sont égales.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous commençons par fixer une base \( B=\{ e_i \}_{i=1,\ldots, n}\) de \( V\). Cette base nous permet de définir
    \begin{equation}
        \begin{aligned}
            \varphi\colon V&\to \eC^n \\
            \sum_kv_ke_k&\mapsto (v_1,\ldots, v_n). 
        \end{aligned}
    \end{equation}
    Cette application linéaire permet d'écrire
    \begin{equation}
        \| v \|_V=\| \varphi(v) \|_{\eC^n}.
    \end{equation}
    À partir de là, la vérification des propriétés de la définition \ref{DefNorme} est immédiate. Par exemple :
    \begin{equation}
        \| v+w \|=\| \varphi(v+w) \|=\| \varphi(v)+\varphi(w) \|\leq \| \varphi(v) \|+\| \varphi(w) \|=\| v \|+\| w \|.
    \end{equation}

    En ce qui concerne la seconde assertion, c'est le théorème \ref{ThoNormesEquiv}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Contre-exemple en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecPOlynomesCE}

Lorsque nous considérons des espaces vectoriels de dimension infinie, les choses ne sons plus aussi simples. Nous voyons ici sur l'exemple de l'espace des polynômes que le théorème~\ref{ThoNormesEquiv} n'est plus valable si on enlève l'hypothèse de dimension finie.

On considère l'ensemble des fonctions polynomiales à coefficients réels sur  l'intervalle $[0,1]$.
\begin{equation}
\mathcal{P}_\eR([0,1])=\{p:[0,1]\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots, \, a_i\in\eR,\,\forall i\in \eN\}.
\end{equation}
Cet ensemble, muni des opérations usuelles de somme entre polynômes et multiplications par les scalaires, est un espace vectoriel.

Sur $\mathcal{P}(\eR)$ on définit les normes suivantes
\begin{equation}
\begin{aligned}
&\|p\|_\infty=\sup_{x\in[0,1]}\{p(x)\},\\
&\|p\|_1 =\int_0^1|p(x)|\, dx,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}.\\
\end{aligned}
\end{equation}
Les inégalités suivantes sont  immédiates
\begin{equation}
\begin{aligned}
&\|p\|_1 =\int_0^1|p(x)|\, dx\leq \|p\|_\infty,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}\leq \|p\|_\infty,\\
\end{aligned}
\end{equation}
mais la norme $\|\cdot\|_\infty$ n'est  équivalente ni à $\|\cdot\|_1$, ni à $\|\cdot\|_2$. Soit $p_k(x)= x^k$. Alors
\begin{equation}
\begin{aligned}
&\|p_k\|_\infty=1,\\
&\|p_k\|_1 =\int_0^1x^k\, dx=  \frac{1}{k+1},\\
&\|p_k\|_2 =\left(\int_0^1x^{2k}\, dx\right)^{1/2}=\sqrt{\frac{1}{2k+1}}.
\end{aligned}
\end{equation}
Pour $k\to \infty$ les normes $\|p_k\|_1$, $\|p_k\|_2$ tendent vers zéro, alors que la norme $\|p_k\|_\infty$ est constante, donc les normes ne sont pas équivalentes parce que il n'existe pas un nombre positif $m$ tel que
\begin{equation}
\begin{aligned}
& m \|p_k\|_\infty\leq \|p_k\|_1 ,\\
& m \|p_k\|_\infty\leq \|p_k\|_2 ,\\
\end{aligned}
\end{equation}
uniformément pour tout $k$ dans $\eN$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Norme opérateur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La proposition suivante donne une norme (au sens de la définition~\ref{DefNorme}) sur $\aL(V,W)$ dès que \( V\) et \( W\) sont des espaces vectoriels normés.
\begin{propositionDef}[Norme opérateur\cite{ooTZRDooWmjBJi}, thème \ref{THEMEooOJJFooWMSAtL}]          \label{DefNFYUooBZCPTr}
    Soit une application linéaire \( T\colon V\to W\), et le nombre
	\begin{equation}
        \|T\|_{\aL}=\sup_{\substack{x\in V\\x\neq 0}}\frac{\|T(x)\|_{W}}{\|x\|_{V}}.
	\end{equation}
    \begin{enumerate}
        \item
            Si \( V\) est de dimension finie, alors \( \| T \|_{\aL}<\infty\).
        \item
            L'application \( T\mapsto\| T \|_{\aL}\) est une norme sur l'espace vectoriel des applications linéaires \( V\to W\).
        \item       \label{ITEMooUQPRooYQGZzu}
            Nous avons la formule
            \begin{equation}    \label{EqFZPooIoecGH}
                \| T \|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}} =\sup_{\|x\|_{V}=1}\|T(x)\|_{W}
            \end{equation}
    \end{enumerate}
    Le nombre \( \| T \|_{\aL}\) est la \defe{norme opérateur}{norme!d'application linéaire} de $T$. Nous disons que cette norme est \defe{subordonnée}{subordonnée!norme} aux normes choisies sur \( V\) et \( W\).
\end{propositionDef}
\index{norme!d'une application linéaire}

\begin{proof}
    Si \( V\) est de dimension finie alors l'ensemble $\{ \| x \|= 1 \}$ est compact par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}. Alors la fonction
    \begin{equation}
        x\mapsto \frac{ \| T(x) \| }{ \| x \| }
    \end{equation}
    est une fonction continue sur un compact. Le corollaire~\ref{CorFnContinueCompactBorne} nous dit alors qu'elle est bornée. Le supremum est donc un nombre réel fini.

    Nous vérifions que l'application $\| . \|$ de $\aL(V,W)$ dans $\eR$ ainsi définie est effectivement une norme.
    \begin{enumerate}
        \item
            $\|T\|_{\aL}=0$ signifie que $\|T(x)\|=0$ pour tout $x$ dans $V$. Comme  $\|\cdot\|_W$ est une norme nous concluons que $T(x)=0_{n}$ pour tout $x$ dans $V$, donc $T$ est l'application nulle.
    \item
        Pour tout $a$ dans $\eR$ et tout  $T$ dans $\aL(V,W)$ nous avons
        \begin{equation}
            \|aT\|_{\mathcal{L}}=\sup_{\|x\|_{V}\leq 1}\|aT(x)\|_{W}=|a|\sup_{\|x\|_{V}\leq 1}\|T(x)\|_{W}=|a|\|T\|_{\mathcal{L}}.
        \end{equation}
    \item
        Pour tous $T_1$ et $T_2$ dans $\aL(V,W)$ nous avons
      \begin{equation}\nonumber
        \begin{aligned}
           \|T_1+ T_2\|_{\mathcal{L}}&=\sup_{\|x\|\leq 1}\|T_1(x)+T_2(x)\|\leq\\
     &\leq\sup_{\|x\|\leq 1}\|T_1(x)\| +\sup_{\|x\|\leq 1}\|T_2(x)\|\\
     &=\|T_1\|\|T_2\|.
        \end{aligned}
      \end{equation}
    \end{enumerate}


    Enfin nous prouvons la formule alternative \eqref{EqFZPooIoecGH}. Nous allons montrer que les ensembles sur lesquels ont prend le supremum sont en réalité les mêmes :
    \begin{equation}
        \underbrace{\left\{ \frac{ \| Ax \| }{ \| x \| }\right\}_{x\neq 0}}_{A}=\underbrace{\left\{ \| Ax \|\tq \| x \|=1 \right\}}_{B}.
    \end{equation}
    Attention : ce sont des sous-ensembles de réels; pas de sous-ensembles de \( \eM(\eR)\) ou des sous-ensembles de \( \eR^n\).

    Pour la première inclusion, prenons un élément de \( A\), et prouvons qu'il est dans \( B\). C'est-à-dire que nous prenons \( x\in V\) et nous considérons le nombre \( \| Ax \|/\| x \|\). Le vecteur \( y=x/\| x \|\) est un vecteur de norme $1$, donc la norme de \( Ay\) est un élément de \( B\), mais
    \begin{equation}
        \| Ay \|=\frac{ \| Ax \| }{ \| x \| }.
    \end{equation}
    Nous avons donc \( A\subset B\).

    L'inclusion \( B\subset A\) est immédiate.
\end{proof}

En d'autres termes, il y a autant de normes opérateur sur \( \aL(E,F)\) qu'il y a de paires de choix de normes sur \( E\) et \( F\). En particulier, cela donne lieu à toutes les normes \( \| A \|_p\) qui correspondent aux normes \( \| . \|_p\) sur \( \eR^n\).

\begin{example}     \label{EXooXPXAooYyBwMX}
    Voyons la norme opérateur subordonnée à la norme \( \| x \|_{\infty}=\max_i| x_i |\) sur \( \eC^n\). Par définition (et surtout par la propriété~\ref{DefNFYUooBZCPTr}\ref{ITEMooUQPRooYQGZzu}),
    \begin{equation}
        \| A \|_{\infty}=\sup_{\| x \|_{\infty}=1}=\| Ax \|_{\infty}.
    \end{equation}
    Vu que \( (Ax)_i=\sum_kA_{ik}x_k\), lorsque \( \| x \|_{\infty}\leq 1\) nous avons \( | (Ax)_i |\leq \sum_k| A_{ik} |\). Donc nous avons toujours
    \begin{equation}        \label{EQooPLCIooVghasD}
        \| A \|_{\infty}\leq \max_i\sum_{k}| A_{ik} |.
    \end{equation}
\end{example}

\begin{definition}
    La \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs est la topologie de la norme opérateur.
\end{definition}
Lorsque nous considérons un espace vectoriel d'applications linéaires, nous considérons toujours\footnote{Sauf lorsque les événements nous forceront à trahir.} dessus la topologie liée à cette norme.

Il existe aussi la \defe{topologie faible}{topologie!faible} donnée par la notion de convergence\quext{Est-ce qu'on peut décrire cette topologie à partir de ses ouverts ? Facilement ?} \( A_i\to A\) si et seulement si \( A_ix\to Ax\) pour tout \( x\in E\).
    %TODO : il faut mettre au clair quelle est vraiment la topologie faible à partir des ouverts.

\begin{probleme}
    Je crois, mais demande confirmation, que la topologie faible est celle des semi-normes \( \{ p_v \}_{v\in E}\) données par \( p_v(A)=\| A \|\). En effet la notion de convergence associée par la proposition~\ref{PropQPzGKVk} est \( A_i\to A\) si et seulement si \( p_v(A_i-A)\to 0\). Cette condition signifie \( \| A_i(v)-A(v) \|\to 0\), c'est-à-dire \( A_i(v)\to A(v)\).

    Si le lecteur veut parler de cela au jury d'un concours, il est évident qu'il devra être capable d'ajouter des petits symboles au-dessus de toutes les flèches «\( \to\)» du paragraphe précédent pour indiquer pour quelles topologies sont les convergences dont on parle.
\end{probleme}

\begin{remark}
    Il faut noter que la topologie faible n'est pas une topologie métrique. Cela même si la condition \( A_ix\to Ax\), elle, est métrique vu qu'elle est écrite dans \( E\).

    Dans le cas où \( E\) est de dimension infinie, la topologie faible est réellement différente de la topologie forte. Nous verrons à la sous-section~\ref{subsecaeSywF} que dans le cas des projections sur un espaces de Hilbert, l'égalité
    \begin{equation}
        \sum_{i=1}^{\infty}\pr_{u_i}=\id
    \end{equation}
    est vraie pour la topologie faible, mais pas pour la topologie forte.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme d'algèbre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Norme d'algèbre\cite{ooTZRDooWmjBJi}]  \label{DefJWRWQue}
    Si \( A\) est une algèbre\footnote{Définition~\ref{DefAEbnJqI}.}, une \defe{norme d'algèbre}{norme!d'algèbre} sur \( A\) est une norme telle que pour toute \( x,y\in A\),
    \begin{equation}
        \| xy \|\leq \| x \|\| y \|.
    \end{equation}
\end{definition}
La norme opérateur est une norme d'algèbre, comme nous le verrons dans le lemme \ref{LEMooFITMooBBBWGI}.

Un des intérêts d'utiliser une norme d'algèbre est que l'on a l'inégalité \( \| x^k \|\leq \| x \|^k \). Cela sera particulièrement utile lors de l'étude des séries entières, voir par exemple~\ref{secEVnZXgf}.

\begin{definition}[\cite{ooYLHAooCzQvoa}]      \label{DEFooEAUKooSsjqaL}
    Le \defe{rayon spectral}{rayon!spectral} d'une matrice carrée $A$, noté $\rho(A)$, est défini de la manière suivante :
    \begin{equation}    \label{EQooNVNOooNjJhSS}
        \rho(A)=\max_i|\lambda_i|
    \end{equation}
    où les $\lambda_i$ sont les valeurs propres de $A$.
\end{definition}

\begin{normaltext}
    Quelques remarques sur la définition du rayon spectral.
    \begin{itemize}
        \item
             Même si \( A\) est une matrice réelle, les valeurs propres sont dans \( \eC\). Donc dans \eqref{EQooNVNOooNjJhSS}, \( | \lambda_i |\) est le module dans \( \eC\) de \( \lambda_i\).
        \item
            Vu que les valeurs propres de \( A\) sont les racines de son polynôme caractéristique (théorème~\ref{ThoWDGooQUGSTL}), il y en a un nombre fini et le maximum est bien défini.
        \item
            La définition s'applique uniquement pour les espaces de dimension finie.
    \end{itemize}
\end{normaltext}

\begin{lemma}       \label{LEMooIBLEooLJczmu}
    Soient des espaces vectoriels normés \( E\) et \( F\), sur les corps \( \eR\) ou \( \eC\). Pour tout \( A\in \aL(E,F)\), et pour tout \( u\in E\) nous avons la majoration
    \begin{equation}
        \| Au \|\leq \| A \|\| u \|
    \end{equation}
    où la norme sur \( A\) est la norme opérateur subordonnée à la norme sur \( u\).
\end{lemma}

\begin{proof}
    Si \( u\in E\) alors, étant donné que le supremum d'un ensemble est plus grand ou égal à chacun de éléments qui le compose,
    \begin{equation}
        \| A \|=\sup_{x\in E}\frac{ \| Ax \| }{ \| x \| }\geq \frac{ \| Au \| }{ \| u \| },
    \end{equation}
    donc le résultat annoncé : \( \| Au \|\leq \| A \|\| u \|\).
\end{proof}

Le lemme suivant est valable en dimension infinie. Nous en toucherons un mot dans l'exemple \ref{EXooTQPEooRRdddt}.
\begin{lemma}       \label{LEMooWFNXooLyTyyX}
    Soient des espaces vectoriels normés \( E\) et \( F\). Soit \( x\in E\). Alors l'application d'évaluation
    \begin{equation}
        \begin{aligned}
            ev_x\colon \aL(E,F)&\to F \\
            f&\mapsto f(x) 
        \end{aligned}
    \end{equation}
    est continue.
\end{lemma}

\begin{proof}
    Si \( x=0\), alors par linéarité de \( f\) nous avons \( ev_0(f)=0\) pour tout \( f\). Donc d'accord pour la continuité.

    Soit une suite convergente \( f_k\stackrel{\aL(E,F)}{\longrightarrow}f\). Nous voulons prouver que \( ev_x(f_k)\stackrel{F}{\longrightarrow}ev_x(f)\), c'est-à-dire que
    \begin{equation}
        \lim_{k\to \infty} \| f_k(x)-f(x) \|=0.
    \end{equation}
    Par hypothèse si \( k\) est grand, alors \( \| f_k-f  \|_{\aL(E,F)}\leq \epsilon\), c'est-à-dire que\footnote{Définition \ref{DefNFYUooBZCPTr} de la norme sur \( \aL(E,F)\).}
    \begin{equation}
        \sup_{y\in E}\frac{ \| f_k(y)-f(y) \| }{ \| y \| }\leq \epsilon.
    \end{equation}
    En particulier pour notre \( x\) nous avons
    \begin{equation}
        \frac{ \| f_k(x)-f(x) \| }{ \| x \| }\leq \epsilon,
    \end{equation}
    c'est-à-dire \( \| f_k(x)-f(x) \|\leq \| x \|\epsilon\). Vu que \( \| x \|\) est une simple constante et que \( \epsilon\) est arbitraire, cela implique \( f_k(x)\to f(x)\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrices, spectre et norme}
%---------------------------------------------------------------------------------------------------------------------------

La lien entre la norme opérateur d'une matrice et son spectre sera entre autres utilisé pour étudier le conditionnement de problèmes numériques. Voir la définition \ref{DEFooBKQWooJuoCGX} et par exemple son lien avec la résolution numérique de systèmes linéaires dans la proposition \ref{PROPooGIXFooAhJkIs}.

\begin{proposition}[\cite{ooYLHAooCzQvoa}]      \label{PROPooKLFKooSVnDzr}
    Soit une matrice \( A\in \eM(n,\eC)\) de rayon spectral \( \rho(A)\). Soit une norme \( \| . \|\) sur \( \eC^n\) et la norme opérateur correspondante. Alors
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}
    \end{equation}
    pour tout \( k\in \eN\).
\end{proposition}

\begin{proof}
    Soit \( v\in \eC^n\) et \( \lambda\in \eC\) un couple vecteur-valeur propre. Nous avons \( \| Av \|=| \lambda |\| v \|\) et aussi
    \begin{equation}
        | \lambda |^k\| v \|=\| \lambda^kv \|=\| A^kv \|\leq \| A^k \|\| v \|.
    \end{equation}
    La dernière inégalité est due au fait que nous avons choisi sur \( \eM(n,\eC)\) la norme subordonnée à celle choisie sur \( \eC^n\), via le lemme~\ref{LEMooIBLEooLJczmu}. Nous simplifions par \( \| v \|\) et obtenons \( | \lambda |\leq \| A^k \|^{1/k}\). Étant donné que \( \rho(A)\) est la maximum de tous les \( \lambda\) possibles, la majoration passe au maximum :
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}.
    \end{equation}
\end{proof}

\begin{lemma}[La norme opérateur est une norme d'algèbre\cite{MonCerveau}]   \label{LEMooFITMooBBBWGI}
    Soient des espaces vectoriels normés \( E\), \( F\) et \( G\). Soient des opérateurs linéaires bornés \( B\colon E\to F\), \( A\colon F\to G\). Alors
    \begin{equation}
        \| AB \|\leq \| A \|\| B \|.
    \end{equation}
    C'est à dire que la norme opérateur est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}.}.
\end{lemma}

\begin{proof}

    Nous avons les (in)égalités suivantes :
    \begin{subequations}
        \begin{align}
            \| AB \|&=\sup_{x\in E}\frac{ \| ABx \|_G }{ \| x \|_E }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\frac{ \| Bx \|_F }{ \| Bx \|_F }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }\frac{ \| Bx \| }{ \| x \| }\\
            &\leq\underbrace{\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }}_{\leq\| A \|}\underbrace{\sup_{\substack{y\in E\\By\neq 0}}\frac{ \| Bx \| }{ \| y \| }}_{=\| B \|}\\
            &\leq \| A \|\| B \|.
        \end{align}
    \end{subequations}
    La dernière inégalité provient que dans \( \sup_{\substack{x\in E\\Bx\neq 0}}\| ABx \|/\| x \|\), le supremum est pris sur un ensemble plus petit que celui sur lequel porte la définition de la norme de \( A\) : seulement l'image de \( B\) au lieu de tout l'espace de départ de \( A\).
\end{proof}



\begin{proposition}     \label{PROPooJGNFooEwtNmJ}
    Soient deux espaces vectoriels normés \( E\) et \( V\). Soient des applications continues \( f,g\colon E\to \End(V)\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon E&\to \End(V) \\
            x&\mapsto f(x)\circ g(x) 
        \end{aligned}
    \end{equation}
    est continue.
\end{proposition}

\begin{proof}
    Soit une suite \( x_k\stackrel{E}{\longrightarrow}x\). Nous devons montrer que \( \psi(x_k)\stackrel{\End(V)}{\longrightarrow}\psi(x)\). Pour cela nous utilisons le lemme \ref{LEMooFITMooBBBWGI} qui indique que la norme opérateur est une norme d'algèbre. Nous avons :
    \begin{subequations}
        \begin{align}
            \| \psi(x_k)-\psi(x) \|&=\| f(x_k)\circ g(x_k)-f(x)\circ g(x) \|\\
            &\leq \| f(x_k)\circ g(x_k)-f(x_k)\circ g(x) \|+\| f(x_k)\circ g(x)-f(x)\circ g(x) \|\\
            &=\| f(x_k)\circ \big( g(x_k)\circ g(x) \big) \|+\| \big(f(x_k)-f(x)\big)\circ g(x) \|\\
            &\leq \| f(x_k) \|\| g(x_k)-g(x) \|+\| f(x_k)-f(x) \|\| g(x) \|.
        \end{align}
    \end{subequations}
    Pour \( k\to \infty\) nous avons \( \| f(x_k)\to \| f(x) \| \|\), \( \| f(x_k)-f(x) \|\to 0\) (parce que \( f\) est continue) et similaire avec \( g\). Donc le tout tend vers zéro.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rayon spectral}
%---------------------------------------------------------------------------------------------------------------------------

La chose impressionnante dans la proposition suivante est que \( \rho(A)\) est définit indépendamment du choix de la norme sur \( \eM(n,\eK)\) ou sur \( \eK\). Lorsque nous écrivons \( \| A \|\), nous disons implicitement qu'une norme a été choisie sur \( \eK\) et que nous avons pris la norme subordonnée sur \( \eM(n,\eK)\).
\begin{proposition}[\cite{ooETMNooSrtWet}]      \label{PROPooWZJBooTPLSZp}
    Soit \( A\) une matrice de \( \eM(n,\eR)\) ou \( \eM(n,\eC)\). Alors
    \begin{equation}
        \rho(A)\leq \| A \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous devons séparer les cas suivant que le corps de base soit \( \eR\) ou \( \eC\).

    \begin{subproof}
        \item[Pour \( A\in \eM(n,\eC)\)]
            Soit \( \lambda\) une valeur propre de \( A\) telle que \( | \lambda |\) soit la plus grande. Nous avons donc \( \rho(A)=| \lambda |\). Soit un vecteur propre \( u\in \eC^n\) pour la valeur propre \( \lambda\). En prenant la norme sur l'égalité \( Au=\lambda u\), et en utilisant le lemme~\ref{LEMooIBLEooLJczmu},
            \begin{equation}
                | \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
            \end{equation}
            Donc \( | \lambda |\leq \| A \|\) et \( \rho(A)\leq\| A \|\).

        \item[Pour \( A\in \eM(n,\eR)\)]

            L'endroit qui coince dans le raisonnement fait pour \( \eM(n,\eC)\) est que certes \( A\in \eM(n,\eR)\) possède une plus grande valeur propre en module et qu'un vecteur propre lui est associé. Mais ce vecteur propre est a priori dans \( \eC^n\), et non dans \( \eR^n\). Nous pouvons donc écrire \( Au=\lambda u\), mais pas \( \| Au \|=| \lambda |\| u \|\) parce que nous ne savons pas quelle norme prendre sur \( \eC^n\).

            Il n'est pas certain que nous ayons une norme sur \( \eC^n\) qui se réduit sur \( \eR^n\) à celle choisie implicitement dans l'énoncé. Nous allons donc ruser un peu.

            Soit une norme \( N\) sur \( \eC^n\)\footnote{Il y en a plein, par exemple celle du produit scalaire \( \langle x, y\rangle =\sum_kx_k\bar y_k\).}. Nous nommons également \( N\) la norme subordonnée sur \( \eM(n,\eC)\) et la norme restreinte sur \( \eM(n,\eR)\). Vu que \( N\) est une norme sur \( \eM(n,\eR)\) et que ce dernier est de dimension finie, le théorème~\ref{ThoNormesEquiv} nous indique que \( N\) est équivalente à \( \| . \|\). Il existe donc \( C>0\) tel que
            \begin{equation}        \label{EQooBNWMooNgnMxC}
                 N(B)\leq C\| B \|
            \end{equation}
            pour tout \( B\in \eM(n,\eR)\). Nous avons maintenant
            \begin{equation}
                \rho(A)^m\leq N(A^m)\leq C\| A^m \|\leq C\| A \|^m.
            \end{equation}
            Justifications
            \begin{itemize}
                \item Par la proposition~\ref{PROPooKLFKooSVnDzr}.
                \item Parce que \( A^m\in \eM(n,\eR)\) et la relation \eqref{EQooBNWMooNgnMxC}.
                \item Par itération du lemme~\ref{LEMooFITMooBBBWGI}.
            \end{itemize}

            Nous avons donc \( \rho(A)\leq C^{1/m}\| A \|\) pour tout \( m\in\eN\). En prenant \( m\to \infty\) et en tenant compte de \( C^{1/m}\to 1\) nous trouvons \( \rho(A)\leq \| A \|\).
    \end{subproof}
\end{proof}

\begin{lemma}[\cite{ooETMNooSrtWet}]        \label{LEMooGBLJooCPvxNl}
    Soit \( A\in \eM(n,\eK)\) avec \( \eK=\eR\) ou \( \eC\). Soit \( \epsilon>0\). Il existe une norme algébrique sur \( \eM(n,\eK)\) telle que
    \begin{equation}
        N(A)\leq \rho(A)+\epsilon.
    \end{equation}
\end{lemma}

\begin{proof}
    Soit par le lemme~\ref{LemSchurComplHAftTq} une matrice inversible \( U\) telle que \( T=UAU^{-1}\) soit triangulaire supérieure, avec les valeurs propres sur la diagonale. Notons que même si \( A\in \eM(n,\eR)\), les matrices \( U\) et \( T\) sont a priori complexes.

    Soit \( s\in \eR\) ainsi que les matrices
    \begin{equation}
        D_s=\diag(1,s^{-1},s^{-2},\ldots, s^{1-n})
    \end{equation}
    et \( T_s=D_sTD_s^{-1}\). Nous fixerons un choix de \( s\) plus tard.

    La norme que nous considérons est :
    \begin{equation}
        N(B)=\| (D_sU)B(D_sU)^{-1} \|_{\infty}
    \end{equation}
    où \( \| . \|_{\infty}\) est la norme sur \( \eM(,n\eK)\) subordonnée à la norme \( \| . \|_{\infty}\) sur \( \eK^n\) dont nous avons déjà parlé dans l'exemple~\ref{EXooXPXAooYyBwMX}. Cela est bien une norme parce que
    \begin{itemize}
        \item Nous avons \( \| B \|_{\infty}=0\) si et seulement si \( B=0\), et vu que \( (D_sU)\) est inversible nous avons \( (D_sU)B(D_sU)^{-1}=0\) si et seulement si \( B=0\).
        \item \( N(\lambda B)=| \lambda |N(B)\).
        \item Pour l'inégalité triangulaire :
            \begin{subequations}
                \begin{align}
             N(B+C)&=\| (D_sU)B(D_sU)^{-1}+(D_sU)C(D_sU)^{-1} \|_{\infty}\\
             &\leq  \| (D_sU)B(D_sU)^{-1}\|_{\infty} +\| (D_sU)C(D_sU)^{-1} \|_{\infty} \\
             &=N(B)+N(C).
                \end{align}
            \end{subequations}
    \end{itemize}

    En ce qui concerne la matrice \( A\) elle-même, nous avons
    \begin{equation}
        N(A)=\| (D_sU)A(D_sU)^{-1} \|_{\infty}=\| T_s \|_{\infty}.
    \end{equation}
    C'est le moment de se demander comment se présente la matrice \( T_s\). En tenant compte du fait que \( (D_s)_{ik}=\delta_{ik}s^{1-i}\) nous avons
    \begin{equation}
        (T_s)_{ij}=\sum_{kl}(D_s)_{ik}T_{kl}(D^{-1}_s)_{lj}=T_{ij}s^{j-i}.
    \end{equation}
    La matrice \( T\) est encore triangulaire supérieure avec les valeurs propres de \( A\) sur la diagonale. Les éléments au-dessus de la diagonale sont tous multipliés par au moins \( s\). Il est donc possible de choisir \( s\) suffisamment petit pour avoir\quext{Il me semble qu'il manque un module dans \cite{ooETMNooSrtWet}.}
    \begin{equation}        \label{EQooSIEIooTWAXQD}
        \sum_{j=i+1}^n| (T_s)_{ij} |<\epsilon
    \end{equation}
    Avec ce choix, la formule~\ref{EQooPLCIooVghasD} donne
    \begin{equation}
        N(T_s)\leq\max_i\sum_k| (T_s)_{ik} |\leq \epsilon+\rho(A).
    \end{equation}
    En effet le \( \epsilon\) vient de la somme sur toute la ligne sauf la diagonale (c'est-à-dire la partie \( k\neq i\)) et du choix \eqref{EQooSIEIooTWAXQD} pour \( s\). Le \( \rho(A)\) provient du dernier terme de la somme (le terme sur la diagonale) qui est une valeur propre de \( A\), donc majorable par \( \rho(A)\).

    Nous devons encore prouver que \( N\) est une norme algébrique. Pour cela nous allons montrer qu'elle est subordonnée à la norme
    \begin{equation}
        \begin{aligned}
            n\colon \eK^n&\to \eR^+ \\
            v&\mapsto \| (UD_s)v \|_{\infty}.
        \end{aligned}
    \end{equation}
    Cela sera suffisant pour avoir une norme algébrique par le lemme~\ref{LEMooFITMooBBBWGI}. La norme \( n\) sur \( \eK^n\) produit la norme suivante sur \( \eM(n,\eK)\) :
    \begin{equation}
        n(B)=\sup_{v\neq 0}\frac{ n(B) }{ n(v) }=\sup_{v\neq 0}\frac{ \| (UD_s)Bv \|_{\infty} }{ \| UD_sv \|_{\infty} }.
    \end{equation}
    Vu que \( UD_s\) est inversible nous pouvons effectuer le changement de variables \( v\mapsto (UD_s)^{-1} v\) pour écrire
    \begin{equation}
        n(B)=\sup_{v\neq 0}  \frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| (UD_s)(UD_s)^{-1}v \|_{\infty} }=\sup_{v\neq 0}\frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| v \|_{\infty} }=\| (UD_s)B(UD_s)^{-1} \|_{\infty}=N(B).
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooYPLGooWKLbPA}
    Si \( A\in \eM(n,\eR)\) alors \( \rho(A)^m=\rho(A^m)\) pour tout \( m\in \eN\).
\end{proposition}

\begin{proof}
    La matrice \( A\) peut être vue dans \( \eM(n,\eC)\) et nous pouvons lui appliquer le corollaire~\ref{CORooTPDHooXazTuZ} :
    \begin{equation}        \label{EQooJJIYooDBacjn}
        \Spec(A^k)=\{ \lambda^k\tq \lambda\in\Spec(A) \}.
    \end{equation}
    À noter qu'il n'y a pas de magie : le spectre de la matrice réelle \( A\) est déjà défini en voyant \( A\) comme matrice complexe. Le spectre dont il est question dans \eqref{EQooJJIYooDBacjn} est bien celui dont on parle dans la définition du rayon spectral.

    Nous avons ensuite :
    \begin{subequations}
        \begin{align}
            \rho(A^k)&=\max\{ | \lambda |\tq \lambda\in\Spec(A^k) \}\\
            &=\max\{ | \lambda^k |\tq \lambda\in\Spec(A) \}\\
            &=\max\{ | \lambda |^k\tq\lambda\in\Spec(A) \}\\
            &=\rho(A)^k.
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}[Bornée si et seulement si continue\cite{GKPYTMb}]       \label{PROPooQZYVooYJVlBd}
    Soient \( E\) et \( F\) des espaces vectoriels normés. Une application linéaire \( E\to F\) est bornée si et seulement si elle est continue.
\end{proposition}

\begin{proof}
    Nous commençons par supposer que \( A\) est bornée. Par le lemme~\ref{LEMooFITMooBBBWGI}, pour tout \( x,y\in E\), nous avons
    \begin{equation}
        \| A(x)-A(y) \|=\| A(x-y) \|\leq \| A \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
    \begin{equation}
        0\leq \| A(x_n)-A(x) \|\leq \| A \|\| x_n-x \|\to 0
    \end{equation}
    et \( A\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition~\ref{PropFnContParSuite}.

    Nous supposons maintenant que \( \| A \|\) n'est pas borné : l'ensemble \( \{ \| A(x) \|\tq \| x \|=1 \}\) contient des valeurs arbitrairement grandes. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| A(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| A(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( A\) n'est pas continue.
\end{proof}

\begin{definition}[\cite{ooAISYooXtUafT}]      \label{DEFooTLQUooJvknvi}
    Soient \( E\) et \( F\) deux espaces vectoriels normés.
    \begin{itemize}
        \item
            L'ensemble des applications linéaires \( E\to F\) est noté \( \aL(E,F)\).
        \item Un \defe{morphisme}{morphisme!espace vectoriel normé} est une application linéaire \( E\to F\) continue pour la topologie de la norme opérateur. Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que la continuité était équivalente à être bornée. L'ensemble des morphismes est noté \( \cL(E,F)\)\nomenclature[B]{\( \cL(E,F)\)}{applications linéaires bornées (continues)}.
        \item
            Un \defe{isomorphisme}{isomorphisme!espace vectoriel normé} est un morphisme continu inversible dont l'inverse est continu. Nous notons \( \GL(E,F)\) l'ensemble des isomorphismes entre \( E\) et \( F\).
    \end{itemize}
\end{definition}

Le point important de la définition~\ref{DEFooTLQUooJvknvi} est la continuité. En dimension infine, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecNomrApplLin}

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
    La norme $2$ d'une matrice est liée au rayon spectral de la façon suivante :
    \begin{equation}
        \|A\|_2=\sqrt{\rho(A{^t}A)}
    \end{equation}
    ou plus généralement par \( \| A \|_2=\sqrt{\rho(A^*A)}\).
\end{theorem}

\begin{lemma}       \label{LEMooNESTooVvUEOv}
    Soit une matrice \( A\in \eM(n,\eR)\) qui est symétrique, strictement définie positive. Soient \( \lambda_{min}\) et \( \lambda_{max}\) les plus petites et plus grandes valeurs propres. Alors
    \begin{subequations}
        \begin{align}
            \| A \|_2=\lambda_{max}&&\text{ et }&&\|A^{-1}  \|_2=\frac{1}{ \lambda_{min} }.
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Soient les vecteurs \( v_1,\ldots, v_n\) formant une base orthonormée de vecteurs propres\footnote{Possible par le théorème spectral~\ref{ThoeTMXla}.} de \( A\). Nous notons \( v_{max}\) celui de \( \lambda_{max}\). Nous avons :
    \begin{equation}
        \| A \|_2\geq \| Av_{max} \|=| \lambda_{max} |\| v_{max} \|=| \lambda_{max} |=\lambda_{max}.
    \end{equation}
    Voilà l'inégalité dans un sens. Montrons l'inégalité dans l'autre sens. Soit \( x=\sum_ix_iv_i\) avec \( \| x \|_2=1\). Alors
    \begin{equation}
        \| Ax \|=\| \sum_ix_i\lambda_iv_i \|\leq\sqrt{ \sum_ix_i^2\lambda_i^2 }\leq \lambda_{max}\sqrt{ \sum_ix_i^2}=\lambda_{max}.
    \end{equation}

    En ce qui concerne l'affirmation pour la norme de \( A^{-1}\), il suffit de remarquer que ses valeurs propres sont les inverses des valeurs propres de \( A\).
\end{proof}

\begin{proposition} \label{PropMAQoKAg}
    La fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eR)\times \eM(n,\eR)&\to \eR \\
            (X,Y)&\mapsto \tr(X^tY)
        \end{aligned}
    \end{equation}
    est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
    Il faut vérifier la définition~\ref{DefVJIeTFj}.
    \begin{itemize}
        \item La bilinéarité est la linéarité de la trace.
        \item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
        \item L'application \( f\) est définie positive parce que si \( X\in \eM\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
    \end{itemize}
\end{proof}

\begin{example}
	Soit $m=n$, un point $\lambda$ dans $\eR$ et $T_{\lambda}$ l'application linéaire définie par $T_{\lambda}(x)=\lambda x$. La norme de $T_{\lambda}$ est alors
\[
\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
\]
Notez que $T_{\lambda}$ n'est rien d'autre que l'homothétie de rapport $\lambda$ dans $\eR^m$.
\end{example}

\begin{example}
	Considérons la rotation $T_{\alpha}$ d'angle $\alpha$ dans $\eR^2$. Elle est donnée par l'équation matricielle
	\begin{equation}
		T_{\alpha}\begin{pmatrix}
			x	\\
			y
		\end{pmatrix}=\begin{pmatrix}
			\cos\alpha	&	\sin\alpha	\\
			-\sin\alpha	&	\cos\alpha
		\end{pmatrix}\begin{pmatrix}
			x	\\
			y
		\end{pmatrix}=\begin{pmatrix}
			\cos(\alpha)x+\sin(\alpha)y	\\
			-\sin(\alpha)x+\cos(\alpha)y
		\end{pmatrix}
	\end{equation}
	Étant donné que cela est une rotation, c'est une isométrie : $\| T_{\alpha}x \|=\| x \|$. En ce qui concerne la norme de $T_{\alpha}$ nous avons
	\begin{equation}
		\| T_{\alpha} \|=\sup_{x\in\eR^2}\frac{ \| T_{\alpha}(x) \| }{ \| x \| }=\sup_{x\in\eR^2}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
	Toutes les rotations dans le plan ont donc une norme $1$. La même preuve tient pour toutes les rotations en dimension quelconque.
\end{example}

%TODO : le théorème de fuite des compacts qui dit qu'une solution de y'=f(y,t) cesse d'exister seulement si elle tend vers +- infini.

\begin{example}
  Soit $m=n$, un point $b$ dans $\eR^m$ et $T_b$ l'application linéaire définie par $T_b(x)=b\cdot x$ (petit exercice : vérifiez qu'il s'agit vraiment d'une application linéaire).  La norme de $T_b$ satisfait les inégalités suivantes
 \[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^n}\|x\cdot x\|_{\eR^n}\leq\|b \|_{\eR^n},
\]
\[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left\|b\cdot \frac{b}{\|b \|_{\eR^n}}\right\|_{\eR^n}=\|b \|_{\eR^n},
\]
donc $\|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}$.
\end{example}

\begin{proposition}
    Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
      Soit $x$ un point dans $\eR^m$. Nous devons vérifier l'égalité
      \begin{equation}
       \lim_{h\to 0_m}T(x+h)=T(x).
      \end{equation}
      Cela revient à prouver que $\lim_{h\to 0_m}T(h)=0$, parce que $T(x+h)=T(x)+T(h)$. Nous pouvons toujours majorer $\|T(h)\|_n$ par $\|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}$ (lemme~\ref{LEMooIBLEooLJczmu}). Quand $h$ s'approche de $ 0_m $ sa norme $\|h\|_m$ tend vers $0$, ce que nous permet de conclure parce que nous savons que de toutes façons, $\| T \|_{\aL}$ est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition~\ref{PROPooQZYVooYJVlBd}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Application linéaire continue et bornée}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que pour une application linéaire, être bornée est équivalent à être continue. Nous allons maintenant voir un certain nombre d'exemples illustrant ce fait.

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
    Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle $\| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}$ où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.
%TODO : le faire, et regarder si Hilbet n'est pas la complétion de cet espace. Référencer à l'endroit qui définit l'espace vectoriel librement engendré. Ici ce serait par N.

%TODO : dire qu'une application bilinéaire sur RxR n'est pas une application linéaire sur R^2

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]      \label{EXooDMVJooAJywMU}
    En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

    Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

    Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

    Nous avons utilisé le critère séquentiel de la continuité, voir la définition~\ref{DefENioICV} et la proposition~\ref{PropFnContParSuite}.
\end{example}

\begin{remark}  \label{RemOAXNooSMTDuN}
Cette proposition permet de retrouver l'exemple~\ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{example}     \label{EXooTQPEooRRdddt}
    Nous avons vu dans le lemme \ref{LEMooWFNXooLyTyyX} que pour un \( x\in E\) donné, l'application
    \begin{equation}
        \begin{aligned}
            ev_x\colon \aL(E,F)&\to F \\
            f&\mapsto f(x) 
        \end{aligned}
    \end{equation}
    est continue. Vu que \( ev_x\) est linéaire, la proposition \ref{PROPooQZYVooYJVlBd} nous indique que \( ev_x\) est bornée. Vérifions-le directement. Le calcul n'est pas très compliqué :
    \begin{equation}
        \| ev_x \|=\sup_{\| f \|=1}\| ev_x(f) \|=\sup_{\| f \|=1}\| f(x) \|\leq \sup_{\| f \|=1}\| x \|\| f \|=\| x \|
    \end{equation}
    où nous avons utilisé le lemme \ref{LEMooIBLEooLJczmu} en passant. Donc la norme de \( ev_x\) est majorée par \( \| x \|\).

    Elle est même égale à \( \| x \|\). En effet, pour chaque \( f\in \aL(E,F)\) tel que \(  \| f \|=1\), nous avons
    \begin{equation}
        \| ev_x \|\geq \| ev_x(f) \|=\| f(x) \|.
    \end{equation}
    En prenant \( f=\id\) nous trouvons \(  \| ev_x \|\geq \| x \|  \).
\end{example}

\begin{definition}      \label{DEFooKSDFooGIBtrG}
    Soit un espace vectoriel \( E\) sur le corps \( \eK\). Son \defe{dual topologique}{dual topologique}, noté \( E'\), est l'ensemble des formes linéaires continues de \( E\) vers \( \eK\).
\end{definition}

\begin{lemma}   \label{LemWWXVSae}
Soit \( F\) un espace de Banach et deux suites \( A_k\to A\) et \( B_k\to B\) dans \( \aL(F,F)\). Alors \( A_k\circ B_k\to A\circ B\) dans \( \aL(F,F)\), c'est-à-dire
\begin{equation}
    \lim_{n\to \infty} (A_kB_k)=\left( \lim_{n\to \infty} A_k \right)\left( \lim_{n\to \infty} B_k \right).
\end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'écrire
    \begin{equation}
        \| A_kB_k-AB \|\leq \| A_kB_k-A_kB \|+\| A_kB-AB \|.
    \end{equation}
    Le premier terme tend vers zéro pour \( k\to\infty\) parce que
    \begin{subequations}
        \begin{align}
            \| A_kB_k-A_kB \|&=\| A_k(B_k-B) \|\\
            &\leq \| A_k \|\| B_k-B \|\to \| A \|\cdot 0\\
            &=0
        \end{align}
    \end{subequations}
    où nous avons utilisé la propriété fondamentale de la norme opérateur : la proposition~\ref{PROPooQZYVooYJVlBd}. Le second terme tend également vers zéro pour la même raison.
\end{proof}

\begin{proposition}[Distributivité de la somme infinie] \label{PropQXqEPuG}
    Soient \( E\) un espace normé, une suite \( (u_k)\) dans \( \GL(E)\) ainsi que \( a\in\GL(E)\). Pourvu que la série \( \sum_{n=0}^{\infty}u_k\) converge nous avons
    \begin{equation}
        \left( \sum_{k=0}^{\infty}u_k \right)a=\sum_{k=0}^{\infty}(u_ka).
    \end{equation}
\end{proposition}

\begin{proof}
    Par définition de la somme infinie,
    \begin{equation}
        \spadesuit=\left( \sum_{k=0}^{\infty}u_k \right)a=\left( \lim_{n\to \infty} \sum_{k=0}^nu_k \right)a.
    \end{equation}
    Le lemme~\ref{LemWWXVSae} appliqué à \( n\mapsto\sum_{k=0}^nu_k\) et à la suite constante \( a\) nous donne
    \begin{equation}    \label{EqOAoopjz}
        \spadesuit=\lim_{n\to \infty} \left( \sum_{k=0}u_ka \right),
    \end{equation}
    ce que nous voulions par distributivité de la somme finie : dans \eqref{EqOAoopjz}, le \( a\) est dans ou hors de la somme, au choix. L'important est qu'il soit dans la limite.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Banach-Steinhaus}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{BIBooZUTUooNMvrdQ}]     \label{LEMooPIPLooMppGSO}
    Soient des espaces vectoriels normés \( X\) et \( Y\) ainsi qu'une application linéaire bornée \( T\colon X\to Y\). Pour tout \( a\in X\) et pour tout \( r>0\) nous avons
    \begin{equation}
        \sup_{x\in B(a,r)}\| Tx \|\geq r\| T \|
    \end{equation}
\end{lemma}

\begin{proof}
    Nous commençons avec \( a=0\). En utilisant la définition \ref{DefNFYUooBZCPTr} de la norme opérateur,
    \begin{equation}
        \| T \|=\sup_{x\in X}\frac{ \| Tx \| }{ \| x \| }=\sup_{x\in B(0,r)}\frac{ \| Tx \| }{ \| x \| }\leq \frac{1}{ r }\sup_{x\in B(0,r)}\| Tx \|.
    \end{equation}
    Donc
    \begin{equation}
        \sup_{x\in B(0,r)}\| Tx \|\geq r\| T \|.
    \end{equation}
    
    Il y a maintenant une astuce. Nous considérons un maximum :
    \begin{subequations}
        \begin{align}
            \max\{ \| T(a+x),\| T(a-x) \| \| \}&\geq \frac{ 1 }{2}\big( \| T(a+x) \|+\| T(a-x) \| \big) \label{SUBEQooPJPMooDkqRHs}\\
            &\geq \frac{ 1 }{2}\big( \| T(a+x)-T(a+x) \| \big)      \label{SUBEQooEZUUooVlKtfn}\\
            &=\frac{ 1 }{2}\| T(2x) \|\\
            &=\| Tx \|.
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooPJPMooDkqRHs}, la moyenne est plus petite que le maximum.
        \item Pour \eqref{SUBEQooEZUUooVlKtfn}, inégalité triangulaire : \( \| \alpha-\beta \|\leq \| \alpha \|+\| \beta \|\).
    \end{itemize}
    Si maintenant \( y\in B(a,r)\), nous avons \( y=a+x\) pour un certain \( x\in B(0,r)\), donc
    \begin{subequations}
        \begin{align}
            \sup_{y\in B(a,r)}\| Ty \|&=\sup_{x\in B(0,r)}\| T(a+x) \|\\
            &=\sup_{x\in B(0,r)}\max\{ \| T(a+x) \|, \| T(a-x) \| \}        \label{SUBEQooACJSooTHCAWs}\\
            &\geq \sup_{x\in B(0,r)}\| Tx \|\\
            &\geq r\| T \|.
        \end{align}
    \end{subequations}
    Pour \eqref{SUBEQooACJSooTHCAWs}, l'ensemble sur lequel nous prenons le supremum n'est pas modifié fondamentalement si nous regroupons les éléments deux à deux en prenant le maximum : les éléments exclus sont majorés.
\end{proof}

\begin{theorem}[Théorème de Banach-Steinhaus\cite{BIBooZUTUooNMvrdQ}]       \label{THOooJHVNooIDDxyT}
    Soient un espace de Banach\footnote{Définition \ref{DefVKuyYpQ}.} \( X\) et un espace vectoriel normé \( Y\). Soit une famille \( \mF\) d'opérateurs linéaire bornés. Si pour tout \( x\in  X\),
    \begin{equation}
        \sup_{T\in\mF}\| Tx \|<\infty,
    \end{equation}
    alors 
    \begin{equation}
        \sup_{T\in \mF}\| T \|<\infty.
    \end{equation}
\end{theorem}

\begin{proof}
    Nous supposons que \( \sup_{T\in\mF}\| T \|=\infty\), de telle sorte que nous pouvons choisir une suite \( (T_n)\) dans \( \mF\) telle que \( \| T_n \|\to \infty\). Cette suite peut diverger arbitrairement vite, et nous fixerons exactement cela plus tard.

    Soit par ailleurs une suite \( \alpha_n>0\) d'éléments petits et tels que \( \alpha_n\to 0\). Nous supposons que \( \sum_{n=0}^{\infty}\alpha_n<\infty\).

    Si \( a\in X\), le lemme \ref{LEMooPIPLooMppGSO} dit que
    \begin{equation}
        \sup_{x\in B(a,\alpha_n)}\| T_nx \|\geq \| T_n \|\alpha_n.
    \end{equation}
    En posant \( x_0=0\), nous construisons une suite \( (x_n)\) par récurrence en imposant
    \begin{enumerate}
        \item
            \( x_n\in B(x_{n=1}, \alpha_n)\)
        \item
            \( \| T_nx_n \|\geq \| T_n \|\alpha_n\).
    \end{enumerate}
    En utilisant une série télescopique et l'inégalité triangulaire \( \| x_k-x_{k+1} \|\leq \alpha_n\) à chaque étage,
    \begin{equation}
        \| x_p-x_q \|\leq \sum_{k=p}^q\alpha_k\leq \sum_{k=p}^{\infty}\alpha_k.
    \end{equation}
    Mais vu que la somme des \( \alpha_n\) converge, la suite des queues de somme converge vers zéro : \( \lim_{p\to \infty}\sum_{k=p}^{\infty}\alpha_n=0\). Cela implique que \( (x_n)\) est une suite de Cauchy. Vu que \( X\) est de Banach, la suite \( (x_n)\) a une limite dans \( X\). Soit \( x\) cette limite.

    Nous avons \( \beta_n=\| x_n-x \|\to 0\). Il y aurait moyen de calculer \( \beta_n\) en fonction de \( \alpha_n\) (surtout si nous avions donné une forme explicite à \( \alpha_n\)), mais c'est sans importance ici. L'important est que c'est une suite qui tend vers zéro.

    Nous avons
    \begin{equation}
        x\in B(x_n,\beta_n),
    \end{equation}
    et donc il existe \( a_n\in B(0,\beta_n)\) tel que \( x=x_n+a_n\). Avec cela, pour chaque \( n\) nous avons :
    \begin{subequations}
        \begin{align}
            \| T_nx \|&=\| T_n(x_n+a_n) \|\\
            &\geq\| T_nx_n \|-\| T_na_n \|\\
            &\geq \| T_nx_n \|-\| T_n \|\beta_n     \label{SUBEQooPLVQooChVCLU}\\
        &\geq \| T_n \|\alpha_n-\| T_n \|\beta_n\\
        &=\| T_n \|(\alpha_n-\beta_n).
        \end{align}
    \end{subequations}
    Pour \ref{SUBEQooPLVQooChVCLU}, nous avons utilisé \( \| T_na_n \|\leq \| T_n \|\beta_n\). En résumé,
    \begin{equation}
        \| T_nx \|\geq \| T_n \|(\alpha_n-\beta_n).
    \end{equation}
    Il suffit de choisir \( \| T_n \|\) suffisamment rapidement croissant pour que\footnote{Le point important ici est que \( \alpha_n\) (et donc \( \beta_n\)) est choisit sans référence à \( \| T_n \|\).}
    \begin{equation}
       \| T_n \|(\alpha_n-\beta_n)\to \infty,
    \end{equation}
    et nous avons \( \| T_nx \|\to \infty\), qui est contraire aux hypothèses.
\end{proof}

\begin{theorem}[Théorème de Banach-Steinhaus\cite{KXjFWKA,VPvwAaQ}] \label{ThoPFBMHBN}
    Soit \( E\) un espace de Banach\footnote{Définition~\ref{DefVKuyYpQ}.} et \( F\) un espace vectoriel normé. Nous considérons une partie \( H\subset \aL_c(E,F)\) (espace des fonctions linéaires continues). Alors \( H\) est uniformément borné si et seulement s'il est simplement borné.
\end{theorem}
\index{théorème!Banach-Steinhaus}
\index{application!linéaire!théorème de Banach-Steinhaus}

\begin{proof}
    Si \( H\) est uniformément borné, il est borné; pas besoin de rester longtemps sur ce sens de l'équivalence. Supposons donc que \( H\) soit borné. Pour chaque \( k\in \eN^*\) nous considérons l'ensemble
    \begin{equation}
        \Omega_k=\{ x\in E\tq \sup_{f\in H}\| f(x) \|>k \}.
    \end{equation}

    \begin{subproof}
        \item[Les \( \Omega_k\) sont ouverts]

            Soit \( x_0\in \Omega_k\); nous avons alors une fonction \( f\in H\) telle que \(  \| f(x_0) \|>k \), et par continuité de \( f\) il existe \( \rho>0\) tel que \( \| f(x) \|>k\) pour tout \( x\in B(x_0,\rho)\). Par conséquent \( B(x_0,\rho)\subset \Omega_k\) et \( \Omega_k\) est ouvert par le théorème~\ref{ThoPartieOUvpartouv}.

        \item[Les \( \Omega_k\) ne sont pas tous denses dans \( E\)]

            Nous supposons que les ensembles \( \Omega_k\) soient tous dense dans \( E\). Le théorème de Baire~\ref{ThoBBIljNM} nous indique que \( E\) est un espace de Baire (parce que de Banach) et donc que
            \begin{equation}
                \overline{ \bigcap_{k\in \eN}\Omega_k }=E.
            \end{equation}
            En particulier l'intersection des \( \Omega_k\) n'est pas vide. Soit \( x_0\in \bigcap_{k\in \eN}\Omega_k\). Nous avons alors
            \begin{equation}
                \sup_{f\in H}\| f(x) \|=\infty,
            \end{equation}
            ce qui est contraire à l'hypothèse. Donc les ouverts \( \Omega_k\) ne sont pas tous denses dans \(E\).

        \item[La majoration]

            Il existe \( k\geq 0\) tel que \( \Omega_k\) ne soit pas dense dans \( E\), et nous voulons prouver que \( \{ \| f \|\tq f\in H \}\) est un ensemble borné. Soit donc \( k\geq 0\) tel que \( \Omega_k\) ne soit pas dense dans \( E\); il existe un \( x_0\in E\) et \( \rho>0\) tels que
            \begin{equation}
                B(x_0,\rho)\cap \Omega_k=\emptyset.
            \end{equation}
            Si \( x\in B(x_0,\rho)\) alors \( x\) n'est pas dans \( \Omega_k\) et donc
            \begin{equation}
                \sup_{f\in H}\| f(x) \|\leq k.
            \end{equation}
            Afin d'évaluer \( \| f \|\) nous devons savoir ce qu'il se passe avec les vecteurs sur une boule autour de \( 0\). Pour tout \( x\in B(0,\rho)\) et pour tout \( f\in H\), la linéarité de \( f\) donne
            \begin{equation}
                \| f(x) \|=\| f(x+x_0)-f(x_0) \|\leq \| f(x+x_0)+f(x_0) \|\leq 2k.
            \end{equation}
            Par continuité nous avons alors \( \| f(x) \|\leq 2k\) pour tout \( x\in \overline{ B(0,\rho) }\). Si maintenant \( x\in F\) vérifie \( \| x \|=1\) nous avons
            \begin{equation}
                \| f(x) \|=\frac{1}{ \rho }\| f(\rho x) \|\leq \frac{ 2k }{ \rho },
            \end{equation}
            et donc \( \| f \|\leq \frac{ 2k }{ \rho }\), ce qui montre que \( 2k/\rho\) est un majorant de l'ensemble \( \{ \| f \|\tq f\in H \}\).

    \end{subproof}

\end{proof}
Une application du théorème de Banach-Steinhaus est l'existence de fonctions continues et périodiques dont la série de Fourier ne converge pas. Ce sera l'objet de la proposition~\ref{PropREkHdol}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Convergence forte}
%---------------------------------------------------------------------------------------------------------------------------

Lorsque nous avons une suite d'opérateurs linéaires, nous pouvons considérer la convergence d'une suite pour la norme opérateur : \( A_k\to A\) lorsque \( \| A_k-A \|\to 0\).

\begin{definition}[\cite{ooAGRZooTyUUVy}]       \label{DEFooNREQooElLvec}
    Soient un espace vectoriel \( E\) et un espace vectoriel normé \( V\). Nous disons que la suite d'opérateur \( T_k\colon E\to V\) \defe{converge fortement}{convergence forte} vers l'opérateur $T$ si pour tout \( x\in E\) nous avons
    \begin{equation}
        \| T_kx-Tx \|\to 0.
    \end{equation}
\end{definition}

Cette notion s'appelle \emph{forte} par opposition à la convergence \emph{faible} dont nous ne parlerons pas. Elle est cependant moins forte que la convergence en norme dont nous avons déjà parlé.

\begin{proposition}     \label{PROPooRFBLooUjSirP}
    Soient des espaces vectoriels normés \( E\) et \( F\) et une suite d'opérateurs \( T_k\colon E\to F\) convergeant vers \( T\)\footnote{Sans précisions, ce sera toujours la convergence en norme.}. Alors cette suite converge également fortement.
\end{proposition}

\begin{proof}
    Soit \( x\in E\) que nous supposons non nul. Soit \( \lambda\in \eC\) tel que \( x=\lambda y\) avec \( \| y \|=1\). Nous avons
    \begin{equation}
        \| T_kx-Tx \|=| \lambda |\| T_ky-Ty \|\leq | \lambda |\sup_{\| z \|=1}\| T_kz-Tz \|=| \lambda |\| T_k-T \|\to 0.
    \end{equation}
    La dernière étape est la convergence en norme \( T_k\to T\).
\end{proof}

\begin{proposition}
    Soient \( E\) et \( F\), des espaces vectoriels normés de dimension finie. Soit une suite \( (A_n)\) d'applications linéaires \( E\to F\). Si elle converge fortement vers \( A\), alors elle converge en norme vers \( A\).
\end{proposition}

\begin{proof}
    En plusieurs coups.
    \begin{subproof}
        \item[Si une sous-suite converge]
            Commençons par montrer que si \( (B_n)\) est une sous-suite de \( (A_n)\) qui converge vers \( B\), alors \( B=A\). Autrement dit, \( A\) est le seul candidat limite pour \( A_n\).

            Soit \( \| x \|=1\). Nous avons
            \begin{equation}
                \| B_nx-Bx \|\leq \| B_n-B \|\| x \|=\| B_n-B \|,
            \end{equation}
            mais pour la sous-suite \( (B_n)\) nous avons supposé \( \| B_n-B \|\to 0\). Donc \( \| B_nx-Bx \|\to 0\), ce qui signifie que \( B_nx\to Bx\). Mais par hypothèse, \( B_nx\to Ax\). Par unicité de la limite, \( Bx=Ax\) pour tout \( x\) de norme \( 1\). Pour les autres \( x\), c'est la linéarité qui conclu.

        \item[Utilisation de deux gros résultats]
        Par l'hypothèse de convergence, pour chaque \( x\) nous avons \( \sup_n\| A_nx \|<\infty\). Le théorème de Banach-Steinhaus \ref{THOooJHVNooIDDxyT} nous indique alors que l'ensemble \( \mF=\{ A_n \}_{n\in \eN}\) est borné. Il existe donc \( M > 0\) tel que \( \| A_n \|< M\) pour tout \( n\).

        Nous utilisons à présent l'hypothèse de dimension finie en disant que l'espace des applications linéaires \( E\to F\) est de dimension finie, de telle sorte que ses boules fermées soient compactes.

        Donc la suite \( (A_n)\) est contenue dans un compact.
        

        \item[Les sous-suite convergentes]

            La suite \( (A_n)\) est contenue dans un compact. Toutes ses sous-suites sont dans ce compact et possèdent donc une sous-suite convergente (théorème \ref{ThoBWFTXAZNH}). Toutes ces sous-sous-suites convergent nécessairement vers \( A\) par ce que nous avons dit dans la première étape de la preuve. Le lemme \ref{LEMooSJKMooKSiEGq} nous dit alors que \( A_n\to A\).
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit fini d'espaces vectoriels normés}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{sec_prod}

Dans cette sections nous parlons de produits finis d'espaces. Cela ne signifie pas que chacun des espaces soient séparément de dimension finie.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme}
%---------------------------------------------------------------------------------------------------------------------------

La définition de la norme sur un produit d'espaces vectoriels normés découle immédiatement de la définition de la distance~\ref{DefZTHxrHA} :
\begin{lemmaDef}  \label{DefFAJgTCE}
    Soient $V$ et $W$ deux espaces vectoriels normés. 
    \begin{enumerate}
        \item
            L'ensemble
            \begin{equation}
            V\times W=\{(v,w)\,|\, v\in V,\, w\in W\}
            \end{equation}
            est un espace vectoriel.
        \item 
            L'opération
            \begin{equation}	\label{EqNormeVxWmax}
                \|(v,w) \|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}.
            \end{equation}
            est une norme sur \( V\times V\).
            
    \end{enumerate}
    L'espace vectoriel \( V\times W\) muni de cette norme est l'\defe{espace produit}{produit!d'espaces vectoriels normés} de $V$ et $W$.
\end{lemmaDef}

\begin{proof}
    Il est presque immédiat de vérifier que le produit cartésien $V\times W$ est un espace vectoriel pour les opération de somme et multiplication par les scalaires définies composante par composante. C'est-à-dire,  si $(v_1,w_1)$, $(v_2,w_2)$ sont dans $V\times W$ et $a$, $b$ sont des scalaires, alors
    \begin{equation}
        a (v_1,w_1)+ b(v_2,w_2)=(av_1,aw_1)+ (bv_2,bw_2)=(av_1+bv_2,aw_1+bw_2).
    \end{equation}

	On doit vérifier les trois conditions de la définition~\ref{DefNorme}.
	\begin{itemize}
		\item Soit $(v,w)$ dans $V\times W$ tel que $\|(v,w)\|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}=0$. Alors $\|v\|_V=0$ et $\|w\|_W=0$, donc $v=0_V$ et $w=0_W$. Cela implique $(v,w)=(0_v,0_w)=0_{V\times W}$.
		\item Pour tout $a$ dans $\eR$ et $(v,w)$ dans $V\times W$, la norme $\|a (v,w)\|_{V\times W}$ se calcule de la façon suivante :
            \begin{equation}
                \|a (v,w)\|_{V\times W}= \max\{ \| av \|_V,\| aw \|_W \} =|a|\max\{\|v\|_{V},\|w\|_W\}=|a|\|(v,w)\|_{V\times W}.
            \end{equation}
		\item Soient $(v_1,w_1)$ et $(v_2,w_2)$ dans $V\times W$.
		\begin{equation}
			\begin{aligned}
				\|(v_1,w_1)+(v_2,w_2)\|_{V\times W}&=\max\{\|v_1+v_2\|_{V},\|w_1+w_2\|_W\}\\
				&\leq \max\{\|v_1\|_V+\|v_2\|_{V},\|w_1\|_W+\|w_2\|_W\}\\
				&\leq\max\{\|v_1\|_V,\|w_1\|_W\}+ \max\{\|v_2\|_{V},\|w_2\|_W\}\\
				&=\|(v_1,w_1)\|_{V\times W}+\|(v_2,w_2)\|_{V\times W}.
			\end{aligned}
		\end{equation}
	\end{itemize}
\end{proof}

Toutes ces définitions se généralisent à un produit fini d'espaces vectoriels normés. Si les espaces \( V_i\) sont des espaces vectoriels normés, nous pouvons mettre sur le produit une topologie et une norme :
\begin{itemize}
    \item La topologie produit donnée en~\ref{DefIINHooAAjTdY}
    \item La norme maximum \( \| v_1,\ldots, v_n \|_{max}=\max\{ \| v_1 \|,\ldots, \| v_n \| \}\). Dans le membre de droites, toutes les normes sont différentes.
\end{itemize}
Une question qui vient est la compatibilité entre ces deux constructions. Est-ce que la topologie associée à la norme maximum est le topologie produit ? Oui.

\begin{lemma}[\cite{ooALKGooMAzKpz}]       \label{LEMooWVVCooIGgAdJ}
    La topologie de la norme maximum est la topologie produit\footnote{Définition~\ref{DefIINHooAAjTdY}.}.
\end{lemma}

En particulier, pour la topologie de la norme maximum, la convergence d'une suite implique la convergence «composante par composante» par la proposition~\ref{PROPooNRRIooCPesgO}.

\begin{proposition}[\cite{ooCUHNooNYIeGt}]      \label{PROPooQFTSooPFfbCc}
    Soient des espaces vectoriels normés \( V\) et \( W\) ainsi qu'une forme sesquilinéaire \( \phi\colon V\times W\to \eC\). Il y a équivalence des faits suivants.
    \begin{enumerate}
        \item
            \( \phi\) est continue.
        \item
            \( \phi\) est continue en \( (0,0)\)
        \item
            \( \phi\) est bornée
        \item
            Il existe \( C\geq 0\) telle que \( | \phi(x,y) |\leq C\| x \|\| y \|  \) pour tout \( (x,y)\in V\times W\).
    \end{enumerate}
    De plus la norme de \( \phi\) est alors donnée par
    \begin{equation}
        \| \phi \|=\min\{  C\geq 0\tq | \phi(x,y) |\leq C\| x \|\| y \|\forall (x,y)\in V\times W  \}.
    \end{equation}
\end{proposition}

On remarque tout de suite que la norme $\|.\|_\infty$ sur $\eR^2$ est la norme de l'espace produit $\eR\times\eR$. En outre cette définition nous permet de trouver plusieurs nouvelles normes dans les espaces $\eR^p$. Par exemple, si nous écrivons $\eR^4$ comme $\eR^2\times \eR^2$ on peut munir $\eR^4$ de la norme produit
\[
\|(x_1,x_2,x_3,x_4)\|_{\infty, 2}=\max\{\|(x_1,x_2)\|_\infty, \|(x_3,x_4)\|_2\}.
\]
Les applications de projection de l'espace produit $V\times W$ vers les espaces <<facteurs>>, $V$ $W$ sont notées $\pr_V$ et $\pr_W$ et sont définies par
\begin{equation}
	\begin{aligned}
		\pr_V\colon V\times W&\to V \\
		(v,w)&\mapsto v
	\end{aligned}
\end{equation}
et
\begin{equation}
	\begin{aligned}
		\pr_W\colon V\times W &\to W \\
		(v,w)&\mapsto w.
	\end{aligned}
\end{equation}
Les inégalités suivantes sont évidentes
\begin{equation}
	\begin{aligned}[]
		\|\pr_V(v,w)\|_V&\leq \|(v,w)\|_{V\times W} \\
		\|\pr_W(v,w)\|_W&\leq \|(v,w)\|_{V\times W}.
	\end{aligned}
\end{equation}
La topologie de l'espace produit est induite par les topologies des espaces <<facteurs>>. La construction est faite en deux passages : d'abord nous disons que une partie $A\times B$ de $V\times W$ est ouverte si $A$ et $B$ sont des parties ouvertes de $V$ et de $W$ respectivement.  Ensuite nous définissons que une partie quelconque de $V\times W$ est ouverte si elle est une intersection finie ou une réunion de parties ouvertes de $V\times W$ de la forme $A\times B$.

Ce choix de topologie donne deux propriétés utiles de l'espace produit
\begin{enumerate}
	\item
		Les projections sont des \defe{applications ouvertes}{application!ouverte}. Cela veut dire que l'image par $\pr_V$ (respectivement $\pr_W$) de toute partie ouverte de $V\times W$ est une partie ouverte de $V$ (respectivement $W$).
	\item
		Pour toute partir $A$ de $V$ et $B$ de $W$, nous avons $\Int (A\times B)=\Int A\times \Int B$.\label{PgovlABeqbAbB}
\end{enumerate}
Une propriété moins facile a prouver est que pour toute partie $A$ de $V$ et $B$ de $W$ nous avons  $\overline{A\times B}=\bar{A}\times \bar{B}$. Voir le lemme~\ref{LemCvVxWcvVW}.
% position 26329
%et l'exercice~\ref{exoGeomAnal-0009}.

Ce que nous avons dit jusqu'ici est valable pour tout produit d'un nombre fini d'espaces vectoriels normés. En particulier, pour tout $m>0$  l'espace  $\eR^m$ peut être considéré comme le produit de $m$ copies de $\eR$.

\begin{example}
	Si $V$ et $W$ sont deux espaces vectoriels, nous pouvons considérer le produit $E=V\times W$. Les projections $\pr_V$ et $\pr_W$\nomenclature{$\pr_V$}{projection de $V\times W$ sur $V$}, définies dans la section~\ref{sec_prod}, sont des applications linéaires.

	En effet, la projection $\pr_V\colon V\times W\to V$ est donnée par $\pr_V(v,w)=v$. Alors,
	\begin{equation}
		\begin{aligned}[]
			\pr_V\big( (v,w)+(v',w') \big)&=\pr_V\big( (v+v'),(w+w') \big)\\
			&=v+v'\\
			&=\pr_V(v,w)+\pr_V(v',w'),
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\pr_V\big( \lambda(v,w) \big)=\pr_V\big( (\lambda v,\lambda w) \big)=\lambda v=\lambda\pr_V(v,w).
	\end{equation}
	Nous laissons en exercice le soin d'adapter ces calculs pour montrer que $\pr_W$ est également une projection.
\end{example}

\begin{proposition} \label{PropDXR_KbaLC}
    Si \( \mO\) est un voisinage de \( (a,b)\) dans \( V\times W\) alors \( \mO\) contient un ouvert de la forme \( B(a,r)\times B(b,r)\).
\end{proposition}

\begin{proof}
    Vu que \( \mO\) est un voisinage, il contient un ouvert et donc une boule
    \begin{equation}
        B\big( (a,b),r \big)=\{ (v,w)\in V\times W\tq \max\{ \| v-a \|,\| w-b \| \}< r \}.
    \end{equation}
    Évidemment l'ensemble \( B(a,r)\times B(b,r)\) est dedans.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Suites}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons maintenant parler de suites dans $V\times W$. Nous noterons $(v_n,w_n)$ la suite dans $V\times W$ dont l'élément numéro $n$ est le couple $(v_n,w_n)$ avec $v_n\in V$ et $w_n\in W$. La notions de convergence de suite découle de la définition de la norme via la définition usuelle~\ref{DefCvSuiteEGVN}. Il se fait que dans le cas des produits d'espaces, la convergence d'une suite est équivalente à la convergence des composantes. Plus précisément, nous avons le lemme suivant.
\begin{lemma}		\label{LemCvVxWcvVW}
	La suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$ si et seulement les suites $(v_n)$ et $(w_n)$ convergent séparément vers $v$ et $w$ respectivement dans $V$ et $W$.
\end{lemma}

\begin{proof}
	Pour le sens direct, nous devons étudier le comportement de la norme de $(v_n,w_n)-(v,w)$ lorsque $n$ devient grand. En vertu de la définition de la norme dans $V\times W$ nous avons
	\begin{equation}
		\Big\| (v_n,w_n)-(v,w) \Big\|_{V\times W}=\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}.
	\end{equation}
	Soit $\varepsilon>0$. Par définition de la convergence de la suite $(v_n,w_n)$, il existe un $N\in\eN$ tel que $n>N$ implique
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	et donc en particulier les deux inéquations
	\begin{subequations}
		\begin{align}
			\| v_n-v \|&<\varepsilon\\
			\| w_n-w \|&<\varepsilon.
		\end{align}
	\end{subequations}
	De la première, il ressort que $(v_n)\to v$, et de la seconde que $(w_n)\to w$.

	Pour le sens inverse, nous avons pour tout $\varepsilon$ un $N_1$ tel que $\| v_n-v \|_V\leq\varepsilon$ pour tout $n>N_1$ et un $N_2$ tel que $\| w_n-w \|_W\leq\varepsilon$ pour tout $n>N_2$. Si nous posons $N=\max\{ N_1,N_2 \}$ nous avons les deux inégalités simultanément, et donc
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	ce qui signifie que la suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]          \label{PROPooKDGOooDjWQct}
    Soit un espace \( E\) muni d'un produit scalaire à valeurs dans \( \eK\) (si \( \eK=\eC\) nous supposons le produit hermitien, mais ce n'est pas très important ici). Alors l'application
    \begin{equation}
        \begin{aligned}
            a\colon E\times E&\to \eK \\
            (x,y)&\mapsto \langle x, y\rangle
        \end{aligned}
    \end{equation}
    est continue.
\end{proposition}

\begin{proof}
    Nous ne disons pas que l'espace \( V\times V\) est muni d'un produit scalaire. Mais en tout cas c'est un espace métrique, et \( \eK\) l'est aussi. Donc \( a\) est une application entre deux espaces métriques et elle sera continue si et seulement si elle est séquentiellement continue (proposition~\ref{PropFnContParSuite}\ref{ItemWJHIooMdugfu}).

    Soit donc une suite convergente dans \( E\times E\), c'est-à-dire \( (x_k,y_k)\stackrel{E\times E}{\longrightarrow}(x,y)\). Nous devons démontrer que \( \langle x_k, y_k\rangle \stackrel{\eR}{\longrightarrow}\langle x, y\rangle \). Les majorations usuelles donnent
    \begin{subequations}
        \begin{align}
            \big| \langle x_k, y_k\rangle -\langle x, y\rangle  \big|&\leq \big| \langle x_k, y_k\rangle -\langle x, y_k\rangle  \big|+\big| \langle x, y_k\rangle -\langle x, y\rangle  \big|\\
            &=\big| \langle x_k-x, y_k\rangle  \big|+\big| \langle x, y_k-y\rangle  \big|.
        \end{align}
    \end{subequations}
    Nous savons du lemme~\ref{LemCvVxWcvVW} que les suites \( (x_k)\) et \( (y_k)\) sont séparément convergentes : \( x_k\stackrel{E}{\longrightarrow}x\) et \( y_k\stackrel{E}{\longrightarrow}y\). En utilisant l'inégalité de Cauchy-Schwarz~\ref{EQooZDSHooWPcryG} nous trouvons
    \begin{equation}
        \big| \langle x_k-x, y_k\rangle  \big|\leq \| x_k-x \|\| y_k \|.
    \end{equation}
    Nous avons \( \| x_k-x \|\to 0\) et \( \| y_k \|\to \| y \|\), et par la règle du produit de limites dans \( \eR\) nous avons que \( \big| \langle x_k-x, y_k\rangle  \big|\to 0\).
\end{proof}

\begin{remark}		\label{RemTopoProdPasRm}
	Il faut remarquer que la norme \eqref{EqNormeVxWmax} est une norme \emph{par défaut}. C'est la norme qu'on met quand on ne sait pas quoi mettre. Or il y a au moins un cas d'espace produit dans lequel on sait très bien quelle norme prendre : les espaces $\eR^m$. La norme qu'on met sur $\eR^2$ est
	\begin{equation}
		\| (x,y) \|=\sqrt{x^2+y^2},
	\end{equation}
	et non la norme «par défaut» de $\eR^2=\eR\times\eR$ qui serait
	\begin{equation}
		\| (x,y) \|=\max\{ | x |,| y | \}.
	\end{equation}
	Les théorèmes que nous avons donc démontré à propos de $V\times W$ ne sont donc pas immédiatement applicables au cas de $\eR^2$.

	Cette remarque est valables pour tous les espaces $\eR^m$. À moins de mention contraire explicite, nous ne considérons jamais la norme par défaut \eqref{EqNormeVxWmax} sur un espace $\eR^m$.
\end{remark}

Étant donné la remarque~\ref{RemTopoProdPasRm}, nous ne savons pas comment calculer par exemple la fermeture du produit d'intervalle $\mathopen] 0,1 ,  \mathclose[\times\mathopen[ 4 , 5 [$. Il se fait que, dans $\eR^m$, les fermetures de produits sont quand même les produits de fermetures.

\begin{proposition}		\label{PropovlAxBbarAbraB}
	Soit $A\subset\eR^m$ et $B\subset\eR^m$. Alors dans $\eR^{m+n}$ nous avons $\overline{ A\times B }=\bar A\times \bar B$.
\end{proposition}

La démonstration risque d'être longue; nous ne la faisons pas ici.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Continuité du produit de matrices}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooOAWAooFcyUfI}

Nous avons introduit des normes sur \( \eM(n,\eK)\), entre autres la norme opérateur de la définition~\ref{DefNFYUooBZCPTr}. Qui dit norme dit topologie. Il advient alors la question évidente : est-ce que des opérations aussi élémentaires que le produit de matrices sont continues pour ces topologies ?

Une façon simple de répondre à cela est d'introduire sur \( \eM(n,\eK)\) une nouvelle norme très simple : celle de \( \eK^n\). C'est la topologie par composante. Pour cette topologie, il est simple de voir que le produit matriciel est continu parce que les éléments de \( AB\) sont des polynômes en les éléments de \( A\) \( B\). Ensuite il suffit d'invoquer l'équivalence de toutes les normes (théorème~\ref{ThoNormesEquiv}).

Voyons comment montrer cela de façon plus directe (bien que le raisonnement précédent soit une démonstration qui devrait déjà avoir convaincu les plus sceptiques). La preuve suivante va donc s'amuser à bien préciser les topologies et caractérisations utilisées.

\begin{lemma}
    Si \( \| . \|\) est une norme algébrique sur \( \eM(n,\eK)\) (\( \eK\) est \( \eR\) ou \( \eC\)) alors l'application
    \begin{equation}
        \begin{aligned}
            p\colon \eM(n,\eK)\times \eM(n,\eK)&\to \eM(n,\eK) \\
            (A,B)&\mapsto AB
        \end{aligned}
    \end{equation}
    est continue.
\end{lemma}

\begin{proof}
    L'espace \( \eM(n,\eK)\times \eM(n,\eK)\) est métrique (définition~\ref{DefFAJgTCE}), donc la caractérisation séquentielle de la continuité (proposition~\ref{PropXIAQSXr}) s'applique. Nous considérons donc une suite \( (A_k,B_k)\) dans \( \eM(n,\eK)\times \eM(n,\eK)\) convergente vers \( AB\).

    Nous savons que la topologie sur \( \eM(n,\eK)\times \eM(n,\eK)\) est la topologie produit (lemme~\ref{LEMooWVVCooIGgAdJ}) et que celle-ci donne la convergence composante par composante dès que nous avons convergence d'une suite; c'est la proposition~\ref{PROPooNRRIooCPesgO}. Nous avons donc \( A_k\stackrel{\eM(n,\eK)}{\longrightarrow}A\) et \( B_k\stackrel{\eM(n,\eK)}{\longrightarrow}B\).

    Voilà pour le contexte. Maintenant, la preuve de la continuité. Nous effectuons les majorations suivantes :
    \begin{subequations}
        \begin{align}
            \| p(A_k,B_K)-AB \|&\leq \| p(A_k,B_k)-p(A_k,B) \|+\| p(A_k,B)-AB \|\\
            &=\| A_Kb_k-A_kB \|+\| A_kB-AB \|\\
            &=\| A_k(B_k-B) \|+\| (A_k-A)B \|\\
            &\leq \underbrace{\| A_k \|}_{\to \| A \|}\underbrace{\| B_k-B \|}_{\to 0}+\underbrace{\| A_k-A \|}_{\to 0}\| B \|.
        \end{align}
    \end{subequations}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications multilinéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Application multilinéaire]       \label{DefFRHooKnPCT}
    Une application $T: \eR^{m_1}\times \ldots \times\eR^{m_k}\to\eR^p $ est dite \defe{\( k\)-linéaire}{application!multilinéaire} si pour tout $X=(x_1, \ldots,x_k)$ dans $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ les applications $x_i\mapsto T(x_1, \ldots, x_i,\ldots,x_k)$ sont linéaires pour tout $i$ dans $\{1,\ldots,k\}$, c'est-à-dire
	\begin{equation}
		\begin{aligned}[]
			T(\cdot,x_2, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_1}, \eR^p),\\
			T(x_1,\cdot, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_2}, \eR^p),\\
						& \vdots\\
			T(x_1, \ldots, x_i,\ldots,x_{k-1},\cdot)&\in \mathcal{L}(\eR^{m_k}, \eR^p).\\
		\end{aligned}
	\end{equation}
	En particulier lorsque $k=2$, nous parlons d'applications \defe{bilinéaires}{bilinéaire}. Vous pouvez deviner ce que sont les applications \emph{tri}linéaire ou \emph{quadri}linéaire.
\end{definition}

L'ensemble des applications $k$-linéaires de $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ dans $\eR^p$ est noté $\mathcal{L}(\eR^{m_1}\times \ldots \times\eR^{m_k}, \eR^p)$ ou $\mathcal{L}(\eR^{m_1}, \ldots,\eR^{m_k}; \eR^p)$.

\begin{example}
  Soit $A$ une matrice avec $m$ lignes et $n$ colonnes. L'application bilinéaire de $\eR^m\times \eR^n$ dans $\eR$ associée à $A$ est définie par
\[
T_A(x,y)= x^TAy=\sum_{i,j}a_{i,j}x_i y_j, \qquad \forall x\in \eR^m, \, y \in \eR^n.
\]
\end{example}

Nous énonçons la proposition suivante dans le cas d'espaces vectoriels normés\footnote{Sans hypothèses sur la dimension.} parce que nous allons l'utiliser dans ce cas, mais le cas particulier \( E_i=\eR^{m_i}\) et \( F=\eR^p\) est important.
\begin{proposition} \label{PropUADlSMg}
    Soient des espaces vectoriels normés \( E_i\) et \( F\). Une application \( n\)-linéaire
    \begin{equation}
        T\colon E_1\times\ldots\times E_n\to F
    \end{equation}
    est est continue si et seulement s'il existe un réel $L\geq 0$ tel que
  \begin{equation}\label{limitatezza}
     \|T(x_1, \ldots,x_n)\|_F\leq L \|x_1\|_{F_1}\cdots\|x_n\|_{F_n}, \qquad \forall x_i\in E_i.
  \end{equation}
\end{proposition}

\begin{proof}
    Pour simplifier l'exposition nous nous limitons au cas $n=2$ et nous notons $T(x,y)=x*y$

    Supposons que l'inégalité \eqref{limitatezza} soit satisfaite.
    \begin{equation}\label{LimImplCont}
      \begin{aligned}
        \|x*y-x_0*y_0\|&=\|(x-x_0)*y-x_0*(y-y_0)\|\\
    &\leq \|(x-x_0)*y\|+\|x_0*(y-y_0)\|\\
    &\leq L\|x-x_0\|\|y\| + L\|x_0\|\|y-y_0\|.
      \end{aligned}
    \end{equation}
    Si $x\to x_0$ et $y\to y_0$  on voit que $T$ est continue en passant à la limite aux deux côtés de l'inégalité \eqref{LimImplCont}.

    Soit $T$ continue en $(0,0)$. Évidemment\footnote{Dans la formule suivante, les trois zéros sont les zéros de trois espaces différents.} $0*0=0$, donc il existe $\delta>0$ tel que si $x\in B_{E_1}(0,\delta)$ et $y\in B_{E_2}(0,\delta)$ alors $\|x*y\|\leq 1$. En particulier si \( (x,y)\in B_{E_1\times E_2}(0,\delta)\) nous sommes dans ce cas. Soient maintenant  $x\in E_1\setminus\{ 0 \}$  et $y\in E_2\setminus\{ 0\}$
    \begin{equation}
        x*y=\left(\frac{\|x\|}{\delta}\frac{\delta x}{\|x\|}\right)*\left(\frac{\|y\|}{\delta}\frac{\delta y}{\|y\|}\right)
    =\frac{\|x\|\|y\|}{\delta^2} \left(\frac{\delta x}{\|x\|}\right)*\left(\frac{\delta y}{\|y\|}\right).
     \end{equation}
    On remarque que $\delta x/\|x\|_m$ est dans la boule de rayon $\delta$ centrée en $0_m$ et que $\delta y/\|y\|_n$ est dans la boule de rayon $\delta$ centrée en $0_n$. On conclut
    \[
     x*y\leq \frac{\|x\|_m\|y\|_n}{\delta^2}.
    \]
    Il faut prendre $L=1/\delta^2$.
\end{proof}

La norme de \( T\) est alors définie comme la plus petite constante \( L\) qui fait fonctionner la proposition~\ref{PropUADlSMg}.
\begin{definition}  \label{DefKPBYeyG}
	La norme sur l'espace $\aL(E_1\times \cdots\times E_n, F)$ des applications $k$-linéaires et continues est
	\begin{equation}
        \|T\|_{E_1\times \ldots\times E_n}=\sup\{ \|T(u_1, \ldots,u_k)\|_{F}\,\vert\,\|u_i\|_{E_i}\leq 1, i=1,\ldots, k \}.
	\end{equation}
\end{definition}
Nous avons donc automatiquement
\begin{equation}    \label{EqYLnbRbC}
    \| T(u,v) \|\leq \| T \|\| u \|\| v \|.
\end{equation}
Et nous notons que cette norme est uniquement définie pour les applications linéaires continues. Ce n'est pas très grave parce qu'alors nous définissons \( \| T \|=\infty\) si \( T\) n'est pas continue. Cela pour retrouver le principe selon lequel on est continue si et seulement si on est borné.

\begin{proposition}\label{isom_isom}
  On définit les fonctions
  \begin{equation}
    \begin{array}{rccc}
      \omega_g: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{m}, \mathcal{L}(\eR^{n}, \eR^p)),\\
      \omega_d: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{n}, \mathcal{L}(\eR^{m}, \eR^p)),
    \end{array}
  \end{equation}
par
\[
\omega_g(T)(x)=T(x,\cdot), \qquad \forall x\in\eR^m,
\]
et
\[
\omega_d(T)(y)=T(\cdot, y), \qquad \forall y\in\eR^n.
\]
Les fonctions $\omega_g$ et $\omega_d$ sont des isomorphismes qui préservent les normes.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Séries}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooYCQBooSZNXhd}

Pour une définition plus générale de somme indexée par un ensemble infine, voir la définition \ref{DefIkoheE}.
\begin{definition}\label{DefGFHAaOL}
    Soit \( (a_k)\) une suite dans un espace vectoriel normé \( (V,\| . \| )\). La suite des \defe{sommes partielles}{somme!partielle} associée est la suite \( (s_k)\) définie par
    \begin{equation}
        s_k=\sum_{i=0}^ka_i
    \end{equation}
    La \defe{série}{série!dans un espace vectoriel normé} associée est la limite des sommes partielles
    \begin{equation}
        \sum_{n=0}^{\infty}a_k=\lim_{k\to \infty} \sum_{k=0}^na_k
    \end{equation}
    si elle existe.

    Si une telle limite existe nous disons que \( \sum_{k=0}^{\infty}a_k\) \defe{converge}{série convergente} dans \( V\). Si la limite de la suite des sommes partielles n'existe pas nous disons que la série \defe{diverge}{série divergente}.
\end{definition}

\begin{remark}
    Si la limite de la suite des sommes partielles n'existe pas dans \( V\), alors elle peut parfois exister dans des extensions de \( V\). Par exemple une série de rationnels convergeant vers \( \sqrt{2}\) dans \( \eR\) ne converge pas dans \( \eQ\). Autre exemple : avec une bonne topologie sur \( \bar \eR\), une série peut ne pas converger dans \( \eR\) mais converger vers \( \pm\infty\) dans \( \bar \eR\).
\end{remark}

Dans le cas des espaces de fonctions, nous avons une norme importante : la norme uniforme définie par \( \| f \|_{\infty}=\sup\{ f(x) \}\) où le supremum est pris sur l'ensemble de définition de \( f\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les trois types de convergence}
%---------------------------------------------------------------------------------------------------------------------------

Trois notions de convergence à ne pas confondre :
\begin{enumerate}
    \item
        La convergence absolue,
    \item
        la convergence normale. C'est la même que la convergence absolue, mais dans le cas particulier d'un espace de fonctions muni de la norme uniforme.
    \item
        la convergence uniforme.
\end{enumerate}
Voici les définitions.


\begin{definition}[Convergence absolue] \label{DefVFUIXwU}
    Nous disons que la série \( \sum_{n=0}^{\infty}a_n\) dans l'espace vectoriel normé \( V\) \defe{converge absolument}{convergence absolue} si la série \( \sum_{n=0}^{\infty}\| a_n \|\) converge dans \( \eR\).
\end{definition}

\begin{definition}[Convergence normale] \label{DefVBrJUxo}
    Une série de fonctions \( \sum_{n\in \eN}u_n \) converge \defe{normalement}{convergence normale} si la série de nombres \( \sum_n\| u_n \|_{\infty}\) converge. C'est-à-dire si la série converge absolument pour la norme \( \| f \|_{\infty}\).
\end{definition}


\begin{definition}[Convergence uniforme]
    La somme \( \sum_nf_n\) \defe{converge uniformément}{convergence uniforme!série de fonctions} vers la fonction \( F\) si la suite des sommes partielles converge uniformément, c'est-à-dire si
    \begin{equation}        \label{EqLNCJooVCTiIw}
        \lim_{N\to \infty} \| \sum_{n=1}^Nf_n-F \|_{\infty}=0.
    \end{equation}
\end{definition}

\begin{proposition} \label{PropAKCusNM}
    Une série convergeant absolument dans un espace de Banach\footnote{Un espace vectoriel normé complet. Typiquement \( \eR\).} y converge au sens usuel.
\end{proposition}

\begin{proof}
    Soit \( (a_k)\) une suite dans un espace vectoriel normé complet dont la série converge absolument. Nous allons montrer que la suite des sommes partielles est de Cauchy. Cela suffira à montrer sa convergence par hypothèse de complétude.

    Nous avons
    \begin{equation}
        \| s_p-s_l \|=\| \sum_{k=l+1}^{p}a_k\|  \leq\sum_{k=l+1}^p\| a_k \|=\bar s_p-\bar s_l
    \end{equation}
    où \( \bar s_n=\sum_{k=0}^n \| a_k \|\) est la suite des sommes partielles de la série des normes (qui converge). Vu que la suite \( (\bar s_n)\) converge dans \( \eR\), elle y est de Cauchy par la proposition~\ref{PROPooTFVOooFoSHPg}. Donc il existe un \( N\) tel que \( p,l>N\) implique
    \begin{equation}
        \| s_p-s_l \|=\bar s_p-\bar s_l\leq \epsilon.
    \end{equation}
    Cela signifie que \( (s_n)\) est une suite de Cauchy et donc convergente.
\end{proof}

\begin{example}[Si l'espace n'est pas complet\cite{MonCerveau}]
    Dans un espace pas complet, il est possible de construire un série qui converge absolument sans converger au sens usuel.

    Nous allons trouver dans \( \eQ\) une série qui converge simplement vers \( \sqrt{ 2 }\) (et donc ne converge pas dans \( \eQ\)) mais absolument vers \( 4\).

    La base est que si \( A,B\in \eQ\) avec \( A<B\) il est possible de résoudre
    \begin{subequations}
        \begin{numcases}{}
            r_1+r_2=A\\
            | r_1 |+| r_2 |=B
        \end{numcases}
    \end{subequations}
    pour \( r_1,r_2\in \eQ\). Ce n'est pas très compliqué : la solution est \( r_1=(A+B)/2\) et \( r_2=(A-B)/2\).

    Nous considérons l'espace \( \eQ\) qui n'est pas complet dans \( \eR\). Soit une série \( (a_k)\) dans \( \eQ\) qui converge vers \( \sqrt{ 2 }\) (convergence dans \( \eR\)) nous nommons \( (s_k)\) la suite des ses sommes partielles. Soit aussi la suite \( (b_k)\) qui converge vers \( 4\) (zéro serait encore plus facile mais bon, juste pour faire un peu de généralité).

    Nous supposons que \( a_k<b_k\) pour tout \( k\) et que les deux suites sont constituées de rationnels positifs. Nous nommons \( (s_k)\) et \( (s'_k)\) les sommes partielles. En particulier \( s_n<s'_n\) et ce sont des suites croissantes.

    Nous savons comment trouver \( r_1,r_2\in \eQ\) tels que \( r_1+r_2=s_1\) et \( | r_1 |+| r_2 |=s'_1\). Par récurrence, si nous savons \( r_1,\ldots, r_k\) tels que
    \begin{subequations}
        \begin{numcases}{}
            r_1+\ldots +r_k=s_n\\
            |r_1|+\ldots +|r_k|=s'_n
        \end{numcases}
    \end{subequations}
    (avec, soit dit en passant \( k=2n\)), alors nous pouvons trouver des rationnels \( r_{k+1}\), \( r_{k+2}\) tels que
    \begin{subequations}
        \begin{numcases}{}
            r_1+\ldots +r_k+r_{k+1}+r_{k+2}=s_{n+1}\\
            |r_1|+\ldots +|r_k|+|r_{k+1}|+|r_{k+2}|=s'_{n+1},
        \end{numcases}
    \end{subequations}
    en effet il s'agit de résoudre
    \begin{subequations}
        \begin{numcases}{}
            r_{k+1}+r_{k+2}=s_{n+1}-r_1-\ldots-r_k=s_{n+1}-s_n>0\\
            | r_{k+1} |+| r_{k+2} |=s'_{n+1}-| r_1 | -\ldots -| r_k |=s'_{n+1}-s'_n>0.
        \end{numcases}
    \end{subequations}
    Cela se résout comme plus haut. Au final nous pouvons construire une suite \( (r_k)\) dans \( \eQ\) telle que
    \begin{equation}
        \sum_{k=0}^{2n}r_k=s_n
    \end{equation}
    et
    \begin{equation}
        \sum_{k=0}^{2n}| r_k |=s'_n.
    \end{equation}
\end{example}

\begin{remark}
    Nous savons que sur les espaces vectoriels de dimension finie toutes les normes sont équivalentes (théorème~\ref{DefEquivNorm}). La notion de convergence de série ne dépend alors pas du choix de la norme. Il n'en est pas de même sur les espaces de dimension infinie. Une série peut converger pour une norme mais pas pour une autre.
\end{remark}
Lorsque nous verrons la convergence de séries, nous verrons que la convergence normale est la convergence absolue pour la norme uniforme.

\begin{lemma}       \label{LemCAIPooPMNbXg}
    Si \( E\) et \( F\) sont des espaces de Banach\quext{Je crois qu'il ne faut pas que \( E\) soit complet.}, l'espace \( \aL(E,F)\) est également de Banach.
\end{lemma}

\begin{proof}
    Soit \( (u_n)\) une suite de Cauchy dans \( \aL(E,F)\); si \( x\in E\) il existe \( N\) tel que si \( l,m>N\) alors \( \| u_l-u_m \|<\epsilon\), c'est-à-dire que pour tout \( \| x \|=1\) on a \( \| u_l(x)-u_n(x) \|<\epsilon\). Cela signifie que \( u_n(x)\) est une suite de Cauchy dans l'espace complet \( F\). Cette suite converge et nous pouvons définir l'application \( u\colon E\to F\) par
    \begin{equation}
        u(x)=\lim_{n\to \infty} u_n(x).
    \end{equation}
    Il suffit maintenant de prouver que \( u\) est linéaire, ce qui est une conséquence directe de la linéarité de la limite :
    \begin{equation}
        u(\alpha x+\beta y)=\lim_{n\to \infty} \big( \alpha u_n(x)+\beta u_n(y) \big).
    \end{equation}
\end{proof}

\begin{proposition}  \label{PROPooYDFUooTGnYQg}
    Si une série converge dans un espace complet, la norme de son terme général converge vers $0$.
\end{proposition}

\begin{proof}
    Soit une suite \( (a_n)\) dont la série converge vers \( s\). Soit \( \epsilon>0\). La suite des sommes partielles \( (s_n)\) est de Cauchy et converge vers \( s\) : \( s_n\to s\). En particulier il existe un \( N\) tel que si \( n>N\), nous avons \( \| s_n-s_{n-1} \|<\epsilon\). Pour de telles valeurs de \( n\) nous avons :
    \begin{equation}
        \| a_n \|=\| s_n-s_{n-1} \|\leq \epsilon.
    \end{equation}
    Cela prouve que \( a_n\to 0\).
\end{proof}

\begin{proposition}
    Si la série converge alors la somme est associative :
    \( \sum_k (a_k+b_k) = \sum_k a_k + \sum_k b_k \).
\end{proposition}

\begin{proof}
    Associativité. Supposons que \( \sum_ka_k\) et \( \sum_kb_k\) convergent tous deux. Alors nous avons pour tout \( N\) :
    \begin{equation}
        \sum_{k=0}^N(a_k+b_k)=\sum_{k=0}^Na_k+\sum_{k=0}^Nb_k.
    \end{equation}
    Mais si deux limites existent alors la somme commute avec la limite. C'est le cas pour la limite \( N\to \infty\), donc
    \begin{equation}
        \lim_{N\to \infty} \sum_{k=1}^{\infty}(a_k+b_k)=\lim_{N\to \infty} \sum_{k=0}^{\infty}a_k+\lim_{N\to \infty} \sum_{k=0}^{\infty}b_k.
    \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Série réelle}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{secseries}

La notion de série formalise le concept de somme infinie. L'absence de certaines propriétés de ces objets (problèmes de commutativité et même d'associativité) incite à la prudence et montre à quel point une définition précise est importante.


\subsection{Critères de convergence absolue}

Étant donné le terme général d'une série, il est souvent --dans les cas qui nous intéressent-- difficile de déterminer la somme de la série. L'exemple de la série géométrique est particulier, puisqu'on connaît une formule pour chaque somme partielle, mais pour l'exemple des séries de Riemann il n'y a aucune formule simple pour un $\alpha$ général. D'où l'intérêt d'avoir des critères de convergence ne nécessitant aucune connaissance de l'éventuelle limite de la série.

\begin{lemma}[Critère de comparaison]   \label{LemgHWyfG}
Soient $\sum_i a_i$ et $\sum_j
b_j$ deux séries à termes positifs vérifiant
\begin{equation*}
  0 \leq a_i \leq b_i
\end{equation*}
alors
\begin{enumerate}
\item si $\sum_i a_i$ diverge, alors $\sum_j b_j$ diverge,
\item si $\sum_j b_j$ converge, alors $\sum_i a_i$ converge
  (absolument).
  \end{enumerate}
\end{lemma}

\begin{proposition}[Critère d'équivalence\cite{TrenchRealAnalisys}]
 Soient $\sum_i a_i$ et $\sum_j b_j$ deux séries à termes positifs. Supposons l'existence de la limite (éventuellement infinie) suivante
\begin{equation}
  \limite i \infty \frac{a_i}{b_i} = \alpha
\end{equation}
avec \( \alpha\in \eR\cup\{ +\infty \}\). Alors
\begin{enumerate}
\item si $\alpha \neq 0$ et $\alpha\neq \infty$, alors
  \begin{equation}
    \sum_i a_i \text{~converge} \ssi \sum_j b_j\text{~converge,}
  \end{equation}
\item si $\alpha = 0$ et $\sum_j b_j$ converge, alors $\sum_i a_i$
  converge (absolument),
\item si $\alpha = +\infty$ et $\sum_j b_j$ diverge, alors $\sum_i
  a_i$ diverge.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
    \item
        Le fait que la suite $a_n/b_n$ converge vers $\alpha$ signifie que tant sa limite supérieure que sa limite inférieure convergent vers $\alpha$. En particulier la suite $\frac{ a_n }{ b_n }$ est bornée vers le haut et vers le bas. À partir d'un certain rang $N$, il existe $M$ tel que
        \begin{equation}
            \frac{ a_n }{ b_n }<M
        \end{equation}
        et il existe $m$ tel que
        \begin{equation}
            \frac{ a_n }{ b_n }>m.
        \end{equation}
        Nous avons donc $a_n<Mb_n$ et $a_n>mb_n$. La série de $(a_n)$ converge donc si et seulement si la série de $(b_n)$ converge.
    \item
        Si $\alpha=0$, cela signifie que pour tout $\epsilon$, il existe un rang tel que $\frac{ a_n }{ b_n }<\epsilon$, et donc tel que $a_n<\epsilon b_k$. La suite de $(a_i)$ converge donc dès que la suite de $(b_i)$ converge.
    \item
        Pour tout $M$, il existe un rang dans la suite à partir duquel on a $\frac{ a_i }{ b_i }>M$, et donc $a_k>Mb_k$. Si la série de $(b_k)$ diverge, la série de $(a_k)$ doit également diverger.
\end{enumerate}
\end{proof}


\begin{proposition}[Critère du quotient\cite{KeislerElemCalculus}]     \label{PropOXKUooQmAaJX}
    Soit $\sum_i a_i$ une série. Supposons l'existence de la limite (éventuellement infinie) suivante
    \begin{equation}
      \limite i \infty \abs{\frac{a_{i+1}}{a_i}} = L
    \end{equation}
    avec \( L\in \eR\cup\{ +\infty \}\).  Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L > 1$, la série diverge,
    \item si $L = 1$ le critère échoue : il existe des exemples de convergence et des exemples de divergence.
    \end{enumerate}
\end{proposition}
\index{critère du quotient}

\begin{proof}
\begin{enumerate}
    \item
        Soit $b$ tel que $L<b<1$. À partir d'un certain rang $K$, on a $\left| \frac{ a_{i+1} }{ a_i } \right| <b$. En particulier,
        \begin{equation}
            | a_{K+1} |<b| a_K |,
        \end{equation}
        et pour $a_{K+2}$ nous avons
        \begin{equation}
            | a_{K+2} |<b| a_{K+1} |<b^2| a_K |.
        \end{equation}
        Au final,
        \begin{equation}
            | a_{K+n} |<b^n| a_K |.
        \end{equation}
        Étant donné que la série $\sum_{n\geq K}b^n$ converge (parce que $b<1$), la queue de suite $\sum_{i\geq K}a_i$ converge, et par conséquent la suite au complet converge.
    \item
        Si $L>1$, on a
        \begin{equation}
            | a_K |<| a_{K+1} |<| a_{K+2} |<\ldots
        \end{equation}
        Il est donc impossible que la suite $(a_i)$ converge vers zéro. La série ne peut donc pas converger.
    \item
        Par exemple la suite harmonique $a_n=\frac{1}{ n }$ vérifie $L=1$, mais la série ne converge pas. Par contre, la suite $a_n=\frac{ 1 }{ n^2 }$ vérifie aussi le critère avec $L=1$ tandis que la série $\sum_n\frac{1}{ n^2 }$ converge.
\end{enumerate}
\end{proof}


\begin{proposition}[Critère de la racine\cite{TrenchRealAnalisys}]
    Soit $\sum_i a_i$ une série, et considérons
    \begin{equation*}
      \limsup_{i \rightarrow \infty} \sqrt[i]{\abs{a_i}} = L
    \end{equation*}
    avec \( L\in \eR\cup\{ +\infty \}\). Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L> 1$, la série diverge,
    \item si $L = 1$ le critère échoue.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si $L<1$, il existe un $r\in \mathopen] 0 , 1 \mathclose[$ tel que $| a_n |^{1/n}<r$ pour les grands $n$. Dans ce cas, $| a_n |<r^{n}$, et la série converge absolument parce que la série $\sum_nr^n$ converge du fait que $r<1$.
        \item
            Si $L>1$, il existe un $r>1$ tel que $| a_n |^{1/n}>r>1$. Cela fait que $| a_n |$ prend des valeurs plus grandes que $n$ pour une infinité de termes. Le terme général $a_n$ ne peut donc pas être une suite convergente. Par conséquent la suite diverge au sens où elle ne converge pas.

    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Critères de convergence simple}
%---------------------------------------------------------------------------------------------------------------------------

Les critères de comparaison, d'équivalence, du quotient et de la racine sont des critères de convergence absolue. Pour conclure à une convergence simple qui n'est pas une convergence absolue, le critère d'Abel sera notre outil principal.

\subsubsection{Critère d'Abel}

\begin{proposition}[Critère d'Abel]
    Soit la série $\sum_i c_iz_i$ avec
    \begin{enumerate}
        \item $(c_i)$ est une suite réelle décroissante qui tend vers zéro,
        \item $(z_i)$ est une suite dans $\eC$ dont la suite des sommes partielles est bornée dans $\eC$, c'est-à-dire qu'il existe un $M>0$ tel que pour tout $n$,
        \begin{equation}
            \left| \sum_{i=1}^nz_i \right| \leq M.
        \end{equation}
        Alors la série $\sum_ic_iz_i$ est convergente.
    \end{enumerate}
\end{proposition}
Remarquons que ce critère ne donne pas de convergence absolue.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques séries usuelles}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooDTYHooZjXXJf}

\begin{example}[Série harmonique]
    La \defe{série harmonique}{série!harmonique} est
    \begin{equation}
        \sum_{i=1}^\infty \frac1i
    \end{equation}
    et diverge (possède une limite $+\infty$).
\end{example}

\begin{example}[Série géométrique] \label{ExZMhWtJS}
    La \defe{série géométrique}{série!géométrique} de raison $q \in \eC$ est
    \begin{equation}    \label{EqZQTGooIWEFxL}
        \sum_{i=0}^\infty q^i.
    \end{equation}
    Étudions la somme partielle \( S_N=1+q+q^2+\cdots +q^{N}\). Nous avons évidemment $S_N-qS_N=1-q^{N+1}$ et donc
    \begin{equation}    \label{EqASYTiCK}
        S_N=\sum_{n=0}^Nq^n=\frac{ 1-q^{N+1} }{ 1-q }.
    \end{equation}
    La limite \( \lim_{N\to \infty} S_N\) existe si et seulement si \( | q |\leq 1\) et dans ce cas nous avons
    \begin{equation}    \label{EqRGkBhrX}
        \sum_{n=0}^{\infty}q^n=\frac{ 1 }{ 1-q }.
    \end{equation}
    La convergence est absolue.

    Si la somme commence en \( n=1\) au lieu de \( n=0\) alors
    \begin{equation}        \label{EqPZOWooMdSRvY}
        \sum_{n=1}^{\infty}q^n=\frac{1}{ 1-q }-1=\frac{ q }{ 1-q }.
    \end{equation}
\end{example}

Un cas particulier de la formule \eqref{EqASYTiCK} est le calcul de \( \sum_{j=1}^{N}q^{-j}\) bien utile lorsque l'on joue avec des nombres binaires (voir l'exemple~\ref{EXEMooRHENooGwumoA}). Nous avons
\begin{equation}        \label{EQooFMBAooEJkHWT}
    \sum_{j=1}^Nq^{-j}=\sum_{j=0}^Nq^{-j}-1=\frac{ 1-q^{-N} }{ q-1 }.
\end{equation}

\begin{example}[Série de Riemann]       \label{EXooCTYNooCjYQvJ}
    Pour $\alpha \in \eR$, la \defe{série de Riemann}{série!Riemann}
    \begin{equation}        \label{EqSerRiem}
        \sum_{i=1}^\infty \frac1{i^\alpha}
    \end{equation}
    converge (absolument, puisque réelle et positive) si et seulement si $\alpha > 1$, et diverge sinon.
\end{example}

\begin{example}[Série exponentielle] \label{ExIJMHooOEUKfj}
    La série exponentielle est la série (pour \( t\in \eR\))
    \begin{equation}
        \exp(t)=\sum_{k=0}^{\infty}\frac{ t^k }{ k! }.
    \end{equation}
    Nous montrons qu'elle converge pour tout \( t\in \eR\). Si \( a_k=t^k/k!\) alors \( \frac{ a_{k+1} }{ a_k }=\frac{ t }{ k }\) dont la limite \( k\to \infty\) est zéro (quel que soit \( t\)). En vertu du critère du quotient~\ref{PropOXKUooQmAaJX} la série exponentielle converge (absolument) pour tout \( t\in \eR\).

    Pour tout savoir de l'exponentielle et de ses variations, voir le thème~\ref{THEMEooKXSGooCsQNoY}.
\end{example}
\index{exponentielle!convergence}

\begin{example}[Série arithmético-géométrique\cite{QXuqdoo}]
    Une \defe{suite arithmético-géométrique}{suite!arithmético-géométrique} est une suite vérifiant pour tout \( n\) la relation
    \begin{equation}
        u_{n+1}=au_n+b
    \end{equation}
    avec \( a\) et \( b\) non nuls. Si elle possède une limite, cette dernière doit résoudre \( l=al+b\), et donc être donnée par
    \begin{equation}
        l=\frac{ b }{ 1-a }.
    \end{equation}

    Comportement amusant : la limite peut exister pour certains valeurs de \( a_0\) et pas pour d'autres. Mais elle ne dépend pas de \( a_0\) parmi ceux pour lesquelles la limite existe.

    Il n'est pas très compliqué de trouver le terme général de la suite en fonction de \( a\) et de \( b\). Il suffit de considérer la suite \( v_n=u_n-r\), et de remarquer que cette suite est géométrique :
    \begin{equation}
        v_{n+1}=av_n.
    \end{equation}
    Par conséquent \( v_n=a^nv_0\), ce qui donne pour la suite \( (u_n)\) la formule
    \begin{equation}
        u_n=a^n(u_0-r)+r.
    \end{equation}
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Séries alternées}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Critère des séries alternées\cite{ooXFPIooCLUvzV}]      \label{THOooOHANooHYfkII} 
    Si \( a\) est une suite réelle décroissante à limite nulle, alors
    \begin{enumerate}
        \item
            La série \( \sum_n(-1)^na_n\) converge,
        \item
            si nous notons \( (S_n)\) la suite des sommes partielles, les sous-suites \( (S_{2n})\) et \( (S_{2n+1})\) sont adjacentes de limite \( \sum_{n=1}^{\infty}(-1)^na_n\).
        \item
            Si nous considérons le reste 
            \begin{equation}
                R_n=\sum_{k=n+1}^{\infty}(-1)^ka_k,
            \end{equation}
            nous avons
            \begin{subequations}
                \begin{align}
                    \signe(R_n)=(-1)^{n+1}\\
                    | R_n |\leq a_{n+1}.
                \end{align}
            \end{subequations}
    \end{enumerate}
\end{theorem}

\begin{proof}
    En termes de notations, nous allons écrire \( (S_n)\) la suite des sommes partielles de \( \sum_{k=0}^{\infty}(-1)^ka_k\). Nous notons \( (S_{2n})\) la suite des termes pairs de cette suite. C'est donc la suite \( n\mapsto S_{2n}\).
    Nous divisons en plusieurs morceaux.
    \begin{subproof}
        \item[\( S_{2n}\) est croissante]
            Nous avons simplement
            \begin{equation}
                S_{2n+2}-S_{2n}=a_{2n+2}-a_{2n+1}\leq 0.
            \end{equation}
        \item[\( (S_{2n+1})\) est décroissante]
            Même calcul.
        \item[Les suites \( (S_{2n})\) et \( S_{2n+1}\) sont adjacentes] Nous avons simplement
            \begin{equation}
                S_{2n+1}-S_{2n}=a_{2n+1}\to 0.
            \end{equation}
            Nous concluons par le théorème des suites adjacentes \ref{THOooZJWLooAtGMxD} que les sous-suites des termes pairs et impairs sont convergentes et convergent vers la même limite.
    \end{subproof}
    C'est le moment d'utiliser la proposition \ref{PROPooXOOCooGMqJNe} qui convaincra la lectrice que \( (S_n)\) converge vers la même limite, que nous notons \( S\). Le théorème des suites adjacentes nous dit encore que 
    \begin{equation}
        S_{2n+1}\leq S\leq S_{2n}
    \end{equation}
    et donc que \( R_{2n}=S-S_{2n}\leq 0\). Cela donne la majoration
    \begin{equation}
        | R_{2n} |=| S-S_n |=S_{2n}-S\leq S_{2n}-S_{2n+1}=a_{2n+1}.
    \end{equation}
    Nous faisons le même genre de majorations pour \( R_{2n+1}\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Moyenne de Cesaro}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( (a_n)_{n\in \eN} \) est une suite dans \( \eR\) ou \( \eC\), alors sa \defe{moyenne de Cesaro}{moyenne!de Cesaro}\index{Cesaro!moyenne} est la limite (si elle existe) de la suite
    \begin{equation}
        c_n=\frac{1}{ n }\sum_{k=1}^na_k.
    \end{equation}
    En un mot, c'est la limite des moyennes partielles.
\end{definition}

\begin{lemma}       \label{LemyGjMqM}
    Si la suite \( (a_n)\) converge vers la limite \( \ell\) alors la suite admet une moyenne de Cesaro qui vaudra \( \ell\).
\end{lemma}

\begin{proof}
    Soit \( \epsilon>0\) et \( N\in \eN\) tel que \( | a_n-\ell |<\epsilon\) pour tout \( n>N\). En remarquant que
    \begin{equation}
        \frac{1}{ n }\sum_{k=1}^nk-\ell=\frac{1}{ n }\sum_{k=1}^n(a_k-\ell),
    \end{equation}
    nous avons
    \begin{subequations}
        \begin{align}
            | \frac{1}{ n }\sum_{k=1}^na_k-\ell |&\leq| \frac{1}{ n }\sum_{k=1}^N| a_k-\ell | |+\big| \frac{1}{ n }\sum_{k=N+1}^n\underbrace{| a_k-\ell |}_{\leq \epsilon} \big|\\
            &\leq \epsilon+\frac{ n-N-1 }{ n }\epsilon\\
            &\leq 2\epsilon.
        \end{align}
    \end{subequations}
    Dans ce calcul nous avons redéfinit \( N\) de telle sorte que le premier terme soit inférieur à \( \epsilon\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Écriture décimale d'un nombre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{normaltext}      \label{NORMALooTZWYooPMgOIm}
    Soit \( b\geq 2\) un entier qui sera la base dans laquelle nous allons écrire les nombres. Nous considérons l'ensemble \( \eD_b\)\nomenclature[Y]{\( \eD_b\)}{l'ensemble de écritures décimales en base \( b\)} des suites dans \( \{ 0,1,\ldots, b-1 \}\) qui n'ont pas une queue de suite uniquement formée de \( b-1\). Autrement dit une suite \( (c_n)\) est dans \( \eD_b\) lorsque pour tout \( N\), il existe \( k>N\) tel que \( c_k\neq b-1\). Associé à cet ensemble nous considérons la fonction
    \begin{equation}    \label{EqXXXooOTsCK}
        \begin{aligned}
            \varphi_b\colon \eD_b&\to \mathopen[ 0 , 1 [ \\
                c&\mapsto \sum_{n=1}^{\infty}\frac{ c_n }{ b^n }.
        \end{aligned}
    \end{equation}
\end{normaltext}

\begin{lemma}
    La fonction \( \varphi_b\) est bien définie au sens où elle converge et prend ses valeurs dans \( \mathopen[ 0 , 1 [\).
\end{lemma}

\begin{proof}
    Tout se base sur la somme de la série géométrique \eqref{EqRGkBhrX} sous la forme
    \begin{equation}    \label{EqWZGooXJgwl}
        \sum_{k=0}^{\infty}\frac{1}{ b^k }=\frac{ b }{ b-1 }.
    \end{equation}
    La somme \eqref{EqXXXooOTsCK} est donc majorée par \( \sum_n\frac{ b-1 }{ b^n }\) qui converge.

    Pour prouver que l'image de \( \varphi_b\) est bien \( \mathopen[ 0 , 1 [\), nous savons qu'au moins un des \( c_n\) (en fait une infinité) est plus petit que \( b-1\), donc nous avons la majoration stricte\footnote{Notez que la somme \eqref{EqXXXooOTsCK} commence à un tandis que la série géométrique \eqref{EqWZGooXJgwl} commence à zéro.}
        \begin{equation}
            \varphi_b(c)<\sum_{n=1}^{\infty}\frac{ b-1 }{ b^n }=(b-1)\left( \sum_{n=1}^{\infty}\frac{1}{ b^n }-1 \right)=1
        \end{equation}
\end{proof}

Le fait d'introduire l'ensemble \( \eD\) au lieu de l'ensemble de toutes les suites est justifié par la proposition suivante. Elle explique pourquoi un nombre possède au maximum deux écritures décimales distinctes et que ces deux sont obligatoirement de la forme, par exemple en base \( 10\) :
\begin{equation}
    0.34599999999\ldots=0.34600000\ldots
\end{equation}
mais qu'un nombre commençant par \( 0.347\) ne peut pas être égal. C'est pour cela que dans la définition de \( \eD_b\) nous avons exclu les suites qui terminent par tout des \( b-1\).
\begin{proposition} \label{PropSAOoofRlQR}
    Soit la fonction
    \begin{equation}
        \begin{aligned}
            \varphi\colon \{ 0,\ldots, b-1 \}^{\eN}&\to \mathopen[ 0 , 1 [ \\
                x&\mapsto \sum_{n=1}^{\infty}\frac{ x_n }{ b^n }.
        \end{aligned}
    \end{equation}
    Si \( \varphi(x)=\varphi(y)\) et si \( n_0\) est le plus petit entier tel que \( x_{n_0}\neq y_{n_0}\) alors soit
    \begin{equation}
        x_{n_0}-y_{n_0}=1
    \end{equation}
    et \( x_n=0\), \( y_n=b-1\) pour tout \( n>n_0\), soit le contraire : \( y_{n_0}-x_{n_0}=1\) avec \( y_n=0\) et \( x_n=b-1\) pour tout \( n>n_0\).
\end{proposition}

\begin{proof}
    Nous nous basons sur la formule (facilement dérivable depuis \eqref{EqWZGooXJgwl}) suivante :
    \begin{equation}
        \sum_{k=n_0+1}^{\infty}\frac{1}{ b^k }=\frac{1}{ b^{n_0+1} }\frac{ b }{ b-1 }.
    \end{equation}
    Nous avons
    \begin{equation}
        0=\varphi(x)-\varphi(y)=\frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }\geq \frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }-\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{ x_{n_0}-y_{n_0}-1 }{ b^{n_0} }.
    \end{equation}
    Le dernier terme étant manifestement positif\footnote{C'est ici qu'intervient la subdivision entre le cas \( x_{n_0}-y_{n_0}=1\) ou le contraire. En effet si «ce dernier terme était manifestement \emph{négatif}», il aurait fallu majorer avec de \( 1-b\) au lieu de \( 1-b\).}, il est nul et nous avons \( x_{n_0}-y_{n_0}=1\).

    Nous avons donc maintenant
    \begin{equation}    \label{EqHWQoottPnb}
        0=\varphi(x)-\varphi(y)=\frac{1}{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }.
    \end{equation}
    Nous majorons la dernière somme de la façon suivante, en supposant que \( | x_n-y_n |\neq b-1\) pour un certain \( n>n_0\) :
    \begin{equation}
        \left| \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n } \right| \leq\sum_{n=n_0+1}^{\infty}\frac{ | x_n-y_n | }{ b^n }<\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{1}{ b^{n_0} }.
    \end{equation}
    Étant donné cette inégalité stricte, l'équation \eqref{EqHWQoottPnb} ne peut pas être correcte (valoir zéro). Nous avons donc \( | x_n-b_n |=b-1\) pour tout \( n>n_0\). Donc pour chaque \( n>n_0\) nous avons soit \( x_n=0\) et \( y_n=b-1\), soit \( a_n=b-1\) et \( b_n=0\). Pour conclure il faut encore prouver que le choix doit être le même pour tout \( n\).

    Nous nous mettons dans le cas \( x_{n_0}-y_{n_0}=1\); dans ce cas nous avons bien l'égalité \eqref{EqHWQoottPnb} sans petites nuances de signes. Nous écrivons
    \begin{equation}
        \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }=(b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }
    \end{equation}
    où \( s_n\) est pair ou impair suivant que \( x_n=0\), \( y_n=b-1\) ou le contraire. Si un des \( (-1)^{s_n}\) est pas \( -1\) alors nous avons l'inégalité stricte
    \begin{equation}
        (b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }>(b-1)\sum_{n=n_0+1}^{\infty}\frac{-1}{ b^n }=-\frac{1}{ b^{n_0} }.
    \end{equation}
    Dans ce cas il est impossible d'avoir \( \varphi(x)-\varphi(y)=0\). Nous en concluons que \( (-1)^{s_n}\) est toujours \( -1\), c'est-à-dire \( x_n-y_n=1-b\), ce qui laisse comme seule possibilité \( x_n=0\) et \( y_n=b-1\).
\end{proof}

\begin{theorem} \label{ThoRXBootpUpd}
    L'application \( \varphi_b\colon \eD_b\to \mathopen[ 0 , 1 [\) est bijective.
\end{theorem}

\begin{proof}
    En ce qui concerne l'injection, nous savons de la proposition~\ref{PropSAOoofRlQR} que si \( \varphi_b(x)=\varphi_b(y)\) pour \( x,y\in\{ 0,\ldots, b-1 \}^{\eN}\), alors soit \( x\) soit \( y\) a une queue de suite composée uniquement de \( b-1\), ce qui est exclu dans \( \eD_b\). Nous en déduisons que \( \varphi_b\) est bien injective en prenant \( \eD_b\) comme ensemble départ.

    La partie lourde est la surjectivité. Nous prenons \( x\in \mathopen[ 0 , 1 [\) et nous allons construire par récurrence une suite \( a\in \eD_b\) telle que \( \varphi_b(a)=x\). Si il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que \( x=a_1/b\) alors nous prenons la suite \( (a_1,0,\ldots, )\) et nous avons évidemment \( \varphi(a)=x\). Sinon il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que
        \begin{equation}
            \frac{ a_1 }{ b }<x<\frac{ a_1+1 }{ b }
        \end{equation}
        parce que les autres possibilités pour \( x\) sont dans l'ensemble \( \mathopen[ 0 , 1 \mathclose[\setminus\{ \frac{ k }{ b } \}_{k=0,\ldots, b-1}\) que nous subdivisons en
        \begin{equation}
        \mathopen] 0 , \frac{1}{ b } \mathclose[\cup\mathopen] \frac{1}{ b } , \frac{ 2 }{ b } \mathclose[\cup\ldots\cup\mathopen] \frac{ b-1 }{ b } , 1 \mathclose[.
        \end{equation}
        Pour la récurrence nous supposons avoir trouvé \( a_1,\ldots, a_n\) tels que
        \begin{equation}
            \sum_{k=1}^n\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n-1}\frac{ a_k }{ b^k }+\frac{ a_n+1 }{ b^n }.
        \end{equation}
    Encore une fois s'il existe \( a_{n+1}\in\{ 0,\ldots, b-1 \}\) tel que \( \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }=x\) alors nous prenons ce \( a_{n+1}\) et nous complétons la suite avec des zéros pour avoir \( \varphi(a)=x\). Sinon
%nous subdivisions l'intervalle \( \mathopen]  \frac{ a_n }{ b^n }, \frac{ a_n }{ b^n }+\frac{ a_n+1 }{ b^n } \mathclose[\) (auquel nous retranchons les \( b\) nombres déjà traités) en
 %       \begin{equation}
 %       \mathopen] \frac{ a_n }{ b^n } , \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } \mathclose[ \cup \mathopen] \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{2}{ b^{n+1} } \mathclose[\cup\ldots\cup\mathopen] \frac{ a_n }{ b^n }+\frac{ b-1 }{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{ 1 }{ b^n } \mathclose[.
 %       \end{equation}
        , pour simplifier les notations nous notons \( x'=x-\sum_{k=1}^{n}\frac{ a_k }{ b^k }\) et nous avons
        \begin{equation}
            0<x'<\frac{ a_n+1 }{ b^n }.
        \end{equation}
        Le nombre \( x'\) est forcément dans un des intervalles
        \begin{equation}
                \mathopen] \frac{ s }{ b^{n+1} } , \frac{ s+1 }{ b^{n+1} } \mathclose[
        \end{equation}
        avec \( s\in\{ 0,\ldots, b-1 \}\). Nous prenons le \( s\) correspondant à \( x'\) comme \( a_{n+1}\). Dans ce cas nous avons
        \begin{equation}
            \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n+1}\frac{ a_k }{ b^k }+\frac{1}{ b^{n+1} }.
        \end{equation}
        Note : les deux inégalités sont strictes. La première parce que s'il y avait égalité, nous nous serions déjà arrêté en complétant avec des zéros. La seconde parce que
        \begin{equation}
            \sum_{k=n+2}^{\infty}\frac{ a_k }{ b^k }\leq \sum_{k=n+2}^{\infty}\frac{ b-1 }{ b^k }=\frac{1}{ b^{n+1} }
        \end{equation}
        où l'égalité n'est possible que si \( a_k=b-1\) pour tout \( k\geq n+2\). Dans ce cas nous aurions eu
        \begin{equation}
            x=\sum_{k=1}^{n}\frac{ a_k }{ b^k }+\frac{ a_{n+1}+1 }{ b^{n+1} }
        \end{equation}
        et nous aurions choisi le nombre \( a_{n+1}\) autrement et complété la suite par des zéros à partir de là. Notons que cela prouve au passage que la suite que nous sommes en train de construire est bien dans \( \eD_b\) parce qu'elle ne contiendra pas de queue de suite composée de \( b-1\).

        Ceci termine la construction par récurrence de la suite \( a\in \eD_b\). Par construction nous avons pour tout \( N\geq 1\),
        \begin{equation}
            \sum_{k=1}^N\frac{ a_k }{ b^k }\leq x\leq \sum_{k=1}^N\frac{ a_k }{ b^k }+\frac{1}{ b^{N+1} },
        \end{equation}
        autrement dit : \( \varphi_b(a_1,\ldots, a_N)\in B(x,\frac{1}{ b^{N+1} })\). Nous avons donc bien convergence
        \begin{equation}
            \lim_{N\to \infty} \varphi_b(a_1,\ldots, a_N)=x
        \end{equation}
        et l'application \( \varphi_b\) est surjective.
\end{proof}

L'application \( \varphi_b^{-1}\colon \mathopen[ 0 , 1 [\to \eD_b\) est la \defe{décomposition décimale}{décimale!décomposition} en base \( b\) des nombres de \( \mathopen[ 0 , 1 [\).

Tout cela nous permet de montrer entre autres que \( \eR\) n'est pas dénombrable. Vu qu'il y a une bijection entre \( \mathopen[ 0 , 1 [\) et \( \eD_b\), il suffit de prouver que \( \eD_b\) est non dénombrable. De plus il suffit de démontrer que \( \eD_b\) est non dénombrable pour un entier \( b\geq 2\) donné.

\begin{proposition}[\cite{KZIoofzFLV}]  \label{PropNNHooYTVFw}
    Il n'existe pas de surjection \( \eN\to \eD_b\). Autrement dit \( \eD_b\) est non dénombrable.
\end{proposition}

\begin{proof}
    Nous prenons \( b\neq 2\) pour des raisons qui seront claires plus tard. Soit \( f\colon \eN\to \eD_b\). Pour \( i\in \eN\) nous notons
    \begin{equation}
        f(n)=(c_i^{(n)})_{i\geq 1},
    \end{equation}
    et nous définissons la suite
    \begin{equation}
        c_k=\begin{cases}
            0    &   \text{si } c_k^{(k)}\neq 0\\
            1    &    \text{si } c_k^{(k)}=0.
        \end{cases}
    \end{equation}
    Cela est une suite dans \( \eD_b\) parce que \( b\neq 2\) et que la suite ne contient que des \( 0\) et des \( 1\). Mais nous n'avons \( f(n)=c\) pour aucun \( n\in \eN\) parce que nous avons \( c_n\neq f(n)_n\).

    Si \( b=2\) alors nous savons que \( \eD_2\sim\mathopen[ 0 , 1 [\sim \eD_3\). Donc \( \eD_2\sim \eD_3\) et \( \eD_2\) ne peut pas plus être mis en bijection avec \( \eN\) que \( \eD_3\).
\end{proof}
\begin{remark}
    La preuve ne fonctionne pas en base \( b=2\) parce que rien n'empêche d'avoir une queue de \( 1\). Il y a alors toutefois moyen de se débrouiller en construisant la suite \( c\) de façon plus subtile. Si \( b=2\) et \( n\in \eN\) alors \( f(n)\) est une suite de \( 0\) et \( 1\) contenant une infinité de \( 0\) (parce qu'il n'y a pas de queue de suite ne contenant que des \( 1\)). Nous construisons alors \( c\) de la façon suivante : d'abord nous recopions \( f(0)\) jusqu'à son \emph{deuxième} zéro que nous changeons en \( 1\); nommons \( n_0\) le rang de ce deuxième zéro. Ensuite nous recopions les éléments de \( f(1) \) à partir du rang \( n_0+1\) jusqu'au second zéro que nous changeons en \( 1\), etc.

    Le fait de prendre le deuxième zéro nous garanti que la suite \( c\) n'aura pas de queue de suite ne contenant que des \( 1\).

    Notons que cette construction s'adapte à tout \( b\); il suffit de prendre le second terme qui n'est pas \( b-1\) et le remplacer par \( b-1\).
\end{remark}

\begin{corollary}
    L'ensemble \( \mathopen[ 0 , 1 [\) n'est pas dénombrable.
\end{corollary}

\begin{proof}
    L'ensemble \( \mathopen[ 0 , 1 [\) est en bijection avec \( \eD_b\) que nous venons de prouver n'être pas dénombrable.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sommes de familles infinies}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooHHDXooUgLhHR}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Convergence commutative}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( x_k\) une suite dans un espace vectoriel normé \( E\). Nous disons que la suite \defe{converge commutativement}{convergence!commutative} vers \( x\in E\) si \( \lim_{n\to \infty}\| x_n-x \| =0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons aussi
    \begin{equation}
        \lim_{n\to \infty} \| x_{\tau(k)}-x \|=0.
    \end{equation}
    La notion de convergence commutative est surtout intéressante pour les séries. La somme
    \begin{equation}
        \sum_{k=0}^{\infty}x_k
    \end{equation}
    converge commutativement vers \( x\) si \( \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_k \|=0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons
    \begin{equation}
        \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_{\tau(k)} \|=0.
    \end{equation}
\end{definition}

Nous démontrons maintenant qu'une série converge réelle commutativement si et seulement si elle converge absolument.

\begin{proposition} \label{PopriXWvIY}
    Soit \( (a_i)_{i\in \eN}\) une suite absolument convergente\footnote{Définition \ref{DefVFUIXwU}.} dans \( \eC\). Alors elle converge commutativement.
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). Nous posons \( \sum_{i=0}^\infty a_i=a\) et nous considérons \( N\) tel que
    \begin{equation}
        | \sum_{i=0}^Na_i-a |<\epsilon.
    \end{equation}
    Étant donné que la série des \( | a_i |\) converge, il existe \( N_1\) tel que pour tout \( p,q>N_1\) nous ayons \( \sum_{i=p}^q| a_i |<\epsilon\). Nous considérons maintenant une bijection \( \tau\colon \eN\to \eN \). Prouvons que la série \( \sum_{i=0}^{\infty}| a_{\tau(i)} |\) converge. Nous choisissons \( M\) de telle sorte que pour tout \( n>M\), \( \tau(n)>N_1\). Si \( s_k\) est la somme partielle de la suite \( ( a_{\tau(i)} )_{i\in \eN}\) et si \( M<p<q \) nous avons
    \begin{equation}
        | s_q-s_p |= | \sum_{i=p}^q a_{\tau(i)} | \leq \sum_{i=p}^q| a_{\tau(i)} |<\epsilon.
    \end{equation}
    Cela montre que \( (s_k)\) est une suite de Cauchy. Elle est alors convergente et nous en déduisons que la série
    \begin{equation}
        \sum_{i=0}^{\infty}a_{\tau(i)}
    \end{equation}
    converge. Nous devons montrer à présent qu'elle converge vers la même limite que la somme «usuelle» \( \lim_{N\to \infty} \sum_{i=0}^Na_i\).

    Soit \( n>\max\{ M,N \}\). Alors
    \begin{equation}
        \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k=\sum_{k=0}^Ma_{\tau(k)}-\sum_{k=0}^Na_k+\underbrace{\sum_{M+1}^na_{\tau(k)}}_{<\epsilon}-\underbrace{\sum_{k=N+1}^na_k}_{<\epsilon}.
    \end{equation}
    Par construction les deux derniers termes sont plus petits que \( \epsilon\) parce que \( M\) et \( N\) sont les constantes de Cauchy pour les séries \( \sum a_{\tau(i)}\) et \( \sum a_i\). Afin de traiter les deux premiers termes, quitte à redéfinir \( M\), nous supposons que \( \{ 1,\ldots, N \}\subset \tau\{ 1,\ldots, M \}\); par conséquent tous les \( a_i\) avec \( i<N\) sont atteints par les \( a_{\tau(i)}\) avec \( i<M\). Dans ce cas, les termes qui restent dans la différence
    \begin{equation}
        \sum_{k=0}a_{\tau(k)}-\sum_{k=0}^Na_k
    \end{equation}
    sont des \( a_k\) avec \( k>N\). Cette différence est donc en valeur absolue plus petite que \( \epsilon\), et nous avons en fin de compte que
    \begin{equation}
        \left| \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k \right| <\epsilon.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PropyFJXpr}
    Soit \( \sum_{k=0}^{\infty}a_k\) une série réelle qui converge mais qui ne converge pas absolument. Alors pour tout \( b\in \eR\), il existe une bijection \( \tau\colon \eN\to \eN\) telle que \( \sum_{i=0}^{\infty}a_{\tau(i)}=b\).
\end{proposition}
Pour une preuve, voir \href{http://gilles.dubois10.free.fr/analyse_reelle/seriescomconv.html}{chez Gilles Dubois}.

Les propositions~\ref{PopriXWvIY} et~\ref{PropyFJXpr} disent entre autres qu'une série dans \( \eC\) est commutativement sommable si et seulement si elle est absolument sommable.

Soit \( (a_i)_{i\in I}\) une famille de nombres complexes indexée par un ensemble \( I\) quelconque. Nous allons nous intéresser à la somme \( \sum_{i\in I}a_i\).


Soit \( \{ a_i \}_{i\in I}\) des nombres positifs. Nous définissons la somme
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini}}\sum_{j\in J}a_j.
\end{equation}
Notons que cela est une définition qui ne fonctionne bien que pour les sommes de nombres positifs. Si \( a_i=(-1)^i\), alors selon la définition nous aurions \( \sum_i(-1)^i=\infty\). Nous ne voulons évidemment pas un tel résultat.

Dans le cas de familles de nombres réels positifs, nous avons une première définition de la somme.
\begin{definition}  \label{DefHYgkkA}
Soit \( (a_i)_{i\in I}\) une famille de nombres réels positifs indexés par un ensemble quelconque \( I\). Nous définissons
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini dans } I}\sum_{j\in J}a_j.
\end{equation}
\end{definition}

\begin{definition}  \label{DefIkoheE}
    Si \( \{ v_i \}_{i\in I}\) est une famille de vecteurs dans un espace vectoriel normé indexée par un ensemble quelconque \( I\). Nous disons que cette famille est \defe{sommable}{famille!sommable} de somme \( v\) si pour tout \( \epsilon>0\), il existe un \( J_0\) fini dans \( I\) tel que pour tout ensemble fini \( K\) tel que \( J_0\subset K\) nous avons
    \begin{equation}
        \| \sum_{j\in K}v_j-v \|<\epsilon.
    \end{equation}
\end{definition}
Notons que cette définition implique la convergence commutative.

\begin{example}
    La suite \( a_i=(-1)^i\) n'est pas sommable parce que quel que soit \( J_0\) fini dans \( \eN\), nous pouvons trouver \( J\) fini contenant \( J_0\) tel que \( \sum_{j\in J}(-1)^j>10\). Pour cela il suffit d'ajouter à \( J_0\) suffisamment de termes pairs. De la même façon en ajoutant des termes impairs, on peut obtenir \( \sum_{j\in J'}(-1)^i<-10\).
\end{example}

\begin{example}
    De temps en temps, la somme peut sortir d'un espace. Si nous considérons l'espace des polynômes \( \mathopen[ 0 , 1 \mathclose]\to \eR\) muni de la norme uniforme, la somme de l'ensemble
    \begin{equation}
        \{ 1,-1,\pm\frac{ x^n }{ n! } \}_{n\in \eN}
    \end{equation}
    est zéro.

    Par contre la somme de l'ensemble \( \{ 1,\frac{ x^n }{ n! } \}_{n\in \eN}\) est l'exponentielle qui n'est pas un polynôme.
\end{example}

\begin{example}     \label{EXooULLXooTDFYqf}
    Au sens de la définition~\ref{DefIkoheE} la famille
    \begin{equation}
        \frac{ (-1)^n }{ n }
    \end{equation}
    n'est pas sommable. En effet la somme des termes pairs est \( \infty\) alors que la somme des termes impairs est \( -\infty\). Quel que soit \( J_0\in \eN\), nous pouvons concocter, en ajoutant des termes pairs, un \( J\) avec \( J_0\subset J\) tel que \( \sum_{j\in J}(-1)^j/j\) soit arbitrairement grand. En ajoutant des termes négatifs, nous pouvons également rendre \( \sum_{j\in J}(-1)^j/j\) arbitrairement petit.
\end{example}

\begin{proposition} \label{PropVQCooYiWTs}
    Si \( (a_{ij})\) est une famille de nombres positifs indexés par \( \eN\times \eN\) alors
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}=\sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big)
    \end{equation}
    où la somme de gauche est celle de la définition~\ref{DefHYgkkA}.
\end{proposition}
%TODO : cette proposition peut être vue comme une application de Fubini pour la mesure de comptage. Le faire et référentier ici.

\begin{proof}
    Nous considérons \( J_{m,n}=\{ 0,\ldots, m \}\times \{ 0,\ldots, n \}\) et nous avons pour tout \( m\) et \( n\) :
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\Big( \sum_{j=1}^na_{ij} \Big).
    \end{equation}
    Si nous fixons \( m\) et que nous prenons la limite \( n\to \infty\) (qui commute avec la somme finie sur \( i\)) nous trouvons
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq =\sum_{i=1}^m\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cela étant valable pour tout \( m\), c'est encore valable à la limite \( m\to \infty\) et donc
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}

    Pour l'inégalité inverse, il faut remarquer que si \( J\) est fini dans \( \eN^2\), il est forcément contenu dans \( J_{m,n}\) pour \( m\) et \( n\) assez grand. Alors
    \begin{equation}
        \sum_{(i,j)\in J}a_{ij}\leq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\sum_{j=1}^na_{ij}\leq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cette inégalité étant valable pour tout ensemble fini \( J\subset \eN^2\), elle reste valable pour le supremum.
\end{proof}

La définition générale de la somme~\ref{DefIkoheE} est compatible avec la définition usuelle dans les cas où cette dernière s'applique.
\begin{proposition}[commutative sommabilité]\label{PropoWHdjw}
    Soit \( I\) un ensemble dénombrable et une bijection \( \tau\colon \eN\to I\). Soit \( (a_i)_{i\in I}\) une famille dans un espace vectoriel normé.  Si \( \sum_{i\in I}a_i\) existe, alors il est donné par
    \begin{equation}
        \sum_{i\in I}a_i=\lim_{N\to \infty} \sum_{k=0}^Na_{\tau(k)}.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous posons \( a=\sum_{i\in I}a_i\). Soit \( \epsilon>0\) et \( J_0\) comme dans la définition. Nous choisissons
    \begin{equation}
        N>\max_{j\in J_0}\{ \tau^{-1}(j) \}.
    \end{equation}
    En tant que sommes sur des ensembles finis, nous avons l'égalité
    \begin{equation}
        \sum_{k=0}^Na_{\tau(k)}=\sum_{j\in J_0}a_j
    \end{equation}
    où \( J\) est un sous-ensemble de \( I\) contenant \( J_0\). Soit \( J\) fini dans \( I\) tel que \( J_0\subset J\). Nous avons alors
    \begin{equation}
        \| \sum_{k=0}^Na_{\tau(k)}-a \|=\| \sum_{j\in J}a_j-a \|<\epsilon.
    \end{equation}
    Nous avons prouvé que pour tout \( \epsilon\), il existe \( N\) tel que \( n>N\) implique \( \| \sum_{k=0}^na_{\tau(k)}-a\| <\epsilon\).
\end{proof}

La réciproque n'est pas vraie. Même en supposant que \( \lim_{N\to \infty} \sum_{n=0}^Na_n\) existe, il n'est pas forcé que \( \sum_{n\in\eN}a_n\) existe. Cela est une conséquence de l'exemple \ref{EXooULLXooTDFYqf}.

\begin{corollary}       \label{CORooBPILooWDXpUM}       % Il ne faut pas référentier ce corollaire qui est sans doute faux.
    Nous pouvons permuter une somme dénombrable et une fonction linéaire continue. C'est-à-dire que si \( f\) est une fonction linéaire continue sur l'espace vectoriel normé \( E\) et \( (a_i)_{i\in I}\) une famille sommable dans \( E\) alors
    \begin{equation}
        f\left( \sum_{i\in I}a_i \right)=\sum_{i\in I}f(a_i).
    \end{equation}
\end{corollary}

\begin{probleme}
    À mon avis, ce corollaire est faux parce qu'il manque l'hypothèse que la famille \( f(a_i)\) est sommable. Voir la proposition \ref{PROPooWLEDooJogXpQ}.
\end{probleme}

\begin{proof}
    En utilisant une bijection \( \tau\) entre \( I\) et \( \eN\) avec la proposition~\ref{PropoWHdjw} ainsi que le résultat connu à propos des sommes sur \( \eN\), nous avons
    \begin{subequations}
        \begin{align}
            f\left( \sum_{i\in I}a_i \right)&=f\left( \sum_{k=0}^{\infty}a_{\tau(k)} \right)\\
            &=\sum_{k=0}^{\infty}f(a_{\tau(k)}) \label{SUBEQooCVUTooPmnHER}\\
            &=\sum_{i\in I}f(a_i).
        \end{align}
    \end{subequations}
    Notons que le passage à \eqref{SUBEQooCVUTooPmnHER} n'est pas du tout une trivialité à deux francs cinquante. Il s'agit d'écrire la somme comme la limite des sommes partielles, et de permuter \( f\) avec la limite en invoquant la continuité, puis de permuter \( f\) avec la somme partielle en invoquant sa linéarité.

    Ah, tiens et tant qu'on y est-à-dire qu'il y a des choses évidentes qui ne le sont pas, oui, il existe des applications linéaires non continues, voir le thème~\ref{THEMEooYCBUooEnFdUg}.
\end{proof}

La proposition suivante nous enseigne que les sommes infinies peuvent être manipulée de façon usuelle.
\begin{proposition} \label{PropMpBStL}
    Soit \( I\) un ensemble dénombrable. Soient \( (a_i)_{i\in I}\) et \( (b_i)_{i\in I}\), deux familles de réels positifs telles que \( a_i<b_i\) et telles que \( (b_i)\) est sommable. Alors \( (a_i)\) est sommable.

    Si \( (a_i)_{i\in I}\) est une famille de complexes telle que \( (| a_i |)\) est sommable, alors \( (a_i)\) est sommable.
\end{proposition}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooWLEDooJogXpQ}
    Soit un espace vectoriel normé \( E\) et une famille sommable\footnote{Définition~\ref{DefIkoheE}.} \( \{ v_i \}_{i\in I}\) d'éléments de \( E\). Soit \( f\colon E\to \eC\) une application sur laquelle nous supposons
    \begin{enumerate}
        \item
            \( f\) est linéaire et continue;
        \item
            la partie \( \{ f(v_i)_{i\in I} \} \) est sommable.
    \end{enumerate}
    Alors nous pouvons permuter la somme et \( f\) :
    \begin{equation}        \label{EQooONHXooKqIEbY}
        f\big( \sum_{i\in I}v_i \big)=\sum_{i\in I}f(v_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\); vu que les familles \( \{ v_i \}_{i\in I}\) et \( \{ f(v_i) \}_{i\in I}\) sont sommables, nous pouvons considérer les parties finies \( J_1\) et \( J_2\) de \( I\) telles que
    \begin{equation}
        \big\| \sum_{j\in J_1}v_j-\sum_{i\in I}v_i \big\|\leq \epsilon
    \end{equation}
    et
    \begin{equation}
        \big\| \sum_{j\in J_2}f(v_j)-\sum_{i\in I}f(v_i) \big\|\leq \epsilon
    \end{equation}
    Ensuite nous posons \( J=J_1\cup J_2\). Avec cela nous calculons un peu avec les majorations usuelles :
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|+  \| f(\sum_{j\in J}v_j)-\sum_i\in If(v_i) \|.
    \end{equation}
    Le second terme est majoré par \( \epsilon\), tandis que le premier, en utilisant la linéarité de \( f\) possède la majoration
    \begin{equation}
        \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|=\| f(\sum_{i\in I}v_i-\sum_{j\in J}v_j) \|\leq \| f \| \| \sum_{i\in I}v_i- \sum_{j\in J}v_j\|\leq \epsilon\| f \|.
    \end{equation}
    Donc pour tout \( \epsilon>0\) nous avons
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \epsilon(1+\| f \|).
    \end{equation}
    D'où l'égalité \eqref{EQooONHXooKqIEbY}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Produit tensoriel d'espaces vectoriels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Si vous êtes pressés, vous pouvez aller lire la définition \ref{DEFooKTVDooSPzAhH} de produit tensoriel d'espaces vectoriels. Mais si vous étiez vraiment pressés, vous ne seriez pas en train de lire des choses sur le produit tensoriel (il vous suffit de croire que \( x\otimes y\) n'est finalement que la concatenation de \( x\) et \( y\)).

\begin{definition}
    Soient un espace vectoriel \( V\) et un sous-espace \( N\). Le \defe{quotient}{quotient d'un espace vectoriel} de \( V\) par \( N\), noté \( V/N\) est l'ensemble des classes d'équivalence pour la relation \( x\sim y\) si et seulement si \( x-y\in N\).
\end{definition}

\begin{proposition}
    Soient un espace vectoriel \( V\) et un sous-espace vectoriel \( N\) de \( V\). Les définitions
    \begin{enumerate}
        \item
            \( [v]+[w]=[v+w]\)
        \item
            \( \lambda[v]=[\lambda v]\)
    \end{enumerate}
    ont un sens et définissent une structure d'espace vectoriel sur \( V/N\).
\end{proposition}

\begin{proof}
    Un élément général de la classe \( [v]\) est de la forme \( v+n\) avec \( n\in N\). Le calcul suivant montre que la somme fonctionne : 
    \begin{equation}
        [v+n_1]+[w+n_2]=[v+w+n_1+n_2]=[v+w]
    \end{equation}
    parce que \( n_1+n_2\in N\). De même,
    \begin{equation}
        \lambda[v+n]=[\lambda v+\lambda n]=[\lambda v]
    \end{equation}
    toujours parce que \( \lambda n\in N\).

    Notons que nous avons utilisé de façon on ne peut plus cruciale le fait que \( N\) soit un sous-espace vectoriel.
\end{proof}

\begin{proposition}
    Si \( \{ e_i \}\) est une base de \( V\) et si \( N\) est un sous-espace de \( V\), alors \( \{ [e_i] \}\) est une partie génératrice de \( V/N\).
\end{proposition}

\begin{proof}
    Si \( x=\sum_kx_ke_k\), alors \( [x]=\sum_kx_k[e_k]\), donc oui.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Somme directe d'espaces vectoriels}
%---------------------------------------------------------------------------------------------------------------------------

Si \( V\) et \( W\) sont des espaces vectoriels, ce que nous notons \( V\oplus W\) n'est rien d'autre que l'espace vectoriel de l'ensemble \( V\times W\).

\begin{propositionDef}[\cite{ooXISFooTypogf}]
    Si \( V\) et \( W\) sont des espaces vectoriels sur le même corps \( \eK\), alors les définitions
    \begin{enumerate}
        \item
            \( (v_1,w_1)+(v_2,w_2)=(v_1+v_2,w_1+w_2)\)
        \item
            \( \lambda(v,w)=(\lambda v,\lambda w)\)
    \end{enumerate}
    donnent une structure d'espace vectoriel sur \( V\times W\). 

    Cet espace sera noté \( V\oplus W\) et est appelé \defe{somme directe}{somme directe} de \( V\) et \( W\).
\end{propositionDef}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les produits tensoriels}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons procéder en deux temps. D'abord nous allons définir ce qu'est \emph{un} produit tensoriel entre deux espaces vectoriels \( V\) et \( W\), et nous allons montrer que tous les produits tensoriels possibles sont isomorphes. Ensuite nous allons montrer qu'un produit tensoriel existe en en construisant un. Voir la proposition \ref{PROPooIWZDooRRZNCf}.

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooXKKQooAvWRNp}
    Soient deux espaces vectoriels \( V\) et \( W\). Un \defe{produit tensoriel}{produit tensoriel} de \( V\) et \( W\) est un couple \( (T,h)\) où \( T\) est un espace vectoriel et \( h\colon V\oplus W\to T\) est une application
    \begin{enumerate}
        \item
            bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.}
        \item
            surjective
        \item       \label{ITEMooJCNYooGvjjtL}
            telle que pour tout espace vectoriel \( U\) et toute applications bilinéaire \( f\colon V\oplus W\to U\), il existe une application linéaire \( g\colon T\to U\) telle que \( f=g\circ h\).
    \end{enumerate}
    La propriété \ref{ITEMooJCNYooGvjjtL} est appelée \defe{propriété universelle}{propriété universelle} du produit tensoriel.
\end{definition}

\begin{definition}  \label{DEFooPLHTooRiHjlE}
    Un \defe{morphisme}{morphisme de produits tensoriels} entre \( (T,h)\) et \( (T',h')\) est une application linéaire \( \psi\colon T\to T'\) telle que \( h'=\psi\circ h\).

    Nous parlons d'\defe{isomorphisme}{isomorphisme} si \( \psi\) a un inverse qui est également un morphisme.
\end{definition}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]      \label{PROPooROPHooQXqNzZ}
    Si \( V\) et \( W\) sont des espaces vectoriels, tous les produits tensoriels entre \( V\) et \( W\) sont isomorphes entre eux au sens de la définition \ref{DEFooPLHTooRiHjlE}.

    Plus précisément, si \( (T,h)\) et \( (T',h')\) sont deux produits tensoriels de \( V\) et \( W\), alors 
    \begin{enumerate}
        \item
            il existe une unique unique application linéaire \( g\colon T\to T'\) telle que \( h'=g\circ h\),
        \item
            cette application \( g\) est inversible.
    \end{enumerate}
    En particulier, l'application \( g\) est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Soient deux produits tensoriels \( (T,h)\) et \( (T',h')\). 

    \begin{subproof}
        \item[Existence]
    
    L'application \( h'\colon V\oplus W\to T'\) est bilinéaire, et \( (T,h)\) est un produit tensoriel. Donc il existe \( g\colon T\to T'\) tel que \( h'=g\circ h\). De même, il existe une application \( g'\colon T'\to T\) telle que \( h=g'\circ h\).

\item[Unicité]

    En ce qui concerne l'unicité, vu que \( h\colon V\oplus W\to T\) est surjective, la relation \( h'=g\circ h\) prescrit les valeurs de \( g\) sur tous les éléments de \( T\).

\item[Inversible]
    
    Ces deux applications \( g\) et \( g'\) vérifient $h'=gg'h$ et $h=g'gh$, et de plus \( h\colon V\oplus W\to T\) est surjective. Soient \( t\in T\) et \( x\in V\oplus W\) tel que \( t=h(x)\). Nous avons \( h(x)=g'gh(x)\). C'est-à-dire \( t=(g'\circ g)(t)\). De même dans l'autre sens, il existe \( x'\in V\oplus W\) tel que \( t=h'(x')\). En appliquant l'égalité \( h'=gg'h'\) à \( x'\), nous trouvons \( t=(g\circ g')(t)\).

    Tout cela pour dire que \( g'=g^{-1}\). Cette application \( g\) est donc un isomorphisme de produits tensoriels entre \( (T,h)\) et \( (T',h')\).
    \end{subproof}
    Au final, l'application \( g\colon T\to T'\) étant linéaire et inversible, elle est un isomorphisme d'espaces vectoriels.
\end{proof}

Tout cela est fort bien : nous avons unicité à isomorphisme près du produit tensoriel d'espaces vectoriels. Mais nous n'avons pas encore de certitudes à propos de l'existence d'un couple \( (T,h)\) vérifiant les propriétés demandées pour être un produit tensoriel.

Nous allons maintenant construire un produit tensoriel.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Le produit tensoriel}
%---------------------------------------------------------------------------------------------------------------------------

C'est le moment pour vous de relire la définition \ref{DEFooCPNIooNxsYMY} d'espace vectoriel librement engendré, et surtout le lemme \ref{LEMooLOPAooUNQVku} qui en donne une base.

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooKTVDooSPzAhH}
    Soient deux espaces vectoriels \( V\) et \( W\) sur le corps commutatif\footnote{À part mention du contraire, tous les corps du Frido sont commutatifs.} \( \eK\). Dans \( F_{\eK}(V\times W)\) nous considérons les sous-espaces suivants:
    \begin{subequations}
        \begin{align}
            A_1&=\{ \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)}\tq v_1,v_2\in V,w\in W  \}\\
            A_2&=\{ \delta_{(v,w_1)}+\delta_{(v,w_2)}-\delta_{(v,w_1+w_2)}\tq v\in V,w_1,w_2\in W  \} \label{SUBEQooSHBJooJLPVbK} \\
            A_3&=\{ \lambda\delta_{v,w}-\delta_{(\lambda v, w)}\tq v\in V,w\in W,\lambda\in \eK \}\\
            A_4&=\{ \lambda\delta_{v,w}-\delta_{(v,\lambda w)}\tq v\in V,w\in W,\lambda\in \eK \}.
        \end{align}
    \end{subequations}
    Nous considérons alors \( N=\Span(A_1,A_2,A_3,A_4)\) et le quotient
    \begin{equation}
        V\otimes_{\eK}W=F_{\eK}(V\times W)/N.
    \end{equation}
    Ce dernier espace vectoriel est le \defe{produit tensoriel}{produit tensoriel} de \( V\) par \( W\).
\end{definition}

\begin{remark}      \label{REMooSLEGooWEiutz}
    Quelque remarques.
    \begin{enumerate}
        \item
            Les éléments de \( V\otimes W\) ne s'écrivent pas tous sous la forme \( v\otimes w\). Certains ont vraiment besoin d'être écrits avec des sommes. En cela, la situation de \( V\otimes W\) est réellement différente de celle de \( V\times W\). Dans ce dernier, tous les éléments sont des couples.
        \item
            La classe de l'élément \( \delta_{(v,w)}\in F(V\times W)\) sera d'habitude noté \( v\otimes w\).
        \item
            Pour insister sur la notion de classe, nous allons aussi noter \( [x]\) la classe de \( x\in F(V\times W)\).
        \item       \label{ITEMooPVWHooMkgQoT}
            L'arithmétique dans \( V\otimes W\) est relativement simple. En ajoutant et soustrayant le même élément de \( A_3\) nous avons par exemple
            \begin{equation}
                (\lambda v)\otimes w=(\lambda v)\otimes w+\lambda (v\otimes w)-(\lambda v)\otimes w.
            \end{equation}
            Nous obtenons de cette façon
            \begin{equation}
                \lambda(v\otimes w)=(\lambda v)\otimes w=v\otimes (\lambda w),
            \end{equation}
            que nous noterons \( \lambda v\otimes w\) sans plus de précision.
    \end{enumerate}
\end{remark}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]     \label{PROPooIWZDooRRZNCf}
    L'espace vectoriel \( V\times W\) muni de
    \begin{equation}
        \begin{aligned}
            h\colon V\oplus W&\to V\otimes W \\
            (v,w) &\mapsto v\otimes w 
        \end{aligned}
    \end{equation}
    est un produit tensoriel entre \( V\) et \( W\).
\end{proposition}

\begin{proof}
    Nous devons prouver les conditions de la définition \ref{DEFooXKKQooAvWRNp}. 
    
    \begin{subproof}
        \item[\( h\) est bilinéaire]

            Ce sont des calculs tels que faits dans la remarque \ref{REMooSLEGooWEiutz}\ref{ITEMooPVWHooMkgQoT} qui font le travail.

        \item[\(h \) est surjective]
    
            Un élément de \( V\otimes W\) est la classe d'un élément de \( F(V\times W)\), c'est-à-dire de la forme
            \begin{equation}
                \big[ \sum_{i\alpha}\delta_{(v_i,w_{\alpha})} \big]=\sum_{i\alpha}a_{i\alpha}v_i\otimes w_{\alpha}.
            \end{equation}
            Cet élément est dans l'image de \( h\) comme le montre le calcul suivant\footnote{Faites bien la distinction entre \( \delta_{v,w}\), \( (v,w)\) et \( v\otimes w\). Sachez dans quel ensemble se trouvent chacun de ces trois objets.} :
            \begin{equation}
                h\big( \sum_{i\alpha}(v_i,w_{\alpha}) \big)=\sum_{i\alpha}a_{i\alpha}h(v_i,w_{\alpha})=\sum_{i\alpha}v_i\otimes w_{\alpha}.
            \end{equation}

        \item[Propriété universelle]

            Soient un espace vectoriel \( U\) et une application linéaire \( f\colon V\oplus W\to U \). Nous devons trouver une application linéaire \( g\colon V\otimes W\to U\) telle que \( f=g\circ h\). Pour cela nous commençons par considérer l'application
            \begin{equation}
                \begin{aligned}
                    g\colon F(V\times W)&\to U \\
                    \delta_{(v,w)}&\mapsto f(v,w) 
                \end{aligned}
            \end{equation}
            définie sur tout \( F(V\times W)\) par linéarité sans encombres parce que les \( \delta_{v,w}\) forment une base par le lemme \ref{LEMooLOPAooUNQVku}.

            Nous démontrons que \( g(N)=0\) pour avoir le droit de passer \( g\) aux classes et le considérer comme application partant de \( V\otimes W\) au lieu de \( F(V\times W)\). Prenons par exemple
            \begin{subequations}
                \begin{align}
                    g\big( \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)} \big)&=g( \delta_{(v_1,w)} )+g(\delta_{(v_2,w)})-g(\delta_{v_1+v_2,w})\\
                    &=f(v_1,w)+f(v_2,w)-f(v_1+v_2,w)\\
                    &=0
                \end{align}
            \end{subequations}
            par la bilinéarité de \( f\). Cela montre que \( g(A_1)=0\). Nous montrons de même que \( g(A_2)=g(A_3)=g(A_4)=0\), et enfin toujours par linéarité que \( g(N)=0\). Pour rappel, les éléments de \( N\) sont les combinaisons linéaires finies d'éléments de \( A_1\), \( A_2\), \( A_3\) et \( A_4\).

            Par passage aux classes, nous avons une application (que nous notons également \( g\))
            \begin{equation}
                g\colon F(V\times W)/N\to U
            \end{equation}
            vérifiant \( g(v\otimes w)=f(v,w)\). Mais comme \( h(v,w)=v\otimes w\), nous avons $g\circ h\colon V\oplus W\to U$ vérifiant \( g\circ h=f\).
    \end{subproof}
    L'espace vectoriel \( V\otimes W\) est donc un produit tensoriel.
\end{proof}

\begin{normaltext}
    Vu que \( V\otimes W\) est un produit tensoriel de \( V\) et \( W\), et vu qu'il y a unicité par la proposition \ref{PROPooROPHooQXqNzZ}, nous avons bien le droit de dire que \( V\otimes W\) est \emph{le} produit tensoriel. Cela justifie le titre.
\end{normaltext}

\begin{normaltext}
    Les prochains lemmes et propositions vont nous dire que l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V^*\otimes W&\to \aL(V,W) \\
            \alpha\otimes w&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels lorsque \( V\) est de dimension finie. Vu que nous aimons les énoncés très explicites, ça va être découpé en plusieurs morceaux, l'énoncé va devenir un peu long; mais c'est pour la bonne cause.
\end{normaltext}

\begin{lemma}       \label{LEMooOJEBooQruWEp}
    Soient deux espaces vectoriels \( V\) et \( W\) dont \( W\) est de dimension finie. Alors l'application définie par
    \begin{equation}
        \begin{aligned}
            \varphi\colon F(V^*\times W)&\to \aL(V,W) \\
            \delta_{(\alpha,w)}&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    sur la base «canonique» de \( F(V^*\times W)\) passe aux classes.
\end{lemma}

\begin{proof}
    Avec les notations de la définition \ref{DEFooKTVDooSPzAhH} nous devons prouver que \( \varphi(N)=0\). Nous montrons que \( \varphi(A_4)=0\), et nous vous laissons faire les autres. Pour \( \lambda\in \eK\), \( \alpha\in V^*\) et \( w\in W\) en utilisant la linéarité de \( \varphi\) nous avons :
    \begin{subequations}
        \begin{align}
            \varphi\big( \lambda\delta_{(\alpha,w)}-\delta_{(\alpha,\lambda w)} \big)v&=\lambda\varphi(\delta_{(\alpha,w)})(v)-\varphi(\delta_{(\alpha,\lambda w)})(v)\\
            &=\lambda\alpha(v)w-\alpha(v)(\lambda w)\\
            &=0
        \end{align}
    \end{subequations}
    parce que \( \alpha(v)(\lambda w)=\lambda \alpha(v)w\) du fait que \( \eK\) est commutatif. La commutativité de \( \eK\) est ce qui permet de permuter le produit \( \lambda \alpha(v)\).

    Nous laissons à la lectrice le soin de prouver que \( \varphi(A_1)=\varphi(A_2)=\varphi(A_3)=0\).
\end{proof}

\begin{lemma}       \label{LEMooUQZHooWjIGsy}
    Si \( W\) est de dimension finie, alors \( \aL(V,W)\) muni de 
    \begin{equation}
        \begin{aligned}
            h'\colon V^*\oplus W&\to \aL(V,W) \\
            (\alpha,w)&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est un produit tensoriel\footnote{Définition \ref{DEFooXKKQooAvWRNp}.} de \( V^*\) par \( W\).
\end{lemma}

\begin{proof}
    Nous devons prouver que
    \begin{itemize}
        \item \( h\) est bilinéaire,
        \item \( h\) est surjective
        \item pour tout espace vectoriel \( U\), et pour toute application bilinéaire \( f\colon V^*\oplus W\to U\), il existe une application linéaire \( g\colon \aL(V,W)\to U\) tel que \( f=g\circ h\).
    \end{itemize}

    \begin{subproof}
        \item[Bilinéaire]
            Le fait que \( h\) soit bilinéaire est une simple vérification.
        \item[Surjective]
            L'espace \( W\) étant de dimension finie, nous pouvons en considérer une base \( \{ z_i \}_{i\in I}\). Soit \( \alpha\in \aL(V,W)\). Si \( v\in V\), l'élément \( \alpha(v)\) peut être décomposé dans la base \( \{ z_i \}\), ce qui définit des applications linéaires \( \alpha_i\colon V\to \eK\) par
            \begin{equation}
                \alpha(v)=\sum_{i\in I}\alpha_i(v)z_i.
            \end{equation}
            Notons que \( \alpha_i\in V^*\). En comparant avec la définition de \( h'\), nous voyons que
            \begin{equation}
                \alpha(v)=\sum_i h(\alpha_i,z_i)(v),
            \end{equation}
            c'est-à-dire \( \alpha=\sum_ih(\alpha_i,w_i)=h\big( \sum_i(\alpha_i,z_i) \big)\). Nous avons donc bien \( \alpha\in h(V^*\oplus W)\).
        \item[Propriété universelle]

            Soient un espace vectoriel \( U\) et une application bilinéaire \( f\colon V^*\oplus W\to U\). Pour \( \alpha\in\aL(V,W)\) nous définissons \( g(\alpha)\) comme suit. D'abord nous écrivons \( \alpha\) sous la forme
            \begin{equation}
                \alpha(v)=\sum_i\alpha_i(v)z_i,
            \end{equation}
            et nous posons 
            \begin{equation}
                g(\alpha)=\sum_if(\alpha_i,z_i).
            \end{equation}
            Avec cette définition, en posant \( w=\sum_iw_iz_i\), nous avons
            \begin{subequations}
                \begin{align}
                    (g\circ h')(\alpha,w)&=g\big( v\mapsto \alpha(v)w \big)\\
                    &=g\big( v\mapsto \sum_i\alpha(v)w_iz_i \big)\\
                    &=\sum_if(w_i\alpha,z_i)\\
                    &=\sum_if(\alpha,w_iz_i)\\
                    &=f(\alpha,\sum_iw_iz_i)\\
                    &=f(\alpha,w).
                \end{align}
            \end{subequations}
            Cela prouve que \( g\circ h=f\).
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooKJTCooVTXWAQ}
    Soient deux espaces vectoriels \( V\) et \( W\) dont \( V\) est de dimension finie. Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V^*\otimes W&\to \aL(V,W) \\
            \alpha\otimes w&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est bien définie\footnote{Au sens où il existe une fonction $\varphi$ définie sur tout $V^*\otimes W$ qui se réduit à cela pour les éléments de la forme $\alpha\otimes w$.} et est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Le lemme \ref{LEMooUQZHooWjIGsy} donne une structure de produit tensoriel de \( V^*\) par \( W\) sur \( \aL(V,W)\). Rappelons les structures :
    \begin{equation}
        \begin{aligned}
            h\colon V^*\oplus W&\to V^*\otimes W \\
            (\alpha,w)&\mapsto \alpha\otimes w 
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            h'\colon V^*\oplus W&\to \aL(V,W) \\
            (\alpha,w)&\mapsto \big[ v\mapsto \alpha(v)w \big].
        \end{aligned}
    \end{equation}

    La proposition \ref{PROPooROPHooQXqNzZ} a déjà fait tout le boulot. La seule chose à faire est de vérifier qu'il existe une application \( \varphi\colon V^*\otimes W\to \aL(V,W)\) vérifiant simultanément les deux conditions suivantes :
    \begin{enumerate}
        \item       \label{ITEMooVNNSooNIXRoG}
            \( \varphi(\alpha\otimes w)=\big[ v\mapsto \alpha(v)w \big]\)
        \item 
            \( h'=\varphi\varphi\circ h\).
    \end{enumerate}
    La seconde condition assure que \( \varphi\) sera un isomorphisme d'espaces vectoriels.

    L'existence de \( \varphi\) vérifiant la condition \ref{ITEMooVNNSooNIXRoG} est un effet du lemme \ref{LEMooOJEBooQruWEp} qui donne une fonction sur \( F(V^*\times W)\) dont le \( \varphi\) qui nous concerne est un quotient. Il reste à voir que cette application vérifie \( h'=\varphi\circ h\).
    
    En nous rappellant que \( \alpha\otimes w=[\delta_{(\alpha,w)}]\) et en écrivant \( \varphi\) à la fois l'application et son passage au quotient,
    \begin{equation}
        (\varphi\circ h)(\alpha,w)=\varphi(\alpha\otimes w)=\varphi\big( [\delta_{(\alpha,w)}] \big)=\varphi(\delta_{(\alpha,w)}).
    \end{equation}
    En appliquant à \( v\in V\) nous avons:
    \begin{equation}
        (\varphi\circ h)(\alpha,w)v=\varphi(\delta_{(\alpha,w)})v=\alpha(v)w=h'(\alpha,w)v.
    \end{equation}
    Et voila. Nous avons \( \varphi\circ h=h'\).
\end{proof}

Une conséquence de la proposition \ref{PROPooKJTCooVTXWAQ} est que
\begin{equation}
    \dim(V\otimes W)=\dim(V)\dim(W)
\end{equation}
via le lemme \ref{LEMooJXFIooKDzRWR}\ref{ITEMooPMLWooNbTyJI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Bases}
%---------------------------------------------------------------------------------------------------------------------------

Voici un lemme entièrement dédié au principe «dans le Frido, on ne fait pas d'abus de notations, sauf pour la logique formelle et la théorie des ensembles, que nous admettons».
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooXFIMooDkTSrq}
    Si \( \tau\colon V_1\to V_2\) est un isomorphisme d'espaces vectoriels, alors 
    \begin{equation}        \label{EQooEYUGooYYRZxD}
        \begin{aligned}
            \varphi\colon V_1\otimes W&\to V_2\otimes W \\
            v\otimes w&\mapsto \tau(v)\otimes W 
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels.
\end{lemma}

\begin{proof}
    Comme d'habitude, l'expression \eqref{EQooEYUGooYYRZxD} ne définit pas réellement \( \varphi\) parce que nous ne savons pas du tout si \( \{v\otimes w\tq v\in V,w\in W\}\) est plus ou moins une base de \( V\otimes W\)\footnote{Ne lisez pas la proposition \ref{PROPooTHDPooWgjUwk} qui dévoile toute l'intrigue.}. Ce que dit réellement ce lemme est qu'il existe une application \( V_1\otimes W\to V_2\otimes W\) qui est isomorphisme et qui se réduit à l'expression donnée dans le cas d'éléments de \( V_1\otimes W\) de la forme \( v\otimes w\).

    L'application
    \begin{equation}
        \begin{aligned}
            \varphi_0\colon F(V_1\times W)&\to F(V_2\times W) \\
            \delta{(v,w)}&\mapsto \delta_{\big( \tau(v),w \big)}
        \end{aligned}
    \end{equation}
    est un isomorphisme.

    Cette application passe aux classes, mais pas au sens où \( x\in [y]\) impliquerait \( \varphi_0(x)=\varphi_0(y)\); au sens où si \( x\in [y]\), alors \( \varphi_0(x)\in[\varphi_0(y)]\). Par exemple
    \begin{equation}
        \varphi_0\big( \lambda\delta_{(v,w)}-\delta_{(v,\lambda w)} \big)=\lambda\delta_{\big( \tau(v),w \big)}-\delta_{\big( \tau(v),w \big)}\in [0].
    \end{equation}
    Nous vous laissons le soin de vérifier les égalités correspondantes pour les autres parties de \( N\).

    Le passage au classes de \( \varphi_0\) signifie que l'on considère l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V_1\otimes W&\to V_2\otimes W \\
            [x]&\mapsto [\varphi_0(x)] 
        \end{aligned}
    \end{equation}
    où vous aurez noté que la prise de classe à gauche n'est pas la même que celle à droite.

    Il faut prouver que ce \( \varphi\) est un isomorphisme. En ce qui concerne la linéarité,
    \begin{subequations}
        \begin{align}
            \varphi\big( [x]+[y] \big)&=\varphi\big( [x+y] \big)\\
            &=[\varphi_0(x+y)]\\
            &=[\varphi_0(x)+\varphi_0(y)]\\
            &=[\varphi_0(x)]+[\varphi_0(y)]\\
            &=\varphi([x])+\varphi([y]).
        \end{align}
    \end{subequations}
    Je vous laisse le reste de la linéarité. Et en ce qui concerne le fait que ce soit une bijection, allez-y.
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooTHDPooWgjUwk}
    Soient des espaces vectoriels de dimension finie \( V\) et \( W\). Soient une base \( \{e_i\}\) de \( V\) et une base \( \{f_{\alpha}\}\) de \( W\).
    
    Alors :
    \begin{enumerate}
        \item       \label{ITEMooQCILooUncdGl}
            La partie \( \{e_i\otimes f_{\alpha}\}\) est une base de \( V\otimes W\).
        \item
    Au niveau des dimensions, \( \dim(V\otimes W)=\dim(V)\dim(W)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Vu que \( V\) est de dimension finie, nous avons un isomorphisme d'espaces vectoriels \( V^*=V\), et même un isomorphisme d'espaces vectoriels
    \begin{equation}
        \begin{aligned}
            \tau\colon V&\to (V^*)^* \\
            \tau(v)\alpha&=\alpha(v).
        \end{aligned}
    \end{equation}
    Recopions l'isomorphisme de la proposition \ref{PROPooKJTCooVTXWAQ} en utilisant \( V^*\) au lieu de \( V\) :
    \begin{equation}
        \begin{aligned}
            \psi_0\colon (V^*)^*\otimes W&\to \aL(V^*,W) \\
           \tau(v)\otimes w &\mapsto \big( \alpha\mapsto \tau(v)(\alpha)w =\alpha(v)w \big).
        \end{aligned}
    \end{equation}
    En écrivant cela, nous avons tenu compte du fait que tout élément de \( (V^*)^*\) peut être écrit de façon univoque sous la forme \( \tau(v)\) pour un certain \( v\in V\).

    Vu que \( \tau\) est un isomorphisme, l'application suivante est encore un isomorphisme\footnote{Lemme \ref{LEMooXFIMooDkTSrq}.} :
    \begin{equation}        \label{EQooAEFRooPfmAnj}
        \begin{aligned}
            \psi\colon V\otimes W&\to \aL(V^*,W) \\
            v\otimes w&\mapsto \big( \alpha\mapsto \alpha(v)w \big). 
        \end{aligned}
    \end{equation}
    Nous avançons. Vu que nous avons un isomorphisme, nous pouvons faire passer des bases. Le lemme \ref{LEMooJXFIooKDzRWR} nous donne une base de \( \aL(V^*,W)\) en les éléments \( \beta_{i\alpha}\colon V^*\to W\) définies par
    \begin{equation}
        \beta_{ij}(\alpha)=\alpha(e_i)f_{\alpha}.
    \end{equation}
    Donc \( \{ \psi^{-1}(\beta_{i\alpha}) \}\) est une base de \( V\otimes W\).

    Pour \( a=\sum_ia_ie_i^*\) (base duale, définition \ref{DEFooTMSEooZFtsqa}) nous avons :
    \begin{equation}
        \psi(e_i\otimes f_{\alpha})a=a(e_i)f_{\alpha}=\beta_{i\alpha}(a).
    \end{equation}
    Cela prouve que \( \psi^{-1}(\beta_{i\alpha})=e_i\otimes f_{\alpha}\), et donc que ces \( e_i\otimes f_{\alpha}\) est une base de \( V\otimes W\).

    La formule concernant les dimensions est simplement la définition \ref{DEFooWRLKooArTpgh} de la dimension : le nombre d'éléments dans une base.
\end{proof}

\begin{example}
    Dans le produit tensoriel \( \eR\otimes \eR\), nous avons \( x\otimes 1=1\otimes x=x(1\otimes x)\) pour tout \( x\in \eR\). Et si \( x\geq 0\) nous avons aussi \( x\otimes 1=\sqrt{ x }\otimes \sqrt{ x }\).
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons des espaces vectoriels \( V\) et \( W\) de dimension finie. L'application \eqref{EQooAEFRooPfmAnj} donne un isomorphisme d'espaces vectoriels 
\begin{equation} 
    \begin{aligned}
        \psi\colon V\otimes W&\to \aL(V^*,W) \\
        v\otimes w&\mapsto \big( \alpha\mapsto \alpha(v)w \big). 
    \end{aligned}
\end{equation}
Et ça, c'est très bien, parce que nous connaissons une norme sur \( \aL(V^*,W)\) :  la norme opérateur \ref{DefNFYUooBZCPTr}.

\begin{definition}[\cite{MonCerveau}]      \label{DEFooEXXNooMgIpSV}
    Soient deux espaces vectoriels normés de dimension finie \( V\) et \( W\). Sur \( V\otimes W\) nous définissons, pour \( t\in V\otimes W\)
    \begin{equation}
        \| t \|=\| \psi(t) \|_{\aL(V^*,W)}.
    \end{equation}
\end{definition}
   
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooQPXHooJWfpmk}
    La norme sur \( V\otimes W\) vérifie
    \begin{equation}
        \| v\otimes w \|=\| v \|\| w \|
    \end{equation}
    pour tout \( v\in V\) et \( w\in W\).
\end{lemma}

\begin{proof}
    C'est un simple(?) calcul :
    \begin{equation}
        \| v\otimes w \|=\| \psi(v\otimes w) \|=\| \alpha\mapsto \alpha(v)w \|=\sup_{\| \alpha \|=1}\| \alpha(v)w \|=\sup_{\| \alpha \|=1}| \alpha(v) |\| w \|.
    \end{equation}
    Étant donné que \( V\) est de dimension finie, \( \sup_{\| \alpha \|=1}| \alpha(v) |=\| v \|\)\quext{Cela est une des raisons pour lesquelles nous sommes en dimension finie : je ne sais pas si cette égalité est vraie en dimension inifinie.}. Nous avons donc
    \begin{equation}
        \| v\otimes w \|=\| v \|\| w \|.
    \end{equation}
\end{proof}

Le lemme suivant montre que \( \eR\otimes \eR\) n'est pas du tout \( \eR\times \eR=\eR^2\). Au contraire, \( \eR\otimes \eR\) est isomorphe à \( \eR\).
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooVONEooQpPgcn}
    L'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon \eR\otimes \eR&\to \eR \\
            1\otimes 1&\mapsto 1 
        \end{aligned}
    \end{equation}
    prolongée par linéarité est un isomorphisme isométrique.
\end{lemma}

\begin{proof}
    D'abord une base de \( \eR\) est \( \{ 1 \}\); donc une base de \( \eR\otimes \eR\) est \( \{ 1\otimes 1 \}\) par la proposition \ref{PROPooTHDPooWgjUwk}. Donc l'application proposée se prolonge par linéarité à tout \( \eR\otimes \eR\).

    Le fait que \( \varphi\) soit une bijection provient du fait que \( \varphi\) transforme une base en une base; si vous n'y croyez pas, la vérification de l'injectivité et de la surjectivité est facile.

    Pour que \( \varphi\) soit isométrique, nous faisons le calcul
    \begin{equation}
        \| \varphi(x\otimes y) \|=\| xy(1\otimes 1) \|=| xy |\| 1\otimes 1 \|=| xy |=\| x\otimes y \|.
    \end{equation}
    Nous avons utilisé la propriété \ref{DefNorme}\ref{ItemDefNormeii} d'une norme ainsi que le lemme \ref{LEMooQPXHooJWfpmk} pour la norme sur \( \eR\otimes \eR\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Applications bilinéaires, matrices et produit tensoriel}
%---------------------------------------------------------------------------------------------------------------------------
\label{SECooUKRYooZjagcX}

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}        \label{EQooUNRYooKBrXyK}
    (\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
    (a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
    (a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est une vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Application d'opérateurs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemMyKPzY}
    Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
    \begin{equation}        \label{EqXdxvSu}
        (Ax\otimes By)=A(x\otimes y)B^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
    \begin{subequations}
        \begin{align}
            (Ax\otimes By)_{ij}&=(Ax)_i(By)_j\\
            &=\sum_{kl}A_{ik}x_kB_{jl}y_l\\
            &=A_{ik}(x\otimes y)_{kl}B_{jl}\\
            &=\big( A(x\otimes y)B^t \big)_{ij}.
        \end{align}
    \end{subequations}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Calcul différentiel dans un espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecLStKEmc}

Quelque motivations pour la notion de différentielle sont données dans \ref{SEBSECooLPRQooJRQCFL}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition de la différentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{MonCerveau}]      \label{DefDifferentiellePta}
    Soient deux espaces vectoriels normés \( E\) et \( F\) ainsi qu'une fonction \( f\colon \mU\to F\) où \( \mU\) est un ouvert de \( E\).

  Si il existe une une application linéaire \( T\in\aL(E,F)\) satisfaisant
  \begin{equation}	\label{EqCritereDefDiff}
      \lim_{\substack{h\to 0\\h\in E}}\frac{f(a+h)-f(a)-T(h)}{\|h\|_E}=0,
  \end{equation}
  alors il en existe une seule.

  Dans ce cas nous disons que $f$ est \defe{différentiable au point $a$}{application!différentiable} et l'application $T$ ainsi définie est appelée \defe{différentielle}{différentielle} de $f$ au point $a$, et nous la notons $df_a$.
\end{propositionDef}

\begin{proof}
    Soient deux applications linéaires \( T_1\), \( T_2\) satisfaisant la condition \eqref{EqCritereDefDiff}. Nous avons
    \begin{equation}
        \frac{ \| T_1(h)-T_2(h) \|_F }{ \| h \|_E }\leq \frac{ \| T_1(h)-f(a+h)+f(a) \| }{ \| h \| }+\frac{ \| f(a+h)-f(a)-T_2(h) \| }{ \| h \| }\to 0.
    \end{equation}
    Nous avons donc
    \begin{equation}
        \lim_{h\to 0} \frac{ \| (T_1-T_2)(h) \|_F }{ \| h \|_E }=0.
    \end{equation}
    Soit \( \epsilon>0\). Ce que signifie la limite est qu'il existe un \( r>0\) tel que pour tout \( u\in B_E(0,r)\), nous ayons
    \begin{equation}
        \frac{ \| (T_1-T_2)(u) \|_F }{ \| u \|_E }<\epsilon.
    \end{equation}
    Soit \( v\in E\). Nous considérons \( \lambda\in\eR\) tel que \( \lambda v\in B(0,r)\), par exemple \( \lambda<r/\| v \|\). Nous avons
    \begin{equation}
        \epsilon>\frac{ \| (T_1-T_2)(\lambda v) \|_F }{ \| \lambda v \|_E }=\frac{ \| (T_1-T_2)(v) \| }{ \| v \| }.
    \end{equation}
    Cela donne
    \begin{equation}
        \| (T_1-T_2)(v) \|<\| v \|\epsilon.
    \end{equation}
    Nous avons donc \( \| (T_1-T_2)(v) \|=0\), soit \( T_1(v)=T_2(v)\).
\end{proof}


L'application différentielle
\begin{equation}
    \begin{aligned}
        df\colon E&\to \aL(E,F) \\
        a&\mapsto df_a
    \end{aligned}
\end{equation}
est également très importante.

\begin{definition}      \label{DefJYBZooPTsfZx}
Une application \( f\colon E\to F\) est de \defe{classe \( C^1\)}{classe $C^1$} lorsque l'application différentielle \( df\colon E\to \aL(E,F)\) est continue. Voir aussi les définitions~\ref{DefPNjMGqy} pour les applications de classe \( C^k\).
\end{definition}

\begin{remark}      \label{RemATQVooDnZBbs}
    L'application norme étant continue, le critère du théorème~\ref{ThoWeirstrassRn} est en réalité assez général. Par exemple à partir d'une application différentiable\footnote{Définition~\ref{DefDifferentiellePta}.} \( f\colon X\to Y\)  nous pouvons considérer la fonction réelle
    \begin{equation}
        a\mapsto \|  df_a   \|
    \end{equation}
    où la norme est la norme opérateur\footnote{Définition~\ref{DefNFYUooBZCPTr}.}. Si \( f\) est de classe \( C^1\) alors cette application est continue et donc bornée sur un compact \( K\) de \( X\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Accroissements finis}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooYQZZooVybqjK}
    Soit une fonction \( f\colon E\to V\) (espaces vectoriels normés) différentiable en \( a\in E\). Alors il existe une fonction \( \alpha\colon E\to V\) telle que
    \begin{subequations}
        \begin{numcases}{}
            \lim_{h\to 0} \frac{ \alpha(h) }{ \| h \| }=0\\
            f(a+h)=f(a)+df_a(h)+\alpha(h).
        \end{numcases}
    \end{subequations}
\end{lemma}

\begin{proof}
    Il s'agit seulement de poser
    \begin{equation}
        \alpha(h)=f(a+h)-f(a)-df_a(h).
    \end{equation}
    Le fait que \( \alpha(h)/\| h \|\to 0\) est alors la définition de la différentiabilité de \( f\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{(non ?) Différentiabilité des applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) et \( F\) sont deux espaces vectoriels nous notons \( \aL(E,F)\)\nomenclature[Y]{\( \aL(E,F)\)}{Les applications linéaires de \( E\) vers \( F\)} l'ensemble des applications linéaires de \( E\) vers \( F\) et \( \cL(E,F)\)\nomenclature[Y]{\( \cL\)}{Les applications linéaires continues de \( E\) vers \( F\)} l'ensemble des applications linéaires continues de \( E\) vers \( F\). Ces espaces seront bien entendu, sauf mention du contraire, toujours munis de la norme opérateur de la définition~\ref{DefNFYUooBZCPTr}.

\begin{lemma}       \label{LemooXXUGooUqCjmp}
    Soit une application linéaire \( f\).
    \begin{enumerate}
        \item
            Si \( f\) est continue, alors elle est différentiable et \( df_a(u)=f(u)\) pour tout \( a\) et \( u\).
        \item
            Si \( f\) n'est pas continue, alors elle n'est pas différentiable.
    \end{enumerate}
\end{lemma}

\begin{proof}
    La linéarité de \( f\) donne :
    \begin{equation}
        f(a+h)-f(a)-f(h)=0,
    \end{equation}
    et donc prendre \( T=f\) dans la définition~\ref{DefDifferentiellePta} fait fonctionner la limite. De plus \( T\) est alors continue par hypothèse; elle est donc bien la différentielle de \( f\).

    Supposons que \( f\) ne soit pas continue, prenons une application linéaire continue \( T\), et calculons
    \begin{equation}        \label{EQooFLYMooEKTeOC}
        \frac{ f(a+h)-f(a)-T(h) }{ \| h \| }=\frac{ (f-T)(h) }{ \| h \| }=(f-T)(e_h)
    \end{equation}
    où \( e_h\) est le vecteur unitaire dans la direction de \( h\). Vu que \( f\) n'est pas continue et que \( T\) l'est, l'application \( f-T\) n'est pas continue. Elle n'est pas pas bornée par la proposition~\ref{PROPooQZYVooYJVlBd}. Il existe alors un vecteur \( h\) tel que \( \| (f-T)(e_h) \|>1\) (et même plus grand que ce qu'on veut).

    Donc la limite de \eqref{EQooFLYMooEKTeOC} pour \( h\to 0\) ne peut pas être nulle.
\end{proof}

\begin{lemma}   \label{LemLLvgPQW}
    Une application linéaire continue est de classe \(  C^{\infty}\).
\end{lemma}

\begin{proof}
    Soit \( a\in E\). Étant donné que \( f\) est linéaire et continue, elle est différentiable et
    \begin{equation}
        \begin{aligned}
            df\colon E&\to \cL(E,F) \\
            a&\mapsto f
        \end{aligned}
    \end{equation}
    est une fonction constante et en particulier continue; nous avons donc \( f\in C^1\). Pour la différentielle seconde nous avons \( d(df)_a=0\) parce que \( df(a+h)-df(a)=f-f=0\). Toutes les différentielles suivantes sont nulles.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dérivation en chaine et formule de Leibnitz}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropOYtgIua}
    Soient \( f_i\colon U\to F_i\), des fonctions de classe \( C^r\) où \( U\) est ouvert dans l'espace vectoriel normé \( E\) et les \( F_i\) sont des espaces vectoriels normés. Alors l'application
    \begin{equation}
        \begin{aligned}
        f=f_1\times \cdots\times f_n\colon U&\to F_1\times \cdots\times F_n \\
    x&\mapsto \big( f_1(x),\ldots, f_n(x) \big)
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}
    d^rf=d^rf_1\times\ldots d^rf_n.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( x\in U\) et \( h\in E\). La différentiabilité des fonctions \( f_i\) donne
    \begin{equation}
        f_i(x+h)=f_i(x)+(df_i)_x(h)+\alpha_i(h)
    \end{equation}
    avec \( \lim_{h\to 0} \alpha_i(h)/\| h \|=0\). Par conséquent
    \begin{subequations}
        \begin{align}
            f(x+h)&=\big( \ldots, f_i(x)+(df_i)_x(h)+\alpha_i(h),\ldots \big)\\
            &= \big( \ldots,f_i(x),\ldots \big)+ \big( \ldots,(df_i)_x(h),\ldots \big)+ \big( \ldots,\alpha_i(h),\ldots \big).
        \end{align}
    \end{subequations}
    Mais la définition~\ref{DefFAJgTCE} de la norme dans un espace produit donne
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \big( \alpha_1(h),\ldots, \alpha_n(h) \big) \| }{ \| h \| }=0,
    \end{equation}
    ce qui nous permet de noter \( \alpha(h)=\big( \alpha_1(h),\ldots, \alpha_n(h) \big)\) et avoir \( \lim_{h\to 0} \alpha(h)/\| h \|=0\). Avec tout ça nous avons bien
    \begin{equation}
        f(x+h)=f(x)+\big( (df_1)_x(h)+\cdots +(df_n)_x(h) \big)+\alpha(h),
    \end{equation}
    ce qui signifie que \( f\) est différentiable et
    \begin{equation}
        df_x=\big( df_1,\ldots, df_n \big).
    \end{equation}
\end{proof}

\begin{theorem}     \label{THOooIHPIooIUyPaf}
    Soient des espaces vectoriels normés \( E,V\) et \( W\). Nous considérons deux fonctions \( f\colon E\to V\) et \( g\colon V\to W\). Nous supposons que \( f\) est différentiable en \( a\in E\) et que \( g\) est différentiable en \( f(a)\in V\). 

    Nous supposons de plus que \( df_a\) est de norme finie\quext{Je ne suis pas totalement certain que cette hypothèse soit nécessaire, mais en tout cas, elle est utilisée.}.
    
    
    Alors \( g\circ f\colon E\to W\) est différentiable en \( a\) et
    \begin{equation}
        f(g\circ f)_a(u)=df_{f(a)}\big( df_a(u) \big),
    \end{equation}
    ou encore
    \begin{equation}
        f(g\circ f)_a=dg_{f(a)}\circ df_a.
    \end{equation}
\end{theorem}

\begin{proof}
    En utilisant le lemme \ref{LEMooYQZZooVybqjK} pour les fonctions \( f\) et \( g\), nous avons
    \begin{equation}        \label{EQooXNWZooJSPjRS}
        f(a+h)=f(a)+df_a(h)+\alpha(h)
    \end{equation}
    et
    \begin{equation}        \label{EQooIQZZooWPyMbE}
        g\big( f(a)+k \big)=g\big( f(a) \big)+dg_{f(a)}(k)+\beta(k).
    \end{equation}
    L'application \( dg_{f(a)}\circ df_a\) est une application linéaire, et est notre candidat différentielle. En suivant la définition \ref{DefDifferentiellePta}, nous allons calculer
    \begin{equation}
        \lim_{h\to 0} \frac{ (g\circ f)(a+h)-(g\circ f)(a)-(dg_{f(a)}\circ df_a)(h) }{ \| h \| }.
    \end{equation}
    Si cette limite existe et vaut zéro, alors nous aurons prouvé que le candidat différentielle est correct.

    Pour cela, nous emboîtons les formules \eqref{EQooXNWZooJSPjRS} et \eqref{EQooIQZZooWPyMbE} l'une dans l'autre pour avoir : 
    \begin{equation}
        g(a+h)=g\big( f(a)+df_a(h)+\alpha(h) \big)=g\big( f(a) \big)+dg_{f(a)}\big( df_a(h)+\alpha(h) \big)+\beta\big( df_a(h)+\alpha(h) \big).
    \end{equation}
    Vu que \( dg_{f(a)}\) est linéaire, le deuxième terme peut être coupé en deux et après recombinaisons,
    \begin{equation}
        (g\circ f)(a+h)-(g\circ f)(a)-(df_{f(a)}\circ df_a)(h)=dg_{f(a)}\big( \alpha(h) \big)+\beta\big( df_a(h)+\alpha(h) \big).
    \end{equation}
    Étant donné que \( dg_{f(a)}\) est linéaire,
    \begin{equation}
        \frac{ dg_{f(a)}\big(\alpha(h)\big) }{ \| h \| }=dg_{f(a)}\left( \frac{ \alpha(h) }{ \| h \| } \right)\to 0.
    \end{equation}
    Il nous reste à voir que
    \begin{equation}        \label{EQooUQNUooFgNyJp}
        \lim_{h\to 0} \frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| h \| }
    \end{equation}
    existe au vaut zéro. Vu que \( df_a\) est linéaire, il existe \( M>0\) tel que\footnote{Ce \( M\) est par exemple la norme opérateur de \( df_a\), comme nous l'assure le lemme \ref{LEMooIBLEooLJczmu}. C'est pour ce passage-ci que nous avons supposé que \( df_a\) était de norme finie.} \( \| df_a(h) \|\leq M\| h \|\). D'autre part, vu que \( \alpha(h)/\| h \|\to 0\), nous avons \( \| \alpha(h) \|\leq \| h \|\) pour tout \( h\) suffisamment petit.

    Donc si \( h\) est assez petit, nous avons
    \begin{equation}        \label{EQooEQJBooSmacrD}
        \| df_a(h)+\alpha(h) \|\leq (M+1)\| h \|.
    \end{equation}
    Soit \( \epsilon>0\). Soit \( \delta>0\) tel que \( \| h \|\leq \delta\) implique \( \beta(h)/\| h \|\leq \epsilon\) et \eqref{EQooEQJBooSmacrD} en même temps. Soit \( r\) tel que \( (M+1)r<\delta\); et notons que \( r<\delta\). Nous considérons alors \( h\in B(0,r)\) et nous calculons :
    \begin{equation}
        \frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| h \| }=\frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| df_a(h)+\alpha(h) \| }\frac{ \| df_a(h)+\alpha(h) \| }{ \| h \| }\leq (M+1)\epsilon.
    \end{equation}
    La limite \eqref{EQooUQNUooFgNyJp} existe donc et vaut zéro.
\end{proof}

\begin{theorem}[Différentielle de fonctions composées\cite{SNPdukn}]    \label{ThoAGXGuEt}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et \( V\) ouvert dans \( F\). Soient des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f\colon U\to V\\
            g\colon V\to G.
        \end{align}
    \end{subequations}
    Alors l'application \( g\circ f\colon V\to G\) est de classe \( C^r\) et
    \begin{equation}\label{EqHFmezmr}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
\end{theorem}

\begin{proof}
    Nous nous fixons \( x\in U\). La fonction \( f\) est différentiable en \( x\in U\) et \( g\) en \( f(x)\), donc nous pouvons écrire
    \begin{equation}
        f(x+h)=f(x)+df_x(h)+\alpha(h)
    \end{equation}
    et
    \begin{equation}
        g\big( f(x)+u \big)=g\big( f(x) \big)+dg_{f(x)}(u)+\beta(u)
    \end{equation}
    où la fonction \( \alpha\) a la propriété que
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \alpha(h) \| }{ \| h \| }=0;
    \end{equation}
    et la même chose pour \( \beta\). La fonction composée en \( x+h\) s'écrit donc
    \begin{equation}    \label{EqCXcfhfH}
        (g\circ f)(x+h)=g\big( f(x)+df_x(h)+\alpha(h) \big)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h)+\alpha(h) \big)+\beta\big( df_x(h)+\alpha(h) \big).
    \end{equation}
    Nous montrons que tous les «petits» termes de cette formule peuvent être groupés. D'abord si \( h\) est proche de \( 0\), nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq\frac{ \| df_x \|\| h \| }{ \| h \| }+\frac{ \| \alpha(h) \| }{ \| h \| }.
    \end{equation}
    Si \( h\) est petit, le second terme est arbitrairement petit, donc en prenant n'importe que \( M>\| df_x \|\) nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq M.
    \end{equation}
    Par ailleurs, nous avons
    \begin{equation}
        \frac{ \| \beta\big( df_x(h)+\alpha(h) \big) \| }{ \| h \| }=\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{ \| df_x(h)+\alpha(h) \| }\frac{  \| df_x(h)+\alpha(h) \|  }{ \| h \| }\leq M\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{   \| df_x(h)+\alpha(h) \| }.
    \end{equation}
    Vu que la fraction est du type \( \frac{ \beta( f(h)) }{ f(h) }\) avec \( \lim_{h\to 0} f(h)=0\), la fraction tend vers zéro lorsque \( h\to 0\). En posant
    \begin{equation}
        \gamma_1(h)=\beta\big( df_x(h)+\alpha(h) \big)
    \end{equation}
    nous avons \( \lim_{h\to 0} \gamma_1(h)/\| h \|=0\).

    L'autre candidat à être un petit terme dans \eqref{EqCXcfhfH} est traité en utilisant le lemme~\ref{LEMooFITMooBBBWGI} :
    \begin{equation}
        \| dg_{f(x)}\big( \alpha(h) \big) \|\leq \| dg_{f(x)} \|\| \alpha(h) \|.
    \end{equation}
    Donc
    \begin{equation}
        \frac{ \| dg_{f(x)}\big( \alpha(h) \big) \| }{ \| h \| }\leq \| dg_{f(x)} \|\frac{ \| \alpha(h) \| }{ \| h \| },
    \end{equation}
    ce qui nous permet de poser
    \begin{equation}
        \gamma_2(h)=dg_{f(x)}\big( \alpha(h) \big)
    \end{equation}
    avec \( \gamma_2\) qui a la même propriété que \( \gamma_1\). Avec tout cela, en posant \( \gamma=\gamma_1+\gamma_2\) nous récrivons
    \begin{equation}
        (g\circ f)(x+h)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h) \big)+\gamma(h)
    \end{equation}
    avec \( \lim_{h\to 0} \frac{ \gamma(h) }{ \| h \| }=0\). Tout cela pour dire que
    \begin{equation}
        \lim_{h\to 0} \frac{ (g\circ f)(x+h)-(g\circ f)(x)-\big( dg_{f(x)}\circ df_x \big)(h) }{ \| h \| }=0,
    \end{equation}
    ce qui signifie que
    \begin{equation}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
    Nous avons donc montré que si \( f\) et \( g\) sont différentiables, alors \( g\circ f\) est différentiable avec différentielle donnée par \eqref{EqHFmezmr}.

    Nous passons à la régularité. Nous supposons maintenant que \( f\) et \( g\) sont de classe \( C^r\) et nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon L(F,G)\times L(E,F)&\to L(E,G) \\
            (A,B)&\mapsto A\circ B.
        \end{aligned}
    \end{equation}
    Montrons que l'application \( \varphi\) est continue en montrant qu'elle est bornée\footnote{Proposition~\ref{PROPooQZYVooYJVlBd}.}. Pour cela nous écrivons la norme opérateur
    \begin{equation}
        \| \varphi \|=\sup_{\| (A,B) \|=1}\| \varphi(A,B) \|=\sup_{\| (A,B) \|=1}\| A\circ B \|\leq\sup_{\| (A,B) \|=1}\| A \|\| B \|\leq 1.
    \end{equation}
    Justifications : d'une part la norme opérateur est une norme algébrique\footnote{Lemme \ref{LEMooFITMooBBBWGI}.}, et d'autre part la définition \ref{DefFAJgTCE} de la norme sur un espace produit pour la dernière majoration. L'application \( \varphi\) est donc continue et donc \(  C^{\infty}\) par le lemme~\ref{LemLLvgPQW}. Nous considérons également l'application
    \begin{equation}
        \begin{aligned}
        \psi\colon U&\to L(F,G)\times L(E,F) \\
        x&\mapsto \big( dg_{f(x)},df_x \big).
        \end{aligned}
    \end{equation}
    Vu que \( f\) et \( g\) sont \( C^1\), l'application \( \psi\) est continue. Ces deux applications \( \varphi\) et \( \psi\) sont choisies pour avoir
    \begin{equation}
        (\varphi\circ\psi)(x)=\varphi\big( dg_{f(x)},df_x \big)=dg_{f(x)}\circ df_x,
    \end{equation}
    c'est-à-dire \( \varphi\circ\psi=d(g\circ f)\). Les applications \( \varphi\) et \( \psi\) étant continues, l'application \( d(g\circ f)\) est continue, ce qui prouve que \( g\circ f\) est \( C^1\).

    Si \( f\) et \( g\) sont \( C^r\) alors \( dg\in C^{r-1}\) et \( dg\circ f\in C^{r-1}\) où il ne faut pas se tromper : \( dg\colon F\to L(F,G)\) et \( f\colon U\to F\); la composée est \( dg\circ f\colon x\mapsto dg_{f(x)}\in L(F,G)\).

    Pour la récurrence nous supposons que \( f,g\in C^{r-1}\) implique \( g\circ f\in C^{r-1}\) pour un certain \( r\geq 2\) (parce que nous venons de prouver cela avec \( r=1\) et \( r=2\)). Soient \( f,g\in C^r\) et montrons que \( g\circ f\in C^r\). Par la proposition~\ref{PropOYtgIua} nous avons
    \begin{equation}
        \psi=dg\circ f\times df\in C^{r-1},
    \end{equation}
    et donc \( d(g\circ f)=\varphi\circ\psi\in C^{r-1}\), ce qui signifie que \( g\circ f\in C^r\).
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRCZOooSgvpSE}
    Soit une application \( f\colon E\to V\) de classe \( C^1\). Soit une application linéaire \( \varphi\colon V \to W\). Alors \( \varphi\circ f\) est de classe \( C^p\).
\end{proposition}

\begin{proof}
    Toute la preuve est un grand jeu de cohérence des espaces en présence, alors soyez attentifs et capable de dire précisément à quel espace appartient chacun de objets entrant en jeu.

    Nous posons \( V_0=V\) et \( V_{k+1}=\aL(E,V_k)\). Idem pour les espaces \( W_k\). Ensuite nous posons
    \begin{equation}
        \begin{aligned}
            \varphi_1\colon \aL(E,V)&\to \aL(E,W) \\
            \alpha&\mapsto \varphi\circ \alpha.
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            \varphi_k\colon \aL(E,V_{k-1})&\to \aL(E,W_{k-1}) \\
            \alpha&\mapsto \varphi_{k-1}\circ \alpha.
        \end{aligned}
    \end{equation}
    Notez la cohérence : si \( a\in E\), \( \alpha(a)\in V_{k-1}=\aL(E,V_{k-2})\), et donc
    \begin{equation}
        (\varphi_{k-1}\circ\alpha)(a)=\varphi_{k-1}\big( \alpha(a) \big).
    \end{equation}
    À droite nous avons \( \varphi_{k-1}\big( \alpha(a) \big)\in \aL(E,W_{k-2})=V_{k-1}\).

    De plus, \( \varphi\) est linéaire; ça se prouve par récurrence en partant de \( \varphi_1\) et en se basant sur le fait que \( \varphi\) est linéaire.

    C'est parti pour une récurrence.

    \begin{subproof}
        \item[Énoncé]
            Nous allons prouver par récurrence que
            \begin{equation}
                d^k(\varphi\circ f)=\varphi_k\circ d^kf.
            \end{equation}
            pour tout \( k\leq p\).
        \item[Initialisation]
 
            D'abord, \( f\) est de classe \( C^p\), donc différentiable et \( \varphi\) est linéaire donc différentiable. Donc la composée est différentiable et le théorème \ref{THOooIHPIooIUyPaf} nous donne la différentiabilité de \( \varphi\circ f\) ainsi que la formule
            \begin{equation}
                d(\varphi\circ f)_a(u)=d\varphi_{f(a)}\big( df_a(u) \big)=(\varphi\circ df_a)(u)=\varphi_1(df_a)(u).
            \end{equation}
            Donc \( d(\varphi\circ f)_a=\varphi_1(df_a)\), ce qui signifie 
            \begin{equation}
                d(\varphi\circ f)=\varphi_1\circ df.
            \end{equation}
            C'est bon pour \( k=1\).


        \item[La pas de récurrence]

            Vu que \( f\) est de classe \( C^p\), \( d^kf\) est encore différentiable. Vu que \( \varphi_k\) est encore linéaire, nous pouvons encore utiliser la règle de différentiation de fonctions composées sur l'application \( \varphi_k\circ d^kf\). Nous avons :
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=d\big( d^k(\varphi\circ f) \big)_a(u)=d(\varphi_k\circ d^kf)_a(u).
            \end{equation}
            C'est le moment d'utiliser la formule de différentiation en chaîne :
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=\big( (d\varphi_k)_{d^kf_a}\circ d^{k+1}f_a \big)(u).
            \end{equation}
            Mais \( \varphi_k\) étant linéaire, \( (d\varphi_k)_{d^kf_a}=\varphi_k\), donc
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=(\varphi_k\circ d^{k+1}f_a)(u).
            \end{equation}
            Donc, en oubliant l'application au vecteur \( u\),
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a=\varphi_k\circ d^{k+1}f_a=\varphi_{k+1}\big( d^{k+1}f_a \big)=(\varphi_{k+1}\circ d^{k+1}f)(a).
            \end{equation}
            Nous avons donc bien
            \begin{equation}
                d^{k+1}(\varphi\circ f)=\varphi_{k+1}\circ d^{k+1}f.
            \end{equation}
    \end{subproof}
\end{proof}

\begin{lemma}       \label{LemooTJSZooWkuSzv}
    Si \( f\colon U\to V\) est un difféomorphisme\footnote{Définition~\ref{DefAQIQooYqZdya}} alors pour tout \( a\in U\), l'application \( df_a\) est inversible et
    \begin{equation}
        (df_a)^{-1}=(df^{-1})_{f(a)}.
    \end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'apercevoir qu'en vertu de la règle de différentiation en chaîne \eqref{EqHFmezmr},
    \begin{equation}
        (df_a)(df^{-1})_{f(a)}=d(f\circ f^{-1})_{f(a)}=\id.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooNONAooCyAtce}
    Soient des ouverts \( A\) de \( \eR^p\) et \( B\) de \( \eR^m\). Si il existe un difféomorphisme \( f\colon A\to B\), alors \( p=m\).
\end{proposition}

\begin{proof}
    Vu que \( f\) est un difféomorphisme, le lemme \ref{LemooTJSZooWkuSzv} fait son travail : l'application linéaire \( df_a\colon \eR^p\to \eR^m\) est inversible d'inverse \( df^{-1}_{f(a)}\colon \eR^m\to \eR^m\).

    Or une application linéaire ne peut pas être bijective entre espaces de dimensions différentes (finies). Donc \( p=m\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentiation de produit}
%---------------------------------------------------------------------------------------------------------------------------

Si nous avons deux application \( f\colon E\to V\) et \( g\colon E\to W\), alors nous voudrions considérer la fonction
\begin{equation}
    \begin{aligned}
        f\otimes g\colon E&\to V\otimes W \\
        a&\mapsto f(a)\otimes g(a). 
    \end{aligned}
\end{equation}
Le problème avec cette notation est que très souvent, les applications \( f\) et \( g\) sont des éléments d'espaces vectoriels. Si par exemple \( f\in \aL(E,V)\) et \( g\in \aL(E,W)\), nous avons \( f\otimes g\in \aL(E,V)\otimes \aL(E,W)\). Dans le Frido nous ne nous permettons pas de dire calmement que \( \aL(E,V)\otimes \aL(E,W)=\aL(E,V\otimes W)\). Et je ne vous dit même pas à quel point il n'est pas évident, si \( f\in C^{\infty}(E,V)\) et \( g\in  C^{\infty}(E,W)\) que nous aurions \( f\otimes g\in C^{\infty}(E,V)\otimes  C^{\infty}(E,W)= C^{\infty}(E,V\otimes W)\).

Tout cela pour dire que nous n'allons pas nous lancer dans des abus de notations. Non. Au lieu de cela, nous introduisons une notation. Pour rappel, dans tout le Frido, \( \Fun(A,B)\) désigne l'ensemble de toutes les application de \( A\) vers \( B\) sans suppositions de régularité. Pour les puristes, nous précisions que si \( f\in\Fun(A,B)\), nous supposons que \( f\) est définie sur tout \( A\). hum \ldots sauf mention du contraire.
\begin{definition}      \label{DEFooMVNDooFWFtRn}
    Si \( f\in \Fun(E,V)\) et \( g\in \Fun(E,W)\), alors nous définissons
    \begin{equation}
        \begin{aligned}
            f\tilde\otimes g\colon E&\to V\otimes W \\
            a&\mapsto f(a)\otimes g(a). 
        \end{aligned}
    \end{equation}
\end{definition}

\begin{proposition}     \label{PROPooCRVXooEGxdZl}
    Soient des applications continues \( f\colon E\to V\) et \( g\colon E\to W\) entre espaces vectoriels de dimension finies. Alors la fonction \( f\tilde\otimes g\colon E\to V\otimes W\) est continue.
\end{proposition}

\begin{proof}
    Soit \( a\in E\) et une suite \( x_k\to a\) dans \( E\). Nous voulons prouver que \( f\tilde\otimes g(x_k)\stackrel{V\otimes W}{\longrightarrow}f(a)\otimes g(a)\). Nous avons :
    \begin{equation}        \label{EQooSNXUooXrYOeY}
        \| f(x_k)\otimes g(x_k)-f(a)\otimes g(a) \|\leq \| f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a) \|+\| f(x_k)\otimes g(a)-f(a)\otimes g(a) \|.
    \end{equation}
    Ensuite en utilisant la classe d'équivalence \eqref{SUBEQooSHBJooJLPVbK}, 
    \begin{equation}
        f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a)=f(x_k)\otimes \big( g(x_k)-g(a) \big),
    \end{equation}
    et en ce qui concerne les normes,
    \begin{equation}
    \|   f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a)\|  =\|f(x_k)\|  \|\otimes \big( g(x_k)-g(a) \big)\|.
    \end{equation}
    Mais par hypothèse, \( f(x_k)\to f(a)\) et \( g(x_k)\to g(a)\). Donc le tout tend vers zéro lorsque \( k\to \infty\).

    Le même raisonnement fonctionne avec le second terme de \eqref{EQooSNXUooXrYOeY}.
\end{proof}

Lorsque nous parlons de différentielle de produit de fonctions, nous voulons étudier la différentiabilité de \( f\tilde\otimes g\) sous l'hypothèse de différentiabilité de \( f\) et \( g\). Et aussi, si \( f\) et \( g\) sont de classe \( C^p\), est-ce que \( f\tilde\otimes g\) est également de classe \( C^p\) ?

Nous voudrions avoir une formule du type
\begin{equation}
    d(f\tilde\otimes g)=df\tilde\otimes g+f\tilde\otimes dg,
\end{equation}
mais ça ne colle pas au niveau des espaces. En effet, en évaluant cela en \( a\in E\), nous avons à gauche \( d(f\tilde\otimes g)_a\in\aL(E,V\otimes W)\), tandis qu'à droite nous avons \( df_a\otimes g(a)\in \aL(E,V)\otimes W\) et \( f(a)\otimes dg_a\in V\otimes \aL(E,W)\).

Nous pourrions bien entendu dire que \( V\otimes \aL(E,W)\) est isomorphe à \( \aL(E,V\otimes W)\) et hop voila, on n'en parle plus. Ce serait passer sur deux points importants. D'abord est-ce que \( V\otimes \aL(E,W)\) est vraiment isomorphe à \( \aL(E,V\otimes W)\) ? Et ensuite, l'isomorphisme implique une utilisation du théorème \ref{THOooIHPIooIUyPaf} qui est tout sauf une trivialité.

Bref, fidèle au principe fridesque de ne pas cacher des difficultés techniques sous des abus de notations, nous allons écrire les choses explicitement.

\begin{lemma}
    Si \( E\), \( V\) et \( W\) sont de dimension finie, les applications
    \begin{equation}        \label{EQooVWXRooCesUqH}
        \begin{aligned}
            \psi\colon \aL(E,V)\otimes W&\to \aL(E,V\otimes W) \\
            f\otimes w&\mapsto \Big( u\mapsto f(u)\otimes w \Big) 
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            \varphi\colon V\otimes \aL(E,W)&\to \aL(E,V\otimes W) \\
            v\otimes g&\mapsto \big( a\mapsto v\otimes g(a) \big). 
        \end{aligned}
    \end{equation}
    sont des isomorphismes d'espaces vectoriels.
\end{lemma}
Dans le meilleur des mondes, ces applications devraient être affublés d'indices \( V\) et \( W\).

\begin{proof}
    Nous donnons des détails à propos de \( \psi\). Pour \( \varphi\) c'est la même chose.
    \begin{subproof}
        \item[Linéaire]
            La formule \eqref{EQooVWXRooCesUqH} définit \( \psi\) en particulier sur une base de \( \aL(E,V)\otimes W\) par la proposition \ref{PROPooTHDPooWgjUwk}\ref{ITEMooQCILooUncdGl}. Ce que signifie réellement la formule \eqref{EQooVWXRooCesUqH} est que \( \psi\) est ainsi définie sur la base et est prolongée par continuité.
        \item[Injective]
            Si pour un \( f\) et un \( w\) fixé nous avons \( \psi(f\otimes w)=0\), alors il y a deux cas : soit \( w=0\) soit \( w\neq0\). Dans le premier cas, \( f\otimes w=0\), et dans le second cas, nous remarquons que 
            \begin{equation}
                0=\psi(f\otimes w)(a)=f(a)\otimes w
            \end{equation}
            pour tout \( a\in E\). Cela implique \( f(a)=0\) pour tout \( a\) et donc \( f=0\), ce qui signifie que \( f\otimes w=0\).
        \item[Bijective]
            En utilisant la proposition \ref{PROPooTHDPooWgjUwk} et le lemme \ref{LEMooJXFIooKDzRWR}\ref{ITEMooPMLWooNbTyJI}, nous avons égalité des dimensions entre \( \aL(E,V)\otimes W\) et \( \aL(E,V\otimes W)\).

            Une application linéaire injective entre deux espaces vectoriels de même dimension (finie) est une bijection.
    \end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooZOAFooRMeBgI}
    Soient des espaces vectoriels normés de dimension finie. Soient \( f\colon E\to V\) et \( g\colon E\to W\) des fonctions de classe \( C^1\). Alors \( f\tilde\otimes g\colon E\to V\otimes W\) est de classe \( C^1\) nous avons les formules
    \begin{equation}        \label{EQooSUSCooBhZXFC}
        d(f\tilde\otimes g)_a(u)=df_a(u)\otimes g(a)+f(a)\otimes dg_a(u)
    \end{equation}
    ainsi que
    \begin{equation}        \label{EQooOCEEooUrsIDd}
        d(f\tilde\otimes g)=\psi\circ(df\tilde\otimes g)+\varphi\circ(f\tilde\otimes dg).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous commençons par prouver que \( f\tilde\otimes g\) est différentiable en injectant le candidat \eqref{EQooSUSCooBhZXFC} dans la définition. Au numérateur nous avons :
    \begin{equation}        \label{EQooOMXSooYsAiKh}
        (f\tilde\otimes g)(a+h)-(f\tilde\otimes g)(a)-df_a(h)\otimes g(a)-f(a)\otimes dg_a(h).
    \end{equation}
    Le lemme \ref{LEMooYQZZooVybqjK} assure qu'il existe une fonction \( \alpha\colon E\to V\) telle que \( \lim_{h\to 0} \alpha(h)/\| h \|\) et \( f(a+h)+f(a)+df_a(h)+\alpha(h)\). Même chose pour \( g\). Nous avons donc
    \begin{equation}
        (f\tilde\otimes g)(a+h)=f(a+h)\otimes g(a+h)=\big( f(a)+df_a(h)+\alpha(h) \big)\otimes \big( g(a)+dg_a(h)+\beta(h) \big)
    \end{equation}
    qui se développe en \( 9\) termes. En effectuant les différences dans \eqref{EQooOMXSooYsAiKh}, nous nous retrouvons avec un numérateur qui vaut
    \begin{equation}
        f(a)\otimes \beta(h)+df_a(h)\otimes dg_a(h)+df_a(h)\otimes \beta(h)+\alpha(h)\otimes g(a)+\alpha(h)\otimes dg_a(h)+\alpha(h)\otimes \beta(h).
    \end{equation}
    Nous pouvons prouver terme à terme qu'en divisant par \( \| h \|\) nous avons une limite qui vaut zéro. Par exemple,
    \begin{equation}
        \lim_{h\to 0} \frac{ f(a)\otimes \beta(h) }{ \| h \| }
    \end{equation}
    se calcule en prenant la norme du numérateur et en utilisant le lemme \ref{LEMooQPXHooJWfpmk} :
    \begin{equation}
        \frac{ \| f(a)\otimes \beta(h) \| }{ \| h \| }=\frac{ \| f(a) \|\| \beta(h) \| }{ \| h \| }\to 0.
    \end{equation}
    Tous les termes contenant \( \alpha(h)\) ou \( \beta(h)\) se traitent de la même manière. Le dernier terme à traiter est
    \begin{equation}
        \lim_{h\to 0} \frac{ df_a(h)\otimes dg_a(h) }{ \| h \| }.
    \end{equation}
    En prenant la norme du numérateur, en utilisant encore le lemme \ref{LEMooQPXHooJWfpmk} et en utilisant le lemme \ref{LEMooIBLEooLJczmu}, nous avons
    \begin{equation}
        \| df_a(h)\otimes dg_a(h) \|=\| df_a(h) \|\| dg_a(h) \|\leq \| df_a \|\| dg_a \|\| h \|^2,
    \end{equation}
    donc
    \begin{equation}
        \lim_{h\to 0} \frac{ df_a(h)\otimes dg_a(h) }{ \| h \| }=0.
    \end{equation}
    Notons que l'utilisation du lemme \ref{LEMooIBLEooLJczmu} requière que \( df_a\) soit continue, ce qui n'est pas évident en dimension infinie : une application linéaire n'est pas spécialement continue. C'est donc ici que nous utilisons le fait que \( E\), \( V\) et \( W\) sont de dimension finie\quext{Il y a surement moyen de paufiner, et d'affaiblir cette hypothèse, mais je ne me lance pas là-dedans.}.

    Ceci prouve que \( f\tilde\otimes g\) est différentiable et nous donne la formule \eqref{EQooSUSCooBhZXFC} pour appliquer sa différentielle à un élément de \( E\). La formule \eqref{EQooOCEEooUrsIDd} est un corollaire : elle se vérifie en l'appliquant à \( a\) puis à \( u\).
    
    Pour terminer nous devons prouver que \( d(f\tilde\otimes g)\) est continue. Vu que \( f\) et \( g\) sont de classe \( C^1\), les applications \( f\), \( g\), \( df\) et \( dg\) sont continues. Les applications \( \psi\) et \( \varphi\) sont également continues parce que linéaires sur des espaces de dimensioy finie. La proposition \ref{PROPooCRVXooEGxdZl} appliquée à \( df\) et \( g\) montre que \( df\tilde\otimes g\) est continue. La composition avec \( \psi\) qui est linéaire conserve la continuité.

    Dons le membre de droite de \eqref{EQooOCEEooUrsIDd} est continu et \( f\tilde\otimes g\) est a une différentielle continue. Elle est donc de classe \( C^1\).
\end{proof}

Il est temps de démontrer le truc difficile, à savoir que si \( f\) et \( g\) sont de classe \( C^p\), alors \( f\tilde\otimes g\) est également de classe \( C^p\). 

\begin{proposition}     \label{PROPooAWZFooMlhoCN}
    Nous applellons \( P_k\) la propriété suivante :
    \begin{quote}
        Pour tout \( p\geq k\), pour tout espaces vectoriels normés \( E\), \( V\), \( W\) de dimension finies et pour toutes applications \( f\colon E\to V\) et \( g\colon E\to W\) de classe \( C^k\), la fonction \( f\tilde\otimes g\) est de classe \( C^k\).
    \end{quote}
    \begin{enumerate}
        \item       \label{ITEMooDQRYooAEdxrW}
            La propriété \( P_k\) est vraie pour tout \( k\).
        \item       \label{ITEMooUUIFooGDyTMM}
            Si \( f\colon E\to V\) et \( g\colon E\to W\) sont de classe \( C^p\), alors \( f\tilde\otimes g\colon E\to V\otimes W\) est de classe \( C^p\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le gros de la preuve est le point \ref{ITEMooDQRYooAEdxrW}. Le point \ref{ITEMooUUIFooGDyTMM} est alors une utilisation de la propriété \( P_p\) avec \( p=k\).

    Pour \( k=0\). Si \( f\) et \( g\) sont de classe \( C^p\) avec \( p\geq k\), alors \( f\) et \( g\) sont a fortiori continues. La proposition \ref{PROPooCRVXooEGxdZl} montre alors que \( f\tilde\otimes g\) est continue.

    Bien que ce ne soit pas tout à fait nécessaire, nous prouvons que \( P_1\) est également vraie avant de passer à la récurrence. Si \( f\) et \( g\) sont de classe \( C^p\) avec \( p\geq 1\), alors elles sont de classe \( C^1\) et la proposition \ref{PROPooZOAFooRMeBgI} s'applique : \( f\tilde\otimes g\) est de classe \( C^1\).

    Nous faisons la récurrence en supposant que \( P_k\) est vraie, et en prouvant que \( P_{k+1}\) est vraie. Soit \( p\geq k+1\) ainsi que des applications \( f\colon E\to V\) et \( g\colon E\to W\) de classe \( C^{k+1}\). La proposition \ref{PROPooZOAFooRMeBgI} dit que \( f\tilde\otimes g\) est de classe \( C^1\) et que
    \begin{equation}
        d(f\tilde\otimes g)=\psi\circ(df\tilde\otimes g)+\varphi\circ(f\tilde\otimes dg).
    \end{equation}
    À droite, \( df\) et \( g\) sont de classe \( C^k\) parce que \( f\) et \( g\) sont de classe \( C^{k+1}\). Donc \( df\tilde\otimes g\) est de classe \( C^k\) par l'hypothèse de récurrence appliquée aux espaces \( \aL(E,V)\) et \( W\). La proposition \ref{PROPooRCZOooSgvpSE} nous assure alors que \( \psi\circ(df\tilde\otimes g)\) est de classe \( C^k\) également.

    Nous avons prouvé que \( d(f\tilde\otimes g)\) est de classe \( C^k\), donc \( f\tilde\otimes g\) est de classe \( C^{k+1}\). Cela nous fait la récurrence.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Formule des accroissements finis}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropDQLhSoy}
    Soient \( a<b\) dans \( \eR\) et deux fonctions
    \begin{subequations}
        \begin{align}
            f\colon \mathopen[ a , b \mathclose]\to E\\
            g\colon \mathopen[ a , b \mathclose]\to \eR
        \end{align}
    \end{subequations}
    continues sur \( \mathopen[ a , b \mathclose]\) et dérivables sur \( \mathopen] a , b \mathclose[\). Si pour tout \( t\in\mathopen] a , b \mathclose[\) nous avons \( \| f'(t) \|\leq g'(t)\) alors
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
        \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\) et la fonction
    \begin{equation}
        \begin{aligned}
            \varphi_{\epsilon}\colon \mathopen[ a , b \mathclose]&\to \eR \\
            t&\mapsto \| f(t)-f(a) \|-g(t)-\epsilon t.
        \end{aligned}
    \end{equation}
    Cela est une fonction continue réelle à variable réelle. En particulier pour tout \( u\in\mathopen] a , b \mathclose[\) la fonction \( \varphi_{\epsilon}\) est continue sur le compact \( \mathopen[ u , b \mathclose]\) et donc y atteint son minimum en un certain point \( c\in\mathopen[ u , b \mathclose]\); c'est le bon vieux théorème de Weierstrass~\ref{ThoWeirstrassRn}. Nous commençons par montrer que pour tout \( u\), ledit minimum ne peut être que \( b\). Pour cela nous allons montrer que si \( t\in\mathopen[ u , b [\), alors \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) pour un certain \( s>t\). Par continuité si \( s\) est proche de \( t\) nous avons
        \begin{equation}
            \left\|  \frac{ f(s)-f(t) }{ s-t }  \right\|-\frac{ \epsilon }{2}<\| f'(t) \|<g'(t)+\frac{ \epsilon }{2}=\frac{ g(s)-g(t) }{ s-t }+\frac{ \epsilon }{2}.
        \end{equation}
        Ces inégalités proviennent de la limite
        \begin{equation}
            \lim_{s\to t} \frac{ f(s)-f(t) }{ s-t }=f'(t),
        \end{equation}
        donc si \( s\) et \( t\) sont proches,
        \begin{equation}
            \left\| \frac{ f(s)-f(t) }{ s-t }-f'(t) \right\|
        \end{equation}
        est petit. Si \( s>t\) nous pouvons oublier des valeurs absolues et transformer l'inégalité en
        \begin{equation}
            \| f(s)-f(t) \|<g(s)-g(t)+\epsilon(s-t).
        \end{equation}
        Utilisant cela et l'inégalité triangulaire,
        \begin{subequations}
            \begin{align}
                \varphi_{\epsilon}(s)&\leq\| f(s)-f(t) \|+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &\leq g(s)-g(t)+\epsilon s-\epsilon t+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &=\varphi_{\epsilon}(t).
            \end{align}
        \end{subequations}
        Donc nous avons bien \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) avec l'inégalité stricte. Par conséquent pour tout \( u\in\mathopen] a , b \mathclose[\) nous avons \( \varphi_{\epsilon}(b)<\varphi_{\epsilon}(u)\) et en prenant la limite \( u\to a\) nous avons
        \begin{equation}
            \varphi_{\epsilon}(b)\leq \varphi_{\epsilon}(a).
        \end{equation}
        Cette inégalité donne immédiatement
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a)+\epsilon(b-a)
        \end{equation}
         pour tout \( \epsilon>0\) et donc
         \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
         \end{equation}
\end{proof}

\begin{theorem}[Théorème des accroissements finis]\label{ThoNAKKght}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application différentiable \( f\colon U\to F\). Pour tout segment \( \mathopen[ a , b \mathclose]\subset U\) nous avons
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \| \right)\| b-a \|.
    \end{equation}
\end{theorem}
\index{théorème!accroissements finis}


\begin{proof}
    Nous prenons les applications
    \begin{equation}
        \begin{aligned}
            k\colon \mathopen[ 0 , 1 \mathclose]&\to E \\
            t&\mapsto f\big( (1-t)a+tb \big)
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            g\colon \mathopen[ 0 , 1 \mathclose]&\to \eR \\
            t&\mapsto t\sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \|\| b-a \|.
        \end{aligned}
    \end{equation}
    Pour tout \( t\) nous avons \( g'(t)=M\| b-a \|\) où il n'est besoin de dire ce qu'est \( M\). D'un autre côté nous avons aussi
    \begin{equation}
        \begin{aligned}[]
            k'(t)&=\lim_{\epsilon\to 0}\frac{ f\big( (1-t-\epsilon)a+(t+\epsilon)b \big)-f\big( (1-t)a+tb \big) }{ \epsilon }\\
            &=\Dsdd{ f\big( (1-t)a+tb+\epsilon(b-a) \big)  }{\epsilon}{0}\\
            &=df_{(1-t)a+tb}(b-a)
        \end{aligned}
    \end{equation}
    où nous avons utilisé l'hypothèse de différentiabilité de \( f\) sur \( \mathopen[ a , b \mathclose]\) et donc en \( (1-t)a+tb\). Nous avons donc
    \begin{equation}
        \| k'(t) \|\leq \| b-a \|\| df_{(1-t)a+tb} \|\leq M\| b-a \|=g'(t)
    \end{equation}
    La proposition~\ref{PropDQLhSoy} est donc utilisable et
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est-à-dire
    \begin{equation}
        \| f(b)-f(a) \|=M\| b-a \|
    \end{equation}
    comme il se doit.
\end{proof}

\begin{proposition} \label{ProFSjmBAt}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application \( f\colon U\to F\). Soient \( a,b\in U\) tels que \( \mathopen[ a , b \mathclose]\subset U\). Nous posons \( u=(b-a)/\| b-a \|\) et nous supposons que pour tout \( x\in\mathopen[ a , b \mathclose]\), la dérivée directionnelle
    \begin{equation}
        \frac{ \partial f }{ \partial u }(x)=\Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe. Nous supposons de plus que \( \frac{ \partial f }{ \partial u }(x)\) est continue en \( x=a\). Alors
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \| \right)\| b-a \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous posons évidemment
    \begin{equation}
        M=\sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|
    \end{equation}
    et nous considérons les fonctions
    \begin{equation}
        k(t)=f\big( (1-t)a+tb \big)
    \end{equation}
    et
    \begin{equation}
        g(t)=tM\| b-a \|.
    \end{equation}
    Pour alléger les notations nous posons \( x=(1-t)a+tb\) et nous calculons avec un petit changement de variables dans la limite :
    \begin{equation}
        k'(t)=\Dsdd{  f\big( x+\epsilon(b-a) \big)  }{\epsilon}{0}=\| b-a \|\Dsdd{ f\big( x+\frac{ \epsilon }{ \| b-a \| }(b-a) \big) }{\epsilon}{0}=\| b-a \|\frac{ \partial f }{ \partial u }(x),
    \end{equation}
    et donc encore une fois nous avons
    \begin{equation}
        \| k'(t) \|\leq g'(t),
    \end{equation}
    ce qui donne
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est-à-dire
    \begin{equation}
        \| f(b)-f(a) \|\leq \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|\| b-a \|.
    \end{equation}
\end{proof}

\begin{theorem} \label{ThoOYwdeVt}
    Soient \( E,V\) deux espaces vectoriels normés, une application \( f\colon E\to V\), un point \( a\in E\) tel que pour tout \( u\in E\), la dérivée
    \begin{equation}
        \Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe pour tout \( x\in B(a,r)\) et est continue (par rapport à \( x\)) en \( x=a\). Nous supposons de plus que
    \begin{equation}
        \frac{ \partial f }{ \partial u }(a)=0
    \end{equation}
    pour tout \( u\in E\). Alors \( f\) est différentiable en \( a\) et
    \begin{equation}
        df_a=0
    \end{equation}
\end{theorem}

\begin{proof}
    Soit \( \epsilon>0\). Pourvu que \( \| h \|\) soit assez petit pour que \( a+h\in B(a,r)\), la proposition~\ref{ProFSjmBAt} nous donne
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \sup_{x\in\mathopen[ a , a+h \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|  |h |
    \end{equation}
    où \( u=h/\| h \|\). Par continuité de \( \partial_uf(x)\) en \( x=a\) et par le fait que cela vaut \( 0\) en \( x=a\), il existe un \( \delta>0\) tel que si \( \| h \|<\delta\) alors
    \begin{equation}
        \| \frac{ \partial f }{ \partial u }(a+h) \|\leq \epsilon.
    \end{equation}
    Pour de tels \( h\) nous avons
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \epsilon\| h \|,
    \end{equation}
    ce qui prouve que l'application linéaire \( T(u)=0\) convient parfaitement pour faire fonctionner la définition \ref{DefDifferentiellePta}.
%
%    Nous ne supposons plus que les dérivées directionnelles de \( f\) sont nulles en \( x=a\). Alors nous posons, pour \( x\in U\),
%    \begin{equation}    \label{EqCUgHXHy}
%        g(x)=f(x)-\Dsdd{ f(a+s(x-a)) }{s}{0}.
%    \end{equation}
%    Le fait que cette fonction soit bien définie est encore un coup de hypothèses sur les dérivées directionnelles de \( f\) qui sont bien définies autour de \( a\). Cette nouvelle fonction \( g\) satisfait à \( \frac{ \partial g }{ \partial v }(a)=0\) pour tout \( v\in E\) parce que
%    \begin{subequations}
%        \begin{align}
%            \frac{ \partial g }{ \partial v }(a)&=\Dsdd{ g(a+tv) }{t}{0}\\
%            &=\Dsdd{ f(a+tv)-\Dsdd{ f\big( a+s(tv) \big) }{s}{0} }{t}{0}\\
%            &=\frac{ \partial f }{ \partial v }(a)-\Dsdd{ t\frac{ \partial f }{ \partial v }(a) }{t}{0}\\
%            &=0.
%        \end{align}
%    \end{subequations}
%    Pour la dérivée par rapport à \( s\) nous avons effectué le changement de variables \( s\to ts\), ce qui explique la présence d'un \( t\) en facteur. La fonction \( g\) est donc différentiable en \( a\).
%
%
% Position 229262367
    % Attention : ce qui suit est faux. Mais il y a peut-être moyen d'adapter.
%\item[Dérivées non nulles]
%
%    Nous allons montrer que la fonction
%    \begin{equation}
%        l(x)=\Dsdd{ f\big( a+s(x-a) \big) }{t}{0}
%    \end{equation}
%    est différentiable en \( x=a\), de différentielle \( T(u)=l(u+a)\). Cela fournira la différentiabilité de \( f\) parce que \eqref{EqCUgHXHy} donnerait alors \( f\) comme somme de deux fonctions différentiables.
%
%    En premier lieu nous devons montrer que \( T\) ainsi définie est linéaire.
%
%    Notre but est donc de prouver que
%    \begin{equation}
%        \lim_{h \to 0}\frac{ \| l(x+h)-l(x)-l(h) \| }{ \| h \| }=0.
%    \end{equation}
%    Un premier pas est de calculer
%    \begin{subequations}
%        \begin{align}
%            l(x+h)-l(x)-l(h)&=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(0)-f(sx)+f(0)-f(sh)+f(0) }{ s }\\
%            &=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) }{ s }.
%        \end{align}
%    \end{subequations}
%    Ensuite nous étudions le numérateur en utilisant la proposition~\ref{ProFSjmBAt}:
%    \begin{subequations}
%        \begin{align}
%            \| f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) \|&\leq  \| f\big( s(x+h) \big)-f(sx)\| + \|f(sh)-f(0) \|  \\
%            &\leq \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|\\
%            &\quad +\sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|.
%        \end{align}
%    \end{subequations}
%    La division par \( s\) se passe bien et nous avons
%    \begin{subequations}
%        \begin{align}
%            \| l(x+h)-l(x)-l(h) \|&\leq \lim_{s\to 0}  \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|+ \sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|\\
%            &=2\| h \|\| \frac{ \partial f }{ \partial h }(0) \|        \label{SubeqVMMoSDH}\\
%            &=2\| h \|^2\| \frac{ \partial f }{ \partial u }(0) \|
%        \end{align}
%    \end{subequations}
%    où nous avons posé \( u=h/\| h \|\). Pour l'égalité \eqref{SubeqVMMoSDH} nous avons utilisé la continuité de \( \frac{ \partial f }{ \partial h }(z)\) en \( z=0\). Du coup
%    \begin{equation}
%        \lim_{y\to 0} \frac{ \| f(x+h)-f(x)-f(h) \| }{ \| h \| }=\lim_{h\to 0} 2\| h \|\| \frac{ \partial f }{ \partial u }(0) \|=0.
%    \end{equation}
%    Cela prouve que \( l\) est bien différentiable en \( x=0\).
%
%    \end{subproof}
%
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Applications multilinéaires}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons déjà parlé d'applications multilinéaires dans la définition \ref{DefFRHooKnPCT}.

\begin{lemma}[Leibnitz pour les formes bilinéaires\cite{SNPdukn}]\label{LemFRdNDCd}
    Si \( B\colon E\times F\to G\) est bilinéaire et continue, elle est \(  C^{\infty}\) et
    \begin{equation}    \label{EqXYJgDBt}
        dB_{(x,y)}(u,v)=B(x,v)+B(u,y).
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord le membre de droite de \eqref{EqXYJgDBt} est une application linéaire et continue, donc c'est un bon candidat à être différentielle. Nous allons prouver que ça l'est, ce qui prouvera la différentiabilité de \( B\). Avec ce candidat, le numérateur de la définition \eqref{DefDifferentiellePta} s'écrit dans notre cas
    \begin{equation}
        B\big( (x,y)+(u,v) \big)-B(x,y)-B(x,v)-B(u,y)=B(u,v).
    \end{equation}
    Il reste à voir que
    \begin{equation}
        \lim_{ (u,v)\to (0,0) } \frac{ B(u,v) }{ \| (u,v) \| }=0
    \end{equation}
    Par l'équation \eqref{EqYLnbRbC} nous avons
    \begin{equation}
        \frac{ \| B(u,v) \| }{ \| (u,v) \| }\leq \frac{ \| B \|\| u \|\| v \| }{ \| u \| }=\| B \|\| v \|
    \end{equation}
    parce que \( \| (u,v) \|\geq \| u \|\). À partir de là il est maintenant clair que
    \begin{equation}
        \lim_{(u,v)\to (0,0)}\frac{ \| B(u,v) \| }{ \| (u,v) \| }=0,
    \end{equation}
    ce qu'il fallait.
\end{proof}

\begin{proposition}[Règle de Leibnitz\cite{SNPdukn}]
    Soient \( E,F_1,F_2\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f_1\colon U\to F_1\\
            f_2\colon U\to F_2\\
        \end{align}
    \end{subequations}
    et \( B\in\cL(F_1\times F_2,G)\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon U&\to G \\
            x&\mapsto B\big( f_1(x),f_2(x) \big)
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}    \label{EqMNGBXWc}
        d\varphi_x(u)=\varphi\big( (df_1)_x(u),f_2(x) \big)+\varphi\big( f_1(x),(df_2)_x(u) \big).
    \end{equation}
\end{proposition}
\index{Leibnitz!applications entre espaces vectoriels normés}

\begin{proof}
    Par hypothèse \( B\) est continue (c'est la définition de l'espace \( \cL\)), et donc \(  C^{\infty}\) par le lemme~\ref{LemFRdNDCd}. Par ailleurs la fonction \( f_1\times f_2\) est de classe \( C^r\) parce que \( f_1\) et \( f_2\) le sont et parce que la proposition~\ref{PropOYtgIua} le dit. L'application composée \( B\circ(f_1\times f_2)\) est donc également de classe \( C^r\) par le théorème~\ref{ThoAGXGuEt}.

    Il ne nous reste donc qu'à prouver la formule~\ref{EqMNGBXWc}. En utilisant la différentielle du produit cartésien\footnote{Proposition~\ref{PropOYtgIua}.} nous avons
    \begin{equation}
        f\big( B\circ(f_1\times f_2) \big)_x(h)=dB_{(f_1\times f_2)(x)}\big( (df_1)_x(h),(df_2)_x(h) \big).
    \end{equation}
    Nous développons cela en utilisant le lemme~\ref{LemFRdNDCd} :
    \begin{subequations}
        \begin{align}
        d\big( B\circ(f_1\times f_2) \big)_x(h)&=dB_{\big( f_1(x),f_2(x) \big)}\big( (df_1)_x(h),(df_2)_x(h) \big)\\
        &=B\big( f_1(x),(df_2)_x(h) \big)+B\big( (df_1)_x(h),f_2(x) \big),
        \end{align}
    \end{subequations}
    comme souhaité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Différentielle partielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Différentielle partielle]    \label{VJM_CtSKT}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés et une fonction \( f\colon E\times F\to G\). Nous définissons sa \defe{différentielle partielle}{différentielle!partielle} sur l'espace \( E\) par
    \begin{equation}
        \begin{aligned}
            d_1f_{(x_0,y_0)}\colon E&\to G \\
            u&\mapsto \Dsdd{ f(x_0+tu,y_0 }{t}{0} .
        \end{aligned}
    \end{equation}
    La différentielle \( d_2\) se définit de la même façon.
\end{definition}

\begin{proposition}[\cite{SNPdukn}] \label{PropLDN_nHWDF}
    Soient \( E_1\), \( E_2\) et \( F\) des espaces vectoriels normés, soit un ouvert \( U\subset E_1\times E_2\) et une fonction \( f\colon U\to F\).
    \begin{enumerate}
        \item   \label{ItemRDD_oPmXVi}
            Si \( f\) est différentiable alors les différentielles partielles existent et
            \begin{subequations}
                \begin{align}
                    d_1f_{(x_0,y_0)}(u)=df_{(x_0,y_0)}(u,0)\\
                    d_2f_{(x_0,y_0)}(v)=df_{(x_0,y_0)}(0,v)
                \end{align}
            \end{subequations}
            où \( u\in E_1\) et \( v\in E_2\).
        \item
            Si \( f\) est différentiable alors
            \begin{equation}
                df_{(x_0,y_0)}(u,v)=d_1f_{(x_,y_0)}(u)+d_2f_{(x_0,y_0)}(v).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons \( \alpha=(x_0,y_0)\in U\) et
    \begin{equation}
        \begin{aligned}
            j_{\alpha}^{(1)}\colon E_1&\to E_1\times E_2 \\
            x&\mapsto (x,y_0).
        \end{aligned}
    \end{equation}
    C'est une fonction de classe \(  C^{\infty}\) et
    \begin{equation}
        (dj_{\alpha}^{(1)})_{x_0}(u)=\Dsdd{ j_{\alpha}^{(1)}(x_0+tu) }{t}{0}=\Dsdd{ (x_0+tu,y_0) }{t}{0}=(u,0).
    \end{equation}
    D'autre part
    \begin{subequations}
        \begin{align}
            (d_1f)_{\alpha}(u)&=\Dsdd{ f(x_0+tu,y_0) }{t}{0}\\
            &=\Dsdd{ (f\circ j_{\alpha}^{(1)})(x_0+tu) }{t}{0}\\
            &=\big( d(f\circ j_{\alpha}^{(1)}) \big)_{x_0}(u).
        \end{align}
    \end{subequations}
    À ce moment nous utilisons la règle des différentielles composées~\ref{ThoAGXGuEt} pour dire que
    \begin{equation}
        (d_1f)_{\alpha}(u)=df_{j_{\alpha}^{(1)}(x_0)}\circ (dj_{\alpha}^{(1)})_{x_0}(u)=df_{\alpha}(u,0).
    \end{equation}
    Voila qui prouve déjà le point~\ref{ItemRDD_oPmXVi}.

    Pour la suite nous considérons les fonctions
    \begin{equation}
        \begin{aligned}[]
            P_1(x,y)&=x,&&&J_1(u)&=(u,0),\\
            P_2(x,y)&=y,&&&J_2(v)&=(0,v)
        \end{aligned}
    \end{equation}
    et nous avons l'égalité évidente
    \begin{equation}
        J_1\circ P_1+J_2\circ P_2=\mtu
    \end{equation}
    sur \( E_1\times E_2\). En appliquant \( df_{\alpha}\) à cette dernière égalité, en appliquant à \( (u,v)\) et en utilisant la linéarité de \( df_{\alpha}\) nous trouvons
    \begin{subequations}
        \begin{align}
            df_{\alpha}(u,v)&=df_{\alpha}\big( (J_1\circ P_1)(u,v) \big)+df_{\alpha}\big( (J_2\circ P_2)(u,v) \big)\\
            &=df_{\alpha}(u,0)+df_{\alpha}(0,v)\\
            &=(d_1f)_{\alpha}(u)+(d_2f)_{\alpha}(v)
        \end{align}
    \end{subequations}
    où nous avons utilisé le point~\ref{ItemRDD_oPmXVi} pour la dernière égalité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{L'inverse, sa différentielle}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) est un espace de Banach, nous sommes intéressés à l'espace \( \GL(E)\) des endomorphismes inversibles de \( E\) sur \( E\). Cet ensemble est métrique par la formule usuelle
\begin{equation}
    \| T \|=\sup_{\| x \|=1}\| T(x) \|_E.
\end{equation}

\begin{proposition}[Thème~\ref{THEMEooPQKDooTAVKFH}]     \label{PropQAjqUNp}
    Soit \( E\) un espace de Banach (espace vectoriel normé complet). Si \( A\) est un endomorphisme de \( E\) satisfaisant  \( \| A \|<1\) pour la norme opérateur, alors \( (\mtu-A)\) est inversible et son inverse est donné par
    \begin{equation}
        (\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k.
    \end{equation}
\end{proposition}
\index{série!donnant \( (1-A)^{-1}\)}

\begin{proof}
    Étant donné que la norme opérateur est une norme algébrique (lemme~\ref{LEMooFITMooBBBWGI}), nous avons \( \| A^k \|\leq \| A \|^k\). Par conséquent la série \( \| A^k \|\) est majorée par la série géométrique qui converge\footnote{Voir l'exemple \ref{ExZMhWtJS}.}. Par conséquent \( \sum_{k}A^k\) est une série absolument convergente et donc convergente par la proposition~\ref{PropAKCusNM} et le fait que \( \aL(E)\) est complet (proposition~\ref{LemCAIPooPMNbXg}).

    Montrons à présent que la somme est l'inverse de \( \mtu-A\) en utilisant le produit terme à terme autorisé par la proposition~\ref{PropQXqEPuG} :
    \begin{equation}
        \sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
    \end{equation}
    Par conséquent
    \begin{equation}
        \| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\leq \| A \|^{n+1}\to 0.
    \end{equation}
\end{proof}

\begin{theorem}[Inverse dans \( \GL(E)\)\cite{laudenbach2000calcul,SNPdukn}]    \label{ThoCINVBTJ}
    Soient \( E\) et \( F\) des espaces vectoriels normés.
    \begin{enumerate}
        \item
        L'ensemble \( \GL(E)\) est ouvert dans \( \End(E)\).
    \item
        L'application inverse
    \begin{equation}
        \begin{aligned}
        i\colon \GL(E,F)&\to \GL(F,E) \\
        u&\mapsto u^{-1}
        \end{aligned}
    \end{equation}
    est de classe \( C^{\infty}\) et
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}\circ h\circ u_0^{-1}
    \end{equation}
    pour tout \( h\in\End(E)\)
    \end{enumerate}
\end{theorem}
\index{différentielle!de $u\mapsto u^{-1}$}

\begin{proof}
Nous supposons que \( \GL(E,F)\) n'est pas vide, sinon ce n'est pas du jeu.
        \begin{subproof}

        \item[Cas de dimension finie]

            Si la dimension de \( E\) et \( F\) est finie, elles doivent être égales, sinon il n'y a pas de fonctions inversibles \( E\to F\). L'ensemble \( \GL(E,F)\) est donc naturellement \( \GL(n,\eR)\). Un élément de \( \eM(n,\eR)\) est dans \( \GL(n,\eR)\) si et seulement si son déterminant est non nul. Le déterminant étant une fonction continue (polynomiale) en les entrées de la matrice, l'ensemble \( \GL(n,\eR)\) est ouvert dans \( \eM(n,\eR)\).

            Même idée pour la régularité de la fonction \( i\colon \GL(n,\eR)\to \GL(n,\eR)\), \( X\mapsto X^{-1}\). Les entrées de \( X^{-1}\) sont les cofacteurs de \( X\) divisé par \( \det(X)\), et donc des polynômes en les entrées de \( X\) divisés par un polynôme qui ne s'annule pas sur \( \GL(n,\eR)\), et donc sur un ouvert autour de \( X\) et de \( X^{-1}\). Bref, tout est \(  C^{\infty}\).

            Le reste de la preuve parle de la dimension infinie.

        \item[Ouvert autour de l'identité]

        Nous commençons par prouver que \( B(\mtu,1)\subset \GL(E)\). Pour cela il suffit de remarquer que si \( \| u \|<1\) alors le lemme~\ref{PropQAjqUNp} nous donne un inverse de \( (1+u)\) en la personne de \( \sum_{k=0}^{\infty}(-u)^k\).

    \item[Ouvert en général]

        Soit maintenant \( u_0\in\GL(E)\). Si \( \| u \|<\frac{1}{ \| u_0^{-1} \| }\) alors \( \| u_0^{-1}u \|<1\), ce qui signifie que
        \begin{equation}
            \mtu+u_0^{-1}u
        \end{equation}
    est inversible. Mais \( u_0+u=u_0(\mtu+u_0^{-1}u)\), donc \( u_0+u\in\GL(E)\) ce qui signifie que
    \begin{equation}
    B\left( u_0,\frac{1}{ \| u_0^{-1} \| } \right)\subset \GL(E).
    \end{equation}

    \item[Différentielle en l'identité]

    Nous commençons par prouver que \( di_{\mtu}(u)=-u\). Pour cela nous posons
    \begin{equation}
        \alpha(h)=\sum_{k=2}^{\infty}(-1)^kh^k
    \end{equation}
    et nous calculons
    \begin{equation}
    di_{\mtu}(u)=\Dsdd{ i(\mtu+tu) }{t}{0}=\Dsdd{ \mtu-tu+\alpha(tu) }{t}{0}.
    \end{equation}
    Il suffit de prouver que \( \Dsdd{ \alpha(tu) }{t}{0}=0\) pour conclure que \( di_{\mtu}(u)=-u\). Pour cela, nous remarquons que \( \alpha(0)=0\) et donc que
    \begin{subequations}
        \begin{align}
        \Dsdd{ \alpha(tu) }{t}{0}&=\lim_{t\to 0} \frac{ \alpha(tu)-\alpha(0) }{ t }\\
        &=\lim_{t\to 0} \sum_{k=2}^{\infty}(-1)^k\frac{ (tu)^k }{ t }\\
        &=-\lim_{t\to 0} u\sum_{k=1}^{\infty}(-1)^kt^ku^k.
        \end{align}
    \end{subequations}
    La norme de ce qui est dans la limite est majorée par
    \begin{equation}
    \| u \|\sum_{k=1}^{\infty}\| tu \|^k=\| u \|\left( \frac{1}{ 1-\| tu \| }-1 \right),
    \end{equation}
    et cela tend vers zéro lorsque \( t\to\infty\). Nous avons utilisé la somme~\ref{EqRGkBhrX} de la série géométrique. Nous avons bien prouvé que \( di_{\mtu}(u)=-u\).

    \item[Différentielle en général]
    Soit maintenant \( u_0\in\GL(E)\) et \( h\in\End(E)\) tel que \( u_0+h\in \GL(E)\); par le premier point, il suffit de prendre \( \| h \|\) suffisamment petit. Vu que \( u_0+h=u_0(\mtu+u_0^{-1}h)\) nous avons
    \begin{equation}
        (u_0+h)^{-1}=(\mtu+u_0^{-1}h)^{-1}u_0^{-1}.
    \end{equation}
    Nous pouvons donc calculer
    \begin{equation}
        (u_0+h)^{-1}=\big( \mtu-u_0^{-1}h+\alpha(u_0^{-1}h) \big)u_0^{-1}=u_0^{-1}-u_0^{-1}hu_0^{-1}+\alpha(u_0^{-1}h)u_0^{-1},
    \end{equation}
    et ensuite
    \begin{equation}
        di_{u_0}(h)=\Dsdd{ i(u_0+th) }{t}{0}=\Dsdd{ u_0^{-1}-tu_0^{-1}hu_0^{-1}+\alpha(tu_0^{-1}h)u_0^{-1} }{t}{0},
    \end{equation}
    mais nous avons déjà vu que
    \begin{equation}
        \Dsdd{ \alpha(th) }{t}{0}=0,
    \end{equation}
    donc
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}hu_0^{-1}
    \end{equation}
    Cela donne la différentielle de l'application inverse.

    \item[Continuité de l'inverse]

        L'application \( i\) est continue parce que différentiable.
    \item[L'inverse est \(  C^{\infty}\)]

        Nous allons écrire la fonction inverse comme une composée. Soient les applications
        \begin{equation}
            \begin{aligned}
                B\colon \cL(F,E)\times \cL(F,E)&\to \cL\big( \cL(E,F),\cL(F,E) \big) \\
                B(\psi_1,\psi_2)(A)&= -\psi_1\circ A\circ\psi_2
            \end{aligned}
        \end{equation}
        et
        \begin{equation}
            \begin{aligned}
                \Delta\colon \cL(F,E)&\to \cL(F,E)\times \cL(F,E) \\
                \varphi&\mapsto (\varphi,\varphi)
            \end{aligned}
        \end{equation}
        Nous avons alors
        \begin{equation}
            di=B\circ\Delta\circ i.
        \end{equation}
        L'application \( \Delta\) est de classe \(  C^{\infty}\). Nous devons voir que \( B\) l'est aussi. Pour le voir nous commençons par prouver qu'elle est bornée :
        \begin{equation}
            \begin{aligned}[]
                \| B \|&=\sup_{\| \psi_1 \|,\| \psi_2 \|=1}\| B(\psi_1,\psi_2) \|_{\aL\big( L(E,F),L(F,E) \big)}\\
                &=\sup_{  \| \psi_1 \|,\| \psi_2 \|=1 }\sup_{\| A \|=1}\| \psi_1\circ A\circ\psi_2 \|_{L(F,E)}\\
                &\leq \sup_{\| \psi_1 \|,\| \psi_2 \|=1}\sup_{\| A \|=1}\| \psi_1 \|\| A \|\| \psi_2 \|\\
                &\leq 1.
            \end{aligned}
        \end{equation}
        Donc \( B\) est bien bornée et par conséquent continue. Une application bilinéaire continue est \(  C^{\infty}\) par le lemme~\ref{LemFRdNDCd}. La décomposition \( di=B\circ \Delta\circ i\) nous donne donc que \( i\in C^{\infty}\) dès que \( i\) est continue, ce que nous avions déjà montré.
        \end{subproof}
\end{proof}


