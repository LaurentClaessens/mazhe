% This is part of Le Frido
% Copyright (c) 2008-2021
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Plusieurs choses sur les espaces vectoriels normés (dont la définition \ref{DefNorme}) ont déjà été vues dans la section \ref{SECooWKJNooKOqpsx}. Voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme, produit scalaire et Cauchy-Schwarz (cas réel)}
%---------------------------------------------------------------------------------------------------------------------------

Dans la suite, le produit scalaire de \( x\) et \( y\) pourra être noté indifféremment par \( x\cdot y\), \( \langle x, y\rangle \) ou \( b(x,y)\) lorsque une forme bilinéaire est donnée.

Nous rappelons au passage que les espaces vectoriels réels sont susceptibles de recevoir un produit scalaire, alors que les espaces vectoriels complexes sont susceptibles de recevoir un produit hermitien. Bien que de nombreux résultats soient identiques ou très similaires, ces deux notions sont à ne pas confondre.

Nous commençons par prouver qu'un produit scalaire étant donné, nous pouvons définir une norme par la formule \( \| x \|^2=\langle x, x\rangle \). Pour cela nous aurons besoin de l'inégalité de Cauchy-Schwarz.

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas réel]      \label{ThoAYfEHG}
    Soit un espace vectoriel muni d'un produit scalaire \( (x,y)\mapsto x\cdot y\). En posant\footnote{Attention à la notation : pour l'instant nous ne savons pas que c'est une norme. Ce sera justifié dans la proposition~\ref{PropEQRooQXazLz}.}
    \begin{equation}
        \| x \|=\sqrt{ x\cdot x },
    \end{equation}
    nous avons
    \begin{equation}        \label{EQooZDSHooWPcryG}
		| x\cdot y |\leq \| x \|\| y \|.
	\end{equation}
    Nous avons une égalité si et seulement si \( x\) et \( y\) sont multiples l'un de l'autre.
\end{theorem}
\index{Cauchy-Schwarz}
\index{inégalité!Cauchy-Schwarz}

\begin{proof}
	Étant donné que les deux membres de l'inéquation sont positifs, nous allons travailler en passant au carré afin d'éviter les racines carrés dans le second membre.

	Nous considérons le polynôme
	\begin{equation}
		P(t)=\| x+ty \|^2=(x+ty)\cdot(x+ty)=x\cdot x+x\cdot ty+ty\cdot x+t^2y\cdot y.
	\end{equation}
    En utilisant la bilinéarité (pour sortir les \( t\)) et la symétrique du produit scalaire, puis en ordonnant les termes selon les puissances de $t$,
	\begin{equation}
		P(t)=\| y \|^2t^2+2(x\cdot y)t+\| x \|^2.
	\end{equation}
    Cela est un polynôme du second degré en $t$ dont le signe est toujours positif (ou nul). Par la proposition \ref{PROPooEZIKooKjJroH} nous en déduisons que le fameux \( b^2-4ac\) doit être négatif ou nul. Nous avons donc
	\begin{equation}
		\Delta=4(x\cdot y)^2-4\| x \|^2\| y \|^2\leq 0,
	\end{equation}
	ce qui donne immédiatement
	\begin{equation}
		(x\cdot y)^2\leq\| x \|^2\| y \|^2.
	\end{equation}

    En ce qui concerne le cas d'égalité, si nous avons \( x\cdot y=\| x \|\| y \|\), alors le discriminant \( \Delta\) ci-dessus est nul et le polynôme \( P\) admet une racine double \( t_0\). Pour cette valeur nous avons
    \begin{equation}
        P(t_0)=| x+t_0y |=0,
    \end{equation}
    ce qui implique \( x+t_0y=0\) et donc que \( x\) et \( y\) sont liés.
\end{proof}

La proposition suivante montre que toute norme dérivant d'un produit scalaire vérifie l'identité du parallélogramme. Ce résultat sert souvent à prouver que des normes ne dérivent pas d'un produit scalaire. C'est le cas de la norme \( N(x,y)=| x |+| y |\) du lemme \ref{LEMooRWJYooOIJkZc} ainsi que du théorème de Weinersmith \ref{THOooCCMBooGulxkQ}.
\begin{proposition}[Norme dérivant d'un produit scalaire] \label{PropEQRooQXazLz}
    Si \( x,y\mapsto x\cdot y\) est un produit scalaire sur un espace vectoriel réel \( E\). Nous posons \( \| x \|=\sqrt{x\cdot x}\). Alors
    \begin{enumerate}
        \item
            L'opération \( \| . \|\) est une norme\footnote{Définition \ref{DefNorme}.}.
        \item
            Cette norme vérifie l'identité du parallélogramme :
            \begin{equation}        \label{EqYCLtWfJ}
                \| x-y \|^2+\| x+y \|^2=2\| x \|^2+2\| y \|^2.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    En deux parties.
    \begin{subproof}
        \item[C'est une norme]
            Nous allons nous contenter de prouver l'inégalité triangulaire. Si \( x,y\in E\) nous avons
            \begin{equation}
                \| x+y \|=\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}.
            \end{equation}
            Par l'inégalité de Cauchy-Schwarz, théorème~\ref{ThoAYfEHG} nous avons aussi
            \begin{equation}
                2x\cdot y\leq 2\| x \|\| y \|.
            \end{equation}
            Nous pouvons donc majorer ce qui est dans la racine carrée :
            \begin{equation}
                \| x \|^2+\| y \|^2+2x\cdot y\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|=\big( \| x \|+\| y \| \big)^2.
            \end{equation}
            En remettant les bouts ensemble,
            \begin{equation}
                \| x+y \|  =\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}  \leq \sqrt{\big( \| x \|+\| y \| \big)^2}=\| x \|+\| y \|.
            \end{equation}

        \item[Inégalité du parallélogramme]
            Cette assertion est seulement un calcul :
            \begin{equation}
                \begin{aligned}[]
                    \| x-y \|^2+\| x+y \|^2&=(x-y)\cdot (x-y)+(x+y)\cdot(x+y)\\
                    &=x\cdot x-x\cdot y-y\cdot x+y\cdot y\\
                    &\quad +x\cdot x+x\cdot y+y\cdot x+y\cdot y\\
                    &=2x\cdot x+2y\cdot y\\
                    &=2\| x \|^2+2\| y \|^2.
                \end{aligned}
            \end{equation}
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{RTzQrdx}]     \label{PropHIWjdMX}
    Soit \( b\) une forme bilinéaire et symétrique. Alors
    \begin{enumerate}
        \item
            \( \ker(b)\subset C(b)\) (cône d'isotropie, définition~\ref{DefVKMnUEM})
        \item
            si \( b\) est positive alors \( \ker(b)=C(b)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si \( z\in\ker(b)\) alors pour tout \( y\in E\) nous avons \( b(z,y)=0\). En particulier pour \( y=z\) nous avons \( b(z,z,)=0\) et donc \( z\in C(b)\).
        \item
            Soit \( b\) positive et \( x\in C(b)\). Par l'inégalité de Cauchy-Schwarz (proposition~\ref{ThoAYfEHG}) nous avons
            \begin{equation}
                | b(x,y) |\leq \sqrt{   b(x,x)b(y,y) }=0.
            \end{equation}
            Donc pour tout \( y\) nous avons \( b(x,y)=0\).
    \end{enumerate}
\end{proof}

\begin{lemma}       \label{LEMooEZFIooXyYybe}
    Soit un espace vectoriel euclidien\footnote{C'est-à-dire qu'il possède un produit scalaire, voir la définition \ref{DefLZMcvfj}.} \( E\) sur le corps \( \eK\). Si \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( E\) et si \( f\colon E\to E\) est un endomorphisme, alors
    \begin{equation}        \label{EQooQAZLooZutFUz}
        \det(f)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\langle e_{\sigma(i)}, f(e_i)\rangle.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous utilisons la définition \ref{LEMooQTRVooAKzucd} du déterminant d'un endomorphisme \( \det(f)=\det_B\big( f(B) \big)\) en prenant la liste des vecteurs \( \{ e_i \}\) comme \( B\). En l'occurrence, le \( i\)\ieme\ vecteur de la famille \( B\) est \( f(e_i)\).

    Vu que la base est orthonormée, nous avons \( e^*_k(v)=\langle e_k, v\rangle \) et donc aussi
    \begin{equation}
        e^*_{\sigma(i)}(v_i)=\langle e_{\sigma(i)}^*, f(e_i)\rangle.
    \end{equation}
\end{proof}

Et si vous avez tout suivi, vous aurez remarqué que les produits scalaires impliqués dans la formule \eqref{EQooQAZLooZutFUz} sont les éléments de la matrice de \( f\) dans la base \( \{ e_i \}\) parce que \( \langle e_i, f(e_j)\rangle \) est la composante \( i\) de l'image de \( e_j\) par \( f\). Si la matrice est composée en mettant en colonne les images des vecteurs de base, le compte est bon.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Théorème spectral autoadjoint}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{theorem}[Théorème spectral autoadjoint] \label{ThoRSBahHH}
    Un endomorphisme autoadjoint d'un espace euclidien
    \begin{enumerate}
        \item
            est diagonalisable dans une base orthonormée,
        \item
            a son spectre réel.
    \end{enumerate}
\end{theorem}
\index{théorème!spectral!autoadjoint}
\index{diagonalisation!endomorphisme autoadjoint}

\begin{proof}
    Nous procédons par récurrence sur la dimension de \( E\), et nous commençons par \( n=1\)\footnote{Dans \cite{KXjFWKA}, l'auteur commence avec \( n=0\) mais moi je n'en ai \wikipedia{en}{Vacuous_truth}{pas le courage.}.}. Soit donc \( f\colon E\to E\) avec \( \langle f(x), y\rangle =\langle x, f(y)\rangle \). Étant donné que \( f\) est également linéaire, il existe \( \lambda\in \eR\) tel que \( f(x)=\lambda x\) pour tout \( x\in E\). Tous les vecteurs de \( E\) sont donc vecteurs propres de \( f\).

    Passons à la récurrence. Nous considérons \( \dim(E)=n+1\) et \( f\in\gS(E)\). Nous considérons la forme bilinéaire symétrique \( \Phi_f\) et la forme quadratique associée \( \phi_f\). Pour rappel,
    \begin{subequations}
        \begin{align}
        \Phi_f(x,y)=\langle x, f(y)\rangle \\
        \phi_f(x)=\Phi_f(x,x).
        \end{align}
    \end{subequations}
    Et nous allons laisser tomber les indices \( f\) pour noter simplement \( \Phi\) et \( \phi\). Étant donné que \( \overline{ B(0,1) }\) est compacte et que \( \phi\) est continue, il existe \( x_0\in\overline{ B(0,1) }\) tel que
    \begin{equation}
        \lambda=\phi(x_0)=\sup_{x\in\overline{ B(0,1) }}\phi(x).
    \end{equation}
    Notons aussi que \( \| x_0 \|=1\) : le maximum est pris sur le bord. Nous posons
    \begin{equation}
        g=\lambda\id-f
    \end{equation}
    ainsi que
    \begin{equation}
        \Phi_1(x,y)=\langle x, g(y)\rangle .
    \end{equation}
    Cela est une forme bilinéaire et symétrique parce que
    \begin{equation}
        \Phi_1(y,x)=\langle y, g(x)\rangle =\langle g(y), x\rangle =\langle x, g(y)\rangle =\Phi_1(x,y)
    \end{equation}
    où nous avons utilisé le fait que \( g\) était autoadjoint et la symétrie du produit scalaire. De plus \( \Phi_1\) est semi-définie positive parce que
    \begin{equation}
        \Phi_1(x,x)=\langle x, \lambda x-f(x)\rangle =\lambda\| x \|^2-\phi(x).
    \end{equation}
    Vu que \( \lambda\) est le maximum, nous avons tout de suite \( \Phi_1(x)\geq 0\) tant que \( \| x \|=1\). Et si \( x\) n'est pas de norme \( 1\), c'est le même prix parce qu'on se ramène à \( \| x \|=1\) en multipliant par un nombre positif. Attention cependant :
    \begin{equation}
        \Phi_1(x_0,x_0)=\lambda\| x_0 \|^2-\phi(x_0)=0.
    \end{equation}
    Donc \( \Phi_1\) a un noyau contenant \( x_0\) par la proposition~\ref{PropHIWjdMX}. Nous en déduisons que \( \Image(g)\neq E\) en effet, \( x_0\in\Image(g)^{\perp}\), mais nous avons la proposition~\ref{PropXrTDIi} sur les dimensions :
    \begin{equation}
        \dim E=\dim(\Image(g))+\dim( \Image(g)^{\perp}).
    \end{equation}
    Vu que \( \Image(g)^{\perp}\) est un espace vectoriel non réduit à \( \{ 0 \}\), la dimension de \( \Image(g)\) ne peut pas être celle de \( E\). L'endomorphisme \( g\) n'étant pas surjectif, il ne peut pas être injectif non plus parce que nous sommes en dimension finie; il existe donc \( e_1\in E\) tel que \( g(e_1)=0\) et tant qu'à faire nous choisissons \( \| e_1 \|=1\) (ici la norme est bien celle de l'espace euclidien considéré). Par définition,
    \begin{equation}
        f(e_1)=\lambda e_1,
    \end{equation}
    c'est-à-dire que \( \lambda\in\Spec(f)\). Et \( \phi\) étant une forme quadratique réelle nous avons \( \lambda\in \eR\).

    Nous posons à présent \( H=\Span\{ e_1 \}^{\perp}\). C'est un sous-espace stable par \( f\) parce que si \( x\in H\) alors
    \begin{equation}
        \langle e_1, f(x)\rangle =\langle f(e_1j),x\rangle =\lambda\langle e_1, x\rangle =0.
    \end{equation}
    Nous pouvons donc considérer la restriction de \( f\) à \( H\) : \( f_H\colon H\to H\). Cet endomorphisme est bilinéaire et symétrique sur l'espace \( H\) de dimension inférieure à celle de \( E\), donc la récurrence nous donne une base orthonormée
    \begin{equation}
        \{ e_2,\ldots, e_n \}
    \end{equation}
    de vecteurs propres de \( f_H\). De plus les valeurs propres sont réelles, toujours par récurrence. Donc
    \begin{equation}
        \Spec(f)=\{ \lambda \}\cup\Spec(f_H)\subset \eR.
    \end{equation}
    Notons pour être complet que si \( i\geq 2\) alors
    \begin{equation}
        \langle e_1, e_i\rangle =0
    \end{equation}
    parce que le vecteur \( e_i\) est par construction choisi dans l'espace \( H=e_1^{\perp}\). Nous avons donc bien une base orthonormée de \( E\) construite sur des vecteurs propres de \( f\).
\end{proof}

\begin{corollary}   \label{CorSMHpoVK}
    Soit \( E\) un espace vectoriel ainsi que \( \phi\) et \( \psi\) des formes quadratiques sur \( E\) avec \( \psi\) définie positive. Alors il existe une base \( \psi\)-orthonormale dans laquelle \( \phi\) est diagonale.
\end{corollary}

\begin{proof}
    Il suffit de considérer l'espace euclidien \( E\) muni du produit scalaire \( \langle x, y\rangle =\psi(x,y)\). Ensuite nous diagonalisons la matrice (symétrique) de \( \phi\) pour ce produit scalaire à l'aide du théorème~\ref{ThoRSBahHH}.
\end{proof}

\begin{definition}      \label{DefYNWUFc}
    Dans le cas de \( V=\eR^m\) nous avons un produit scalaire canonique. Soient $u$ et $v$, deux vecteurs de $\eR^m$. Le \defe{produit scalaire}{produit!scalaire!sur \( \eR^n\)} de $u$ et $v$, noté $\langle u, v\rangle $ ou $u\cdot v$ est le réel
	\begin{equation}		\label{EqDefProdScalsumii}
		\langle u, v\rangle =\sum_{k=1}^m u_kv_k=u_1v_1+u_2v_2+\cdots+u_mv_n.
	\end{equation}
\end{definition}

Calculons par exemple le produit scalaire de deux vecteurs de la base canonique : $\langle e_i, e_j\rangle $. En utilisant la formule de définition et le fait que $(e_i)_k=\delta_{ik}$, nous avons
\begin{equation}
	\langle e_i, e_j\rangle =\sum_{k=1}^m\delta_{ik}\delta_{jk}.
\end{equation}
Nous pouvons effectuer la somme sur $k$ en remarquant qu'à cause du $\delta_{ik}$, seul le terme avec $k=i$ n'est pas nul. Effectuer la somme revient donc à remplacer tous les $k$ par des $i$ :
\begin{equation}
	\langle e_i, e_j\rangle =\delta_{ii}\delta_{ji}=\delta_{ji}.
\end{equation}

Une des propriétés intéressantes du produit scalaire est qu'il permet de décomposer un vecteur dans une base, comme nous le montre la proposition suivante.

\begin{proposition}		\label{PropScalCompDec}
	Si nous notons $v_i$ les composantes du vecteur $v$, c'est-à-dire si $v=\sum_{i=1}^m v_ie_i$, alors nous avons $v_j=\langle v, e_j\rangle $.
\end{proposition}

\begin{proof}
	\begin{equation}		\label{Eqvejscalcomp}
		v\cdot e_j=\sum_{i=1}^m\langle v_ie_i, e_j\rangle =\sum_{i=1}^mv_i\langle e_i, e_j\rangle =\sum_{i=1}^mv_i\delta_{ij}
	\end{equation}
	En effectuant la somme sur $i$ dans le membre de droite de l'équation \eqref{Eqvejscalcomp}, tous les termes sont nuls sauf celui où $i=j$; il reste donc
	\begin{equation}
		v\cdot e_j=v_j.
	\end{equation}
\end{proof}

Le produit scalaire ne dépend en réalité pas de la base orthogonale choisie.

\begin{lemma}
	Si $\{ e_i \}$ est la base canonique, et si $\{ f_i \}$ est une autre base orthonormale, alors si $u$ et $v$ sont deux vecteurs de $\eR^m$, nous avons
	\begin{equation}
		\sum_i u_iv_j=\sum_iu'_iv'_j
	\end{equation}
	où $u_i$ sont les composantes de $u$ dans la base $\{ e_i \}$ et $u'_i$ sont celles dans la base $\{ f_i \}$.
\end{lemma}

\begin{proof}
	La preuve demande un peu d'algèbre linéaire. Étant donné que $\{ f_i \}$ est une base orthonormale, il existe une matrice $A$ orthogonale ($AA^t=\mtu$) telle que $u'_i=\sum_jA_{ij}u_j$ et idem pour $v$. Nous avons alors
	\begin{equation}
		\begin{aligned}[]
			\sum_iu'_iv'_j&=\sum_i\left( \sum_jA_{ij} u_j\right)\left( \sum_k A_{ik}v_k \right)\\
			&=\sum_{ijk}A_{ij}A_{ik}u_jv_k\\
			&=\sum_{jk}\underbrace{\sum_i(A^t)_{ji}A_{ik}}_{=\delta_{jk}}u_jv_k\\
			&=\sum_{jk}\delta_{jk}u_jv_k\\
			&=\sum_ku_jv_k.
		\end{aligned}
	\end{equation}
\end{proof}

Cette proposition nous permet de réellement parler du produit scalaire entre deux vecteurs de façon intrinsèque sans nous soucier de la base dans laquelle nous regardons les vecteurs.

Nous dirons que deux vecteurs sont \defe{orthogonaux}{orthogonal} lorsque leur produit scalaire est nul. Nous écrivons que $u\perp v$ lorsque $\langle u, v\rangle =0$.
\begin{definition}	\label{DefNormeEucleApp}
    La \defe{norme euclidienne}{norme!euclidienne!dans $\eR^m$} d'un élément de $\eR^m$ est définie par $\| u \|=\sqrt{u\cdot u}$\footnote{La racine carré est définie en \ref{DEFooGQTYooORuvQb}.}.
\end{definition}

Cette définition est motivée par le fait que le produit scalaire $u\cdot u$ donne exactement la norme usuelle donnée par le théorème de Pythagore :
\begin{equation}
	u\cdot u=\sum_{i=1}^mu_iu_i=\sum_{i=1}^m u_i^2=u_1^2+u_2^2+\cdots+u_m^2.
\end{equation}

Le fait que $e_i\cdot e_j=\delta_{ij}$ signifie que la base canonique est \defe{orthonormée}{orthonormé}, c'est-à-dire que les vecteurs de la base canonique sont orthogonaux deux à deux et qu'ils ont tout $1$ comme norme.

\begin{lemma}\label{LemSclNormeXi}
	Pour tout $u\in\eR^m$, il existe un $\xi\in\eR^m$ tel que $\| u \|=\xi\cdot u$ et $\| \xi \|=1$.
\end{lemma}

\begin{proof}
	Vérifions que le vecteur $\xi=u/\| u \|$ ait les propriétés requises. D'abord $\| \xi \|=1$ parce que $u\cdot u=\| u \|^2$. Ensuite
	\begin{equation}
		\xi\cdot u=\frac{ u\cdot u }{ \| u \| }=\frac{ \| u \|^2 }{ \| u \| }=\| u \|.
	\end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Inégalité de Minkowski}
%---------------------------------------------------------------------------------------------------------------------------

Ce qui est couramment nommé «inégalité de Minkowski» est la proposition~\ref{PropInegMinkKUpRHg} dans les espaces \( L^p\). Nous allons en donner ici un cas très particulier.

\begin{proposition} \label{PropACHooLtsMUL}
    Si \( q\) est une forme quadratique sur \( \eR^n\) et si \( x,y\in \eR^n\) alors
    \begin{equation}
        \sqrt{q(x+y)}\leq\sqrt{q(x)}+\sqrt{q(y)}.
    \end{equation}
\end{proposition}

\begin{proof}
    La proposition~\ref{PropFWYooQXfcVY} nous permet de «diagonaliser» la forme quadratique \( q\). Quitte à ne plus avoir une base orthonormale, nous pouvons renormaliser les vecteurs de base pour avoir
    \begin{equation}
        q(x)=\sum_ix_i^2.
    \end{equation}
    Le résultat n'est donc rien d'autre que l'inégalité triangulaire pour la norme euclidienne usuelle, laquelle est démontrée dans la proposition~\ref{PropEQRooQXazLz}.
\end{proof}


\begin{normaltext}
    Un produit scalaire fourni donc toujours une norme et donc une topologie. Il ne faudrait cependant pas croire que toute norme dérive d'un produit scalaire, même pas en dimension finie. Et ce, malgré l'équivalence de toutes les normes du théorème~\ref{ThoNormesEquiv} dont vous avez déjà peut-être entendu parler.
\end{normaltext}


L'intérêt du lemme suivant sera apparent en \ref{NORMooNKBCooKziIjx}.
\begin{lemma}   \label{LEMooRWJYooOIJkZc}
    Sur \( \eR^2\), l'application \( N(x,y)=| x |+| y |\) est une norme\footnote{Définition \ref{DefNorme}.} qui ne dérive pas d'un produit scalaire\footnote{La norme d'un produit scalaire est la proposition  \ref{PropEQRooQXazLz}.}.
\end{lemma}

\begin{proof}
    Nous commençons par montrer que \( N\) est une norme. Il faut vérifier les trois conditions de la définition \ref{DefNorme}.
    \begin{enumerate}
        \item
            Il faut utiliser le lemme \ref{LemooANTJooYxQZDw}\ref{ItemooNVDIooSuiSoB} dans les deux sens. Si \( (x,y)=(0,0)\), alors évidemment \( N(x,y)=0\). Dans l'autre sens, si \( N(x,y)=0\) nous avons
            \begin{equation}
                0=| x |+| y |\geq | x |.
            \end{equation}
            Donc \( | x |\leq 0\), mais comme \( | x |\geq 0\), nous avons \( | x |=0\) et donc \( x=0\). Le même raisonnement tient pour \( y\).
        \item
            En tenant compte du fait que \( | \lambda x |=| \lambda | |x |\), nous avons
            \begin{equation}
                N\big( \lambda(x,y) \big)=N(\lambda x,\lambda y)=| \lambda | |x |+| \lambda | |y |=| \lambda |(| x |+| y |)=| \lambda |N(x,y).
            \end{equation}
        \item
            Nous avons le calcul
            \begin{subequations}
                \begin{align}
                    N\big( (x,y)+(a,b) \big)&=N(x+a,y+b)\\
                    &=| x+a |+| y+b |\\
                    &\leq | x |+| a |+| y |+| b |       \label{SUBEQooIXKWooTNQFnu}\\
                    &=N(x,y)+N(a,b)
                \end{align}
            \end{subequations}
            Justification : pour \eqref{SUBEQooIXKWooTNQFnu} nous avons utilité \( | a+b |\leq | a |+| b |\), du lemme \ref{LemooANTJooYxQZDw}.
    \end{enumerate}
    Pour voir qu'elle ne dérive pas d'un produit scalaire, nous montrons qu'elle ne vérifie pas l'identité du parallélogramme de la proposition \ref{PropEQRooQXazLz}.

    Voici un petit bout de code qui nous permet de ne pas faire de recherches à la main :
    \lstinputlisting{tex/sage/sageSnip018.sage}

    Il est vite vu qu'avec \( v=(-1,1)\) et \( w=(1,1)\), l'identité du parallélogramme n'est pas vérifiée.
\end{proof}

\begin{lemma}[\cite{KXjFWKA}]   \label{LemLPOHUme}
    Soit \( V\) un espace vectoriel muni d'un produit scalaire et de la norme associée. Si \( x,y\in V\) satisfont à \( \| x+y \|=\| x \|+\| y \|\), alors il existe \( \lambda\geq 0\) tel que \( x=\lambda y\).
\end{lemma}

\begin{proof}
    Quitte à raisonner avec \( x/\| x \|\) et \( y/\| y \|\), nous supposons que \( \| x \|=\| y \|=1\). Dans ce cas l'hypothèse signifie que \( \| x+y \|^2=4\). D'autre part en écrivant la norme en termes de produit scalaire,
    \begin{equation}
        \| x+y \|^2=\| x \|^2+\| y \|^2+2\langle x, y\rangle ,
    \end{equation}
    ce qui nous mène à affirmer que \( \langle x, y\rangle =1=\| x \|\| y \|\). Nous sommes donc dans le cas d'égalité de l'inégalité de Cauchy-Schwarz\footnote{Théorème~\ref{ThoAYfEHG}.}, ce qui nous donne un \( \lambda\) tel que \( x=\lambda y\). Étant donné que \( \| x \|=\| y \|=1\) nous avons obligatoirement \( \lambda=\pm 1\), mais si \( \lambda=-1\) alors \( \langle x, y\rangle =-1\), ce qui est le contraire de ce qu'on a prétendu plus haut. Par souci de cohérence, nous allons donc croire que \( \lambda=1\).
\end{proof}

\begin{proposition}[\cite{BIBooFJROooLckBUN}]			\label{PropVectsOrthLibres}
	si $v_1,\cdots,v_k$ sont des vecteurs non nuls, orthogonaux deux à deux, alors ces vecteurs forment une famille libre.
\end{proposition}

\begin{proof}
    Soit une compbinaison linéaire nulle des \( v_i\) : \( \sum_{i=1}^k\lambda_iv_i=0\). Nous multiplions scalairement par \( v_k\) :
    \begin{equation}
        0=\sum_i\lambda_iv_k\cdot v_i=\sum_i\lambda_i\delta_{ki}\| v_i \|^2=\lambda_k\| v_k \|^2.
    \end{equation}
    Donc \( \lambda_k=0\).
\end{proof}

\begin{lemma}       \label{LEMooYXJZooWKRFRu}
    Une isométrie d'un espace euclidien fixe l'origine.
\end{lemma}

\begin{proof}
    Soit une isométrie \( f\) d'un espace euclidien : \( f(x)\cdot f(y)=x\cdot y\) pour tout \( x,y\in E\). En particulier pour \( x=0\) nous avons
    \begin{equation}
        f(0)\cdot f(y)=0
    \end{equation}
    pour tout \( y\). Vu que \( f\) est une bijection, nous avons \( f(0)\cdot x=0\) pour tout \( x\). Comme le produit scalaire est non dégénéré cela implique que \( f(0)=0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Cauchy-Schwarz etc. cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Inégalité de Cauchy-Schwarz, cas complexe\cite{HilbertLi}]      \label{THOooSUCBooFnpkaF}
     Soit un espace vectoriel complexe muni d'un produit hermitien \( \langle ., .\rangle \). Alors pour tout vecteurs \( x,y\) nous avons
     \begin{equation}
         | \langle x, y\rangle  |\leq \| x \|\| y \|
     \end{equation}
     où nous avons posé \( \| x \|=\sqrt{ \langle x, x\rangle  }\).
\end{theorem}

\begin{proof}
    Si \( \langle x, y\rangle =0\), le résultat est évident; nous supposons que non. Nous posons
    \begin{equation}
        \theta=\frac{ \langle x, y\rangle  }{ | \langle x, y\rangle  | }.
    \end{equation}
    C'est un élément de \( \eC\) de norme \( 1\). Nous avons
    \begin{equation}
        \langle \frac{1}{ \theta }x, y\rangle =\frac{ | \langle x, y\rangle  | }{ \langle x, y\rangle  }\langle x, y\rangle =| \langle x, y\rangle  |\geq 0
    \end{equation}
    où le symbole «\( \geq\)» signifie «est réel et positif». Nous posons \( x'=\frac{1}{ \theta }x\) et nous considérons \( t\in \eR\). Remarquons que \( \| x' \|^2=\| x \|^2\) :
    \begin{equation}
        \| x' \|^2=\langle x', x'\rangle =\frac{1}{ \theta\bar\theta }\langle x, x\rangle =\| x \|^2
    \end{equation}
    parce que \( | \theta |=1\).

    En utilisant le fait que \( \langle a, b\rangle +\langle b, a\rangle =\real(\langle a, b\rangle )\) nous avons :
    \begin{subequations}
        \begin{align}
            0\leq \| x'+ty \|^2&=\| x' \|^2+t\langle x', y\rangle +t\langle y, x'\rangle +t^2\| y \|^2\\
            &=\| y \|^2t^2+2\real(\langle x', y\rangle )t+\| x' \|^2.
        \end{align}
    \end{subequations}
    Cela est un polynôme de degré \( 2\) en \( t\) qui n'est jamais strictement négatif. Autrement dit, il a au maximum une seule racine, ce qui signifie que son discriminant est négatif ou nul :
    \begin{equation}
        \real(\langle x', y\rangle )^2-\| y \|^2\| x' \|^2\leq 0.
    \end{equation}
    Mais nous avons choisi \( x'\) de telle sorte que \( \langle x', y\rangle =| \langle x, y\rangle  |\in \eR\) et \( \| x' \|^2=\| x \|^2\); nous avons donc
    \begin{equation}
        | \langle x, y\rangle  |^2\leq \| x \|^2\| y \|^2,
    \end{equation}
    comme il se devait.
\end{proof}

\begin{proposition}[Identité du parallélogramme\cite{BIBooXLLGooAFwpyU}]       \label{PROPooSSYJooHAXAnC}
    Soit une espace vectoriel complexe \( E\) muni d'un produit hermitien \( \langle ., .\rangle \). Nous posons \( \| x \|=\sqrt{ \langle x, x\rangle  }\). Nous avons
    \begin{enumerate}
        \item
            \( \| . \|\) est une norme.
        \item
            Elle vérifie l'identité du parallélogramme :
            \begin{equation}
                \| x+y \|^2+\| x-y \|^2=2\| x \|^2+2\| b \|^2
            \end{equation}
            pour tout \( x,y\in E\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    En ce qui concerne le fait que \( \| . \|\) soit une norme, tout est essentiellement dans la définition \ref{DefMZQxmQ} d'un produit hermitien. Voyons tout de même l'inégalité triangulaire. Nous avons :
    \begin{subequations}
        \begin{align}
            \| x+y \|^2&=\langle x+y, x+y\rangle\\
            &=\| x \|^2+\| y \|^2+\langle x, y\rangle +\langle y, x\rangle\\
            &=\| x \|^2+\| y \|^2+2\Re\big( \langle x, y\rangle  \big)\\
            &\leq\| x \|^2+\| y \|^2+2|\Re\big( \langle x, y\rangle  \big)|\\
            &\leq\| x \|^2+\| y \|^2+2| \langle x, y\rangle  |\\
            &\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|\label{SUBEQooQGQBooMRJcUc}\\
            &=\big( \| x \|+\| y \| \big)^2.
        \end{align}
    \end{subequations}
    Pour \eqref{SUBEQooQGQBooMRJcUc} nous avons utilisé Cauchy-Schwarz \ref{THOooSUCBooFnpkaF}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, ce qu'on a}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Théorème spectral hermitien]      \label{LEMooVCEOooIXnTpp}
    Nous considérons un espace vectoriel complexe hermitien.  Pour un opérateur hermitien\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.},
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres pour des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part
    \begin{equation}
        \langle Av, v\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

\begin{remark}      \label{REMooMLBCooTuKFmz}
    Un opérateur de la forme \( A^*A\) est évidemment hermitien. De plus ses valeurs propres sont toutes positives parce que si \( A^*Ax=\lambda v\) alors
    \begin{equation}
        0\leq \langle Av, Av\rangle =\langle A^*Av, v\rangle =\lambda\langle v, v\rangle .
    \end{equation}
    Donc \( \lambda\geq 0\).
\end{remark}

\begin{definition}  \label{DefWQNooKEeJzv}
    Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} si il commute avec son adjoint.
\end{definition}

Les opérateurs normaux comprennent évidemment les opérateurs hermitiens, mais également les anti-hermitiens, et ça c'est bien parce que c'est le cas de l'algèbre associée à \( \SU(2)\).

\begin{theorem}[Théorème spectral pour les matrices normales\footnote{Définition~\ref{DefWQNooKEeJzv}}\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver~\ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
    %TODO : le reste.

    Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme~\ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\cdots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

Tant que nous en sommes à parler de spectre de matrices hermitiennes\ldots Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition~\ref{DEFooKEBHooWwCKRK}.} et le théorème~\ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque~\ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{lemma}[\cite{ooLMMRooUXhOdx}]   \label{LEMooHUGEooVYhZdZ}
    Si \( A\) est une matrice carrée et inversible,
    \begin{equation}
        \Spec(A^*A)=\Spec(AA^*)
    \end{equation}
\end{lemma}

\begin{proof}
    Nous allons montrer l'égalité des polynômes caractéristiques. D'abord une simple multiplication montre que
    \begin{equation}
        (A^*A-\lambda\mtu)A^{-1}=A^{-1}(AA^*-\lambda\mtu).
    \end{equation}
    Nous prenons le déterminant de cette égalité en utilisant les propriétés~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} et~\ref{ITEMooZMVXooLGjvCy} :
    \begin{equation}
        \det(A^*A-\lambda\mtu)\det(A^{-1})=\det(A^{-1})\det(AA^*-\lambda\mtu).
    \end{equation}
    En simplifiant par \( \det(A^{-1})\) (qui est non nul parce que \( A\) est inversible) nous obtenons l'égalité des polynômes caractéristiques et donc l'égalité des spectres.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et orthogonalité}
%---------------------------------------------------------------------------------------------------------------------------

La définition du produit scalaire dans \( \eR^n\) est \ref{PROPooSKVRooDGVCYj} et le lien avec la matrice d'une application linéaire est la proposition \ref{PROPooZKWXooWmEzoA}.

\begin{remark}
    Outre l'orthogonalité, le produit scalaire permet de savoir l'angle entre deux vecteurs à travers la définition~\ref{DEFooSVDZooPWHwFQ}. D'autres interprétations géométriques du déterminant sont listées dans le thème~\ref{THMooUXJMooOroxbI}.
\end{remark}

Nous sommes maintenant en mesure de déterminer, pour deux vecteurs quelconques $u$ et $v$, la projection orthogonale de $u$ sur $v$. Ce sera le vecteur $\bar u$ parallèle à $v$ tel que $u-\bar u$ est orthogonal à $v$. Nous avons donc
\begin{equation}
    \bar u=\lambda v
\end{equation}
et
\begin{equation}
    (u-\lambda v)\cdot v=0.
\end{equation}
La seconde équation donne $u\cdot v-\lambda v\cdot v=0$, ce qui fournit $\lambda$ en fonction de $u$ et $v$ :
\begin{equation}
    \lambda=\frac{ u\cdot v }{ \| v \|^2 }.
\end{equation}
Nous avons par conséquent
\begin{equation}
    \bar u=\frac{ u\cdot v }{ \| v \|^2 }v.
\end{equation}
Armés de cette interprétation graphique du produit scalaire, nous comprenons pourquoi nous disons que deux vecteurs sont orthogonaux lorsque leur produit scalaire est nul.

Nous pouvons maintenant savoir quel est le coefficient directeur d'une droite orthogonale à une droite donnée. En effet, supposons que la première droite soit parallèle au vecteur $X$ et la seconde au vecteur $Y$. Les droites seront perpendiculaires si $X\cdot Y=0$, c'est-à-dire si
\begin{equation}
	\begin{pmatrix}
		x_1	\\
		y_1
	\end{pmatrix}\cdot\begin{pmatrix}
		y_1	\\
		y_2
	\end{pmatrix}=0.
\end{equation}
Cette équation se développe en
\begin{equation}		\label{Eqxuyukljsca}
	x_1y_1=-x_2y_2.
\end{equation}
Le coefficient directeur de la première droite est $\frac{ x_2 }{ x_1 }$. Isolons cette quantité dans l'équation \eqref{Eqxuyukljsca} :
\begin{equation}
	\frac{ x_2 }{ x_1 }=-\frac{ y_1 }{ y_2 }.
\end{equation}
Donc le coefficient directeur de la première est l'inverse et l'opposé du coefficient directeur de la seconde.

\begin{example}
	Soit la droite $d\equiv y=2x+3$. Le coefficient directeur de cette droite est $2$. Donc le coefficient directeur d'une droite perpendiculaires doit être $-\frac{ 1 }{ 2 }$.
\end{example}

\begin{proof}[Preuve alternative]
	La preuve peut également être donnée en ne faisant pas référence au produit scalaire. Il suffit d'écrire toutes les quantités en termes des coordonnées de $X$ et $Y$. Si nous posons
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x_1	\\
				x_2	\\
				x_2
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				y_1	\\
				y_2	\\
				y_3
			\end{pmatrix},
		\end{aligned}
	\end{equation}
	l'inégalité à prouver devient
	\begin{equation}
		(x_1y_1+x_2y_2+x_3y_3)^2\leq (x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2).
	\end{equation}
	Nous considérons la fonction
	\begin{equation}
		\varphi(t)=(x_1+ty_1)^2+(x_2+ty_2)^2+(x_3+ty_3)^2
	\end{equation}
	En tant que norme, cette fonction est évidemment positive pour tout $t$. En regroupant les termes de chaque puissance de $t$, nous avons
	\begin{equation}
		\varphi(t)=(y_1^2+y_2^2+y_3^2)t^2+2(x_1y_1+x_2y_2+x_3y_3)t+(x_1^2+x_2^2+x_3^2).
	\end{equation}
    Cela est un polynôme du second degré en $t$. Par conséquent le discriminant doit être négatif\footnote{Proposition \ref{PROPooEZIKooKjJroH}.}. Nous avons donc
	\begin{equation}
		4(x_1y_1+x_2y_2+x_3y_3)^2-(x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2)\leq 0.
	\end{equation}
	La thèse en découle aussitôt.
\end{proof}

\begin{proposition}     \label{PROPooVSVMooZrqxdc}
	La norme euclidienne a les propriétés suivantes :
	\begin{enumerate}
		\item
			Pour tout vecteur $X$ et réel $\lambda$,  $\| \lambda X \|=| \lambda |\| X \|$. Attention à ne pas oublier la valeur absolue !
		\item
			Pour tout vecteurs $X$ et $Y$, $\| X+Y \|\leq \| X \|+\| Y \|$.
	\end{enumerate}
\end{proposition}

\begin{proof}
    Pour le second point, nous avons les inégalités suivantes :
	\begin{subequations}
		\begin{align}
			\| X+Y \|^2&=\| X \|^2+\| Y \|^2+2X\cdot Y\\
			&\leq\| X \|^2+\| Y \|^2+2|X\cdot Y|\\
			&\leq\| X \|^2+\| Y \|^2+2\| X \|\| Y \|\\
			&=\big( \| X \|+\| Y \| \big)^2
		\end{align}
	\end{subequations}
    Nous avons utilisé d'abord la majoration $| x |\geq x$ qui est évidente pour tout nombre $x$; et ensuite l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Pythagore}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons donner une preuve du théorème de Pythagore.

\begin{theorem}[Pythagone\cite{MonCerveau}]     \label{THOooHXHWooCpcDan}
    Soient un espace euclidien\footnote{Définition \ref{DefLZMcvfj}.} \( E\) ainsi que trois points \( a,b,c\in E\) formant un triangle rectangle en \( a\), c'est à dire tel que
    \begin{equation}        \label{EQooRAWAooBxlBcZ}
        (b-a)\cdot (a-c)=0
    \end{equation}
    Alors
    \begin{equation}
        \| b-c \|^2=\| b-a \|^2+\| a-c \|^2.
    \end{equation}
\end{theorem}

\begin{proof}
    D'abord pour développons l'hypothèse \eqref{EQooRAWAooBxlBcZ}:
    \begin{equation}
        b\cdot a-b\cdot c-\| a \|^2+a\cdot c=0,
    \end{equation}
    et nous isolons un bout qui va nous servir plus tard:
    \begin{equation}        \label{EQooWPWZooLjlVJk}
        b\cdot a+a\cdot c=b\cdot c+\| a \|^2.
    \end{equation}
    
    Maintenant nous calculons un peu :
    \begin{subequations}
        \begin{align}
            \| b-a \|^2+\| a-c \|^2&=\| b \|^2-2b\cdot a+\| a \|^2+\| a \|-2a\cdot c+\| c \|^2\\
            &=2\| a \|^2+\| b \|^2+\| c \|^2-2(b\cdot c+\| a \|^2)      \label{SUBEQooHCWXooQHpGTO}\\
            &=\| b \|^2+\| c \|^2-2b\cdot c\\
            &=\| b-c \|^2.
        \end{align}
    \end{subequations}
    Pour \eqref{SUBEQooHCWXooQHpGTO}, nous avons substitué \eqref{EQooWPWZooLjlVJk}.
\end{proof}

\begin{normaltext}
    Je profite de l'occasion pour montrer mon scepticisme quant aux preuves de Pythagore basées sur différents pliages et découpages des carrés construits sur les côtés du triangle.

    Si, comme ici, nous considérons la géométrie dans \( \eR^2\) muni de son produit scalaire, alors le théorème \ref{THOooHXHWooCpcDan} est le théorème de Pythagore et il n'est pas loin d'être la définition de la distance entre deux points. Ce serait exactement la définition pour le triangle \( A=(0,0)\), \( B=(a,0)\), \( C=(a,b)\).

    Pour autant que je le sache, la géométrie dans «le plan» (celle du collège) ne définit pas «longueur» et «aire». Donc bon \ldots Il y a peut-être un moyen de s'en sortir, mais je ne le connais pas.

    Bref, soit on se met d'accord sur les définition (et dans ce cas je serais étonné qu'il existe une démonstration de Pythagore très différente de ce qu'on a ici), soit il faudrait se calmer avec les soit-disant preuves du théorème de Pythagore.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooTNTNooRjhuJZ}
	Soient $u$ et $v$, deux vecteurs de $\eR^3$. Le \defe{produit vectoriel}{produit!vectoriel} de $u$ et $v$ est le vecteur $u\times v$ défini par
    \begin{equation}        \label{EQooCUJRooFuFPaZ}
		u\times v=\det\begin{pmatrix}
			e_1	&	e_2	&	e_3	\\
			u_1	&	u_2	&	u_3	\\
			v_1	&	v_2	&	v_3
		\end{pmatrix}
    \end{equation}
	où les vecteurs $e_1$, $e_2$ et $e_3$ sont les vecteurs de la base canonique de $\eR^3$.
\end{definition}

\begin{lemma}
    Le produit vectoriel \( u\times v\) est également exprimé par
    \begin{subequations}        \label{EQSooOWGZooNYruoy}
        \begin{align}
            u\times v&=(u_2v_3-u_3v_2)e_1+(u_3v_1-u_1v_3)e_2+(u_1v_2-u_2v_1)e_3     \label{SEBEQooVROKooRpUOIr}\\
                &=\sum_{i,j,k}\epsilon_{ijk}v_iw_je_k
        \end{align}
    \end{subequations}
    où $\epsilon_{ijk}$ est défini par $\epsilon_{xyz}=1$ et ensuite $\epsilon_{ijk}$ est $1$ ou $-1$ suivant que la permutation des $x$, $y$ et $z$ est paire ou impaire. C'est-à-dire que \( \epsilon_{ijk}\) est la signature de la permutation qui amène \( (1,2,3)\) sur \( (i,j,k)\).
\end{lemma}

\begin{proof}
    Il s'agit seulement de développer explicitement le déterminant \eqref{EQooCUJRooFuFPaZ}.
\end{proof}

\begin{normaltext}
    Mettons que \( a\times b=v\). En calculant le même produit vectoriel dans la base \( f_i=-e_i\), les composantes de \( a\) et \( b\) changent de signe et la formule \eqref{EQSooOWGZooNYruoy} dit que le produit vectoriel ne change pas. On serait tenté d'écrire, dans la base \( \{ f_i \}\)
    \begin{equation}
        (-a)\times (-b)=v,
    \end{equation}
    tout en pleurant parce que dans la base des \( f_i\), le vecteur \( v\) devient \( -v\).

    Il a des personnes que cela tracasse tellement qu'on entend parler de «le produit vectoriel est une pseudo-vecteur sous \( \SO(2)\)». Les physiciens en théorie quantique des champs --pourtant la plus plaisante des matières-- sont terribles sur ce sujet.

    Il suffit d'être clair. Le produit vectoriel n'est défini que sur \( \eR^3\), et est définit par sa formule dans la base canonique, point barre. Si vous avez des vecteurs \( a\) et \( b\) dont vous connaissez les composantes dans une autre base, vous devez calculer les composantes dans la base canonique, utiliser la formule pour trouver les composantes de \( a\times b\) dans la base canonique. Ensuite, si ça vous chante, vous pouvez calculer à nouveau les composantes de \( a\times b\) dans une autre base.

    Tout cela pour dire que le produit vectoriel n'est pas une opération très généralisable. Il est possible, pour sembler plus intrinsèque, de tenter cette définition : le produit vectoriel \( a\times b\) est le vecteur perpendiculaire à \( a\) et \( b\), de longueur égale à l'aire du parallélogramme construit sur \( a\) et \( b\).

    Cette «définition» a plusieurs inconvénients.
    \begin{itemize}
        \item Elle demande quand même un produit scalaire et des aires; bref, elle demande une structure métrique,
        \item Elle ne donne pas le sens. En effet, dans \( \eR^3\), il y a deux vecteurs de longueur donnée perpendiculaires à \( a\) et \( b\). Il faut donc préciser le sens. Cela revient à donner une orientation et donc, fondamentalement, à choisir une base.
    \end{itemize}
    
    Bref, on retiendra que le produit vectoriel est une opération accrochée à \( \eR^3\) et à sa base canonique.
\end{normaltext}

\begin{lemmaDef}
    Nous avons l'égalité suivante pour tout \( u,v,w\in \eR^3\) :
    \begin{equation}        \label{EQooKJYUooSQgfXU}
        (u\times v)\cdot w=\det\begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix}.
    \end{equation}
    Le résultat est nommé le \defe{produit mixte}{produit!mixte} de trois vecteurs de \( \eR^3\).
\end{lemmaDef}

\begin{normaltext}
    Nous avons donné un nom à la combinaison \( (u\times v)\cdot w\). J'imagine que vous voyez pourquoi nous ne considérons pas la combinaison $(u\cdot v)\times w$.
\end{normaltext}

Le lemme suivant donne un moyen compliqué et peu pratique de calculer la valeur absolue du produit mixte. La formule \eqref{EQooWZUQooYydphW} ne sera utilisée que pour faire le lien entre un jacobien et un élément de volume en dimension trois lorsque nous verrons les intégrales sur des variétés. Voir l'équation \eqref{EQooYIJSooHtkXfu}. 

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooSMWNooCmEZeY}
    Le produit mixte peut également être exprimé par
    \begin{equation}        \label{EQooWZUQooYydphW}
           |(u\times v)\cdot w|^2=\det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
    Si nous notons 
    \begin{equation}
        a= \begin{pmatrix}
                u_1	&	u_2	&	u_3	\\
                v_1	&	v_2	&	v_3	\\
                w_1	&	w_2	&	w_3
        \end{pmatrix},
    \end{equation}
    il faut simplement remarquer que
    \begin{equation}
           \begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=aa^t.
    \end{equation}
    Donc au niveau des déterminants, en utilisant les propositions \ref{PROPooHQNPooIfPEDH} et le lemme \ref{LEMooCEQYooYAbctZ} nous avons
    \begin{equation}
           \det\begin{pmatrix}
            \| u \|^2    &   u\cdot v    &   u\cdot w    \\
            v\cdot u    &   \| v \|^2    &   v\cdot w    \\
            w\cdot u    &   w\cdot v    &   \| w \|^2
        \end{pmatrix}=\det(aa^t)=\det(a)\det(a^t)=\det(a)^2.
    \end{equation}
    Et maintenant, par définition, \( \det(a)=(u\times w)\cdot w\). Donc le résultat annoncé.
\end{proof}

\begin{proposition}		 \label{PropScalMixtLin}
	Les applications produit scalaire, vectoriel et mixte sont multilinéaires. Spécifiquement, nous avons les propriétés suivantes.
	\begin{enumerate}
		\item
			Les applications produit scalaire et vectoriel sont bilinéaires. C'est-à-dire que pour tout vecteurs $a$, $b$, $c$ et pour tout nombre $\alpha$ et $\beta$ nous avons
    \begin{equation}
        \begin{aligned}[]
            a\times (\alpha b +\beta c)&=\alpha(a\times b)+\beta(a\times c)\\
            (\alpha a+\beta b)\times c&=\alpha(a\times c)+\beta(b\times c).
        \end{aligned}
    \end{equation}

        \item
            Le produit mixte est trilinéaire.
		\item
			Le produit vectoriel est antisymétrique, c'est-à-dire $u\times v=-v\times u$.
		\item
			Nous avons $u\times v=0$ si et seulement si $u$ et $v$ sont colinéaires, c'est-à-dire si et seulement si l'équation $\alpha u+\beta v=0$ a une solution différente de la solution triviale $(\alpha,\beta)=(0,0)$.
		\end{enumerate}
\end{proposition}

\begin{proposition}[Identité de Lagrange\cite{ooHFUZooGakvHi}]     \label{PROPooMXAIooJureOD}
    Si \( x,y\in \eR^n\), alors
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
    \end{equation}
    Et si \( n=3\) alors
    \begin{equation}
        \| x\times y \|=\| y \|^2\| y \|^2-(x\cdot y)^2.
    \end{equation}
\end{proposition}

\begin{proof}
    C'est un calcul. D'abord nous avons
    \begin{equation}
        \| x \|^2\| y \|^2-(x\cdot y)^2=\sum_ix_i^2\sum_jy_j^2-\big( \sum_k x_ky_k  \big)^2=\sum_{ij}x_i^2y_j^2-\sum_{kl}x_ky_kx_ly_l.
    \end{equation}
    Ensuite nous coupons les sommes de la façon suivante
    \begin{equation}
        \sum_{ij}=\sum_j\sum_{i<j}+\sum_j(i=j)+\sum_j\sum_{i>j}
    \end{equation}
    pour obtenir
    \begin{equation}
        \begin{aligned}[]
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\sum_{i<j}x_i^2y_j^2+\sum_jx_j^2y_j^2+\sum_j\sum_{i>j}x_i^2y_j^2\\
                &\quad-\sum_l\sum_{k<l}x_ky_kx_ly_l-\sum_kx_k^2y_k^2-\sum_l\sum_{k>l}x_ky_kx_ly_l.
        \end{aligned}
    \end{equation}
    Il y a deux termes qui se simplifient. Notez que si \( A_{kl}\) est symétrique en \( kl\) nous avons
    \begin{equation}
        \sum_l\sum_{k<l}A_{kl}=\sum_k\sum_{l<k}A_{lk}=\sum_k\sum_{l<k}A_{kl}.
    \end{equation}
    La première égalité était seulement un renommage des indices. Le coup des indices symétriques est justement ce qu'il se passe dans les deux termes en\( x_ky_kx_ly_l\), donc nous les regroupons :
    \begin{subequations}
        \begin{align}
            \| x \|^2\| y \|^2-(x\cdot y)^2&=\sum_j\big( \sum_{i<j}x_i^2x_j^2+\sum_{i>j}x_i^2y_j^2-2\sum_{i>j}x_iy_ix_jy_j \big)\\
            &=\sum_j\sum_{i<j}(x_i^2y_j^2+x_j^2y_i^2-2x_iy_ix_jy_j)\\
            &=\sum_j\sum_{i<j}(x_iy_j-x_jy_i)^2.
        \end{align}
    \end{subequations}
    Voila qui prouve la première formule. Pour la seconde, il faut seulement poser \( n=3\) et écrire les sommes explicitement.

    \begin{itemize}
        \item 
    Pour \( j=1\), la somme sur \( i\) est \( \sum_{i<1}\), c'est-à-dire aucun termes.
\item
    Pour \( j=2\), il y a seulement \( i=1\), donc le terme \( (x_1y_2-x_2y_1)^2\).

\item
    Pour \( j=3\), il y a les termes \( i=1\) et \( i=2\), donc les termes \( (x_1y_3-x_3y_1)^2+(x_2y_3-x_3y_2)^2\).
    \end{itemize}
    Ces trois termes collectés sont justement les composants (au carré) de \( x\times y\) données dans la formule \eqref{SEBEQooVROKooRpUOIr}.
\end{proof}

Les trois vecteurs de base $e_x$, $e_y$ et $e_y$ ont des produits vectoriels faciles à retenir :
\begin{equation}
    \begin{aligned}[]
        e_x\times e_y&=e_z\\
        e_y\times e_z&=e_x\\
        e_z\times e_x&=e_y
    \end{aligned}
\end{equation}

Les deux formules suivantes, qui mêlent le produit scalaire et le produit vectoriel, sont souvent utiles en analyse vectorielle :
\begin{equation}
	\begin{aligned}[]
		(u\times v)\cdot w&=u\cdot(v\times w)\\
		(u\times v)\times w&=-(v\cdot w)u+(u\cdot w)v		\label{EqFormExpluxxx}
	\end{aligned}
\end{equation}
pour tout vecteurs $u$, $v$ et $w$ dans $\eR^3$. Nous les admettons sans démonstration. La seconde formule est parfois appelée \defe{formule d'expulsion}{formule!d'expulsion (produit vectoriel)}.

\begin{example}
    Calculons le produit vectoriel $v\times w$ avec
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                3    \\
                -1    \\
                1
            \end{pmatrix}&w=\begin{pmatrix}
                1    \\
                2    \\
                -1
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Les vecteurs s'écrivent sous la forme $v=3e_x-e_y+e_z$ et $w=e_x+2e_y-e_z$. Le produit vectoriel s'écrit
    \begin{equation}
        \begin{aligned}[]
            (3e_x-e_y+e_z)\times (e_x+2e_y-e_z)&=6e_x\times e_y-3e_x\times e_z\\
                                &\quad -e_y\times e_x + e_y\times e_z\\
                                &\quad + e_z\times e_x + 2e_z\times e_y\\
                                &=6e_z+3e_y+e_z+e_x+e_y-2e_x\\
                                &=-e_x+4e_y+7e_z.
        \end{aligned}
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si $a$, $b$ et $c$ sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre $a\cdot(b\times c)$. En écrivant le produit vectoriel sous forme de somme de trois déterminants $2\times 2$, nous avons
\begin{equation}
    \begin{aligned}[]
        a\cdot& (b\times c)\\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
        \begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}e_x-\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}e_y+\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\right)\\
        &=a_1\begin{vmatrix}
            b_2    &   b_3    \\
            c_2    &   c_3
        \end{vmatrix}-a_2\begin{vmatrix}
            b_1    &   b_3    \\
            c_1    &   c_3
        \end{vmatrix}+a_3\begin{vmatrix}
            b_1    &   b_2    \\
            c_1    &   c_2
        \end{vmatrix}\\
        &=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3    \\
            c_1    &   c_2    &   c_3
        \end{vmatrix}.
    \end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
    a\cdot (b\times c)=\begin{vmatrix}
        a_1    &   a_2    &   a_3    \\
        b_1    &   b_2    &   b_3    \\
        c_1    &   c_2    &   c_3
    \end{vmatrix}.
\end{equation}

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}     \label{PROPooTUVKooOQXKKl}
    Le produit vectoriel\footnote{Définition \ref{DEFooTNTNooRjhuJZ}.} $a\times b$ est un vecteur orthogonal à $a$ et $b$.
\end{proposition}

\begin{proof}
    Vérifions que $a\perp (a\times b)$. Pour cela, nous calculons $a\cdot (a\times b)$, c'est-à-dire le produit mixte
    \begin{equation}
        a\cdot(a\times b)=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3
        \end{vmatrix}=0.
    \end{equation}
    L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

Ces résultats admettent une intéressante généralisation.
\begin{lemma}       \label{LEMooFRWKooVloCSM}
    Soit \( X\in \eR^n\) ainsi que \( v_1,\ldots, v_{n-1}\in \eR^n\). Alors
    \begin{enumerate}
        \item
            Nous avons
            \begin{equation}        \label{EQooMQNPooRHHBjz}
                \det(X,v_1,\ldots, v_{n-1})=X\cdot
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
        \item
            Le vecteur
            \begin{equation}
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
            \end{equation}
            est orthogonal à tous les \( v_i\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que les deux côtés de \eqref{EQooMQNPooRHHBjz} vus comme fonctions de \( X\), sont des applications linéaires de \( \eR^n\) dans \( \eR\), il suffit de vérifier l'égalité sur une base.

    Nous posons \( \tau_i\colon \eR^n\to \eR^{n-1}\),
    \begin{equation}
        \tau_i(v)_k=\begin{cases}
            v_k    &   \text{si } k<i\\
            v_{k+1}    &    \text{si } k\geq i\text{.}
        \end{cases}
    \end{equation}
    et nous avons d'une part
    \begin{equation}
        e_k\cdot
                \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =\det\begin{pmatrix}
                     \tau_kv_1   \\
                     \vdots   \\
                     \tau_kv_{n-1}
                 \end{pmatrix}
            \end{equation}
     et d'autre part,
     \begin{equation}
         \det(e_k,v_1,\ldots, v_{n-1})=\det
         \begin{pmatrix}
             0&&&\\
             \vdots&&&\\
             1&v_1&\cdots&v_{n-1}\\
             \vdots&&&\\
             0&&&
         \end{pmatrix}=\det(\tau_k v_1,\ldots, \tau_k v_{n-1}).
     \end{equation}
     La première assertion est démontrée.

     En ce qui concerne la seconde, il suffit d'appliquer la première et se souvenir qu'un déterminant est nul lorsque deux lignes sont égales\footnote{Corolaire \ref{CORooAZFCooSYINvBl}.}. En effet :
     \begin{equation}
         v_k\cdot \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\
                        &   v_{n-1}    &
                 \end{pmatrix}
                 =
                 \det(v_k,v_1,\ldots, v_n)=0.
     \end{equation}
\end{proof}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Procédé de Gram-Schmidt}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Procédé de Gram-Schmidt]    \label{PropUMtEqkb}
    Un espace euclidien possède une base orthonormée.
\end{proposition}
\index{espace!euclidien}
\index{Gram-Schmidt}

\begin{proof}
    Soit \( E\) un espace euclidien et \( \{ v_1,\ldots, v_n \}\), une base quelconque de \( E\). Nous posons d'abord
    \begin{equation}
        \begin{aligned}[]
            f_1&=v_1,&e_1&=\frac{ f_1 }{ \| f_1 \| }.
        \end{aligned}
    \end{equation}
    Ensuite
    \begin{equation}
        \begin{aligned}[]
            f_2&=v_2-\langle v_2, e_1\rangle e_1,&e_2&=\frac{ f_2 }{ \| f_2 \| }.
        \end{aligned}
    \end{equation}
    Notons que \( \{ e_1,e_2 \}\) est une base de \( \Span\{ v_1,v_2 \}\). De plus elle est orthogonale :
    \begin{equation}
        \langle e_1, f_2\rangle =\langle e_1, v_2\rangle -\langle v_2, e_1\rangle \underbrace{\langle e_1, e_1\rangle}_{=1} =0.
    \end{equation}
    Le fait que \( \| e_1 \|=\| e_2 \|=1\) est par construction. Nous avons donc donné une base orthonormée de \( \Span\{ v_1,v_2 \}\).

    Nous continuons par récurrence en posant
    \begin{equation}
        \begin{aligned}[]
            f_k&=v_k-\sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i,&e_k&=\frac{ f_k }{ \| f_k \| }.
        \end{aligned}
    \end{equation}
    Pour tout \( j<k\) nous avons
    \begin{equation}
        \langle e_j, f_k\rangle =\langle e_j, v_k\rangle -\sum_{i=1}^{k-1}\langle v_k, e_i\rangle \underbrace{\langle e_i, e_j\rangle}_{=\delta_{ij}} =0
    \end{equation}
\end{proof}
Cet algorithme de Gram-Schmidt nous donne non seulement l'existence de bases orthonormée pour tout espace euclidien, mais aussi le moyen d'en construire à partir de n'importe quelle base.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Pseudo-réduction simultanée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{corollary}[Pseudo-réduction simultanée\cite{JMYQgLO}]  \label{CorNHKnLVA}
    Soient \( A,B\in \gS(n,\eR)\) avec \( A\) définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.}. Alors il existe \( Q\in \GL(n,\eR)\) telle que \( Q^tBQ\) soit diagonale et \( Q^tAQ=\mtu\).
\end{corollary}

\begin{proof}
    Nous allons noter \( x\cdot y\) le produit scalaire usuel de \( \eR^n\) et \( \{ e_i \}_{i=1,\ldots, n}\) sa base canonique.

    Vu que \( A\) est définie positive, l'expression \( \langle x, y\rangle =x\cdot Ay\) donne un produit scalaire sur \( \eR^n\). Nous avons donc deux produits scalaires sur \( \eR^n\), et nous allons travailler avec les deux.
    
    La proposition \ref{PropUMtEqkb} appliquée à l'espace euclidien \( (\eR^n,\langle ., .\rangle )\) dit qu'il existe une base de \( \eR^n\) orthonormée \( (f_i )_{i=1,\ldots, n}\) pour ce produit scalaire. Nous considérons l'application linéaire \( P\) définie par
    \begin{equation}
        Pe_i=f_i.
    \end{equation}

    Nous démontrons à présent que \( P^tAP=\mtu\). Pour cela, nous calculons
    \begin{subequations}
        \begin{align}
            \delta_{ij}&=\langle f_i, f_j\rangle    \label{SUBEQooGZDJooVMuWNn} \\
            &=f_i\cdot Af_j\\
            &=Pe_i\cdot APe_j       \\
            &=e_i\cdot P^tAPe_j     \label{SUBEQooQNVUooNbyIzM}\\
            &=(P^tAP)_{ij}.     \label{SUBEQooITBKooCEmqxx}
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooGZDJooVMuWNn}, la base \( (f_j)\) est orthonormée pour le produit scalaire \( \langle ., .\rangle \).
        \item Pour \eqref{SUBEQooQNVUooNbyIzM}, la proposition \ref{PROPooNARVooEuhweD} sur la transposée.
        \item Pour \eqref{SUBEQooITBKooCEmqxx}, la formule du produit scalaire usuel pour avoir les éléments de matrice, proposition \ref{PROPooZKWXooWmEzoA}.
    \end{itemize}
    La matrice \( P^tBP\) est une matrice symétrique, donc le théorème spectral~\ref{ThoeTMXla} nous donne une matrice \( R\in \gO(n,\eR)\) telle que \( R^tP^tBPR\) soit diagonale. En posant maintenant \( Q=PR\) nous avons la matrice cherchée.
\end{proof}

\begin{remark}
    Plusieurs remarques
    \begin{enumerate}
        \item

            Nous n'avons pas prouvé l'existence d'une matrice \( P\) telle que \( P^{-1}BP\) et \( P^{-1}AP\) soient diagonales. Au contraire, nous avons \( Q^tBQ\) et \( Q^tAQ\) qui sont diagonales. Tant que \( Q\) n'est pas orthogonales, ce n'est pas la même chose.

            Autrement dit, nous n'avons pas ici une réelle diagonalisation, parce que les matrices \( A\) et \( B\) ne sont pas semblables à des matrices diagonales. Voir les définitions~\ref{DefCNJqsmo} (diagonalisable) et~\ref{DefCQNFooSDhDpB} (semblable).

            C'est pour cela que nous parlons de \emph{pseudo}-diagonalisation.

        \item

            Dans le même ordre d'idée, la démonstration de la pseudo-diagonalisation simultanée parle clairement de formes bilinéaires, et non d'endomorphismes. Or en comparant les lois de transformations \eqref{ooWKTYooOJfclT} et \eqref{EQooZUVTooKjqnJj}, nous voyons bien que la réduction en passant par \( Q^tAQ\) est bien une réduction de forme bilinéaire et non une réduction d'endomorphismes.

        \item

            Nous avons prouvé la pseudo-réduction simultanée comme corolaire du théorème de diagonalisation des matrices symétriques~\ref{ThoeTMXla}. Il aurait aussi pu être vu comme un corolaire du théorème spectral~\ref{ThoRSBahHH} sur les opérateurs autoadjoints via son corolaire~\ref{CorSMHpoVK}.
    \end{enumerate}
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Approximations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Le lemme suivant est surtout intéressant en dimension infinie.
\begin{lemma}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); il existe une suite \( (v_n)\) dans \( A\) telle que \( v_n\stackrel{V}{\longrightarrow}v\) et \( \| v_n \|\leq \| v \|\) pour tout \( n\).
\end{lemma}

\begin{proof}
    Vu que \( A\) est dense, il existe une suite \( a_n\) dans \( A\) telle que \( a_n\to v\). Ensuite il suffit de poser
    \begin{equation}
        v_n=\frac{ n }{ n+1 }\frac{ \| v \| }{ \| a_n \| }a_n.
    \end{equation}
    Par construction nous avons toujours
    \begin{equation}
        \| v_n \|=\frac{ n }{ n+1 }\| v \|\leq \| v \|.
    \end{equation}
    Et de plus, la norme étant continue\footnote{Où dans le calcul suivant nous utilisons la continuité de la norme ? Posez-vous la question.},
    \begin{equation}
        \lim_{n\to \infty} v_n=\lim_{n\to \infty} \frac{ n }{ n+1 }\lim_{n\to \infty} \frac{ \| v \| }{ \| v_n \| }\lim_{n\to \infty} v_n=v.
    \end{equation}

    Le fait que \( v_n\) soit dans \( A\) est dû au fait que \( A\) soit vectoriel.
\end{proof}

\begin{proposition}     \label{PROPooVEMGooYKhMFy}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); pour tout \( a\in \eR\) nous avons
    \begin{equation}
        \sup\{ | v\cdot a |\tq a\in A\text{ et }\| a \|\leq \lambda \}=\lambda\| v \|.
    \end{equation}
\end{proposition}

\begin{proof}
    D'abord pour tout \( a\in A\) vérifiant \( \| a \|\leq \lambda\) l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG} donne
    \begin{equation}
        | v\cdot a |\leq \| v \|\| a \|\leq \lambda\| v \|.
    \end{equation}
    Donc le supremum dont on parle est majoré par \( \lambda\| v \|\).

    Il nous faut l'inégalité dans l'autre sens. Par densité nous pouvons choisir une suite \( v_n\in A\) tel que \( v_n\to v\). Ensuite nous posons
    \begin{equation}
        a_n=\frac{ \lambda }{ \| v_n \| }v_n.
    \end{equation}
    Nous avons \( \| a_n \|=\lambda\) pour tout \( n\) et
    \begin{equation}
        | v\cdot a_n |=\frac{ \lambda }{ \| v_n \| }| v\cdot v_n |,
    \end{equation}
    et en passant à la limite,
    \begin{equation}
        \lim_{n\to \infty} | v\cdot a_n |=\frac{ \lambda }{ \| v \| }\| v\cdot v \|=\lambda\| v \|.
    \end{equation}
    Donc l'ensemble sur lequel nous prenons le supremum contient une suite convergente vers \( \lambda\| v \|\). Le supremum est donc au moins aussi grand que cela.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques exemples de normes sur \texorpdfstring{$\eR^n$}{Rn}}
%---------------------------------------------------------------------------------------------------------------------------

Il est possible de définir de nombreuses normes sur $\eR^n$. Citons-en quelques-unes.

\begin{propositionDef}      \label{PROPooCLZRooIRxCnZ}
    Les formules suivantes définissent des normes sur \( \eR^n\).
    \begin{enumerate}
        \item
    Les normes $\| . \|_{L^p}$ ($p\in\eN$) sont définies de la façon suivante :
    \begin{equation}		\label{EqDeformeLp}
        \| x \|_{L^p}=\Big( \sum_{i=1}^n| x_i |^p\Big)^{1/p},
    \end{equation}
    pour tout $x=(x_1,\ldots,x_n)\in\eR^n$.
\item
    La norme $L^2$ est la \defe{norme euclidienne}{norme!euclidienne}.
\item
    Nous définissons également la \defe{norme supremum}{norme!supremum} par
    \begin{equation}
	    \| x \|_{\infty}=\max_i| x_i |.
    \end{equation}
    \end{enumerate}
\end{propositionDef}

\begin{proof}
    Point par point\quext{Preuve non terminée}.
    \begin{enumerate}
        \item
            Le cas \( p=1\) est déjà fait dans le lemme \ref{LEMooRWJYooOIJkZc}.
        \item
    Le fait que \( x\mapsto\| x \|_{L^2}\) soit une norme provient de la propriété suivante :
    \begin{equation}
        \sqrt{ (a+b)^2 }\leq \sqrt{ a^2 }+\sqrt{ b^2 },
    \end{equation}
    laquelle se démontre en passant au carré :
    \begin{equation}        \label{EQooRYNYooTzZpPz}
        (a+b)^2=a^2+b^2+2ab\leq a^2+b^2+2| ab |=\big( \sqrt{ a^2 }+\sqrt{ b^2 } \big)^2.
    \end{equation}
\item
    \end{enumerate}
\end{proof}

Parmi ces normes, celles qui seront le plus souvent utilisées dans ces notes sont
\begin{equation}
	\begin{aligned}[]
		\| x \|_{L^1}&=\sum_{i=1}^n| x_i |,\\
		\| x \|_{L^2}&=\Big( \sum_{i=1}^n| x_i |^2 \Big)^{1/2}.
	\end{aligned}
\end{equation}

\newcommand{\CaptionFigDistanceEuclide}{La \emph{norme} euclidienne induit la \emph{distance} euclidienne. D'où son nom. Le point $C$ est construit aux coordonnées $(A_x,B_y)$.}
\input{auto/pictures_tex/Fig_DistanceEuclide.pstricks}

Soient $A=(A_x,A_y)$ et $B=(B_x,B_y)$ deux éléments de $\eR^2$. La distance\footnote{Ne pas confondre «distance» et «norme».} euclidienne entre $A$ et $B$ est donnée par $\| A-B \|_2$. En effet, sur la figure~\ref{LabelFigDistanceEuclide}, la distance entre les points $A$ et $B$ est donnée par
\begin{equation}
	| AB |^2=| AC |^2+| CB |^2=| A_x-B_x |^2+| A_y-B_y |^2,
\end{equation}
par conséquent,
\begin{equation}
	| AB |=\sqrt{| A_x-B_x |^2+| A_y-B_y |^2}=\| A-B \|_2.
\end{equation}

\begin{remark}
	Si $A$, $B$ et $C$ sont trois points dans le plan $\eR^2$, alors l'inégalité triangulaire $| AB |\leq| AC |+| CB |$ est précisément la propriété~\ref{ItemDefNormeiii} de la norme (définition~\ref{DefNorme}). En effet l'inégalité triangulaire s'exprime de la façon suivante en termes de la norme $\| . \|_2$ :
	\begin{equation}	\label{EqNDeuxAmBNNdd}
		\| A-B \|_2\leq \| A-C \|_2+\| C-B \|_2.
	\end{equation}
	En notant $u=A-C$ et $v=C-B$, l'équation \eqref{EqNDeuxAmBNNdd} devient exactement la propriété de définition de la norme :
	\begin{equation}
		\| u+v \|_2\leq \| u \|_2+\| v \|_2.
	\end{equation}
	Ceci explique pourquoi cette propriété des normes est appelée «inégalité triangulaire».
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équivalence des normes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{normes_equiv}

Au premier coup d'œil, les notions dont nous parlons dans ce chapitre ont l'air très générales. Nous prenons en effet n'importe quel espace vectoriel $V$ de dimension finie, et nous le munissons de n'importe quelle norme (rien que dans $\eR^m$ nous en avons défini une infinité par l'équation \eqref{EqDeformeLp}). À partir de ces données, nous définissons les boules, la topologie, l'adhérence, etc.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{En dimension finie}
%---------------------------------------------------------------------------------------------------------------------------

Dans $\eR^n$, les normes $\| . \|_{L^1}$, $\| . \|_{L^2}$ et $\| . \|_{\infty}$ ne sont pas égales. Cependant elles ne sont pas complètement indépendantes au sens où l'on sent bien que si un vecteur sera grand pour une norme, il sera également grand pour les autres normes; les normes «vont dans le même sens». Cette notion est précisée par le concept de norme équivalente.

\begin{definition}		\label{DefEquivNorm}
    Deux normes $N_1$ et $N_2$ sur $\eR^m$ sont \defe{\wikipedia{fr}{Norme_équivalente}{équivalentes}}{equivalence@équivalence!norme}\index{norme!équivalence}\index{équivalence!de norme} si il existe deux nombres réels strictement positifs $k_1$ et $k_2$ tels que
	\begin{equation}
		k_1N_1(x)\leq N_2(x)\leq k_2 N_1(x),
	\end{equation}
	pour tout $x$ dans $\eR^m$. Dans ce cas nous écrivons que $N_1\sim N_2$.
\end{definition}

\begin{lemma}       \label{LEMooHAITooWdtLAN}
    La définition de norme équivalentes donne une relation d'équivalence (définition~\ref{DefHoJzMp}) sur l'ensemble des normes existantes sur $\eR^m$.
\end{lemma}

\begin{proposition} \label{PropLJEJooMOWPNi}
    Pour \( \eR^N\), nous avons les équivalences de normes $\| . \|_{L^1}\sim\| . \|_{L^2}$, $\| . \|_{L^1}\sim\| . \|_{\infty}$ et $\| . \|_{L^2}\sim\| . \|_{\infty}$. Plus précisément nous avons les inégalités\footnote{Les racines carrés sont définies en \ref{DEFooGQTYooORuvQb}.}
    \begin{enumerate}
        \item\label{ItemABSGooQODmLNi}
           $ \| x \|_2\leq \| x \|_1\leq\sqrt{n}\| x \|_2$
        \item\label{ItemABSGooQODmLNii}
            $\| x \|_{\infty}\leq \| x \|_1\leq n \| x \|_{\infty}$
        \item\label{ItemABSGooQODmLNiii}
            $\| x \|_{\infty}\leq \| x \|_2\leq \sqrt{n}\| x \|_{\infty}$
    \end{enumerate}
\end{proposition}


\begin{proof}
    En mettant au carré la première inégalité nous voyons que nous devons vérifier l'inégalité
    \begin{equation}
        | x_1 |^2+\cdots+| x_n |^2\leq\big( | x_1 |+\cdots+| x_n | \big)^2
    \end{equation}
    qui est vraie parce que le membre de droite est égal au carré de chaque terme plus les double produits. La seconde inégalité provient de l'inégalité de Cauchy-Schwarz (théorème~\ref{ThoAYfEHG}) sur les vecteurs
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                1/n    \\
                \vdots    \\
                1/n
            \end{pmatrix},
            &w&=\begin{pmatrix}
                | x_1 |    \\
                \vdots    \\
                | x_n |
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous trouvons
    \begin{equation}
        \frac{1}{ n }\sum_i| x_i |\leq\sqrt{n\cdot\frac{1}{ n^2 }}\sqrt{\sum_i| x_i |^2},
    \end{equation}
    et par conséquent
    \begin{equation}
        \sum_i| x_i |\leq\sqrt{n}\| x \|_2.
    \end{equation}

    La première inégalité de~\ref{ItemABSGooQODmLNiii} se démontre en remarquant que si \( a\) et \( b\) sont positifs, \( a\leq\sqrt{a^2+b}\). En appliquant cela à \( a=\max_i| x_i |\), nous avons
    \begin{equation}
        \max_i| x_i |\leq\sqrt{ | x_1 |^2+\cdots+| x_n |^2  }
    \end{equation}
    parce que \( \max_i| x_i |\) est évidemment un des termes de la somme. Pour la seconde inégalité de~\ref{ItemABSGooQODmLNiii}, nous avons
    \begin{equation}
        \sqrt{\sum_k| x_k |^2}\leq\left( \sum_k\max_i| x_i |^2 \right)^{1/2}=\sqrt{n}\| x \|_{\infty}.
    \end{equation}
    Pour obtenir cette inégalité, nous avons remplacé tous les termes \( | x_k |\) par le maximum.
\end{proof}

Pour les autres normes \( \| . \|_p\), il y a des inégalités dans \ref{THOooPPDPooJxTYIy} et \ref{CORooMBQMooWBAIIH}; voir aussi le thème \ref{THEMEooUJVXooZdlmHj}.


Une dernière avant l'équivalence de toutes les normes.
\begin{propositionDef}
    Les topologies suivantes sont égales sur \( \eR^n\).
    \begin{enumerate}
        \item       \label{ITEMooWACPooFBAWhx}
            La topologie produit \( \eR\times \ldots\times \eR\) des espaces topologiques \( (\eR,| . |)\),
        \item       \label{ITEMooJPJHooGTuLen}
            La topologie de la norme
            \begin{equation}
                \| (x_1,\ldots, x_n) \|_{\infty}=\max_i\{ | x_i | \},
            \end{equation}
        \item       \label{ITEMooEBYQooXiOOtb}
            La topologie de la norme
            \begin{equation}
                \| (x_1,\ldots, x_n) \|_2=\sqrt{ \sum_{i=1}^nx_i^2 }.
            \end{equation}
    \end{enumerate}
    Elle est la topologie que nous allons toujours considérer sur \( \eR^n\) (saut mention très explicite du contraire).
\end{propositionDef}

\begin{proof}
    Les topologies \ref{ITEMooWACPooFBAWhx} et \ref{ITEMooJPJHooGTuLen} sont identiques par le lemme \ref{LEMooFQMSooLmdIvD}. Les topologies \ref{ITEMooJPJHooGTuLen} et \ref{ITEMooEBYQooXiOOtb} sont identiques par la proposition \ref{PropLJEJooMOWPNi}.
\end{proof}

En réalité, toutes les normes \( \| . \|_{L^p}\) et \( \| . \|_{\infty}\) sont équivalentes et, plus généralement, nous avons le résultat suivant, très étonnant à première vue, et en réalité assez difficile à prouver :
\begin{theorem}[\cite{TrenchRealAnalisys}]		\label{ThoNormesEquiv}
	Sur un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
\end{theorem}
% TODO : la preuve est à la page 583 de Trench.

\begin{corollary}       \label{CORooBRDYooLmGJDE}
    Soit \( V\) un espace vectoriel de dimension finie et \( \| . \|_1\), \( \| . \|_2\) deux normes sur \( V\). Alors l'identité \( \id\colon V\to V\) est un isomorphisme d'espace topologique \( (V,\| . \|_1)\to (V,\| . \|_2)\).

    De plus les ouverts sont les mêmes : une partie de \( V\) est ouverte dans \( (V,\| . \|_1)\) si et seulement si elle est ouverte dans \( (V,\| . \|_2)\).
\end{corollary}

\begin{normaltext}      \label{NORMooNKBCooKziIjx}
    Le lemme \ref{LEMooRWJYooOIJkZc} donnera une norme sur \( \eR^2\) qui ne dérive pas d'un produit scalaire. Vu que toutes les normes sur \( \eR^2\) produisent la même topologie (c'est le corolaire~\ref{CORooBRDYooLmGJDE}), il y a parfaitement moyen pour deux espaces vectoriels topologiques d'être isomorphes alors que l'un a une norme dérivant d'un produit scalaire et l'autre non.
\end{normaltext}

\begin{normaltext}
    Le théorème d'équivalence de norme sera utilisé pour montrer que l'ensemble des formes quadratiques non dégénérées de signature \( (p,q)\) est ouvert dans l'ensemble des formes quadratiques, proposition~\ref{PropNPbnsMd}. Plus généralement il est utilisé à chaque fois que l'on fait de la topologie sur les espaces de matrices en identifiant \( \eM(n,\eR)\) à \( \eR^{n^2}\), pour se rassurer en se disant que ce qu'on fait ne dépend pas de la norme choisie.

    Voir aussi ce qu'on en fait en \ref{NORMooDAZZooDiGFoW} pour démontrer la différentiabilité à partir des dérivées partielles.
\end{normaltext}

\begin{proposition}[\cite{MonCerveau}] \label{PROPooNTCFooEcwZwt}
    Soit un espace vectoriel \( V\) de dimension finie sur \( \eC\). Pour une base \( B= \{ e_i \}\) de \( V\) nous définissons
    \begin{equation}        \label{EQooEGXVooLASQIC}
        \| \sum_kv_ke_k \|_B= \sqrt{ \sum_k| v_k |^2 }.
    \end{equation}
    \begin{enumerate}
        \item
            La formule \eqref{EQooEGXVooLASQIC} définit une norme sur \( V\).
        \item
            Si \( B\) et \( B'\) sont des bases de \( V\), alors les topologies induites par le norme \( \| . \|_B\) et \( \| . \|_{B'}\) sont égales.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous commençons par fixer une base \( B=\{ e_i \}_{i=1,\ldots, n}\) de \( V\). Cette base nous permet de définir
    \begin{equation}
        \begin{aligned}
            \varphi\colon V&\to \eC^n \\
            \sum_kv_ke_k&\mapsto (v_1,\ldots, v_n). 
        \end{aligned}
    \end{equation}
    Cette application linéaire permet d'écrire
    \begin{equation}
        \| v \|_V=\| \varphi(v) \|_{\eC^n}.
    \end{equation}
    À partir de là, la vérification des propriétés de la définition \ref{DefNorme} est immédiate. Par exemple :
    \begin{equation}
        \| v+w \|=\| \varphi(v+w) \|=\| \varphi(v)+\varphi(w) \|\leq \| \varphi(v) \|+\| \varphi(w) \|=\| v \|+\| w \|.
    \end{equation}

    En ce qui concerne la seconde assertion, c'est le théorème \ref{ThoNormesEquiv}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Contre-exemple en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecPOlynomesCE}

Lorsque nous considérons des espaces vectoriels de dimension infinie, les choses ne sons plus aussi simples. Nous voyons ici sur l'exemple de l'espace des polynômes que le théorème~\ref{ThoNormesEquiv} n'est plus valable si on enlève l'hypothèse de dimension finie.

On considère l'ensemble des fonctions polynomiales à coefficients réels sur  l'intervalle $[0,1]$.
\begin{equation}
\mathcal{P}_\eR([0,1])=\{p:[0,1]\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots, \, a_i\in\eR,\,\forall i\in \eN\}.
\end{equation}
Cet ensemble, muni des opérations usuelles de somme entre polynômes et multiplications par les scalaires, est un espace vectoriel.

Sur $\mathcal{P}(\eR)$ on définit les normes suivantes
\begin{equation}
\begin{aligned}
&\|p\|_\infty=\sup_{x\in[0,1]}\{p(x)\},\\
&\|p\|_1 =\int_0^1|p(x)|\, dx,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}.\\
\end{aligned}
\end{equation}
Les inégalités suivantes sont  immédiates
\begin{equation}
\begin{aligned}
&\|p\|_1 =\int_0^1|p(x)|\, dx\leq \|p\|_\infty,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}\leq \|p\|_\infty,\\
\end{aligned}
\end{equation}
mais la norme $\|\cdot\|_\infty$ n'est  équivalente ni à $\|\cdot\|_1$, ni à $\|\cdot\|_2$. Soit $p_k(x)= x^k$. Alors
\begin{equation}
\begin{aligned}
&\|p_k\|_\infty=1,\\
&\|p_k\|_1 =\int_0^1x^k\, dx=  \frac{1}{k+1},\\
&\|p_k\|_2 =\left(\int_0^1x^{2k}\, dx\right)^{1/2}=\sqrt{\frac{1}{2k+1}}.
\end{aligned}
\end{equation}
Pour $k\to \infty$ les normes $\|p_k\|_1$, $\|p_k\|_2$ tendent vers zéro, alors que la norme $\|p_k\|_\infty$ est constante, donc les normes ne sont pas équivalentes parce que il n'existe pas un nombre positif $m$ tel que
\begin{equation}
\begin{aligned}
& m \|p_k\|_\infty\leq \|p_k\|_1 ,\\
& m \|p_k\|_\infty\leq \|p_k\|_2 ,\\
\end{aligned}
\end{equation}
uniformément pour tout $k$ dans $\eN$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Norme opérateur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La proposition suivante donne une norme (au sens de la définition~\ref{DefNorme}) sur $\aL(V,W)$ dès que \( V\) et \( W\) sont des espaces vectoriels normés.
\begin{propositionDef}[Norme opérateur\cite{ooTZRDooWmjBJi}, thème \ref{THEMEooOJJFooWMSAtL}]          \label{DefNFYUooBZCPTr}
    Soit une application linéaire \( T\colon V\to W\), et le nombre
	\begin{equation}
        \|T\|_{\aL}=\sup_{\substack{x\in V\\x\neq 0}}\frac{\|T(x)\|_{W}}{\|x\|_{V}}.
	\end{equation}
    \begin{enumerate}
        \item       \label{ITEMooGIPIooUvVBIv}
            Si \( V\) est de dimension finie, alors \( \| T \|_{\aL}<\infty\).
        \item
            L'application \( T\mapsto\| T \|_{\aL}\) est une norme sur l'espace vectoriel des applications linéaires \( V\to W\).
        \item       \label{ITEMooUQPRooYQGZzu}
            Nous avons la formule
            \begin{equation}    \label{EqFZPooIoecGH}
                \| T \|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}} =\sup_{\|x\|_{V}=1}\|T(x)\|_{W}
            \end{equation}
    \end{enumerate}
    Le nombre \( \| T \|_{\aL}\) est la \defe{norme opérateur}{norme!d'application linéaire} de $T$. Nous disons que cette norme est \defe{subordonnée}{subordonnée!norme} aux normes choisies sur \( V\) et \( W\).
\end{propositionDef}
\index{norme!d'une application linéaire}

\begin{proof}
    Si \( V\) est de dimension finie alors l'ensemble $\{ \| x \|= 1 \}$ est compact par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}. Alors la fonction
    \begin{equation}
        x\mapsto \frac{ \| T(x) \| }{ \| x \| }
    \end{equation}
    est une fonction continue sur un compact. Le corolaire~\ref{CorFnContinueCompactBorne} nous dit alors qu'elle est bornée. Le supremum est donc un nombre réel fini.

    Nous vérifions que l'application $\| . \|$ de $\aL(V,W)$ dans $\eR$ ainsi définie est effectivement une norme.
    \begin{enumerate}
        \item
            $\|T\|_{\aL}=0$ signifie que $\|T(x)\|=0$ pour tout $x$ dans $V$. Comme  $\|\cdot\|_W$ est une norme nous concluons que $T(x)=0_{n}$ pour tout $x$ dans $V$, donc $T$ est l'application nulle.
    \item
        Pour tout $a$ dans $\eR$ et tout  $T$ dans $\aL(V,W)$ nous avons
        \begin{equation}
            \|aT\|_{\mathcal{L}}=\sup_{\|x\|_{V}\leq 1}\|aT(x)\|_{W}=|a|\sup_{\|x\|_{V}\leq 1}\|T(x)\|_{W}=|a|\|T\|_{\mathcal{L}}.
        \end{equation}
    \item
        Pour tous $T_1$ et $T_2$ dans $\aL(V,W)$ nous avons
      \begin{equation}\nonumber
        \begin{aligned}
           \|T_1+ T_2\|_{\mathcal{L}}&=\sup_{\|x\|\leq 1}\|T_1(x)+T_2(x)\|\leq\\
     &\leq\sup_{\|x\|\leq 1}\|T_1(x)\| +\sup_{\|x\|\leq 1}\|T_2(x)\|\\
     &=\|T_1\|\|T_2\|.
        \end{aligned}
      \end{equation}
    \end{enumerate}


    Enfin nous prouvons la formule alternative \eqref{EqFZPooIoecGH}. Nous allons montrer que les ensembles sur lesquels ont prend le supremum sont en réalité les mêmes :
    \begin{equation}
        \underbrace{\left\{ \frac{ \| Ax \| }{ \| x \| }\right\}_{x\neq 0}}_{A}=\underbrace{\left\{ \| Ax \|\tq \| x \|=1 \right\}}_{B}.
    \end{equation}
    Attention : ce sont des sous-ensembles de réels; pas de sous-ensembles de \( \eM(\eR)\) ou des sous-ensembles de \( \eR^n\).

    Pour la première inclusion, prenons un élément de \( A\), et prouvons qu'il est dans \( B\). C'est-à-dire que nous prenons \( x\in V\) et nous considérons le nombre \( \| Ax \|/\| x \|\). Le vecteur \( y=x/\| x \|\) est un vecteur de norme $1$, donc la norme de \( Ay\) est un élément de \( B\), mais
    \begin{equation}
        \| Ay \|=\frac{ \| Ax \| }{ \| x \| }.
    \end{equation}
    Nous avons donc \( A\subset B\).

    L'inclusion \( B\subset A\) est immédiate.
\end{proof}

En d'autres termes, il y a autant de normes opérateur sur \( \aL(E,F)\) qu'il y a de paires de choix de normes sur \( E\) et \( F\). En particulier, cela donne lieu à toutes les normes \( \| A \|_p\) qui correspondent aux normes \( \| . \|_p\) sur \( \eR^n\).

\begin{example}     \label{EXooXPXAooYyBwMX}
    Voyons la norme opérateur subordonnée à la norme \( \| x \|_{\infty}=\max_i| x_i |\) sur \( \eC^n\). Par définition (et surtout par la propriété~\ref{DefNFYUooBZCPTr}\ref{ITEMooUQPRooYQGZzu}),
    \begin{equation}
        \| A \|_{\infty}=\sup_{\| x \|_{\infty}=1}=\| Ax \|_{\infty}.
    \end{equation}
    Vu que \( (Ax)_i=\sum_kA_{ik}x_k\), lorsque \( \| x \|_{\infty}\leq 1\) nous avons \( | (Ax)_i |\leq \sum_k| A_{ik} |\). Donc nous avons toujours
    \begin{equation}        \label{EQooPLCIooVghasD}
        \| A \|_{\infty}\leq \max_i\sum_{k}| A_{ik} |.
    \end{equation}
\end{example}

\begin{definition}
    La \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs est la topologie de la norme opérateur.
\end{definition}
Lorsque nous considérons un espace vectoriel d'applications linéaires, nous considérons toujours\footnote{Sauf lorsque les événements nous forceront à trahir.} dessus la topologie liée à cette norme.

Il existe aussi la \defe{topologie faible}{topologie!faible} donnée par la notion de convergence\quext{Est-ce qu'on peut décrire cette topologie à partir de ses ouverts ? Facilement ?} \( A_i\to A\) si et seulement si \( A_ix\to Ax\) pour tout \( x\in E\).
    %TODO : il faut mettre au clair quelle est vraiment la topologie faible à partir des ouverts.

\begin{probleme}
    Je crois, mais demande confirmation, que la topologie faible est celle des seminormes \( \{ p_v \}_{v\in E}\) données par \( p_v(A)=\| A \|\). En effet la notion de convergence associée par la proposition~\ref{PropQPzGKVk} est \( A_i\to A\) si et seulement si \( p_v(A_i-A)\to 0\). Cette condition signifie \( \| A_i(v)-A(v) \|\to 0\), c'est-à-dire \( A_i(v)\to A(v)\).

    Si \randomGender{le lecteur}{la lectrice} veut parler de cela au jury d'un concours, il est évident qu'\randomGender{il}{elle} devra être capable d'ajouter des petits symboles au-dessus de toutes les flèches «\( \to\)» du paragraphe précédent pour indiquer pour quelles topologies sont les convergences dont on parle.
\end{probleme}

\begin{remark}
    Il faut noter que la topologie faible n'est pas une topologie métrique. Cela même si la condition \( A_ix\to Ax\), elle, est métrique vu qu'elle est écrite dans \( E\).

    Dans le cas où \( E\) est de dimension infinie, la topologie faible est réellement différente de la topologie forte. Nous verrons à la sous-section~\ref{subsecaeSywF} que dans le cas des projections sur un espaces de Hilbert, l'égalité
    \begin{equation}
        \sum_{i=1}^{\infty}\pr_{u_i}=\id
    \end{equation}
    est vraie pour la topologie faible, mais pas pour la topologie forte.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme d'algèbre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Norme d'algèbre\cite{ooTZRDooWmjBJi}]  \label{DefJWRWQue}
    Si \( A\) est une algèbre\footnote{Définition~\ref{DefAEbnJqI}.}, une \defe{norme d'algèbre}{norme!d'algèbre} sur \( A\) est une norme telle que pour toute \( x,y\in A\),
    \begin{equation}
        \| xy \|\leq \| x \|\| y \|.
    \end{equation}
\end{definition}
La norme opérateur est une norme d'algèbre, comme nous le verrons dans le lemme \ref{LEMooFITMooBBBWGI}.

Un des intérêts d'utiliser une norme d'algèbre est que l'on a l'inégalité \( \| x^k \|\leq \| x \|^k \). Cela sera particulièrement utile lors de l'étude des séries entières, voir par exemple~\ref{secEVnZXgf}.

\begin{definition}[\cite{ooYLHAooCzQvoa}]      \label{DEFooEAUKooSsjqaL}
    Le \defe{rayon spectral}{rayon!spectral} d'une matrice carrée $A$, noté $\rho(A)$, est défini de la manière suivante :
    \begin{equation}    \label{EQooNVNOooNjJhSS}
        \rho(A)=\max_i|\lambda_i|
    \end{equation}
    où les $\lambda_i$ sont les valeurs propres de $A$.
\end{definition}

\begin{normaltext}
    Quelques remarques sur la définition du rayon spectral.
    \begin{itemize}
        \item
             Même si \( A\) est une matrice réelle, les valeurs propres sont dans \( \eC\). Donc dans \eqref{EQooNVNOooNjJhSS}, \( | \lambda_i |\) est le module dans \( \eC\) de \( \lambda_i\).
        \item
            Vu que les valeurs propres de \( A\) sont les racines de son polynôme caractéristique (théorème~\ref{ThoWDGooQUGSTL}), il y en a un nombre fini et le maximum est bien défini.
        \item
            La définition s'applique uniquement pour les espaces de dimension finie.
    \end{itemize}
\end{normaltext}

\begin{lemma}       \label{LEMooIBLEooLJczmu}
    Soient des espaces vectoriels normés \( E\) et \( F\), sur les corps \( \eR\) ou \( \eC\). Pour tout \( A\in \aL(E,F)\), et pour tout \( u\in E\) nous avons la majoration
    \begin{equation}
        \| Au \|\leq \| A \|\| u \|
    \end{equation}
    où la norme sur \( A\) est la norme opérateur subordonnée à la norme sur \( u\).
\end{lemma}

\begin{proof}
    Si \( u\in E\) alors, étant donné que le supremum d'un ensemble est plus grand ou égal à chacun de éléments qui le compose,
    \begin{equation}
        \| A \|=\sup_{x\in E}\frac{ \| Ax \| }{ \| x \| }\geq \frac{ \| Au \| }{ \| u \| },
    \end{equation}
    donc le résultat annoncé : \( \| Au \|\leq \| A \|\| u \|\).
\end{proof}

Le lemme suivant est valable en dimension infinie. Nous en toucherons un mot dans l'exemple \ref{EXooTQPEooRRdddt}.
\begin{lemma}       \label{LEMooWFNXooLyTyyX}
    Soient des espaces vectoriels normés \( E\) et \( F\). Soit \( x\in E\). Alors l'application d'évaluation
    \begin{equation}
        \begin{aligned}
            ev_x\colon \aL(E,F)&\to F \\
            f&\mapsto f(x) 
        \end{aligned}
    \end{equation}
    est continue.
\end{lemma}

\begin{proof}
    Si \( x=0\), alors par linéarité de \( f\) nous avons \( ev_0(f)=0\) pour tout \( f\). Donc d'accord pour la continuité.

    Soit une suite convergente \( f_k\stackrel{\aL(E,F)}{\longrightarrow}f\). Nous voulons prouver que \( ev_x(f_k)\stackrel{F}{\longrightarrow}ev_x(f)\), c'est-à-dire que
    \begin{equation}
        \lim_{k\to \infty} \| f_k(x)-f(x) \|=0.
    \end{equation}
    Par hypothèse si \( k\) est grand, alors \( \| f_k-f  \|_{\aL(E,F)}\leq \epsilon\), c'est-à-dire que\footnote{Définition \ref{DefNFYUooBZCPTr} de la norme sur \( \aL(E,F)\).}
    \begin{equation}
        \sup_{y\in E}\frac{ \| f_k(y)-f(y) \| }{ \| y \| }\leq \epsilon.
    \end{equation}
    En particulier pour notre \( x\) nous avons
    \begin{equation}
        \frac{ \| f_k(x)-f(x) \| }{ \| x \| }\leq \epsilon,
    \end{equation}
    c'est-à-dire \( \| f_k(x)-f(x) \|\leq \| x \|\epsilon\). Vu que \( \| x \|\) est une simple constante et que \( \epsilon\) est arbitraire, cela implique \( f_k(x)\to f(x)\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrices, spectre et norme}
%---------------------------------------------------------------------------------------------------------------------------

La lien entre la norme opérateur d'une matrice et son spectre sera entre autres utilisé pour étudier le conditionnement de problèmes numériques. Voir la définition \ref{DEFooBKQWooJuoCGX} et par exemple son lien avec la résolution numérique de systèmes linéaires dans la proposition \ref{PROPooGIXFooAhJkIs}.

\begin{proposition}[\cite{ooYLHAooCzQvoa}]      \label{PROPooKLFKooSVnDzr}
    Soit une matrice \( A\in \eM(n,\eC)\) de rayon spectral \( \rho(A)\). Soit une norme \( \| . \|\) sur \( \eC^n\) et la norme opérateur correspondante. Alors
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}
    \end{equation}
    pour tout \( k\in \eN\).
\end{proposition}

\begin{proof}
    Soit \( v\in \eC^n\) et \( \lambda\in \eC\) un couple vecteur-valeur propre. Nous avons \( \| Av \|=| \lambda |\| v \|\) et aussi
    \begin{equation}
        | \lambda |^k\| v \|=\| \lambda^kv \|=\| A^kv \|\leq \| A^k \|\| v \|.
    \end{equation}
    La dernière inégalité est due au fait que nous avons choisi sur \( \eM(n,\eC)\) la norme subordonnée à celle choisie sur \( \eC^n\), via le lemme~\ref{LEMooIBLEooLJczmu}. Nous simplifions par \( \| v \|\) et obtenons \( | \lambda |\leq \| A^k \|^{1/k}\). Étant donné que \( \rho(A)\) est la maximum de tous les \( \lambda\) possibles, la majoration passe au maximum :
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}.
    \end{equation}
\end{proof}

\begin{lemma}[La norme opérateur est une norme d'algèbre\cite{MonCerveau}]   \label{LEMooFITMooBBBWGI}
    Soient des espaces vectoriels normés \( E\), \( F\) et \( G\). Soient des opérateurs linéaires bornés \( B\colon E\to F\), \( A\colon F\to G\). Alors
    \begin{equation}
        \| AB \|\leq \| A \|\| B \|.
    \end{equation}
    C'est à dire que la norme opérateur est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}.}.
\end{lemma}

\begin{proof}

    Nous avons les (in)égalités suivantes :
    \begin{subequations}
        \begin{align}
            \| AB \|&=\sup_{x\in E}\frac{ \| ABx \|_G }{ \| x \|_E }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\frac{ \| Bx \|_F }{ \| Bx \|_F }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }\frac{ \| Bx \| }{ \| x \| }\\
            &\leq\underbrace{\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }}_{\leq\| A \|}\underbrace{\sup_{\substack{y\in E\\By\neq 0}}\frac{ \| Bx \| }{ \| y \| }}_{=\| B \|}\\
            &\leq \| A \|\| B \|.
        \end{align}
    \end{subequations}
    La dernière inégalité provient que dans \( \sup_{\substack{x\in E\\Bx\neq 0}}\| ABx \|/\| x \|\), le supremum est pris sur un ensemble plus petit que celui sur lequel porte la définition de la norme de \( A\) : seulement l'image de \( B\) au lieu de tout l'espace de départ de \( A\).
\end{proof}



\begin{proposition}     \label{PROPooJGNFooEwtNmJ}
    Soient deux espaces vectoriels normés \( E\) et \( V\). Soient des applications continues \( f,g\colon E\to \End(V)\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon E&\to \End(V) \\
            x&\mapsto f(x)\circ g(x) 
        \end{aligned}
    \end{equation}
    est continue.
\end{proposition}

\begin{proof}
    Soit une suite \( x_k\stackrel{E}{\longrightarrow}x\). Nous devons montrer que \( \psi(x_k)\stackrel{\End(V)}{\longrightarrow}\psi(x)\). Pour cela nous utilisons le lemme \ref{LEMooFITMooBBBWGI} qui indique que la norme opérateur est une norme d'algèbre. Nous avons :
    \begin{subequations}
        \begin{align}
            \| \psi(x_k)-\psi(x) \|&=\| f(x_k)\circ g(x_k)-f(x)\circ g(x) \|\\
            &\leq \| f(x_k)\circ g(x_k)-f(x_k)\circ g(x) \|+\| f(x_k)\circ g(x)-f(x)\circ g(x) \|\\
            &=\| f(x_k)\circ \big( g(x_k)\circ g(x) \big) \|+\| \big(f(x_k)-f(x)\big)\circ g(x) \|\\
            &\leq \| f(x_k) \|\| g(x_k)-g(x) \|+\| f(x_k)-f(x) \|\| g(x) \|.
        \end{align}
    \end{subequations}
    Pour \( k\to \infty\) nous avons \( \| f(x_k)\to \| f(x) \| \|\), \( \| f(x_k)-f(x) \|\to 0\) (parce que \( f\) est continue) et similaire avec \( g\). Donc le tout tend vers zéro.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rayon spectral}
%---------------------------------------------------------------------------------------------------------------------------

La chose impressionnante dans la proposition suivante est que \( \rho(A)\) est définit indépendamment du choix de la norme sur \( \eM(n,\eK)\) ou sur \( \eK\). Lorsque nous écrivons \( \| A \|\), nous disons implicitement qu'une norme a été choisie sur \( \eK\) et que nous avons pris la norme subordonnée sur \( \eM(n,\eK)\).
\begin{proposition}[\cite{ooETMNooSrtWet}]      \label{PROPooWZJBooTPLSZp}
    Soit \( A\) une matrice de \( \eM(n,\eR)\) ou \( \eM(n,\eC)\). Alors
    \begin{equation}
        \rho(A)\leq \| A \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous devons séparer les cas suivant que le corps de base soit \( \eR\) ou \( \eC\).

    \begin{subproof}
        \item[Pour \( A\in \eM(n,\eC)\)]
            Soit \( \lambda\) une valeur propre de \( A\) telle que \( | \lambda |\) soit la plus grande. Nous avons donc \( \rho(A)=| \lambda |\). Soit un vecteur propre \( u\in \eC^n\) pour la valeur propre \( \lambda\). En prenant la norme sur l'égalité \( Au=\lambda u\), et en utilisant le lemme~\ref{LEMooIBLEooLJczmu},
            \begin{equation}
                | \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
            \end{equation}
            Donc \( | \lambda |\leq \| A \|\) et \( \rho(A)\leq\| A \|\).

        \item[Pour \( A\in \eM(n,\eR)\)]

            L'endroit qui coince dans le raisonnement fait pour \( \eM(n,\eC)\) est que certes \( A\in \eM(n,\eR)\) possède une plus grande valeur propre en module et qu'un vecteur propre lui est associé. Mais ce vecteur propre est à priori dans \( \eC^n\), et non dans \( \eR^n\). Nous pouvons donc écrire \( Au=\lambda u\), mais pas \( \| Au \|=| \lambda |\| u \|\) parce que nous ne savons pas quelle norme prendre sur \( \eC^n\).

            Il n'est pas certain que nous ayons une norme sur \( \eC^n\) qui se réduit sur \( \eR^n\) à celle choisie implicitement dans l'énoncé. Nous allons donc ruser un peu.

            Soit une norme \( N\) sur \( \eC^n\)\footnote{Il y en a plein, par exemple celle du produit scalaire \( \langle x, y\rangle =\sum_kx_k\bar y_k\).}. Nous nommons également \( N\) la norme subordonnée sur \( \eM(n,\eC)\) et la norme restreinte sur \( \eM(n,\eR)\). Vu que \( N\) est une norme sur \( \eM(n,\eR)\) et que ce dernier est de dimension finie, le théorème~\ref{ThoNormesEquiv} nous indique que \( N\) est équivalente à \( \| . \|\). Il existe donc \( C>0\) tel que
            \begin{equation}        \label{EQooBNWMooNgnMxC}
                 N(B)\leq C\| B \|
            \end{equation}
            pour tout \( B\in \eM(n,\eR)\). Nous avons maintenant
            \begin{equation}
                \rho(A)^m\leq N(A^m)\leq C\| A^m \|\leq C\| A \|^m.
            \end{equation}
            Justifications
            \begin{itemize}
                \item Par la proposition~\ref{PROPooKLFKooSVnDzr}.
                \item Parce que \( A^m\in \eM(n,\eR)\) et la relation \eqref{EQooBNWMooNgnMxC}.
                \item Par itération du lemme~\ref{LEMooFITMooBBBWGI}.
            \end{itemize}

            Nous avons donc \( \rho(A)\leq C^{1/m}\| A \|\) pour tout \( m\in\eN\). En prenant \( m\to \infty\) et en tenant compte de \( C^{1/m}\to 1\) nous trouvons \( \rho(A)\leq \| A \|\).
    \end{subproof}
\end{proof}

\begin{lemma}[\cite{ooETMNooSrtWet}]        \label{LEMooGBLJooCPvxNl}
    Soit \( A\in \eM(n,\eK)\) avec \( \eK=\eR\) ou \( \eC\). Soit \( \epsilon>0\). Il existe une norme algébrique sur \( \eM(n,\eK)\) telle que
    \begin{equation}
        N(A)\leq \rho(A)+\epsilon.
    \end{equation}
\end{lemma}

\begin{proof}
    Soit par le lemme~\ref{LemSchurComplHAftTq} une matrice inversible \( U\) telle que \( T=UAU^{-1}\) soit triangulaire supérieure, avec les valeurs propres sur la diagonale. Notons que même si \( A\in \eM(n,\eR)\), les matrices \( U\) et \( T\) sont à priori complexes.

    Soit \( s\in \eR\) ainsi que les matrices
    \begin{equation}
        D_s=\diag(1,s^{-1},s^{-2},\ldots, s^{1-n})
    \end{equation}
    et \( T_s=D_sTD_s^{-1}\). Nous fixerons un choix de \( s\) plus tard.

    La norme que nous considérons est :
    \begin{equation}
        N(B)=\| (D_sU)B(D_sU)^{-1} \|_{\infty}
    \end{equation}
    où \( \| . \|_{\infty}\) est la norme sur \( \eM(,n\eK)\) subordonnée à la norme \( \| . \|_{\infty}\) sur \( \eK^n\) dont nous avons déjà parlé dans l'exemple~\ref{EXooXPXAooYyBwMX}. Cela est bien une norme parce que
    \begin{itemize}
        \item Nous avons \( \| B \|_{\infty}=0\) si et seulement si \( B=0\), et vu que \( (D_sU)\) est inversible nous avons \( (D_sU)B(D_sU)^{-1}=0\) si et seulement si \( B=0\).
        \item \( N(\lambda B)=| \lambda |N(B)\).
        \item Pour l'inégalité triangulaire :
            \begin{subequations}
                \begin{align}
             N(B+C)&=\| (D_sU)B(D_sU)^{-1}+(D_sU)C(D_sU)^{-1} \|_{\infty}\\
             &\leq  \| (D_sU)B(D_sU)^{-1}\|_{\infty} +\| (D_sU)C(D_sU)^{-1} \|_{\infty} \\
             &=N(B)+N(C).
                \end{align}
            \end{subequations}
    \end{itemize}

    En ce qui concerne la matrice \( A\) elle-même, nous avons
    \begin{equation}
        N(A)=\| (D_sU)A(D_sU)^{-1} \|_{\infty}=\| T_s \|_{\infty}.
    \end{equation}
    C'est le moment de se demander comment se présente la matrice \( T_s\). En tenant compte du fait que \( (D_s)_{ik}=\delta_{ik}s^{1-i}\) nous avons
    \begin{equation}
        (T_s)_{ij}=\sum_{kl}(D_s)_{ik}T_{kl}(D^{-1}_s)_{lj}=T_{ij}s^{j-i}.
    \end{equation}
    La matrice \( T\) est encore triangulaire supérieure avec les valeurs propres de \( A\) sur la diagonale. Les éléments au-dessus de la diagonale sont tous multipliés par au moins \( s\). Il est donc possible de choisir \( s\) suffisamment petit pour avoir\quext{Il me semble qu'il manque un module dans \cite{ooETMNooSrtWet}.}
    \begin{equation}        \label{EQooSIEIooTWAXQD}
        \sum_{j=i+1}^n| (T_s)_{ij} |<\epsilon
    \end{equation}
    Avec ce choix, la formule~\ref{EQooPLCIooVghasD} donne
    \begin{equation}
        N(T_s)\leq\max_i\sum_k| (T_s)_{ik} |\leq \epsilon+\rho(A).
    \end{equation}
    En effet le \( \epsilon\) vient de la somme sur toute la ligne sauf la diagonale (c'est-à-dire la partie \( k\neq i\)) et du choix \eqref{EQooSIEIooTWAXQD} pour \( s\). Le \( \rho(A)\) provient du dernier terme de la somme (le terme sur la diagonale) qui est une valeur propre de \( A\), donc majorable par \( \rho(A)\).

    Nous devons encore prouver que \( N\) est une norme algébrique. Pour cela nous allons montrer qu'elle est subordonnée à la norme
    \begin{equation}
        \begin{aligned}
            n\colon \eK^n&\to \eR^+ \\
            v&\mapsto \| (UD_s)v \|_{\infty}.
        \end{aligned}
    \end{equation}
    Cela sera suffisant pour avoir une norme algébrique par le lemme~\ref{LEMooFITMooBBBWGI}. La norme \( n\) sur \( \eK^n\) produit la norme suivante sur \( \eM(n,\eK)\) :
    \begin{equation}
        n(B)=\sup_{v\neq 0}\frac{ n(B) }{ n(v) }=\sup_{v\neq 0}\frac{ \| (UD_s)Bv \|_{\infty} }{ \| UD_sv \|_{\infty} }.
    \end{equation}
    Vu que \( UD_s\) est inversible nous pouvons effectuer le changement de variables \( v\mapsto (UD_s)^{-1} v\) pour écrire
    \begin{equation}
        n(B)=\sup_{v\neq 0}  \frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| (UD_s)(UD_s)^{-1}v \|_{\infty} }=\sup_{v\neq 0}\frac{  \| (UD_s)B(UD_s)^{-1}v \|_{\infty} }{ \| v \|_{\infty} }=\| (UD_s)B(UD_s)^{-1} \|_{\infty}=N(B).
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooYPLGooWKLbPA}
    Si \( A\in \eM(n,\eR)\) alors \( \rho(A)^m=\rho(A^m)\) pour tout \( m\in \eN\).
\end{proposition}

\begin{proof}
    La matrice \( A\) peut être vue dans \( \eM(n,\eC)\) et nous pouvons lui appliquer le corolaire~\ref{CORooTPDHooXazTuZ} :
    \begin{equation}        \label{EQooJJIYooDBacjn}
        \Spec(A^k)=\{ \lambda^k\tq \lambda\in\Spec(A) \}.
    \end{equation}
    À noter qu'il n'y a pas de magie : le spectre de la matrice réelle \( A\) est déjà défini en voyant \( A\) comme matrice complexe. Le spectre dont il est question dans \eqref{EQooJJIYooDBacjn} est bien celui dont on parle dans la définition du rayon spectral.

    Nous avons ensuite :
    \begin{subequations}
        \begin{align}
            \rho(A^k)&=\max\{ | \lambda |\tq \lambda\in\Spec(A^k) \}\\
            &=\max\{ | \lambda^k |\tq \lambda\in\Spec(A) \}\\
            &=\max\{ | \lambda |^k\tq\lambda\in\Spec(A) \}\\
            &=\rho(A)^k.
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}[Bornée si et seulement si continue\cite{GKPYTMb}]       \label{PROPooQZYVooYJVlBd}
    Soient \( E\) et \( F\) des espaces vectoriels normés. Une application linéaire \( E\to F\) est bornée si et seulement si elle est continue.
\end{proposition}

\begin{proof}
    Nous commençons par supposer que \( A\) est bornée. Par le lemme~\ref{LEMooFITMooBBBWGI}, pour tout \( x,y\in E\), nous avons
    \begin{equation}
        \| A(x)-A(y) \|=\| A(x-y) \|\leq \| A \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
    \begin{equation}
        0\leq \| A(x_n)-A(x) \|\leq \| A \|\| x_n-x \|\to 0
    \end{equation}
    et \( A\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition~\ref{PropFnContParSuite}.

    Nous supposons maintenant que \( \| A \|\) n'est pas borné : l'ensemble \( \{ \| A(x) \|\tq \| x \|=1 \}\) contient des valeurs arbitrairement grandes. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| A(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| A(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( A\) n'est pas continue.
\end{proof}

Le fait que les applications linéaires soient continues est valable dans une assez large gamme d'espaces vectoriels\cite{BIBooUWMLooWEPxcC}. Nous voyons ici dans le cas des espaces vectoriels normés de dimension finies.
\begin{proposition}     \label{PROPooADPDooOtukQP}
    Soient des espaces vectoriels normés \( E\) et \( F\). Si \( f\colon E\to F\) est une application linéaire et si \( E\) est de dimension finie, alors \( f\) est continue.
\end{proposition}

\begin{proof}
    La proposition \ref{DefNFYUooBZCPTr}\ref{ITEMooGIPIooUvVBIv} nous dit que \( \| f \|<\infty\), c'est à dire que \( f\) est borné. Donc la proposition \ref{PROPooQZYVooYJVlBd} conclut.
\end{proof}

\begin{proposition}     \label{PROPooXEQLooHvzVVm}
    Soient des espaces vectoriels normés \( V\) de dimension \( n\) et \( W\) de dimension \( m\) sur \( \eK\) (corps normé). Nous considérons une base \( \{ e_s \}_{s=1,\ldots, n}\) de \( V\) et \( \{ f_{\alpha} \}_{\alpha=1,\ldots, m}\) de \( W\).

    Alors l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon \eM(n\times m,\eK)  &\to \aL(V,W) \\
            \psi(A)v&=\sum_{s\alpha}A_{s\alpha}v_sf_{\alpha}
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces topologiques.

    Pour rappel, la topologie sur \( \eM(n,\eK)\) est donnée par la définition \ref{DEFooCQHDooYpUAhG}.
\end{proposition}

\begin{proof}
    Nous savons déjà que \( \psi\) est une bijection. De plus, elle est linéaire et donc continue par la proposition \ref{PROPooADPDooOtukQP}. En ce qui concerne son inverse, c'est également une application linéaire (lemme \ref{LEMooLGEHooVEEoiU}); elle est alors également continue.
\end{proof}


\begin{definition}[\cite{ooAISYooXtUafT}]      \label{DEFooTLQUooJvknvi}
    Soient \( E\) et \( F\) deux espaces vectoriels normés.
    \begin{itemize}
        \item
            L'ensemble des applications linéaires \( E\to F\) est noté \( \aL(E,F)\).
        \item Un \defe{morphisme}{morphisme!espace vectoriel normé} est une application linéaire \( E\to F\) continue pour la topologie de la norme opérateur. Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que la continuité était équivalente à être bornée. L'ensemble des morphismes est noté \( \cL(E,F)\)\nomenclature[B]{\( \cL(E,F)\)}{applications linéaires bornées (continues)}.
        \item
            Un \defe{isomorphisme}{isomorphisme!espace vectoriel normé} est un morphisme continu inversible dont l'inverse est continu. Nous notons \( \GL(E,F)\) l'ensemble des isomorphismes entre \( E\) et \( F\).
    \end{itemize}
\end{definition}

Le point important de la définition~\ref{DEFooTLQUooJvknvi} est la continuité. En dimension infine, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

Si \( V\) est un espace vectoriel normé, nous avons déjà défini son dual topologique \( V'\) comme étant l'ensemble des applications linéaires continues \( V\to \eC\) ou \( V\to \eR\) selon le corps de base de \( V\). C'est la définition \ref{DefJPGSHpn}.

\begin{proposition}
    Soient un espace vectoriel normé \( V\) et un élément \( v\in V\) vérifiant \( \| v \|=1\). Il existe une forme \( \varphi\in V'\) telle que \( \| \varphi \|=1\) et \( \varphi(v)=1\).
\end{proposition}

\begin{proof}
    Nous allons utiliser le théorème de la base incomplète \ref{THOooOQLQooHqEeDK}. Pour cela nous considérons \( I=V\) et la partie clairement génératrice \( G=\{ e_i=i \}_{i\in I}\) (si vous avez bien suivi, \( G=V\) en fait; rien de bien profond). Nous considérons ensuite \( I_0=\{ v \}\). Le théorème de la base incomplète nous donne l'existence de \( I_1\) tel que \( I_0\subset I_I\subset I\) et tel que \( B=\{ e_i \}_{i\in I_1}\) est une base.

    Tout cela pour dire que \( B=\{ e_i \}_{i\in I_1}\) est une base contenant \( v\). Nous allons aussi éventuellement redéfinir la norme de \( e_i\) pour avoir \( \| e_i \|=1\). Cette renormalisation n'affecte pas le fait que \( v\in B\).

    Nous passons maintenant à la définition de \( \varphi\colon V\to \eK\). Pour \( x\in V\) nous commençons par écrire
    \begin{equation}
        x=\sum_{j\in J}x_je_j
    \end{equation}
    et nous posons
    \begin{equation}
        \varphi(x)=\begin{cases}
            x_v    &   \text{si } v\in J\\
            0    &    \text{sinon. }
        \end{cases}
    \end{equation}
    Cette définition a un sens par la partie unicité de la proposition \ref{PROPooEIQIooXfWDDV} de décomposition d'un élément dans une base.

    Nous devons calculer la norme de \( \varphi\). Par la proposition \ref{DefNFYUooBZCPTr}\ref{ITEMooUQPRooYQGZzu} nous avons
    \begin{equation}        \label{EQooEFLLooOWPSev}
        \| \varphi \|=\sup_{\| x \|=1}| \varphi(x) |.
    \end{equation}
    Avec \( x=v\) nous avons \( \varphi(x)=1\) et donc \( \| \varphi \|\geq 1\).

    Nous devons encore montrer que \( \| \varphi \|\leq 1\). Un élément \( x\in V\) s'écrit toujours sous la forme
    \begin{equation}
        x=\sum_{i\in J}x_je_j
    \end{equation}
    pour un certain \( J\) fini dans \( I_1\) et pour certains \( x_j\in \eK\). Pour un tel \( x\) nous avons \( \varphi(x)=x_v\). Si \( |\varphi(x)|\geq 1\), alors \( | x_v |\geq 1\), mais alors
    \begin{equation}
        \| x \|\leq \sum_{j\in J}| x_j |\| e_j \|=\sum_{j\in J}| x_j |\geq | x_v |>1,
    \end{equation}
    ce qui fait que ce \( x\) ne participe pas au supremum \eqref{EQooEFLLooOWPSev}.

    Notons que \( \varphi\) est continue (et donc bien dans \( V'\)) parce qu'elle est bornée (proposition \ref{PROPooQZYVooYJVlBd}).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecNomrApplLin}

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
    La norme $2$ d'une matrice est liée au rayon spectral de la façon suivante :
    \begin{equation}
        \|A\|_2=\sqrt{\rho(A{^t}A)}
    \end{equation}
    ou plus généralement par \( \| A \|_2=\sqrt{\rho(A^*A)}\).
\end{theorem}

\begin{lemma}       \label{LEMooNESTooVvUEOv}
    Soit une matrice \( A\in \eM(n,\eR)\) qui est symétrique, strictement définie positive. Soient \( \lambda_{min}\) et \( \lambda_{max}\) les plus petites et plus grandes valeurs propres. Alors
    \begin{subequations}
        \begin{align}
            \| A \|_2=\lambda_{max}&&\text{ et }&&\|A^{-1}  \|_2=\frac{1}{ \lambda_{min} }.
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Soient les vecteurs \( v_1,\ldots, v_n\) formant une base orthonormée de vecteurs propres\footnote{Possible par le théorème spectral~\ref{ThoeTMXla}.} de \( A\). Nous notons \( v_{max}\) celui de \( \lambda_{max}\). Nous avons :
    \begin{equation}
        \| A \|_2\geq \| Av_{max} \|=| \lambda_{max} |\| v_{max} \|=| \lambda_{max} |=\lambda_{max}.
    \end{equation}
    Voilà l'inégalité dans un sens. Montrons l'inégalité dans l'autre sens. Soit \( x=\sum_ix_iv_i\) avec \( \| x \|_2=1\). Alors
    \begin{equation}
        \| Ax \|=\| \sum_ix_i\lambda_iv_i \|\leq\sqrt{ \sum_ix_i^2\lambda_i^2 }\leq \lambda_{max}\sqrt{ \sum_ix_i^2}=\lambda_{max}.
    \end{equation}

    En ce qui concerne l'affirmation pour la norme de \( A^{-1}\), il suffit de remarquer que ses valeurs propres sont les inverses des valeurs propres de \( A\).
\end{proof}

\begin{proposition} \label{PropMAQoKAg}
    La fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eR)\times \eM(n,\eR)&\to \eR \\
            (X,Y)&\mapsto \tr(X^tY)
        \end{aligned}
    \end{equation}
    est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
    Il faut vérifier la définition~\ref{DefVJIeTFj}.
    \begin{itemize}
        \item La bilinéarité est la linéarité de la trace.
        \item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
        \item L'application \( f\) est définie positive parce que si \( X\in \eM\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
    \end{itemize}
\end{proof}

\begin{example}
	Soit $m=n$, un point $\lambda$ dans $\eR$ et $T_{\lambda}$ l'application linéaire définie par $T_{\lambda}(x)=\lambda x$. La norme de $T_{\lambda}$ est alors
\[
\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
\]
Notez que $T_{\lambda}$ n'est rien d'autre que l'homothétie de rapport $\lambda$ dans $\eR^m$.
\end{example}

\begin{example}
	Toutes les isométries ont norme \( 1\). En effet si \( T\) est une isométrie, $\| Tx \|=\| x \|$. En ce qui concerne la norme de $T$ nous avons alors
	\begin{equation}
		\| T \|=\sup_{x\in\eR^2}\frac{ \| T(x) \| }{ \| x \| }=\sup_{x\in\eR^2}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
\end{example}

\begin{example}
  Soit $m=n$, un point $b$ dans $\eR^m$ et $T_b$ l'application linéaire définie par $T_b(x)=b\cdot x$ (petit exercice : vérifiez qu'il s'agit vraiment d'une application linéaire).  La norme de $T_b$ satisfait les inégalités suivantes
 \[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^n}\|x\cdot x\|_{\eR^n}\leq\|b \|_{\eR^n},
\]
\[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left\|b\cdot \frac{b}{\|b \|_{\eR^n}}\right\|_{\eR^n}=\|b \|_{\eR^n},
\]
donc $\|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}$.
\end{example}

\begin{proposition}
    Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
      Soit $x$ un point dans $\eR^m$. Nous devons vérifier l'égalité
      \begin{equation}
       \lim_{h\to 0_m}T(x+h)=T(x).
      \end{equation}
      Cela revient à prouver que $\lim_{h\to 0_m}T(h)=0$, parce que $T(x+h)=T(x)+T(h)$. Nous pouvons toujours majorer $\|T(h)\|_n$ par $\|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}$ (lemme~\ref{LEMooIBLEooLJczmu}). Quand $h$ s'approche de $ 0_m $ sa norme $\|h\|_m$ tend vers $0$, ce que nous permet de conclure parce que nous savons que de toutes façons, $\| T \|_{\aL}$ est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition~\ref{PROPooQZYVooYJVlBd}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Application linéaire continue et bornée}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu dans la proposition~\ref{PROPooQZYVooYJVlBd} que pour une application linéaire, être bornée est équivalent à être continue. Nous allons maintenant voir un certain nombre d'exemples illustrant ce fait.

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
    Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle $\| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}$ où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.
%TODO : le faire, et regarder si Hilbet n'est pas la complétion de cet espace. Référencer à l'endroit qui définit l'espace vectoriel librement engendré. Ici ce serait par N.

%TODO : dire qu'une application bilinéaire sur RxR n'est pas une application linéaire sur R^2

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]      \label{EXooDMVJooAJywMU}
    En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

    Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

    Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

    Nous avons utilisé le critère séquentiel de la continuité, voir la définition~\ref{DefENioICV} et la proposition~\ref{PropFnContParSuite}.
\end{example}

\begin{remark}  \label{RemOAXNooSMTDuN}
Cette proposition permet de retrouver l'exemple~\ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{example}     \label{EXooTQPEooRRdddt}
    Nous avons vu dans le lemme \ref{LEMooWFNXooLyTyyX} que pour un \( x\in E\) donné, l'application
    \begin{equation}
        \begin{aligned}
            ev_x\colon \aL(E,F)&\to F \\
            f&\mapsto f(x) 
        \end{aligned}
    \end{equation}
    est continue. Vu que \( ev_x\) est linéaire, la proposition \ref{PROPooQZYVooYJVlBd} nous indique que \( ev_x\) est bornée. Vérifions-le directement. Le calcul n'est pas très compliqué :
    \begin{equation}
        \| ev_x \|=\sup_{\| f \|=1}\| ev_x(f) \|=\sup_{\| f \|=1}\| f(x) \|\leq \sup_{\| f \|=1}\| x \|\| f \|=\| x \|
    \end{equation}
    où nous avons utilisé le lemme \ref{LEMooIBLEooLJczmu} en passant. Donc la norme de \( ev_x\) est majorée par \( \| x \|\).

    Elle est même égale à \( \| x \|\). En effet, pour chaque \( f\in \aL(E,F)\) tel que \(  \| f \|=1\), nous avons
    \begin{equation}
        \| ev_x \|\geq \| ev_x(f) \|=\| f(x) \|.
    \end{equation}
    En prenant \( f=\id\) nous trouvons \(  \| ev_x \|\geq \| x \|  \).
\end{example}

\begin{definition}      \label{DEFooKSDFooGIBtrG}
    Soit un espace vectoriel \( E\) sur le corps \( \eK\). Son \defe{dual topologique}{dual topologique}, noté \( E'\), est l'ensemble des formes linéaires continues de \( E\) vers \( \eK\).
\end{definition}

\begin{lemma}   \label{LemWWXVSae}
Soit \( F\) un espace de Banach et deux suites \( A_k\to A\) et \( B_k\to B\) dans \( \aL(F,F)\). Alors \( A_k\circ B_k\to A\circ B\) dans \( \aL(F,F)\), c'est-à-dire
\begin{equation}
    \lim_{n\to \infty} (A_kB_k)=\left( \lim_{n\to \infty} A_k \right)\left( \lim_{n\to \infty} B_k \right).
\end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'écrire
    \begin{equation}
        \| A_kB_k-AB \|\leq \| A_kB_k-A_kB \|+\| A_kB-AB \|.
    \end{equation}
    Le premier terme tend vers zéro pour \( k\to\infty\) parce que
    \begin{subequations}
        \begin{align}
            \| A_kB_k-A_kB \|&=\| A_k(B_k-B) \|\\
            &\leq \| A_k \|\| B_k-B \|\to \| A \|\cdot 0\\
            &=0
        \end{align}
    \end{subequations}
    où nous avons utilisé la propriété fondamentale de la norme opérateur : la proposition~\ref{PROPooQZYVooYJVlBd}. Le second terme tend également vers zéro pour la même raison.
\end{proof}

\begin{proposition}[\cite{ooCUHNooNYIeGt}]      \label{PROPooQFTSooPFfbCc}
    Soient des espaces vectoriels normés \( V\) et \( W\) ainsi qu'une forme sesquilinéaire \( \phi\colon V\times W\to \eC\). Il y a équivalence des faits suivants.
    \begin{enumerate}
        \item
            \( \phi\) est continue.
        \item
            \( \phi\) est continue en \( (0,0)\)
        \item
            \( \phi\) est bornée
        \item
            Il existe \( C\geq 0\) telle que \( | \phi(x,y) |\leq C\| x \|\| y \|  \) pour tout \( (x,y)\in V\times W\).
    \end{enumerate}
    De plus la norme de \( \phi\) est alors donnée par
    \begin{equation}
        \| \phi \|=\min\{  C\geq 0\tq | \phi(x,y) |\leq C\| x \|\| y \|\forall (x,y)\in V\times W  \}.
    \end{equation}
\end{proposition}

On remarque tout de suite que la norme $\|.\|_\infty$ sur $\eR^2$ est la norme de l'espace produit $\eR\times\eR$. En outre cette définition nous permet de trouver plusieurs nouvelles normes dans les espaces $\eR^p$. Par exemple, si nous écrivons $\eR^4$ comme $\eR^2\times \eR^2$ on peut munir $\eR^4$ de la norme produit
\[
\|(x_1,x_2,x_3,x_4)\|_{\infty, 2}=\max\{\|(x_1,x_2)\|_\infty, \|(x_3,x_4)\|_2\}.
\]
Les applications de projection de l'espace produit $V\times W$ vers les espaces <<facteurs>>, $V$ $W$ sont notées $\pr_V$ et $\pr_W$ et sont définies par
\begin{equation}
	\begin{aligned}
		\pr_V\colon V\times W&\to V \\
		(v,w)&\mapsto v
	\end{aligned}
\end{equation}
et
\begin{equation}
	\begin{aligned}
		\pr_W\colon V\times W &\to W \\
		(v,w)&\mapsto w.
	\end{aligned}
\end{equation}
Les inégalités suivantes sont évidentes
\begin{equation}
	\begin{aligned}[]
		\|\pr_V(v,w)\|_V&\leq \|(v,w)\|_{V\times W} \\
		\|\pr_W(v,w)\|_W&\leq \|(v,w)\|_{V\times W}.
	\end{aligned}
\end{equation}
La topologie de l'espace produit est induite par les topologies des espaces <<facteurs>>. La construction est faite en deux passages : d'abord nous disons que une partie $A\times B$ de $V\times W$ est ouverte si $A$ et $B$ sont des parties ouvertes de $V$ et de $W$ respectivement.  Ensuite nous définissons que une partie quelconque de $V\times W$ est ouverte si elle est une intersection finie ou une réunion de parties ouvertes de $V\times W$ de la forme $A\times B$.

Ce choix de topologie donne deux propriétés utiles de l'espace produit
\begin{enumerate}
	\item
		Les projections sont des \defe{applications ouvertes}{application!ouverte}. Cela veut dire que l'image par $\pr_V$ (respectivement $\pr_W$) de toute partie ouverte de $V\times W$ est une partie ouverte de $V$ (respectivement $W$).
	\item
		Pour toute partir $A$ de $V$ et $B$ de $W$, nous avons $\Int (A\times B)=\Int A\times \Int B$.\label{PgovlABeqbAbB}
\end{enumerate}
Une propriété moins facile a prouver est que pour toute partie $A$ de $V$ et $B$ de $W$ nous avons  $\overline{A\times B}=\bar{A}\times \bar{B}$. Voir le lemme~\ref{LemCvVxWcvVW}.
% position 26329
%et l'exercice~\ref{exoGeomAnal-0009}.

Ce que nous avons dit jusqu'ici est valable pour tout produit d'un nombre fini d'espaces vectoriels normés. En particulier, pour tout $m>0$  l'espace  $\eR^m$ peut être considéré comme le produit de $m$ copies de $\eR$.

\begin{example}
	Si $V$ et $W$ sont deux espaces vectoriels, nous pouvons considérer le produit $E=V\times W$. Les projections $\pr_V$ et $\pr_W$\nomenclature{$\pr_V$}{projection de $V\times W$ sur $V$}, définies dans la section~\ref{sec_prod}, sont des applications linéaires.

	En effet, la projection $\pr_V\colon V\times W\to V$ est donnée par $\pr_V(v,w)=v$. Alors,
	\begin{equation}
		\begin{aligned}[]
			\pr_V\big( (v,w)+(v',w') \big)&=\pr_V\big( (v+v'),(w+w') \big)\\
			&=v+v'\\
			&=\pr_V(v,w)+\pr_V(v',w'),
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\pr_V\big( \lambda(v,w) \big)=\pr_V\big( (\lambda v,\lambda w) \big)=\lambda v=\lambda\pr_V(v,w).
	\end{equation}
	Nous laissons en exercice le soin d'adapter ces calculs pour montrer que $\pr_W$ est également une projection.
\end{example}

\begin{proposition} \label{PropDXR_KbaLC}
    Si \( \mO\) est un voisinage de \( (a,b)\) dans \( V\times W\) alors \( \mO\) contient un ouvert de la forme \( B(a,r)\times B(b,r)\).
\end{proposition}

\begin{proof}
    Vu que \( \mO\) est un voisinage, il contient un ouvert et donc une boule
    \begin{equation}
        B\big( (a,b),r \big)=\{ (v,w)\in V\times W\tq \max\{ \| v-a \|,\| w-b \| \}< r \}.
    \end{equation}
    Évidemment l'ensemble \( B(a,r)\times B(b,r)\) est dedans.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Suites}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons maintenant parler de suites dans $V\times W$. Nous noterons $(v_n,w_n)$ la suite dans $V\times W$ dont l'élément numéro $n$ est le couple $(v_n,w_n)$ avec $v_n\in V$ et $w_n\in W$. La notions de convergence de suite découle de la définition de la norme via la proposition \ref{PROPooOSXCooJWXkWH}. Il se fait que dans le cas des produits d'espaces, la convergence d'une suite est équivalente à la convergence des composantes. Plus précisément, nous avons le lemme suivant.
\begin{lemma}		\label{LemCvVxWcvVW}
	La suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$ si et seulement les suites $(v_n)$ et $(w_n)$ convergent séparément vers $v$ et $w$ respectivement dans $V$ et $W$.
\end{lemma}

\begin{proof}
	Pour le sens direct, nous devons étudier le comportement de la norme de $(v_n,w_n)-(v,w)$ lorsque $n$ devient grand. En vertu de la définition de la norme dans $V\times W$ nous avons
	\begin{equation}
		\Big\| (v_n,w_n)-(v,w) \Big\|_{V\times W}=\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}.
	\end{equation}
	Soit $\varepsilon>0$. Par définition de la convergence de la suite $(v_n,w_n)$, il existe un $N\in\eN$ tel que $n>N$ implique
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	et donc en particulier les deux inéquations
	\begin{subequations}
		\begin{align}
			\| v_n-v \|&<\varepsilon\\
			\| w_n-w \|&<\varepsilon.
		\end{align}
	\end{subequations}
	De la première, il ressort que $(v_n)\to v$, et de la seconde que $(w_n)\to w$.

	Pour le sens inverse, nous avons pour tout $\varepsilon$ un $N_1$ tel que $\| v_n-v \|_V\leq\varepsilon$ pour tout $n>N_1$ et un $N_2$ tel que $\| w_n-w \|_W\leq\varepsilon$ pour tout $n>N_2$. Si nous posons $N=\max\{ N_1,N_2 \}$ nous avons les deux inégalités simultanément, et donc
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	ce qui signifie que la suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]          \label{PROPooKDGOooDjWQct}
    Soit un espace \( E\) muni d'un produit scalaire à valeurs dans \( \eK\) (si \( \eK=\eC\) nous supposons le produit hermitien, mais ce n'est pas très important ici). Alors l'application
    \begin{equation}
        \begin{aligned}
            a\colon E\times E&\to \eK \\
            (x,y)&\mapsto \langle x, y\rangle
        \end{aligned}
    \end{equation}
    est continue.
\end{proposition}

\begin{proof}
    Nous ne disons pas que l'espace \( V\times V\) est muni d'un produit scalaire. Mais en tout cas c'est un espace métrique, et \( \eK\) l'est aussi. Donc \( a\) est une application entre deux espaces métriques et elle sera continue si et seulement si elle est séquentiellement continue (proposition~\ref{PropFnContParSuite}\ref{ItemWJHIooMdugfu}).

    Soit donc une suite convergente dans \( E\times E\), c'est-à-dire \( (x_k,y_k)\stackrel{E\times E}{\longrightarrow}(x,y)\). Nous devons démontrer que \( \langle x_k, y_k\rangle \stackrel{\eR}{\longrightarrow}\langle x, y\rangle \). Les majorations usuelles donnent
    \begin{subequations}
        \begin{align}
            \big| \langle x_k, y_k\rangle -\langle x, y\rangle  \big|&\leq \big| \langle x_k, y_k\rangle -\langle x, y_k\rangle  \big|+\big| \langle x, y_k\rangle -\langle x, y\rangle  \big|\\
            &=\big| \langle x_k-x, y_k\rangle  \big|+\big| \langle x, y_k-y\rangle  \big|.
        \end{align}
    \end{subequations}
    Nous savons du lemme~\ref{LemCvVxWcvVW} que les suites \( (x_k)\) et \( (y_k)\) sont séparément convergentes : \( x_k\stackrel{E}{\longrightarrow}x\) et \( y_k\stackrel{E}{\longrightarrow}y\). En utilisant l'inégalité de Cauchy-Schwarz~\ref{EQooZDSHooWPcryG} nous trouvons
    \begin{equation}
        \big| \langle x_k-x, y_k\rangle  \big|\leq \| x_k-x \|\| y_k \|.
    \end{equation}
    Nous avons \( \| x_k-x \|\to 0\) et \( \| y_k \|\to \| y \|\), et par la règle du produit de limites dans \( \eR\) nous avons que \( \big| \langle x_k-x, y_k\rangle  \big|\to 0\).
\end{proof}

\begin{remark}		\label{RemTopoProdPasRm}
	Il faut remarquer que la norme \eqref{EqNormeVxWmax} est une norme \emph{par défaut}. C'est la norme qu'on met quand on ne sait pas quoi mettre. Or il y a au moins un cas d'espace produit dans lequel on sait très bien quelle norme prendre : les espaces $\eR^m$. La norme qu'on met sur $\eR^2$ est
	\begin{equation}
		\| (x,y) \|=\sqrt{x^2+y^2},
	\end{equation}
	et non la norme «par défaut» de $\eR^2=\eR\times\eR$ qui serait
	\begin{equation}
		\| (x,y) \|=\max\{ | x |,| y | \}.
	\end{equation}
	Les théorèmes que nous avons donc démontré à propos de $V\times W$ ne sont donc pas immédiatement applicables au cas de $\eR^2$.

	Cette remarque est valables pour tous les espaces $\eR^m$. À moins de mention contraire explicite, nous ne considérons jamais la norme par défaut \eqref{EqNormeVxWmax} sur un espace $\eR^m$.
\end{remark}

Étant donné la remarque~\ref{RemTopoProdPasRm}, nous ne savons pas comment calculer par exemple la fermeture du produit d'intervalle $\mathopen] 0,1 ,  \mathclose[\times\mathopen[ 4 , 5 [$. Il se fait que, dans $\eR^m$, les fermetures de produits sont quand même les produits de fermetures.

\begin{proposition}		\label{PropovlAxBbarAbraB}
	Soit $A\subset\eR^m$ et $B\subset\eR^m$. Alors dans $\eR^{m+n}$ nous avons $\overline{ A\times B }=\bar A\times \bar B$.
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Continuité du produit de matrices}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooOAWAooFcyUfI}

Nous avons introduit des normes sur \( \eM(n,\eK)\), entre autres la norme opérateur de la définition~\ref{DefNFYUooBZCPTr}. Qui dit norme dit topologie. Il advient alors la question évidente : est-ce que des opérations aussi élémentaires que le produit de matrices sont continues pour ces topologies ?

Une façon simple de répondre à cela est d'introduire sur \( \eM(n,\eK)\) une nouvelle norme très simple : celle de \( \eK^n\). C'est la topologie par composante. Pour cette topologie, il est simple de voir que le produit matriciel est continu parce que les éléments de \( AB\) sont des polynômes en les éléments de \( A\) \( B\). Ensuite il suffit d'invoquer l'équivalence de toutes les normes (théorème~\ref{ThoNormesEquiv}).

Voyons comment montrer cela de façon plus directe (bien que le raisonnement précédent soit une démonstration qui devrait déjà avoir convaincu les plus sceptiques). La preuve suivante va donc s'amuser à bien préciser les topologies et caractérisations utilisées.

\begin{lemma}
    Si \( \| . \|\) est une norme algébrique sur \( \eM(n,\eK)\) (\( \eK\) est \( \eR\) ou \( \eC\)) alors l'application
    \begin{equation}
        \begin{aligned}
            p\colon \eM(n,\eK)\times \eM(n,\eK)&\to \eM(n,\eK) \\
            (A,B)&\mapsto AB
        \end{aligned}
    \end{equation}
    est continue.
\end{lemma}

\begin{proof}
    L'espace \( \eM(n,\eK)\times \eM(n,\eK)\) est métrique (définition~\ref{DefFAJgTCE}), donc la caractérisation séquentielle de la continuité (proposition~\ref{PropXIAQSXr}) s'applique. Nous considérons donc une suite \( (A_k,B_k)\) dans \( \eM(n,\eK)\times \eM(n,\eK)\) convergente vers \( AB\).

    Nous savons que la topologie sur \( \eM(n,\eK)\times \eM(n,\eK)\) est la topologie produit (lemme~\ref{DefFAJgTCE}) et que celle-ci donne la convergence composante par composante dès que nous avons convergence d'une suite; c'est la proposition~\ref{PROPooNRRIooCPesgO}. Nous avons donc \( A_k\stackrel{\eM(n,\eK)}{\longrightarrow}A\) et \( B_k\stackrel{\eM(n,\eK)}{\longrightarrow}B\).

    Voilà pour le contexte. Maintenant, la preuve de la continuité. Nous effectuons les majorations suivantes :
    \begin{subequations}
        \begin{align}
            \| p(A_k,B_K)-AB \|&\leq \| p(A_k,B_k)-p(A_k,B) \|+\| p(A_k,B)-AB \|\\
            &=\| A_Kb_k-A_kB \|+\| A_kB-AB \|\\
            &=\| A_k(B_k-B) \|+\| (A_k-A)B \|\\
            &\leq \underbrace{\| A_k \|}_{\to \| A \|}\underbrace{\| B_k-B \|}_{\to 0}+\underbrace{\| A_k-A \|}_{\to 0}\| B \|.
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}     \label{PROPooDRHMooYzXbkl}
    Soit un espace vectoriel normé \( V\) de dimension finie. Soit une suite d'opérateurs \( T_n\in \End(V)\). Si \( \{ e_i \}\) est une base de \( V\) et si \( T_n(e_i)\stackrel{V}{\longrightarrow}e_i\) pour tout \( i\), alors \( T_n\stackrel{\End(V)}{\longrightarrow}\id\).
\end{proposition}

\begin{proof}
    Nous utilisons l'application \( \psi\colon \eM(n,\eK)\to \End(V)\) définie en \ref{DEFooJVOAooUgGKme}. Elle nous permet d'écrire
    \begin{equation}
        T_n(x)=\sum_{kl}\psi^{-1}(T_i)_{kl}x_le_k,
    \end{equation}
    que nous allons particulariser à \( x=e_j\). Nous avons
    \begin{subequations}
        \begin{align}
            e_j&=\lim_{n\to \infty} T_n(e_j)\\
            &=\lim_{n\to \infty} \sum_{kl}\psi^{-1}(T_n)_{kl}\delta_{jl}e_k\\
            &=\sum_{k}\big( \lim_{n\to \infty} \psi^{-1}(T_n)_{kj} \big)e_k
        \end{align}
    \end{subequations}
    En identifiant les coefficients de \( e_j\), on trouve
    \begin{equation}
        \lim_{n\to \infty} \psi^{-1}(T_n)_{kj}=\delta_{kj}.
    \end{equation}
    Pour chaque \( k\) et \( l\), à gauche nous avons une limite dans \( \eK\). Vue la topologie sur \( \eM(n,\eK)\)\footnote{Toutes les normes sur un espace vectoriel de dimension finie sont équivalentes (théorème \ref{ThoNormesEquiv}). Sur \( \eM(n,\eK)\), nous avons convenu dans la définition \ref{DEFooCQHDooYpUAhG} de considérer la norme maximum.}, nous pouvons écrire cela comme une limite dans \( \eM(n,\eK)\) :
    \begin{equation}
        \lim_{n\to \infty} \psi^{-1}(T_n)=\mtu.
    \end{equation}
    Nous savons que \( \psi^{-1}\) est continue (proposition \ref{PROPooXEQLooHvzVVm}) de telle sorte que nous pouvons la commuter avec la limite :
    \begin{equation}
        \mtu= \lim_{n\to \infty} \psi^{-1}(T_n)=\psi^{-1}\big( \lim_{n\to \infty T_n}  \big).
    \end{equation}
    Appliquant maintenant \( \psi\) des deux côtés, \( \psi(\mtu)=\id\) et
    \begin{equation}
        \id=\lim_{n\to \infty} T_n.
    \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications multilinéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Application multilinéaire]       \label{DefFRHooKnPCT}
    Une application $T: \eR^{m_1}\times \ldots \times\eR^{m_k}\to\eR^p $ est dite \defe{\( k\)-linéaire}{application!multilinéaire} si pour tout $X=(x_1, \ldots,x_k)$ dans $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ les applications $x_i\mapsto T(x_1, \ldots, x_i,\ldots,x_k)$ sont linéaires pour tout $i$ dans $\{1,\ldots,k\}$, c'est-à-dire
	\begin{equation}
		\begin{aligned}[]
			T(\cdot,x_2, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_1}, \eR^p),\\
			T(x_1,\cdot, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_2}, \eR^p),\\
						& \vdots\\
			T(x_1, \ldots, x_i,\ldots,x_{k-1},\cdot)&\in \mathcal{L}(\eR^{m_k}, \eR^p).\\
		\end{aligned}
	\end{equation}
	En particulier lorsque $k=2$, nous parlons d'applications \defe{bilinéaires}{bilinéaire}. Vous pouvez deviner ce que sont les applications \emph{tri}linéaire ou \emph{quadri}linéaire.
\end{definition}

L'ensemble des applications $k$-linéaires de $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ dans $\eR^p$ est noté $\mathcal{L}(\eR^{m_1}\times \ldots \times\eR^{m_k}, \eR^p)$ ou $\mathcal{L}(\eR^{m_1}, \ldots,\eR^{m_k}; \eR^p)$.

\begin{example}
  Soit $A$ une matrice avec $m$ lignes et $n$ colonnes. L'application bilinéaire de $\eR^m\times \eR^n$ dans $\eR$ associée à $A$ est définie par
\[
T_A(x,y)= x^TAy=\sum_{i,j}a_{i,j}x_i y_j, \qquad \forall x\in \eR^m, \, y \in \eR^n.
\]
\end{example}

Nous énonçons la proposition suivante dans le cas d'espaces vectoriels normés\footnote{Sans hypothèses sur la dimension.} parce que nous allons l'utiliser dans ce cas, mais le cas particulier \( E_i=\eR^{m_i}\) et \( F=\eR^p\) est important.
\begin{proposition} \label{PropUADlSMg}
    Soient des espaces vectoriels normés \( E_i\) et \( F\). Une application \( n\)-linéaire
    \begin{equation}
        T\colon E_1\times\ldots\times E_n\to F
    \end{equation}
    est est continue si et seulement si il existe un réel $L\geq 0$ tel que
  \begin{equation}\label{limitatezza}
     \|T(x_1, \ldots,x_n)\|_F\leq L \|x_1\|_{F_1}\cdots\|x_n\|_{F_n}, \qquad \forall x_i\in E_i.
  \end{equation}
\end{proposition}

\begin{proof}
    Pour simplifier l'exposition nous nous limitons au cas $n=2$ et nous notons $T(x,y)=x*y$

    Supposons que l'inégalité \eqref{limitatezza} soit satisfaite.
    \begin{equation}\label{LimImplCont}
      \begin{aligned}
        \|x*y-x_0*y_0\|&=\|(x-x_0)*y-x_0*(y-y_0)\|\\
    &\leq \|(x-x_0)*y\|+\|x_0*(y-y_0)\|\\
    &\leq L\|x-x_0\|\|y\| + L\|x_0\|\|y-y_0\|.
      \end{aligned}
    \end{equation}
    Si $x\to x_0$ et $y\to y_0$  on voit que $T$ est continue en passant à la limite aux deux côtés de l'inégalité \eqref{LimImplCont}.

    Soit $T$ continue en $(0,0)$. Évidemment\footnote{Dans la formule suivante, les trois zéros sont les zéros de trois espaces différents.} $0*0=0$, donc il existe $\delta>0$ tel que si $x\in B_{E_1}(0,\delta)$ et $y\in B_{E_2}(0,\delta)$ alors $\|x*y\|\leq 1$. En particulier si \( (x,y)\in B_{E_1\times E_2}(0,\delta)\) nous sommes dans ce cas. Soient maintenant  $x\in E_1\setminus\{ 0 \}$  et $y\in E_2\setminus\{ 0\}$
    \begin{equation}
        x*y=\left(\frac{\|x\|}{\delta}\frac{\delta x}{\|x\|}\right)*\left(\frac{\|y\|}{\delta}\frac{\delta y}{\|y\|}\right)
    =\frac{\|x\|\|y\|}{\delta^2} \left(\frac{\delta x}{\|x\|}\right)*\left(\frac{\delta y}{\|y\|}\right).
     \end{equation}
    On remarque que $\delta x/\|x\|_m$ est dans la boule de rayon $\delta$ centrée en $0_m$ et que $\delta y/\|y\|_n$ est dans la boule de rayon $\delta$ centrée en $0_n$. On conclut
    \[
     x*y\leq \frac{\|x\|_m\|y\|_n}{\delta^2}.
    \]
    Il faut prendre $L=1/\delta^2$.
\end{proof}

La norme de \( T\) est alors définie comme la plus petite constante \( L\) qui fait fonctionner la proposition~\ref{PropUADlSMg}.
\begin{definition}  \label{DefKPBYeyG}
	La norme sur l'espace $\aL(E_1\times \cdots\times E_n, F)$ des applications $k$-linéaires et continues est
	\begin{equation}
        \|T\|_{E_1\times \ldots\times E_n}=\sup\{ \|T(u_1, \ldots,u_k)\|_{F}\,\vert\,\|u_i\|_{E_i}\leq 1, i=1,\ldots, k \}.
	\end{equation}
\end{definition}
Nous avons donc automatiquement
\begin{equation}    \label{EqYLnbRbC}
    \| T(u,v) \|\leq \| T \|\| u \|\| v \|.
\end{equation}
Et nous notons que cette norme est uniquement définie pour les applications linéaires continues. Ce n'est pas très grave parce qu'alors nous définissons \( \| T \|=\infty\) si \( T\) n'est pas continue. Cela pour retrouver le principe selon lequel on est continue si et seulement si on est borné.

\begin{proposition}\label{isom_isom}
  On définit les fonctions
  \begin{equation}
    \begin{array}{rccc}
      \omega_g: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{m}, \mathcal{L}(\eR^{n}, \eR^p)),\\
      \omega_d: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{n}, \mathcal{L}(\eR^{m}, \eR^p)),
    \end{array}
  \end{equation}
par
\[
\omega_g(T)(x)=T(x,\cdot), \qquad \forall x\in\eR^m,
\]
et
\[
\omega_d(T)(y)=T(\cdot, y), \qquad \forall y\in\eR^n.
\]
Les fonctions $\omega_g$ et $\omega_d$ sont des isomorphismes qui préservent les normes.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Séries}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooYCQBooSZNXhd}

Pour une somme indexée par un ensemble infini, nous aurons la définition plus générale \ref{DefIkoheE}.
\begin{definition}\label{DefGFHAaOL}
    Soit \( (a_k)\) une suite dans un espace vectoriel normé \( (V,\| . \| )\). La suite des \defe{sommes partielles}{somme!partielle} associée est la suite \( (s_k)\) définie par
    \begin{equation}
        s_k=\sum_{i=0}^ka_i
    \end{equation}
    La \defe{série}{série!dans un espace vectoriel normé} associée est la limite des sommes partielles
    \begin{equation}
        \sum_{n=0}^{\infty}a_k=\lim_{k\to \infty} \sum_{k=0}^na_k
    \end{equation}
    si elle existe.

    Si une telle limite existe nous disons que \( \sum_{k=0}^{\infty}a_k\) \defe{converge}{série convergente} dans \( V\). Si la limite de la suite des sommes partielles n'existe pas nous disons que la série \defe{diverge}{série divergente}.
\end{definition}

\begin{remark}
    Si la limite de la suite des sommes partielles n'existe pas dans \( V\), alors elle peut parfois exister dans des extensions de \( V\). Par exemple une série de rationnels convergeant vers \( \sqrt{2}\) dans \( \eR\) ne converge pas dans \( \eQ\). Autre exemple : avec une bonne topologie sur \( \bar \eR\), une série peut ne pas converger dans \( \eR\) mais converger vers \( \pm\infty\) dans \( \bar \eR\).
\end{remark}

Dans le cas des espaces de fonctions, nous avons une norme importante : la norme uniforme définie par \( \| f \|_{\infty}=\sup\{ f(x) \}\) où le supremum est pris sur l'ensemble de définition de \( f\).

\begin{lemma}       \label{LEMooHUZEooSyPipb}
    Soit une suite \( (a_k)\) dans un espace métrique complet\footnote{Définition \ref{DEFooHBAVooKmqerL}.} dont la série converge.
    
    \begin{enumerate}
        \item   \label{ITEMooPFSQooDhKFGL}
            Pour tout \( N\) nous avons
            \begin{equation}
                \sum_{k=0}^{\infty}a_k=\sum_{k=0}^Na_k+\sum_{k=N+1}^{\infty}a_k.
            \end{equation}
        \item       \label{ITEMooQNHMooUPjupB}
            La suite des queues de série converge vers \( 0\), c'est-à-dire que
            \begin{equation}
                \lim_{N\to \infty} \sum_{k=N}^{\infty}a_k=0.
            \end{equation}
    \end{enumerate}
\end{lemma}

\begin{proof}
    Voici un petit calcul :
    \begin{subequations}
        \begin{align}
            \lim_{n\to \infty} \sum_{k=0}^na_k&=\lim_{n\to \infty} \big( \sum_{k=0}^Na_k+\sum_{k=N+1}^{n}a_k \big)      \label{SUBEQooZRSHooSjismK}\\
            &=\lim_{n\to \infty} \sum_{k=0}^{N}a_k+\lim_{n\to \infty} \sum_{k=N+1}^{n}a_k       \label{SUBEQooTLVKooQfYXam}\\
            &=\sum_{k=0}^Na_k+\sum_{k=N+1}^{\infty}a_k.
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooZRSHooSjismK}. Pour chaque \( n\), la somme est finie et nous pouvons la décomposer. Si vous voulez vraiment couper les cheveux en quatre, vous devez fixer un \( \epsilon\), et un \( n\) de telle sorte à avoir \( n>N\), parce que \( N\) est fixé dans l'énoncé du lemme.
        \item Pour \eqref{SUBEQooTLVKooQfYXam}. Nous sommes dans un cas \( \lim_{n\to \infty}(u_n+v_n) \) où \( (u_n)\) est constante et où \( (u_n+v_n)\) converge. Nous pouvons donc permuter limite et somme\footnote{Pour rappel, la proposition \ref{PROPooICZMooGfLdPc} demande la convergence des deux suites pour fonctionner.}.
    \end{itemize}
    Voila que \ref{ITEMooPFSQooDhKFGL} est prouvé.

    Nous écrivons \( s_n=\sum_{k=0}^na_k\); l'hypothèse est que la suite \( (s_n)\) est une suite convergente dans un espace métrique. Elle est donc de Cauchy par la proposition \ref{PROPooZZNWooHghltd}.

    Soit \( \epsilon>0\). Il existe \( N\in \eN\) tel que pour tout \( p,q>N\), nous ayons \( \| s_p-s_q \|\leq \epsilon\). Soit \( p>N\). Pour tout \( n\geq 0\) nous avons
    \begin{equation}
        \epsilon>\| s_{p+n}-s_{p+1} \|=\| \sum_{k=p}^{p+n}a_k \|.
    \end{equation}
    En prenant la limite \( n\to \infty\) nous avons
    \begin{equation}
        \| \sum_{k=p}^{\infty}a_k \|\leq \epsilon.
    \end{equation}
    Nous avons donc démontré qu'il existe \( N\) tel que si \( p>N\), alors \( \| \sum_{k=p}^{\infty}a_k \|\leq \epsilon\). Cela signifie exactement que \( \lim_{n\to \infty} \sum_{k=n}^{\infty}a_k=0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les trois types de convergence}
%---------------------------------------------------------------------------------------------------------------------------

Trois notions de convergence à ne pas confondre :
\begin{enumerate}
    \item
        La convergence absolue,
    \item
        la convergence normale. C'est la même que la convergence absolue, mais dans le cas particulier d'un espace de fonctions muni de la norme uniforme.
    \item
        la convergence uniforme.
\end{enumerate}
Voici les définitions.


\begin{definition}[Convergence absolue] \label{DefVFUIXwU}
    Nous disons que la série \( \sum_{n=0}^{\infty}a_n\) dans l'espace vectoriel normé \( V\) \defe{converge absolument}{convergence absolue} si la série \( \sum_{n=0}^{\infty}\| a_n \|\) converge dans \( \eR\).
\end{definition}

\begin{definition}[Convergence normale] \label{DefVBrJUxo}
    Une série de fonctions \( \sum_{n\in \eN}u_n \) converge \defe{normalement}{convergence normale} si la série de nombres \( \sum_n\| u_n \|_{\infty}\) converge. C'est-à-dire si la série converge absolument pour la norme \( \| f \|_{\infty}\).
\end{definition}


\begin{definition}[Convergence uniforme]
    La somme \( \sum_nf_n\) \defe{converge uniformément}{convergence uniforme!série de fonctions} vers la fonction \( F\) si la suite des sommes partielles converge uniformément, c'est-à-dire si
    \begin{equation}        \label{EqLNCJooVCTiIw}
        \lim_{N\to \infty} \| \sum_{n=1}^Nf_n-F \|_{\infty}=0.
    \end{equation}
\end{definition}

\begin{proposition} \label{PropAKCusNM}
    Une série convergeant absolument dans un espace de Banach\footnote{Un espace vectoriel normé complet. Typiquement \( \eR\).} y converge au sens usuel.
\end{proposition}

\begin{proof}
    Soit \( (a_k)\) une suite dans un espace vectoriel normé complet dont la série converge absolument. Nous allons montrer que la suite des sommes partielles est de Cauchy. Cela suffira à montrer sa convergence par hypothèse de complétude.

    Nous avons
    \begin{equation}
        \| s_p-s_l \|=\| \sum_{k=l+1}^{p}a_k\|  \leq\sum_{k=l+1}^p\| a_k \|=\bar s_p-\bar s_l
    \end{equation}
    où \( \bar s_n=\sum_{k=0}^n \| a_k \|\) est la suite des sommes partielles de la série des normes (qui converge). Vu que la suite \( (\bar s_n)\) converge dans \( \eR\), elle y est de Cauchy par la proposition~\ref{PROPooTFVOooFoSHPg}. Donc il existe un \( N\) tel que \( p,l>N\) implique
    \begin{equation}
        \| s_p-s_l \|=\bar s_p-\bar s_l\leq \epsilon.
    \end{equation}
    Cela signifie que \( (s_n)\) est une suite de Cauchy et donc convergente.
\end{proof}

\begin{example}[Si l'espace n'est pas complet\cite{MonCerveau}]
    Dans un espace pas complet, il est possible de construire un série qui converge absolument sans converger au sens usuel.

    Nous allons trouver dans \( \eQ\) une série qui converge simplement vers \( \sqrt{ 2 }\) (et donc ne converge pas dans \( \eQ\)) mais absolument vers \( 4\).

    La base est que si \( A,B\in \eQ\) avec \( A<B\) il est possible de résoudre
    \begin{subequations}
        \begin{numcases}{}
            r_1+r_2=A\\
            | r_1 |+| r_2 |=B
        \end{numcases}
    \end{subequations}
    pour \( r_1,r_2\in \eQ\). Ce n'est pas très compliqué : la solution est \( r_1=(A+B)/2\) et \( r_2=(A-B)/2\).

    Nous considérons l'espace \( \eQ\) qui n'est pas complet dans \( \eR\). Soit une série \( (a_k)\) dans \( \eQ\) qui converge vers \( \sqrt{ 2 }\) (convergence dans \( \eR\)) nous nommons \( (s_k)\) la suite des ses sommes partielles. Soit aussi la suite \( (b_k)\) qui converge vers \( 4\) (zéro serait encore plus facile mais bon, juste pour faire un peu de généralité).

    Nous supposons que \( a_k<b_k\) pour tout \( k\) et que les deux suites sont constituées de rationnels positifs. Nous nommons \( (s_k)\) et \( (s'_k)\) les sommes partielles. En particulier \( s_n<s'_n\) et ce sont des suites croissantes.

    Nous savons comment trouver \( r_1,r_2\in \eQ\) tels que \( r_1+r_2=s_1\) et \( | r_1 |+| r_2 |=s'_1\). Par récurrence, si nous savons \( r_1,\ldots, r_k\) tels que
    \begin{subequations}
        \begin{numcases}{}
            r_1+\ldots +r_k=s_n\\
            |r_1|+\ldots +|r_k|=s'_n
        \end{numcases}
    \end{subequations}
    (avec, soit dit en passant \( k=2n\)), alors nous pouvons trouver des rationnels \( r_{k+1}\), \( r_{k+2}\) tels que
    \begin{subequations}
        \begin{numcases}{}
            r_1+\ldots +r_k+r_{k+1}+r_{k+2}=s_{n+1}\\
            |r_1|+\ldots +|r_k|+|r_{k+1}|+|r_{k+2}|=s'_{n+1},
        \end{numcases}
    \end{subequations}
    en effet il s'agit de résoudre
    \begin{subequations}
        \begin{numcases}{}
            r_{k+1}+r_{k+2}=s_{n+1}-r_1-\ldots-r_k=s_{n+1}-s_n>0\\
            | r_{k+1} |+| r_{k+2} |=s'_{n+1}-| r_1 | -\ldots -| r_k |=s'_{n+1}-s'_n>0.
        \end{numcases}
    \end{subequations}
    Cela se résout comme plus haut. Au final nous pouvons construire une suite \( (r_k)\) dans \( \eQ\) telle que
    \begin{equation}
        \sum_{k=0}^{2n}r_k=s_n
    \end{equation}
    et
    \begin{equation}
        \sum_{k=0}^{2n}| r_k |=s'_n.
    \end{equation}
\end{example}

\begin{remark}
    Nous savons que sur les espaces vectoriels de dimension finie toutes les normes sont équivalentes (théorème~\ref{DefEquivNorm}). La notion de convergence de série ne dépend alors pas du choix de la norme. Il n'en est pas de même sur les espaces de dimension infinie. Une série peut converger pour une norme mais pas pour une autre.
\end{remark}
Lorsque nous verrons la convergence de séries, nous verrons que la convergence normale est la convergence absolue pour la norme uniforme.

\begin{lemma}       \label{LemCAIPooPMNbXg}
    Si \( E\) et \( F\) sont des espaces de Banach\quext{Je crois qu'il ne faut pas que \( E\) soit complet.}, l'espace \( \aL(E,F)\) est également de Banach.
\end{lemma}

\begin{proof}
    Soit \( (u_n)\) une suite de Cauchy dans \( \aL(E,F)\); si \( x\in E\) il existe \( N\) tel que si \( l,m>N\) alors \( \| u_l-u_m \|<\epsilon\), c'est-à-dire que pour tout \( \| x \|=1\) on a \( \| u_l(x)-u_n(x) \|<\epsilon\). Cela signifie que \( u_n(x)\) est une suite de Cauchy dans l'espace complet \( F\). Cette suite converge et nous pouvons définir l'application \( u\colon E\to F\) par
    \begin{equation}
        u(x)=\lim_{n\to \infty} u_n(x).
    \end{equation}
    Il suffit maintenant de prouver que \( u\) est linéaire, ce qui est une conséquence directe de la linéarité de la limite :
    \begin{equation}
        u(\alpha x+\beta y)=\lim_{n\to \infty} \big( \alpha u_n(x)+\beta u_n(y) \big).
    \end{equation}
\end{proof}

\begin{proposition}  \label{PROPooYDFUooTGnYQg}
    Si une série converge dans un espace complet, la norme de son terme général converge vers $0$.
\end{proposition}

\begin{proof}
    Soit une suite \( (a_n)\) dont la série converge vers \( s\). Soit \( \epsilon>0\). La suite des sommes partielles \( (s_n)\) est de Cauchy et converge vers \( s\) : \( s_n\to s\). En particulier il existe un \( N\) tel que si \( n>N\), nous avons \( \| s_n-s_{n-1} \|<\epsilon\). Pour de telles valeurs de \( n\) nous avons :
    \begin{equation}
        \| a_n \|=\| s_n-s_{n-1} \|\leq \epsilon.
    \end{equation}
    Cela prouve que \( a_n\to 0\).
\end{proof}

Dans le même ordre d'idée nous avons la convergence des queues de suites.

\begin{lemma}       \label{LEMooFUCOooCOqLRj}
    Si \( \sum_{k=0}^{\infty}a_k\) est finie, alors
    \begin{equation}
        \lim_{n\to \infty} \sum_{k=n}^{\infty}a_k=0.
    \end{equation}
\end{lemma}

\begin{proposition}     \label{PROPooUEBWooUQBQvP}
    Si la série converge alors la somme est associative :
    \( \sum_k (a_k+b_k) = \sum_k a_k + \sum_k b_k \).
\end{proposition}

\begin{proof}
    Associativité. Supposons que \( \sum_ka_k\) et \( \sum_kb_k\) convergent tous deux. Alors nous avons pour tout \( N\) :
    \begin{equation}
        \sum_{k=0}^N(a_k+b_k)=\sum_{k=0}^Na_k+\sum_{k=0}^Nb_k.
    \end{equation}
    Mais si deux limites existent alors la somme commute avec la limite. C'est le cas pour la limite \( N\to \infty\), donc
    \begin{equation}
        \lim_{N\to \infty} \sum_{k=1}^{\infty}(a_k+b_k)=\lim_{N\to \infty} \sum_{k=0}^{\infty}a_k+\lim_{N\to \infty} \sum_{k=0}^{\infty}b_k.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Séries dans une algèbre normée}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons parler d'exponentielle de matrice. Avant cela, quelques propriétés qui sont valables sur des algèbres normées. Le principal exemple que nous avons en tête est \( \eA=\eM(n,\eC)\).

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooMZZQooEhQsgQ}
    Soit une algèbre normée \( \eA\). Soient une suite d'éléments \( A_k\in \eA\) et un élément \( B\). Nous supposons que la somme \( \sum_{k=0}^{\infty}A_k\) converge. Alors
    \begin{equation}
        B\sum_kA_k=\sum_k(BA_k).
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( N\in \eN\). Nous avons:
    \begin{subequations}
        \begin{align}
            \| \sum_{k=0}^NBA_k-B\sum_{k=0}^{\infty}A_k \|&=\| B\sum_{k=N+1}^{\infty}A_k \| \label{SUBEQooDTNAooWpXOKP}\\
            &\leq \| B \|\| \sum_{k=N+1}^{\infty}A_k \|     \label{SUBEQooJPSJooAqXtOJ}
        \end{align}
    \end{subequations}
    Justifications:
    \begin{itemize}
        \item Pour \eqref{SUBEQooDTNAooWpXOKP}, c'est la linéarité du produit matriciel.
        \item Pour \eqref{SUBEQooJPSJooAqXtOJ}, c'est que la norme est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}. Pour rappel, la norme opérateur en est une par le lemme \ref{LEMooFITMooBBBWGI}.}.
    \end{itemize}
    À droite, la limite \( N\to \infty\) donne zéro parce que \( \| B \|\) est un simple nombre, et \( \| \sum_{k=N+1}^{\infty}A_k \|\) est une queue de suite convergente par hypothèse.

    Nous avons donc bien convergence
    \begin{equation}
        \lim_{N\to \infty}\sum_{k=0}^{N}BA_k=B\sum_{k=0}^{\infty}A_k.
    \end{equation}
\end{proof}

\begin{proposition}[Produit de Cauchy dans une algèbre normée\cite{MonCerveau}]      \label{PROPooFMEXooCNjdhS}
    Soient une algèbre normée \( \eA\), un élément \( A\in \eA\), ainsi que des séries convergentes \( \sum_{k=0}^{\infty}a_kA^k\) et \( \sum_{l=0}^{\infty}b_lA^l\). Alors
    \begin{equation}
        \left( \sum_ka_kA^k \right)\left( \sum_lb_lA^l \right)=\sum_{n=0}^{\infty}\big( \sum_{m=0}^na_mb_{n-m} \big)A^n.
    \end{equation}
\end{proposition}

\begin{proof}
    Un calcul :
    \begin{subequations}
        \begin{align}
            \left( \sum_ka_kA^k \right)\left( \sum_lb_lA^l \right) &=\sum_k\big( \sum_lb_lA^l \big)a_kA^k       \label{SUBEQooFAECooWFCaNW}\\
            & = \sum_k\big( \sum_lb_la_kA^{l+k} \big)   \label{SUBEQooDZTHooMwmKxJ}\\
            &=\lim_{K\to\infty} \sum_{k=0}^K\big( \lim_{L\to \infty} \sum_{l=0}^Lb_la_kA^{k+l} \big)\\
            &=\lim_{K\to \infty} \lim_{L\to \infty} \sum_{k=0}^K\sum_{l=0}^Lb_la_kA^{k+l}       \label{SUBEQooISSHooJsyMTv}\\
            &=\lim_{K\to \infty} \lim_{L\to \infty} \sum_{n=0}^{K+L}\sum_{m=0}^na_mb_{n-m}A^n       \label{SUBEQooAWUQooZCHIXH}\\
            &=\lim_{K\to \infty} \sum_{n=0}^{\infty}\sum_{m=0}^na_mb_{n-m}A^m       \label{SUBEQooUVOBooSPGjrA}\\
            &=\sum_{n=0}^{\infty}\sum_{m=0}^na_mb_{n-m}A^m      \label{SUBEQooCGRGooGIDCYv}
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooFAECooWFCaNW}, la proposition \ref{PROPooMZZQooEhQsgQ} nous permet d'entrer l'élément \( \sum_lb_lA^l\in \eA\) dans la somme sur \( k\).
        \item 
            Pour \eqref{SUBEQooDZTHooMwmKxJ}, c'est la même chose.
        \item
            Pour \eqref{SUBEQooISSHooJsyMTv}, la somme sur \( k\) étant finie (pour chaque \( K\)), elle commute avec la limite sur \( L\).
        \item
            Pour \eqref{SUBEQooAWUQooZCHIXH}, c'est une manipulation de sommes finies. On regroupe les termes selon les puissances de \( A\).
        \item
            Pour \eqref{SUBEQooUVOBooSPGjrA}, c'est effectuer la limite sur \( L\) pour \( K\) fixé.
        \item
            Pour \eqref{SUBEQooCGRGooGIDCYv}, l'expression dans la limite sur \( K\) ne dépend pas de \( K\). Donc nous pouvons simplement supprimer la limite.
    \end{itemize}
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Série réelle}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{secseries}

La notion de série formalise le concept de somme infinie\footnote{La définition d'une somme infinie est la définition \ref{DefHYgkkA}.}. L'absence de certaines propriétés de ces objets (problèmes de commutativité et même d'associativité) incite à la prudence et montre à quel point une définition précise est importante.


\subsection{Critères de convergence absolue}

Étant donné le terme général d'une série, il est souvent --dans les cas qui nous intéressent-- difficile de déterminer la somme de la série. L'exemple de la série géométrique est particulier\footnote{Voir la proposition \ref{PROPooWOWQooWbzukS}.}, puisqu'on connaît une formule pour chaque somme partielle, mais pour l'exemple des séries de Riemann il n'y a aucune formule simple pour un $\alpha$ général. D'où l'intérêt d'avoir des critères de convergence ne nécessitant aucune connaissance de l'éventuelle limite de la série.

\begin{lemma}[Critère de comparaison]   \label{LemgHWyfG}
Soient $\sum_i a_i$ et $\sum_j
b_j$ deux séries à termes positifs vérifiant
\begin{equation*}
  0 \leq a_i \leq b_i
\end{equation*}
alors
\begin{enumerate}
\item si $\sum_i a_i$ diverge, alors $\sum_j b_j$ diverge,
\item si $\sum_j b_j$ converge, alors $\sum_i a_i$ converge
  (absolument).
  \end{enumerate}
\end{lemma}

\begin{proposition}[Critère d'équivalence\cite{TrenchRealAnalisys}]
 Soient $\sum_i a_i$ et $\sum_j b_j$ deux séries à termes positifs. Supposons l'existence de la limite (éventuellement infinie) suivante
\begin{equation}
  \limite i \infty \frac{a_i}{b_i} = \alpha
\end{equation}
avec \( \alpha\in \eR\cup\{ +\infty \}\). Alors
\begin{enumerate}
\item si $\alpha \neq 0$ et $\alpha\neq \infty$, alors
  \begin{equation}
    \sum_i a_i \text{~converge} \ssi \sum_j b_j\text{~converge,}
  \end{equation}
\item si $\alpha = 0$ et $\sum_j b_j$ converge, alors $\sum_i a_i$
  converge (absolument),
\item si $\alpha = +\infty$ et $\sum_j b_j$ diverge, alors $\sum_i
  a_i$ diverge.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
    \item
        Le fait que la suite $a_n/b_n$ converge vers $\alpha$ signifie que tant sa limite supérieure que sa limite inférieure convergent vers $\alpha$. En particulier la suite $\frac{ a_n }{ b_n }$ est bornée vers le haut et vers le bas. À partir d'un certain rang $N$, il existe $M$ tel que
        \begin{equation}
            \frac{ a_n }{ b_n }<M
        \end{equation}
        et il existe $m$ tel que
        \begin{equation}
            \frac{ a_n }{ b_n }>m.
        \end{equation}
        Nous avons donc $a_n<Mb_n$ et $a_n>mb_n$. La série de $(a_n)$ converge donc si et seulement si la série de $(b_n)$ converge.
    \item
        Si $\alpha=0$, cela signifie que pour tout $\epsilon$, il existe un rang tel que $\frac{ a_n }{ b_n }<\epsilon$, et donc tel que $a_n<\epsilon b_k$. La suite de $(a_i)$ converge donc dès que la suite de $(b_i)$ converge.
    \item
        Pour tout $M$, il existe un rang dans la suite à partir duquel on a $\frac{ a_i }{ b_i }>M$, et donc $a_k>Mb_k$. Si la série de $(b_k)$ diverge, la série de $(a_k)$ doit également diverger.
\end{enumerate}
\end{proof}

\begin{proposition}[Critère du quotient\cite{KeislerElemCalculus}]     \label{PropOXKUooQmAaJX}
    Soit $\sum_i a_i$ une série. Supposons l'existence de la limite (éventuellement infinie) suivante
    \begin{equation}
      \limite i \infty \abs{\frac{a_{i+1}}{a_i}} = L
    \end{equation}
    avec \( L\in \eR\cup\{ +\infty \}\).  Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L > 1$, la série diverge,
    \item si $L = 1$ le critère échoue : il existe des exemples de convergence et des exemples de divergence.
    \end{enumerate}
\end{proposition}
\index{critère du quotient}

\begin{proof}
\begin{enumerate}
    \item
        Soit $b$ tel que $L<b<1$. À partir d'un certain rang $K$, on a $\left| \frac{ a_{i+1} }{ a_i } \right| <b$. En particulier,
        \begin{equation}
            | a_{K+1} |<b| a_K |,
        \end{equation}
        et pour $a_{K+2}$ nous avons
        \begin{equation}
            | a_{K+2} |<b| a_{K+1} |<b^2| a_K |.
        \end{equation}
        Au final,
        \begin{equation}
            | a_{K+n} |<b^n| a_K |.
        \end{equation}
        Étant donné que la série $\sum_{n\geq K}b^n$ converge (parce que $b<1$), la queue de suite $\sum_{i\geq K}a_i$ converge, et par conséquent la suite au complet converge.
    \item
        Si $L>1$, on a
        \begin{equation}
            | a_K |<| a_{K+1} |<| a_{K+2} |<\ldots
        \end{equation}
        Il est donc impossible que la suite $(a_i)$ converge vers zéro. La série ne peut donc pas converger.
    \item
        Par exemple la suite harmonique $a_n=\frac{1}{ n }$ vérifie $L=1$, mais la série ne converge pas. Par contre, la suite $a_n=\frac{ 1 }{ n^2 }$ vérifie aussi le critère avec $L=1$ tandis que la série $\sum_n\frac{1}{ n^2 }$ converge.
\end{enumerate}
\end{proof}


\begin{proposition}[Critère de la racine\cite{TrenchRealAnalisys}]
    Soit $\sum_i a_i$ une série, et considérons
    \begin{equation*}
      \limsup_{i \rightarrow \infty} \sqrt[i]{\abs{a_i}} = L
    \end{equation*}
    avec \( L\in \eR\cup\{ +\infty \}\). Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L> 1$, la série diverge,
    \item si $L = 1$ le critère échoue.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si $L<1$, il existe un $r\in \mathopen] 0 , 1 \mathclose[$ tel que $| a_n |^{1/n}<r$ pour les grands $n$. Dans ce cas, $| a_n |<r^{n}$, et la série converge absolument parce que la série $\sum_nr^n$ converge du fait que $r<1$.
        \item
            Si $L>1$, il existe un $r>1$ tel que $| a_n |^{1/n}>r>1$. Cela fait que $| a_n |$ prend des valeurs plus grandes que $n$ pour une infinité de termes. Le terme général $a_n$ ne peut donc pas être une suite convergente. Par conséquent la suite diverge au sens où elle ne converge pas.

    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Critères de convergence simple}
%---------------------------------------------------------------------------------------------------------------------------

Les critères de comparaison, d'équivalence, du quotient et de la racine sont des critères de convergence absolue. Pour conclure à une convergence simple qui n'est pas une convergence absolue, le critère d'Abel sera notre outil principal.

\subsubsection{Critère d'Abel}

\begin{proposition}[Critère d'Abel]
    Soit la série $\sum_i c_iz_i$ avec
    \begin{enumerate}
        \item $(c_i)$ est une suite réelle décroissante qui tend vers zéro,
        \item $(z_i)$ est une suite dans $\eC$ dont la suite des sommes partielles est bornée dans $\eC$, c'est-à-dire qu'il existe un $M>0$ tel que pour tout $n$,
        \begin{equation}
            \left| \sum_{i=1}^nz_i \right| \leq M.
        \end{equation}
        Alors la série $\sum_ic_iz_i$ est convergente.
    \end{enumerate}
\end{proposition}
Remarquons que ce critère ne donne pas de convergence absolue.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques séries usuelles}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooDTYHooZjXXJf}

\begin{example}[Série harmonique]       \label{EXooDVQZooEZGoiG}
    La \defe{série harmonique}{série!harmonique} est
    \begin{equation}
        \sum_{i=k}^\infty \frac{1}{ k }=+\infty.
    \end{equation}
\end{example}

\begin{propositionDef}[Série géométrique]      \label{PROPooWOWQooWbzukS}
    La \defe{série géométrique}{série!géométrique} de raison $q \in \eC$ est
    \begin{equation}    \label{EqZQTGooIWEFxL} 
        \sum_{i=0}^\infty q^i.
    \end{equation}
    \begin{enumerate}
        \item
            Elle converge si et seulement si \( | q |<1\).
        \item       \label{ITEMooBJHBooBMEmiG}
            Si \( | q |<1\) alors
    \begin{equation}    \label{EqRGkBhrX}
        \sum_{n=0}^{\infty}q^n=\frac{ 1 }{ 1-q }.
    \end{equation}
\item
        Quand la série géométrique converge, la convergence est absolue.
    \item
        Si la somme commence en \( n=1\) au lieu de \( n=0\) alors
        \begin{equation}        \label{EqPZOWooMdSRvY}
            \sum_{n=1}^{\infty}q^n=\frac{ q }{ 1-q }.
        \end{equation}
    \end{enumerate}
\end{propositionDef}

\begin{proof}
    La somme partielle est déjà donnée dans le lemme \ref{LEMooAFSCooWEVlvp} :
    \begin{equation}
        S_N=\sum_{n=0}^Nq^n=\frac{ 1-q^{N+1} }{ 1-q }.
    \end{equation}
    En vertu de \eqref{EQooATTQooRpJeCo}, la limite \( \lim_{N\to \infty} S_N\) existe si et seulement si \( | q |\leq 1\) et dans ce cas nous avons le résultat parce que \( q^{N+1}\to 0\).

    Pour le dernier point, il s'agit seulement du calcul
    \begin{equation}
        \sum_{n=1}^{\infty}q^n=\frac{1}{ 1-q }-1=\frac{ q }{ 1-q }.
    \end{equation}
\end{proof}



Un cas particulier de la formule \eqref{EqASYTiCK} est le calcul de \( \sum_{j=1}^{N}q^{-j}\) bien utile lorsque l'on joue avec des nombres binaires (voir l'exemple~\ref{EXEMooRHENooGwumoA}). Nous avons
\begin{equation}        \label{EQooFMBAooEJkHWT}
    \sum_{j=1}^Nq^{-j}=\sum_{j=0}^Nq^{-j}-1=\frac{ 1-q^{-N} }{ q-1 }.
\end{equation}

La série de Riemann est très liée aux intégrales impropres de la proposition \ref{PropBKNooPDIPUc}.
\begin{proposition}[Série de Riemann] \label{PROPooFPVZooGnsqrs}      \label{EXooCTYNooCjYQvJ}
    Pour $\alpha \in \eR$, la \defe{série de Riemann}{série!Riemann}
    \begin{equation}        \label{EqSerRiem}
        \sum_{k=1}^\infty \frac{ 1 }{ k^{\alpha}}
    \end{equation}
    converge (absolument, puisque réelle et positive) si et seulement si $\alpha > 1$, et diverge sinon.
\end{proposition}

\begin{example}[Série exponentielle] \label{ExIJMHooOEUKfj}
    La série exponentielle est la série (pour \( t\in \eR\))
    \begin{equation}
        \exp(t)=\sum_{k=0}^{\infty}\frac{ t^k }{ k! }.
    \end{equation}
    Nous montrons qu'elle converge pour tout \( t\in \eR\). Si \( a_k=t^k/k!\) alors \( \frac{ a_{k+1} }{ a_k }=\frac{ t }{ k }\) dont la limite \( k\to \infty\) est zéro (quel que soit \( t\)). En vertu du critère du quotient~\ref{PropOXKUooQmAaJX} la série exponentielle converge (absolument) pour tout \( t\in \eR\).

    Pour tout savoir de l'exponentielle et de ses variations, voir le thème~\ref{THEMEooKXSGooCsQNoY}.
\end{example}
\index{exponentielle!convergence}

\begin{example}[Série arithmético-géométrique\cite{QXuqdoo}]
    Une \defe{suite arithmético-géométrique}{suite!arithmético-géométrique} est une suite vérifiant pour tout \( n\) la relation
    \begin{equation}
        u_{n+1}=au_n+b
    \end{equation}
    avec \( a\) et \( b\) non nuls. Si elle possède une limite, cette dernière doit résoudre \( l=al+b\), et donc être donnée par
    \begin{equation}
        l=\frac{ b }{ 1-a }.
    \end{equation}

    Comportement amusant : la limite peut exister pour certains valeurs de \( a_0\) et pas pour d'autres. Mais elle ne dépend pas de \( a_0\) parmi ceux pour lesquelles la limite existe.

    Il n'est pas très compliqué de trouver le terme général de la suite en fonction de \( a\) et de \( b\). Il suffit de considérer la suite \( v_n=u_n-r\), et de remarquer que cette suite est géométrique :
    \begin{equation}
        v_{n+1}=av_n.
    \end{equation}
    Par conséquent \( v_n=a^nv_0\), ce qui donne pour la suite \( (u_n)\) la formule
    \begin{equation}
        u_n=a^n(u_0-r)+r.
    \end{equation}
\end{example}

\begin{lemma}[\cite{BIBooTIZHooGeFZri}]     \label{LEMooKDHPooPlFTIT}
    Nous avons :
    \begin{equation}
        \sum_{k=1}^N\frac{1}{ k(k+1) }=\frac{ N }{ N+1 }.
    \end{equation}
    et
    \begin{equation}
        \sum_{k=1}^{\infty}\frac{1}{ k(k+1) }=1.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons
    \begin{subequations}
        \begin{align}
            f(n)&=\sum_{k=1}^n\frac{1}{ k(k+1) }\\
            g(n)&=\frac{ n }{ n+1 }
        \end{align}
    \end{subequations}
    et nous montrons par récurrence que \( f(n)=g(n)\). Pour \( n=1\) nous avons \( f(1)=g(1)=\frac{ 1 }{2}\).

    Nous supposons que \( f(n)=g(n)\) et nous prouvons que \( f(n+1)=g(n+1)\). Facile :
    \begin{subequations}
        \begin{align}
            f(n+1)&=f(n)+\frac{1}{ (n+1)(n+2) }\\
            &=\frac{ n }{ n+1 }+\frac{1}{ (n+1)(n+2) }\\
            &=\frac{ n(n+2)+1 }{ (n+1)(n+2) }\\
            &=\frac{ n^2+2n+1 }{ (n+1)(n+2) }\\
            &=\frac{ (n+1)^2 }{ (n+1)(n+2) }\\
            &=\frac{ n+1 }{ n+2 }\\
            &=g(n+1).
        \end{align}
    \end{subequations}
    En ce qui concerne la seconde formule, par définition\footnote{Définition d'une série, \ref{DefGFHAaOL}.}
    \begin{equation}
        \sum_{k=1}^{\infty}\frac{1}{ k(k+1) }=\lim_{n\to \infty} \sum_{k=1}^n\frac{1}{ k(k+1) }=\lim_{n\to \infty}\frac{ n }{ n+1 } =1.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Séries alternées}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Critère des séries alternées\cite{ooXFPIooCLUvzV}]      \label{THOooOHANooHYfkII} 
    Si \( (a_n)_{n\in \eN}\) est une suite positive décroissante à limite nulle, alors
    \begin{enumerate}
        \item
            Si nous notons \( (S_n)\) la suite des sommes partielles, les sous-suites \( (S_{2n})\) et \( (S_{2n+1})\) sont adjacentes\footnote{Définition \ref{DEFooDMZLooDtNPmu}.}.
        \item
            La série \( \sum_n(-1)^na_n\) converge.
        \item       \label{ITEMooWEPWooXhLMYL}
            Si nous considérons le reste 
            \begin{equation}
                R_n=\sum_{k=n+1}^{\infty}(-1)^ka_k,
            \end{equation}
            nous avons
            \begin{subequations}
                \begin{align}
                    \signe(R_n)=(-1)^{n+1}\\
                    | R_n |\leq a_{n+1}.
                \end{align}
            \end{subequations}
    \end{enumerate}
\end{theorem}

\begin{proof}
    En termes de notations, nous allons écrire \( (S_n)\) la suite des sommes partielles de \( \sum_{k=0}^{\infty}(-1)^ka_k\). Nous notons \( (S_{2n})\) la suite des termes pairs de cette suite. C'est donc la suite \( n\mapsto S_{2n}\).
    Nous divisons en plusieurs morceaux.
    \begin{subproof}
        \item[\( S_{2n}\) est croissante]
            Nous avons simplement
            \begin{equation}
                S_{2n+2}-S_{2n}=a_{2n+2}-a_{2n+1}\leq 0.
            \end{equation}
        \item[\( (S_{2n+1})\) est décroissante]
            Même calcul.
        \item[Les suites \( (S_{2n})\) et \( S_{2n+1}\) sont adjacentes] Nous avons simplement
            \begin{equation}
                S_{2n+1}-S_{2n}=a_{2n+1}\to 0.
            \end{equation}
            Nous concluons par le théorème des suites adjacentes \ref{THOooZJWLooAtGMxD} que les sous-suites des termes pairs et impairs sont convergentes et convergent vers la même limite.
    \end{subproof}
    C'est le moment d'utiliser la proposition \ref{PROPooXOOCooGMqJNe} qui convaincra la lectrice que \( (S_n)\) converge vers la même limite, que nous notons \( S\). Le théorème des suites adjacentes nous dit encore que 
    \begin{equation}
        S_{2n+1}\leq S\leq S_{2n}
    \end{equation}
    et donc que \( R_{2n}=S-S_{2n}\leq 0\). Cela donne la majoration
    \begin{equation}
        | R_{2n} |=| S-S_n |=S_{2n}-S\leq S_{2n}-S_{2n+1}=a_{2n+1}.
    \end{equation}
    Nous faisons le même genre de majorations pour \( R_{2n+1}\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Moyenne de Cesaro}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( (a_n)_{n\in \eN} \) est une suite dans \( \eR\) ou \( \eC\), alors sa \defe{moyenne de Cesaro}{moyenne!de Cesaro}\index{Cesaro!moyenne} est la limite (si elle existe) de la suite
    \begin{equation}
        c_n=\frac{1}{ n }\sum_{k=1}^na_k.
    \end{equation}
    En un mot, c'est la limite des moyennes partielles.
\end{definition}

\begin{lemma}       \label{LemyGjMqM}
    Si la suite \( (a_n)\) converge vers la limite \( \ell\) alors la suite admet une moyenne de Cesaro qui vaudra \( \ell\).
\end{lemma}

\begin{proof}
    Soit \( \epsilon>0\) et \( N\in \eN\) tel que \( | a_n-\ell |<\epsilon\) pour tout \( n>N\). En remarquant que
    \begin{equation}
        \frac{1}{ n }\sum_{k=1}^nk-\ell=\frac{1}{ n }\sum_{k=1}^n(a_k-\ell),
    \end{equation}
    nous avons
    \begin{subequations}
        \begin{align}
            | \frac{1}{ n }\sum_{k=1}^na_k-\ell |&\leq| \frac{1}{ n }\sum_{k=1}^N| a_k-\ell | |+\big| \frac{1}{ n }\sum_{k=N+1}^n\underbrace{| a_k-\ell |}_{\leq \epsilon} \big|\\
            &\leq \epsilon+\frac{ n-N-1 }{ n }\epsilon\\
            &\leq 2\epsilon.
        \end{align}
    \end{subequations}
    Dans ce calcul nous avons redéfinit \( N\) de telle sorte que le premier terme soit inférieur à \( \epsilon\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Écriture décimale d'un réel}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons déjà vu la fonction \eqref{EQooWWTUooHAnSEv} qui permet d'écrire des naturels dans une base \( b\geq 2\) donnée. Nous allons maintenant construire une fonction du même type, pour la partie décimale d'un réel.

\begin{normaltext}      \label{NORMALooTZWYooPMgOIm}
    Soit \( b\geq 2\) un entier qui sera la base dans laquelle nous allons écrire les nombres. Nous considérons l'ensemble \( \eD_b\)\nomenclature[Y]{\( \eD_b\)}{l'ensemble de écritures décimales en base \( b\)} des suites dans \( \{ 0,1,\ldots, b-1 \}\) qui n'ont pas une queue de suite uniquement formée de \( b-1\). Autrement dit une suite \( (c_n)\) est dans \( \eD_b\) lorsque pour tout \( N\), il existe \( k>N\) tel que \( c_k\neq b-1\). Associé à cet ensemble nous considérons la fonction
    \begin{equation}    \label{EqXXXooOTsCK}
        \begin{aligned}
            \varphi_b\colon \eD_b&\to \mathopen[ 0 , 1 [ \\
                c&\mapsto \sum_{n=1}^{\infty}\frac{ c_n }{ b^n }.
        \end{aligned}
    \end{equation}
\end{normaltext}

\begin{lemma}
    La fonction \( \varphi_b\) est bien définie au sens où elle converge et prend ses valeurs dans \( \mathopen[ 0 , 1 [\).
\end{lemma}

\begin{proof}
    Tout se base sur la somme de la série géométrique \eqref{EqRGkBhrX} sous la forme
    \begin{equation}    \label{EqWZGooXJgwl}
        \sum_{k=0}^{\infty}\frac{1}{ b^k }=\frac{ b }{ b-1 }.
    \end{equation}
    La somme \eqref{EqXXXooOTsCK} est donc majorée par \( \sum_n\frac{ b-1 }{ b^n }\) qui converge.

    Pour prouver que l'image de \( \varphi_b\) est bien \( \mathopen[ 0 , 1 [\), nous savons qu'au moins un des \( c_n\) (en fait une infinité) est plus petit que \( b-1\), donc nous avons la majoration stricte\footnote{Notez que la somme \eqref{EqXXXooOTsCK} commence à un tandis que la série géométrique \eqref{EqWZGooXJgwl} commence à zéro.}
        \begin{equation}
            \varphi_b(c)<\sum_{n=1}^{\infty}\frac{ b-1 }{ b^n }=(b-1)\left( \sum_{n=1}^{\infty}\frac{1}{ b^n }-1 \right)=1
        \end{equation}
\end{proof}

Le fait d'introduire l'ensemble \( \eD\) au lieu de l'ensemble de toutes les suites est justifié par la proposition suivante. Elle explique pourquoi un nombre possède au maximum deux écritures décimales distinctes et que ces deux sont obligatoirement de la forme, par exemple en base \( 10\) :
\begin{equation}
    0.34599999999\ldots=0.34600000\ldots
\end{equation}
mais qu'un nombre commençant par \( 0.347\) ne peut pas être égal. C'est pour cela que dans la définition de \( \eD_b\) nous avons exclu les suites qui terminent par tout des \( b-1\).

La proposition suivante complète ce qui est déjà dit dans le lemme \ref{LEMooIQBXooUEtdoy}.
\begin{proposition} \label{PropSAOoofRlQR}
    Soit la fonction
    \begin{equation}
        \begin{aligned}
            \varphi\colon \{ 0,\ldots, b-1 \}^{\eN}&\to \mathopen[ 0 , 1 [ \\
                x&\mapsto \sum_{n=1}^{\infty}\frac{ x_n }{ b^n }.
        \end{aligned}
    \end{equation}
    Si \( \varphi(x)=\varphi(y)\) et si \( n_0\) est le plus petit entier tel que \( x_{n_0}\neq y_{n_0}\) alors soit
    \begin{equation}
        x_{n_0}-y_{n_0}=1
    \end{equation}
    et \( x_n=0\), \( y_n=b-1\) pour tout \( n>n_0\), soit le contraire : \( y_{n_0}-x_{n_0}=1\) avec \( y_n=0\) et \( x_n=b-1\) pour tout \( n>n_0\).
\end{proposition}

\begin{proof}
    Nous nous basons sur la formule (facilement dérivable depuis \eqref{EqWZGooXJgwl}) suivante :
    \begin{equation}
        \sum_{k=n_0+1}^{\infty}\frac{1}{ b^k }=\frac{1}{ b^{n_0+1} }\frac{ b }{ b-1 }.
    \end{equation}
    Nous avons
    \begin{equation}
        0=\varphi(x)-\varphi(y)=\frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }\geq \frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }-\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{ x_{n_0}-y_{n_0}-1 }{ b^{n_0} }.
    \end{equation}
    Le dernier terme étant manifestement positif\footnote{C'est ici qu'intervient la subdivision entre le cas \( x_{n_0}-y_{n_0}=1\) ou le contraire. En effet si «ce dernier terme était manifestement \emph{négatif}», il aurait fallu majorer avec de \( 1-b\) au lieu de \( 1-b\).}, il est nul et nous avons \( x_{n_0}-y_{n_0}=1\).

    Nous avons donc maintenant
    \begin{equation}    \label{EqHWQoottPnb}
        0=\varphi(x)-\varphi(y)=\frac{1}{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }.
    \end{equation}
    Nous majorons la dernière somme de la façon suivante, en supposant que \( | x_n-y_n |\neq b-1\) pour un certain \( n>n_0\) :
    \begin{equation}
        \left| \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n } \right| \leq\sum_{n=n_0+1}^{\infty}\frac{ | x_n-y_n | }{ b^n }<\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{1}{ b^{n_0} }.
    \end{equation}
    Étant donné cette inégalité stricte, l'équation \eqref{EqHWQoottPnb} ne peut pas être correcte (valoir zéro). Nous avons donc \( | x_n-b_n |=b-1\) pour tout \( n>n_0\). Donc pour chaque \( n>n_0\) nous avons soit \( x_n=0\) et \( y_n=b-1\), soit \( a_n=b-1\) et \( b_n=0\). Pour conclure il faut encore prouver que le choix doit être le même pour tout \( n\).

    Nous nous mettons dans le cas \( x_{n_0}-y_{n_0}=1\); dans ce cas nous avons bien l'égalité \eqref{EqHWQoottPnb} sans petites nuances de signes. Nous écrivons
    \begin{equation}
        \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }=(b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }
    \end{equation}
    où \( s_n\) est pair ou impair suivant que \( x_n=0\), \( y_n=b-1\) ou le contraire. Si un des \( (-1)^{s_n}\) est pas \( -1\) alors nous avons l'inégalité stricte
    \begin{equation}
        (b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }>(b-1)\sum_{n=n_0+1}^{\infty}\frac{-1}{ b^n }=-\frac{1}{ b^{n_0} }.
    \end{equation}
    Dans ce cas il est impossible d'avoir \( \varphi(x)-\varphi(y)=0\). Nous en concluons que \( (-1)^{s_n}\) est toujours \( -1\), c'est-à-dire \( x_n-y_n=1-b\), ce qui laisse comme seule possibilité \( x_n=0\) et \( y_n=b-1\).
\end{proof}

\begin{theorem} \label{ThoRXBootpUpd}
    L'application \( \varphi_b\colon \eD_b\to \mathopen[ 0 , 1 [\) est bijective.
\end{theorem}

\begin{proof}
    En ce qui concerne l'injection, nous savons de la proposition~\ref{PropSAOoofRlQR} que si \( \varphi_b(x)=\varphi_b(y)\) pour \( x,y\in\{ 0,\ldots, b-1 \}^{\eN}\), alors soit \( x\) soit \( y\) a une queue de suite composée uniquement de \( b-1\), ce qui est exclu dans \( \eD_b\). Nous en déduisons que \( \varphi_b\) est bien injective en prenant \( \eD_b\) comme ensemble départ.

    La partie lourde est la surjectivité. Nous prenons \( x\in \mathopen[ 0 , 1 [\) et nous allons construire par récurrence une suite \( a\in \eD_b\) telle que \( \varphi_b(a)=x\). Si il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que \( x=a_1/b\) alors nous prenons la suite \( (a_1,0,\ldots, )\) et nous avons évidemment \( \varphi(a)=x\). Sinon il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que
        \begin{equation}
            \frac{ a_1 }{ b }<x<\frac{ a_1+1 }{ b }
        \end{equation}
        parce que les autres possibilités pour \( x\) sont dans l'ensemble \( \mathopen[ 0 , 1 \mathclose[\setminus\{ \frac{ k }{ b } \}_{k=0,\ldots, b-1}\) que nous subdivisons en
        \begin{equation}
        \mathopen] 0 , \frac{1}{ b } \mathclose[\cup\mathopen] \frac{1}{ b } , \frac{ 2 }{ b } \mathclose[\cup\ldots\cup\mathopen] \frac{ b-1 }{ b } , 1 \mathclose[.
        \end{equation}
        Pour la récurrence nous supposons avoir trouvé \( a_1,\ldots, a_n\) tels que
        \begin{equation}
            \sum_{k=1}^n\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n-1}\frac{ a_k }{ b^k }+\frac{ a_n+1 }{ b^n }.
        \end{equation}
    Encore une fois si il existe \( a_{n+1}\in\{ 0,\ldots, b-1 \}\) tel que \( \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }=x\) alors nous prenons ce \( a_{n+1}\) et nous complétons la suite avec des zéros pour avoir \( \varphi(a)=x\). Sinon
%nous subdivisions l'intervalle \( \mathopen]  \frac{ a_n }{ b^n }, \frac{ a_n }{ b^n }+\frac{ a_n+1 }{ b^n } \mathclose[\) (auquel nous retranchons les \( b\) nombres déjà traités) en
 %       \begin{equation}
 %       \mathopen] \frac{ a_n }{ b^n } , \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } \mathclose[ \cup \mathopen] \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{2}{ b^{n+1} } \mathclose[\cup\ldots\cup\mathopen] \frac{ a_n }{ b^n }+\frac{ b-1 }{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{ 1 }{ b^n } \mathclose[.
 %       \end{equation}
        , pour simplifier les notations nous notons \( x'=x-\sum_{k=1}^{n}\frac{ a_k }{ b^k }\) et nous avons
        \begin{equation}
            0<x'<\frac{ a_n+1 }{ b^n }.
        \end{equation}
        Le nombre \( x'\) est forcément dans un des intervalles
        \begin{equation}
                \mathopen] \frac{ s }{ b^{n+1} } , \frac{ s+1 }{ b^{n+1} } \mathclose[
        \end{equation}
        avec \( s\in\{ 0,\ldots, b-1 \}\). Nous prenons le \( s\) correspondant à \( x'\) comme \( a_{n+1}\). Dans ce cas nous avons
        \begin{equation}
            \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n+1}\frac{ a_k }{ b^k }+\frac{1}{ b^{n+1} }.
        \end{equation}
        Note : les deux inégalités sont strictes. La première parce que si il y avait égalité, nous nous serions déjà arrêté en complétant avec des zéros. La seconde parce que
        \begin{equation}
            \sum_{k=n+2}^{\infty}\frac{ a_k }{ b^k }\leq \sum_{k=n+2}^{\infty}\frac{ b-1 }{ b^k }=\frac{1}{ b^{n+1} }
        \end{equation}
        où l'égalité n'est possible que si \( a_k=b-1\) pour tout \( k\geq n+2\). Dans ce cas nous aurions eu
        \begin{equation}
            x=\sum_{k=1}^{n}\frac{ a_k }{ b^k }+\frac{ a_{n+1}+1 }{ b^{n+1} }
        \end{equation}
        et nous aurions choisi le nombre \( a_{n+1}\) autrement et complété la suite par des zéros à partir de là. Notons que cela prouve au passage que la suite que nous sommes en train de construire est bien dans \( \eD_b\) parce qu'elle ne contiendra pas de queue de suite composée de \( b-1\).

        Ceci termine la construction par récurrence de la suite \( a\in \eD_b\). Par construction nous avons pour tout \( N\geq 1\),
        \begin{equation}
            \sum_{k=1}^N\frac{ a_k }{ b^k }\leq x\leq \sum_{k=1}^N\frac{ a_k }{ b^k }+\frac{1}{ b^{N+1} },
        \end{equation}
        autrement dit : \( \varphi_b(a_1,\ldots, a_N)\in B(x,\frac{1}{ b^{N+1} })\). Nous avons donc bien convergence
        \begin{equation}
            \lim_{N\to \infty} \varphi_b(a_1,\ldots, a_N)=x
        \end{equation}
        et l'application \( \varphi_b\) est surjective.
\end{proof}

L'application \( \varphi_b^{-1}\colon \mathopen[ 0 , 1 [\to \eD_b\) est la \defe{décomposition décimale}{décomposition décimale} en base \( b\) des nombres de \( \mathopen[ 0 , 1 [\).

Tout cela nous permet de montrer entre autres que \( \eR\) n'est pas dénombrable. Vu qu'il y a une bijection entre \( \mathopen[ 0 , 1 [\) et \( \eD_b\), il suffit de prouver que \( \eD_b\) est non dénombrable. De plus il suffit de démontrer que \( \eD_b\) est non dénombrable pour un entier \( b\geq 2\) donné.

\begin{proposition}[\cite{KZIoofzFLV}]  \label{PropNNHooYTVFw}
    Il n'existe pas de surjection \( \eN\to \eD_b\). Autrement dit \( \eD_b\) est non dénombrable.
\end{proposition}

\begin{proof}
    Nous prenons \( b\neq 2\) pour des raisons qui seront claires plus tard. Soit \( f\colon \eN\to \eD_b\). Pour \( i\in \eN\) nous notons
    \begin{equation}
        f(n)=(c_i^{(n)})_{i\geq 1},
    \end{equation}
    et nous définissons la suite
    \begin{equation}
        c_k=\begin{cases}
            0    &   \text{si } c_k^{(k)}\neq 0\\
            1    &    \text{si } c_k^{(k)}=0.
        \end{cases}
    \end{equation}
    Cela est une suite dans \( \eD_b\) parce que \( b\neq 2\) et que la suite ne contient que des \( 0\) et des \( 1\). Mais nous n'avons \( f(n)=c\) pour aucun \( n\in \eN\) parce que nous avons \( c_n\neq f(n)_n\).

    Si \( b=2\) alors nous savons que \( \eD_2\sim\mathopen[ 0 , 1 [\sim \eD_3\). Donc \( \eD_2\sim \eD_3\) et \( \eD_2\) ne peut pas plus être mis en bijection avec \( \eN\) que \( \eD_3\).
\end{proof}

\begin{remark}
    Le cas de la base \( b=2\) doit être fait à part parce que rien n'empêche d'avoir une queue de \( 1\). Il y a alors toutefois moyen de se débrouiller en construisant la suite \( c\) de façon plus subtile. Si \( b=2\) et \( n\in \eN\) alors \( f(n)\) est une suite de \( 0\) et \( 1\) contenant une infinité de \( 0\) (parce qu'il n'y a pas de queue de suite ne contenant que des \( 1\)). Nous construisons alors \( c\) de la façon suivante : d'abord nous recopions \( f(0)\) jusqu'à son \emph{deuxième} zéro que nous changeons en \( 1\); nommons \( n_0\) le rang de ce deuxième zéro. Ensuite nous recopions les éléments de \( f(1) \) à partir du rang \( n_0+1\) jusqu'au second zéro que nous changeons en \( 1\), etc.

    Le fait de prendre le deuxième zéro nous garantit que la suite \( c\) n'aura pas de queue de suite ne contenant que des \( 1\).

    Notons que cette construction s'adapte à tout \( b\); il suffit de prendre le second terme qui n'est pas \( b-1\) et le remplacer par \( b-1\).
\end{remark}

\begin{corollary}
    L'ensemble \( \mathopen[ 0 , 1 [\) n'est pas dénombrable.
\end{corollary}

\begin{proof}
    L'ensemble \( \mathopen[ 0 , 1 [\) est en bijection avec \( \eD_b\) que nous venons de prouver n'être pas dénombrable.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Banach-Steinhaus}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{BIBooZUTUooNMvrdQ}]     \label{LEMooPIPLooMppGSO}
    Soient des espaces vectoriels normés \( X\) et \( Y\) ainsi qu'une application linéaire bornée \( T\colon X\to Y\). Pour tout \( a\in X\) et pour tout \( r>0\) nous avons
    \begin{equation}
        \sup_{x\in B(a,r)}\| Tx \|\geq r\| T \|
    \end{equation}
\end{lemma}

\begin{proof}
    Nous commençons avec \( a=0\). En utilisant la définition \ref{DefNFYUooBZCPTr} de la norme opérateur,
    \begin{equation}
        \| T \|=\sup_{x\in X}\frac{ \| Tx \| }{ \| x \| }=\sup_{x\in B(0,r)}\frac{ \| Tx \| }{ \| x \| }\leq \frac{1}{ r }\sup_{x\in B(0,r)}\| Tx \|.
    \end{equation}
    Donc
    \begin{equation}
        \sup_{x\in B(0,r)}\| Tx \|\geq r\| T \|.
    \end{equation}
    
    Il y a maintenant une astuce. Nous considérons un maximum :
    \begin{subequations}
        \begin{align}
            \max\{ \| T(a+x),\| T(a-x) \| \| \}&\geq \frac{ 1 }{2}\big( \| T(a+x) \|+\| T(a-x) \| \big) \label{SUBEQooPJPMooDkqRHs}\\
            &\geq \frac{ 1 }{2}\big( \| T(a+x)-T(a+x) \| \big)      \label{SUBEQooEZUUooVlKtfn}\\
            &=\frac{ 1 }{2}\| T(2x) \|\\
            &=\| Tx \|.
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooPJPMooDkqRHs}, la moyenne est plus petite que le maximum.
        \item Pour \eqref{SUBEQooEZUUooVlKtfn}, inégalité triangulaire : \( \| \alpha-\beta \|\leq \| \alpha \|+\| \beta \|\).
    \end{itemize}
    Si maintenant \( y\in B(a,r)\), nous avons \( y=a+x\) pour un certain \( x\in B(0,r)\), donc
    \begin{subequations}
        \begin{align}
            \sup_{y\in B(a,r)}\| Ty \|&=\sup_{x\in B(0,r)}\| T(a+x) \|\\
            &=\sup_{x\in B(0,r)}\max\{ \| T(a+x) \|, \| T(a-x) \| \}        \label{SUBEQooACJSooTHCAWs}\\
            &\geq \sup_{x\in B(0,r)}\| Tx \|\\
            &\geq r\| T \|.
        \end{align}
    \end{subequations}
    Pour \eqref{SUBEQooACJSooTHCAWs}, l'ensemble sur lequel nous prenons le supremum n'est pas modifié fondamentalement si nous regroupons les éléments deux à deux en prenant le maximum : les éléments exclus sont majorés.
\end{proof}

\begin{theorem}[Théorème de Banach-Steinhaus\cite{BIBooZUTUooNMvrdQ}]       \label{THOooJHVNooIDDxyT}
    Soient un espace de Banach\footnote{Définition \ref{DefVKuyYpQ}.} \( X\) et un espace vectoriel normé \( Y\). Soit une famille \( \mF\) d'opérateurs linéaire bornés. Si pour tout \( x\in  X\),
    \begin{equation}
        \sup_{T\in\mF}\| Tx \|<\infty,
    \end{equation}
    alors 
    \begin{equation}
        \sup_{T\in \mF}\| T \|<\infty.
    \end{equation}
\end{theorem}

\begin{proof}
    Nous supposons que \( \sup_{T\in\mF}\| T \|=\infty\), de telle sorte que nous pouvons choisir une suite \( (T_n)\) dans \( \mF\) telle que \( \| T_n \|\to \infty\). Cette suite peut diverger arbitrairement vite, et nous fixerons exactement cela plus tard.

    Soit par ailleurs une suite \( \alpha_n>0\) d'éléments petits et tels que \( \alpha_n\to 0\). Nous supposons que \( \sum_{n=0}^{\infty}\alpha_n<\infty\).

    Si \( a\in X\), le lemme \ref{LEMooPIPLooMppGSO} dit que
    \begin{equation}
        \sup_{x\in B(a,\alpha_n)}\| T_nx \|\geq \| T_n \|\alpha_n.
    \end{equation}
    En posant \( x_0=0\), nous construisons une suite \( (x_n)\) par récurrence en imposant
    \begin{enumerate}
        \item
            \( x_n\in B(x_{n=1}, \alpha_n)\)
        \item
            \( \| T_nx_n \|\geq \| T_n \|\alpha_n\).
    \end{enumerate}
    En utilisant une série télescopique et l'inégalité triangulaire \( \| x_k-x_{k+1} \|\leq \alpha_n\) à chaque étage,
    \begin{equation}
        \| x_p-x_q \|\leq \sum_{k=p}^q\alpha_k\leq \sum_{k=p}^{\infty}\alpha_k.
    \end{equation}
    Mais vu que la somme des \( \alpha_n\) converge, la suite des queues de somme converge vers zéro\footnote{Lemme \ref{LEMooHUZEooSyPipb}\ref{ITEMooQNHMooUPjupB}.} : \( \lim_{p\to \infty}\sum_{k=p}^{\infty}\alpha_n=0\). Cela implique que \( (x_n)\) est une suite de Cauchy\footnote{Proposition \ref{PROPooZZNWooHghltd}.}. Vu que \( X\) est de Banach, la suite \( (x_n)\) a une limite dans \( X\). Soit \( x\) cette limite.

    Nous avons \( \beta_n=\| x_n-x \|\to 0\). Il y aurait moyen de calculer \( \beta_n\) en fonction de \( \alpha_n\) (surtout si nous avions donné une forme explicite à \( \alpha_n\)), mais c'est sans importance ici. L'important est que c'est une suite qui tend vers zéro.

    Nous avons
    \begin{equation}
        x\in B(x_n,\beta_n),
    \end{equation}
    et donc il existe \( a_n\in B(0,\beta_n)\) tel que \( x=x_n+a_n\). Avec cela, pour chaque \( n\) nous avons :
    \begin{subequations}
        \begin{align}
            \| T_nx \|&=\| T_n(x_n+a_n) \|\\
            &\geq\| T_nx_n \|-\| T_na_n \|\\
            &\geq \| T_nx_n \|-\| T_n \|\beta_n     \label{SUBEQooPLVQooChVCLU}\\
        &\geq \| T_n \|\alpha_n-\| T_n \|\beta_n\\
        &=\| T_n \|(\alpha_n-\beta_n).
        \end{align}
    \end{subequations}
    Pour \ref{SUBEQooPLVQooChVCLU}, nous avons utilisé \( \| T_na_n \|\leq \| T_n \|\beta_n\). En résumé,
    \begin{equation}
        \| T_nx \|\geq \| T_n \|(\alpha_n-\beta_n).
    \end{equation}
    Il suffit de choisir \( \| T_n \|\) suffisamment rapidement croissant pour que\footnote{Le point important ici est que \( \alpha_n\) (et donc \( \beta_n\)) est choisi sans référence à \( \| T_n \|\).}
    \begin{equation}
       \| T_n \|(\alpha_n-\beta_n)\to \infty,
    \end{equation}
    et nous avons \( \| T_nx \|\to \infty\), qui est contraire aux hypothèses.
\end{proof}

\begin{theorem}[Théorème de Banach-Steinhaus\cite{KXjFWKA,VPvwAaQ}] \label{ThoPFBMHBN}
    Soit \( E\) un espace de Banach\footnote{Définition~\ref{DefVKuyYpQ}.} et \( F\) un espace vectoriel normé. Nous considérons une partie \( H\subset \aL_c(E,F)\) (espace des fonctions linéaires continues). Alors \( H\) est uniformément borné si et seulement si il est simplement borné.
\end{theorem}
\index{théorème!Banach-Steinhaus}
\index{application!linéaire!théorème de Banach-Steinhaus}

\begin{proof}
    Si \( H\) est uniformément borné, il est borné; pas besoin de rester longtemps sur ce sens de l'équivalence. Supposons donc que \( H\) soit borné. Pour chaque \( k\in \eN^*\) nous considérons l'ensemble
    \begin{equation}
        \Omega_k=\{ x\in E\tq \sup_{f\in H}\| f(x) \|>k \}.
    \end{equation}

    \begin{subproof}
        \item[Les \( \Omega_k\) sont ouverts]

            Soit \( x_0\in \Omega_k\); nous avons alors une fonction \( f\in H\) telle que \(  \| f(x_0) \|>k \), et par continuité de \( f\) il existe \( \rho>0\) tel que \( \| f(x) \|>k\) pour tout \( x\in B(x_0,\rho)\). Par conséquent \( B(x_0,\rho)\subset \Omega_k\) et \( \Omega_k\) est ouvert par le théorème~\ref{ThoPartieOUvpartouv}.

        \item[Les \( \Omega_k\) ne sont pas tous denses dans \( E\)]

            Nous supposons que les ensembles \( \Omega_k\) soient tous dense dans \( E\). Le théorème de Baire~\ref{ThoBBIljNM} nous indique que \( E\) est un espace de Baire (parce que de Banach) et donc que
            \begin{equation}
                \overline{ \bigcap_{k\in \eN}\Omega_k }=E.
            \end{equation}
            En particulier l'intersection des \( \Omega_k\) n'est pas vide. Soit \( x_0\in \bigcap_{k\in \eN}\Omega_k\). Nous avons alors
            \begin{equation}
                \sup_{f\in H}\| f(x) \|=\infty,
            \end{equation}
            ce qui est contraire à l'hypothèse. Donc les ouverts \( \Omega_k\) ne sont pas tous denses dans \(E\).

        \item[La majoration]

            Il existe \( k\geq 0\) tel que \( \Omega_k\) ne soit pas dense dans \( E\), et nous voulons prouver que \( \{ \| f \|\tq f\in H \}\) est un ensemble borné. Soit donc \( k\geq 0\) tel que \( \Omega_k\) ne soit pas dense dans \( E\); il existe un \( x_0\in E\) et \( \rho>0\) tels que
            \begin{equation}
                B(x_0,\rho)\cap \Omega_k=\emptyset.
            \end{equation}
            Si \( x\in B(x_0,\rho)\) alors \( x\) n'est pas dans \( \Omega_k\) et donc
            \begin{equation}
                \sup_{f\in H}\| f(x) \|\leq k.
            \end{equation}
            Afin d'évaluer \( \| f \|\) nous devons savoir ce qu'il se passe avec les vecteurs sur une boule autour de \( 0\). Pour tout \( x\in B(0,\rho)\) et pour tout \( f\in H\), la linéarité de \( f\) donne
            \begin{equation}
                \| f(x) \|=\| f(x+x_0)-f(x_0) \|\leq \| f(x+x_0)+f(x_0) \|\leq 2k.
            \end{equation}
            Par continuité nous avons alors \( \| f(x) \|\leq 2k\) pour tout \( x\in \overline{ B(0,\rho) }\). Si maintenant \( x\in F\) vérifie \( \| x \|=1\) nous avons
            \begin{equation}
                \| f(x) \|=\frac{1}{ \rho }\| f(\rho x) \|\leq \frac{ 2k }{ \rho },
            \end{equation}
            et donc \( \| f \|\leq \frac{ 2k }{ \rho }\), ce qui montre que \( 2k/\rho\) est un majorant de l'ensemble \( \{ \| f \|\tq f\in H \}\).

    \end{subproof}

\end{proof}
Une application du théorème de Banach-Steinhaus est l'existence de fonctions continues et périodiques dont la série de Fourier ne converge pas. Ce sera l'objet de la proposition~\ref{PropREkHdol}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Convergence forte}
%---------------------------------------------------------------------------------------------------------------------------

Lorsque nous avons une suite d'opérateurs linéaires, nous pouvons considérer la convergence d'une suite pour la norme opérateur : \( A_k\to A\) lorsque \( \| A_k-A \|\to 0\).

\begin{definition}[\cite{ooAGRZooTyUUVy}]       \label{DEFooNREQooElLvec}
    Soient un espace vectoriel \( E\) et un espace vectoriel normé \( V\). Nous disons que la suite d'opérateur \( T_k\colon E\to V\) \defe{converge fortement}{convergence forte} vers l'opérateur $T$ si pour tout \( x\in E\) nous avons
    \begin{equation}
        \| T_kx-Tx \|\to 0.
    \end{equation}
\end{definition}

Cette notion s'appelle \emph{forte} par opposition à la convergence \emph{faible} dont nous ne parlerons pas. Elle est cependant moins forte que la convergence en norme dont nous avons déjà parlé.

\begin{proposition}     \label{PROPooRFBLooUjSirP}
    Soient des espaces vectoriels normés \( E\) et \( F\) et une suite d'opérateurs \( T_k\colon E\to F\) convergeant vers \( T\)\footnote{Sans précisions, ce sera toujours la convergence en norme.}. Alors cette suite converge également fortement.
\end{proposition}

\begin{proof}
    Soit \( x\in E\) que nous supposons non nul. Soit \( \lambda\in \eC\) tel que \( x=\lambda y\) avec \( \| y \|=1\). Nous avons
    \begin{equation}
        \| T_kx-Tx \|=| \lambda |\| T_ky-Ty \|\leq | \lambda |\sup_{\| z \|=1}\| T_kz-Tz \|=| \lambda |\| T_k-T \|\to 0.
    \end{equation}
    La dernière étape est la convergence en norme \( T_k\to T\).
\end{proof}

\begin{proposition}
    Soient \( E\) et \( F\), des espaces vectoriels normés de dimension finie. Soit une suite \( (A_n)\) d'applications linéaires \( E\to F\). Si elle converge fortement vers \( A\), alors elle converge en norme vers \( A\).
\end{proposition}

\begin{proof}
    En plusieurs coups.
    \begin{subproof}
        \item[Si une sous-suite converge]
            Commençons par montrer que si \( (B_n)\) est une sous-suite de \( (A_n)\) qui converge vers \( B\), alors \( B=A\). Autrement dit, \( A\) est le seul candidat limite pour \( A_n\).

            Soit \( \| x \|=1\). Nous avons
            \begin{equation}
                \| B_nx-Bx \|\leq \| B_n-B \|\| x \|=\| B_n-B \|,
            \end{equation}
            mais pour la sous-suite \( (B_n)\) nous avons supposé \( \| B_n-B \|\to 0\). Donc \( \| B_nx-Bx \|\to 0\), ce qui signifie que \( B_nx\to Bx\). Mais par hypothèse, \( B_nx\to Ax\). Par unicité de la limite, \( Bx=Ax\) pour tout \( x\) de norme \( 1\). Pour les autres \( x\), c'est la linéarité qui conclu.

        \item[Utilisation de deux gros résultats]
        Par l'hypothèse de convergence, pour chaque \( x\) nous avons \( \sup_n\| A_nx \|<\infty\). Le théorème de Banach-Steinhaus \ref{THOooJHVNooIDDxyT} nous indique alors que l'ensemble \( \mF=\{ A_n \}_{n\in \eN}\) est borné. Il existe donc \( M > 0\) tel que \( \| A_n \|< M\) pour tout \( n\).

        Nous utilisons à présent l'hypothèse de dimension finie en disant que l'espace des applications linéaires \( E\to F\) est de dimension finie, de telle sorte que ses boules fermées soient compactes.

        Donc la suite \( (A_n)\) est contenue dans un compact.
        
        \item[Les sous-suite convergentes]

            La suite \( (A_n)\) est contenue dans un compact. Toutes ses sous-suites sont dans ce compact et possèdent donc une sous-suite convergente (théorème \ref{ThoBWFTXAZNH}). Toutes ces sous-sous-suites convergent nécessairement vers \( A\) par ce que nous avons dit dans la première étape de la preuve. Le lemme \ref{LEMooSJKMooKSiEGq} nous dit alors que \( A_n\to A\).
    \end{subproof}
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Application ouverte}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[application ouverte]
    Soient deux espaces topologiques \( X\) et \( Y\). Un application \( f\colon X\to Y\) est \defe{ouverte}{application ouverte} si l'image de tout ouvert de \( X\) par \( f\) est un ouvert de \( Y\).

    Nous disons que \( f\) est ouverte en \( a\in X\) si l'image de tout ouvert contenant \( a\) est ouverte.
\end{definition}

\begin{proposition}     \label{PROPooXGEGooHoMsne}
    Une application bijective est ouverte si et seulement si son inverse est continue.
\end{proposition}

\begin{proof}
    Cela est seulement la définition, mais pour le sport nous démontrons le sens direct.

    Soit donc une application \( f\colon X\to Y\) bijective et ouverte entre les espaces topologiques \( X\) et \( Y\). Prouvons que \( f^{-1}\colon Y\to X\) est continue. Pour cela nous considérons un ouvert \( \mO\) dans \( X\), et nous prouvons que \( (f^{-1})^{-1}(\mO)\) est ouvert dans \( Y\). Par définition de l'inverse, \( (f^{-1})^{-1}(\mO)=f(\mO)\) et vu que \( f\) est ouverte, \( f(\mO)\) est ouvert.
\end{proof}

\begin{lemma}       \label{LEMooHHIPooEpGfCg}
    Une application \( f\colon X\to Y\) est ouverte si et seulement si pour tout \( x\in X\) et pour tout voisinage \( U\) de \( x\), la partie \( f(U)\) est un voisinage de \( f(x)\).
\end{lemma}

\begin{proof}
    La preuve suit celle de la proposition \ref{PROPooOXBCooIzLaPe}. Le sens direct est un à fortiori.

    Dans l'autre sens. Soit un ouvert \( \mO\) de \( X\). Pour prouver que \( f(\mO)\) est ouvert, nous considérons \( y\in f(\mO)\), ainsi que \( x\in\mO\) tel que \( f(x)=y\). Vu que \( \mO\) est un voisinage de \( x\), la partie \( f(\mO)\) est un voisinage de \( y=f(x)\).

    Il existe donc un ouvert \( V\) de \( Y\) tel que \( y=f(x)\in V\subset f(\mO)\). Donc la partie \( f(\mO)\) contient un ouvert autour de chacun de ses points, et elle est ouverte par le théorème \ref{ThoPartieOUvpartouv}.
\end{proof}


\begin{lemma}[\cite{BIBooNJJUooDaGnPZ}]
    Une application linéaire entre espaces vectoriels topologiques est ouverte si et seulement si elle est ouverte en \( 0\).
\end{lemma}

\begin{proof}
    Le sens direct est un à fortiori.

    Soit un ouvert \( \mO\) et \( a\in \mO\). La partie \( \mO-a\) est ouverte et contient \( 0\). Donc \( f(\mO-a)\) est un ouvert parce que \( f\) est ouverte en \( 0\). Nous en déduisons, par linéarité, que \( f(\mO)-f(a)\) est ouvert et donc que \( f(\mO)\) est ouverte.
\end{proof}

\begin{lemma}[\cite{BIBooNJJUooDaGnPZ}]
    Soient des espaces vectoriels normés \( E\) et \( F\). Une application linéaire ouverte \( f\colon E\to F\) est surjective.
\end{lemma}

\begin{proof}
    Soit un ouvert \( B(0,r)\) dans \( E\). Vu que \( f\) est ouverte, la partie \( f\big( B(0,r) \big)\) est ouverte dans \( F\), et contient donc une boule \( B_F(0,r')\) pour un certain \( r'>0\).

    Soit \( v\in F\). Nous considérons
    \begin{equation}
        v'=r'\frac{ v }{ 2\| v \| }.
    \end{equation}
    Nous avons \( \| v' \|=r'/2\), et donc \( v'\in B_F(0,r')\). Il existe donc \( x\in E\) (et même dans \( B_E(0,r)\)) tel que \( f(x)=v'\). Nous avons alors
    \begin{equation}
        f\big( \frac{ 2\| v \| }{ r' }x \big)=v,
    \end{equation}
    ce qui prouve que \( v\) est dans l'image de \( f\), et donc que \( f\) est surjective.
\end{proof}

\begin{theorem}[théorème de l'application ouverte\cite{BIBooNJJUooDaGnPZ, BIBooYJXXooTvzpDW,BIBooMKAVooDLCzUX}]     \label{THOooATZKooXHWCRD}
    Soient des espaces de Banach\footnote{Espace de Banach : vectoriel, normé, complet. Définition \ref{DefVKuyYpQ}.} \( E\) et \( F\). Si l'application \( f\colon E\to F\) est linéaire, surjective et continue, alors elle est ouverte.
\end{theorem}

\begin{proof}
    En plusieurs étapes.
    \begin{subproof}
    \item[Une union de fermés]
        Soit \( y\in F\). Vu que \( f\) est surjective, il existe \( x\in E\) tel que \( y=f(x)\). Soit \( n\in \eN\) tel que \( x\in B(0,n)\). Nous avons alors
        \begin{equation}
            y\in f\big( B(0,1) \big)\subset \overline{ f\big( B(0,n) \big) }
        \end{equation}
        En notant 
        \begin{equation}
            F_n=\overline{ f\big( B_E(0,n) \big) },
        \end{equation}
        nous avons
        \begin{equation}
            F=\bigcup_{n=0}^{\infty}F_n.
        \end{equation}
    \item[Théorème de Baire]
        Le théorème \ref{ThoBBIljNM} nous indique que \( F\) est un espace de Baire. Le lemme \ref{LEMooTOJDooQDtWUC} nous dit alors qu'il existe un \( n\) tel que \( F_n\) soit d'intérieur non vide. Mettons \( F_N\) d'intérieur non vide.
    \item[Boule unité]
        Vu que \( F_N\) est d'intérieur non vide, il existe \( y\in F_N\) et \( \eta>0\) tels que \( B_F(y,\eta)\subset F_N\). Nous avons aussi
        \begin{equation}
            B_F(0,\eta)=B_F(y,\eta)-y,
        \end{equation}
        et vu que \( y\in F_N\) nous avons \( B_F(0,\eta)\subset F_N-F_N\), et vu qu'en plus \( -F_N=F_N\), nous avons
        \begin{equation}
            B_F(0,\eta)\subset 2F_N=\overline{ f\big( B_E(0,2N) \big) }.
        \end{equation}
        Nous avons ensuite    
        \begin{equation}
            B_F(0,1)=\frac{1}{ \eta }B_F(0,\eta)\subset\frac{1}{ \eta }\overline{ f\big( B_E(0,2N) \big) }=\overline{ f\big( B_E(0,2N/\eta) \big) }.
        \end{equation}
        Ceci pour dire qu'il existe un \( M\in \eR\) tel que
        \begin{equation}
            B_F(0,1)\subset \overline{ f\big( B_E(0,M) \big) }.
        \end{equation}
        Nous avons de même que
        \begin{equation}        \label{EQooCMSPooYtzAuC}
            B_F\big( 0,\frac{1}{ 2^n } \big)\subset \overline{ f\big( B_E(0,M/2^n) \big) }.
        \end{equation}
        Nous voudrions maintenant avoir la même inclusion sans la fermeture.

    \item[Une suite par récurrence]
        Soit \( z\in B_F(0,1)\). Nous allons définir par récurrence une suite \( (x_n)\) dans \( E\) telle que
        \begin{subequations}        \label{SUBEQSooLJEMooOaFncH}
            \begin{numcases}{}
                x_n\in B_E\big( 0,\frac{ M }{ 2^{n-1} } \big)\\
                \| z-f(x_1+\ldots +x_n) \|<\frac{1}{ 2^n }.
            \end{numcases}
        \end{subequations}
        \begin{subproof}
        \item[Le premier élément]
        Vu que \( z\in B_F(0,1)\subset\overline{ f\big( B_E(0,M) \big) }\), nous avons
        \begin{equation}
            B(z,\frac{ 1 }{2})\cap f\big( B_E(0,M) \big)\neq \emptyset.
        \end{equation}
        Nous pouvons donc considérer \( x_1\in B_E(0,M)\) tel que \( f(x_1)\in B(z,\frac{ 1 }{2})\).

        Ce \( x_1\) vérifie les conditions \eqref{SUBEQSooLJEMooOaFncH}.
    \item[La récurrence]
        En utilisant l'hypothèse de récurrence et \eqref{EQooCMSPooYtzAuC},
        \begin{equation}
            z-f(x_1+\ldots +x_n)\in B_F\big( 0,\frac{1}{ 2^n } \big)\subset\overline{ f\big( B_E(0,M/2^n) \big) },
        \end{equation}
        de telle sorte que
        \begin{equation}
            B_F\big( z-f(x_1,\ldots, x_n),\frac{1}{ 2^{n+1} } \big)\cap f\big( B_E(0,M/2^n) \big)\neq \emptyset.
        \end{equation}
        Nous pouvons donc considérer \( x_{n+1}\in B_E(0,M/2^n)\) tel que
        \begin{equation}
            f(x_{n+1})\in B_F\big( z-f(x_1+\ldots +x_n),\frac{1}{ 2^{n+1} } \big).
        \end{equation}
        Donc
        \begin{equation}
            z-f(x_1+\ldots +x_n)-f(x_{n+1})\in B_F\big( 0,\frac{1}{ 2^{n+1} } \big).
        \end{equation}
        Nous avons donc bien
        \begin{equation}
            \| z-f(x_1+\ldots +x_{n+1}) \|<\frac{1}{ 2^{n+1} }.
        \end{equation}
        \end{subproof}
    \item[Convergence]
        Nous avons, pour tout \( n\), que \( \| x_n \|<\frac{ M }{ 2^{n-1} }\). Donc la série
        \begin{equation}
            \sum_{n=1}^{\infty}\| x_n \|\leq \sum_{n=1}^{\infty}\frac{ M }{ 2^{n-1} }
        \end{equation}
        converge. Autrement dit, la série des \( x_n\) converge absolument\footnote{Définition \ref{DefVFUIXwU}.}. Vu que \( E\) est une espace de Banach, la proposition \ref{PropAKCusNM} nous dit que \( \sum_{n=1}^{\infty}x_n\) converge dans \( E\). Nous posons
        \begin{equation}
            x=\sum_{n=1}^{\infty}x_n.
        \end{equation}
        En utilisant la série géométrique de la proposition \ref{PROPooWOWQooWbzukS}\ref{ITEMooBJHBooBMEmiG}, nous trouvons
        \begin{equation}
            \| x \|\leq \sum_{k=1}^{\infty}\| x_k \|\leq M\sum_{k=1}^{\infty}\frac{1}{ 2^{n+1} }=M\sum_{k=0}^{\infty}\frac{1}{ 2^k }=2M.
        \end{equation}
    \item[Passage à la limite]
        Nous avons \( x\in B_E(0,2M)\), et
        \begin{equation}
            \lim_{n\to \infty} \| z-f(x_1+\ldots +x_n) \|=0.
        \end{equation}
        Vu que \( \| . \|\), \( t\mapsto z-t\) et \( f\) sont continue\footnote{Oui, la continuité de \( f\) est une hypothèse en plus de sa linéarité parce que nous n'avons pas d'hypothèses sur la dimension de \( E\) et \( F\).}, nous pouvons rentrer la limite de partout et écrire
        \begin{equation}
            \| z-f(x) \|=0,
        \end{equation}
        ce qui signifie que \( z=f(x)\). Vu que \( z\) est un élément arbitraire de \( B_F(0,1)\) nous avons prouvé que
        \begin{equation}
            B_F(0,1)\subset f\big( B_E(0,2M) \big).
        \end{equation}
        Nous avons donc aussi que pour tout \( r>0\), il existe \( r'\) tel que
        \begin{equation}
            B_F\big( 0, r \big)\subset f\big( B_E(0,r) \big).
        \end{equation}
        En l'occurrence, \( r'=r/2M\).
    \item[Passage aux voisinages]
        Nous montrons que l'image de tout voisinage de \( x\in E\) contient un voisinage de \( f(x)\) dans \( F\). Soit \( x\in E\) ainsi qu'un voisinage \( V\) de \( x\). Il existe \( r>0\) tel que \( B(x,r)\subset V\). Vu que \( f\) est linéaire,
        \begin{equation}
            f\big( B(x,r) \big)=f(x)+f\big( B(0,r) \big),
        \end{equation}
        et il existe un \( r'\) tel que \( B_F(0,r')\subset f\big( B_E(0,r) \big)\). Cela pour dire que
        \begin{equation}
            f(x)+B_F(0,r')\subset f\big( B(0,r) \big)\subset f(V).
        \end{equation}
        Vu que \( f(x)+B_F(0,r')\) est un ouvert autour de \( f(x)\), nous avons prouvé que \( f(V)\) contient un ouvert autour de \( f(x)\), c'est à dire que \( f(V)\) est un voisinage de \( f(x)\).
    \item[Conclusion]
        Le lemme \ref{LEMooHHIPooEpGfCg} conclu que \( f\) est ouverte.
    \end{subproof}
    
\end{proof}

