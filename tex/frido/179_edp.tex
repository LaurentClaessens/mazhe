% This is part of Le Frido
% Copyright (c) 2017-2018
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Symbole principal, équation des caractéristiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit l'équation différentielle semi-linéaire d'ordre \( k\)
\begin{equation}        \label{EQooEZXSooOxZTFi}
    \sum_{| \alpha |=k}a_{\alpha}(x)(\partial^{\alpha}u)(x)+F\Big( x,u(x),(Du)(x),\ldots, (D^{k-1}u)(x) \Big)=0
\end{equation}
pour la fonction \( u\colon \eR^d\to \eR\).

\begin{definition}
    Le \defe{symbole principal}{symbole principal} de l'équation \eqref{EQooEZXSooOxZTFi} est l'application
    \begin{equation}
        \begin{aligned}
            \sigma\colon \eR^d\times \eR^d&\to \eR \\
            (x,\xi)&\mapsto \sum_{| \alpha |=k}a_{\alpha}(x)\xi^{\alpha}
        \end{aligned}
    \end{equation}
    où si \( \alpha=(\alpha_1,\ldots, \alpha_d)\) et \( \xi=(\xi_1,\ldots, \xi_d)\) alors \( \xi^{\alpha}=\xi_1^{\alpha_1}\ldots\xi_d^{\alpha_d}\).
\end{definition}

\begin{definition}      \label{DEFooYYNOooZlZMxu}
    Les \defe{caractéristiques}{caractéristique!d'une équation différentielle} de l'équation \eqref{EQooEZXSooOxZTFi} est une surface \( S\) de \( \eR^d\) donné par une équation de la forme \( \phi(x)=0\) où \( \phi\) satisfait à
    \begin{equation}
        \sigma\big( x,\nabla\phi(x) \big)=0
    \end{equation}
    et \( \nabla\phi(x)\neq 0\) pour tout \( x\in S\).
\end{definition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Méthode des caractéristiques pour l'ordre \( 1\)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooHKSLooOCYNDz}

Nous\cite{ooEIHMooRXOzwa,ooAUICooVUjyqo} voulons étudier l'équation d'ordre \( 1\)
\begin{equation}        \label{EQooGJKZooCCEpRj}
    a(x,y)\frac{ \partial u }{ \partial x }(x,y)+b(x,y)\frac{ \partial u }{ \partial y }(x,y)+c(x,y)u(x,y)=f(x,y)
\end{equation}
Le champ de vecteurs associé à cette équation est
\begin{equation}
    v=\begin{pmatrix}
        a    \\
        b
    \end{pmatrix},
\end{equation}
et l'équation peut être écrite sous la forme
\begin{equation}
    (v\cdot\nabla)+cu=f.
\end{equation}

\begin{definition}
    Le \defe{flot}{flot} de ce champ de vecteurs sont les courbes paramétriques \( \gamma(t)=\big( x(t), y(t) \big)\) vérifiant \( \gamma't(t)=v\big( \gamma(t) \big)\).
\end{definition}
Les équations du flot pour l'équation \eqref{EQooGJKZooCCEpRj} sont
\begin{subequations}        \label{EQooURBTooVdFzZR}
            \begin{numcases}{}
                x'(t)=a\big( x(y),y(t) \big)\\
                y'(t)=b\big( x(y),y(t) \big).
            \end{numcases}
        \end{subequations}
Ce sont des équations différentielles ordinaires. Un système de deux équations couplées du premier ordre.

Quel est l'intérêt du flot ? Nous allons voir que sur la ligne \( t\mapsto\gamma(t)\), la fonction \( u\) est constante. Or des solutions \( \gamma\) au système \eqref{EQooURBTooVdFzZR}, il y en aura plusieurs : une pour chaque valeur des constantes d'intégration. Pour peu que ces lignes recouvrent tout le plan, nous pourrons résoudre l'équation de départ ligne par ligne.

Nous posons
\begin{subequations}
    \begin{align}
        \tilde u(t)&=u\big( x(t),y(t) \big)\\
        \tilde c(t)&=c\big( x(t),y(t) \big)\\
        \tilde f(t)&=f\big( x(t),y(t) \big).
    \end{align}
\end{subequations}
La fonction \( \tilde u\) est une fonction \( \eR\to \eR\) normale qui se dérive normalement, en suivant la règle de dérivation des fonctions composées :
\begin{subequations}
    \begin{align}
    \tilde u'(t)&=\frac{ \partial u }{ \partial x }\big( x(t),y(t) \big)x'(t)+\frac{ \partial u }{ \partial y }\big( x(t),y(t) \big)y'(t)\\
    &=a\frac{ \partial u }{ \partial x }+b\frac{ \partial u }{ \partial y }\\
    &=f\big( x(t),y(t) \big)-c\big( x(t),y(t) \big)u\big( x(t),y(t) \big)\\
    &=\tilde f(t)-\tilde c(t)\tilde u(t).
    \end{align}
\end{subequations}
Nous avons pour \( \tilde u\) l'équation différentielle ordinaire
\begin{equation}
    \tilde u'+\tilde c\tilde u=\tilde f
\end{equation}
qui est résolue par la proposition~\ref{PROPooZCXQooPQpkdQ}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Un exemple complet un peu minimal}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons l'équation différentielle\cite{ooEIHMooRXOzwa}
\begin{equation}
    \frac{ \partial u }{ \partial x }-\frac{ \partial u }{ \partial y }-(x-y)u=0.
\end{equation}
Et nous allons la résoudre.

Les équations du flot, sont simples parce que les coefficients sont des constantes : \( x'(t)=1\), \( y'(t)=-1\). Donc
\begin{subequations}
    \begin{align}
        x(t)&=t+C_1\\
        y(t)&=-t+C_2.
    \end{align}
\end{subequations}
A priori nous avons une caractéristique pour chaque choix de \( (C_1,C_2)\) et nous espérons que le tout recouvre le plan \( \eR^2\). En fait seule une des deux constantes doit être laissée libre, l'autre consiste seulement en décaler le paramètre \( t\). Nous posons donc \( C_1=0\) et nous considérons les courbes caractéristiques
\begin{equation}
    \gamma_C(t)=\begin{pmatrix}
        t    \\
        -t+C
    \end{pmatrix}.
\end{equation}
Ces courbes recouvrent bien tout le plan. Pour savoir les valeurs de \( u\) sur la courbe \( \gamma_C\), nous devons résoudre l'équation différentielle ordinaire
\begin{equation}
    \tilde u_C'+\tilde c\tilde u_C=\tilde f,
\end{equation}
en sachant que \( \tilde c(t)=c\big( x(t),y(t) \big)=-\big( x(t)-y(t) \big)=2t-C\). Cela se fait en suivant la méthode décrite dans l'exemple~\ref{EXooVVLGooPWaHUI} et résumée dans la proposition~\ref{PROPooZCXQooPQpkdQ}.

En termes de notations, \( \tilde u_C(t)=u\big( \gamma_C(t) \big)\). Récrivons l'équation :
\begin{equation}
    \tilde u'(t)-(2t-C)\tilde u(t)=0.
\end{equation}
La méthode pour la résoudre est de mettre les \( \tilde u\) d'un côté et les \( t\) de l'autre :
\begin{equation}
    \frac{ \tilde u' }{ u }=2t-C.
\end{equation}
En intégrant par rapport à \( t\) des deux côtés,
\begin{equation}
    \ln(\tilde u)=t^2-Ct+K_C,
\end{equation}
c'est-à-dire (avec redéfinition de \( K_C\))
\begin{equation}
    \tilde u(t)=K_C e^{t^2-Ct}
\end{equation}
ou encore
\begin{equation}   \label{EQooSSTJooEQfRnP}
    u\big( \gamma_C(t) \big)=K_C e^{t^2-Ct}
\end{equation}
où \( C\) est le paramètre que nous déterminons en sachant sur quelle caractéristique se trouve le point \( (x,y)\) où nous voulons calculer \( u(x,y)\) et \( K\) est une constante (strictement positive parce que si vous avez suivi le mouvement, c'est une exponentielle) qui doit être déterminée par les conditions initiales. Dès que \( K\) est fixé pour un des points de la courbe \( \gamma_C\), alors il est fixé pour tous les points.

Ce que nous avons obtenu est qu'il existe un \( K_C\) tel que pour tout \( t\) nous avons
\begin{equation}
    u\big( \gamma_C(t) \big)=K_C e^{t^2-Ct}.
\end{equation}

Soit donc un point \( (x_0,y_0)\in \eR^2\). Nous devons d'abord déterminer où ce point se trouve par rapport aux caractéristiques, c'est-à-dire quelle est la valeur de \( C\) pour laquelle \( (x_0,y_0)\) est sur la courbe \( \gamma_C\), et ensuite déterminer pour quelle valeur de \( t\) nous aurons \( \gamma_C(t)=(x_0,y_0)\). À résoudre :
\begin{equation}
    \gamma_C(t_0)=\begin{pmatrix}
        t_0    \\
        -t_0+C
    \end{pmatrix}=\begin{pmatrix}
        x_0    \\
        y_0
    \end{pmatrix}.
\end{equation}
Donc \( t_0=x_0\) et \( C=x_0+y_0\). En reprenant \eqref{EQooSSTJooEQfRnP} nous avons
\begin{equation}
    u\big( \gamma_C(t_0) \big)=K e^{x_0^2-(x_0+y_0)x_0}=K e^{-x_0y_0}.
\end{equation}

Pour peu que des conditions soient donnée sur chaque caractéristique, nous pouvons déterminer \( K\). Attention : ce \( K\) est une constante d'intégration de l'équation différentielle ordinaire pour \( \tilde u\). Donc elle n'est valable que sur chaque caractéristique séparément. Cela n'est donc pas du tout une constante sur \( \eR^2\).

Nous pouvons maintenant écrire la solution générale de l'équation de départ. L'équation cartésienne de la courbe \( \gamma_C\) est
\begin{equation}
    x+y=C.
\end{equation}
Donc \( K\) est une fonction de \( x+y\), pas de \( x\) et \( y\) séparément. Cela est important à comprendre. A priori nous avons
\begin{equation}
    u(x,y)=K(x,y) e^{-xy}
\end{equation}
où \( K(x,y)\) est constante sur la courbe \( \gamma_C\) contenant \( (x,y)\). Nous avons
\begin{itemize}
    \item si \( x_1+y_1=x_2+y_2\),
    \item alors il existe \( C\) tel que \( (x_1,y_1)\) et \( (x_2,y_2)\) sont sur \( \gamma_C\),
    \item alors \( K(x_1,y_2)=K(x_2,y_2)\).
\end{itemize}
Donc il existe une fonction \( \eR\to \eR\) telle que \( K(x,y)=f(x+y)\).

Au final, la solution générale de l'équation est
\begin{equation}
    u(x,y)=f(x+y) e^{-xy}
\end{equation}
où \( f\) est une fonction à déterminer par les conditions initiales qui peuvent être données. Typiquement nous espérons que les conditions imposent une et une seule valeur de \( u\) sur chacune des courbes \( \gamma_C\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Un théorème d'existence et d'unicité}
%---------------------------------------------------------------------------------------------------------------------------

La méthode des caractéristiques donne essentiellement une preuve de l'unicité des solutions aux équations de transport, et une méthode pour construire cette solution. En effet, la procédure suivante permet de construire \( u(x_0,y_0)\).
\begin{itemize}
    \item Trouver la caractéristique passant par le point \( (x_0,y_0)\)
    \item Calculer en quel point elle passe par une condition initiale donnée.
    \item Attribuer à \( u(x_0,y_0)\) la valeur trouvée sur la caractéristique là où elle passe par une condition initiale.
\end{itemize}
Rien ne permet a priori de savoir que cette procédure construit effectivement une solution. En particulier, comment calculer \( \partial_xu\) ? Le quotient différentiel serait
\begin{equation}
    \frac{ \partial u }{ \partial x }(x,y)=\lim_{\epsilon\to 0}\frac{ u(x+\epsilon,y)-u(x,y) }{ \epsilon },
\end{equation}
mais la caractéristique donnant la valeur de \( u(x+\epsilon,y)\) est différente pour chaque \( \epsilon\). Rien a priori ne permet d'affirmer que le calcul soit simple, ni qu'il arrive à une solution du problème donné.

D'où la nécessité d'avoir un résultat un peu rigoureux donnant des conditions sous lesquelles les choses vont bien.

\begin{proposition}[Équation de transport à coefficients variables\cite{ooAUICooVUjyqo,ooPMPXooEpbDkm}]     \label{PROPooVQLBooQyFfEH}
    Soit une fonction \( c\colon \eR^2\to \eR\) de classe \( C^2\) en ses deux variables et uniformément Lipschitziennes en sa première variable\quext{Dans \cite{ooPMPXooEpbDkm}, on ne demande que continue puis uniformément Lipschitz. Moi je crois que ce n'est pas assez pour assurer la dérivabilité de \( X\) par rapport à \( x\), et encore moins pour permuter les dérivées dans \( \partial^2_{tx}Y\).} et \( g\in C^1(\eR)\). Alors l'équation aux dérivées partielles de premier ordre
    \begin{subequations}        \label{SUBEQSooKWBFooKkpihH}
        \begin{numcases}{}
        \frac{ \partial u }{ \partial x }(x,t)+c(x,t)\frac{ \partial u }{ \partial t }(x,y)=0\\
        u(x,0)=h(y)
        \end{numcases}
    \end{subequations}
    admet une unique solution de classe \( C^1\).

    Cette solution est construite de la façon suivante\footnote{Le fait que la construction ait un sens fait partie des choses à prouver}. D'abord nous considérons la solution \( X\) au problème
    \begin{subequations}        \label{SUBEQSooTOCEooHRhXiv}
        \begin{numcases}{}
            \frac{ \partial X }{ \partial s }(s;x,t)=c\big( X(s;x,t),s \big)\\
            X(t;x,t)=x,
        \end{numcases}
    \end{subequations}
    et ensuite le problème \eqref{SUBEQSooKWBFooKkpihH} a pour unique solution
    \begin{equation}        \label{EQooLFYNooJewRhE}
        u(x,t)=h\big( X(0;x,t) \big).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous commençons par étudier l'existence et l'unicité de la fonction \( X\) définie par le problème~\ref{SUBEQSooTOCEooHRhXiv}. La fonction \( c\) ici est dans les hypothèses de la fonction $f$ du théorème de Cauchy-Lipschitz global~\ref{THOooZIVRooPSWMxg}. D'où l'existence et l'unicité de la fonction \( s\mapsto X(s;x,t)\) sur \( \eR\) pour chaque \( (x,y)\) donné\footnote{Avec l'énoncé tel que donné dans \cite{ooAUICooVUjyqo}, il faut utiliser la technique de~\ref{NORMooZROGooZfsdnZ} pour l'existence globale, parce que la fonction \( b\) là-dedans n'est pas dans les mêmes hypothèses.}.

    Le lemme~\ref{LEMooQWDNooOjNXhl} nous dit que \( X\) est de classe \( C^2\) en \( (s,x,t)\). Donc nous pourrons dériver et permuter les dérivées autant que nous voudrons (sans exagérer : ordre \( 2\) au maximum).

    \begin{subproof}
        \item[Unicité]
            Nous montrons que \( u\) doit être constante sur le chemin
            \begin{equation}
                \gamma_{(x,t)}(s)=\begin{pmatrix}
                    X(s;x,t)    \\
                    s
                \end{pmatrix}.
            \end{equation}
            En effet, en posant
            \begin{equation}
                \varphi(s)=u\big( \gamma(s) \big)=u\big( X(s;x,t),s \big),
            \end{equation}
            et en dérivant nous obtenons
            \begin{equation}
                \varphi'(s)=\frac{ \partial u }{ \partial t }\big( X(s;x,t),s \big)\frac{ \partial X }{ \partial s }(s;x,t)+\frac{ \partial u }{ \partial t }\big( X(s;x,t) \big)=0.
            \end{equation}
            Par conséquent la valeur commune de tous les \(   u\big( \gamma_{(x,t)}(s) \big)    \) doit être celle en \( \gamma_{(x,t)}(0)=h(  X(0;x,t)  ) \)

            Cela prouve l'unicité parce que la valeur de \( u\) est fixée en tout point. Nous devons encore vérifier que la fonction \( u\) ainsi construite est bien une solution du problème. C'est l'objet de la partie «existence» de la preuve.

        \item[Existence]

            Même l'existence est divisée en plusieurs étapes.

            \begin{subproof}
                \item[Mise en place]

            Nous prouvons que la fonction \( u\) donné par \eqref{EQooLFYNooJewRhE} est une solution du problème. Nous avons :
            \begin{equation}
                \frac{ \partial u }{ \partial t }(x,t)=h'\big( X(0;x,t) \big)\frac{ \partial X }{ \partial t }(0;x,t)
            \end{equation}
            et
            \begin{equation}
                \frac{ \partial u }{ \partial x }(x,t)=h'\big( X(0;x,t) \big)\frac{ \partial X }{ \partial x }(0;x,t),
            \end{equation}
            de sorte qu'en posant
            \begin{equation}
                g(s;x,t)=\frac{ \partial X }{ \partial t }(s;x,t)+c(x,t)\frac{ \partial X }{ \partial x }(s;x,t)
            \end{equation}
            nous avons
            \begin{equation}
                \frac{ \partial u }{ \partial t }+c\frac{ \partial u }{ \partial x }=h'\big( X(0;x,t) \big)g(0;x,t).
            \end{equation}

        \item[Une équation différentielle pour \( g\)]

            Nous allons prouver que
            \begin{equation}
                \frac{ \partial g }{ \partial s }(s;x,t)=\alpha_{(x,t)}(s)g(s;x,t)
            \end{equation}
            avec\quext{La ligne suivante est une de celles qui me font penser qu'il manque des hypothèses dans \cite{ooPMPXooEpbDkm}. Il faut bien pouvoir dériver \( c\).}
            \begin{equation}
                \alpha_{(x,t)}(s)=\frac{ \partial c }{ \partial x }\big( X(s;x,t),s \big).
            \end{equation}
            D'abord nous avons
            \begin{equation}
                \frac{ \partial g }{ \partial s }(s;x,t)=\frac{ \partial^2X }{ \partial s\partial t }(s;x,t)+c(x,t)\frac{ \partial^2X }{ \partial s\partial x }(s;x,t).
            \end{equation}
            Nous permutons les dérivées et nous tenons compte de \eqref{SUBEQSooTOCEooHRhXiv} :
            \begin{equation}
                \frac{ \partial g }{ \partial s }(s;x,t)=\frac{ \partial  }{ \partial t }\Big( c\big( X(s;x,t),s \big) \Big)+c(x,t)\frac{ \partial  }{ \partial x }\Big( c\big( X(s;x,t),s \big) \Big).
            \end{equation}
            Nous dérivons maintenant plus en profondeur. D'une part
            \begin{equation}
                \frac{ \partial  }{ \partial t }\Big( c\big( X(s;x,t),s \big) \Big)=\frac{ \partial c }{ \partial x }\Big( X(s;x,t),s \Big)\frac{ \partial X }{ \partial t }(s;x,t)
            \end{equation}
            et d'autre part,
            \begin{equation}
                \frac{ \partial  }{ \partial x }\Big( c\big( X(s;x,t),s \big) \Big)=\frac{ \partial c }{ \partial x }\Big( X(s;x,t),s \Big)\frac{ \partial X }{ \partial x }(s;x,t),
            \end{equation}
            de telle sorte que
            \begin{subequations}
                \begin{align}
                \frac{ \partial g }{ \partial s }(s;x,t)&=\frac{ \partial c }{ \partial x }\big( X(s;x,t),s \big)\Big[ \frac{ \partial X }{ \partial t }(s;x,t)+c(x,t)\frac{ \partial X }{ \partial x }(s;x,t) \Big]\\
                &=\alpha_{(x,t)}(s)g(s;x,t).
                \end{align}
            \end{subequations}

        \item[Une condition initiale pour \( g\)]

            Nous montrons maintenant que \( g(t;x,t)=0\). La condition initiale pour \( X\) est \( X(t;x,t)=x\) pour tout \( t,x\in \eR\). Nous dérivons cette dernière par rapport à \( x\) et à \( t\) :
            \begin{subequations}
                \begin{align}
                    \frac{ \partial X }{ \partial x }(t;x,t)&=1         \label{EQooQKVLooMpfFDC}\\
                    \frac{ \partial X }{ \partial s }(t;x,t)+\frac{ \partial X }{ \partial t }(t;x,t)&=0.       \label{EQooNAWYooTzYkuQ}
                \end{align}
            \end{subequations}
            Mais \( \partial_sX(t;x,t)=c\big( X(t;x,t),t \big)\), donc la relation \eqref{EQooNAWYooTzYkuQ} donne
            \begin{equation}        \label{EQooVOMVooUhxkIx}
                \frac{ \partial X }{ \partial t }(t;x,t)=-c\big( X(t;x,t),t \big)=-c(x,t)
            \end{equation}
            où nous avons tenu compte du fait que \( X(t;x,t)=x\).

            Voyons à présent ce que \eqref{EQooQKVLooMpfFDC} et \eqref{EQooVOMVooUhxkIx} donnent pour \( g(t;x,t)\) :
            \begin{equation}
                g(t;x,t)=\frac{ \partial X }{ \partial t }(t;x,t)+c(x,t)\frac{ \partial X }{ \partial x }(t;x,t)=-c(x,t)+c(x,t)=0.
            \end{equation}

        \item[Conclusion pour \( g\)]

            La fonction \( g\) vérifie l'équation différentielle
            \begin{subequations}
                \begin{numcases}{}
                    \frac{ \partial g }{ \partial s }(s)=\alpha(s)g(s)\\
                    g(t)=0.
                \end{numcases}
            \end{subequations}
            Bien entendu, \( g(s)=0\) est une solution. Mais la solution à ce système est unique par Cauchy-Lipschitz~\ref{ThokUUlgU}. Ici nous utilisons le fait que
            \begin{equation}
                (s,y)\mapsto \alpha(s)y
            \end{equation}
            est continue. C'est-à-dire entre autres que
            \begin{equation}
                s\mapsto\frac{ \partial c }{ \partial x }\big( X(s;x,t),s \big)
            \end{equation}
            doit être continue. C'est le cas parce que \( c\) est de classe \( C^1\) en sa première variable.

            \end{subproof}
    \end{subproof}
\end{proof}

Les hypothèses de la proposition~\ref{PROPooVQLBooQyFfEH} sont loin d'être optimales. Voici un exemple dans lequel \( c\) n'est même pas dérivable par rapport à \( t\) et qui se passe très bien quand même.

\begin{example}
    Soit l'équation différentielle
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial u }{ \partial t }(x,t)+| t-1 |\frac{ \partial u }{ \partial x }(x,t)=0\\
            u(x,0)=h(x)
        \end{numcases}
    \end{subequations}
    où \( h\) est une fonction bien régulière; mettons \( C^p\). En suivant la méthode de la proposition nous devrions poser l'équation différentielle
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial X }{ \partial s }(s;x,t)=| s-1 |\\
            X(t;x,t)=x.
        \end{numcases}
    \end{subequations}
    Cela est la caractéristique passant par \( (x,t)\). Cependant il sera plus simple de chercher les caractéristiques en demandant qu'elles passent par \( (x_0,0)\). Nous allons donc plutôt résoudre pour \( X_{x_0}\) l'équation différentielle
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial X }{ \partial s }(s)=| s-1 |\\
            X_{x_0}(0)=x_0
        \end{numcases}
    \end{subequations}
    et les courbes caractéristiques seront les chemins
    \begin{equation}
        \gamma_{x_0}(s)=\begin{pmatrix}
            X_{x_0}(s)    \\
            s
        \end{pmatrix}.
    \end{equation}
    La résolution donne d'abord
    \begin{equation}
        X_{x_0}(s)=\begin{cases}
            -\frac{ s^2 }{2}+s+K_1    &   \text{si } s<1\\
            \frac{ s^2 }{2}-s+K_2    &    \text{si } s>1.
        \end{cases}
    \end{equation}
    Vu que la condition initiale est donnée pour \( s=0\), nous fixons \( K_1\) pour la condition initiale et \( K_2\) pour la continuité :
    \begin{equation}
        X_{x_0}(s)=\begin{cases}
            -\frac{ s^2 }{2}+s+x_1   &   \text{si }  s<1\\
            \frac{ 1 }{2}+x_0    &    \text{si } s=1\\
            \frac{ s^2 }{2}-s+1+x_0    &    \text{si } s>1.
        \end{cases}
    \end{equation}
    Soit \( (x,t)\in \eR^2\). Quelle caractéristique passe par là ? Nous allons déterminer la fonction \( x_0(x,t)\) qui donne le \( x_0\) tel que la caractéristique \( \gamma_{x_0}\) passe par \( (x,t)\). Nous devons résoudre
    \begin{equation}
        \begin{pmatrix}
            X_{x_0}(s)    \\
            s
        \end{pmatrix}=\begin{pmatrix}
            x    \\
            t
        \end{pmatrix}.
    \end{equation}
    Directement : \( s=t\). Et ensuite \( X_{x_0}(t)=x\). Nous avons
    \begin{equation}
        x_0(x,t)=\begin{cases}
            x+\frac{ t^2 }{2}-t   &   \text{si } t<1\\
            x-\frac{ 1 }{2}    &    \text{si } t=1\\
            x-\frac{ t^2 }{2}+t-1    &    \text{si } t>1.
        \end{cases}
    \end{equation}
    Le truc presque étonnant est que \( x_0\) est de classe \( C^1\). En effet le calcul de
    \begin{equation}
        \frac{ \partial x_0 }{ \partial t }(1)=\lim_{\epsilon\to 0}\frac{ x_0(x,1+\epsilon)-x_0(x,1) }{ \epsilon }
    \end{equation}
    se fait en séparant les limites \( \epsilon\to 0^+\) et \( \epsilon\to 0^-\). Le résultat est que \( \partial_tx_0(1)=0\). Nous avons donc
    \begin{equation}
        \frac{ \partial x_0 }{ \partial t }(t)=\begin{cases}
            t-1    &   \text{si } t<1\\
            0    &    \text{si } t=1\\
            -t+1    &    \text{si }t>1.
        \end{cases}
    \end{equation}
    Cela étant continu, la fonction \( x_0\) est de classe \( C^1\) en \( t\), et la dérivée en \( x\) étant toujours \( 1\), elle est de classe \( C^1\).

    En ce qui concerne la solution de l'équation de départ,
    \begin{equation}
        u(x,t)=h\big( X_{x_0(x,t)}(0) \big)=h\big( x_0(x,t) \big).
    \end{equation}
    Pourvu que \( h\) soit assez régulière, la fonction \( u\) est facilement de classe \( C^1\).
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Méthode des caractéristiques pour l'ordre \( 2\)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Principe général}
%---------------------------------------------------------------------------------------------------------------------------

Soit l'opérateur différentiel agissant sur \( C^2(\eR^2)\) :
\begin{equation}
    D=a(x,y)\frac{ \partial^2 }{ \partial x^2 }+b(x,y)\frac{ \partial^2 }{ \partial x\partial y }+c(x,y)\frac{ \partial^2 }{ \partial y^2 }.
\end{equation}
Nous voulons résoudre des équations du type \( Du=0\) pour \( u\colon \eR^2\to \eR\).

Pour commencer\cite{ooEIHMooRXOzwa}, et c'est le point crucial, nous voyons \( D\) comme un polynôme en \( \partial_x\) et \( \partial_y\) et nous le factorisons : si
\begin{equation}
    aX^2+bXY+cY=(\alpha X+\beta Y)(\gamma X+\delta Y)
\end{equation}
alors nous avons
\begin{equation}
    D=\left( \alpha\frac{ \partial  }{ \partial x }+\beta\frac{ \partial  }{ \partial y } \right)\left( \gamma\frac{ \partial  }{ \partial x }+\delta\frac{ \partial y }{ \partial  } \right)+\text{ termes d'ordre inférieurs}.
\end{equation}
Les «termes d'ordre inférieurs» sont ceux de la forme \( \alpha(x,y)\frac{ \partial \delta }{ \partial x }\frac{ \partial  }{ \partial y }\).


L'astuce est de poser
\begin{equation}
    v=(\gamma\partial_x+\delta\partial_y)u,
\end{equation}
et de résoudre le système
\begin{subequations}        \label{SUBESQooGCMNooEDWQHd}
    \begin{numcases}{}
        (\alpha\partial_x+\beta\partial_y)v=0\\
        (\gamma\partial_x+\delta\partial_y)u=v.
    \end{numcases}
\end{subequations}
Cela sont deux équations différentielles du premier ordre pour lesquelles nous avons déjà des techniques décrites en la section~\ref{SECooHKSLooOCYNDz}.

Afin que les fonctions \( \alpha\), \( \beta\), \( \gamma\) et \( \delta\) soient réelles, il faut que \( b^2-4ac\geq 0\). Sachant que \( a=\alpha\gamma\), \( b=\alpha\delta+\beta\gamma\) et \( c=\beta\delta\) cette condition sur \( a\), \( b\) et \( c\) donne
\begin{equation}
    (\alpha\delta+\beta\gamma)^2-4\alpha\gamma\beta\delta\geq 0.
\end{equation}
Cela revient à
\begin{equation}
    (\alpha\delta-\beta\gamma)^2\geq 0.
\end{equation}
Nous supposons à présent que l'inégalité soit stricte (cas hyperbolique). Nous avons en particulier que
\begin{equation}
    \alpha\delta-\beta\gamma\neq 0.
\end{equation}
Cette condition implique que les équations
\begin{equation}
    \begin{aligned}[]
        \frac{ dx }{ dt }=\alpha(x,y)&&\frac{ dy }{ dt }=\beta(x,y)
    \end{aligned}
\end{equation}
sont indépendantes des équations
\begin{equation}
    \begin{aligned}[]
        \frac{ dx }{ dt }=\gamma(x,y)&&\frac{ dy }{ dt }=\delta(x,y)
    \end{aligned}
\end{equation}
Ce sont les équations caractéristiques des équations \eqref{SUBESQooGCMNooEDWQHd}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Exemple : l'équation d'onde}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooYBBKooUOIlCS}

Nous considérons l'équation aux dérivées partielles
\begin{equation}
    \frac{ \partial^2u }{ \partial t^2 }-c^2\frac{ \partial^2 u }{ \partial x^2 }=0
\end{equation}
où \( c\) est une constante réelle. Nous en cherchons des solutions de classe \( C^2\).

L'opérateur différentiel est donné par le polynôme \( P(T,X)=T^2-c^2X^2\) qui se factorise en
\begin{equation}
    P=(T+cX)(T-cX),
\end{equation}
c'est-à-dire que nous pouvons récrire l'équation des ondes sous la forme
\begin{equation}
    (\partial_t+c\partial_x)(\partial_t-c\partial_x)u=0.
\end{equation}
Nous posons donc \( v=(\partial_t-c\partial_x)u\) et nous avons le système\cite{ooUQOJooSPNjlt}
\begin{subequations}
    \begin{numcases}{}
        (\partial_t+c\partial_x)v=0   \label{SUBEQooLQAPooVoJccp}\\
        (\partial_t-c\partial_x)u=v     \label{SUBEQooPWXMooDjThlJ}.
    \end{numcases}
\end{subequations}
La méthode des caractéristiques est efficace pour résoudre la première, et pour trouver la solution générale de l'homogène associée à la seconde.

Nous nous lançons dans la résolution de \eqref{SUBEQooLQAPooVoJccp}. Le flot est \( v=\begin{pmatrix}
    1    \\
    c
\end{pmatrix}\), et nous cherchons ses courbes intégrales sous la forme \( \varphi(t)=\big( t,x(t) \big)\). Immédiatement, \( x'(t)=c\), ce qui donne
\begin{equation}
    \gamma_C(t)=\begin{pmatrix}
        t    \\
        ct+C
    \end{pmatrix}.
\end{equation}
Cela donne une caractéristique pour chaque valeur de \( C\). En posant \( \tilde v_C(t)=v(t,ct+C)\) nous avons
\begin{equation}
    \tilde v'_C(t)=\frac{ \partial v }{ \partial t }\big( \gamma_C(t) \big)+c\frac{ \partial v }{ \partial t }\big( \gamma_C(t) \big)=0.
\end{equation}
Donc \( \tilde v_C\) est une fonction constante. Donc \( u\) est constant sur la courbe \( \gamma_C\) dont l'équation cartésienne est \( x-ct=C\). Cela implique que
\begin{equation}
    v(t,x)=f(x-ct)
\end{equation}
où \( f\) est une fonction de classe \( C^1\). En effet si \( (t_1,x_1)\) et \( (t_2,x_2)\) vérifient \( x_1-ct_1=x_2-ct_2\) alors \( v(t_1,x_2)=v(t_2,x_2)\). Le fait que \( f\) soit \( C^1\) est une demande que \( u\) soit au final dans \( C^2\).

Nous devons maintenant résoudre l'équation \eqref{SUBEQooPWXMooDjThlJ}
\begin{equation}
    (\partial_t-c\partial_x)u=v.
\end{equation}

Nous allons agir conformément à la stratégie expliquée par le lemme~\ref{LEMooEWUPooXNJMcc}. Nous devons résoudre \( Du=v\) avec
\begin{equation}
    \begin{aligned}
        D\colon C^2(\eR)&\to D(C^2(\eR)) \\
        u&\mapsto (\partial_t-c\partial_x)u.
    \end{aligned}
\end{equation}
Par la même méthode des caractéristiques que celle déjà menée plus haut nous trouvons \( \ker(D)\) comme solution générale de \( (\partial_t-c\partial_x)u_G=0\). C'est-à-dire
\begin{equation}
    u_G=g(x+ct)
\end{equation}
où \( g\) est une fonction quelconque de classe \( C^2\).

Il nous faut maintenant une solution particulière de
\begin{equation}
    (\partial_t-c\partial_x)u_P(t,x)=f(x-ct).
\end{equation}
Si \( F\) est une primitive de \( f\) alors
\begin{equation}
    u_P(t,x)=-\frac{ 1 }{ 2c }F(x-ct)
\end{equation}
fonctionne. Vu que \( f\) est quelconque dans \( C^1(\eR)\), la fonction \( F\) est un élément quelconque de \( C^2(\eR)\). Au final, la solution générale de l'équation des ondes est
\begin{equation}
    u(t,x)=g_1(x+ct)+g_2(x-ct)
\end{equation}
où \( g_1\) et \( g_2\) sont des éléments de \( C^2(\eR)\).

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Classification des équations du second ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit une équation générale d'ordre \( 2\) sur \( \eR^2\) :
\begin{equation}
    a\frac{ \partial^2 u }{ \partial x^2 }+b\frac{ \partial^2u }{ \partial x\partial y }+c\frac{ \partial^2u }{ \partial y^2 }+d\frac{ \partial u }{ \partial x }+e\frac{ \partial u }{ \partial y }+\beta u=f.
\end{equation}
En ce qui concerne son symbole principal nous avons
\begin{equation}
    \sigma(x,y,\xi_1,\xi_2)=a(x,y)\xi_1^2+b(x,y)\xi_1\xi_2+c(x,y)\xi_2^2,
\end{equation}
ce qui donne l'équation des caractéristiques
\begin{equation}        \label{EQooSAFNooEimPhO}
    a\left( \frac{ \partial \phi }{ \partial x } \right)^2+b\frac{ \partial \phi }{ \partial x }\frac{ \partial \phi }{ \partial y }+c\left( \frac{ \partial \phi }{ \partial y } \right)^2=0.
\end{equation}
Si nous nous posons sur un point \( (x_0,y_0)\) tel que \( \nabla\phi(x_0,y_0)=0\) et \( \partial_x\phi(x_0,y_0)\neq 0\) alors, via le théorème de la fonction implicite\footnote{Théorème~\ref{ThoAcaWho}.}, la condition \( \phi(x,y)=0\) définit une fonction \( y\mapsto x(y)\) vérifiant
\begin{equation}
    \phi\big( x(y),y \big)=0
\end{equation}
pour tout \( y\) dans un voisinage de \( y_0\).

Nous pouvons obtenir une équation différentielle ordinaire pour \( x\) de la façon suivante. D'abord nous posons \( \varphi(y)=\phi\big( x(y),y \big)\) et ensuite nous calculons la dérivée de \( \varphi\) (qui est nulle par construction) :
\begin{equation}
    0=\varphi'(y)=\frac{ \partial \phi }{ \partial x }x'+\frac{ \partial \phi }{ \partial y }.
\end{equation}
Nous pouvons donc remplacer \( \partial_y\phi\) par \( x'\partial_x\phi\) dans l'équation des caractéristiques \eqref{EQooSAFNooEimPhO} :
\begin{equation}
    a\left( \frac{ \partial \phi }{ \partial x } \right)^2+bx'\left( \frac{ \partial \phi }{ \partial x } \right)^2+c\left( \frac{ \partial \phi }{ \partial x } \right)^2(x')^2=0.
\end{equation}
Vu que nous avons supposé \( (\partial_x\phi)\neq 0\) sur un voisinage de \( (x_0,y_0)\) nous pouvons simplifier par \( (\partial_x\phi)^2\) et avoir l'équation différentielle ordinaire
\begin{equation}
    a\big( x(y),y \big)+b\big( x(y),y \big)x'(y)+c\big( x(y),y \big)x'(y)^2=0.
\end{equation}
Notons que, conformément à ce que raconte le théorème des fonctions implicites, nous avons pris un voisinage de \( y_0\) suffisamment petit pour que \( x(y)\) reste dans un voisinage de \( x_0\). Ce voisinage étant, nous pouvons le restreindre pour nous assurer du signe de \( a\), \( b\) et \( c\). Cela est évidemment très théorique parce que le théorème de la fonction implicite parle de l'existence de voisinages, mais pas de façon de les construire.

Nous donnons la classification suivante.

\begin{definition}
    Si \( b^2-4ac<0\) alors l'équation est \defe{elliptique}{elliptique!équation aux dérivées partielles}.

    Si \( b^2-4ac>0\) alors l'équation est \defe{hyperbolique}{hyperbolique!équation aux dérivées partielles}.

    Si \( b^2-4ac=0\) alors l'équation est \defe{parabolique}{parabolique!équation aux dérivées partielles}.
\end{definition}

\begin{example}
    Un exemple d'équation parabolique est l'équation de la chaleur
    \begin{equation}
        \frac{ \partial u }{ \partial t }-\alpha\frac{ \partial^2u }{ \partial x^2 }=0
    \end{equation}
    où \( \alpha>0\) est une constante. Cette équation est avec \( a=c=0\), donc elle est parabolique.
\end{example}

Une équation aux dérivées partielles peut changer de nature selon le point.

\begin{example}
    Soit l'équation
    \begin{equation}
        \frac{ \partial^2u }{ \partial x^2 }-(x^2-y^2)\frac{ \partial^2u }{ \partial y^2 }=0.
    \end{equation}
    Son \( b^2-4ac\) vaut \( 4(x^2-y^2)\). Elle peut donc être hyperbolique, parabolique ou elliptique selon le point où l'on se trouve.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Problème au limite}
%---------------------------------------------------------------------------------------------------------------------------

Dans les définitions qui suivent nous considérons un ouvert \( \Omega\subset \eR^d\) assez régulier et possédant en particulier un vecteur normal extérieur \( n(x)\) pour tout point \( x\in\partial\Omega\).

\begin{definition}
    Un problème aux limites \defe{de Dirichlet}{problème!limite de Dirichlet} est d'imposer la condition
    \begin{equation}
        u(x)=g(x)
    \end{equation}
    pour tout \( x\in\Gamma\subset \partial\Omega\). C'est-à-dire imposer la valeur de \( u\) sur une partie du bord du domaine.
\end{definition}

\begin{definition}
    Un problème aux limites de \defe{Von Neumann}{problème!limite de Von Neumann} est d'imposer
    \begin{equation}
        \frac{ \partial u }{ \partial n }\cdot x=g(x)
    \end{equation}
    pour tout \( x\in\Gamma\subset\partial\Omega\). C'est-à-dire imposer les valeurs de la dérivée normale de \( u\) sur une partie du bord.
\end{definition}

\begin{example}
    Lorsqu'on veut imposer un flux de chaleur aux bords d'un domaine pour l'équation de la chaleur, il s'agit de poser des conditions de type Von Neumann.
\end{example}

\begin{definition}
    Soit un domaine \( \Omega\) de \( \eR^d\) et un opérateur différentiel \( L\) sur une partie de \( \Fun(\Omega)\). Soit une fonction \( g\) sur \( \partial\Omega\). Un problème \defe{aux limites stationnaires}{problème!aux limites stationnaires} est un problème du type : trouver \( u\) définie sur \( \Omega\) telle que
    \begin{subequations}
        \begin{numcases}{}
            L(u)=f\\
            u|_{\partial\Omega}=g.
        \end{numcases}
    \end{subequations}
\end{definition}

\begin{definition}
    Un problème \defe{aux limites d'évolution}{problème!aux limites d'évolution} est du type : trouver \( u\in\Fun\big( \mathopen] 0 , \infty \mathclose[\times \Omega \big)\) tel que
        \begin{equation}
            \begin{cases}
            \frac{ \partial^mu }{ \partial t^m }+L(u)=f    &   \text{sur } \mathopen] 0 , \infty \mathclose[\times \Omega\\
            u(t,.)=g(t,.)    &    \text{sur }\mathopen] 0 , \infty \mathclose[\times \partial\Omega\\
                u(0,.)=u_0    &    \text{sur }\Omega
            \end{cases}
        \end{equation}
        où \( u_0\) est une fonction sur \( \Omega\).

        L'opérateur \( L\) ne doit pas opérer sur la partie «\( t\)» de \( u\).
\end{definition}

\begin{definition}[Problème bien posé au sens de Hadamard]      \label{DEFooSNIRooBFYSFh}
    Un problème aux limites est \defe{bien posé au sens de Hadamard}{problème!bien posé} si
    \begin{enumerate}
        \item
            Il admet une unique solution.
        \item
            La solution dépend de façon continue en les données du problème.
    \end{enumerate}
    La continuité est au sens des normes sur les fonctions sur \( \partial\Omega\) et sur \( \Omega\) en ce qui concerne les fonctions «données» du problème et des normes pour les fonctions sur \( \Omega\) ou \( \bar\Omega\) en ce qui concerne la solution.
\end{definition}
\index{Hadamard!conditions}

\begin{example}[Un problème de Dirichlet bien posé]
    Trouver la fonction \( u\) définie sur \( \Omega= \mathopen[ 0 , 1 \mathclose]^2\) telle que
    \begin{equation}
        \begin{cases}
            -\Delta u=0    &   \text{sur } \Omega\\
            u=g    &    \text{sur }\partial \Omega.
        \end{cases}
    \end{equation}

\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Principe du maximum}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{ooWWBQooKIciWi}]        \label{LEMooSPVUooDQOeom}
    Soit un ouvert borné \( \Omega\subset \eR^n\). Soit une matrice symétrique strictement définie positive \( A\) telle que \( A_{ij}\in C^O(\bar \Omega)\) pour laquelle il existe \( \lambda>0\) minorant toutes les valeurs propres de toutes les matrices \( A(x)\) pour \( x\in \Omega\)\footnote{Cela est plus que dire que toutes les \( A(x)\) son symétriques strictement définie positive.}.

    Nous posons \( L'=-\sum_{ij}A_{ij}\partial_{ij}\). Si \( u\in C^2(\Omega)\) atteint un minimum local en \( x_0\in \Omega\), alors
    \begin{equation}
        (L'u)(x_0)\leq 0.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous allons bien entendu diagonaliser \( A\). Si \( T\) est une matrice nous avons, en posant \( u(x)=v(Tx)\) :
    \begin{equation}
        \frac{ \partial u }{ \partial x_i }(x)=\sum_k\frac{ \partial v }{ \partial x_k }(Tx)T_{ki}
    \end{equation}
    et
    \begin{equation}
        \frac{ \partial^2u }{ \partial x_j\partial x_i }(x)=\sum_{kl}T_{ki}T_{lj}\frac{ \partial^2v }{ \partial x_l\partial x_k }(Tx).
    \end{equation}
    Si \( T\) est en particulier une matrice orthogonale diagonalisant \( A\) (théorème~\ref{ThoeTMXla}) nous avons \( T^{-1}=T^t\) et
    \begin{equation}
        \sum_{ij}T_{ki}A_{ij}T^{-1}_{jl}=D_{kl}=\lambda_k\delta_{kl}
    \end{equation}
    où les \( \lambda_k\) sont les valeurs propres de \( A\). Notons que partout ici, tout est fonction de \( x\) sur \( \Omega\) : tant \( A\) que \( T\) que les \( \lambda_k\). Avec tous ces résultats nous calculons vite que
    \begin{equation}
        (Lu)(x_0)=-\sum_k\lambda_k(\partial^2_kv)(Tx_0).
    \end{equation}
    Si \( x_0\in \Omega\) est un minimum local de \( u\), alors l'application \( v\) a un minimum local en \( T^{-1} x_0\). Et donc
    \begin{equation}
        \frac{ \partial^2v }{ \partial x_k^2 }(Tx_0)\geq 0.
    \end{equation}
    Du coup,
    \begin{equation}
        (Lu)(x_0)\leq 0.
    \end{equation}
\end{proof}


\begin{lemma}[\cite{ooWWBQooKIciWi}]        \label{LEMooDVUYooHEkrsL}
    Soit un ouvert borné \( \Omega\subset \eR^n\). Soit une matrice symétrique strictement définie positive \( A\) telle que \( A_{ij}\in C^O(\bar \Omega)\) pour laquelle il existe \( \lambda>0\) minorant toutes les valeurs propres de toutes les matrices \( A(x)\) pour \( x\in \Omega\).

    Nous posons
    \begin{equation}
        L=-\sum_{ij}A_{ij}\partial^2_{ij}+\sum_ib_i\partial_i+c
    \end{equation}
    où \( b_i,c\in C^0(\bar \Omega)\).

    Soit \( u\in C^0(\bar \Omega)\cap C^2(\Omega)\) telle que \( Lu\geq 0\) sur \( \Omega\). Alors
    \begin{enumerate}
        \item       \label{ITEMooZNUVooNGPXVc}
            Si \( c=0\) alors
            \begin{equation}        \label{EQooOJRWooIMemRN}
                \min_{\bar \Omega}(u)=\min_{\partial\Omega}(u).
            \end{equation}
        \item       \label{ITEMooQOQIooJqTzdA}
            Si \( c\geq 0\) alors
            \begin{equation}        \label{EQooGSXVooGUnRnt}
                \min_{\bar\Omega}(u)\geq \min_{\partial\Omega}(-u_-)
            \end{equation}
            où \( u_-\) est défini par
            \begin{equation}
                u_-(x)=\begin{cases}
                    0    &   \text{si } u(x)\geq 0\\
                    -u(x)    &    \text{si }u(x)\leq 0.
                \end{cases}
            \end{equation}
    \end{enumerate}
\end{lemma}

\begin{proof}
    D'abord, vu que \( \Omega\) est borné, la fermeture \( \bar \Omega\) est compacte et \( u\) y atteint son minimum. De plus \( \partial \Omega\) est également compact (borné et le complémentaire est ouvert parce que \( \Omega\) est ouvert). Donc \( u\) y atteint également son minimum. Cela pour dire que les minima écrits dans \eqref{EQooOJRWooIMemRN} et \eqref{EQooGSXVooGUnRnt} ont un sens.

    Pour~\ref{ITEMooZNUVooNGPXVc}.

    \begin{subproof}
        \item[\( Lu\geq \eta>0\)]

            Nous supposons qu'il existe \( \eta>0\) tel que \( (Lu)(x)\geq \eta \) pour tout \( x\in \Omega\). Soit \( x_0\) le point minimum sur \(\bar \Omega\). Si \( x_0\in \Omega\) alors il est intérieur et \( \partial_iu(x_0)=0\) par la proposition~\ref{PropUQRooPgJsuz}. Dans ce cas nous avons
            \begin{equation}
                (Lu)(x_0)=(L'u)(x_0)> 0,
            \end{equation}
            ce qui contredit le lemme~\ref{LEMooSPVUooDQOeom}. Nous en déduisons que le minimum de \( u\) sur \( \bar \Omega\) n'est pas atteint dans \( \Omega\), mais sur \( \partial\Omega\). Pour la définition de la frontière, voir~\ref{DEFooACVLooRwehTl}. Ici nous avons \( \bar\Omega\setminus\Omega=\partial\Omega\).

            Cela prouve \eqref{EQooOJRWooIMemRN} dans ce cas.

        \item[\( Lu\geq 0\) sur \( \Omega\)]

            Nous prenons maintenant le cas général. Nous posons
            \begin{equation}
                u_{\gamma,\epsilon}(x)=u(x)-\epsilon e^{\gamma x_1}.
            \end{equation}
            Nous avons\footnote{Nous abusons un peu de l'écriture parce que ce que nous calculons vraiment est \( L\big( x\mapsto  e^{\gamma x_1} \big)\).}
            \begin{equation}
                L( e^{\gamma x_1})= e^{\gamma x_1}\big( -A_{11}(x)\gamma^2+b_1(x)\gamma \big).
            \end{equation}
            Soit \( \gamma\) suffisamment grand pour que
            \begin{equation}
                \lambda\gamma^2-\| b_1 \|_{\bar \Omega}\gamma>0.
            \end{equation}
            Ici \( \lambda\) minore toutes les valeurs propres des \( A(x)\) et nous notons que \( \gamma\) ne dépend pas de \( \epsilon\). En utilisant l'inégalité du lemme~\ref{LemWZFSooYvksjw}\ref{LemWZFSooYvksjw}, pour tout \( \xi\in \eR^n\) nous avons
            \begin{equation}
                \sum_{ij}A_{ij}\xi_i\xi_j\geq \lambda| \xi |^2,
            \end{equation}
            ce qui donne avec \( \xi=e_1\) : \( A_{11}\geq \lambda\), et même pour être plus précis : \( A_{11}(x)\geq \lambda\) pour tout \( x\). Ces inégalités donnent
            \begin{equation}
                -A_{11}(x)\gamma^2+b_1(x)\gamma\leq -\lambda\gamma^2+\| b_1 \|_{\bar \Omega}\gamma<0.
            \end{equation}
            Nous avons donc
            \begin{equation}        \label{EQooXCGQooLGMnvL}
                L( e^{\gamma x_1})= e^{\gamma x_1}\big( -A_{11}(x)\gamma^2+b_1(x)\gamma \big)\leq  -\lambda\gamma^2+\| b_1 \|_{\bar \Omega}\gamma<0.
            \end{equation}
            Nous posons
            \begin{equation}
                \eta=\epsilon\big( \lambda\gamma^2-\| b_1 \|_{\bar\Omega}\gamma \big)\min_{\bar \Omega}( e^{\gamma x_1}),
            \end{equation}
            où le minimum a un sens parce que \( \Omega\) est borné. Nous avons \( \eta>0\).

            C'est le moment de calculer ce que \( u_{\epsilon}\) peut pour nous :
            \begin{subequations}
                \begin{align}
                    L(u_{\epsilon})&=Lu-\epsilon L( e^{\gamma x_1})\\
                    &\geq -\epsilon L( e^{\gamma x_1})       \label{SUBEQooIZTWooVjjlTZ}\\
                    &=-\epsilon e^{\gamma x_1}\big( -A_{11}(x)\gamma^2+b_1(x)\gamma \big)\\
                    &\geq -\epsilon e^{\gamma x_1}\big( \lambda\gamma^2-\| b_1 \|_{\bar\Omega}\gamma \big)  \label{SUBEQooHBALooSXxUeL}\\
                    &\geq \eta>0.
                \end{align}
            \end{subequations}
            Justification :
            \begin{itemize}
                \item \eqref{SUBEQooIZTWooVjjlTZ} parce que \( Lu\geq 0\).
                \item \eqref{SUBEQooHBALooSXxUeL} par \eqref{EQooXCGQooLGMnvL}.
            \end{itemize}

            La fonction \( u_{\epsilon}\) est donc dans le cas précédent et nous avons
            \begin{equation}
                \min_{x\in\bar \Omega}\big( u(x)-\epsilon e^{\gamma x_1} \big)=\min_{x\in\partial \Omega}\big( u(x)-\epsilon e^{\gamma x_1} \big).
            \end{equation}
            Mentionnons le fait que le choix fait de \( \gamma\) ne dépend pas de \( \epsilon\). Nous pouvons donc encore faire varier \( \epsilon\) sans toucher à \( \gamma\) et en maintenant toutes les inégalités prouvées jusqu'ici.

            Vu que l'expression \(  e^{\gamma x_1}\) est majorable sur \( \bar \Omega\) nous avons convergence uniforme
            \begin{equation}
                u_{\epsilon} \stackrel{\| . \|_{\bar\Omega}}{\longrightarrow}u
            \end{equation}
            pour \( \epsilon\to 0\).

            Supposons qu'aucun point de \( \partial \Omega\) ne réalise le minimum de \( u\). Alors il existe \( x_0\in \Omega\) tel que \( u(x_0)>u(x)\) pour tout \( x\in \partial \Omega\). Mais comme \( \partial\Omega\) est compact, il existe \( \eta>0\) tel que
            \begin{equation}
                u(x_0)<u(x)+\eta.
            \end{equation}
            Soit \( \epsilon\) tel que \( \| u-u_{\epsilon} \|_{\bar \Omega}< \eta/2\). Nous avons
            \begin{equation}
                u(x_0)<u(x)-\eta
            \end{equation}
            et donc aussi
            \begin{equation}
                u_{\epsilon}(x_0)<u(x_0)+\frac{ \eta }{2}<u(x)-\frac{ \eta }{2}<u_{\epsilon}(x),
            \end{equation}
            ce qui signifierait que \( u_{\epsilon}\) prend son minimum dans \( \Omega\). Or nous savons que ce n'est pas le cas.

            Donc il existe un point de \( \partial\Omega\) qui réalise le minimum de \( u\).

    \end{subproof}

    Pour~\ref{ITEMooQOQIooJqTzdA}.

    Supposons pour commencer que \( u\geq 0\) sur \( \Omega\). Alors par continuité \( u\geq 0\) sur \( \bar \Omega\). Alors \( u_-=0\) et
    \begin{equation}
        \min_{\bar\Omega}u\geq 0,
    \end{equation}
    ce qui fait que l'inégalité \eqref{EQooGSXVooGUnRnt} est évidente.

    Nous supposons donc que l'ensemble
    \begin{equation}
        \Omega_-=\{ x\in \Omega\tq u(x)<0 \}
    \end{equation}
    est non vide. Notons que c'est également un ouvert.

    Soit \( \bar Lu=Lu-cu\). Vu que \( c\geq 0\) et que \( Lu\geq 0\) nous avons \( \bar Lu\geq 0\) sur \( \Omega_-\). Par le point~\ref{ITEMooZNUVooNGPXVc} nous avons
    \begin{equation}
        \min_{x\in \bar \Omega}u(x)=\min_{x\in\partial\Omega}u(x).
    \end{equation}
    Mais le minimum de \( u\) est certainement atteint dans \( \Omega_-\), donc
    \begin{equation}
        \min_{x\in\bar\Omega_-}u(x)=\min_{x\in\bar\Omega}u(x).
    \end{equation}
    Cela nous donne une première bonne égalité :
    \begin{equation}
        \min_{x\in\partial\Omega_-}u(x)=\min_{x\in\bar \Omega}u(x).
    \end{equation}
    Nous pouvons la prolonger :
    \begin{subequations}
        \begin{align}
            \min_{x\in\bar\Omega}u(x)&=\min_{x\in \partial\Omega_-}u(x)\\
            &=\min_{x\in\partial\Omega_-}\big( -u_-(x) \big)\\
            &=\min_{x\in\partial\Omega_-\cap\partial\Omega}\big( -u_-(x) \big)  \label{SUBEQooUYFXooKKrHiF}\\
            &=\min_{x\in\partial\Omega}\big( -u_-(x) \big)          \label{SUBEQooKIHRooPiADmK}
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooUYFXooKKrHiF} nous avons la décomposition
            \begin{equation}
                \partial \Omega_-=\big( \partial\Omega_-\cap\Omega \big)\cup\big( \partial\Omega_-\cap\partial \Omega \big).
            \end{equation}
            Or sur \( \partial\Omega_-\cap\Omega\) nous avons \( u(x)=0\) et donc pas le minimum.
        \item Pour \eqref{SUBEQooKIHRooPiADmK}. Sur \( \partial\Omega\), le minimum est atteint dans la partie \( \partial\Omega_-\) parce que le reste ne contient que des valeurs positives de \( u\).
    \end{itemize}

    Cela prouve ce que nous voulions.
\end{proof}

\begin{theorem}(Principe du maximum fort)
    Soit un ouvert borné \( \Omega\) de \( \eR^n\). Soit une matrice \( A\) dont
    \begin{itemize}
        \item \( A_{ij}\) est dans \( C^0(\bar \Omega)\)
        \item \( A(x)\) est symétrique strictement définie positive pour tout \( x\).
        \item Il existe \( \lambda>0\) minimisant toutes les valeurs propres des toutes les matrices \( A(x)\) sur \( \bar \Omega\).
    \end{itemize}
    Soit \( b_i,c\in C^0(\bar \Omega)\) avec \( c(x)\geq 0\) sur \( \bar \Omega\).

    Soit \( u\in C^0(\bar \Omega)\cap C^2(\Omega)\) telle que
    \begin{subequations}
        \begin{numcases}{}
            -\sum_{ij}A_{ij}\partial_{ij}u(x)+\sum_ib_i\partial_iu(x)\geq 0\\
            u(x)\geq 0\;\forall x\in\partial\Omega.
        \end{numcases}
    \end{subequations}
    Alors \( u\geq 0\) sur \( \bar\Omega\).
\end{theorem}

\begin{proof}
    Nous appliquons le lemme~\ref{LEMooDVUYooHEkrsL}\ref{ITEMooQOQIooJqTzdA} :
    \begin{equation}
        \min_{\bar \Omega}u\geq \min_{\partial\Omega}(-u_-).
    \end{equation}
    Vu que \( u(x)\geq 0\) sur \( \partial \Omega\), nous avons \( u_-=0\) sur \( \partial\Omega\) et donc \( \min_{\bar\Omega}u\geq 0\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Quelques exemples}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Un changement de variables}
%---------------------------------------------------------------------------------------------------------------------------

Soit l'équation différentielle
\begin{equation}        \label{EQooPGDPooTjiVhB}
    \frac{ \partial u }{ \partial t }-\frac{ \partial^2u }{ \partial x^2 }=0
\end{equation}
sur \( \Omega=\mathopen] 0 , \infty \mathclose[\times \eR\). Nous imposons la condition aux bords
    \begin{equation}        \label{EQooJHPVooMkvODe}
    u(0,x)=\begin{cases}
        0    &   \text{si } x<0\\
        1    &    \text{si } x>0.
    \end{cases}
\end{equation}
Nous cherchons les solutions sous la forme
\begin{equation}
    u(t,x)=f\left( \frac{ x }{ \sqrt{ t } } \right).
\end{equation}

Nous avons
\begin{equation}
    \frac{ \partial u }{ \partial t }=-\frac{ 1 }{2}xt^{-3/2}f'\left( \frac{ x }{ \sqrt{ t } } \right))
\end{equation}
ainsi que
\begin{equation}
    \frac{ \partial u }{ \partial x }=\frac{1}{ \sqrt{ t } }f'\left( \frac{ x }{ \sqrt{ t } } \right)
\end{equation}
et
\begin{equation}
    \frac{ \partial^2u }{ \partial x^2 }=\frac{1}{ t }f''\left( \frac{ x }{ \sqrt{ t } } \right).
\end{equation}
En remettant le tout dans l'équation de départ et en simplifiant par \( 1/t\) (qui est permis parce que \( t>0\)) :
\begin{equation}        \label{EQooCRKIooYNhvaA}
    -\frac{ 1 }{2}f'\left( \frac{ x }{ \sqrt{ t } } \right)\frac{ x }{ \sqrt{ t } }-f''\left( \frac{ x }{ \sqrt{ t } } \right)=0.
\end{equation}

Nous résolvons l'équation différentielle
\begin{equation}        \label{EQooEFQPooGWVoUq}
    \frac{ z }{ 2 }g'(z)+g''(z)
\end{equation}
pour la fonction \( g\) de la variable réelle \( z\). Cela fait, la réponse sera \( f=g\circ z\) où \( z\) serait la fonction
\begin{equation}
    z(x,t)=x/\sqrt{ t },
\end{equation}

Pour résoudre \eqref{EQooEFQPooGWVoUq} nous commençons par résoudre pour la dérivée \( h=g'\), c'est-à-dire l'équation différentielle
\begin{equation}
    \frac{ z }{ 2 }h(z)+h'(z)=0
\end{equation}
qui donne
\begin{equation}
    \frac{ g'(z) }{ g'z }=-z/2.
\end{equation}
Une intégration fournit \( \ln\big( h(z) \big)=-z^2/4+K\) et donc
\begin{equation}
    h(z)=K e^{-z^2/4}
\end{equation}
et
\begin{equation}
    g(z)=C+K\int_0^z e^{-s^2/4}ds.
\end{equation}
Nous ne pouvons pas aller plus loin parce que nous ne sommes pas capables de calculer la primitive demandée. Nous laissons donc \( g\) sous cette forme et nous posons
\begin{equation}
    u(x,t)=g(x/\sqrt{ t }),
\end{equation}
en pleine confiance du fait que cela soit une solution de l'équation aux dérivées partielles \eqref{EQooPGDPooTjiVhB}.

Nous devons fixer \( K\) et \( C\) de telle façon à respecter les conditions aux bords \eqref{EQooJHPVooMkvODe}. D'abord écrivons aussi explicitement que possible la fonction \( u\) :
\begin{equation}
    u(t,x)=K+C\int_0^{x\sqrt{ t }} e^{-s^2/4}ds.
\end{equation}
Il faut calculer \( u(0,x)\) en termes de \( C\) et \( K\). Pour cela nous calculons, pour un \( x\) fixé :
\begin{equation}
    \lim_{t\to 0^+} u(t,x)=K+C\lim_{t\to 0^+} \int_0^{x/\sqrt{ t }} e^{-s^2/4}ds.
\end{equation}
En utilisant un petit changement de variables sur l'intégrale gaussienne de l'exemple~\ref{EXooLUFAooGcxFUW}, et en remarquant que la fonction est symétrique,
\begin{equation}
    \lim_{t\to 0^+} u(t,x) = \begin{cases}
        K+C\sqrt{ \pi }    &   \text{si } x>0 \\
        K-C\sqrt{ \pi }    &    \text{si } x<0.
    \end{cases}
\end{equation}
À résoudre :
\begin{subequations}
    \begin{numcases}{}
        K+C\sqrt{ \pi }=1\\
        K-C\sqrt{ \pi }=0.
    \end{numcases}
\end{subequations}
Solution : \( C=1/2\sqrt{ \pi }\) et \( K=1/2\). Au final,
\begin{equation}
    u(t,x)=\frac{ 1 }{2}+\frac{1}{ 2\sqrt{ \pi } }\int_0^{x/\sqrt{ t }} e^{-s^2/4}.
\end{equation}
Notons que cela donne une valeur pour \( u(t,0)\) :
\begin{equation}
    u(t,0)=\frac{ 1 }{2}.
\end{equation}
