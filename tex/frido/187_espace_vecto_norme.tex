% This is part of Le Frido
% Copyright (c) 2008-2020
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sommes de familles infinies}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooHHDXooUgLhHR}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Convergence commutative}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( x_k\) une suite dans un espace vectoriel normé \( E\). Nous disons que la suite \defe{converge commutativement}{convergence!commutative} vers \( x\in E\) si \( \lim_{n\to \infty}\| x_n-x \| =0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons aussi
    \begin{equation}
        \lim_{n\to \infty} \| x_{\tau(k)}-x \|=0.
    \end{equation}
    La notion de convergence commutative est surtout intéressante pour les séries. La somme
    \begin{equation}
        \sum_{k=0}^{\infty}x_k
    \end{equation}
    converge commutativement vers \( x\) si \( \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_k \|=0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons
    \begin{equation}
        \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_{\tau(k)} \|=0.
    \end{equation}
\end{definition}

Nous démontrons maintenant qu'une série converge réelle commutativement si et seulement si elle converge absolument.

\begin{proposition} \label{PopriXWvIY}
    Soit \( (a_i)_{i\in \eN}\) une suite absolument convergente\footnote{Définition \ref{DefVFUIXwU}.} dans \( \eC\). Alors elle converge commutativement.
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). Nous posons \( \sum_{i=0}^\infty a_i=a\) et nous considérons \( N\) tel que
    \begin{equation}
        | \sum_{i=0}^Na_i-a |<\epsilon.
    \end{equation}
    Étant donné que la série des \( | a_i |\) converge, il existe \( N_1\) tel que pour tout \( p,q>N_1\) nous ayons \( \sum_{i=p}^q| a_i |<\epsilon\). Nous considérons maintenant une bijection \( \tau\colon \eN\to \eN \). Prouvons que la série \( \sum_{i=0}^{\infty}| a_{\tau(i)} |\) converge. Nous choisissons \( M\) de telle sorte que pour tout \( n>M\), \( \tau(n)>N_1\). Si \( s_k\) est la somme partielle de la suite \( ( a_{\tau(i)} )_{i\in \eN}\) et si \( M<p<q \) nous avons
    \begin{equation}
        | s_q-s_p |= | \sum_{i=p}^q a_{\tau(i)} | \leq \sum_{i=p}^q| a_{\tau(i)} |<\epsilon.
    \end{equation}
    Cela montre que \( (s_k)\) est une suite de Cauchy. Elle est alors convergente et nous en déduisons que la série
    \begin{equation}
        \sum_{i=0}^{\infty}a_{\tau(i)}
    \end{equation}
    converge. Nous devons montrer à présent qu'elle converge vers la même limite que la somme «usuelle» \( \lim_{N\to \infty} \sum_{i=0}^Na_i\).

    Soit \( n>\max\{ M,N \}\). Alors
    \begin{equation}
        \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k=\sum_{k=0}^Ma_{\tau(k)}-\sum_{k=0}^Na_k+\underbrace{\sum_{M+1}^na_{\tau(k)}}_{<\epsilon}-\underbrace{\sum_{k=N+1}^na_k}_{<\epsilon}.
    \end{equation}
    Par construction les deux derniers termes sont plus petits que \( \epsilon\) parce que \( M\) et \( N\) sont les constantes de Cauchy pour les séries \( \sum a_{\tau(i)}\) et \( \sum a_i\). Afin de traiter les deux premiers termes, quitte à redéfinir \( M\), nous supposons que \( \{ 1,\ldots, N \}\subset \tau\{ 1,\ldots, M \}\); par conséquent tous les \( a_i\) avec \( i<N\) sont atteints par les \( a_{\tau(i)}\) avec \( i<M\). Dans ce cas, les termes qui restent dans la différence
    \begin{equation}
        \sum_{k=0}a_{\tau(k)}-\sum_{k=0}^Na_k
    \end{equation}
    sont des \( a_k\) avec \( k>N\). Cette différence est donc en valeur absolue plus petite que \( \epsilon\), et nous avons en fin de compte que
    \begin{equation}
        \left| \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k \right| <\epsilon.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PropyFJXpr}
    Soit \( \sum_{k=0}^{\infty}a_k\) une série réelle qui converge mais qui ne converge pas absolument. Alors pour tout \( b\in \eR\), il existe une bijection \( \tau\colon \eN\to \eN\) telle que \( \sum_{i=0}^{\infty}a_{\tau(i)}=b\).
\end{proposition}
Pour une preuve, voir \href{http://gilles.dubois10.free.fr/analyse_reelle/seriescomconv.html}{chez Gilles Dubois}.

Les propositions~\ref{PopriXWvIY} et~\ref{PropyFJXpr} disent entre autres qu'une série dans \( \eC\) est commutativement sommable si et seulement si elle est absolument sommable.

Soit \( (a_i)_{i\in I}\) une famille de nombres complexes indexée par un ensemble \( I\) quelconque. Nous allons nous intéresser à la somme \( \sum_{i\in I}a_i\).


Soit \( \{ a_i \}_{i\in I}\) des nombres positifs. Nous définissons la somme
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini}}\sum_{j\in J}a_j.
\end{equation}
Notons que cela est une définition qui ne fonctionne bien que pour les sommes de nombres positifs. Si \( a_i=(-1)^i\), alors selon la définition nous aurions \( \sum_i(-1)^i=\infty\). Nous ne voulons évidemment pas un tel résultat.


\begin{definition}  \label{DefIkoheE}
    Si \( \{ v_i \}_{i\in I}\) est une famille de vecteurs dans un espace vectoriel normé indexée par un ensemble quelconque \( I\). Nous disons que cette famille est \defe{sommable}{famille!sommable} de somme \( v\) si pour tout \( \epsilon>0\), il existe un \( J_0\) fini dans \( I\) tel que pour tout ensemble fini \( K\) tel que \( J_0\subset K\) nous avons
    \begin{equation}
        \| \sum_{j\in K}v_j-v \|<\epsilon.
    \end{equation}
\end{definition}
Notons que cette définition implique la convergence commutative.

Dans le cas de familles de nombres réels positifs, nous avons une caractérisation plus comode.
\begin{proposition}  \label{DefHYgkkA}
        % TODOooCNAJooDWEXlI prouver ceci, changer le label et les endroits où c'est référentié.
Soit \( (a_i)_{i\in I}\) une famille de nombres réels positifs indexés par un ensemble quelconque \( I\). Nous définissons
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini dans } I}\sum_{j\in J}a_j.
\end{equation}
\end{proposition}

\begin{lemma}       \label{LEMooGXPGooZTJPoN}
    Soient un espace vectoriel normé \( V\) ainsi qu'une suite \( (a_k)_{k\in \eN}\) telles que \( \sum_{k\in \eN}\) existe. Alors
    \begin{equation}
        \sum_{k\in \eN}a_k=\lim_{N\to \infty} \sum_{k=0}^N a_k.
    \end{equation}
    La somme à gauche est celle de la définition \ref{DefIkoheE} et celle de droite est donnée par la définition \ref{DEFooNEVNooJlmJOC}.
\end{lemma}

\begin{example}
    La suite \( a_i=(-1)^i\) n'est pas sommable parce que quel que soit \( J_0\) fini dans \( \eN\), nous pouvons trouver \( J\) fini contenant \( J_0\) tel que \( \sum_{j\in J}(-1)^j>10\). Pour cela il suffit d'ajouter à \( J_0\) suffisamment de termes pairs. De la même façon en ajoutant des termes impairs, on peut obtenir \( \sum_{j\in J'}(-1)^i<-10\).
\end{example}

\begin{example}
    De temps en temps, la somme peut sortir d'un espace. Si nous considérons l'espace des polynômes \( \mathopen[ 0 , 1 \mathclose]\to \eR\) muni de la norme uniforme, la somme de l'ensemble
    \begin{equation}
        \{ 1,-1,\pm\frac{ x^n }{ n! } \}_{n\in \eN}
    \end{equation}
    est zéro.

    Par contre la somme de l'ensemble \( \{ 1,\frac{ x^n }{ n! } \}_{n\in \eN}\) est l'exponentielle qui n'est pas un polynôme.
\end{example}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooJLQAooAEbIvZ}
    Soient un espace vectoriel normé \( V\), deux ensembles disjoints \( A\) et \( B\) ainsi que \( v\colon A\cup B\to V\). Si \( \sum_{k\in A}v_k\) et \( \sum_{k\in B}v_k\) sont sommables\footnote{Définition \ref{DefIkoheE}.}, alors
    \begin{equation}
        \sum_{k\in A\cup B}v_k=\sum_{k\in A}v_k+\sum_{k\in B}v_k.
    \end{equation}
\end{proposition}

\begin{proposition}[Distributivité de la somme infinie] \label{PropQXqEPuG}
    Soient \( E\) un espace normé, une suite \( (u_k)\) dans \( \GL(E)\) ainsi que \( a\in\GL(E)\). Pourvu que la série \( \sum_{n=0}^{\infty}u_k\) converge nous avons
    \begin{equation}
        \left( \sum_{k=0}^{\infty}u_k \right)a=\sum_{k=0}^{\infty}(u_ka).
    \end{equation}
\end{proposition}

\begin{proof}
    Par définition de la somme infinie\footnote{Définition \ref{LEMooGXPGooZTJPoN}.},
    \begin{equation}
        \spadesuit=\left( \sum_{k=0}^{\infty}u_k \right)a=\left( \lim_{n\to \infty} \sum_{k=0}^nu_k \right)a.
    \end{equation}
    Le lemme~\ref{LemWWXVSae} appliqué à \( n\mapsto\sum_{k=0}^nu_k\) et à la suite constante \( a\) nous donne
    \begin{equation}    \label{EqOAoopjz}
        \spadesuit=\lim_{n\to \infty} \left( \sum_{k=0}u_ka \right),
    \end{equation}
    ce que nous voulions par distributivité de la somme finie : dans \eqref{EqOAoopjz}, le \( a\) est dans ou hors de la somme, au choix. L'important est qu'il soit dans la limite.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Somme non dénombrables}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons voir que les sommes non dénombrables ne sont pas intéressantes : si le nombre de valeurs non nulles parmi les \( (x_i)_{i\in I}\) est non dénombrable, alors la somme est infinie. La bonne généralisation de somme infinie dans le cas non dénombrable est l'intégrale qui viendra seulement avec la définition \ref{DefTVOooleEst} et la mesure de Lebesgue \ref{DefooYZSQooSOcyYN}.

\begin{lemma}       \label{LEMooYJCVooHajEbg}
    Si \( A\) est non dénombrable dans \( \eR\), alors il existe \( \delta>0\) tel que \( A\cap \{ | x |\geq \delta \}\) est non dénombrable.
\end{lemma}

\begin{proof}
    Nous y allons par l'absurde, et nous supposons que \( A\) ne contient pas zéro (sinon il faut ajouter zéro aux \( A_n\) ci-dessous, et ça alourdit les notations). Nous supposons donc que les parties
    \begin{equation}
        A_n=A\cap\{ | x |\geq \frac{1}{ n } \}
    \end{equation}
    sont dénombrables. Mais
    \begin{equation}
        A\subset \bigcup_{n=1}^{\infty}A_n.
    \end{equation}
    Une union dénombrable d'ensembles dénombrables est dénombrable\footnote{Proposition \ref{PROPooENTPooSPpmhY}.}. Vu qu'un ensemble non dénombrable ne peut être inclus à un ensemble dénombrable\footnote{Proposition \ref{PropQEPoozLqOQ}.}, nous avons une contradiction.
\end{proof}

\begin{lemma}       \label{LEMooQIMGooOUpZjk}
    Soit un ensemble \( I\) et une «suite» \( (x_i)_{i\in I}\) avec \( x_i\geq 0\) pour tout \( i\). Si l'ensemble
    \begin{equation}
        F=\{ i\in I\tq x_i>0 \}
    \end{equation}
    est non dénombrable, alors
    \begin{equation}
        \sum_{i\in I}x_i=\infty.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous considérons l'ensemble des valeurs non nulles atteintes par \( x\) :
    \begin{equation}
        V=\{ x_i\tq i\in F \}.
    \end{equation}
    Il y a deux possibilités : soit \( V\) est dénombrable (ou fini), soit il est non dénombrable.

    \begin{subproof}
        \item[\( V\) est fini ou dénombrable]
            Dans ce cas, l'application \( x\colon F\to \mathopen[ 0 , \infty \mathclose[\) est une application d'un ensemble indénombrable vers un ensemble dénombrable. Le lemme \ref{LEMooGTOTooFbpvzU} nous indique qu'il existe \( y\in \eR\) tel que \( x^{-1}(y)\) est indénombrable et en particulier infini. La somme \( \sum_{i\in x^{-1}(y)}x_i\) est une somme indénombrable de termes tous égaux et strictement positifs. Elle est infinie.

            \item[\( V\) est indénombrable]
                La partie \( V\) de \( \eR\) est non dénombrable; elle est donc sujette au lemme \ref{LEMooYJCVooHajEbg} : il existe \( \delta>0\) tel que \( W=V\cap\{ x\geq \delta \}\) est indénombrable. Vu que \( x_i\geq \delta\) pour tout \( i\) dans \( x^{-1}(W)\) nous avons
                \begin{equation}
                    \sum_{i\in x^{-1}(W)}x_i=\infty.
                \end{equation}
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Sommes dénombrables}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu que les sommes non dénombrables ne sont pas intéressantes. La notion \ref{DefHYgkkA} de sommes est par contre réellement plus utile que la notion de somme sur \( \eN\) parce que \( \eN\) a un ordre. En effet une somme sur \( \eN\) peut être définie par les sommes partielles avec un ordre sans réelle discussions, alors que l'ordre de sommation sur \( \eZ\) est déjà plus discutable. Bref, nous allons voir maintenant quelque propriétés de la somme \ref{DefHYgkkA} dans le cas dénombrable.

\begin{example}     \label{EXooULLXooTDFYqf}
    Au sens de la définition~\ref{DefIkoheE} la famille
    \begin{equation}
        \frac{ (-1)^n }{ n }
    \end{equation}
    n'est pas sommable. En effet la somme des termes pairs est \( \infty\) alors que la somme des termes impairs est \( -\infty\). Quel que soit \( J_0\in \eN\), nous pouvons concocter, en ajoutant des termes pairs, un \( J\) avec \( J_0\subset J\) tel que \( \sum_{j\in J}(-1)^j/j\) soit arbitrairement grand. En ajoutant des termes négatifs, nous pouvons également rendre \( \sum_{j\in J}(-1)^j/j\) arbitrairement petit.
\end{example}

\begin{proposition} \label{PropVQCooYiWTs}
    Si \( (a_{ij})\) est une famille de nombres positifs indexés par \( \eN\times \eN\) alors
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}=\sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big)
    \end{equation}
    où la somme de gauche est celle de la définition~\ref{DefHYgkkA}.
\end{proposition}
%TODO : cette proposition peut être vue comme une application de Fubini pour la mesure de comptage. Le faire et référentier ici.

\begin{proof}
    Nous considérons \( J_{m,n}=\{ 0,\ldots, m \}\times \{ 0,\ldots, n \}\) et nous avons pour tout \( m\) et \( n\) :
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\Big( \sum_{j=1}^na_{ij} \Big).
    \end{equation}
    Si nous fixons \( m\) et que nous prenons la limite \( n\to \infty\) (qui commute avec la somme finie sur \( i\)) nous trouvons
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq =\sum_{i=1}^m\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cela étant valable pour tout \( m\), c'est encore valable à la limite \( m\to \infty\) et donc
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}

    Pour l'inégalité inverse, il faut remarquer que si \( J\) est fini dans \( \eN^2\), il est forcément contenu dans \( J_{m,n}\) pour \( m\) et \( n\) assez grand. Alors
    \begin{equation}
        \sum_{(i,j)\in J}a_{ij}\leq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\sum_{j=1}^na_{ij}\leq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cette inégalité étant valable pour tout ensemble fini \( J\subset \eN^2\), elle reste valable pour le supremum.
\end{proof}

La définition générale de la somme~\ref{DefIkoheE} est compatible avec la définition usuelle dans les cas où cette dernière s'applique.
\begin{proposition}[commutative sommabilité]\label{PropoWHdjw}
    Soit \( I\) un ensemble dénombrable et une bijection \( \tau\colon \eN\to I\). Soit \( (a_i)_{i\in I}\) une famille dans un espace vectoriel normé.  Si \( \sum_{i\in I}a_i\) existe, alors il est donné par
    \begin{equation}
        \sum_{i\in I}a_i=\lim_{N\to \infty} \sum_{k=0}^Na_{\tau(k)}.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous posons \( a=\sum_{i\in I}a_i\). Soit \( \epsilon>0\) et \( J_0\) comme dans la définition. Nous choisissons
    \begin{equation}
        N>\max_{j\in J_0}\{ \tau^{-1}(j) \}.
    \end{equation}
    En tant que sommes sur des ensembles finis, nous avons l'égalité
    \begin{equation}
        \sum_{k=0}^Na_{\tau(k)}=\sum_{j\in J_0}a_j
    \end{equation}
    où \( J\) est un sous-ensemble de \( I\) contenant \( J_0\). Soit \( J\) fini dans \( I\) tel que \( J_0\subset J\). Nous avons alors
    \begin{equation}
        \| \sum_{k=0}^Na_{\tau(k)}-a \|=\| \sum_{j\in J}a_j-a \|<\epsilon.
    \end{equation}
    Nous avons prouvé que pour tout \( \epsilon\), il existe \( N\) tel que \( n>N\) implique \( \| \sum_{k=0}^na_{\tau(k)}-a\| <\epsilon\).
\end{proof}

La réciproque n'est pas vraie. Même en supposant que \( \lim_{N\to \infty} \sum_{n=0}^Na_n\) existe, il n'est pas forcé que \( \sum_{n\in\eN}a_n\) existe. Cela est une conséquence de l'exemple \ref{EXooULLXooTDFYqf}.

\begin{corollary}       \label{CORooBPILooWDXpUM}       % Il ne faut pas référentier ce corolaire qui est sans doute faux.
    Nous pouvons permuter une somme dénombrable et une fonction linéaire continue. C'est-à-dire que si \( f\) est une fonction linéaire continue sur l'espace vectoriel normé \( E\) et \( (a_i)_{i\in I}\) une famille sommable dans \( E\) alors
    \begin{equation}
        f\left( \sum_{i\in I}a_i \right)=\sum_{i\in I}f(a_i).
    \end{equation}
\end{corollary}

\begin{probleme}
    À mon avis, ce corolaire est faux parce qu'il manque l'hypothèse que la famille \( f(a_i)\) est sommable. Voir la proposition \ref{PROPooWLEDooJogXpQ}.
\end{probleme}

\begin{proof}
    En utilisant une bijection \( \tau\) entre \( I\) et \( \eN\) avec la proposition~\ref{PropoWHdjw} ainsi que le résultat connu à propos des sommes sur \( \eN\), nous avons
    \begin{subequations}
        \begin{align}
            f\left( \sum_{i\in I}a_i \right)&=f\left( \sum_{k=0}^{\infty}a_{\tau(k)} \right)\\
            &=\sum_{k=0}^{\infty}f(a_{\tau(k)}) \label{SUBEQooCVUTooPmnHER}\\
            &=\sum_{i\in I}f(a_i).
        \end{align}
    \end{subequations}
    Notons que le passage à \eqref{SUBEQooCVUTooPmnHER} n'est pas du tout une trivialité à deux francs cinquante. Il s'agit d'écrire la somme comme la limite des sommes partielles, et de permuter \( f\) avec la limite en invoquant la continuité, puis de permuter \( f\) avec la somme partielle en invoquant sa linéarité.

    Ah, tiens et tant qu'on y est-à-dire qu'il y a des choses évidentes qui ne le sont pas, oui, il existe des applications linéaires non continues, voir le thème~\ref{THEMEooYCBUooEnFdUg}.
\end{proof}

La proposition suivante nous enseigne que les sommes infinies peuvent être manipulée de façon usuelle.
\begin{proposition} \label{PropMpBStL}
    Soit \( I\) un ensemble dénombrable. Soient \( (a_i)_{i\in I}\) et \( (b_i)_{i\in I}\), deux familles de réels positifs telles que \( a_i<b_i\) et telles que \( (b_i)\) est sommable. Alors \( (a_i)\) est sommable.

    Si \( (a_i)_{i\in I}\) est une famille de complexes telle que \( (| a_i |)\) est sommable, alors \( (a_i)\) est sommable.
\end{proposition}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooWLEDooJogXpQ}
    Soit un espace vectoriel normé \( E\) et une famille sommable\footnote{Définition~\ref{DefIkoheE}.} \( \{ v_i \}_{i\in I}\) d'éléments de \( E\). Soit \( f\colon E\to \eC\) une application sur laquelle nous supposons
    \begin{enumerate}
        \item
            \( f\) est linéaire et continue;
        \item
            la partie \( \{ f(v_i)_{i\in I} \} \) est sommable.
    \end{enumerate}
    Alors nous pouvons permuter la somme et \( f\) :
    \begin{equation}        \label{EQooONHXooKqIEbY}
        f\big( \sum_{i\in I}v_i \big)=\sum_{i\in I}f(v_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\); vu que les familles \( \{ v_i \}_{i\in I}\) et \( \{ f(v_i) \}_{i\in I}\) sont sommables, nous pouvons considérer les parties finies \( J_1\) et \( J_2\) de \( I\) telles que
    \begin{equation}
        \big\| \sum_{j\in J_1}v_j-\sum_{i\in I}v_i \big\|\leq \epsilon
    \end{equation}
    et
    \begin{equation}
        \big\| \sum_{j\in J_2}f(v_j)-\sum_{i\in I}f(v_i) \big\|\leq \epsilon
    \end{equation}
    Ensuite nous posons \( J=J_1\cup J_2\). Avec cela nous calculons un peu avec les majorations usuelles :
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|+  \| f(\sum_{j\in J}v_j)-\sum_i\in If(v_i) \|.
    \end{equation}
    Le second terme est majoré par \( \epsilon\), tandis que le premier, en utilisant la linéarité de \( f\) possède la majoration
    \begin{equation}
        \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|=\| f(\sum_{i\in I}v_i-\sum_{j\in J}v_j) \|\leq \| f \| \| \sum_{i\in I}v_i- \sum_{j\in J}v_j\|\leq \epsilon\| f \|.
    \end{equation}
    Donc pour tout \( \epsilon>0\) nous avons
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \epsilon(1+\| f \|).
    \end{equation}
    D'où l'égalité \eqref{EQooONHXooKqIEbY}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Produit tensoriel d'espaces vectoriels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Si vous êtes pressés, vous pouvez aller lire la définition \ref{DEFooKTVDooSPzAhH} de produit tensoriel d'espaces vectoriels. Mais si vous étiez vraiment pressés, vous ne seriez pas en train de lire des choses sur le produit tensoriel (il vous suffit de croire que \( x\otimes y\) n'est finalement que la concatenation de \( x\) et \( y\)).

\begin{definition}
    Soient un espace vectoriel \( V\) et un sous-espace \( N\). Le \defe{quotient}{quotient d'un espace vectoriel} de \( V\) par \( N\), noté \( V/N\) est l'ensemble des classes d'équivalence pour la relation \( x\sim y\) si et seulement si \( x-y\in N\).
\end{definition}

\begin{proposition}
    Soient un espace vectoriel \( V\) et un sous-espace vectoriel \( N\) de \( V\). Les définitions
    \begin{enumerate}
        \item
            \( [v]+[w]=[v+w]\)
        \item
            \( \lambda[v]=[\lambda v]\)
    \end{enumerate}
    ont un sens et définissent une structure d'espace vectoriel sur \( V/N\).
\end{proposition}

\begin{proof}
    Un élément général de la classe \( [v]\) est de la forme \( v+n\) avec \( n\in N\). Le calcul suivant montre que la somme fonctionne : 
    \begin{equation}
        [v+n_1]+[w+n_2]=[v+w+n_1+n_2]=[v+w]
    \end{equation}
    parce que \( n_1+n_2\in N\). De même,
    \begin{equation}
        \lambda[v+n]=[\lambda v+\lambda n]=[\lambda v]
    \end{equation}
    toujours parce que \( \lambda n\in N\).

    Notons que nous avons utilisé de façon on ne peut plus cruciale le fait que \( N\) soit un sous-espace vectoriel.
\end{proof}

\begin{proposition}
    Si \( \{ e_i \}\) est une base de \( V\) et si \( N\) est un sous-espace de \( V\), alors \( \{ [e_i] \}\) est une partie génératrice de \( V/N\).
\end{proposition}

\begin{proof}
    Si \( x=\sum_kx_ke_k\), alors \( [x]=\sum_kx_k[e_k]\), donc oui.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Somme directe d'espaces vectoriels}
%---------------------------------------------------------------------------------------------------------------------------

Si \( V\) et \( W\) sont des espaces vectoriels, ce que nous notons \( V\oplus W\) n'est rien d'autre que l'espace vectoriel de l'ensemble \( V\times W\).

\begin{propositionDef}[\cite{ooXISFooTypogf}]
    Si \( V\) et \( W\) sont des espaces vectoriels sur le même corps \( \eK\), alors les définitions
    \begin{enumerate}
        \item
            \( (v_1,w_1)+(v_2,w_2)=(v_1+v_2,w_1+w_2)\)
        \item
            \( \lambda(v,w)=(\lambda v,\lambda w)\)
    \end{enumerate}
    donnent une structure d'espace vectoriel sur \( V\times W\). 

    Cet espace sera noté \( V\oplus W\) et est appelé \defe{somme directe}{somme directe} de \( V\) et \( W\).
\end{propositionDef}

\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooCASNooEqisqa}
    Soient un espace vectoriel de dimension finie \( V\) et deux sous-espaces \( M_1\) et \( M_2\) satisfaisant
    \begin{enumerate}
        \item
            \( M_1\cap M_2=\{ 0 \}\),
        \item
            \( \dim(M_1)+\dim(M_2)\geq \dim(V)\).
    \end{enumerate}
    Alors \( V=M_1\oplus M_2\).
\end{proposition}

\begin{proof}
    Soient une base \( \{ e_i \}_{i\in I}\) de \( M_1\) et \( \{ f_{\alpha} \}\) de \( M_2\). Nous commençons par prouver que la partie \( B=\{ e_i \}\cup \{ f_{\alpha} \}\) est libre.

    Supposons en effet avoir des coefficients \( a_i\) et \( b_{\alpha}\) tels que
    \begin{equation}
        \sum_ia_ie_i+\sum_{\alpha}b_{\alpha}f_{\alpha}.
    \end{equation}
    Cela implique que \( \sum_ia_ie_i=-\sum_{\alpha}b_{\alpha}f_{\alpha}\). Or \( \sum_ia_ie_i\in M_1\) et \( -\sum_{\alpha}b_{\alpha}f_{\alpha}\in M_2\). Donc les éléments \( \sum_ia_ie_i\) et \( \sum_{\alpha}b_{\alpha}f_{\alpha}\) sont dans \( M_1\cap M_2=\{ 0 \}\). Nous avons alors les égalités
    \begin{equation}
        \sum_ia_ie_i=0
    \end{equation}
    et
    \begin{equation}
        \sum_{\alpha}b_{\alpha}f_{\alpha}=0.
    \end{equation}
    La première implique \( a_i=0\) pour tout \( i\) et la seconde implique \( b_{\alpha}=0\) pour tout \( \alpha\).

    Donc \( B\) est une partie libre de \( V\) contenant \( \dim(M_1)+\dim(M_2)\geq \dim(V)\) éléments. La proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooUUFCooIVtGgz} nous indique alors qu'en réalité \( \dim(M_1)+\dim(M_2)=\dim(V)\). Vu que \( B\) est une partie libre contenant \( \dim(V)\) éléments, c'est une base par la proposition \ref{PROPooVEVCooHkrldw}\ref{ITEMooSGGCooOUsuBs}.
\end{proof}

La proposition suivante est une version plus «pragmatique» de la proposition \ref{PropXrTDIi}.
\begin{proposition}[\cite{BIBooGTTEooGCUNkM}]       \label{PROPooNITTooCYcrrT}
    Soient un espace euclidien\footnote{Qui possède un produit scalaire, définition \ref{DefLZMcvfj}.} de dimension finie \( V\) ainsi qu'un sous-espace \( M\). Nous posons
    \begin{equation}
        M^{\perp}=\{ x\in V\tq x\cdot y=0\forall y\in M \}.
    \end{equation}
    Alors \( M\oplus M^{\perp}=V\).
\end{proposition}

\begin{proof}
    D'abord si \( x\in M\cap M^{\perp}\), alors \( x\cdot x=0\) et donc \( x=0\). Donc nous avons déjà \( M\cap M^{\perp}=\{ 0 \}\). Nous considérons une base \( \{b_1,\ldots, b_k\}\) de \( M\), et nous définissons l'application linéaire
    \begin{equation}
        \begin{aligned}
            f\colon V&\to \eR^k \\
            x&\mapsto (x\cdot b_1,\ldots, x\cdot b_k). 
        \end{aligned}
    \end{equation}
    Nous avons que \( M^{\perp}=\ker(f)\). Le théorème du rang \ref{ThoGkkffA} nous indique que
    \begin{equation}
        \dim(V)=\dim\big( \ker(f) \big)+\dim\big( \Image(f) \big)\leq \dim(M^{\perp})+k=\dim(M^{\perp})+\dim(M).
    \end{equation}
    Une justification : vu que \( f\) prend ses valeurs dans \( \eR^k\), la dimension de son image est majorée par \( k\).

    Nous en déduisons que 
    \begin{equation}
        \dim(M)+\dim(M^{\perp})\geq\dim(V),
    \end{equation}
    et la proposition \ref{PROPooCASNooEqisqa} nous permet de conclure que \( M\oplus M^{\perp}=V\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les produits tensoriels}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons procéder en deux temps. D'abord nous allons définir ce qu'est \emph{un} produit tensoriel entre deux espaces vectoriels \( V\) et \( W\), et nous allons montrer que tous les produits tensoriels possibles sont isomorphes. Ensuite nous allons montrer qu'un produit tensoriel existe en en construisant un. Voir la proposition \ref{PROPooIWZDooRRZNCf}.

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooXKKQooAvWRNp}
    Soient deux espaces vectoriels \( V\) et \( W\). Un \defe{produit tensoriel}{produit tensoriel} de \( V\) et \( W\) est un couple \( (T,h)\) où \( T\) est un espace vectoriel et \( h\colon V\oplus W\to T\) est une application
    \begin{enumerate}
        \item
            bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.}
        \item
            surjective
        \item       \label{ITEMooJCNYooGvjjtL}
            telle que pour tout espace vectoriel \( U\) et toute applications bilinéaire \( f\colon V\oplus W\to U\), il existe une application linéaire \( g\colon T\to U\) telle que \( f=g\circ h\).
    \end{enumerate}
    La propriété \ref{ITEMooJCNYooGvjjtL} est appelée \defe{propriété universelle}{propriété universelle} du produit tensoriel.
\end{definition}

\begin{definition}  \label{DEFooPLHTooRiHjlE}
    Un \defe{morphisme}{morphisme de produits tensoriels} entre \( (T,h)\) et \( (T',h')\) est une application linéaire \( \psi\colon T\to T'\) telle que \( h'=\psi\circ h\).

    Nous parlons d'\defe{isomorphisme}{isomorphisme} si \( \psi\) a un inverse qui est également un morphisme.
\end{definition}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]      \label{PROPooROPHooQXqNzZ}
    Si \( V\) et \( W\) sont des espaces vectoriels, tous les produits tensoriels entre \( V\) et \( W\) sont isomorphes entre eux au sens de la définition \ref{DEFooPLHTooRiHjlE}.

    Plus précisément, si \( (T,h)\) et \( (T',h')\) sont deux produits tensoriels de \( V\) et \( W\), alors 
    \begin{enumerate}
        \item
            il existe une unique unique application linéaire \( g\colon T\to T'\) telle que \( h'=g\circ h\),
        \item
            cette application \( g\) est inversible.
    \end{enumerate}
    En particulier, l'application \( g\) est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Soient deux produits tensoriels \( (T,h)\) et \( (T',h')\). 

    \begin{subproof}
        \item[Existence]
    
    L'application \( h'\colon V\oplus W\to T'\) est bilinéaire, et \( (T,h)\) est un produit tensoriel. Donc il existe \( g\colon T\to T'\) tel que \( h'=g\circ h\). De même, il existe une application \( g'\colon T'\to T\) telle que \( h=g'\circ h\).

\item[Unicité]

    En ce qui concerne l'unicité, vu que \( h\colon V\oplus W\to T\) est surjective, la relation \( h'=g\circ h\) prescrit les valeurs de \( g\) sur tous les éléments de \( T\).

\item[Inversible]
    
    Ces deux applications \( g\) et \( g'\) vérifient $h'=gg'h$ et $h=g'gh$, et de plus \( h\colon V\oplus W\to T\) est surjective. Soient \( t\in T\) et \( x\in V\oplus W\) tel que \( t=h(x)\). Nous avons \( h(x)=g'gh(x)\). C'est-à-dire \( t=(g'\circ g)(t)\). De même dans l'autre sens, il existe \( x'\in V\oplus W\) tel que \( t=h'(x')\). En appliquant l'égalité \( h'=gg'h'\) à \( x'\), nous trouvons \( t=(g\circ g')(t)\).

    Tout cela pour dire que \( g'=g^{-1}\). Cette application \( g\) est donc un isomorphisme de produits tensoriels entre \( (T,h)\) et \( (T',h')\).
    \end{subproof}
    Au final, l'application \( g\colon T\to T'\) étant linéaire et inversible, elle est un isomorphisme d'espaces vectoriels.
\end{proof}

Tout cela est fort bien : nous avons unicité à isomorphisme près du produit tensoriel d'espaces vectoriels. Mais nous n'avons pas encore de certitudes à propos de l'existence d'un couple \( (T,h)\) vérifiant les propriétés demandées pour être un produit tensoriel.

Nous allons maintenant construire un produit tensoriel.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Le produit tensoriel}
%---------------------------------------------------------------------------------------------------------------------------

C'est le moment pour vous de relire la définition \ref{DEFooCPNIooNxsYMY} d'espace vectoriel librement engendré, et surtout le lemme \ref{LEMooLOPAooUNQVku} qui en donne une base.

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooKTVDooSPzAhH}
    Soient deux espaces vectoriels \( V\) et \( W\) sur le corps commutatif\footnote{À part mention du contraire, tous les corps du Frido sont commutatifs.} \( \eK\). Dans \( F_{\eK}(V\times W)\) nous considérons les sous-espaces suivants:
    \begin{subequations}
        \begin{align}
            A_1&=\{ \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)}\tq v_1,v_2\in V,w\in W  \}\\
            A_2&=\{ \delta_{(v,w_1)}+\delta_{(v,w_2)}-\delta_{(v,w_1+w_2)}\tq v\in V,w_1,w_2\in W  \} \label{SUBEQooSHBJooJLPVbK} \\
            A_3&=\{ \lambda\delta_{v,w}-\delta_{(\lambda v, w)}\tq v\in V,w\in W,\lambda\in \eK \}\\
            A_4&=\{ \lambda\delta_{v,w}-\delta_{(v,\lambda w)}\tq v\in V,w\in W,\lambda\in \eK \}.
        \end{align}
    \end{subequations}
    Nous considérons alors \( N=\Span(A_1,A_2,A_3,A_4)\) et le quotient
    \begin{equation}
        V\otimes_{\eK}W=F_{\eK}(V\times W)/N.
    \end{equation}
    Ce dernier espace vectoriel est le \defe{produit tensoriel}{produit tensoriel} de \( V\) par \( W\).
\end{definition}

\begin{remark}      \label{REMooSLEGooWEiutz}
    Quelque remarques.
    \begin{enumerate}
        \item
            Les éléments de \( V\otimes W\) ne s'écrivent pas tous sous la forme \( v\otimes w\). Certains ont vraiment besoin d'être écrits avec des sommes. En cela, la situation de \( V\otimes W\) est réellement différente de celle de \( V\times W\). Dans ce dernier, tous les éléments sont des couples.
        \item
            La classe de l'élément \( \delta_{(v,w)}\in F(V\times W)\) sera d'habitude noté \( v\otimes w\).
        \item
            Pour insister sur la notion de classe, nous allons aussi noter \( [x]\) la classe de \( x\in F(V\times W)\).
        \item       \label{ITEMooPVWHooMkgQoT}
            L'arithmétique dans \( V\otimes W\) est relativement simple. En ajoutant et soustrayant le même élément de \( A_3\) nous avons par exemple
            \begin{equation}
                (\lambda v)\otimes w=(\lambda v)\otimes w+\lambda (v\otimes w)-(\lambda v)\otimes w.
            \end{equation}
            Nous obtenons de cette façon
            \begin{equation}
                \lambda(v\otimes w)=(\lambda v)\otimes w=v\otimes (\lambda w),
            \end{equation}
            que nous noterons \( \lambda v\otimes w\) sans plus de précision.
    \end{enumerate}
\end{remark}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]     \label{PROPooIWZDooRRZNCf}
    L'espace vectoriel \( V\times W\) muni de
    \begin{equation}
        \begin{aligned}
            h\colon V\oplus W&\to V\otimes W \\
            (v,w) &\mapsto v\otimes w 
        \end{aligned}
    \end{equation}
    est un produit tensoriel entre \( V\) et \( W\).
\end{proposition}

\begin{proof}
    Nous devons prouver les conditions de la définition \ref{DEFooXKKQooAvWRNp}. 
    
    \begin{subproof}
        \item[\( h\) est bilinéaire]

            Ce sont des calculs tels que faits dans la remarque \ref{REMooSLEGooWEiutz}\ref{ITEMooPVWHooMkgQoT} qui font le travail.

        \item[\(h \) est surjective]
    
            Un élément de \( V\otimes W\) est la classe d'un élément de \( F(V\times W)\), c'est-à-dire de la forme
            \begin{equation}
                \big[ \sum_{i\alpha}\delta_{(v_i,w_{\alpha})} \big]=\sum_{i\alpha}a_{i\alpha}v_i\otimes w_{\alpha}.
            \end{equation}
            Cet élément est dans l'image de \( h\) comme le montre le calcul suivant\footnote{Faites bien la distinction entre \( \delta_{v,w}\), \( (v,w)\) et \( v\otimes w\). Sachez dans quel ensemble se trouvent chacun de ces trois objets.} :
            \begin{equation}
                h\big( \sum_{i\alpha}(v_i,w_{\alpha}) \big)=\sum_{i\alpha}a_{i\alpha}h(v_i,w_{\alpha})=\sum_{i\alpha}v_i\otimes w_{\alpha}.
            \end{equation}

        \item[Propriété universelle]

            Soient un espace vectoriel \( U\) et une application linéaire \( f\colon V\oplus W\to U \). Nous devons trouver une application linéaire \( g\colon V\otimes W\to U\) telle que \( f=g\circ h\). Pour cela nous commençons par considérer l'application
            \begin{equation}
                \begin{aligned}
                    g\colon F(V\times W)&\to U \\
                    \delta_{(v,w)}&\mapsto f(v,w) 
                \end{aligned}
            \end{equation}
            définie sur tout \( F(V\times W)\) par linéarité sans encombres parce que les \( \delta_{v,w}\) forment une base par le lemme \ref{LEMooLOPAooUNQVku}.

            Nous démontrons que \( g(N)=0\) pour avoir le droit de passer \( g\) aux classes et le considérer comme application partant de \( V\otimes W\) au lieu de \( F(V\times W)\). Prenons par exemple
            \begin{subequations}
                \begin{align}
                    g\big( \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)} \big)&=g( \delta_{(v_1,w)} )+g(\delta_{(v_2,w)})-g(\delta_{v_1+v_2,w})\\
                    &=f(v_1,w)+f(v_2,w)-f(v_1+v_2,w)\\
                    &=0
                \end{align}
            \end{subequations}
            par la bilinéarité de \( f\). Cela montre que \( g(A_1)=0\). Nous montrons de même que \( g(A_2)=g(A_3)=g(A_4)=0\), et enfin toujours par linéarité que \( g(N)=0\). Pour rappel, les éléments de \( N\) sont les combinaisons linéaires finies d'éléments de \( A_1\), \( A_2\), \( A_3\) et \( A_4\).

            Par passage aux classes, nous avons une application (que nous notons également \( g\))
            \begin{equation}
                g\colon F(V\times W)/N\to U
            \end{equation}
            vérifiant \( g(v\otimes w)=f(v,w)\). Mais comme \( h(v,w)=v\otimes w\), nous avons $g\circ h\colon V\oplus W\to U$ vérifiant \( g\circ h=f\).
    \end{subproof}
    L'espace vectoriel \( V\otimes W\) est donc un produit tensoriel.
\end{proof}

\begin{normaltext}
    Vu que \( V\otimes W\) est un produit tensoriel de \( V\) et \( W\), et vu qu'il y a unicité par la proposition \ref{PROPooROPHooQXqNzZ}, nous avons bien le droit de dire que \( V\otimes W\) est \emph{le} produit tensoriel. Cela justifie le titre.
\end{normaltext}

\begin{normaltext}
    Les prochains lemmes et propositions vont nous dire que l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V^*\otimes W&\to \aL(V,W) \\
            \alpha\otimes w&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels lorsque \( V\) est de dimension finie. Vu que nous aimons les énoncés très explicites, ça va être découpé en plusieurs morceaux, l'énoncé va devenir un peu long; mais c'est pour la bonne cause.
\end{normaltext}

\begin{lemma}       \label{LEMooOJEBooQruWEp}
    Soient deux espaces vectoriels \( V\) et \( W\) dont \( W\) est de dimension finie. Alors l'application définie par
    \begin{equation}
        \begin{aligned}
            \varphi\colon F(V^*\times W)&\to \aL(V,W) \\
            \delta_{(\alpha,w)}&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    sur la base «canonique» de \( F(V^*\times W)\) passe aux classes.
\end{lemma}

\begin{proof}
    Avec les notations de la définition \ref{DEFooKTVDooSPzAhH} nous devons prouver que \( \varphi(N)=0\). Nous montrons que \( \varphi(A_4)=0\), et nous vous laissons faire les autres. Pour \( \lambda\in \eK\), \( \alpha\in V^*\) et \( w\in W\) en utilisant la linéarité de \( \varphi\) nous avons :
    \begin{subequations}
        \begin{align}
            \varphi\big( \lambda\delta_{(\alpha,w)}-\delta_{(\alpha,\lambda w)} \big)v&=\lambda\varphi(\delta_{(\alpha,w)})(v)-\varphi(\delta_{(\alpha,\lambda w)})(v)\\
            &=\lambda\alpha(v)w-\alpha(v)(\lambda w)\\
            &=0
        \end{align}
    \end{subequations}
    parce que \( \alpha(v)(\lambda w)=\lambda \alpha(v)w\) du fait que \( \eK\) est commutatif. La commutativité de \( \eK\) est ce qui permet de permuter le produit \( \lambda \alpha(v)\).

    Nous laissons à la lectrice le soin de prouver que \( \varphi(A_1)=\varphi(A_2)=\varphi(A_3)=0\).
\end{proof}

\begin{lemma}       \label{LEMooUQZHooWjIGsy}
    Si \( W\) est de dimension finie, alors \( \aL(V,W)\) muni de 
    \begin{equation}
        \begin{aligned}
            h'\colon V^*\oplus W&\to \aL(V,W) \\
            (\alpha,w)&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est un produit tensoriel\footnote{Définition \ref{DEFooXKKQooAvWRNp}.} de \( V^*\) par \( W\).
\end{lemma}

\begin{proof}
    Nous devons prouver que
    \begin{itemize}
        \item \( h\) est bilinéaire,
        \item \( h\) est surjective
        \item pour tout espace vectoriel \( U\), et pour toute application bilinéaire \( f\colon V^*\oplus W\to U\), il existe une application linéaire \( g\colon \aL(V,W)\to U\) tel que \( f=g\circ h\).
    \end{itemize}

    \begin{subproof}
        \item[Bilinéaire]
            Le fait que \( h\) soit bilinéaire est une simple vérification.
        \item[Surjective]
            L'espace \( W\) étant de dimension finie, nous pouvons en considérer une base \( \{ z_i \}_{i\in I}\). Soit \( \alpha\in \aL(V,W)\). Si \( v\in V\), l'élément \( \alpha(v)\) peut être décomposé dans la base \( \{ z_i \}\), ce qui définit des applications linéaires \( \alpha_i\colon V\to \eK\) par
            \begin{equation}
                \alpha(v)=\sum_{i\in I}\alpha_i(v)z_i.
            \end{equation}
            Notons que \( \alpha_i\in V^*\). En comparant avec la définition de \( h'\), nous voyons que
            \begin{equation}
                \alpha(v)=\sum_i h(\alpha_i,z_i)(v),
            \end{equation}
            c'est-à-dire \( \alpha=\sum_ih(\alpha_i,w_i)=h\big( \sum_i(\alpha_i,z_i) \big)\). Nous avons donc bien \( \alpha\in h(V^*\oplus W)\).
        \item[Propriété universelle]

            Soient un espace vectoriel \( U\) et une application bilinéaire \( f\colon V^*\oplus W\to U\). Pour \( \alpha\in\aL(V,W)\) nous définissons \( g(\alpha)\) comme suit. D'abord nous écrivons \( \alpha\) sous la forme
            \begin{equation}
                \alpha(v)=\sum_i\alpha_i(v)z_i,
            \end{equation}
            et nous posons 
            \begin{equation}
                g(\alpha)=\sum_if(\alpha_i,z_i).
            \end{equation}
            Avec cette définition, en posant \( w=\sum_iw_iz_i\), nous avons
            \begin{subequations}
                \begin{align}
                    (g\circ h')(\alpha,w)&=g\big( v\mapsto \alpha(v)w \big)\\
                    &=g\big( v\mapsto \sum_i\alpha(v)w_iz_i \big)\\
                    &=\sum_if(w_i\alpha,z_i)\\
                    &=\sum_if(\alpha,w_iz_i)\\
                    &=f(\alpha,\sum_iw_iz_i)\\
                    &=f(\alpha,w).
                \end{align}
            \end{subequations}
            Cela prouve que \( g\circ h=f\).
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooKJTCooVTXWAQ}
    Soient deux espaces vectoriels \( V\) et \( W\) dont \( V\) est de dimension finie. Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V^*\otimes W&\to \aL(V,W) \\
            \alpha\otimes w&\mapsto \big( v\mapsto \alpha(v)w \big) 
        \end{aligned}
    \end{equation}
    est bien définie\footnote{Au sens où il existe une fonction $\varphi$ définie sur tout $V^*\otimes W$ qui se réduit à cela pour les éléments de la forme $\alpha\otimes w$.} et est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
    Le lemme \ref{LEMooUQZHooWjIGsy} donne une structure de produit tensoriel de \( V^*\) par \( W\) sur \( \aL(V,W)\). Rappelons les structures :
    \begin{equation}
        \begin{aligned}
            h\colon V^*\oplus W&\to V^*\otimes W \\
            (\alpha,w)&\mapsto \alpha\otimes w 
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            h'\colon V^*\oplus W&\to \aL(V,W) \\
            (\alpha,w)&\mapsto \big[ v\mapsto \alpha(v)w \big].
        \end{aligned}
    \end{equation}

    La proposition \ref{PROPooROPHooQXqNzZ} a déjà fait tout le boulot. La seule chose à faire est de vérifier qu'il existe une application \( \varphi\colon V^*\otimes W\to \aL(V,W)\) vérifiant simultanément les deux conditions suivantes :
    \begin{enumerate}
        \item       \label{ITEMooVNNSooNIXRoG}
            \( \varphi(\alpha\otimes w)=\big[ v\mapsto \alpha(v)w \big]\)
        \item 
            \( h'=\varphi\varphi\circ h\).
    \end{enumerate}
    La seconde condition assure que \( \varphi\) sera un isomorphisme d'espaces vectoriels.

    L'existence de \( \varphi\) vérifiant la condition \ref{ITEMooVNNSooNIXRoG} est un effet du lemme \ref{LEMooOJEBooQruWEp} qui donne une fonction sur \( F(V^*\times W)\) dont le \( \varphi\) qui nous concerne est un quotient. Il reste à voir que cette application vérifie \( h'=\varphi\circ h\).
    
    En nous rappellant que \( \alpha\otimes w=[\delta_{(\alpha,w)}]\) et en écrivant \( \varphi\) à la fois l'application et son passage au quotient,
    \begin{equation}
        (\varphi\circ h)(\alpha,w)=\varphi(\alpha\otimes w)=\varphi\big( [\delta_{(\alpha,w)}] \big)=\varphi(\delta_{(\alpha,w)}).
    \end{equation}
    En appliquant à \( v\in V\) nous avons:
    \begin{equation}
        (\varphi\circ h)(\alpha,w)v=\varphi(\delta_{(\alpha,w)})v=\alpha(v)w=h'(\alpha,w)v.
    \end{equation}
    Et voila. Nous avons \( \varphi\circ h=h'\).
\end{proof}

Une conséquence de la proposition \ref{PROPooKJTCooVTXWAQ} est que
\begin{equation}
    \dim(V\otimes W)=\dim(V)\dim(W)
\end{equation}
via le lemme \ref{LEMooJXFIooKDzRWR}\ref{ITEMooPMLWooNbTyJI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Bases}
%---------------------------------------------------------------------------------------------------------------------------

Voici un lemme entièrement dédié au principe «dans le Frido, on ne fait pas d'abus de notations, sauf pour la logique formelle et la théorie des ensembles, que nous admettons».
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooXFIMooDkTSrq}
    Si \( \tau\colon V_1\to V_2\) est un isomorphisme d'espaces vectoriels, alors 
    \begin{equation}        \label{EQooEYUGooYYRZxD}
        \begin{aligned}
            \varphi\colon V_1\otimes W&\to V_2\otimes W \\
            v\otimes w&\mapsto \tau(v)\otimes W 
        \end{aligned}
    \end{equation}
    est un isomorphisme d'espaces vectoriels.
\end{lemma}

\begin{proof}
    Comme d'habitude, l'expression \eqref{EQooEYUGooYYRZxD} ne définit pas réellement \( \varphi\) parce que nous ne savons pas du tout si \( \{v\otimes w\tq v\in V,w\in W\}\) est plus ou moins une base de \( V\otimes W\)\footnote{Ne lisez pas la proposition \ref{PROPooTHDPooWgjUwk} qui dévoile toute l'intrigue.}. Ce que dit réellement ce lemme est qu'il existe une application \( V_1\otimes W\to V_2\otimes W\) qui est isomorphisme et qui se réduit à l'expression donnée dans le cas d'éléments de \( V_1\otimes W\) de la forme \( v\otimes w\).

    L'application
    \begin{equation}
        \begin{aligned}
            \varphi_0\colon F(V_1\times W)&\to F(V_2\times W) \\
            \delta{(v,w)}&\mapsto \delta_{\big( \tau(v),w \big)}
        \end{aligned}
    \end{equation}
    est un isomorphisme.

    Cette application passe aux classes, mais pas au sens où \( x\in [y]\) impliquerait \( \varphi_0(x)=\varphi_0(y)\); au sens où si \( x\in [y]\), alors \( \varphi_0(x)\in[\varphi_0(y)]\). Par exemple
    \begin{equation}
        \varphi_0\big( \lambda\delta_{(v,w)}-\delta_{(v,\lambda w)} \big)=\lambda\delta_{\big( \tau(v),w \big)}-\delta_{\big( \tau(v),w \big)}\in [0].
    \end{equation}
    Nous vous laissons le soin de vérifier les égalités correspondantes pour les autres parties de \( N\).

    Le passage au classes de \( \varphi_0\) signifie que l'on considère l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon V_1\otimes W&\to V_2\otimes W \\
            [x]&\mapsto [\varphi_0(x)] 
        \end{aligned}
    \end{equation}
    où vous aurez noté que la prise de classe à gauche n'est pas la même que celle à droite.

    Il faut prouver que ce \( \varphi\) est un isomorphisme. En ce qui concerne la linéarité,
    \begin{subequations}
        \begin{align}
            \varphi\big( [x]+[y] \big)&=\varphi\big( [x+y] \big)\\
            &=[\varphi_0(x+y)]\\
            &=[\varphi_0(x)+\varphi_0(y)]\\
            &=[\varphi_0(x)]+[\varphi_0(y)]\\
            &=\varphi([x])+\varphi([y]).
        \end{align}
    \end{subequations}
    Je vous laisse le reste de la linéarité. Et en ce qui concerne le fait que ce soit une bijection, allez-y.
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooTHDPooWgjUwk}
    Soient des espaces vectoriels de dimension finie \( V\) et \( W\). Soient une base \( \{e_i\}\) de \( V\) et une base \( \{f_{\alpha}\}\) de \( W\).
    
    Alors :
    \begin{enumerate}
        \item       \label{ITEMooQCILooUncdGl}
            La partie \( \{e_i\otimes f_{\alpha}\}\) est une base de \( V\otimes W\).
        \item
    Au niveau des dimensions, \( \dim(V\otimes W)=\dim(V)\dim(W)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Vu que \( V\) est de dimension finie, nous avons un isomorphisme d'espaces vectoriels \( V^*=V\), et même un isomorphisme d'espaces vectoriels
    \begin{equation}
        \begin{aligned}
            \tau\colon V&\to (V^*)^* \\
            \tau(v)\alpha&=\alpha(v).
        \end{aligned}
    \end{equation}
    Recopions l'isomorphisme de la proposition \ref{PROPooKJTCooVTXWAQ} en utilisant \( V^*\) au lieu de \( V\) :
    \begin{equation}
        \begin{aligned}
            \psi_0\colon (V^*)^*\otimes W&\to \aL(V^*,W) \\
           \tau(v)\otimes w &\mapsto \big( \alpha\mapsto \tau(v)(\alpha)w =\alpha(v)w \big).
        \end{aligned}
    \end{equation}
    En écrivant cela, nous avons tenu compte du fait que tout élément de \( (V^*)^*\) peut être écrit de façon univoque sous la forme \( \tau(v)\) pour un certain \( v\in V\).

    Vu que \( \tau\) est un isomorphisme, l'application suivante est encore un isomorphisme\footnote{Lemme \ref{LEMooXFIMooDkTSrq}.} :
    \begin{equation}        \label{EQooAEFRooPfmAnj}
        \begin{aligned}
            \psi\colon V\otimes W&\to \aL(V^*,W) \\
            v\otimes w&\mapsto \big( \alpha\mapsto \alpha(v)w \big). 
        \end{aligned}
    \end{equation}
    Nous avançons. Vu que nous avons un isomorphisme, nous pouvons faire passer des bases. Le lemme \ref{LEMooJXFIooKDzRWR} nous donne une base de \( \aL(V^*,W)\) en les éléments \( \beta_{i\alpha}\colon V^*\to W\) définies par
    \begin{equation}
        \beta_{ij}(\alpha)=\alpha(e_i)f_{\alpha}.
    \end{equation}
    Donc \( \{ \psi^{-1}(\beta_{i\alpha}) \}\) est une base de \( V\otimes W\).

    Pour \( a=\sum_ia_ie_i^*\) (base duale, définition \ref{DEFooTMSEooZFtsqa}) nous avons :
    \begin{equation}
        \psi(e_i\otimes f_{\alpha})a=a(e_i)f_{\alpha}=\beta_{i\alpha}(a).
    \end{equation}
    Cela prouve que \( \psi^{-1}(\beta_{i\alpha})=e_i\otimes f_{\alpha}\), et donc que ces \( e_i\otimes f_{\alpha}\) est une base de \( V\otimes W\).

    La formule concernant les dimensions est simplement la définition \ref{DEFooWRLKooArTpgh} de la dimension : le nombre d'éléments dans une base.
\end{proof}

\begin{example}
    Dans le produit tensoriel \( \eR\otimes \eR\), nous avons \( x\otimes 1=1\otimes x=x(1\otimes x)\) pour tout \( x\in \eR\). Et si \( x\geq 0\) nous avons aussi \( x\otimes 1=\sqrt{ x }\otimes \sqrt{ x }\).
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons des espaces vectoriels \( V\) et \( W\) de dimension finie. L'application \eqref{EQooAEFRooPfmAnj} donne un isomorphisme d'espaces vectoriels 
\begin{equation} 
    \begin{aligned}
        \psi\colon V\otimes W&\to \aL(V^*,W) \\
        v\otimes w&\mapsto \big( \alpha\mapsto \alpha(v)w \big). 
    \end{aligned}
\end{equation}
Et ça, c'est très bien, parce que nous connaissons une norme sur \( \aL(V^*,W)\) :  la norme opérateur \ref{DefNFYUooBZCPTr}.

\begin{definition}[\cite{MonCerveau}]      \label{DEFooEXXNooMgIpSV}
    Soient deux espaces vectoriels normés de dimension finie \( V\) et \( W\). Sur \( V\otimes W\) nous définissons, pour \( t\in V\otimes W\)
    \begin{equation}
        \| t \|=\| \psi(t) \|_{\aL(V^*,W)}.
    \end{equation}
\end{definition}
   
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooQPXHooJWfpmk}
    La norme sur \( V\otimes W\) vérifie
    \begin{equation}
        \| v\otimes w \|=\| v \|\| w \|
    \end{equation}
    pour tout \( v\in V\) et \( w\in W\).
\end{lemma}

\begin{proof}
    C'est un simple(?) calcul :
    \begin{equation}
        \| v\otimes w \|=\| \psi(v\otimes w) \|=\| \alpha\mapsto \alpha(v)w \|=\sup_{\| \alpha \|=1}\| \alpha(v)w \|=\sup_{\| \alpha \|=1}| \alpha(v) |\| w \|.
    \end{equation}
    Étant donné que \( V\) est de dimension finie, \( \sup_{\| \alpha \|=1}| \alpha(v) |=\| v \|\)\quext{Cela est une des raisons pour lesquelles nous sommes en dimension finie : je ne sais pas si cette égalité est vraie en dimension inifinie.}. Nous avons donc
    \begin{equation}
        \| v\otimes w \|=\| v \|\| w \|.
    \end{equation}
\end{proof}

Le lemme suivant montre que \( \eR\otimes \eR\) n'est pas du tout \( \eR\times \eR=\eR^2\). Au contraire, \( \eR\otimes \eR\) est isomorphe à \( \eR\).
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooVONEooQpPgcn}
    L'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon \eR\otimes \eR&\to \eR \\
            1\otimes 1&\mapsto 1 
        \end{aligned}
    \end{equation}
    prolongée par linéarité est un isomorphisme isométrique.
\end{lemma}

\begin{proof}
    D'abord une base de \( \eR\) est \( \{ 1 \}\); donc une base de \( \eR\otimes \eR\) est \( \{ 1\otimes 1 \}\) par la proposition \ref{PROPooTHDPooWgjUwk}. Donc l'application proposée se prolonge par linéarité à tout \( \eR\otimes \eR\).

    Le fait que \( \varphi\) soit une bijection provient du fait que \( \varphi\) transforme une base en une base; si vous n'y croyez pas, la vérification de l'injectivité et de la surjectivité est facile.

    Pour que \( \varphi\) soit isométrique, nous faisons le calcul
    \begin{equation}
        \| \varphi(x\otimes y) \|=\| xy(1\otimes 1) \|=| xy |\| 1\otimes 1 \|=| xy |=\| x\otimes y \|.
    \end{equation}
    Nous avons utilisé la propriété \ref{DefNorme}\ref{ItemDefNormeii} d'une norme ainsi que le lemme \ref{LEMooQPXHooJWfpmk} pour la norme sur \( \eR\otimes \eR\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Applications bilinéaires, matrices et produit tensoriel}
%---------------------------------------------------------------------------------------------------------------------------
\label{SECooUKRYooZjagcX}

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}        \label{EQooUNRYooKBrXyK}
    (\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
    (a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
    (a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est une vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Application d'opérateurs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemMyKPzY}
    Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
    \begin{equation}        \label{EqXdxvSu}
        (Ax\otimes By)=A(x\otimes y)B^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
    \begin{subequations}
        \begin{align}
            (Ax\otimes By)_{ij}&=(Ax)_i(By)_j\\
            &=\sum_{kl}A_{ik}x_kB_{jl}y_l\\
            &=A_{ik}(x\otimes y)_{kl}B_{jl}\\
            &=\big( A(x\otimes y)B^t \big)_{ij}.
        \end{align}
    \end{subequations}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Calcul différentiel dans un espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecLStKEmc}

Quelques motivations pour la notion de différentielle sont données dans \ref{SEBSECooLPRQooJRQCFL}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition de la différentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{MonCerveau}]      \label{DefDifferentiellePta}
    Soient deux espaces vectoriels normés \( E\) et \( F\) ainsi qu'une fonction \( f\colon \mU\to F\) où \( \mU\) est un ouvert de \( E\).

  Si il existe une une application linéaire \( T\in\aL(E,F)\) satisfaisant
  \begin{equation}	\label{EqCritereDefDiff}
      \lim_{\substack{h\to 0\\h\in E}}\frac{f(a+h)-f(a)-T(h)}{\|h\|_E}=0,
  \end{equation}
  alors il en existe une seule.

  Dans ce cas nous disons que $f$ est \defe{différentiable au point $a$}{application!différentiable} et l'application $T$ ainsi définie est appelée \defe{différentielle}{différentielle} de $f$ au point $a$, et nous la notons $df_a$.
\end{propositionDef}

\begin{proof}
    Soient deux applications linéaires \( T_1\), \( T_2\) satisfaisant la condition \eqref{EqCritereDefDiff}. Nous avons
    \begin{equation}
        \frac{ \| T_1(h)-T_2(h) \|_F }{ \| h \|_E }\leq \frac{ \| T_1(h)-f(a+h)+f(a) \| }{ \| h \| }+\frac{ \| f(a+h)-f(a)-T_2(h) \| }{ \| h \| }\to 0.
    \end{equation}
    Nous avons donc
    \begin{equation}
        \lim_{h\to 0} \frac{ \| (T_1-T_2)(h) \|_F }{ \| h \|_E }=0.
    \end{equation}
    Soit \( \epsilon>0\). Ce que signifie la limite est qu'il existe un \( r>0\) tel que pour tout \( u\in B_E(0,r)\), nous ayons
    \begin{equation}
        \frac{ \| (T_1-T_2)(u) \|_F }{ \| u \|_E }<\epsilon.
    \end{equation}
    Soit \( v\in E\). Nous considérons \( \lambda\in\eR\) tel que \( \lambda v\in B(0,r)\), par exemple \( \lambda<r/\| v \|\). Nous avons
    \begin{equation}
        \epsilon>\frac{ \| (T_1-T_2)(\lambda v) \|_F }{ \| \lambda v \|_E }=\frac{ \| (T_1-T_2)(v) \| }{ \| v \| }.
    \end{equation}
    Cela donne
    \begin{equation}
        \| (T_1-T_2)(v) \|<\| v \|\epsilon.
    \end{equation}
    Nous avons donc \( \| (T_1-T_2)(v) \|=0\), soit \( T_1(v)=T_2(v)\).
\end{proof}


L'application différentielle
\begin{equation}
    \begin{aligned}
        df\colon E&\to \aL(E,F) \\
        a&\mapsto df_a
    \end{aligned}
\end{equation}
est également très importante.

\begin{definition}      \label{DefJYBZooPTsfZx}
Une application \( f\colon E\to F\) est de \defe{classe \( C^1\)}{classe $C^1$} lorsque l'application différentielle \( df\colon E\to \aL(E,F)\) est continue. Voir aussi les définitions~\ref{DefPNjMGqy} pour les applications de classe \( C^k\).
\end{definition}

\begin{remark}      \label{RemATQVooDnZBbs}
    L'application norme étant continue, le critère du théorème~\ref{ThoWeirstrassRn} est en réalité assez général. Par exemple à partir d'une application différentiable\footnote{Définition~\ref{DefDifferentiellePta}.} \( f\colon X\to Y\)  nous pouvons considérer la fonction réelle
    \begin{equation}
        a\mapsto \|  df_a   \|
    \end{equation}
    où la norme est la norme opérateur\footnote{Définition~\ref{DefNFYUooBZCPTr}.}. Si \( f\) est de classe \( C^1\) alors cette application est continue et donc bornée sur un compact \( K\) de \( X\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Accroissements finis}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooYQZZooVybqjK}
    Soit une fonction \( f\colon E\to V\) (espaces vectoriels normés) différentiable en \( a\in E\). Alors il existe une fonction \( \alpha\colon E\to V\) telle que
    \begin{subequations}
        \begin{numcases}{}
            \lim_{h\to 0} \frac{ \alpha(h) }{ \| h \| }=0\\
            f(a+h)=f(a)+df_a(h)+\alpha(h).
        \end{numcases}
    \end{subequations}
\end{lemma}

\begin{proof}
    Il s'agit seulement de poser
    \begin{equation}
        \alpha(h)=f(a+h)-f(a)-df_a(h).
    \end{equation}
    Le fait que \( \alpha(h)/\| h \|\to 0\) est alors la définition de la différentiabilité de \( f\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{(non ?) Différentiabilité des applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) et \( F\) sont deux espaces vectoriels nous notons \( \aL(E,F)\)\nomenclature[Y]{\( \aL(E,F)\)}{Les applications linéaires de \( E\) vers \( F\)} l'ensemble des applications linéaires de \( E\) vers \( F\) et \( \cL(E,F)\)\nomenclature[Y]{\( \cL\)}{Les applications linéaires continues de \( E\) vers \( F\)} l'ensemble des applications linéaires continues de \( E\) vers \( F\). Ces espaces seront bien entendu, sauf mention du contraire, toujours munis de la norme opérateur de la définition~\ref{DefNFYUooBZCPTr}.

\begin{lemma}       \label{LemooXXUGooUqCjmp}
    Soit une application linéaire \( f\).
    \begin{enumerate}
        \item
            Si \( f\) est continue, alors elle est différentiable et \( df_a(u)=f(u)\) pour tout \( a\) et \( u\).
        \item
            Si \( f\) n'est pas continue, alors elle n'est pas différentiable.
    \end{enumerate}
\end{lemma}

\begin{proof}
    La linéarité de \( f\) donne :
    \begin{equation}
        f(a+h)-f(a)-f(h)=0,
    \end{equation}
    et donc prendre \( T=f\) dans la définition~\ref{DefDifferentiellePta} fait fonctionner la limite. De plus \( T\) est alors continue par hypothèse; elle est donc bien la différentielle de \( f\).

    Supposons que \( f\) ne soit pas continue, prenons une application linéaire continue \( T\), et calculons
    \begin{equation}        \label{EQooFLYMooEKTeOC}
        \frac{ f(a+h)-f(a)-T(h) }{ \| h \| }=\frac{ (f-T)(h) }{ \| h \| }=(f-T)(e_h)
    \end{equation}
    où \( e_h\) est le vecteur unitaire dans la direction de \( h\). Vu que \( f\) n'est pas continue et que \( T\) l'est, l'application \( f-T\) n'est pas continue. Elle n'est pas pas bornée par la proposition~\ref{PROPooQZYVooYJVlBd}. Il existe alors un vecteur \( h\) tel que \( \| (f-T)(e_h) \|>1\) (et même plus grand que ce qu'on veut).

    Donc la limite de \eqref{EQooFLYMooEKTeOC} pour \( h\to 0\) ne peut pas être nulle.
\end{proof}

\begin{lemma}   \label{LemLLvgPQW}
    Une application linéaire continue est de classe \(  C^{\infty}\).
\end{lemma}

\begin{proof}
    Soit \( a\in E\). Étant donné que \( f\) est linéaire et continue, elle est différentiable et
    \begin{equation}
        \begin{aligned}
            df\colon E&\to \cL(E,F) \\
            a&\mapsto f
        \end{aligned}
    \end{equation}
    est une fonction constante et en particulier continue; nous avons donc \( f\in C^1\). Pour la différentielle seconde nous avons \( d(df)_a=0\) parce que \( df(a+h)-df(a)=f-f=0\). Toutes les différentielles suivantes sont nulles.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dérivation en chaine et formule de Leibnitz}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropOYtgIua}
    Soient \( f_i\colon U\to F_i\), des fonctions de classe \( C^r\) où \( U\) est ouvert dans l'espace vectoriel normé \( E\) et les \( F_i\) sont des espaces vectoriels normés. Alors l'application
    \begin{equation}
        \begin{aligned}
        f=f_1\times \cdots\times f_n\colon U&\to F_1\times \cdots\times F_n \\
    x&\mapsto \big( f_1(x),\ldots, f_n(x) \big)
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}
    d^rf=d^rf_1\times\ldots d^rf_n.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( x\in U\) et \( h\in E\). La différentiabilité des fonctions \( f_i\) donne
    \begin{equation}
        f_i(x+h)=f_i(x)+(df_i)_x(h)+\alpha_i(h)
    \end{equation}
    avec \( \lim_{h\to 0} \alpha_i(h)/\| h \|=0\). Par conséquent
    \begin{subequations}
        \begin{align}
            f(x+h)&=\big( \ldots, f_i(x)+(df_i)_x(h)+\alpha_i(h),\ldots \big)\\
            &= \big( \ldots,f_i(x),\ldots \big)+ \big( \ldots,(df_i)_x(h),\ldots \big)+ \big( \ldots,\alpha_i(h),\ldots \big).
        \end{align}
    \end{subequations}
    Mais la définition~\ref{DefFAJgTCE} de la norme dans un espace produit donne
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \big( \alpha_1(h),\ldots, \alpha_n(h) \big) \| }{ \| h \| }=0,
    \end{equation}
    ce qui nous permet de noter \( \alpha(h)=\big( \alpha_1(h),\ldots, \alpha_n(h) \big)\) et avoir \( \lim_{h\to 0} \alpha(h)/\| h \|=0\). Avec tout ça nous avons bien
    \begin{equation}
        f(x+h)=f(x)+\big( (df_1)_x(h)+\cdots +(df_n)_x(h) \big)+\alpha(h),
    \end{equation}
    ce qui signifie que \( f\) est différentiable et
    \begin{equation}
        df_x=\big( df_1,\ldots, df_n \big).
    \end{equation}
\end{proof}

\begin{theorem}     \label{THOooIHPIooIUyPaf}
    Soient des espaces vectoriels normés \( E,V\) et \( W\). Nous considérons deux fonctions \( f\colon E\to V\) et \( g\colon V\to W\). Nous supposons que \( f\) est différentiable en \( a\in E\) et que \( g\) est différentiable en \( f(a)\in V\). 

    Nous supposons de plus que \( df_a\) est de norme finie\quext{Je ne suis pas totalement certain que cette hypothèse soit nécessaire, mais en tout cas, elle est utilisée.}.
    
    
    Alors \( g\circ f\colon E\to W\) est différentiable en \( a\) et
    \begin{equation}
        f(g\circ f)_a(u)=df_{f(a)}\big( df_a(u) \big),
    \end{equation}
    ou encore
    \begin{equation}
        f(g\circ f)_a=dg_{f(a)}\circ df_a.
    \end{equation}
\end{theorem}

\begin{proof}
    En utilisant le lemme \ref{LEMooYQZZooVybqjK} pour les fonctions \( f\) et \( g\), nous avons
    \begin{equation}        \label{EQooXNWZooJSPjRS}
        f(a+h)=f(a)+df_a(h)+\alpha(h)
    \end{equation}
    et
    \begin{equation}        \label{EQooIQZZooWPyMbE}
        g\big( f(a)+k \big)=g\big( f(a) \big)+dg_{f(a)}(k)+\beta(k).
    \end{equation}
    L'application \( dg_{f(a)}\circ df_a\) est une application linéaire, et est notre candidat différentielle. En suivant la définition \ref{DefDifferentiellePta}, nous allons calculer
    \begin{equation}
        \lim_{h\to 0} \frac{ (g\circ f)(a+h)-(g\circ f)(a)-(dg_{f(a)}\circ df_a)(h) }{ \| h \| }.
    \end{equation}
    Si cette limite existe et vaut zéro, alors nous aurons prouvé que le candidat différentielle est correct.

    Pour cela, nous emboîtons les formules \eqref{EQooXNWZooJSPjRS} et \eqref{EQooIQZZooWPyMbE} l'une dans l'autre pour avoir : 
    \begin{equation}
        g(a+h)=g\big( f(a)+df_a(h)+\alpha(h) \big)=g\big( f(a) \big)+dg_{f(a)}\big( df_a(h)+\alpha(h) \big)+\beta\big( df_a(h)+\alpha(h) \big).
    \end{equation}
    Vu que \( dg_{f(a)}\) est linéaire, le deuxième terme peut être coupé en deux et après recombinaisons,
    \begin{equation}
        (g\circ f)(a+h)-(g\circ f)(a)-(df_{f(a)}\circ df_a)(h)=dg_{f(a)}\big( \alpha(h) \big)+\beta\big( df_a(h)+\alpha(h) \big).
    \end{equation}
    Étant donné que \( dg_{f(a)}\) est linéaire,
    \begin{equation}
        \frac{ dg_{f(a)}\big(\alpha(h)\big) }{ \| h \| }=dg_{f(a)}\left( \frac{ \alpha(h) }{ \| h \| } \right)\to 0.
    \end{equation}
    Il nous reste à voir que
    \begin{equation}        \label{EQooUQNUooFgNyJp}
        \lim_{h\to 0} \frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| h \| }
    \end{equation}
    existe au vaut zéro. Vu que \( df_a\) est linéaire, il existe \( M>0\) tel que\footnote{Ce \( M\) est par exemple la norme opérateur de \( df_a\), comme nous l'assure le lemme \ref{LEMooIBLEooLJczmu}. C'est pour ce passage-ci que nous avons supposé que \( df_a\) était de norme finie.} \( \| df_a(h) \|\leq M\| h \|\). D'autre part, vu que \( \alpha(h)/\| h \|\to 0\), nous avons \( \| \alpha(h) \|\leq \| h \|\) pour tout \( h\) suffisamment petit.

    Donc si \( h\) est assez petit, nous avons
    \begin{equation}        \label{EQooEQJBooSmacrD}
        \| df_a(h)+\alpha(h) \|\leq (M+1)\| h \|.
    \end{equation}
    Soit \( \epsilon>0\). Soit \( \delta>0\) tel que \( \| h \|\leq \delta\) implique \( \beta(h)/\| h \|\leq \epsilon\) et \eqref{EQooEQJBooSmacrD} en même temps. Soit \( r\) tel que \( (M+1)r<\delta\); et notons que \( r<\delta\). Nous considérons alors \( h\in B(0,r)\) et nous calculons :
    \begin{equation}
        \frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| h \| }=\frac{ \beta\big( df_a(h)+\alpha(h) \big) }{ \| df_a(h)+\alpha(h) \| }\frac{ \| df_a(h)+\alpha(h) \| }{ \| h \| }\leq (M+1)\epsilon.
    \end{equation}
    La limite \eqref{EQooUQNUooFgNyJp} existe donc et vaut zéro.
\end{proof}

\begin{theorem}[Différentielle de fonctions composées\cite{SNPdukn}]    \label{ThoAGXGuEt}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et \( V\) ouvert dans \( F\). Soient des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f\colon U\to V\\
            g\colon V\to G.
        \end{align}
    \end{subequations}
    Alors l'application \( g\circ f\colon V\to G\) est de classe \( C^r\) et
    \begin{equation}\label{EqHFmezmr}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
\end{theorem}

\begin{proof}
    Nous nous fixons \( x\in U\). La fonction \( f\) est différentiable en \( x\in U\) et \( g\) en \( f(x)\), donc nous pouvons écrire
    \begin{equation}
        f(x+h)=f(x)+df_x(h)+\alpha(h)
    \end{equation}
    et
    \begin{equation}
        g\big( f(x)+u \big)=g\big( f(x) \big)+dg_{f(x)}(u)+\beta(u)
    \end{equation}
    où la fonction \( \alpha\) a la propriété que
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \alpha(h) \| }{ \| h \| }=0;
    \end{equation}
    et la même chose pour \( \beta\). La fonction composée en \( x+h\) s'écrit donc
    \begin{equation}    \label{EqCXcfhfH}
        (g\circ f)(x+h)=g\big( f(x)+df_x(h)+\alpha(h) \big)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h)+\alpha(h) \big)+\beta\big( df_x(h)+\alpha(h) \big).
    \end{equation}
    Nous montrons que tous les «petits» termes de cette formule peuvent être groupés. D'abord si \( h\) est proche de \( 0\), nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq\frac{ \| df_x \|\| h \| }{ \| h \| }+\frac{ \| \alpha(h) \| }{ \| h \| }.
    \end{equation}
    Si \( h\) est petit, le second terme est arbitrairement petit, donc en prenant n'importe que \( M>\| df_x \|\) nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq M.
    \end{equation}
    Par ailleurs, nous avons
    \begin{equation}
        \frac{ \| \beta\big( df_x(h)+\alpha(h) \big) \| }{ \| h \| }=\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{ \| df_x(h)+\alpha(h) \| }\frac{  \| df_x(h)+\alpha(h) \|  }{ \| h \| }\leq M\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{   \| df_x(h)+\alpha(h) \| }.
    \end{equation}
    Vu que la fraction est du type \( \frac{ \beta( f(h)) }{ f(h) }\) avec \( \lim_{h\to 0} f(h)=0\), la fraction tend vers zéro lorsque \( h\to 0\). En posant
    \begin{equation}
        \gamma_1(h)=\beta\big( df_x(h)+\alpha(h) \big)
    \end{equation}
    nous avons \( \lim_{h\to 0} \gamma_1(h)/\| h \|=0\).

    L'autre candidat à être un petit terme dans \eqref{EqCXcfhfH} est traité en utilisant le lemme~\ref{LEMooFITMooBBBWGI} :
    \begin{equation}
        \| dg_{f(x)}\big( \alpha(h) \big) \|\leq \| dg_{f(x)} \|\| \alpha(h) \|.
    \end{equation}
    Donc
    \begin{equation}
        \frac{ \| dg_{f(x)}\big( \alpha(h) \big) \| }{ \| h \| }\leq \| dg_{f(x)} \|\frac{ \| \alpha(h) \| }{ \| h \| },
    \end{equation}
    ce qui nous permet de poser
    \begin{equation}
        \gamma_2(h)=dg_{f(x)}\big( \alpha(h) \big)
    \end{equation}
    avec \( \gamma_2\) qui a la même propriété que \( \gamma_1\). Avec tout cela, en posant \( \gamma=\gamma_1+\gamma_2\) nous récrivons
    \begin{equation}
        (g\circ f)(x+h)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h) \big)+\gamma(h)
    \end{equation}
    avec \( \lim_{h\to 0} \frac{ \gamma(h) }{ \| h \| }=0\). Tout cela pour dire que
    \begin{equation}
        \lim_{h\to 0} \frac{ (g\circ f)(x+h)-(g\circ f)(x)-\big( dg_{f(x)}\circ df_x \big)(h) }{ \| h \| }=0,
    \end{equation}
    ce qui signifie que
    \begin{equation}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
    Nous avons donc montré que si \( f\) et \( g\) sont différentiables, alors \( g\circ f\) est différentiable avec différentielle donnée par \eqref{EqHFmezmr}.

    Nous passons à la régularité. Nous supposons maintenant que \( f\) et \( g\) sont de classe \( C^r\) et nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon L(F,G)\times L(E,F)&\to L(E,G) \\
            (A,B)&\mapsto A\circ B.
        \end{aligned}
    \end{equation}
    Montrons que l'application \( \varphi\) est continue en montrant qu'elle est bornée\footnote{Proposition~\ref{PROPooQZYVooYJVlBd}.}. Pour cela nous écrivons la norme opérateur
    \begin{equation}
        \| \varphi \|=\sup_{\| (A,B) \|=1}\| \varphi(A,B) \|=\sup_{\| (A,B) \|=1}\| A\circ B \|\leq\sup_{\| (A,B) \|=1}\| A \|\| B \|\leq 1.
    \end{equation}
    Justifications : d'une part la norme opérateur est une norme algébrique\footnote{Lemme \ref{LEMooFITMooBBBWGI}.}, et d'autre part la définition \ref{DefFAJgTCE} de la norme sur un espace produit pour la dernière majoration. L'application \( \varphi\) est donc continue et donc \(  C^{\infty}\) par le lemme~\ref{LemLLvgPQW}. Nous considérons également l'application
    \begin{equation}
        \begin{aligned}
        \psi\colon U&\to L(F,G)\times L(E,F) \\
        x&\mapsto \big( dg_{f(x)},df_x \big).
        \end{aligned}
    \end{equation}
    Vu que \( f\) et \( g\) sont \( C^1\), l'application \( \psi\) est continue. Ces deux applications \( \varphi\) et \( \psi\) sont choisies pour avoir
    \begin{equation}
        (\varphi\circ\psi)(x)=\varphi\big( dg_{f(x)},df_x \big)=dg_{f(x)}\circ df_x,
    \end{equation}
    c'est-à-dire \( \varphi\circ\psi=d(g\circ f)\). Les applications \( \varphi\) et \( \psi\) étant continues, l'application \( d(g\circ f)\) est continue, ce qui prouve que \( g\circ f\) est \( C^1\).

    Si \( f\) et \( g\) sont \( C^r\) alors \( dg\in C^{r-1}\) et \( dg\circ f\in C^{r-1}\) où il ne faut pas se tromper : \( dg\colon F\to L(F,G)\) et \( f\colon U\to F\); la composée est \( dg\circ f\colon x\mapsto dg_{f(x)}\in L(F,G)\).

    Pour la récurrence nous supposons que \( f,g\in C^{r-1}\) implique \( g\circ f\in C^{r-1}\) pour un certain \( r\geq 2\) (parce que nous venons de prouver cela avec \( r=1\) et \( r=2\)). Soient \( f,g\in C^r\) et montrons que \( g\circ f\in C^r\). Par la proposition~\ref{PropOYtgIua} nous avons
    \begin{equation}
        \psi=dg\circ f\times df\in C^{r-1},
    \end{equation}
    et donc \( d(g\circ f)=\varphi\circ\psi\in C^{r-1}\), ce qui signifie que \( g\circ f\in C^r\).
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRCZOooSgvpSE}
    Soit une application \( f\colon E\to V\) de classe \( C^1\). Soit une application linéaire \( \varphi\colon V \to W\). Alors \( \varphi\circ f\) est de classe \( C^p\).
\end{proposition}

\begin{proof}
    Toute la preuve est un grand jeu de cohérence des espaces en présence, alors soyez attentifs et capable de dire précisément à quel espace appartient chacun de objets entrant en jeu.

    Nous posons \( V_0=V\) et \( V_{k+1}=\aL(E,V_k)\). Idem pour les espaces \( W_k\). Ensuite nous posons
    \begin{equation}
        \begin{aligned}
            \varphi_1\colon \aL(E,V)&\to \aL(E,W) \\
            \alpha&\mapsto \varphi\circ \alpha.
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            \varphi_k\colon \aL(E,V_{k-1})&\to \aL(E,W_{k-1}) \\
            \alpha&\mapsto \varphi_{k-1}\circ \alpha.
        \end{aligned}
    \end{equation}
    Notez la cohérence : si \( a\in E\), \( \alpha(a)\in V_{k-1}=\aL(E,V_{k-2})\), et donc
    \begin{equation}
        (\varphi_{k-1}\circ\alpha)(a)=\varphi_{k-1}\big( \alpha(a) \big).
    \end{equation}
    À droite nous avons \( \varphi_{k-1}\big( \alpha(a) \big)\in \aL(E,W_{k-2})=V_{k-1}\).

    De plus, \( \varphi\) est linéaire; ça se prouve par récurrence en partant de \( \varphi_1\) et en se basant sur le fait que \( \varphi\) est linéaire.

    C'est parti pour une récurrence.

    \begin{subproof}
        \item[Énoncé]
            Nous allons prouver par récurrence que
            \begin{equation}
                d^k(\varphi\circ f)=\varphi_k\circ d^kf.
            \end{equation}
            pour tout \( k\leq p\).
        \item[Initialisation]
 
            D'abord, \( f\) est de classe \( C^p\), donc différentiable et \( \varphi\) est linéaire donc différentiable. Donc la composée est différentiable et le théorème \ref{THOooIHPIooIUyPaf} nous donne la différentiabilité de \( \varphi\circ f\) ainsi que la formule
            \begin{equation}
                d(\varphi\circ f)_a(u)=d\varphi_{f(a)}\big( df_a(u) \big)=(\varphi\circ df_a)(u)=\varphi_1(df_a)(u).
            \end{equation}
            Donc \( d(\varphi\circ f)_a=\varphi_1(df_a)\), ce qui signifie 
            \begin{equation}
                d(\varphi\circ f)=\varphi_1\circ df.
            \end{equation}
            C'est bon pour \( k=1\).


        \item[La pas de récurrence]

            Vu que \( f\) est de classe \( C^p\), \( d^kf\) est encore différentiable. Vu que \( \varphi_k\) est encore linéaire, nous pouvons encore utiliser la règle de différentiation de fonctions composées sur l'application \( \varphi_k\circ d^kf\). Nous avons :
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=d\big( d^k(\varphi\circ f) \big)_a(u)=d(\varphi_k\circ d^kf)_a(u).
            \end{equation}
            C'est le moment d'utiliser la formule de différentiation en chaine :
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=\big( (d\varphi_k)_{d^kf_a}\circ d^{k+1}f_a \big)(u).
            \end{equation}
            Mais \( \varphi_k\) étant linéaire, \( (d\varphi_k)_{d^kf_a}=\varphi_k\), donc
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a(u)=(\varphi_k\circ d^{k+1}f_a)(u).
            \end{equation}
            Donc, en oubliant l'application au vecteur \( u\),
            \begin{equation}
                d^{k+1}(\varphi\circ f)_a=\varphi_k\circ d^{k+1}f_a=\varphi_{k+1}\big( d^{k+1}f_a \big)=(\varphi_{k+1}\circ d^{k+1}f)(a).
            \end{equation}
            Nous avons donc bien
            \begin{equation}
                d^{k+1}(\varphi\circ f)=\varphi_{k+1}\circ d^{k+1}f.
            \end{equation}
    \end{subproof}
\end{proof}

\begin{lemma}       \label{LemooTJSZooWkuSzv}
    Si \( f\colon U\to V\) est un difféomorphisme\footnote{Définition~\ref{DefAQIQooYqZdya}} alors pour tout \( a\in U\), l'application \( df_a\) est inversible et
    \begin{equation}
        (df_a)^{-1}=(df^{-1})_{f(a)}.
    \end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'apercevoir qu'en vertu de la règle de différentiation en chaine \eqref{EqHFmezmr},
    \begin{equation}
        (df_a)(df^{-1})_{f(a)}=d(f\circ f^{-1})_{f(a)}=\id.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooNONAooCyAtce}
    Soient des ouverts \( A\) de \( \eR^p\) et \( B\) de \( \eR^m\). Si il existe un difféomorphisme \( f\colon A\to B\), alors \( p=m\).
\end{proposition}

\begin{proof}
    Vu que \( f\) est un difféomorphisme, le lemme \ref{LemooTJSZooWkuSzv} fait son travail : l'application linéaire \( df_a\colon \eR^p\to \eR^m\) est inversible d'inverse \( df^{-1}_{f(a)}\colon \eR^m\to \eR^m\).

    Or une application linéaire ne peut pas être bijective entre espaces de dimensions différentes (finies). Donc \( p=m\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentiation de produit}
%---------------------------------------------------------------------------------------------------------------------------

Si nous avons deux application \( f\colon E\to V\) et \( g\colon E\to W\), alors nous voudrions considérer la fonction
\begin{equation}
    \begin{aligned}
        f\otimes g\colon E&\to V\otimes W \\
        a&\mapsto f(a)\otimes g(a). 
    \end{aligned}
\end{equation}
Le problème avec cette notation est que très souvent, les applications \( f\) et \( g\) sont des éléments d'espaces vectoriels. Si par exemple \( f\in \aL(E,V)\) et \( g\in \aL(E,W)\), nous avons \( f\otimes g\in \aL(E,V)\otimes \aL(E,W)\). Dans le Frido nous ne nous permettons pas de dire calmement que \( \aL(E,V)\otimes \aL(E,W)=\aL(E,V\otimes W)\). Et je ne vous dit même pas à quel point il n'est pas évident, si \( f\in C^{\infty}(E,V)\) et \( g\in  C^{\infty}(E,W)\) que nous aurions \( f\otimes g\in C^{\infty}(E,V)\otimes  C^{\infty}(E,W)= C^{\infty}(E,V\otimes W)\).

Tout cela pour dire que nous n'allons pas nous lancer dans des abus de notations. Non. Au lieu de cela, nous introduisons une notation. Pour rappel, dans tout le Frido, \( \Fun(A,B)\) désigne l'ensemble de toutes les application de \( A\) vers \( B\) sans suppositions de régularité. Pour les puristes, nous précisions que si \( f\in\Fun(A,B)\), nous supposons que \( f\) est définie sur tout \( A\). hum \ldots sauf mention du contraire.
\begin{definition}      \label{DEFooMVNDooFWFtRn}
    Si \( f\in \Fun(E,V)\) et \( g\in \Fun(E,W)\), alors nous définissons
    \begin{equation}
        \begin{aligned}
            f\tilde\otimes g\colon E&\to V\otimes W \\
            a&\mapsto f(a)\otimes g(a). 
        \end{aligned}
    \end{equation}
\end{definition}

\begin{proposition}     \label{PROPooCRVXooEGxdZl}
    Soient des applications continues \( f\colon E\to V\) et \( g\colon E\to W\) entre espaces vectoriels de dimension finies. Alors la fonction \( f\tilde\otimes g\colon E\to V\otimes W\) est continue.
\end{proposition}

\begin{proof}
    Soient \( a\in E\) ainsi qu'une suite \( x_k\to a\) dans \( E\). Nous voulons prouver que \( f\tilde\otimes g(x_k)\stackrel{V\otimes W}{\longrightarrow}f(a)\otimes g(a)\). Nous avons :
    \begin{equation}        \label{EQooSNXUooXrYOeY}
        \| f(x_k)\otimes g(x_k)-f(a)\otimes g(a) \|\leq \| f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a) \|+\| f(x_k)\otimes g(a)-f(a)\otimes g(a) \|.
    \end{equation}
    Ensuite en utilisant la classe d'équivalence \eqref{SUBEQooSHBJooJLPVbK}, 
    \begin{equation}
        f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a)=f(x_k)\otimes \big( g(x_k)-g(a) \big),
    \end{equation}
    et en ce qui concerne les normes,
    \begin{equation}
    \|   f(x_k)\otimes g(x_k)-f(x_k)\otimes g(a)\|  =\|f(x_k)\|  \|\otimes \big( g(x_k)-g(a) \big)\|.
    \end{equation}
    Mais par hypothèse, \( f(x_k)\to f(a)\) et \( g(x_k)\to g(a)\). Donc le tout tend vers zéro lorsque \( k\to \infty\).

    Le même raisonnement fonctionne avec le second terme de \eqref{EQooSNXUooXrYOeY}.
\end{proof}

Lorsque nous parlons de différentielle de produit de fonctions, nous voulons étudier la différentiabilité de \( f\tilde\otimes g\) sous l'hypothèse de différentiabilité de \( f\) et \( g\). Et aussi, si \( f\) et \( g\) sont de classe \( C^p\), est-ce que \( f\tilde\otimes g\) est également de classe \( C^p\) ?

Nous voudrions avoir une formule du type
\begin{equation}
    d(f\tilde\otimes g)=df\tilde\otimes g+f\tilde\otimes dg,
\end{equation}
mais ça ne colle pas au niveau des espaces. En effet, en évaluant cela en \( a\in E\), nous avons à gauche \( d(f\tilde\otimes g)_a\in\aL(E,V\otimes W)\), tandis qu'à droite nous avons \( df_a\otimes g(a)\in \aL(E,V)\otimes W\) et \( f(a)\otimes dg_a\in V\otimes \aL(E,W)\).

Nous pourrions bien entendu dire que \( V\otimes \aL(E,W)\) est isomorphe à \( \aL(E,V\otimes W)\) et hop voila, on n'en parle plus. Ce serait passer sur deux points importants. D'abord est-ce que \( V\otimes \aL(E,W)\) est vraiment isomorphe à \( \aL(E,V\otimes W)\) ? Et ensuite, l'isomorphisme implique une utilisation du théorème \ref{THOooIHPIooIUyPaf} qui est tout sauf une trivialité.

Bref, fidèle au principe fridesque de ne pas cacher des difficultés techniques sous des abus de notations, nous allons écrire les choses explicitement.

\begin{lemma}
    Si \( E\), \( V\) et \( W\) sont de dimension finie, les applications
    \begin{equation}        \label{EQooVWXRooCesUqH}
        \begin{aligned}
            \psi\colon \aL(E,V)\otimes W&\to \aL(E,V\otimes W) \\
            f\otimes w&\mapsto \Big( u\mapsto f(u)\otimes w \Big) 
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            \varphi\colon V\otimes \aL(E,W)&\to \aL(E,V\otimes W) \\
            v\otimes g&\mapsto \big( a\mapsto v\otimes g(a) \big). 
        \end{aligned}
    \end{equation}
    sont des isomorphismes d'espaces vectoriels.
\end{lemma}
Dans le meilleur des mondes, ces applications devraient être affublés d'indices \( V\) et \( W\).

\begin{proof}
    Nous donnons des détails à propos de \( \psi\). Pour \( \varphi\) c'est la même chose.
    \begin{subproof}
        \item[Linéaire]
            La formule \eqref{EQooVWXRooCesUqH} définit \( \psi\) en particulier sur une base de \( \aL(E,V)\otimes W\) par la proposition \ref{PROPooTHDPooWgjUwk}\ref{ITEMooQCILooUncdGl}. Ce que signifie réellement la formule \eqref{EQooVWXRooCesUqH} est que \( \psi\) est ainsi définie sur la base et est prolongée par continuité.
        \item[Injective]
            Si pour un \( f\) et un \( w\) fixé nous avons \( \psi(f\otimes w)=0\), alors il y a deux cas : soit \( w=0\) soit \( w\neq0\). Dans le premier cas, \( f\otimes w=0\), et dans le second cas, nous remarquons que 
            \begin{equation}
                0=\psi(f\otimes w)(a)=f(a)\otimes w
            \end{equation}
            pour tout \( a\in E\). Cela implique \( f(a)=0\) pour tout \( a\) et donc \( f=0\), ce qui signifie que \( f\otimes w=0\).
        \item[Bijective]
            En utilisant la proposition \ref{PROPooTHDPooWgjUwk} et le lemme \ref{LEMooJXFIooKDzRWR}\ref{ITEMooPMLWooNbTyJI}, nous avons égalité des dimensions entre \( \aL(E,V)\otimes W\) et \( \aL(E,V\otimes W)\).

            Une application linéaire injective entre deux espaces vectoriels de même dimension (finie) est une bijection.
    \end{subproof}
\end{proof}

\begin{proposition}     \label{PROPooZOAFooRMeBgI}
    Soient des espaces vectoriels normés de dimension finie. Soient \( f\colon E\to V\) et \( g\colon E\to W\) des fonctions de classe \( C^1\). Alors \( f\tilde\otimes g\colon E\to V\otimes W\) est de classe \( C^1\) nous avons les formules
    \begin{equation}        \label{EQooSUSCooBhZXFC}
        d(f\tilde\otimes g)_a(u)=df_a(u)\otimes g(a)+f(a)\otimes dg_a(u)
    \end{equation}
    ainsi que
    \begin{equation}        \label{EQooOCEEooUrsIDd}
        d(f\tilde\otimes g)=\psi\circ(df\tilde\otimes g)+\varphi\circ(f\tilde\otimes dg).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous commençons par prouver que \( f\tilde\otimes g\) est différentiable en injectant le candidat \eqref{EQooSUSCooBhZXFC} dans la définition. Au numérateur nous avons :
    \begin{equation}        \label{EQooOMXSooYsAiKh}
        (f\tilde\otimes g)(a+h)-(f\tilde\otimes g)(a)-df_a(h)\otimes g(a)-f(a)\otimes dg_a(h).
    \end{equation}
    Le lemme \ref{LEMooYQZZooVybqjK} assure qu'il existe une fonction \( \alpha\colon E\to V\) telle que \( \lim_{h\to 0} \alpha(h)/\| h \|\) et \( f(a+h)+f(a)+df_a(h)+\alpha(h)\). Même chose pour \( g\). Nous avons donc
    \begin{equation}
        (f\tilde\otimes g)(a+h)=f(a+h)\otimes g(a+h)=\big( f(a)+df_a(h)+\alpha(h) \big)\otimes \big( g(a)+dg_a(h)+\beta(h) \big)
    \end{equation}
    qui se développe en \( 9\) termes. En effectuant les différences dans \eqref{EQooOMXSooYsAiKh}, nous nous retrouvons avec un numérateur qui vaut
    \begin{equation}
        f(a)\otimes \beta(h)+df_a(h)\otimes dg_a(h)+df_a(h)\otimes \beta(h)+\alpha(h)\otimes g(a)+\alpha(h)\otimes dg_a(h)+\alpha(h)\otimes \beta(h).
    \end{equation}
    Nous pouvons prouver terme à terme qu'en divisant par \( \| h \|\) nous avons une limite qui vaut zéro. Par exemple,
    \begin{equation}
        \lim_{h\to 0} \frac{ f(a)\otimes \beta(h) }{ \| h \| }
    \end{equation}
    se calcule en prenant la norme du numérateur et en utilisant le lemme \ref{LEMooQPXHooJWfpmk} :
    \begin{equation}
        \frac{ \| f(a)\otimes \beta(h) \| }{ \| h \| }=\frac{ \| f(a) \|\| \beta(h) \| }{ \| h \| }\to 0.
    \end{equation}
    Tous les termes contenant \( \alpha(h)\) ou \( \beta(h)\) se traitent de la même manière. Le dernier terme à traiter est
    \begin{equation}
        \lim_{h\to 0} \frac{ df_a(h)\otimes dg_a(h) }{ \| h \| }.
    \end{equation}
    En prenant la norme du numérateur, en utilisant encore le lemme \ref{LEMooQPXHooJWfpmk} et en utilisant le lemme \ref{LEMooIBLEooLJczmu}, nous avons
    \begin{equation}
        \| df_a(h)\otimes dg_a(h) \|=\| df_a(h) \|\| dg_a(h) \|\leq \| df_a \|\| dg_a \|\| h \|^2,
    \end{equation}
    donc
    \begin{equation}
        \lim_{h\to 0} \frac{ df_a(h)\otimes dg_a(h) }{ \| h \| }=0.
    \end{equation}
    Notons que l'utilisation du lemme \ref{LEMooIBLEooLJczmu} requière que \( df_a\) soit continue, ce qui n'est pas évident en dimension infinie : une application linéaire n'est pas spécialement continue. C'est donc ici que nous utilisons le fait que \( E\), \( V\) et \( W\) sont de dimension finie\quext{Il y a surement moyen de paufiner, et d'affaiblir cette hypothèse, mais je ne me lance pas là-dedans.}.

    Ceci prouve que \( f\tilde\otimes g\) est différentiable et nous donne la formule \eqref{EQooSUSCooBhZXFC} pour appliquer sa différentielle à un élément de \( E\). La formule \eqref{EQooOCEEooUrsIDd} est un corolaire : elle se vérifie en l'appliquant à \( a\) puis à \( u\).
    
    Pour terminer nous devons prouver que \( d(f\tilde\otimes g)\) est continue. Vu que \( f\) et \( g\) sont de classe \( C^1\), les applications \( f\), \( g\), \( df\) et \( dg\) sont continues. Les applications \( \psi\) et \( \varphi\) sont également continues parce que linéaires sur des espaces de dimensioy finie. La proposition \ref{PROPooCRVXooEGxdZl} appliquée à \( df\) et \( g\) montre que \( df\tilde\otimes g\) est continue. La composition avec \( \psi\) qui est linéaire conserve la continuité.

    Dons le membre de droite de \eqref{EQooOCEEooUrsIDd} est continu et \( f\tilde\otimes g\) est a une différentielle continue. Elle est donc de classe \( C^1\).
\end{proof}

Il est temps de démontrer le truc difficile, à savoir que si \( f\) et \( g\) sont de classe \( C^p\), alors \( f\tilde\otimes g\) est également de classe \( C^p\). 

\begin{proposition}     \label{PROPooAWZFooMlhoCN}
    Nous applellons \( P_k\) la propriété suivante :
    \begin{quote}
        Pour tout \( p\geq k\), pour tout espaces vectoriels normés \( E\), \( V\), \( W\) de dimension finies et pour toutes applications \( f\colon E\to V\) et \( g\colon E\to W\) de classe \( C^k\), la fonction \( f\tilde\otimes g\) est de classe \( C^k\).
    \end{quote}
    \begin{enumerate}
        \item       \label{ITEMooDQRYooAEdxrW}
            La propriété \( P_k\) est vraie pour tout \( k\).
        \item       \label{ITEMooUUIFooGDyTMM}
            Si \( f\colon E\to V\) et \( g\colon E\to W\) sont de classe \( C^p\), alors \( f\tilde\otimes g\colon E\to V\otimes W\) est de classe \( C^p\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le gros de la preuve est le point \ref{ITEMooDQRYooAEdxrW}. Le point \ref{ITEMooUUIFooGDyTMM} est alors une utilisation de la propriété \( P_p\) avec \( p=k\).

    Pour \( k=0\). Si \( f\) et \( g\) sont de classe \( C^p\) avec \( p\geq k\), alors \( f\) et \( g\) sont a fortiori continues. La proposition \ref{PROPooCRVXooEGxdZl} montre alors que \( f\tilde\otimes g\) est continue.

    Bien que ce ne soit pas tout à fait nécessaire, nous prouvons que \( P_1\) est également vraie avant de passer à la récurrence. Si \( f\) et \( g\) sont de classe \( C^p\) avec \( p\geq 1\), alors elles sont de classe \( C^1\) et la proposition \ref{PROPooZOAFooRMeBgI} s'applique : \( f\tilde\otimes g\) est de classe \( C^1\).

    Nous faisons la récurrence en supposant que \( P_k\) est vraie, et en prouvant que \( P_{k+1}\) est vraie. Soit \( p\geq k+1\) ainsi que des applications \( f\colon E\to V\) et \( g\colon E\to W\) de classe \( C^{k+1}\). La proposition \ref{PROPooZOAFooRMeBgI} dit que \( f\tilde\otimes g\) est de classe \( C^1\) et que
    \begin{equation}
        d(f\tilde\otimes g)=\psi\circ(df\tilde\otimes g)+\varphi\circ(f\tilde\otimes dg).
    \end{equation}
    À droite, \( df\) et \( g\) sont de classe \( C^k\) parce que \( f\) et \( g\) sont de classe \( C^{k+1}\). Donc \( df\tilde\otimes g\) est de classe \( C^k\) par l'hypothèse de récurrence appliquée aux espaces \( \aL(E,V)\) et \( W\). La proposition \ref{PROPooRCZOooSgvpSE} nous assure alors que \( \psi\circ(df\tilde\otimes g)\) est de classe \( C^k\) également.

    Nous avons prouvé que \( d(f\tilde\otimes g)\) est de classe \( C^k\), donc \( f\tilde\otimes g\) est de classe \( C^{k+1}\). Cela nous fait la récurrence.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Formule des accroissements finis}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropDQLhSoy}
    Soient \( a<b\) dans \( \eR\) et deux fonctions
    \begin{subequations}
        \begin{align}
            f\colon \mathopen[ a , b \mathclose]\to E\\
            g\colon \mathopen[ a , b \mathclose]\to \eR
        \end{align}
    \end{subequations}
    continues sur \( \mathopen[ a , b \mathclose]\) et dérivables sur \( \mathopen] a , b \mathclose[\). Si pour tout \( t\in\mathopen] a , b \mathclose[\) nous avons \( \| f'(t) \|\leq g'(t)\) alors
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
        \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\) et la fonction
    \begin{equation}
        \begin{aligned}
            \varphi_{\epsilon}\colon \mathopen[ a , b \mathclose]&\to \eR \\
            t&\mapsto \| f(t)-f(a) \|-g(t)-\epsilon t.
        \end{aligned}
    \end{equation}
    Cela est une fonction continue réelle à variable réelle. En particulier pour tout \( u\in\mathopen] a , b \mathclose[\) la fonction \( \varphi_{\epsilon}\) est continue sur le compact \( \mathopen[ u , b \mathclose]\) et donc y atteint son minimum en un certain point \( c\in\mathopen[ u , b \mathclose]\); c'est le bon vieux théorème de Weierstrass~\ref{ThoWeirstrassRn}. Nous commençons par montrer que pour tout \( u\), ledit minimum ne peut être que \( b\). Pour cela nous allons montrer que si \( t\in\mathopen[ u , b [\), alors \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) pour un certain \( s>t\). Par continuité si \( s\) est proche de \( t\) nous avons
        \begin{equation}
            \left\|  \frac{ f(s)-f(t) }{ s-t }  \right\|-\frac{ \epsilon }{2}<\| f'(t) \|<g'(t)+\frac{ \epsilon }{2}=\frac{ g(s)-g(t) }{ s-t }+\frac{ \epsilon }{2}.
        \end{equation}
        Ces inégalités proviennent de la limite
        \begin{equation}
            \lim_{s\to t} \frac{ f(s)-f(t) }{ s-t }=f'(t),
        \end{equation}
        donc si \( s\) et \( t\) sont proches,
        \begin{equation}
            \left\| \frac{ f(s)-f(t) }{ s-t }-f'(t) \right\|
        \end{equation}
        est petit. Si \( s>t\) nous pouvons oublier des valeurs absolues et transformer l'inégalité en
        \begin{equation}
            \| f(s)-f(t) \|<g(s)-g(t)+\epsilon(s-t).
        \end{equation}
        Utilisant cela et l'inégalité triangulaire,
        \begin{subequations}
            \begin{align}
                \varphi_{\epsilon}(s)&\leq\| f(s)-f(t) \|+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &\leq g(s)-g(t)+\epsilon s-\epsilon t+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &=\varphi_{\epsilon}(t).
            \end{align}
        \end{subequations}
        Donc nous avons bien \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) avec l'inégalité stricte. Par conséquent pour tout \( u\in\mathopen] a , b \mathclose[\) nous avons \( \varphi_{\epsilon}(b)<\varphi_{\epsilon}(u)\) et en prenant la limite \( u\to a\) nous avons
        \begin{equation}
            \varphi_{\epsilon}(b)\leq \varphi_{\epsilon}(a).
        \end{equation}
        Cette inégalité donne immédiatement
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a)+\epsilon(b-a)
        \end{equation}
         pour tout \( \epsilon>0\) et donc
         \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
         \end{equation}
\end{proof}

\begin{theorem}[Théorème des accroissements finis]\label{ThoNAKKght}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application différentiable \( f\colon U\to F\). Pour tout segment \( \mathopen[ a , b \mathclose]\subset U\) nous avons
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \| \right)\| b-a \|.
    \end{equation}
\end{theorem}
\index{théorème!accroissements finis}


\begin{proof}
    Nous prenons les applications
    \begin{equation}
        \begin{aligned}
            k\colon \mathopen[ 0 , 1 \mathclose]&\to E \\
            t&\mapsto f\big( (1-t)a+tb \big)
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            g\colon \mathopen[ 0 , 1 \mathclose]&\to \eR \\
            t&\mapsto t\sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \|\| b-a \|.
        \end{aligned}
    \end{equation}
    Pour tout \( t\) nous avons \( g'(t)=M\| b-a \|\) où il n'est besoin de dire ce qu'est \( M\). D'un autre côté nous avons aussi
    \begin{equation}
        \begin{aligned}[]
            k'(t)&=\lim_{\epsilon\to 0}\frac{ f\big( (1-t-\epsilon)a+(t+\epsilon)b \big)-f\big( (1-t)a+tb \big) }{ \epsilon }\\
            &=\Dsdd{ f\big( (1-t)a+tb+\epsilon(b-a) \big)  }{\epsilon}{0}\\
            &=df_{(1-t)a+tb}(b-a)
        \end{aligned}
    \end{equation}
    où nous avons utilisé l'hypothèse de différentiabilité de \( f\) sur \( \mathopen[ a , b \mathclose]\) et donc en \( (1-t)a+tb\). Nous avons donc
    \begin{equation}
        \| k'(t) \|\leq \| b-a \|\| df_{(1-t)a+tb} \|\leq M\| b-a \|=g'(t)
    \end{equation}
    La proposition~\ref{PropDQLhSoy} est donc utilisable et
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est-à-dire
    \begin{equation}
        \| f(b)-f(a) \|=M\| b-a \|
    \end{equation}
    comme il se doit.
\end{proof}

\begin{proposition} \label{ProFSjmBAt}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application \( f\colon U\to F\). Soient \( a,b\in U\) tels que \( \mathopen[ a , b \mathclose]\subset U\). Nous posons \( u=(b-a)/\| b-a \|\) et nous supposons que pour tout \( x\in\mathopen[ a , b \mathclose]\), la dérivée directionnelle
    \begin{equation}
        \frac{ \partial f }{ \partial u }(x)=\Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe. Nous supposons de plus que \( \frac{ \partial f }{ \partial u }(x)\) est continue en \( x=a\). Alors
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \| \right)\| b-a \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous posons évidemment
    \begin{equation}
        M=\sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|
    \end{equation}
    et nous considérons les fonctions
    \begin{equation}
        k(t)=f\big( (1-t)a+tb \big)
    \end{equation}
    et
    \begin{equation}
        g(t)=tM\| b-a \|.
    \end{equation}
    Pour alléger les notations nous posons \( x=(1-t)a+tb\) et nous calculons avec un petit changement de variables dans la limite :
    \begin{equation}
        k'(t)=\Dsdd{  f\big( x+\epsilon(b-a) \big)  }{\epsilon}{0}=\| b-a \|\Dsdd{ f\big( x+\frac{ \epsilon }{ \| b-a \| }(b-a) \big) }{\epsilon}{0}=\| b-a \|\frac{ \partial f }{ \partial u }(x),
    \end{equation}
    et donc encore une fois nous avons
    \begin{equation}
        \| k'(t) \|\leq g'(t),
    \end{equation}
    ce qui donne
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est-à-dire
    \begin{equation}
        \| f(b)-f(a) \|\leq \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|\| b-a \|.
    \end{equation}
\end{proof}

\begin{theorem} \label{ThoOYwdeVt}
    Soient \( E,V\) deux espaces vectoriels normés, une application \( f\colon E\to V\), un point \( a\in E\) tel que pour tout \( u\in E\), la dérivée
    \begin{equation}
        \Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe pour tout \( x\in B(a,r)\) et est continue (par rapport à \( x\)) en \( x=a\). Nous supposons de plus que
    \begin{equation}
        \frac{ \partial f }{ \partial u }(a)=0
    \end{equation}
    pour tout \( u\in E\). Alors \( f\) est différentiable en \( a\) et
    \begin{equation}
        df_a=0
    \end{equation}
\end{theorem}

\begin{proof}
    Soit \( \epsilon>0\). Pourvu que \( \| h \|\) soit assez petit pour que \( a+h\in B(a,r)\), la proposition~\ref{ProFSjmBAt} nous donne
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \sup_{x\in\mathopen[ a , a+h \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|  |h |
    \end{equation}
    où \( u=h/\| h \|\). Par continuité de \( \partial_uf(x)\) en \( x=a\) et par le fait que cela vaut \( 0\) en \( x=a\), il existe un \( \delta>0\) tel que si \( \| h \|<\delta\) alors
    \begin{equation}
        \| \frac{ \partial f }{ \partial u }(a+h) \|\leq \epsilon.
    \end{equation}
    Pour de tels \( h\) nous avons
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \epsilon\| h \|,
    \end{equation}
    ce qui prouve que l'application linéaire \( T(u)=0\) convient parfaitement pour faire fonctionner la définition \ref{DefDifferentiellePta}.
%
%    Nous ne supposons plus que les dérivées directionnelles de \( f\) sont nulles en \( x=a\). Alors nous posons, pour \( x\in U\),
%    \begin{equation}    \label{EqCUgHXHy}
%        g(x)=f(x)-\Dsdd{ f(a+s(x-a)) }{s}{0}.
%    \end{equation}
%    Le fait que cette fonction soit bien définie est encore un coup de hypothèses sur les dérivées directionnelles de \( f\) qui sont bien définies autour de \( a\). Cette nouvelle fonction \( g\) satisfait à \( \frac{ \partial g }{ \partial v }(a)=0\) pour tout \( v\in E\) parce que
%    \begin{subequations}
%        \begin{align}
%            \frac{ \partial g }{ \partial v }(a)&=\Dsdd{ g(a+tv) }{t}{0}\\
%            &=\Dsdd{ f(a+tv)-\Dsdd{ f\big( a+s(tv) \big) }{s}{0} }{t}{0}\\
%            &=\frac{ \partial f }{ \partial v }(a)-\Dsdd{ t\frac{ \partial f }{ \partial v }(a) }{t}{0}\\
%            &=0.
%        \end{align}
%    \end{subequations}
%    Pour la dérivée par rapport à \( s\) nous avons effectué le changement de variables \( s\to ts\), ce qui explique la présence d'un \( t\) en facteur. La fonction \( g\) est donc différentiable en \( a\).
%
%
% Position 229262367
    % Attention : ce qui suit est faux. Mais il y a peut-être moyen d'adapter.
%\item[Dérivées non nulles]
%
%    Nous allons montrer que la fonction
%    \begin{equation}
%        l(x)=\Dsdd{ f\big( a+s(x-a) \big) }{t}{0}
%    \end{equation}
%    est différentiable en \( x=a\), de différentielle \( T(u)=l(u+a)\). Cela fournira la différentiabilité de \( f\) parce que \eqref{EqCUgHXHy} donnerait alors \( f\) comme somme de deux fonctions différentiables.
%
%    En premier lieu nous devons montrer que \( T\) ainsi définie est linéaire.
%
%    Notre but est donc de prouver que
%    \begin{equation}
%        \lim_{h \to 0}\frac{ \| l(x+h)-l(x)-l(h) \| }{ \| h \| }=0.
%    \end{equation}
%    Un premier pas est de calculer
%    \begin{subequations}
%        \begin{align}
%            l(x+h)-l(x)-l(h)&=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(0)-f(sx)+f(0)-f(sh)+f(0) }{ s }\\
%            &=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) }{ s }.
%        \end{align}
%    \end{subequations}
%    Ensuite nous étudions le numérateur en utilisant la proposition~\ref{ProFSjmBAt}:
%    \begin{subequations}
%        \begin{align}
%            \| f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) \|&\leq  \| f\big( s(x+h) \big)-f(sx)\| + \|f(sh)-f(0) \|  \\
%            &\leq \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|\\
%            &\quad +\sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|.
%        \end{align}
%    \end{subequations}
%    La division par \( s\) se passe bien et nous avons
%    \begin{subequations}
%        \begin{align}
%            \| l(x+h)-l(x)-l(h) \|&\leq \lim_{s\to 0}  \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|+ \sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|\\
%            &=2\| h \|\| \frac{ \partial f }{ \partial h }(0) \|        \label{SubeqVMMoSDH}\\
%            &=2\| h \|^2\| \frac{ \partial f }{ \partial u }(0) \|
%        \end{align}
%    \end{subequations}
%    où nous avons posé \( u=h/\| h \|\). Pour l'égalité \eqref{SubeqVMMoSDH} nous avons utilisé la continuité de \( \frac{ \partial f }{ \partial h }(z)\) en \( z=0\). Du coup
%    \begin{equation}
%        \lim_{y\to 0} \frac{ \| f(x+h)-f(x)-f(h) \| }{ \| h \| }=\lim_{h\to 0} 2\| h \|\| \frac{ \partial f }{ \partial u }(0) \|=0.
%    \end{equation}
%    Cela prouve que \( l\) est bien différentiable en \( x=0\).
%
%    \end{subproof}
%
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Applications multilinéaires}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons déjà parlé d'applications multilinéaires dans la définition \ref{DefFRHooKnPCT}.

\begin{lemma}[Leibnitz pour les formes bilinéaires\cite{SNPdukn}]\label{LemFRdNDCd}
    Si \( B\colon E\times F\to G\) est bilinéaire et continue, elle est \(  C^{\infty}\) et
    \begin{equation}    \label{EqXYJgDBt}
        dB_{(x,y)}(u,v)=B(x,v)+B(u,y).
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord le membre de droite de \eqref{EqXYJgDBt} est une application linéaire et continue, donc c'est un bon candidat à être différentielle. Nous allons prouver que ça l'est, ce qui prouvera la différentiabilité de \( B\). Avec ce candidat, le numérateur de la définition \eqref{DefDifferentiellePta} s'écrit dans notre cas
    \begin{equation}
        B\big( (x,y)+(u,v) \big)-B(x,y)-B(x,v)-B(u,y)=B(u,v).
    \end{equation}
    Il reste à voir que
    \begin{equation}
        \lim_{ (u,v)\to (0,0) } \frac{ B(u,v) }{ \| (u,v) \| }=0
    \end{equation}
    Par l'équation \eqref{EqYLnbRbC} nous avons
    \begin{equation}
        \frac{ \| B(u,v) \| }{ \| (u,v) \| }\leq \frac{ \| B \|\| u \|\| v \| }{ \| u \| }=\| B \|\| v \|
    \end{equation}
    parce que \( \| (u,v) \|\geq \| u \|\). À partir de là il est maintenant clair que
    \begin{equation}
        \lim_{(u,v)\to (0,0)}\frac{ \| B(u,v) \| }{ \| (u,v) \| }=0,
    \end{equation}
    ce qu'il fallait.
\end{proof}

\begin{proposition}[Règle de Leibnitz\cite{SNPdukn}]
    Soient \( E,F_1,F_2\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f_1\colon U\to F_1\\
            f_2\colon U\to F_2\\
        \end{align}
    \end{subequations}
    et \( B\in\cL(F_1\times F_2,G)\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon U&\to G \\
            x&\mapsto B\big( f_1(x),f_2(x) \big)
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}    \label{EqMNGBXWc}
        d\varphi_x(u)=\varphi\big( (df_1)_x(u),f_2(x) \big)+\varphi\big( f_1(x),(df_2)_x(u) \big).
    \end{equation}
\end{proposition}
\index{Leibnitz!applications entre espaces vectoriels normés}

\begin{proof}
    Par hypothèse \( B\) est continue (c'est la définition de l'espace \( \cL\)), et donc \(  C^{\infty}\) par le lemme~\ref{LemFRdNDCd}. Par ailleurs la fonction \( f_1\times f_2\) est de classe \( C^r\) parce que \( f_1\) et \( f_2\) le sont et parce que la proposition~\ref{PropOYtgIua} le dit. L'application composée \( B\circ(f_1\times f_2)\) est donc également de classe \( C^r\) par le théorème~\ref{ThoAGXGuEt}.

    Il ne nous reste donc qu'à prouver la formule~\ref{EqMNGBXWc}. En utilisant la différentielle du produit cartésien\footnote{Proposition~\ref{PropOYtgIua}.} nous avons
    \begin{equation}
        f\big( B\circ(f_1\times f_2) \big)_x(h)=dB_{(f_1\times f_2)(x)}\big( (df_1)_x(h),(df_2)_x(h) \big).
    \end{equation}
    Nous développons cela en utilisant le lemme~\ref{LemFRdNDCd} :
    \begin{subequations}
        \begin{align}
        d\big( B\circ(f_1\times f_2) \big)_x(h)&=dB_{\big( f_1(x),f_2(x) \big)}\big( (df_1)_x(h),(df_2)_x(h) \big)\\
        &=B\big( f_1(x),(df_2)_x(h) \big)+B\big( (df_1)_x(h),f_2(x) \big),
        \end{align}
    \end{subequations}
    comme souhaité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Différentielle partielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Différentielle partielle]    \label{VJM_CtSKT}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés et une fonction \( f\colon E\times F\to G\). Nous définissons sa \defe{différentielle partielle}{différentielle!partielle} sur l'espace \( E\) par
    \begin{equation}
        \begin{aligned}
            d_1f_{(x_0,y_0)}\colon E&\to G \\
            u&\mapsto \Dsdd{ f(x_0+tu,y_0 }{t}{0} .
        \end{aligned}
    \end{equation}
    La différentielle \( d_2\) se définit de la même façon.
\end{definition}

\begin{proposition}[\cite{SNPdukn}] \label{PropLDN_nHWDF}
    Soient \( E_1\), \( E_2\) et \( F\) des espaces vectoriels normés, soit un ouvert \( U\subset E_1\times E_2\) et une fonction \( f\colon U\to F\).
    \begin{enumerate}
        \item   \label{ItemRDD_oPmXVi}
            Si \( f\) est différentiable alors les différentielles partielles existent et
            \begin{subequations}
                \begin{align}
                    d_1f_{(x_0,y_0)}(u)=df_{(x_0,y_0)}(u,0)\\
                    d_2f_{(x_0,y_0)}(v)=df_{(x_0,y_0)}(0,v)
                \end{align}
            \end{subequations}
            où \( u\in E_1\) et \( v\in E_2\).
        \item
            Si \( f\) est différentiable alors
            \begin{equation}
                df_{(x_0,y_0)}(u,v)=d_1f_{(x_,y_0)}(u)+d_2f_{(x_0,y_0)}(v).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons \( \alpha=(x_0,y_0)\in U\) et
    \begin{equation}
        \begin{aligned}
            j_{\alpha}^{(1)}\colon E_1&\to E_1\times E_2 \\
            x&\mapsto (x,y_0).
        \end{aligned}
    \end{equation}
    C'est une fonction de classe \(  C^{\infty}\) et
    \begin{equation}
        (dj_{\alpha}^{(1)})_{x_0}(u)=\Dsdd{ j_{\alpha}^{(1)}(x_0+tu) }{t}{0}=\Dsdd{ (x_0+tu,y_0) }{t}{0}=(u,0).
    \end{equation}
    D'autre part
    \begin{subequations}
        \begin{align}
            (d_1f)_{\alpha}(u)&=\Dsdd{ f(x_0+tu,y_0) }{t}{0}\\
            &=\Dsdd{ (f\circ j_{\alpha}^{(1)})(x_0+tu) }{t}{0}\\
            &=\big( d(f\circ j_{\alpha}^{(1)}) \big)_{x_0}(u).
        \end{align}
    \end{subequations}
    À ce moment nous utilisons la règle des différentielles composées~\ref{ThoAGXGuEt} pour dire que
    \begin{equation}
        (d_1f)_{\alpha}(u)=df_{j_{\alpha}^{(1)}(x_0)}\circ (dj_{\alpha}^{(1)})_{x_0}(u)=df_{\alpha}(u,0).
    \end{equation}
    Voila qui prouve déjà le point~\ref{ItemRDD_oPmXVi}.

    Pour la suite nous considérons les fonctions
    \begin{equation}
        \begin{aligned}[]
            P_1(x,y)&=x,&&&J_1(u)&=(u,0),\\
            P_2(x,y)&=y,&&&J_2(v)&=(0,v)
        \end{aligned}
    \end{equation}
    et nous avons l'égalité évidente
    \begin{equation}
        J_1\circ P_1+J_2\circ P_2=\mtu
    \end{equation}
    sur \( E_1\times E_2\). En appliquant \( df_{\alpha}\) à cette dernière égalité, en appliquant à \( (u,v)\) et en utilisant la linéarité de \( df_{\alpha}\) nous trouvons
    \begin{subequations}
        \begin{align}
            df_{\alpha}(u,v)&=df_{\alpha}\big( (J_1\circ P_1)(u,v) \big)+df_{\alpha}\big( (J_2\circ P_2)(u,v) \big)\\
            &=df_{\alpha}(u,0)+df_{\alpha}(0,v)\\
            &=(d_1f)_{\alpha}(u)+(d_2f)_{\alpha}(v)
        \end{align}
    \end{subequations}
    où nous avons utilisé le point~\ref{ItemRDD_oPmXVi} pour la dernière égalité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{L'inverse, sa différentielle}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) est un espace de Banach, nous sommes intéressés à l'espace \( \GL(E)\) des endomorphismes inversibles de \( E\) sur \( E\). Cet ensemble est métrique par la formule usuelle
\begin{equation}
    \| T \|=\sup_{\| x \|=1}\| T(x) \|_E.
\end{equation}

\begin{proposition}[Thème~\ref{THEMEooPQKDooTAVKFH}]     \label{PropQAjqUNp}
    Soit \( E\) un espace de Banach (espace vectoriel normé complet). Si \( A\) est un endomorphisme de \( E\) satisfaisant  \( \| A \|<1\) pour la norme opérateur, alors \( (\mtu-A)\) est inversible et son inverse est donné par
    \begin{equation}
        (\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k.
    \end{equation}
\end{proposition}
\index{série!donnant \( (1-A)^{-1}\)}

\begin{proof}
    Étant donné que la norme opérateur est une norme algébrique (lemme~\ref{LEMooFITMooBBBWGI}), nous avons \( \| A^k \|\leq \| A \|^k\). Par conséquent la série \( \| A^k \|\) est majorée par la série géométrique qui converge\footnote{Voir l'exemple \ref{ExZMhWtJS}.}. Par conséquent \( \sum_{k}A^k\) est une série absolument convergente et donc convergente par la proposition~\ref{PropAKCusNM} et le fait que \( \aL(E)\) est complet (proposition~\ref{LemCAIPooPMNbXg}).

    Montrons à présent que la somme est l'inverse de \( \mtu-A\) en utilisant le produit terme à terme autorisé par la proposition~\ref{PropQXqEPuG} :
    \begin{equation}
        \sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
    \end{equation}
    Par conséquent
    \begin{equation}
        \| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\leq \| A \|^{n+1}\to 0.
    \end{equation}
\end{proof}

\begin{theorem}[Inverse dans \( \GL(E)\)\cite{laudenbach2000calcul,SNPdukn}]    \label{ThoCINVBTJ}
    Soient \( E\) et \( F\) des espaces vectoriels normés.
    \begin{enumerate}
        \item
        L'ensemble \( \GL(E)\) est ouvert dans \( \End(E)\).
    \item
        L'application inverse
    \begin{equation}
        \begin{aligned}
        i\colon \GL(E,F)&\to \GL(F,E) \\
        u&\mapsto u^{-1}
        \end{aligned}
    \end{equation}
    est de classe \( C^{\infty}\) et
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}\circ h\circ u_0^{-1}
    \end{equation}
    pour tout \( h\in\End(E)\)
    \end{enumerate}
\end{theorem}
\index{différentielle!de $u\mapsto u^{-1}$}

\begin{proof}
Nous supposons que \( \GL(E,F)\) n'est pas vide, sinon ce n'est pas du jeu.
        \begin{subproof}

        \item[Cas de dimension finie]

            Si la dimension de \( E\) et \( F\) est finie, elles doivent être égales, sinon il n'y a pas de fonctions inversibles \( E\to F\). L'ensemble \( \GL(E,F)\) est donc naturellement \( \GL(n,\eR)\). Un élément de \( \eM(n,\eR)\) est dans \( \GL(n,\eR)\) si et seulement si son déterminant est non nul. Le déterminant étant une fonction continue (polynomiale) en les entrées de la matrice, l'ensemble \( \GL(n,\eR)\) est ouvert dans \( \eM(n,\eR)\).

            Même idée pour la régularité de la fonction \( i\colon \GL(n,\eR)\to \GL(n,\eR)\), \( X\mapsto X^{-1}\). Les entrées de \( X^{-1}\) sont les cofacteurs de \( X\) divisé par \( \det(X)\), et donc des polynômes en les entrées de \( X\) divisés par un polynôme qui ne s'annule pas sur \( \GL(n,\eR)\), et donc sur un ouvert autour de \( X\) et de \( X^{-1}\). Bref, tout est \(  C^{\infty}\).

            Le reste de la preuve parle de la dimension infinie.

        \item[Ouvert autour de l'identité]

        Nous commençons par prouver que \( B(\mtu,1)\subset \GL(E)\). Pour cela il suffit de remarquer que si \( \| u \|<1\) alors le lemme~\ref{PropQAjqUNp} nous donne un inverse de \( (1+u)\) en la personne de \( \sum_{k=0}^{\infty}(-u)^k\).

    \item[Ouvert en général]

        Soit maintenant \( u_0\in\GL(E)\). Si \( \| u \|<\frac{1}{ \| u_0^{-1} \| }\) alors \( \| u_0^{-1}u \|<1\), ce qui signifie que
        \begin{equation}
            \mtu+u_0^{-1}u
        \end{equation}
    est inversible. Mais \( u_0+u=u_0(\mtu+u_0^{-1}u)\), donc \( u_0+u\in\GL(E)\) ce qui signifie que
    \begin{equation}
    B\left( u_0,\frac{1}{ \| u_0^{-1} \| } \right)\subset \GL(E).
    \end{equation}

    \item[Différentielle en l'identité]

    Nous commençons par prouver que \( di_{\mtu}(u)=-u\). Pour cela nous posons
    \begin{equation}
        \alpha(h)=\sum_{k=2}^{\infty}(-1)^kh^k
    \end{equation}
    et nous calculons
    \begin{equation}
    di_{\mtu}(u)=\Dsdd{ i(\mtu+tu) }{t}{0}=\Dsdd{ \mtu-tu+\alpha(tu) }{t}{0}.
    \end{equation}
    Il suffit de prouver que \( \Dsdd{ \alpha(tu) }{t}{0}=0\) pour conclure que \( di_{\mtu}(u)=-u\). Pour cela, nous remarquons que \( \alpha(0)=0\) et donc que
    \begin{subequations}
        \begin{align}
        \Dsdd{ \alpha(tu) }{t}{0}&=\lim_{t\to 0} \frac{ \alpha(tu)-\alpha(0) }{ t }\\
        &=\lim_{t\to 0} \sum_{k=2}^{\infty}(-1)^k\frac{ (tu)^k }{ t }\\
        &=-\lim_{t\to 0} u\sum_{k=1}^{\infty}(-1)^kt^ku^k.
        \end{align}
    \end{subequations}
    La norme de ce qui est dans la limite est majorée par
    \begin{equation}
    \| u \|\sum_{k=1}^{\infty}\| tu \|^k=\| u \|\left( \frac{1}{ 1-\| tu \| }-1 \right),
    \end{equation}
    et cela tend vers zéro lorsque \( t\to\infty\). Nous avons utilisé la somme~\ref{EqRGkBhrX} de la série géométrique. Nous avons bien prouvé que \( di_{\mtu}(u)=-u\).

    \item[Différentielle en général]
    Soit maintenant \( u_0\in\GL(E)\) et \( h\in\End(E)\) tel que \( u_0+h\in \GL(E)\); par le premier point, il suffit de prendre \( \| h \|\) suffisamment petit. Vu que \( u_0+h=u_0(\mtu+u_0^{-1}h)\) nous avons
    \begin{equation}
        (u_0+h)^{-1}=(\mtu+u_0^{-1}h)^{-1}u_0^{-1}.
    \end{equation}
    Nous pouvons donc calculer
    \begin{equation}
        (u_0+h)^{-1}=\big( \mtu-u_0^{-1}h+\alpha(u_0^{-1}h) \big)u_0^{-1}=u_0^{-1}-u_0^{-1}hu_0^{-1}+\alpha(u_0^{-1}h)u_0^{-1},
    \end{equation}
    et ensuite
    \begin{equation}
        di_{u_0}(h)=\Dsdd{ i(u_0+th) }{t}{0}=\Dsdd{ u_0^{-1}-tu_0^{-1}hu_0^{-1}+\alpha(tu_0^{-1}h)u_0^{-1} }{t}{0},
    \end{equation}
    mais nous avons déjà vu que
    \begin{equation}
        \Dsdd{ \alpha(th) }{t}{0}=0,
    \end{equation}
    donc
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}hu_0^{-1}
    \end{equation}
    Cela donne la différentielle de l'application inverse.

    \item[Continuité de l'inverse]

        L'application \( i\) est continue parce que différentiable.
    \item[L'inverse est \(  C^{\infty}\)]

        Nous allons écrire la fonction inverse comme une composée. Soient les applications
        \begin{equation}
            \begin{aligned}
                B\colon \cL(F,E)\times \cL(F,E)&\to \cL\big( \cL(E,F),\cL(F,E) \big) \\
                B(\psi_1,\psi_2)(A)&= -\psi_1\circ A\circ\psi_2
            \end{aligned}
        \end{equation}
        et
        \begin{equation}
            \begin{aligned}
                \Delta\colon \cL(F,E)&\to \cL(F,E)\times \cL(F,E) \\
                \varphi&\mapsto (\varphi,\varphi)
            \end{aligned}
        \end{equation}
        Nous avons alors
        \begin{equation}
            di=B\circ\Delta\circ i.
        \end{equation}
        L'application \( \Delta\) est de classe \(  C^{\infty}\). Nous devons voir que \( B\) l'est aussi. Pour le voir nous commençons par prouver qu'elle est bornée :
        \begin{equation}
            \begin{aligned}[]
                \| B \|&=\sup_{\| \psi_1 \|,\| \psi_2 \|=1}\| B(\psi_1,\psi_2) \|_{\aL\big( L(E,F),L(F,E) \big)}\\
                &=\sup_{  \| \psi_1 \|,\| \psi_2 \|=1 }\sup_{\| A \|=1}\| \psi_1\circ A\circ\psi_2 \|_{L(F,E)}\\
                &\leq \sup_{\| \psi_1 \|,\| \psi_2 \|=1}\sup_{\| A \|=1}\| \psi_1 \|\| A \|\| \psi_2 \|\\
                &\leq 1.
            \end{aligned}
        \end{equation}
        Donc \( B\) est bien bornée et par conséquent continue. Une application bilinéaire continue est \(  C^{\infty}\) par le lemme~\ref{LemFRdNDCd}. La décomposition \( di=B\circ \Delta\circ i\) nous donne donc que \( i\in C^{\infty}\) dès que \( i\) est continue, ce que nous avions déjà montré.
        \end{subproof}
\end{proof}



%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exponentielle de matrice}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{secAOnIwQM}

\begin{proposition}     \label{PropPEDSooAvSXmY}
    Soit \( V\) un espace vectoriel de dimension finie et \( A\in\End(V)\). La série
    \begin{equation}
        \exp(A)=\mtu+A+\frac{ A^2 }{ 2 }+\frac{ A^3 }{ 3 }+\ldots =\sum_{k=1}^{\infty}\frac{ A^k }{ k! }.
    \end{equation}
    converge normalement dans \( \big( \End(V),\| . \|_{op} \big)\).  L'\defe{exponentielle}{exponentielle!de matrice} de la matrice \( A\) est cette matrice.
\end{proposition}

\begin{proof}
    Vu que la norme opérateur est une norme d'algèbre par le lemme~\ref{LEMooFITMooBBBWGI}, nous avons pour tout \( k\) la majoration \( \| A^k \|\leq \| A \|^k\). Nous avons donc
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \| A^k \| }{ k! }\leq \sum_k\frac{ \| A \|^k }{ k! }.
    \end{equation}
    La dernière somme converge en vertu de la convergence de la série exponentielle donnée en exemple~\ref{ExIJMHooOEUKfj}.
\end{proof}

Étant donné que c'est une limite, il y a une question de convergence et donc de topologie. C'est pour cela que nous ne pouvions pas introduire l'exponentielle de matrice avant d'avoir introduit la norme des matrices. La convergence de la série pour toute matrice sera prouvée au passage dans la proposition~\ref{PropFMqsIE}.


La fonction exponentielle \(  x\mapsto e^{x}\) n'est pas un polynôme en \( x\), mais nous avons le résultat marrant suivant.
\begin{proposition} \label{PropFMqsIE}
    Si \( u\) est un endomorphisme, alors \( \exp(u)\) est un polynôme en \( u\)\footnote{Nan, mais j'te jure : \( \exp\) n'est pas un polynôme, mais $\exp(u)$ est un polynôme de \( u\).}.
\end{proposition}

\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi_u\colon \eK[X]&\to \End(E) \\
            P&\mapsto P(u)
        \end{aligned}
    \end{equation}
    Étant donné que l'image de \( \varphi_u\) est un fermé dans \( \End(E)\), il suffit de montrer que la série
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \varphi_u(X)^k }{ k! }
    \end{equation}
    converge dans \( \End(E)\) pour qu'elle converge dans \( \Image(\varphi_u)\). Pour ce faire nous nous rappelons de la norme opérateur\footnote{Définition~\ref{DefNFYUooBZCPTr}.} et de la propriété fondamentale \( \| A^k \|\leq \| A \|^k\). En notant \( A=\varphi_u(X)\),
    \begin{equation}
        \left\| \sum_{k=n}^m\frac{ A^k }{ k! } \right\|\leq \sum_{k=n}^m\frac{ \| A^k \| }{ k! }\leq \sum_{k=n}^m\frac{ \| A \|^k }{ k! },
    \end{equation}
    ce qui est une morceau du développement de \(  e^{\| A \|}\). La limite \( n\to\infty\) est donc zéro par la convergence de l'exponentielle réelle. La suite des sommes partielles de  $e^{A}$ est donc de Cauchy. La série converge donc parce que nous sommes dans un espace vectoriel réel de dimension finie (\( \End(E)\)).
\end{proof}
% TODO : et tant qu'on y est, justifier la convergence de la série de l'exponentielle réelle.

\begin{normaltext}
    Pourquoi \( \exp(u)\) est-il un polynôme d'endomorphisme alors que \( \exp\) n'est pas un polynôme ? Lorsque nous disons que la fonction \( x\mapsto \exp(x)\) n'est pas un polynôme, nous sommes en train de localiser la fonction \( \exp\) à l'intérieur de l'espace de toutes les fonctions \( \eR\to \eR\), c'est-à-dire à l'intérieur d'un espace de dimension infinie. Au contraire lorsqu'on parle de \( \exp(u)\) et qu'on le compare aux endomorphismes \( P(u)\), nous sommes en train de repérer \( \exp(u)\) à l'intérieur de l'espace des matrices qui est de dimension finie. Il n'est donc pas étonnant que l'on parvienne moins à faire la distinction.

    Si par contre nous considérons \( \exp\) en tant qu'application \( \exp\colon \End(E)\to \End(E)\), ce n'est pas un polynôme.

    Si \( u\) et \( v\) sont des endomorphismes, nous aurons des polynômes \( P\) et \( Q\) tels que \( e^u=P(u)\) et \( e^v=Q(v)\); mais nous n'aurons en général évidemment pas \( P=Q\). En cela, \( \exp\) n'est pas un polynôme.
\end{normaltext}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espace dual}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooKOJNooQVawFY}

\begin{definition}
    Soit un espace vectoriel normé \( (V,\| . \|)\) sur le corps \( \eC\) ou \( \eR\) (que nous nommons \( \eK\)). Son \defe{dual topologique}{dual topologique}, noté \( V'\) est l'ensemble des applications linéaires continues \( V\to \eK\).
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Topologies}
%---------------------------------------------------------------------------------------------------------------------------

Il est possible de mettre sur \( V'\) (au moins) deux topologies distinctes. La première est la topologie de la norme opérateur; rien de nouveau pour elle. La seconde est la topologie \( *\)-faible dont nous avons déjà un peu parlé dans la définition~\ref{DefHUelCDD}.

En termes de notations, nous allons noter les semi-normes de la topologie faible par
\begin{equation}
    p_x(\varphi)=| \varphi(x) |
\end{equation}
pour \( x\in V\) et \( \varphi\in V'\). À droite, les barres dénotent soit la valeur absolue (si \( \eK=\eR\)), soit le module (si \( \eK=\eC\)).

\begin{lemma}       \label{LEMooFMAUooQBIeTh}
    Soit \( \varphi\in V'\) et \( x\in V\). Alors
    \begin{equation}
        p_x(\varphi)\leq\frac{ \| \varphi \| }{ \| x \| }.
    \end{equation}
    Si \( \varphi_0\in V'\), si \( r>0\) et si \( x\in V\) nous avons aussi :
    \begin{equation}
        B(\varphi_0,r)\subset B_x(\varphi_0,\frac{ r }{ \| x \| }).
    \end{equation}
\end{lemma}

\begin{proof}
    En posant \( x'=x/\| x \|\) nous avons
    \begin{equation}
        p_x(\varphi)=| \varphi(x) |=\frac{1}{ \| x \| }| \varphi(x') |\leq \frac{1}{ \| x \| }\| \varphi \|.
    \end{equation}

    En ce qui concerne la seconde affirmation, si \( \varphi\in B(\varphi_0,r)\) alors en notant \( x'=x/\| x \|\) nous avons :
    \begin{equation}
        p_x(\varphi_0-\varphi)=| \varphi_0(x)-\varphi(x) |=\frac{1}{ \| x \| }| \varphi_0(x')-\varphi(x') |\leq\frac{1}{ \| x \| }\|\varphi_0-\varphi  \|\leq \frac{ r }{ \| x \| }.
    \end{equation}
    Donc \( \varphi\in B_x\big( \varphi_0,\frac{ r }{ \| x \| } \big)\).
\end{proof}

\begin{proposition}
    En ce qui concerne la convergence d'une suite \( (\varphi_k)\) dans \( V'\) mais si elle vérifie
    \begin{equation}
        \varphi_k\stackrel{\| . \|}{\longrightarrow}\varphi
    \end{equation}
    alors
    \begin{equation}
        \varphi_k\stackrel{*}{\longrightarrow}\varphi.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit une suite \( (\varphi_k)\) dans \( V'\), convergente vers \( \varphi\) pour la topologie de la norme.  Soit \( x\in V\), et \( x'=x/\| x \|\). Nous avons
    \begin{equation}
        p_x(\varphi_k-\varphi)=\frac{1}{ \| x \| }| \varphi_k(x')-\varphi(x) |\leq\frac{1}{ \| x \| }\| \varphi_k-\varphi \|\to 0.
    \end{equation}
\end{proof}

\begin{lemma}       \label{LEMooEAVEooAFveHn}
    La translation dans \( V'\) est une opération continue pour la topologie de la norme opérateur et pour celle de la topologie \( *\).
\end{lemma}

\begin{proof}
    Soit une suite \( \varphi_k\) tendant vers \( 0\); nous devons prouver que \( \tau_{\sigma}(\varphi_k)\to \tau_{\sigma}(0)=\sigma\). Et ce, pour chacune des deux topologies.

    \begin{subproof}
        \item[Norme opérateur]

            L'hypothèse \( \varphi_k\stackrel{\| . \|}{\longrightarrow} 0\) signifie que \( \| \varphi_k \|\to 0\), c'est-à-dire que
            \begin{equation}
                \sup_{\| v \|=1}| \varphi_k(v) |\to 0.
            \end{equation}
            Nous avons alors
            \begin{equation}
                \| \tau_{\sigma}(\varphi_k)-\sigma \|=\sup_{\| v \|=1}| \tau_{\sigma}(\varphi_k)v-\sigma(v) |=\sup_{\| v \|=1}| \varphi_k(v) |\to 0.
            \end{equation}
            Donc d'accord pour \( \tau_{\sigma}(\varphi)\to \sigma\).

        \item[Topologie $*$]

            Nous supposons maintenant que \( \varphi_k\stackrel{*}{\longrightarrow}0\). Pour tout \( v\in V\) nous avons
            \begin{equation}
                p_v\big( \tau_{\sigma}(\varphi_k)-\sigma \big)=\big| \tau_{\sigma}(\varphi_k)v-\sigma(v) \big|=| \varphi_k(v) |=p_v(\varphi_k).
            \end{equation}
            Mais par hypothèse, \( p_v(\varphi_k)\to 0\).
    \end{subproof}
\end{proof}

Pour la suite, nous allons préfixer par \( N\) les concepts liés à la topologie de \( V'\) associée à la norme opérateur et par \( *\), les concepts de la topologie \( *\).

\begin{proposition}     \label{PROPooFGXAooFRWweD}
    Soit un espace vectoriel normé \( V\). Un \( *\)-ouvert et toujours un \( N\)-ouvert.
\end{proposition}

\begin{proof}
    Soit un \( *\)-ouvert \( \mO\) de \( V'\). Il existe donc \( x\in V\) et \( r>0\) tels que \( B_x(\varphi,r)\subset \mO\). Nous avons alors, en utilisant le lemme~\ref{LEMooFMAUooQBIeTh},
    \begin{equation}
        B(\varphi,r\| x \|)\subset B_x(\varphi,r)\subset \mO.
    \end{equation}
    Donc \( \mO\) est un \( N\)-ouvert.
\end{proof}

\begin{corollary}
    Soit un espace topologique \( X\). Si \( f\colon (V',*)\to X\) est continue, alors \( f\colon (V',\| . \|)\to X\) est continue.
\end{corollary}

\begin{proof}
    Soit un ouvert \( \mO\) de \( X\). Vu que \( f\) est \( *\)-continue, la partie \( f^{-1}(\mO)\) est un \( *\)-ouvert de \( V'\). Il est onc un \( N\)-ouvert de \( V'\) par la proposition~\ref{PROPooFGXAooFRWweD}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réflexivité}
%---------------------------------------------------------------------------------------------------------------------------

Pour la suite nous notons \( V''\) le dual de \( (V',\| . \|)\). Certes en tant qu'ensembles, \( (V',*)\) et \( (V',\| . \|) \) sont identiques, mais comme ils n'ont pas la même topologie, les duaux ne sont pas les mêmes.

Bref, \( V''\) est l'ensemble des applications linéaires continues \( (V',\| . \|)\to \eC\). Et lorsque nous disons \( \eC\) ici, ça peut aussi bien être \( \eR\) selon le contexte.

De plus nous considérons que \( V''\) la norme opérateur qui dérive de la norme de \( V'\), laquelle dérive de la norme vectorielle sur \( V\).

\begin{propositionDef}      \label{PROPooMAQSooCGFBBM}
    Soit un espace vectoriel normé $V$ sur $\eR$ ou $\eC$. Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            J\colon V&\to V'' \\
            J(x)\varphi&= \varphi(x).
        \end{aligned}
    \end{equation}
    \begin{enumerate}
        \item       \label{ITEMooNVVSooNFXgnE}
            L'application \( J\) est bien définie : \( J(x)\) est continue.
        \item       \label{ITEMooKURHooZZWpbu}
            L'application \( J\) est continue.
        \item       \label{ITEMooTFYVooKhMOjp}
             Elle est injective.
    \end{enumerate}

    Lorsque \( J\) est bijective, l'espace \( V\) est dit \defe{réflexif}{réflexif}.
\end{propositionDef}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[\ref{ITEMooNVVSooNFXgnE}]
            Nous commençons par montrer que \( J(x)\colon (V',\| . \|)\to \eC\) est continue pour chaque \( x\in V\). Soit une suite \( \varphi_k\stackrel{\| . \|}{\longrightarrow}0\). Nous avons :
            \begin{equation}
                J(x)\varphi_k=\varphi_k(x)\leq \| \varphi_k \|\| x \|\to 0
            \end{equation}
            où vous aurez noté l'utilisation du lemme~\ref{LEMooIBLEooLJczmu}.  Cela prouve que \( J(x)\) est continue et donc que \( J\) est bien à valeurs dans \( V''\).
        \item[\ref{ITEMooKURHooZZWpbu}]

            Soit une suite \( x_k\stackrel{V}{\longrightarrow}0\), et étudions \( \| J(x_k) \|\) pour la norme dans \( V''\). Nous posons \( x'_k=x_k/\| x_k \|\) et nous calculons (encore une fois, nous écrivons «\( \eC\)», mais ça pourrait être \( \eR\))
            \begin{equation}
                \| J(x_k) \|=\sup_{\| \varphi \|=1}| J(x_k)\varphi |_{\eC}=\sup_{\| \varphi \|=1}| \varphi(x_k) |=\| x_k \|\sup_{\| \varphi \|=1}| \varphi(x'_k) |\leq \| x_k \|\to 0.
            \end{equation}
            La dernière inégalité pourrait être sans doute une égalité\quext{Écrivez-moi si vous en êtes certain.}, mais nous n'en avons pas besoin ici.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Module de continuité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soient deux espaces topologiques normés \( X\) et \( Y\),  ainsi qu'une application \( f\colon X\to Y\). Le \defe{module de continuité}{module!de continuité} de \( f\) est la fonction
    \begin{equation}
        \begin{aligned}
            \omega_f\colon \eR^+&\to \eR^+\cup\{ \infty \} \\
            h&\mapsto\sup_{\substack{x,y\in X\\d_X(x,y)< h}} d_Y\big( f(x),f(y) \big).
        \end{aligned}
    \end{equation}
    Nous définissons aussi \( \omega_f(h)=0\) pour \( h\leq 0\).
\end{definition}

Notons que le module de continuité est une fonction croissante.

\begin{lemma}   \label{LemLUbgYeo}
    Soit \( f\in C^0\big( \mathopen[ 0 , 1 \mathclose],\eC \big)\) et \( \omega\) son module de continuité. Si \( \lambda\) et \( h\) sont strictement positifs avec \( \lambda h\in\mathopen[ 0 , 1 \mathclose]\) alors
    \begin{equation}
        \phi(\lambda h)\leq (\lambda+1)\omega(h).
    \end{equation}
\end{lemma}

\begin{proof}
    La fonction \( \omega\) est décroissante, et pour \( h,k>0\) nous avons \( \omega(h+k)\leq\omega(h)+\omega(k)\). Par récurrence pour tout \( k\in \eN\) nous avons
    \begin{equation}
        \omega(kh)\leq k\omega(h).
    \end{equation}
    En écrivant cela pour \( k=\lceil \lambda\rceil\), nous avons
    \begin{equation}
        \omega(\lambda h)\leq \omega(kh)\leq k\omega(h)\leq (\lambda+1)\omega(h).
    \end{equation}
\end{proof}

\begin{lemma}   \label{LemeERapq}
    Une fonction est uniformément continue\footnote{Définition \ref{DEFooYIPXooQTscbG}.} si et seulement si son module de continuité est continu en zéro\footnote{Dans ce lemme, nous avons deux espaces métriques, mais nous allons noter \( d\) la distance des deux côtés.}.
\end{lemma}

\begin{proof}
    Nous commençons par supposer que \( f\) est uniformément continue. Soit \( \epsilon>0\). Par uniforme continuité, il existe \( \delta>0\) tel que \( d\big( f(x),f(y) \big)\leq \epsilon\) dès que \( d(x,y)\leq \delta\). Si \( h\in B(0,\delta)\), alors
    \begin{equation}
        \omega_f(h)\leq \omega_f(\delta)=\sup_{\substack{x,y\in X\\d(x,y)\leq \delta}}d\big( f(x),f(y) \big)\leq \epsilon.
    \end{equation}
    Cela prouve que \( \lim_{h\to 0} \omega_f(h)=0\).

    Dans l'autre sens, si \( \epsilon>0\) est fixé, il suffit de prendre \( \delta\) tel que \( \omega_f(h)\leq \epsilon\) pour tout \( h\leq \delta\) pour faire fonctionner la définition de l'uniforme continuité.
\end{proof}

\begin{lemma}[\cite{ooCPZDooOqIIEz}]        \label{LEMooKPPSooPIncvn}
    Soient des espaces métriques \( E\) et \( E'\) et une suite de fonctions \( (f_i)_{i\geq 0}\) qui converge uniformément vers \( f\). Alors pour chaque \( \delta>0\) nous avons
    \begin{equation}
        \limsup_{i\to \infty}\omega_{f_i}(\delta)\leq \omega_f(\delta).
    \end{equation}
\end{lemma}

\begin{proof}
    Soient \( \delta>0\) ainsi que \( x,y\in E\) tels que \( \| x-y \|\leq \delta\). Pour chaque \( i\) nous avons
    \begin{subequations}
        \begin{align}
            | f_i(x)-f_i(y) |&\leq | f_i(x)-f(x) |+| f(x)-f(y) |+| f(y)-f_i(y) |\\
            &\leq | f(x)-f(y) |+2\| f_i-f \|_{\infty}\\
            &\leq \omega_f(\delta)+2\| f_i-f \|_{\infty}.
        \end{align}
    \end{subequations}
    Nous prenons le supremum de cela sur \( \{  x,y\in E\tq \| x-y \|\leq \delta \}\) pour obtenir :
    \begin{equation}
        \omega_{f_i}(\delta)\leq \omega_f(\delta)+2\| f_i-f \|_{\infty}.
    \end{equation}
    La tentation est grande à ce point de prendre la limite des deux côtés pour \( i\to \infty\). Cependant, rien ne nous permet de dire que la suite \( i\mapsto   \omega_{f_i}(\delta)  \) ait une limite. Nous pouvons cependant prendre la limite supérieures\footnote{Définition \ref{ooMVZAooVVCOnP}.} et obtenir
    \begin{equation}
        \limsup_{i\to \infty}\omega_{f_i}(\delta)\leq \omega_f(\delta).
    \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mini introduction aux nombres \texorpdfstring{\( p\)}{p}-adiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\subsection{La flèche d'Achille}\label{s:un}

C'est un grand classique que je donne ici juste comme introduction pour montrer que des séries infinies peuvent donner des nombres finis de manière tout à fait intuitive.

Achille tire une flèche vers un arbre situé à $\unit{10}{\meter}$ de lui. Disons que la flèche avance à une vitesse constante de $\unit{1}{\meter\per\second}$. Il est clair que la flèche mettra $\unit{10}{\second}$ pour toucher l'arbre. En $\unit{5}{\second}$, elle aura parcouru la moitié de son chemin. On le note :
\[
\text{temps}=5s+\ldots
\]
Reste \( \unit{5}{\meter}\) à faire. En $\unit{2.5}{\second}$, elle aura fait la moitié de ce chemin chemin, soit $2.5m=\frac{10}{4}m$. On le note :
\[
\text{temps}=\frac{10}{2}s+\frac{10}{4}s+
\]
Reste $2.5m$ à faire. La moitié de ce trajet, soit $\frac{10}{8}m$, est parcouru en $\frac{10}{8}s$; on le note encore, mais c'est la dernière fois !

\[
\text{temps}=\frac{10}{2}s+\frac{10}{4}s+\frac{10}{8}s+
\]
En continuant ainsi à regarder la flèche qui parcours des demi-trajets puis des moitiés de demi-trajets et encore des moitiés de moitiés de demi-trajets, et en sachant que le temps total est $10s$, on trouve :
\[
10\left( \frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\ldots  \right)=10.
\]
On doit donc croire que la somme jusqu'à l'infini des inverses des puissances de deux vaut $1$ :
\[
   \sum_{n=1}^{\infty}\frac{1}{2^n}=1.
\]
Cela peut être démontré à la loyale.

\subsection{La tortue et Achille}

Maintenant qu'on est convaincu que des sommes infinies peuvent représenter des nombres tout à fait normaux, passons à un truc plus marrant.

Achille, qui marche peinard à $\unit{10}{\meter\per\hour}$, part avec $1m$ d'avance sur une tortue qui avance à $\unit{1}{\meter\per\hour}$. Le temps que la tortue arrive au point de départ d'Achille, Achille aura parcouru $10m$, et le temps que la tortue mettra pour arriver à ce point, eh bien, Achille ne sera déjà plus là : il sera à $100m$. Si la tortue tient bon pendant un temps infini, et si l'on est confiant en le genre de raisonnements faits à la section~\ref{s:un}, elle rattrapera Achille dans
\[
1m+10m+100m+1000m+\ldots
\]
Autant dire que ça ne risque pas d'arriver. Et pourtant, mettons en équations :
\begin{subequations}
    \begin{numcases}{}
        x_{\text{Achile}}(t)=1+10t\\
        x_{\text{tortue}}(t)=t.
    \end{numcases}
\end{subequations}
La tortue rejoint Achille au temps \( t\) tel que \( x_{\text{Achille}(t)}=x_{\text{tortue}}(t)\). Un mini calcul donne $t=-1/9$. Physiquement, c'est une situation logique. Peut-on en déduire une égalité mathématique du style de
\[
1+10+100+1000+\ldots=-\frac{1}{9}\; ???
\]
Là où les choses deviennent jolies, c'est quand on cherche à voir ce que peut bien être la valeur d'un hypothétique $x=1+10+100+1000+\ldots$. En effet, logiquement on devrait avoir
\begin{equation*}
\begin{split}
\frac{x}{10}&=\frac{1}{10}+1+10+100+\ldots\\
            &=\frac{1}{10}+x.
\end{split}
\end{equation*}
Reste à résoudre l'équation du premier degré : $\frac{x}{10}=x+\frac{1}{10}$. Ai-je besoin de donner la solution ?

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dans les nombres \texorpdfstring{\( p\)}{p}-adiques, c'est vrai}
%---------------------------------------------------------------------------------------------------------------------------

Nous nous proposons d'apprendre sur les nombres \( p\)-adiques juste ce qu'il faut pour montrer que l'égalité
\begin{equation}
    \sum_{k=0}^{\infty}10^k=-\frac{1}{ 9 }
\end{equation}
est vraie dans les nombres \( 5\)-adiques. Tout ce qu'il faut est sur \wikipedia{fr}{Nombre_p-adique}{wikipedia}.

Soit \( a\in \eN\) et \( p\), un nombre premier. La \defe{valuation}{valuation!$p$-adique} \( p\)-adique de \( a\) est l'exposant de \( p\) dans la décomposition de \( a\) en nombres premiers. On la note \( v_p(a)\). Pour un rationnel on définit
\begin{equation}
    v_p\left( \frac{ a }{ b } \right)=v_p(a)-v_p(b)
\end{equation}
La \defe{valeur absolue}{valeur absolue!$p$-adique} \( p\)-adique de \( r\in \eQ\) est
\begin{equation}
    | r |_p=p^{-v_p(r)}.
\end{equation}
Nous posons \( | 0 |_p=0\). De là nous considérons la distance
\begin{equation}
    d_p(x,y)=| x-y |_p.
\end{equation}

\begin{lemma}
    L'espace \( (\eQ,d_p)\) est un espace métrique\footnote{Définition~\ref{DefMVNVFsX}}.
\end{lemma}
\index{topologie!\( p\)-adique}

Nous considérons maintenant \( p=5\). Étant donné que \( a=5\cdot 2\) nous avons \( v_5(10)=1\) et
\begin{equation}
    v_5\left( \frac{1}{ 9 } \right)=v_5(1)-v_5(9)=0.
\end{equation}
Nous avons
\begin{equation}
    \sum_{k=0}^N10^k+\frac{1}{ 9 }=\frac{ 10^{N+1} }{ 9 }
\end{equation}
mais
\begin{equation}
    v_p\left( \frac{ 10^{N+1} }{ 9 } \right)=v_5(10^{N+1})-v_5(9)=N+1.
\end{equation}
Par conséquent
\begin{equation}
    d_5\big( \sum_{k=0}^N10^k,-\frac{1}{ 9 } \big)=| \frac{ 10^{N+1} }{ 9 } |_p=p^{-(N+1)}.
\end{equation}
En passant à la limite,
\begin{equation}
    \lim_{N\to \infty} d_5\big( \sum_{k=0}^N10^k,-\frac{1}{ 9 } \big)=0,
\end{equation}
ce qui signifie que\footnote{Voir la définition~\ref{DefGFHAaOL} de la convergence d'une série dans un espace métrique.}
\begin{equation}
    \sum_{k=0}^{\infty}10^k=-\frac{1}{ 9 }.
\end{equation}
