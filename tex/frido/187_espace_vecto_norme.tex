% This is part of Le Frido
% Copyright (c) 2008-2023
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Produit tensoriel d'espaces vectoriels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Si vous êtes pressés, vous pouvez aller lire la définition \ref{DEFooKTVDooSPzAhH} de produit tensoriel d'espaces vectoriels. Mais si vous étiez vraiment pressés, vous ne seriez pas en train de lire des choses sur le produit tensoriel (il vous suffit de croire que \( x\otimes y\) n'est finalement que la concatenation de \( x\) et \( y\)).

\begin{propositionDef}      \label{PROPooYONEooWvwPZT}
	Soient un espace vectoriel \( V\) et un sous-espace \( N\). Le \defe{quotient}{quotient d'un espace vectoriel} de \( V\) par \( N\), noté \( V/N\) est l'ensemble des classes d'équivalence\footnote{Définition \ref{DEFooRHPSooHKBZXl}.} pour la relation \( x\sim y\) si et seulement si \( x-y\in N\).

	Les définitions
	\begin{enumerate}
		\item
		      \( [v]+[w]=[v+w]\)
		\item
		      \( \lambda[v]=[\lambda v]\)
	\end{enumerate}
	ont un sens et définissent une structure d'espace vectoriel sur \( V/N\).

	En ce qui concerne la topologie, ce sera la définition \ref{DEFooHWSYooZZLXQU}.
\end{propositionDef}

\begin{proof}
	Un élément général de la classe \( [v]\) est de la forme \( v+n\) avec \( n\in N\). Le calcul suivant montre que la somme fonctionne :
	\begin{equation}
		[v+n_1]+[w+n_2]=[v+w+n_1+n_2]=[v+w]
	\end{equation}
	parce que \( n_1+n_2\in N\). De même,
	\begin{equation}
		\lambda[v+n]=[\lambda v+\lambda n]=[\lambda v]
	\end{equation}
	toujours parce que \( \lambda n\in N\).

	Notons que nous avons utilisé de façon on ne peut plus cruciale le fait que \( N\) soit un sous-espace vectoriel.
\end{proof}

\begin{proposition}
	Si \( \{ e_i \}\) est une base de \( V\) et si \( N\) est un sous-espace de \( V\), alors \( \{ [e_i] \}\) est une partie génératrice de \( V/N\).
\end{proposition}

\begin{proof}
	Si \( x=\sum_kx_ke_k\), alors \( [x]=\sum_kx_k[e_k]\), donc oui.
\end{proof}

%+++++++++++++++++++++++++++++++++++
\section{Produit tensoriel}
%+++++++++++++++++++++++++++++++++++

Nous allons faire les choses suivantes:
\begin{itemize}
	\item
	      Définir ce qu'est \emph{un} produit tensoriel de \( V\) et \( W\), définition \ref{DEFooXKKQooAvWRNp}.
	\item
	      Prouver que tous les produits tensoriels sont isomorphes, définition \ref{DEFooPLHTooRiHjlE} et proposition \ref{PROPooROPHooQXqNzZ}.
	\item
	      Construire un produit tensoriel relativement abstrait à base d'espace librement engendré par \( V\times W\), définition \ref{DEFooKTVDooSPzAhH} et proposition \ref{PROPooIWZDooRRZNCf}.
	\item
	      Construire un produit tensoriel assez concret en tant qu'espace de formes bilinéaires sur \( V\times W\), proposition \ref{PROPooIFVBooGiMskq}.
\end{itemize}

\begin{normaltext}
	Dans le Frido, la notation \( V\otimes W\) pourra désigner (au moins) trois choses différentes.
	\begin{itemize}
		\item
		      Un produit tensoriel quelconque, c'est à dire un espace quelconque vérifiant les conditions de la définition \ref{DEFooXKKQooAvWRNp}.
		\item
		      L'espace $F_{\eK}(V\times W)/N$ défini en \ref{DEFooKTVDooSPzAhH}.
		\item
		      L'espace \( \aL_2(V\times W,\eK)\) des formes bilinéaires.
	\end{itemize}
	Dans tous les cas l'espace \( T\) qui veut prétendre être le produit tensoriel \( V\otimes W\) doit venir avec une application \( h\). Lorsque \( (T,h)\) est un produit tensoriel de \( V\) et \( W\), nous notons \( v\otimes w\) l'élément \( h(v,z)\in T\).
\end{normaltext}

%-----------------------------------
\subsection{Définition générale}
%-----------------------------------

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooXKKQooAvWRNp}
	Soient deux espaces vectoriels \( V\) et \( W\). Un \defe{produit tensoriel}{produit tensoriel} de \( V\) et \( W\) est un couple \( (T,h)\) où \( T\) est un espace vectoriel et \( h\colon V\times W\to T\) est une application
	\begin{enumerate}
		\item
		      bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.}
		\item       \label{ITEMooJCNYooGvjjtL}
		      telle que pour tout espace vectoriel \( Z\) et toute application bilinéaire \( f\colon V\times W\to Z\), il existe une unique application linéaire \( \tilde f\colon T\to Z\) telle que \( f=\tilde f\circ h\).
	\end{enumerate}
	La propriété \ref{ITEMooJCNYooGvjjtL} est appelée \defe{propriété universelle}{propriété universelle} du produit tensoriel.
\end{definition}

\begin{definition}  \label{DEFooPLHTooRiHjlE}
	Un \defe{morphisme}{morphisme de produits tensoriels} entre \( (T,h)\) et \( (T',h')\) est une application linéaire \( \psi\colon T\to T'\) telle que \( h'=\psi\circ h\).

	Nous parlons d'\defe{isomorphisme}{isomorphisme} si \( \psi\) a un inverse qui est également un morphisme.
\end{definition}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]      \label{PROPooROPHooQXqNzZ}
	Si \( V\) et \( W\) sont des espaces vectoriels, tous les produits tensoriels entre \( V\) et \( W\) sont isomorphes entre eux au sens de la définition \ref{DEFooPLHTooRiHjlE}.

	Plus précisément, si \( (T,h)\) et \( (T',h')\) sont deux produits tensoriels de \( V\) et \( W\), alors
	\begin{enumerate}
		\item
		      il existe une unique application linéaire \( g\colon T\to T'\) telle que \( h'=g\circ h\),
		\item
		      cette application \( g\) est inversible.
	\end{enumerate}
	En particulier, l'application \( g\) est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
	Soient deux produits tensoriels \( (T,h)\) et \( (T',h')\).

	\begin{subproof}
		\spitem[Existence]

		L'application \( h'\colon V\oplus W\to T'\) est bilinéaire, et \( (T,h)\) est un produit tensoriel. Donc il existe \( g\colon T\to T'\) tel que \( h'=g\circ h\). De même, il existe une application \( g'\colon T'\to T\) telle que \( h=g'\circ h\).

		\spitem[Unicité]

		En ce qui concerne l'unicité, vu que \( h\colon V\oplus W\to T\) est surjective, la relation \( h'=g\circ h\) prescrit les valeurs de \( g\) sur tous les éléments de \( T\).

		\spitem[Inversible]

		Ces deux applications \( g\) et \( g'\) vérifient \( h'=gg'h\) et \( h=g'gh\), et de plus \( h\colon V\oplus W\to T\) est surjective. Soient \( t\in T\) et \( x\in V\oplus W\) tel que \( t=h(x)\). Nous avons \( h(x)=g'gh(x)\). C'est-à-dire \( t=(g'\circ g)(t)\). De même dans l'autre sens, il existe \( x'\in V\oplus W\) tel que \( t=h'(x')\). En appliquant l'égalité \( h'=gg'h'\) à \( x'\), nous trouvons \( t=(g\circ g')(t)\).

		Tout cela pour dire que \( g'=g^{-1}\). Cette application \( g\) est donc un isomorphisme de produits tensoriels entre \( (T,h)\) et \( (T',h')\).
	\end{subproof}
	Au final, l'application \( g\colon T\to T'\) étant linéaire et inversible, elle est un isomorphisme d'espaces vectoriels.
\end{proof}

Tout cela est fort bien : nous avons unicité à isomorphisme près du produit tensoriel d'espaces vectoriels. Mais nous n'avons pas encore de certitudes à propos de l'existence d'un couple \( (T,h)\) vérifiant les propriétés demandées pour être un produit tensoriel.

Nous allons maintenant construire un produit tensoriel.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Construction abstraite}
%---------------------------------------------------------------------------------------------------------------------------

C'est le moment pour vous de relire la définition \ref{DEFooCPNIooNxsYMY} d'espace vectoriel librement engendré, et surtout le lemme \ref{LEMooLOPAooUNQVku} qui en donne une base.

\begin{definition}[\cite{ooWHNKooYVCiYc}]       \label{DEFooKTVDooSPzAhH}
	Soient deux espaces vectoriels \( V\) et \( W\) sur le corps commutatif\footnote{À part mention du contraire, tous les corps du Frido sont commutatifs.} \( \eK\). Dans \( F_{\eK}(V\times W)\)\footnote{Espace vectoriel librement engendré, voir la définition \ref{LEMooLOPAooUNQVku} et le lemme \ref{LEMooLOPAooUNQVku}.} nous considérons les sous-espaces suivants:
	\begin{subequations}
		\begin{align}
			A_1 & =\{ \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)}\tq v_1,v_2\in V,w\in W  \}                             \\
			A_2 & =\{ \delta_{(v,w_1)}+\delta_{(v,w_2)}-\delta_{(v,w_1+w_2)}\tq v\in V,w_1,w_2\in W  \} \label{SUBEQooSHBJooJLPVbK} \\
			A_3 & =\{ \lambda\delta_{v,w}-\delta_{(\lambda v, w)}\tq v\in V,w\in W,\lambda\in \eK \}                                \\
			A_4 & =\{ \lambda\delta_{v,w}-\delta_{(v,\lambda w)}\tq v\in V,w\in W,\lambda\in \eK \}.
		\end{align}
	\end{subequations}
	Nous considérons alors \( N=\Span(A_1,A_2,A_3,A_4)\) et le quotient
	\begin{equation}
		V\otimes_{\eK}W=F_{\eK}(V\times W)/N.
	\end{equation}
	Ce dernier espace vectoriel est le \defe{produit tensoriel}{produit tensoriel} de \( V\) par \( W\).
\end{definition}

\begin{remark}      \label{REMooSLEGooWEiutz}
	Quelque remarques.
	\begin{enumerate}
		\item
		      Les éléments de \( V\otimes W\) ne s'écrivent pas tous sous la forme \( v\otimes w\). Certains ont vraiment besoin d'être écrits avec des sommes. En cela, la situation de \( V\otimes W\) est réellement différente de celle de \( V\times W\). Dans ce dernier, tous les éléments sont des couples.
		\item
		      La classe de l'élément \( \delta_{(v,w)}\in F(V\times W)\) sera d'habitude noté \( v\otimes w\).
		\item
		      Pour insister sur la notion de classe, nous allons aussi noter \( [x]\) la classe de \( x\in F(V\times W)\).
		\item       \label{ITEMooPVWHooMkgQoT}
		      L'arithmétique dans \( V\otimes W\) est relativement simple. En ajoutant et soustrayant le même élément de \( A_3\) nous avons par exemple
		      \begin{equation}
			      (\lambda v)\otimes w=(\lambda v)\otimes w+\lambda (v\otimes w)-(\lambda v)\otimes w.
		      \end{equation}
		      Nous obtenons de cette façon
		      \begin{equation}
			      \lambda(v\otimes w)=(\lambda v)\otimes w=v\otimes (\lambda w),
		      \end{equation}
		      que nous noterons \( \lambda v\otimes w\) sans plus de précision.
	\end{enumerate}
\end{remark}

\begin{proposition}[\cite{ooWHNKooYVCiYc}]     \label{PROPooIWZDooRRZNCf}
	L'espace vectoriel \( V\times W\) muni de
	\begin{equation}
		\begin{aligned}
			h\colon V\oplus W & \to V\otimes W     \\
			(v,w)             & \mapsto v\otimes w
		\end{aligned}
	\end{equation}
	est un produit tensoriel entre \( V\) et \( W\).
\end{proposition}

\begin{proof}
	Nous devons prouver les conditions de la définition \ref{DEFooXKKQooAvWRNp}.

	\begin{subproof}
		\spitem[\( h\) est bilinéaire]

		Ce sont des calculs tels que faits dans la remarque \ref{REMooSLEGooWEiutz}\ref{ITEMooPVWHooMkgQoT} qui font le travail.

		\spitem[\(h \) est surjective]

		Un élément de \( V\otimes W\) est la classe d'un élément de \( F(V\times W)\), c'est-à-dire de la forme
		\begin{equation}
			\big[ \sum_{i\alpha}\delta_{(v_i,w_{\alpha})} \big]=\sum_{i\alpha}v_i\otimes w_{\alpha}.
		\end{equation}
		Cet élément est dans l'image de \( h\) comme le montre le calcul suivant\footnote{Faites bien la distinction entre \( \delta_{v,w}\), \( (v,w)\) et \( v\otimes w\). Sachez dans quel ensemble se trouvent chacun de ces trois objets.} :
		\begin{equation}
			h\big( \sum_{i\alpha}(v_i,w_{\alpha}) \big)=\sum_{i\alpha}h(v_i,w_{\alpha})=\sum_{i\alpha}v_i\otimes w_{\alpha}.
		\end{equation}

		\spitem[Propriété universelle]

		Soient un espace vectoriel \( U\) et une application linéaire \( f\colon V\oplus W\to U \). Nous devons trouver une application linéaire \( g\colon V\otimes W\to U\) telle que \( f=g\circ h\). Pour cela nous commençons par considérer l'application
		\begin{equation}
			\begin{aligned}
				g\colon F(V\times W) & \to U          \\
				\delta_{(v,w)}       & \mapsto f(v,w)
			\end{aligned}
		\end{equation}
		définie sur tout \( F(V\times W)\) par linéarité sans encombres parce que les \( \delta_{v,w}\) forment une base par le lemme \ref{LEMooLOPAooUNQVku}.

		Nous démontrons que \( g(N)=0\) pour avoir le droit de passer \( g\) aux classes et le considérer comme application partant de \( V\otimes W\) au lieu de \( F(V\times W)\). Prenons par exemple
		\begin{subequations}
			\begin{align}
				g\big( \delta_{(v_1,w)}+\delta_{(v_2,w)}-\delta_{(v_1+v_2,w)} \big) & =g( \delta_{(v_1,w)} )+g(\delta_{(v_2,w)})-g(\delta_{v_1+v_2,w}) \\
				                                                                    & =f(v_1,w)+f(v_2,w)-f(v_1+v_2,w)                                  \\
				                                                                    & =0
			\end{align}
		\end{subequations}
		par la bilinéarité de \( f\). Cela montre que \( g(A_1)=0\). Nous montrons de même que \( g(A_2)=g(A_3)=g(A_4)=0\), et enfin toujours par linéarité que \( g(N)=0\). Pour rappel, les éléments de \( N\) sont les combinaisons linéaires finies d'éléments de \( A_1\), \( A_2\), \( A_3\) et \( A_4\).

		Par passage aux classes, nous avons une application (que nous notons également \( g\))
		\begin{equation}
			g\colon F(V\times W)/N\to U
		\end{equation}
		vérifiant \( g(v\otimes w)=f(v,w)\). Mais comme \( h(v,w)=v\otimes w\), nous avons \( g\circ h\colon V\oplus W\to U\) vérifiant \( g\circ h=f\).
	\end{subproof}
	L'espace vectoriel \( V\otimes W\) est donc un produit tensoriel.
\end{proof}

\begin{normaltext}
	Vu que \( V\otimes W\) est un produit tensoriel de \( V\) et \( W\), et vu qu'il y a unicité par la proposition \ref{PROPooROPHooQXqNzZ}, nous avons bien le droit de dire que \( V\otimes W\) est \emph{le} produit tensoriel. Cela justifie le titre.
\end{normaltext}

\begin{normaltext}
	Les prochains lemmes et propositions vont nous dire que l'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon V^*\otimes W & \to \aL(V,W)                            \\
			\alpha\otimes w            & \mapsto \big( v\mapsto \alpha(v)w \big)
		\end{aligned}
	\end{equation}
	est un isomorphisme d'espaces vectoriels lorsque \( V\) est de dimension finie. Vu que nous aimons les énoncés très explicites, ça va être découpé en plusieurs morceaux, l'énoncé va devenir un peu long; mais c'est pour la bonne cause.
\end{normaltext}

\begin{lemma}       \label{LEMooOJEBooQruWEp}
	Soient deux espaces vectoriels \( V\) et \( W\) dont \( W\) est de dimension finie. Alors l'application définie par
	\begin{equation}
		\begin{aligned}
			\varphi\colon F(V^*\times W) & \to \aL(V,W)                            \\
			\delta_{(\alpha,w)}          & \mapsto \big( v\mapsto \alpha(v)w \big)
		\end{aligned}
	\end{equation}
	sur la base «canonique» de \( F(V^*\times W)\) passe aux classes.
\end{lemma}

\begin{proof}
	Avec les notations de la définition \ref{DEFooKTVDooSPzAhH} nous devons prouver que \( \varphi(N)=0\). Nous montrons que \( \varphi(A_4)=0\), et nous vous laissons faire les autres. Pour \( \lambda\in \eK\), \( \alpha\in V^*\) et \( w\in W\) en utilisant la linéarité de \( \varphi\) nous avons :
	\begin{subequations}
		\begin{align}
			\varphi\big( \lambda\delta_{(\alpha,w)}-\delta_{(\alpha,\lambda w)} \big)v & =\lambda\varphi(\delta_{(\alpha,w)})(v)-\varphi(\delta_{(\alpha,\lambda w)})(v) \\
			                                                                           & =\lambda\alpha(v)w-\alpha(v)(\lambda w)                                         \\
			                                                                           & =0
		\end{align}
	\end{subequations}
	parce que \( \alpha(v)(\lambda w)=\lambda \alpha(v)w\) du fait que \( \eK\) est commutatif. La commutativité de \( \eK\) est ce qui permet de permuter le produit \( \lambda \alpha(v)\).

	Nous laissons à la lectrice le soin de prouver que \( \varphi(A_1)=\varphi(A_2)=\varphi(A_3)=0\).
\end{proof}

\begin{lemma}       \label{LEMooUQZHooWjIGsy}
	Si \( W\) est de dimension finie, alors \( \aL(V,W)\) muni de
	\begin{equation}
		\begin{aligned}
			h\colon V^*\oplus W & \to \aL(V,W)                            \\
			(\alpha,w)          & \mapsto \big( v\mapsto \alpha(v)w \big)
		\end{aligned}
	\end{equation}
	est un produit tensoriel\footnote{Définition \ref{DEFooXKKQooAvWRNp}.} de \( V^*\) par \( W\).
\end{lemma}

\begin{proof}
	Nous devons prouver que
	\begin{itemize}
		\item \( h\) est bilinéaire,
		\item \( h\) est surjective
		\item pour tout espace vectoriel \( U\), et pour toute application bilinéaire \( f\colon V^*\oplus W\to U\), il existe une application linéaire \( g\colon \aL(V,W)\to U\) tel que \( f=g\circ h\).
	\end{itemize}

	\begin{subproof}
		\spitem[Bilinéaire]
		Le fait que \( h\) soit bilinéaire est une simple vérification.
		\spitem[Surjective]
		L'espace \( W\) étant de dimension finie, nous pouvons en considérer une base \( \{ z_i \}_{i\in I}\). Soit \( \alpha\in \aL(V,W)\). Si \( v\in V\), l'élément \( \alpha(v)\) peut être décomposé dans la base \( \{ z_i \}\), ce qui définit des applications linéaires \( \alpha_i\colon V\to \eK\) par
		\begin{equation}
			\alpha(v)=\sum_{i\in I}\alpha_i(v)z_i.
		\end{equation}
		Notons que \( \alpha_i\in V^*\). En comparant avec la définition de \( h\), nous voyons que
		\begin{equation}
			\alpha(v)=\sum_i h(\alpha_i,z_i)(v),
		\end{equation}
		c'est-à-dire \( \alpha=\sum_ih(\alpha_i,w_i)=h\big( \sum_i(\alpha_i,z_i) \big)\). Nous avons donc bien \( \alpha\in h(V^*\oplus W)\).
		\spitem[Propriété universelle]

		Soient un espace vectoriel \( U\) et une application bilinéaire \( f\colon V^*\oplus W\to U\). Pour \( \alpha\in\aL(V,W)\) nous définissons \( g(\alpha)\) comme suit. D'abord nous écrivons \( \alpha\) sous la forme
		\begin{equation}
			\alpha(v)=\sum_i\alpha_i(v)z_i,
		\end{equation}
		et nous posons
		\begin{equation}
			g(\alpha)=\sum_if(\alpha_i,z_i).
		\end{equation}
		Avec cette définition, en posant \( w=\sum_iw_iz_i\), nous avons
		\begin{subequations}
			\begin{align}
				(g\circ h)(\alpha,w) & =g\big( v\mapsto \alpha(v)w \big)            \\
				                     & =g\big( v\mapsto \sum_i\alpha(v)w_iz_i \big) \\
				                     & =\sum_if(w_i\alpha,z_i)                      \\
				                     & =\sum_if(\alpha,w_iz_i)                      \\
				                     & =f(\alpha,\sum_iw_iz_i)                      \\
				                     & =f(\alpha,w).
			\end{align}
		\end{subequations}
		Cela prouve que \( g\circ h=f\).
	\end{subproof}
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooKJTCooVTXWAQ}
	Soient deux espaces vectoriels \( V\) et \( W\) dont \( V\) est de dimension finie. Alors l'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon V^*\otimes W & \to \aL(V,W)                            \\
			\alpha\otimes w            & \mapsto \big( v\mapsto \alpha(v)w \big)
		\end{aligned}
	\end{equation}
	est bien définie\footnote{Au sens où il existe une fonction \( \varphi\) définie sur tout \( V^*\otimes W\) qui se réduit à cela pour les éléments de la forme \( \alpha\otimes w\).} et est un isomorphisme d'espaces vectoriels.
\end{proposition}

\begin{proof}
	Le lemme \ref{LEMooUQZHooWjIGsy} donne une structure de produit tensoriel de \( V^*\) par \( W\) sur \( \aL(V,W)\). Rappelons les structures :
	\begin{equation}
		\begin{aligned}
			h\colon V^*\oplus W & \to V^*\otimes W        \\
			(\alpha,w)          & \mapsto \alpha\otimes w
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\begin{aligned}
			h'\colon V^*\oplus W & \to \aL(V,W)                             \\
			(\alpha,w)           & \mapsto \big[ v\mapsto \alpha(v)w \big].
		\end{aligned}
	\end{equation}

	La proposition \ref{PROPooROPHooQXqNzZ} a déjà fait tout le boulot. La seule chose à faire est de vérifier qu'il existe une application \( \varphi\colon V^*\otimes W\to \aL(V,W)\) vérifiant simultanément les deux conditions suivantes :
	\begin{enumerate}
		\item       \label{ITEMooVNNSooNIXRoG}
		      \( \varphi(\alpha\otimes w)=\big[ v\mapsto \alpha(v)w \big]\)
		\item
		      \( h'=\varphi\circ h\).
	\end{enumerate}
	La seconde condition assure que \( \varphi\) sera un isomorphisme d'espaces vectoriels.

	L'existence de \( \varphi\) vérifiant la condition \ref{ITEMooVNNSooNIXRoG} est un effet du lemme \ref{LEMooOJEBooQruWEp} qui donne une fonction sur \( F(V^*\times W)\) dont le \( \varphi\) qui nous concerne est un quotient. Il reste à voir que cette application vérifie \( h'=\varphi\circ h\).

	En nous rappelant que \( \alpha\otimes w=[\delta_{(\alpha,w)}]\) et en écrivant \( \varphi\) à la fois l'application et son passage au quotient,
	\begin{equation}
		(\varphi\circ h)(\alpha,w)=\varphi(\alpha\otimes w)=\varphi\big( [\delta_{(\alpha,w)}] \big)=\varphi(\delta_{(\alpha,w)}).
	\end{equation}
	En appliquant à \( v\in V\) nous avons:
	\begin{equation}
		(\varphi\circ h)(\alpha,w)v=\varphi(\delta_{(\alpha,w)})v=\alpha(v)w=h'(\alpha,w)v.
	\end{equation}
	Et voilà. Nous avons \( \varphi\circ h=h'\).
\end{proof}

Une conséquence de la proposition \ref{PROPooKJTCooVTXWAQ} est que
\begin{equation}
	\dim(V\otimes W)=\dim(V)\dim(W)
\end{equation}
via le lemme \ref{LEMooJXFIooKDzRWR}\ref{ITEMooPMLWooNbTyJI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Bases}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooXFIMooDkTSrq}
	Si \( \tau\colon V_1\to V_2\) est un isomorphisme d'espaces vectoriels, alors il existe un isomorphisme d'espaces vectoriels \( \varphi\colon V_1\otimes W\to V_2\otimes W\) tel que \( \varphi(v\otimes w)=\tau(v)\otimes w \)\footnote{La proposition \ref{PROPooTHDPooWgjUwk} dira que cette condition fixe complètement \( \varphi\), mais c'est une autre histoire qui vous sera contée une autre fois.}.
\end{lemma}

\begin{proof}
	L'application
	\begin{equation}
		\begin{aligned}
			\varphi_0\colon F(V_1\times W) & \to F(V_2\times W)                     \\
			\delta_{(v,w)}                 & \mapsto \delta_{\big( \tau(v),w \big)}
		\end{aligned}
	\end{equation}
	est un isomorphisme.

	Cette application passe aux classes, mais pas au sens où \( x\in [y]\) impliquerait \( \varphi_0(x)=\varphi_0(y)\); au sens où si \( x\in [y]\), alors \( \varphi_0(x)\in[\varphi_0(y)]\). Par exemple
	\begin{equation}
		\varphi_0\big( \lambda\delta_{(v,w)}-\delta_{(v,\lambda w)} \big)=\lambda\delta_{\big( \tau(v),w \big)}-\delta_{\big( \tau(v),\lambda w \big)}\in [0].
	\end{equation}
	Nous vous laissons le soin de vérifier les égalités correspondantes pour les autres parties de \( N\).

	Le passage au classes de \( \varphi_0\) signifie que l'on considère l'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon V_1\otimes W & \to V_2\otimes W       \\
			[x]                        & \mapsto [\varphi_0(x)]
		\end{aligned}
	\end{equation}
	où vous aurez noté que la prise de classe à gauche n'est pas la même que celle à droite.

	Il faut prouver que ce \( \varphi\) est un isomorphisme. En ce qui concerne la linéarité,
	\begin{subequations}
		\begin{align}
			\varphi\big( [x]+[y] \big) & =\varphi\big( [x+y] \big)      \\
			                           & =[\varphi_0(x+y)]              \\
			                           & =[\varphi_0(x)+\varphi_0(y)]   \\
			                           & =[\varphi_0(x)]+[\varphi_0(y)] \\
			                           & =\varphi([x])+\varphi([y]).
		\end{align}
	\end{subequations}
	Je vous laisse le reste de la linéarité. Et en ce qui concerne le fait que ce soit une bijection, allez-y.
\end{proof}

\begin{proposition}[\cite{ooNHIGooYlXxMf}]      \label{PROPooTHDPooWgjUwk}
	Soient des espaces vectoriels de dimension finie \( V\) et \( W\). Soient une base \( \{e_i\}\) de \( V\) et une base \( \{f_{\alpha}\}\) de \( W\).

	Alors :
	\begin{enumerate}
		\item       \label{ITEMooQCILooUncdGl}
		      La partie \( \{e_i\otimes f_{\alpha}\}\) est une base de \( V\otimes W\).
		\item
		      Au niveau des dimensions, \( \dim(V\otimes W)=\dim(V)\dim(W)\).
	\end{enumerate}
\end{proposition}

\begin{proof}
	Vu que \( V\) est de dimension finie, nous avons un isomorphisme d'espaces vectoriels \( V^*=V\), et même un isomorphisme d'espaces vectoriels
	\begin{equation}
		\begin{aligned}
			\tau\colon V & \to (V^*)^* \\
			\tau(v)\mu   & =\mu(v).
		\end{aligned}
	\end{equation}
	Recopions l'isomorphisme de la proposition \ref{PROPooKJTCooVTXWAQ} en utilisant \( V^*\) au lieu de \( V\) :
	\begin{equation}
		\begin{aligned}
			\psi_0\colon (V^*)^*\otimes W & \to \aL(V^*,W)                                         \\
			\tau(v)\otimes w              & \mapsto \big( \mu\mapsto \tau(v)(\mu)w =\mu(v)w \big).
		\end{aligned}
	\end{equation}
	En écrivant cela, nous avons tenu compte du fait que tout élément de \( (V^*)^*\) peut être écrit de façon univoque sous la forme \( \tau(v)\) pour un certain \( v\in V\).

	Vu que \( \tau\) est un isomorphisme, l'application suivante est encore un isomorphisme\footnote{Lemme \ref{LEMooXFIMooDkTSrq}.} :
	\begin{equation}        \label{EQooAEFRooPfmAnj}
		\begin{aligned}
			\psi\colon V\otimes W & \to \aL(V^*,W)                          \\
			v\otimes w            & \mapsto \big( \mu\mapsto \mu(v)w \big).
		\end{aligned}
	\end{equation}
	Nous avançons. Vu que nous avons un isomorphisme, nous pouvons faire passer des bases. Le lemme \ref{LEMooJXFIooKDzRWR} nous donne une base de \( \aL(V^*,W)\) en les éléments \( \beta_{i\alpha}\colon V^*\to W\) définies par
	\begin{equation}
		\beta_{ij}(\mu)=\mu(e_i)f_{\alpha}.
	\end{equation}
	Donc \( \{ \psi^{-1}(\beta_{i\alpha}) \}\) est une base de \( V\otimes W\).

	Pour \( a=\sum_ia_ie_i^*\) (base duale, définition \ref{DEFooTMSEooZFtsqa}) nous avons :
	\begin{equation}
		\psi(e_i\otimes f_{\alpha})a=a(e_i)f_{\alpha}=\beta_{i\alpha}(a).
	\end{equation}
	Cela prouve que \( \psi^{-1}(\beta_{i\alpha})=e_i\otimes f_{\alpha}\), et donc que ces \( e_i\otimes f_{\alpha}\) est une base de \( V\otimes W\).

	La formule concernant les dimensions est simplement la définition \ref{DEFooWRLKooArTpgh} de la dimension : le nombre d'éléments dans une base.
\end{proof}

\begin{lemma}       \label{LEMooYJIQooRCkHMq}
	Dans le produit tensoriel \( \eR\otimes \eR\), nous avons
	\begin{enumerate}
		\item
		      \( x\otimes 1=1\otimes x=x(1\otimes 1)\) pour tout \( x\in \eR\).
		\item
		      Si \( x\geq 0\) nous avons aussi \( x\otimes 1=\sqrt{ x }\otimes \sqrt{ x }\).
	\end{enumerate}
\end{lemma}

\begin{lemma}		\label{LEMooKPWNooNjkAru}
	Le produit tensoriel est associatif.

	Plus précisément, si \( U,V,W\) sont des espaces vectoriels, nous avons des isomorphismes d'espaces vectoriels
	\begin{equation}
		U\otimes(V\otimes W)=(U\otimes V)\otimes W.
	\end{equation}
	Dans la suite, nous écrirons \( U\otimes V\otimes W\) sans plus de précisions et même \( \otimes^kV\) pour le produit de \( k\) copies de \( V\).
\end{lemma}

\begin{definition}
	Un \( k\)-\defe{tenseur}{tenseur} sur \( V\) est un élément de \( \otimes^kV^*\).
\end{definition}

\begin{proposition}		\label{PROPooIODGooYajpiy}
	Il y a un isomorphisme d'espaces vectoriels \( \otimes^kV^*\to \aL_k(V,\eR)\).
\end{proposition}

\begin{example}
	Pour \( k=2\), l'isomorphisme de la proposition \ref{PROPooIODGooYajpiy} est
	\begin{equation}
		\begin{aligned}
			\psi\colon \aL(V,V^*) & \to \aL_2(V,\eR) \\
			\psi(\alpha)(u,v)     & =\alpha(u)v.
		\end{aligned}
	\end{equation}
	Il faut se rappeler que le proposition \ref{PROPooKJTCooVTXWAQ} donne déjà un isomorphisme entre \( V^*\otimes V^*\) et \( \aL(V,V^*)\).
\end{example}


%-----------------------------------
\subsection{Construction par les formes multilinéaires}
%-----------------------------------

\begin{lemma}		\label{LEMooCXPRooGfJMMV}
	Soient des espaces vectoriels de dimension finie \( V\), \( W\) et \( Z\). Soient une application bilinéaire \(f \colon V\times W\to Z  \) ainsi que des bases \( \{ e_i \}\) de \( V\) et \( \{ d_j \}\) de \( W\).

	Il existe des éléments \( f_{ij}\) dans \( Z\) tels que
	\begin{equation}
		f(v,w)=\sum_{ij}v_iw_jf_{ij}
	\end{equation}
	pour tout \( v\in V\) et \( w\in W\).
\end{lemma}


\begin{normaltext}
	Ce que nous allons noter \( h(T,S)\) est souvent noté \( T\otimes S\), parce que la proposition \ref{PROPooIFVBooGiMskq} montrera qu'il s'agit bien d'un produit tensoriel. Cependant nous avons besoin de pas mal de propriétés de \( h\) avant de pouvoir l'affirmer.
\end{normaltext}

\begin{propositionDef}		\label{DEFooUUMYooOUCzWk}
	Si \( T\in\mL_k(V,\eR)\) et \( S\in\mL_l(V,\eR)\), nous définissons
	\begin{equation}
		\begin{aligned}
			h(T,S)\colon V^{k+l} & \to \eR                                             \\
			(v_1,\ldots,v_{k+l}) & \mapsto T(v_1,\ldots,v_k)S(v_{k+1},\ldots,v_{k+l}).
		\end{aligned}
	\end{equation}
	\begin{enumerate}
		\item
		      L'application \( h(T,S)\) est multilinéaire\footnote{Application multilinéaire, définition \ref{DefFRHooKnPCT}.}: \( h(T,S)\in\mL_{k+l}(V,\eR)\).
		\item
		      L'application \( h\) est associative : \( h\big( h(T,S),R \big)=h\big( T,h(S,R) \big)\).
		\item
		      L'application \(h \colon \mL_k(V,\eR)\times \mL_l(V,\eR)\to \mL_{k+l}(V,\eR)  \) est bilinéaire.
	\end{enumerate}
\end{propositionDef}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[multilinéaire]
		%-----------------------------------------------------------
		Nous fixons des \( v_i\in V\), nous considérons \( h(T,S)_i\) et nous calculons (en supposant \( i\leq k\), sinon on adapte)
		\begin{subequations}
			\begin{align}
				h(T,S)_i(v)a=h(T,S)(v_1,\ldots,v,\ldots,v_{k+l})         \\
				 & = T(v_1,\ldots,v,\ldots,v_k)S(v_{k+1},\ldots,v_{k+l}) \\
				 & =T_i(v)S(\ldots).
			\end{align}
		\end{subequations}
		Vu que \( T_i\) est linéaire, nous en déduisons que \( h(T,S)_i\) est linéaire et donc que \( h(T,S)\) est multilinéaire.
		\item[Associativité]
		Facile de vérifier que
		\begin{equation}
			h\big( h(T,S),R \big)(v_1,\ldots,v_{k+l+m})=T(v_1,\ldots,v_k)S(v_{k+1},\ldots,v_{k+l})R(v_{k+l+1},\ldots,v_{k+l+m}).
		\end{equation}
		\item[Bilinéaire]
		En utilisant directement la définition, on vérifie que \( h(T,\lambda S)=\lambda h(T,S)\) et \( h(T,S_1+S_2)=h(T,S_1)+h(T,S_2)\).
	\end{subproof}
\end{proof}

\begin{normaltext}[Quelque notations]
	Si \( V_i\) sont des espaces vectoriels, si \( v_i\in V_i\), et si \( \xi_i\in V_i^*\), nous notons
	\begin{equation}
		h(\xi_1,\ldots,\xi_n)(v_1,\ldots,v_n)=\prod_{i=1}^n\xi_i(v_i).
	\end{equation}

	Si \( I\) est un multiindice (\( I=(i_1,\ldots,i_n)\)), et si \( v\) est quelque chose qui peut avoir des indices, alors nous écrivons \( v_I=(v_{i_1},\ldots, v_{i_n})\).

	Nous écrivons en particulier \( \delta_{IJ}=\prod_{k=1}^n\delta_{i_kj_k}\).
\end{normaltext}


\begin{lemma}[égalité sur une base\cite{MonCerveau}]	\label{LEMooPUNAooUnQTpJ}
	Soient des espaces vectoriels \( (V_i)_{i=1,\ldots,N}\) de dimensions \( n_i\). Soient deux formes \( N\)-linéaires \(S,T \colon V_1\times \ldots \times V_N \to \eR  \). Si pour tout multiindice \( I\) nous avons \( S(e_I)=T(e_I)\), alors \( S=T\).

	Si les \( \xi_i\) sont des formes linéaires, nous notons \( \xi_I\) la forme multilinéaire \( h(\xi_1,\ldots,\xi_n)\).
\end{lemma}

\begin{proof}
	Nous considérons les bases \( \{ e^{(k)}_i \}_{i=1,\ldots,n_i}\) de \( V_k\). Un élément générique de \( V_1\times \ldots V_N\) est de la forme
	\begin{equation}
		\Big(  \sum_{j_1}^{n_1}x_{j_1}^{(1)}e^{(1)}_{j_1},\ldots, \sum_{j_N=1}^{n_N}x_{j_N}^{(N)}e_{j_N}^{(N)} \Big).
	\end{equation}
	Nous appliquons \( S\) à cela en tenant compte de la multilinéarité :
	\begin{subequations}
		\begin{align}
			S\Big(  \sum_{j_1}^{n_1}x_{j_1}^{(1)}e^{(1)}_{j_1},\ldots, \sum_{j_N=1}^{n_N}x_{j_N}^{(N)}e_{j_N}^{(N)} \Big) & =\sum_{j_1=1}^{n_1}\ldots \sum_{j_N=1}^{n_N}\prod_{i=1}^Nx_{j_i}^{(i)}S(e^{(1)}_{j_1},\ldots,e_{j_N}^{(N)}) \\
			                                                                                                              & =\sum_J\prod_{i=1}^Nx_{j_i}^{(i)}S(e_J).
		\end{align}
	\end{subequations}
	Le même calcul avec \( T\) donne la même chose, compte tenu de l'hypothèse \( S(e_J)=T(e_J)\).
\end{proof}

\begin{theorem}[\cite{BIBooDEEYooRGFyDD}]		\label{THOooTAGKooDscwFG}
	Soient des espaces vectoriels \( (V_i)_{i=1,\ldots,N}\) de dimensions \( n_i\). Nous notons, pour chaque \( i\) une base \( \{ e_j^{(i)}\}_{j=1,\ldots,n_i} \} \) de \( V_i\). Nous notons \( \{ \alpha_j^{(i)} \}\) les bases duales.

	Alors \( \{ \alpha_I \}_I\) est une base de \( \aL_N(V_1\times \ldots V_N),\eR\).
\end{theorem}

\begin{proof}
	Nous commençons par prouver que \( \{ \alpha_I \}\) est libre. Supposons que \( \sum_Ia_I\alpha_I=0\). En l'appliquant à \( e_J\), nous avons
	\begin{equation}
		0=\sum_Ia_I\alpha_I(e_J)=\sum_{I}a_I\delta_{IJ}=a_J.
	\end{equation}
	Cela prouve que tous les \( a_I\) sont nuls, et donc que la partie est libre.

	Pour prouver que la partie est génératrice nous considérons \( T\in \aL_N(V_1\times\ldots\times V_N,\eR)\) et nous posons \( T_I=T(e_I)\) pour tout multiindice \( I\). En posant \( S=\sum_IT_I\alpha_I\), nous
	\begin{equation}
		S(e_J)=\sum_{I}T_I\alpha_I(e_J)=\sum_IT_I\delta_{IJ}=T_J.
	\end{equation}
	Donc \( S(e_J)=T(e_J)\). Le lemme \ref{LEMooPUNAooUnQTpJ} conclu.
\end{proof}

\begin{lemma}		\label{LEMooPBLRooJePqlk}
	Soient deux espaces vectoriels de dimension finie \( V\) et \( W\). Nous notons \( \{ \alpha_i \}\) une base de \( V^*\) et \( \{ \beta_j \}\) une base de \( W^*\). Pour chaque \( \xi\in V^*\) et \( \sigma\in W^*\) nous considérons l'application bilinéaire
	\begin{equation}
		\begin{aligned}
			h(\xi,\sigma)\colon V\times W & \to \eR                  \\
			(v,w)                         & \mapsto \xi(v)\sigma(w).
		\end{aligned}
	\end{equation}
	La partie \( \{ h(\alpha_i,\beta_j) \}\) forment une base de l'espace des applications bilinéaires sur \( V\times W\).
\end{lemma}
%TODOooEJKIooSFisRM En principe, un résultat plus général pour les application multininéaires est prouvé quelque part. Il faut la trouver.

\begin{proposition}[\cite{MonCerveau,BIBooJKAIooIoDPjK}]		\label{PROPooIFVBooGiMskq}
	Nous considérons l'application \(h \colon V^*\times W^*\to \aL_2(V\times W,\eR)  \) donnée par
	\begin{equation}
		h(\xi,\sigma)(v,w)=\xi(v)\sigma(w).
	\end{equation}
	Le couple \( \big( \aL_2(V\times W,\eR),h \big)\) est un produit tensoriel de \( V^*\) et \( W^*\).
\end{proposition}

\begin{proof}
	Nous devons prouver que l'application \( h\) satisfait à la propriété universelle \ref{DEFooXKKQooAvWRNp}\ref{ITEMooJCNYooGvjjtL}. Nous considérons une base \( \{ \alpha_i \}\) de \( V^*\) et \( \{ \beta_j \}\) de \( W^*\). Si \(f \colon V^*\times W^*\to Z  \) est bilinéaire, le lemme \ref{LEMooCXPRooGfJMMV} nous dit que \( f\) est déterminée par les éléments \(f_{ij}= f(\alpha_i,\beta_)\).

	\begin{subproof}
		\spitem[Existence]
		%-----------------------------------------------------------
		Nous posons
		\begin{equation}
			\begin{aligned}
				\tilde f\colon \aL_2(V\times W,\eR) & \to Z                          \\
				\sum_{ij}a_{ij}h(\alpha_i,\beta_j)  & \mapsto \sum_{ij}a_{ij}f_{ij}.
			\end{aligned}
		\end{equation}
		Notez deux choses.
		\begin{enumerate}
			\item
			      C'est une bonne définition parce que les \( h(\alpha_i,\beta_j)\) forment une base de \( \aL_2(V\times W,\eR)\) (lemme \ref{LEMooPBLRooJePqlk}).
			\item
			      L'application \( \tilde  f\) est linéaire.
		\end{enumerate}

		Nous devons prouver que \( \tilde f\circ h=f\). Soient \( \xi=\sum_i\xi_i\alpha_i\) et \( \sigma=\sum_j\sigma_j\beta_j\). Par bilinéarité nous avons
		\begin{equation}
			h(\xi,\sigma)=\sum_{ij}\xi_i\sigma_jh(\alpha_i,\beta_j).
		\end{equation}
		Nous avons donc
		\begin{equation}
			(\tilde f\circ h)(\xi,\sigma)=\sum_{ij}\xi_i\sigma_jf_{ij}=\sum_{ij}\xi_i\sigma_j(\alpha_i,\beta_j)=f(\xi,\sigma),
		\end{equation}
		et donc bien \( \tilde f\circ h=f\).
		\spitem[Unicité]
		%-----------------------------------------------------------
		Nous savons que les éléments \( h(\alpha_i,\beta_j)\) forment une base de \( \aL_2(V\times W,\eR)\). Si \(\tilde f \colon \aL_2(V\times W,\eR)\to Z  \) est une application linéaire vérifiant \( \tilde f\circ h=f\), alors nous avons
		\begin{equation}
			\tilde f\big( h(\alpha_i,\beta_j) \big)=(\tilde f\circ h)(\alpha_i,\beta_j)=f(\alpha_i,\beta_j)=f_{ij}.
		\end{equation}
		Donc l'application \( \tilde f \) est entièrement déterminée sur une base par les éléments \( f_{ij}\in Z\).
	\end{subproof}
\end{proof}

\begin{normaltext}
	Lorsque \( V\) et \( W\) sont des espaces vectoriels, si rien n'est précisé, le produit tensoriel
	\begin{equation}
		V\otimes W
	\end{equation}
	sera celui de la proposition \ref{PROPooIFVBooGiMskq}, et lorsque \( \xi_1\in V^*\) et \( \xi_2\in W^*\) nous notons
	\begin{equation}
		\xi_1\otimes \xi_2
	\end{equation}
	l'élément \( h(\xi_1,\xi_2)\).

	Ce serait mieux de garder la notation \( V\otimes W\) pour un produit tensoriel abstrait (définition \ref{DEFooXKKQooAvWRNp}), mais bon \ldots Nous n'allons pas non plus nous tuer sur les notations.
\end{normaltext}

%-------------------------------------------------------
\subsection{Décomposabilité}
%----------------------------------------------------

\begin{definition}[\cite{BIBooDEEYooRGFyDD}]		\label{DEFooTSSVooUGybzL}
	Une application \( T\in\aL_k(V,\eR)\) est \defe{décomposable}{application multilinéaire!décomposable} si il existe des formes \( \xi_1,\ldots,\xi_k\in V^*\) telles que
	\begin{equation}
		T(v_1,\ldots,v_k)=\prod_{i=1}^k\xi_i(v_i)
	\end{equation}
	autrement dit \( T=h(\xi_1,\ldots,\xi_k)\), ou encore \( T=\xi_1\otimes \ldots \otimes \xi_k\).
\end{definition}

\begin{lemma}[\cite{BIBooDEEYooRGFyDD, MonCerveau}]		\label{LEMooLEIBooROVcim}
	Une forme \( 2\)-multilinéaire \( T=\sum_{ij}a_{ij}h(\alpha_i,\alpha_j)\) est décomposable\footnote{Définition \ref{DEFooTSSVooUGybzL}.} si et seulement si la matrice \( a\) est de rang \( 1\).
\end{lemma}

\begin{proof}
	En deux parties.
	\begin{subproof}
		\spitem[\( \Rightarrow\)]
		%-----------------------------------------------------------
		Soient \( f_1, f_2\in V^*\) tels que \( T(v_1,v_2)=f_1(v_1)f_2(v_2)\) pour tout \( v_1,v_2\in V\). Nous appliquons \( T\) à \( (e_1), e_2\) :
		\begin{equation}
			T(e_k, e_l)=\sum_{ij}a_{ij}(\alpha_i\otimes \alpha_j)(e_k,e_l)=\sum_{ij}a_{ij}\alpha_i(e_i)\alpha_j(e_l)=a_{kl},
		\end{equation}
		mais aussi
		\begin{equation}
			T(e_k,e_l)=f_1(e_k)f_2(e_l).
		\end{equation}
		Donc \( a_{kl}=f_1(e_k)f_2(e_l)\). Notons \( A\) la matrice (et l'application linéaire) formée par ces nombres. Nous avons :
		\begin{equation}
			A(e_i)=\sum_ka_{ki}e_k=\sum_kf_1(e_k)f_2(e_i)e_k=f_2(e_i)\sum_kf_1(e_k)e_k.
		\end{equation}
		En notons \( v\) le vecteur \( \sum_kf_1(e_k)e_k\), nous avons
		\begin{equation}
			A(\eR^n)=\Span(v).
		\end{equation}
		Cela montre que la dimension de l'image de \( A\) est \( 1\), et donc que \( A\) est de rang \( 1\).

		\spitem[\( \Leftarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( a\) est de rang \( 1\), et nous posons
		\begin{equation}
			\begin{aligned}
				T\colon V^2 & \to \eR                        \\
				(v,w)       & \mapsto \sum_{kl}a_{kl}v_kw_l.
			\end{aligned}
		\end{equation}
		Vu que \( \rank(a)=1\), il existe \( u\in V\) tel que \( a(\eR^n)=\Span(u)\). En particulier pour chaque \( i\), il existe \( \lambda_i\) tel que \( a(e_i)=\lambda_i u\). Donc
		\begin{equation}
			a(v)=\sum_iv_ia(e_i)=\sum_iv_i\lambda_iu=(v\cdot \lambda)u,
		\end{equation}
		et
		\begin{equation}
			a_{kl}=a(e_k)_l=\lambda_ku_l.
		\end{equation}
		Avec tout ça,
		\begin{equation}
			T(v,w)=\sum_{kl}a_{kl}v_kw_l=(\lambda\cdot v)(u\cdot w).
		\end{equation}
		Donc nous avons \( T=f_1\otimes f_2\) avec
		\begin{equation}
			\begin{aligned}
				f_1\colon \eR^n & \to \eR                \\
				x               & \mapsto \lambda\cdot x
			\end{aligned}
		\end{equation}
		et \( f_2(x)=u\cdot x\).
	\end{subproof}
\end{proof}

\begin{normaltext}
	Il est dit dans \cite{BIBooDEEYooRGFyDD} que l'étude de la décomposabilité devient très compliquée à partir des \( 3\)-tenseurs, et que c'est lié à l'intrication quantique. Personnellement je ne suis pas sûr de ce que ça veut dire, mais si ça vous parle, c'est sans doute intéressant.
\end{normaltext}

%-------------------------------------------------------
\subsection{Contraction}
%----------------------------------------------------

\begin{definition}		\label{DEFooMQDHooBsyVpr}
	Un \( (l,k)\)-tenseur sur \( V\) est une application \( k+l\)-multilinéaire sur \( (V^*)^l\times V^k\).
\end{definition}

\begin{propositionDef}[\cite{MonCerveau}]
	Soit un \( (l,k)\)-tenseur\footnote{Définition \ref{DEFooMQDHooBsyVpr}.} \( T\) sur \( V\). Soient une base \( \{ e_i \}\), et sa base duale \( \{ \alpha_i \}\). Nous considérons une bijection linéaire \(A \colon V\to V  \), la base \( \{ Ae_i \}\) et la duale \( \{ \beta_i \}\).

	Nous considérons l'application d'insertion \(\iota \colon V^n\to V^{n+1}  \) donnée par
	\begin{equation}
		\iota_k(v)(v_1,\ldots,v_n)=(v_1,\ldots,v_{k-1},v,v_k,\ldots,v_n).
	\end{equation}

	Alors :
	\begin{enumerate}
		\item		\label{ITEMooSROIooGtTXCL}
		      Pour tout \( r\leq l\), pour tout \( s\leq k\), pour tout \( \xi_i\in V^*\) et \( v_j\in V\) nous avons
		      \begin{equation}
			      \sum_i T(   \iota_r(\alpha_i)(\xi_1,\ldots,\xi_{l-1}), \iota_s(e_i)(v_1,\ldots,v_{k-1})   )=
			      \sum_i T(   \iota_r(\beta_i)(\xi_1,\ldots,\xi_{l-1}), \iota_s(Ae_i)(v_1,\ldots,v_{k-1})   ).
		      \end{equation}
		\item		\label{ITEMooPWLBooOYrluK}
		      En posant
		      \begin{equation}
			      \begin{aligned}
				      C_s^r\colon (V^*)^{l-1}\times V^{k-1}       & \to \eR \\
				      (\xi_1,\ldots,\xi_{l-1},v_1,\ldots,v_{k-1}) & \mapsto
				      \sum_i T(   \iota_r(\alpha_i)(\xi_1,\ldots,\xi_{l-1}), \iota_s(e_i)(v_1,\ldots,v_{k-1})   ),
			      \end{aligned}
		      \end{equation}
		      l'application \( C_s^r(T)\) est un \( (l-1, k-1)\)-tenseur.
	\end{enumerate}
	Ce \( C_s^r(T)\) est la \( (r,s)\)-\defe{contraction}{contraction} de \( T\).
\end{propositionDef}

\begin{proof}
	Le point \ref{ITEMooPWLBooOYrluK} est facile parce que c'est juste la multilinéarité. Nous faisons le point \ref{ITEMooPWLBooOYrluK}.
	Nous partons du lemme \ref{LEMooSVRIooFbxfue} qui dit que \( \beta_i=\sum_jA^{-1}_{ij}\alpha_j\). Remarquez que
	\begin{equation}
		T\big( \iota_r(\sigma)(\xi_1,\ldots,\xi_{l-1}), \ldots \big)
	\end{equation}
	est linéaire en \( \sigma\). Cela nous permet de sortir plein de sommes de \( T\) dans le calcul suivant :
	\begin{subequations}
		\begin{align}
			 & \qquad\sum_iT\Big(  \iota_r(\beta_i)(\xi_1,\ldots,\xi_{l-1}),\iota_s(Ae_i)(v_1,\ldots,v_{k-1})  \Big)                       \\
			 & =\sum_iT\Big( \iota_r(\sum_jA^{-1}_{ij}\alpha_j)(\xi_1,\ldots,\xi_{l-1}),\iota_s(\sum_lA_{li}e_l)(v_1,\ldots,v_{k-1}) \Big) \\
			 & =\sum_{ijl}A^{-1}_{ij}A_{li}T\big( \iota_r(\alpha_j)(\ldots), \iota_s(e_l)(\ldots) \big)                                    \\
			 & =\sum_{i}T\big( \iota_r(\alpha_i)(\ldots), \iota_s(e_l)(\ldots) \big)
		\end{align}
	\end{subequations}
\end{proof}

\begin{proposition}[\cite{BIBooDEEYooRGFyDD}]
	Soient \( v,w\in V\) ainsi que \( \beta,\gamma,\sigma\in V^*\). Nous avons
	\begin{equation}
		C_2^1(v\otimes w\otimes w\otimes \beta\otimes \gamma\otimes \sigma)=\gamma(v)w\otimes\beta\otimes\sigma.
	\end{equation}
\end{proposition}

\begin{proof}
	Nous choisissons une base \( \{ e_i \}\) de \( V\) et sa base duale \( \{ \alpha_i \}\). Ensuite allons appliquer \( C_2^1(v\otimes w\otimes \beta\otimes \gamma\otimes \sigma)\) à \( (\xi,v_1,v_2)\) avec \( \xi\in V^*\), \( v_1,v_2\in V\). Remarquons d'abord que si nous adoptons la décomposition \( v=\sum_kv_ke_k\) et \( \gamma=\sum_l\gamma_l\alpha_l\), nous avons \( \alpha_i(v)=v_i\) et \( \gamma(e_i)=\gamma_i\). Voici le calcul :
	\begin{subequations}
		\begin{align}
			C_1^1(v\otimes w\otimes \beta\otimes \gamma\otimes \sigma)(\xi,v_1,v_2) & = \sum_i(v\otimes w\otimes \beta\otimes \gamma\otimes \sigma)(\alpha_i,\xi,v_1,e_i,v_2) \\
			                                                                        & =\sum_i \alpha_i(v)\xi(w)\beta(v_1)\gamma(e_i)\sigma(v_2)                               \\
			                                                                        & =\sum_iv_i\gamma_i\xi(w)\beta(v_1)\sigma(v_2)                                           \\
			                                                                        & =\gamma(v)(w\times \beta\otimes \sigma)(\xi,v_1,v_2).
		\end{align}
	\end{subequations}
	Pour la dernière ligne, \( \sum_iv_i\gamma_i=\gamma(v)\) parce que
	\begin{equation}
		\gamma(v)=\sum_{kl}\gamma_l\alpha_l(v_ke_k)=\sum_{kl}\gamma_lv_k\delta_{lk}=\sum_k\gamma_kv_k.
	\end{equation}
\end{proof}


%-------------------------------------------------------
\subsection{Tenseurs symétriques et alternés}
%----------------------------------------------------

Si \( T\) est une application \( k\)-multilinéaire sur \( V\), et si \(\sigma \colon \{ 1,\ldots,k \}\to \{ 1,\ldots,k \}  \) est une permuation, nous notons \( T^{\sigma} \) l'application \( k\)-multilinéaire donnée par
\begin{equation}
	\begin{aligned}
		T^{\sigma}\colon V^k & \to \eR                                         \\
		(v_1,\ldots,v_k)     & \mapsto T(v_{\sigma(1)},\ldots, v_{\sigma(k)}).
	\end{aligned}
\end{equation}

\begin{definition}[\cite{BIBooDEEYooRGFyDD}]
	Un \( k\)-tenseur\footnote{C'est à dire une application \( k\)-multilinéaire \(T \colon V^k\to \eR  \).} \( T\) sur \( V\) est \defe{symétrique}{tenseur symétrique} si pour toute permutation \(\sigma \colon \{ 1,\ldots,k \}\to \{ 1,\ldots,k \}  \), nous avons
	\begin{equation}
		T(v_1,\ldots,v_k)=T(v_{\sigma(1)},\ldots,v_{\sigma(k)}),
	\end{equation}
	c'est à dire \( T^{\sigma}=T\).
\end{definition}

\begin{definition}[\cite{BIBooDEEYooRGFyDD}]
	Un \( k\)-tenseur \( T\) est \defe{alterné}{tenseur alterné} si \( T^{\sigma}=(-1)^{\sigma}T\). L'espace vectoriel des \( k\)-tenseurs alternés est noté \( \Lambda^kV^*\). Par soucis de cohérence, nous notons aussi \( \Lambda^0V^*=\eR\).
\end{definition}

\begin{lemma}		\label{LEMooJEZYooMmrtgu}
	Si \( T\) est un \( k\)-tenseur, et si \( \sigma\) et \( \pi\) sont des permutations, alors
	\begin{enumerate}
		\item
		      L'application \( T\mapsto T^{\sigma}\) est linéaire en \( T\).
		\item
		      \( (T^{\pi})^{\sigma}=T^{\pi\circ \sigma}\).
		\item
		      \( (-1)^{\pi}(-1)^{\sigma}=(-1)^{\pi\circ \sigma}\).
	\end{enumerate}
\end{lemma}


\begin{lemma}		\label{LEMooZMTPooKnqSuz}
	Si \( V\) est un espace vectoriel réel de dimension \( n\), et si \( B\) est une base de \( V\), alors le déterminant\footnote{Définition \ref{DEFooODDFooSNahPb}.} \(\det_B \colon V^n\to \eR  \) est un \( n\)-tenseur alterné.
\end{lemma}

\begin{definition}
	Si \( T\) est un \( k\)-tenseur, nous définissons
	\begin{equation}
		\Alt(T)=\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}T^{\pi}
	\end{equation}
	où la somme porte sur le groupe symétrique\footnote{Groupe symétrique, définition \ref{DEFooJNPIooMuzIXd}.}.
\end{definition}

\begin{lemma}[\cite{BIBooDEEYooRGFyDD}]		\label{LEMooWYCAooKoFpRu}
	L'application \(\Alt \colon \aL_k(V)\to \Lambda^kV^*  \) est une projection linéaire, c'est à dire
	\begin{enumerate}
		\item\label{ITEMooLADTooNfbVlO} \( \Alt\big( \aL_k(V) \big)\subset \Lambda^kV^*  \)
		\item\label{ITEMooQPJKooTvriBv} \( \Alt|_{\Lambda^kV^*}=\id\).
		\item		\label{ITEMooYOXTooTmzziO}
		      L'application \( \Alt\) est linéaire
	\end{enumerate}
\end{lemma}

\begin{proof}
	Pour \ref{ITEMooLADTooNfbVlO}, nous prouvons que \( \big( \Alt(T) \big)^{\sigma}=(-1)^{\sigma}\Alt(T)\). Pour cela nous utilisons les formules du lemme \ref{LEMooJEZYooMmrtgu}. Nous avons
	\begin{subequations}
		\begin{align}
			\Alt(T)^{\sigma} & = \left( \frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}T^{\pi} \right)^{\sigma}                 \\
			                 & = \frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}T^{\pi\circ\sigma}                              \\
			                 & = (-1)^{\sigma}\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\sigma}(-1)^{\pi}T^{\pi\circ\sigma}    \\
			                 & = (-1)^{\sigma}\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi\circ \sigma}T^{\pi\circ\sigma}     \\
			                 & = (-1)^{\sigma}\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}T^{\pi}		\label{SUBEQooTWZPooNpAjjd} \\
			                 & =(-1)^{\sigma}\Alt(T).
		\end{align}
	\end{subequations}
	Pour \eqref{SUBEQooTWZPooNpAjjd}, nous avons utilisé le fait que \( \pi\mapsto\pi\circ\sigma\) est une bijection de \( S_k\) et la proposition \ref{PROPooJBQVooNqWErk}.

	Pour le point \ref{ITEMooQPJKooTvriBv}, nous considérons un tenseur alterné \( T\). Il vérifie donc \( T^{\pi}=(-1)^{\pi}T\). Nous avons alors le calcul suivant :
	\begin{subequations}
		\begin{align}
			\Alt(T) & =\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}T^{\pi}     \\
			        & =\frac{1}{ k!}\sum_{\pi\in S_k}(-1)^{\pi}(-1)^{\pi}T \\
			        & =\frac{1}{ k!}\sum_{\pi\in S_k}T                     \\
			        & =T
		\end{align}
	\end{subequations}
	parce que \( S_k\) contient exactement \( k!\) éléments par le lemme \ref{LEMooSGWKooKFIDyT}.

	Pour le point \ref{ITEMooYOXTooTmzziO}, il s'agit de la linéarité de \( T\mapsto T^{\pi}\) donnée par le lemme \ref{LEMooJEZYooMmrtgu}.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]		\label{LEMooIMYIooZIxYRp}
	Si \( T\in\aL_k(V)\) et \( S\in aL_l(V)\), nous avons
	\begin{equation}
		\Alt(T\otimes S)=(-1)^{kl}\Alt(S\otimes T).
	\end{equation}
\end{lemma}

\begin{proof}
	Nous introduisons la permutation \( \sigma\in S_{k+l}\) qui permute \( \{ 1,\ldots,k \}\) avec \( \{ k+1,\ldots,k+l \}\)
\end{proof}<++>

\begin{definition}[Produit extérieur]
	Pour \( T\in \Lambda^kV^*\) et \( S\in\Lambda^lV^*\) nous définissons
	\begin{equation}
		T\wedge S=\frac{ (k+l)! }{ k!l! }\Alt(T\otimes S).
	\end{equation}
	Cette opération est le \defe{produit extérieur}{produit extérieur}.
\end{definition}

\begin{lemma}[\cite{BIBooDEEYooRGFyDD}]		\label{LEMooKEOWooNDXqgr}
	Propriétés du produit extérieur.
	\begin{enumerate}
		\item		\label{ITEMooELVSooHlORJy}
		      L'opération \( \wedge\) est une application \(\wedge \colon \Lambda^kV^*\times\Lambda^lV^*\to \Lambda^{k+l}V^*  \).
		\item		\label{ITEMooRUBEooKVHFSz}
		      L'application \(\wedge \colon \Lambda^kV^*\times\Lambda^lV^*\to \Lambda^{k+l}V^*  \) est linéaire en ses deux arguments.
		\item		\label{ITEMooTCWPooIJYaRE}
		      Nous avons \( T\wedge S=(-1)^{kl}S\wedge T\).
		\item		\label{ITEMooKFPZooFenmCT}
		      \( (T\wedge S)\wedge R=T\wedge(S\wedge R)\).
	\end{enumerate}
\end{lemma}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Pour \ref{ITEMooELVSooHlORJy}]
		%-----------------------------------------------------------
		Nous savons que \( T\otimes S\in\aL_{k+l}(V)\). Le lemme \ref{LEMooWYCAooKoFpRu} conclut.
		\spitem[Pour \ref{ITEMooRUBEooKVHFSz}]
		%-----------------------------------------------------------
		C'est la bilinéarité du produit tensoriel \( \otimes\) et celle de l'antisymétrisation \( \Alt\) donnée dans le lemme \ref{LEMooWYCAooKoFpRu}\ref{ITEMooYOXTooTmzziO}.

		Notez que la bilinéarité du produit tensoriel \( \otimes\) remonte à la définition d'un produit tensoriel \( (T,h)\) \ref{DEFooXKKQooAvWRNp} qui demande à \( h\) d'être bilinéaire. La proposition \ref{DEFooUUMYooOUCzWk} prouve que le produit \( T\otimes S\) entre applications multilinéaires donne bien un produit tensoriel. Bref. Tout cela pour nous rappeler que ce que nous notons \( T\otimes S\) pourrait tout aussi bien se noter \( h(T,S)\).
		\spitem[Pour \ref{ITEMooTCWPooIJYaRE}]
		%-----------------------------------------------------------
		\spitem[Pour \ref{ITEMooKFPZooFenmCT}]
		%-----------------------------------------------------------

	\end{subproof}<++>
\end{proof}<++>

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons des espaces vectoriels \( V\) et \( W\) de dimension finie. L'application \eqref{EQooAEFRooPfmAnj} donne un isomorphisme d'espaces vectoriels
\begin{equation}
	\begin{aligned}
		\psi\colon V\otimes W & \to \aL(V^*,W)                                \\
		v\otimes w            & \mapsto \big( \alpha\mapsto \alpha(v)w \big).
	\end{aligned}
\end{equation}
Et ça, c'est très bien, parce que nous connaissons une norme sur \( \aL(V^*,W)\) :  la norme opérateur \ref{DefNFYUooBZCPTr}.

\begin{definition}[\cite{MonCerveau}]      \label{DEFooEXXNooMgIpSV}
	Soient deux espaces vectoriels normés de dimension finie \( V\) et \( W\). Sur \( V\otimes W\) nous définissons, pour \( t\in V\otimes W\)
	\begin{equation}
		\| t \|=\| \psi(t) \|_{\aL(V^*,W)}.
	\end{equation}
\end{definition}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooQPXHooJWfpmk}
	La norme sur \( V\otimes W\) vérifie
	\begin{equation}
		\| v\otimes w \|=\| v \|\| w \|
	\end{equation}
	pour tout \( v\in V\) et \( w\in W\).
\end{lemma}

\begin{proof}
	C'est un simple(?) calcul :
	\begin{equation}
		\| v\otimes w \|=\| \psi(v\otimes w) \|=\| \alpha\mapsto \alpha(v)w \|=\sup_{\| \alpha \|=1}\| \alpha(v)w \|=\sup_{\| \alpha \|=1}| \alpha(v) |\| w \|.
	\end{equation}
	Étant donné que \( V\) est de dimension finie, \( \sup_{\| \alpha \|=1}| \alpha(v) |=\| v \|\)\quext{Cela est une des raisons pour lesquelles nous sommes en dimension finie : je ne sais pas si cette égalité est vraie en dimension inifinie.}. Nous avons donc
	\begin{equation}
		\| v\otimes w \|=\| v \|\| w \|.
	\end{equation}
\end{proof}

Le lemme suivant montre que \( \eR\otimes \eR\) n'est pas du tout \( \eR\times \eR=\eR^2\). Au contraire, \( \eR\otimes \eR\) est isomorphe à \( \eR\).
\begin{lemma}[\cite{MonCerveau}]        \label{LEMooVONEooQpPgcn}
	L'application
	\begin{equation}
		\begin{aligned}
			\varphi\colon \eR\otimes \eR & \to \eR   \\
			1\otimes 1                   & \mapsto 1
		\end{aligned}
	\end{equation}
	prolongée par linéarité est un isomorphisme isométrique.
\end{lemma}

\begin{proof}
	D'abord une base de \( \eR\) est \( \{ 1 \}\); donc une base de \( \eR\otimes \eR\) est \( \{ 1\otimes 1 \}\) par la proposition \ref{PROPooTHDPooWgjUwk}. Donc l'application proposée se prolonge par linéarité à tout \( \eR\otimes \eR\).

	Le fait que \( \varphi\) soit une bijection provient du fait que \( \varphi\) transforme une base en une base; si vous n'y croyez pas, la vérification de l'injectivité et de la surjectivité est facile.

	Pour que \( \varphi\) soit isométrique, nous faisons le calcul
	\begin{equation}
		\| \varphi(x\otimes y) \|=\| xy(1\otimes 1) \|=| xy |\| 1\otimes 1 \|=| xy |=\| x\otimes y \|.
	\end{equation}
	Nous avons utilisé la propriété \ref{DefNorme}\ref{ItemDefNormeii} d'une norme ainsi que le lemme \ref{LEMooQPXHooJWfpmk} pour la norme sur \( \eR\otimes \eR\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Applications bilinéaires, matrices et produit tensoriel}
%---------------------------------------------------------------------------------------------------------------------------
\label{SECooUKRYooZjagcX}

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}        \label{EQooUNRYooKBrXyK}
	(\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
	(a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
	(a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est un vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Application d'opérateurs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemMyKPzY}
	Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
	\begin{equation}        \label{EqXdxvSu}
		(Ax\otimes By)=A(x\otimes y)B^t.
	\end{equation}
\end{lemma}

\begin{proof}
	Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
	\begin{subequations}
		\begin{align}
			(Ax\otimes By)_{ij} & =(Ax)_i(By)_j                       \\
			                    & =\sum_{kl}A_{ik}x_kB_{jl}y_l        \\
			                    & =A_{ik}(x\otimes y)_{kl}B_{jl}      \\
			                    & =\big( A(x\otimes y)B^t \big)_{ij}.
		\end{align}
	\end{subequations}
\end{proof}


Le fait que les applications linéaires soient continues\footnote{Proposition \ref{PROPooQZYVooYJVlBd}.} est valable dans une assez large gamme d'espaces vectoriels\cite{BIBooUWMLooWEPxcC}. Nous voyons ici dans le cas des espaces vectoriels normés de dimension finies.
\begin{proposition}     \label{PROPooADPDooOtukQP}
	Soient des espaces vectoriels normés \( E\) et \( F\). Si \( f\colon E\to F\) est une application linéaire et si \( E\) est de dimension finie, alors \( f\) est continue.
\end{proposition}

\begin{proof}
	La proposition \ref{DefNFYUooBZCPTr}\ref{ITEMooGIPIooUvVBIv} nous dit que \( \| f \|<\infty\), c'est-à-dire que \( f\) est borné. Donc la proposition \ref{PROPooQZYVooYJVlBd} conclut.
\end{proof}


\begin{lemma}   \label{LemWWXVSae}
	Soit \( F\) un espace de Banach et deux suites \( A_k\to A\) et \( B_k\to B\) dans \( \aL(F,F)\). Alors \( A_k\circ B_k\to A\circ B\) dans \( \aL(F,F)\), c'est-à-dire
	\begin{equation}
		\lim_{k\to \infty} (A_kB_k)=\left( \lim_{k\to \infty} A_k \right)\left( \lim_{k\to \infty} B_k \right).
	\end{equation}
\end{lemma}

\begin{proof}
	Il suffit d'écrire
	\begin{equation}
		\| A_kB_k-AB \|\leq \| A_kB_k-A_kB \|+\| A_kB-AB \|.
	\end{equation}
	Le premier terme tend vers zéro pour \( k\to\infty\) parce que
	\begin{subequations}
		\begin{align}
			\| A_kB_k-A_kB \| & =\| A_k(B_k-B) \|                           \\
			                  & \leq \| A_k \|\| B_k-B \|\to \| A \|\cdot 0 \\
			                  & =0
		\end{align}
	\end{subequations}
	où nous avons utilisé la propriété fondamentale de la norme opérateur : la proposition~\ref{PROPooQZYVooYJVlBd}. Le second terme tend également vers zéro pour la même raison.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Convergence en norme et par composante}
%---------------------------------------------------------------------------------------------------------------------------

En dimension infinie, la convergence en norme et la convergence composante par composante ne s'impliquent ni dans un sens ni dans l'autre.

L'exemple suivant devrait être formalisé dans l'espace \( \ell^2\) des suites de carré sommable, mais vous voyez l'idée.
\begin{example}
	Nous considérons l'ensemble des suites réelle munie de la norme \( \| x \|=\sqrt{ \sum_{k=0}^{\infty}| x_k |^2 } \). Dedans nous considérons les vecteurs de base \( e_i\) donnés par
	\begin{equation}
		(e_i)_n=\delta_{in}.
	\end{equation}
	Ensuite nous considérons la base
	\begin{equation}
		f_i=e_1+\frac{1}{ 2^i }e_i.
	\end{equation}
	La suite \( x_n=f_n-f_1\), dans cette base a toujours \( -1\) comme première composante\footnote{N'essayez pas de faire un dessin : ça ne fonctionne qu'en dimension infinie.}. Et pourtant elle converge en norme vers \( 0\).
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Calcul différentiel dans un espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecLStKEmc}

Quelques motivations pour la notion de différentielle sont données dans \ref{SEBSECooLPRQooJRQCFL}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition de la différentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}[\cite{MonCerveau}]      \label{DefDifferentiellePta}
	Soient deux espaces vectoriels normés\footnote{Définition \ref{DefNorme}.} \( E\) et \( F\) ainsi qu'une fonction \( f\colon \mU\to F\) où \( \mU\) est un ouvert de \( E\). Si il existe une une application linéaire \( T\in\aL(E,F)\) satisfaisant
	\begin{equation}	\label{EqCritereDefDiff}
		\lim_{\substack{h\to 0\\h\in E}}\frac{f(a+h)-f(a)-T(h)}{\|h\|_E}=0,
	\end{equation}
	alors il en existe une seule.

	Dans ce cas nous disons que \( f\) est \defe{différentiable au point \( a\)}{application!différentiable} et l'application \( T\) ainsi définie est appelée \defe{différentielle}{différentielle} de \( f\) au point \( a\), et nous la notons \( df_a\).
\end{propositionDef}

\begin{proof}
	Soient deux applications linéaires \( T_1\), \( T_2\) satisfaisant la condition \eqref{EqCritereDefDiff}. Nous avons
	\begin{equation}
		\frac{ \| T_1(h)-T_2(h) \|_F }{ \| h \|_E }\leq \frac{ \| T_1(h)-f(a+h)+f(a) \| }{ \| h \| }+\frac{ \| f(a+h)-f(a)-T_2(h) \| }{ \| h \| }\to 0.
	\end{equation}
	Nous avons donc
	\begin{equation}
		\lim_{h\to 0} \frac{ \| (T_1-T_2)(h) \|_F }{ \| h \|_E }=0.
	\end{equation}
	Soit \( \epsilon>0\). Ce que signifie la limite est qu'il existe un \( r>0\) tel que pour tout \( u\in B_E(0,r)\), nous ayons
	\begin{equation}
		\frac{ \| (T_1-T_2)(u) \|_F }{ \| u \|_E }<\epsilon.
	\end{equation}
	Soit \( v\in E\). Nous considérons \( \lambda\in\eR\) tel que \( \lambda v\in B(0,r)\), par exemple \( \lambda<r/\| v \|\). Nous avons
	\begin{equation}
		\epsilon>\frac{ \| (T_1-T_2)(\lambda v) \|_F }{ \| \lambda v \|_E }=\frac{ \| (T_1-T_2)(v) \| }{ \| v \| }.
	\end{equation}
	Cela donne
	\begin{equation}
		\| (T_1-T_2)(v) \|<\| v \|\epsilon.
	\end{equation}
	Nous avons donc \( \| (T_1-T_2)(v) \|=0\), soit \( T_1(v)=T_2(v)\).
\end{proof}

L'application différentielle
\begin{equation}
	\begin{aligned}
		df\colon E & \to \aL(E,F) \\
		a          & \mapsto df_a
	\end{aligned}
\end{equation}
est également très importante.


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Quelque mots à propos des différentielles d'ordre supérieures}
%---------------------------------------------------------------------------------------------------------------------------

Soient deux espaces vectoriels normés \( V\) et \( W\) ainsi qu'une application \( f\colon V\to W\). La différentielle est une application \( df\colon V\to \aL(V,W)\). Pour être clair, la différentielle seconde consiste à différentier \( df_x\) par rapport à \( x\). C'est-à-dire que la différentielle seconde est une application \( d(df)\colon V\to \aL\big( V,\aL(V,W) \big)\).

Et c'est là que commencent les problèmes. Les différentielles successives font intervenir des emboîtements de plus en plus profonds d'espaces comme \( d^3f\colon V\to \aL\Big( V,\aL\big( V,\aL(V,W) \big) \Big)\).

Nous introduisons quelque notations pour traiter ces espaces.

\begin{definition}[\cite{ZCKMFRg, MonCerveau}]  \label{DefPNjMGqy}
	Soient deux espaces vectoriels normés \( V\) et \( W\) ainsi qu'une application \( f\colon V\to W\). Nous disons que \( f\) est
	\begin{itemize}
		\item de classe  \( C^0\) si elle est continue,
		\item de classe \( C^1\) si l'application différentielle \( df\colon V\to \aL(V,W)\) est continue,
		\item de classe \( C^k\) si sa différentielle est de classe \( C^{k-1}\).
		\item de classe \( C^{\infty}\) si elle est de classe \( C^k\) pour tout \( k\).
	\end{itemize}

	Lorsque nous demandons que la différentielle de \( f\) soit continue, nous entendons bien la continuité de \( df\colon V\to \aL(V,W)\), c'est-à-dire la continuité de \( df_x\) par rapport à \( x\). Sur\( \aL(V,E)\), nous considérons la topologie de la norme opérateur \ref{DefNFYUooBZCPTr}.

\end{definition}
\index{application!différentiable}
\index{application!de classe \( C^k\)}
Le lien entre classe \( C^k\) et dérivées partielles d'ordre \( k\) sera le théorème \ref{THOooPZTAooTASBhZ}.

\begin{definition}      \label{DEFooJYOPooBzditG}
	Soient des espaces vectoriels normés \( V\) et \( W\). Nous définissons les espaces emboîtés par récurrence :
	\begin{subequations}
		\begin{numcases}{}
			E_0=E\\
			E_{k+1}=\aL(V,E_k).
		\end{numcases}
	\end{subequations}
\end{definition}

\begin{definition}
	Si \( \{ e_i \}\) est une base d'un espace vectoriel \( V\), nous allons noter
	\begin{equation}
		\begin{aligned}
			\omega_i\colon V & \to \eR      \\
			x                & \mapsto x_i.
		\end{aligned}
	\end{equation}
	Ce \( \omega_i\) est ce qu'on appelle souvent \( e_i^*\). Plus généralement, si \( I\) est le multiindice \( (i_1,\ldots, i_l)\) nous notons \( \omega_I\in \aL^l(V,\eR)\) par
	\begin{equation}
		\begin{aligned}
			\omega_I\colon V^l        & \to \eR                                     \\
			(x^{(1)},\ldots, x^{(l)}) & \mapsto  x^{(1)}_{i_1}\ldots x^{(l)}_{i_l}.
		\end{aligned}
	\end{equation}
	Ce seront nos formes multilinéaires de base.
\end{definition}

Afin de garder des notations très explicites, nous ne pouvons pas écrire des formules comme
\[
	df_a=\sum_i\frac{ \partial f }{ \partial x_i }(a)\omega_i
\]
parce que si \( f\) prend ses valeurs dans \( \aL(V,\eR)\), lorsqu'on écrit \( df_a(v)\), il n'y a aucune raison à priori de vouloir que \( v\) soit pris par \( \omega_i\) au lieu de \( \partial_if(a)\).

Nous introduisons donc un produit fait exprès pour dire que «c'est celui de droite qui prend».
\begin{definition}[\cite{MonCerveau}]       \label{DEFooLULCooYjBEaZ}
	Si \( W\) est un espace vectoriel, nous définissons le produit \( \times_n\) par
	\begin{equation}
		\begin{aligned}
			\times_1\colon W\times \aL(V,\eR) & \to \aL(V,W) \\
			(w\times_1\alpha)(v)              & =\alpha(v)w
		\end{aligned}
	\end{equation}
	et par\footnote{Définition \ref{DEFooJYOPooBzditG} pour les espaces \( E_n\) et \( \eR_n\).}
	\begin{equation}
		\begin{aligned}
			\times_n\colon W\times \eR_n & \to W_n                  \\
			(w\times_n\alpha)(v)         & =w\times_{n-1} \alpha(v)
		\end{aligned}
	\end{equation}
\end{definition}
Cette notation sera utilisée dans la proposition \ref{PROPooUDJLooHwzjQF} pour écrire correctement \( df_a\). Pour l'instant nous n'en avons pas besoin.



\begin{definition}[difféomorphisme]      \label{DefAQIQooYqZdya}
	Soient \( U\) et \( V\), deux ouverts d'un espace vectoriel normé. Une application \( f\) de \( U\) dans \( V\) est un \defe{difféomorphisme}{difféomorphisme} si elle est bijective, différentiable\footnote{Différentiables, définition \ref{DefDifferentiellePta}.} et dont l'inverse \( f^{-1}:V\to U \) est aussi différentiable.

	Un \( C^k\)-difféomorphisme est un difféomorphisme qui est \( C^k\) et dont l'inverse est \( C^k\).
\end{definition}

\begin{normaltext}
	Truc marrant : un \( C^1\)-difféomorphisme n'est pas seulement un difféomorphisme qui est \( C^1\). L'inverse doit également être \( C^1\). Comment nommer un difféomorphisme qui est par ailleurs un application de classe \( C^1\) ? Je ne sais pas.
\end{normaltext}

\begin{remark}  % TOTOooVTLSooSNLVBD justifier ça.
	Il n'existe pas de bijection bicontinues d'un ouvert de \( \eR^m\) vers un ouvert de \( \eR^n\) si \( m\neq n\). Il n'y a donc pas de notion de difféomorphismes entre ouverts de dimensions différentes.
\end{remark}

\begin{remark}      \label{RemATQVooDnZBbs}
	L'application norme étant continue, le critère du théorème~\ref{ThoWeirstrassRn} est en réalité assez général. Par exemple à partir d'une application différentiable\footnote{Définition~\ref{DefDifferentiellePta}.} \( f\colon X\to Y\)  nous pouvons considérer la fonction réelle
	\begin{equation}
		a\mapsto \|  df_a   \|
	\end{equation}
	où la norme est la norme opérateur\footnote{Définition~\ref{DefNFYUooBZCPTr}.}. Si \( f\) est de classe \( C^1\) alors cette application est continue et donc bornée sur un compact \( K\) de \( X\).
\end{remark}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentielle d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Différentielle d'une application linéaire]       \label{LEMooZSNMooCfjzOB}
	Soient deux espaces vectoriels normés \( E\) et \( F\). Soit une application linéaire \( f\colon E\to F\).
	\begin{enumerate}
		\item
		      Si \( f\colon E\to F\) est linéaire, alors sa différentielle est
		      \begin{equation}
			      \begin{aligned}
				      df\colon E & \to  \aL(E,F) \\
				      a          & \mapsto f.
			      \end{aligned}
		      \end{equation}
		\item
		      Si \( f\colon E\to F\) est linéaire, toutes les différentielles d'ordre supérieures sont nulles.
		\item
		      Toute application linéaire est de classe \(  C^{\infty}\).
		\item
		      Toute application affine est de classe \(  C^{\infty}\).
	\end{enumerate}
\end{lemma}
%TODOooMGPJooZCgTRv Je crois que seul le premier point est prouvé. En particulier je suis sûr que le dernier n'est pas prouvé.
% La partie sur les applications affines C^oo est utilisée dans LEMooAJDLooIPcmIV.

\begin{proof}
	Pour rappel, toujours bon à avoir en tête : \( df\colon E\to \aL(E,F)\). Soit \( a\in E\); nous avons
	\begin{equation}
		\lim_{h\to 0} \frac{ \| f(a+h)-f(a)- f(h) \|_F }{ \| h \|_E }=0
	\end{equation}
	parce que le numérateur est nul pour tout \( h\). Donc \( h\mapsto f(h)\) est la différentielle de \( f\) au point \( a\) parce que elle vérifie la condition \eqref{DefDifferentiellePta}.

	Nous avons prouvé que la différentielle de \( f\) est l'application constante
	\begin{equation}
		\begin{aligned}
			df\colon E & \to \aL(E,F) \\
			a          & \mapsto f
		\end{aligned}
	\end{equation}

	En ce qui concerne la différentielle seconde, nous prouvons que \( d(df)_a=0\) pour tout \( a\in E\). En effet,
	\begin{equation}
		\lim_{h\to 0} \frac{ \| df_{a+h}-df_a \|_{\aL(E,F)} }{ \| h \|_E }=0
	\end{equation}
	parce que le numérateur vaut \( f-f=0\).

	Maintenant il n'est pas compliqué de faire une récurrence : si \( f\) est de classe \( C^k\) et si \( d^k(f)=0\), alors \( d^k(f)\) est de classe \( C^1\) et \( d^kf=0\).
\end{proof}

\begin{lemma}       \label{LEMooAJDLooIPcmIV}
	Soient \( a<b\) dans \( \eR\). L'application
	\begin{equation}        \label{EQooIINJooAlSqKF}
		\begin{aligned}
			f\colon \mathopen[ a , b \mathclose] & \to \mathopen[ 0 , 1 \mathclose] \\
			x                                    & \mapsto \frac{ x-a }{ b-a }
		\end{aligned}
	\end{equation}
	est un \(  C^{\infty}\)-difféomorphisme\footnote{Définition \ref{DefPNjMGqy}.}.
\end{lemma}

\begin{proof}
	En plusieurs parties.
	\begin{subproof}
		\spitem[Valeurs dans \( \mathopen\lbrack 0 , 1 \mathclose\rbrack\)]     \label{ITEMooLRECooOSEqJL}
		Nous devons prouver que pour tout \( x\in\mathopen[ a , b \mathclose]\), nous avons \( f(x)\in \mathopen[ 0 , 1 \mathclose]\). D'une part si \( x\in\mathopen[ a , b \mathclose]\), alors \( x-a\geq 0\) et donc \( (x-a)/(b-a)\geq 0\).

		Dans l'autre sens, si \( (x-a)/(b-a)>1\), alors \( x-a>b-a\) et donc \( x>b\). Donc \( f(x)>1\) n'arrive jamais pour \( x\in \mathopen[ a , b \mathclose]\).
		\spitem[Injectif]
		% -------------------------------------------------------------------------------------------- 
		Si \( f(x)=f(t)\), alors en simplifiant par \( b-a\neq 0\), nous trouvons \( x-a=t-a\) et donc \( x=t\) (ne citez le lemme \ref{LEMooFQMVooDNaTDT} que si vous êtes capables de le prouver, sinon faites comme si c'était évident et il ne vous arrivera rien).
		\spitem[Surjectif]
		% -------------------------------------------------------------------------------------------- 
		Il est vite vérifié que
		\begin{equation}       \label{EQooPSAWooNEJFih}
			f^{-1}(t)=t(b-a)+a,
		\end{equation}
		et en procédant de même qu'au point \ref{ITEMooLRECooOSEqJL}, nous voyons que pour tout \( t\in \mathopen[ 0 , 1 \mathclose]\), \( f^{-1}(t)\in\mathopen[ a , b \mathclose]\).
		\spitem[De classe \(  C^{\infty}\)]
		% -------------------------------------------------------------------------------------------- 
		C'est le lemme \ref{LEMooZSNMooCfjzOB} qui fait le travail parce que \eqref{EQooIINJooAlSqKF} est affine.
		\spitem[Inverse de classe \(  C^{\infty}\)]
		% -------------------------------------------------------------------------------------------- 
		Encore le lemme \ref{LEMooZSNMooCfjzOB} parce que \eqref{EQooPSAWooNEJFih} est affine.
	\end{subproof}
\end{proof}
