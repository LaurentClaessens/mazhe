% This is part of Mes notes de mathématique
% Copyright (c) 2008-2023, 2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation et trigonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------



\begin{propositionDef}[Matrices équivalentes]      \label{PROPooXGXTooQNAAxw}
	Nous définissons, sur l'ensemble \( \eM(n,\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\), la relation  \( A\sim B\) si et seulement si il existe des matrice inversibles \( P\) et \( Q\) telles que \( A=PBQ^{-1}\).

	Cette relation est une relation d'équivalence.

	Deux telles matrice sont dite \defe{équivalentes}{matrices équivalentes}.
\end{propositionDef}

\begin{proof}
	Il faut vérifier les points de la définition \ref{DefHoJzMp}. D'abord \( A\sim A\) en prenant \( P=Q=\mtu\). Si \( A\sim B\), alors il existe des matrices \( P\) et \( Q\) telles que \( A=PBQ^{-1}\). En multipliant ) gauche par \( P^{-1}\) et à droite par \( Q\) nous trouvons \( B=P^{-1}AQ\), ce qui montre que \( B\sim A\) par les matrices \( P^{-1}\) et \( Q^{-1}\). Enfin si \( A\sim B\) et \( B\sim C\), nous avons des matrices \( P,Q,R,S\) tells que \( A=PBQ^{-1}\) et \( B=RCS^{-1}\). Nous avons alors
	\begin{equation}
		A=PBQ^{-1}=P(RCS^{-1})Q^{-1}=(PR)C(QS)^{-1}.
	\end{equation}
	Donc \( A\) et \( C\) sont équivalentes par les matrices \( PR\) et \(  (QS)^{-1} \).
\end{proof}


\begin{propositionDef}[matrices semblables\cite{MonCerveau}] \label{DefCQNFooSDhDpB}
	Nous définissons, sur l'ensemble \( \eM(n,\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\), la relation  \( A\sim B\) si et seulement si il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\).

	Cette relation est une relation d'équivalence.

	Deux matrices équivalentes en ce sens sont dites \defe{semblables}{matrices semblables}.
\end{propositionDef}

\begin{proof}
	Comme \ref{PROPooXGXTooQNAAxw}, mais en plus facile parce qu'il n'y a qu'une seule matrice au lieu de deux.
\end{proof}

\begin{propositionDef}[Applications linéaires semblables]      \label{PROPooIXFSooZsFWHm}
	Soit un espace vectoriel \( E\). Nous définissons sur \( \End(E)\) la relation \( f\sim g\) si et seulement si il existe une application inversible \( u\colon E\to E\) telle que \( f=u^{-1}\circ g\circ u\).

	Cette relation est une relation d'équivalence.

	Deux endomorphismes équivalents en ce sens sont dits \defe{semblables}{applications linéaires semblables}.
\end{propositionDef}

\begin{proof}
	Toujours la même chose que \ref{PROPooXGXTooQNAAxw}.
\end{proof}

\begin{proposition}     \label{PROPooBGJBooXlDYEv}
	Deux applications linéaires sont semblables si et seulement si leurs matrices sont semblables dans une base.

	Dans ce cas, les matrices sont semblables dans toutes les bases.
\end{proposition}

\begin{proof}
	Supposons que \( f\sim g\). Il existe un endomorphisme inversible \( u\) tel que \( f=u^{-1}\circ g\circ u\). En vertu de la proposition \ref{PROPooFMBFooEVCLKA}, l'application qui a une application linéaire fait correspondre sa matrice est un morphisme d'algèbres. Donc si \( A\) est la matrice de \( f\), si \( B\) est celle de \( g\) et \( X\) celle de \( u\), nous avons
	\begin{equation}
		A=X^{-1}BX,
	\end{equation}
	ce qui montre que \( A\sim B\).
\end{proof}

\begin{lemma}       \label{LEMooKUHJooDfNKeg}
	Le polynôme caractéristique\footnote{Définition~\ref{DefOWQooXbybYD}.} est un invariant sous les similitudes.
\end{lemma}

\begin{proof}
	En effet si \( P\) est une matrice inversible,
	\begin{subequations}
		\begin{align}
			\chi_{P^{-1}AP} & = \det(P^{-1}AP-\lambda X\mtu)                    \\
			                & = \det\big( P(P^{-1}AP-\lambda X\mtu)P^{-1} \big) \\
			                & = \det(A-\lambda X\mtu)                           \\
			                & = \chi_A.
		\end{align}
	\end{subequations}
\end{proof}

La permutation de lignes ou de colonnes ne sont pas des similitudes, comme le montrent les exemples suivants :
\begin{equation}
	\begin{aligned}[]
		A & =\begin{pmatrix}
			     1 & 2 \\
			     3 & 4
		     \end{pmatrix} &
		B & =\begin{pmatrix}
			     2 & 1 \\
			     4 & 3
		     \end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons \( \chi_A=X^2-5X-2\) tandis que \( \chi_B=X^2-5X+2\) alors que le polynôme caractéristique est un invariant de similitude.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Trace de matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooRMYQooWkEpJJ}
	Invariance de la trace par similarité.
	\begin{enumerate}
		\item
		      Soit une application linéaire \( f\). Si la matrice de \( f\) dans une base est \( A\) et est \( B\) dans une autre base, alors
		      \begin{equation}
			      \trace(A)=\trace(B).
		      \end{equation}
		\item
		      Deux opérateurs semblables\footnote{Opérateurs semblables, définition \ref{DefCQNFooSDhDpB}} ont même trace.
		      %TODOooJHMPooIvpAzw : prouver cette partie.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Les matrices \( A\) et \( B\) sont liées par la proposition \ref{PROPooNZBEooWyCXTw} : \( B=Q^{-1}AQ\) où \( Q\) est la matrice qui lie les vecteurs des deux bases. L'invariance cyclique de la trace donnée en le lemme \ref{LEMooUXDRooWZbMVN} implique que
	\begin{equation}
		\trace(B)=\trace(Q^{-1}AQ)=\trace(QQ^{-1}A)=\trace(A).
	\end{equation}
\end{proof}

\begin{lemma}       \label{LEMooXXEYooKHyQjb}
	Soit une matrice \( A\). Nous avons
	\begin{equation}
		\tr(A^{\dag} A)=\sum_{ij}| A_{ij} |^2.
	\end{equation}
\end{lemma}

\begin{proof}
	Utilisant la proposition \ref{LEMooBOXMooSDyCfm} pour les éléments de la matrice \( A^{\dag}\), nous avons
	\begin{equation}
		\tr(A^{\dag}A)=\sum_k(A^{\dag}A)_{kk}
		=\sum_{kl}A^*_{lk}A_{lk}
		=\sum_{kl}| A_{lk} |^2.
	\end{equation}
\end{proof}

\begin{lemma}[\cite{BIBooTIQCooTVspjH}]     \label{LEMooQXFQooLGPcIt}
	Si les matrice \( A\) et \( B\) sont unitairement semblables\footnote{Définition \ref{DefCQNFooSDhDpB}.}, alors
	\begin{equation}
		\sum_{ij}| A_{ij} |^2=\sum_{ij}| B_{ij} |^2.
	\end{equation}
\end{lemma}

\begin{proof}
	Soit une matrice unitaire \( U\) telle que \( B=UAU^{\dag}\). Nous savons par le proposition \ref{PROPooRMYQooWkEpJJ} que des matrice semblables ont même trace. Or \( B^{\dag}B\) est unitairement semblable à \( A^{\dag}A\) parce que
	\begin{equation}
		B^{\dag}B=UA^{\dag}U^{\dag}UAU^{\dag}=UA^{\dag}AU^{\dag}.
	\end{equation}
	Donc \( \tr(A^{\dag}A)=\tr(B^{\dag}B)\). Le lemme \ref{LEMooXXEYooKHyQjb} conclut.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes nilpotents}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}	\label{DEFooHDFBooSbMgZN}
	La \defe{trace}{trace!matrice} d'une matrice \( A\in \eM(n,\eK)\) est la somme de ses éléments diagonaux :
	\begin{equation}
		\tr(A)=\sum_{i=1}^nA_{ii}.
	\end{equation}
\end{definition}
Une propriété importante est son invariance cyclique.

\begin{lemma}   \label{LemhbZTay}
	Quelques propriétés de la trace.
	\begin{enumerate}
		\item		\label{ITEMooBRUGooFWQDss}
		      Si \( A\) et \( B\) sont des matrices carrées, alors \( \tr(AB)=\tr(BA)\).
		\item
		      La trace est un invariant de similitude.
	\end{enumerate}
\end{lemma}

\begin{proof}
	C'est un simple calcul :
	\begin{equation}
		\tr(AB)=\sum_{ik}A_{ik}B_{ki}
		=\sum_{ik}A_{ki}B_{ik}
		=\sum_{ik}B_{ik}A_{ki}
		=\sum_i(BA)_{ii}
		=\tr(BA)
	\end{equation}
	où nous avons simplement renommé les indices \( i\leftrightarrow k\).

	En particulier, la trace est un invariant de similitude parce que \( \tr(ABA^{-1})=\tr(A^{-1} AB)=\tr(B)\) par l'invariance cyclique démontrée en \ref{LEMooUXDRooWZbMVN}\ref{ITEMooXDYQooAlnArd}.
\end{proof}

\begin{propositionDef}	\label{PROPooSQLOooCbTQOC}
	Soit un espace vectoriel de dimension finie \( E\). Soit une application linéaire \(f \colon E\to E  \). Nous considérons deux bases \( \mB_1\) et \( \mB_2\) pour \( E\). Les matrices \( A\) et \( B\) sont celles de \( f\) dans ces  deux bases.

	Alors \( \tr(A)=\tr(B)\).

	Ce nombre est appelé \defe{trace}{trace d'endomorphisme} de \( f\).
\end{propositionDef}

\begin{proof}
	La proposition \ref{PROPooNZBEooWyCXTw} nous dit que les matrices \( A\)  et \( B\) sont liées par \( A=QBQ^{-1}\) pour une certaine matrice \( Q\). Nous avons alors, en utilisant \ref{LemhbZTay}\ref{ITEMooBRUGooFWQDss},
	\begin{equation}
		\tr(A)=\tr(QBQ^{-1})=\tr(Q^{-1}QB)=\tr(B).
	\end{equation}
\end{proof}


\begin{proposition}	\label{PROPooLSBZooKGfeFI}
	Si la matrice est diagonalisable, alors la trace est la somme des valeurs propres.
	%TODOooVKSMooSBkufZ. Prouver ça.
\end{proposition}


\begin{lemma}[\cite{fJhCTE}]   \label{LemzgNOjY}
	L'endomorphisme \( u\in\End(\eC^n)\) est nilpotent si et seulement si \( \tr(u^p)=0\) pour tout \( p\).
\end{lemma}

\begin{proof}
	Supposons que \( u\) est nilpotent. Alors ses valeurs propres sont toutes nulles et celles de \( u^p\) le sont également. La trace étant la somme des valeurs propres, nous avons alors tout de suite \( \tr(u^p)=0\).

	Supposons maintenant que \( \tr(u^p)=0\) pour tout \( p\). Le polynôme caractéristique \eqref{Eqkxbdfu} est
	\begin{equation}    \label{EqfnCqWq}
		\chi_u=(-1)^nX^{\alpha}(X-\lambda_1)^{\alpha_1}\ldots (X-\lambda_r)^{\alpha_r}.
	\end{equation}
	où les \( \lambda_i\) (\( i=1,\ldots, r\)) sont les valeurs propres non nulles distinctes de \( u\).

	Il est vite vu que le coefficient de \( X^{n-1}\) dans \( \chi_u\) est \( -\tr(u)\) parce que le coefficient de \( X^{n-1}\) se calcule en prenant tous les \( X\) sauf une fois \( -\lambda_i\). D'autre part le polynôme caractéristique de \( u^p \) est le même que celui de \( u\), en remplaçant \( \lambda_i\) par \( \lambda_i^p\); cela est dû au fait que si \( v\) est vecteur propre de valeur propre \( \lambda\), alors \( u^pv=\lambda^pv\).

	Par l'équation \eqref{EqfnCqWq}, nous voyons que le coefficient du terme \( X^{n-1}\) dans le polynôme caractéristique est
	\begin{equation}        \label{eqSoDSKH}
		0=\tr(u^p)=\alpha_1\lambda_1^p+\cdots +\alpha_r\lambda_r^p.
	\end{equation}
	Donc les nombres \( (\alpha_1,\ldots, \alpha_r)\) sont une solution non triviale\footnote{Si \( \alpha_1=\ldots=\alpha_r=0\), alors les valeurs propres sont toutes nulles et la matrice est en réalité nulle dès le départ.} du système
	\begin{subequations}    \label{EqDpvTnu}
		\begin{numcases}{}
			\lambda_1X_1    +\cdots +\lambda_rX_r=0   \\
			\qquad\vdots                              \\
			\lambda^r_1X_1  +\cdots +\lambda_r^rX_r=0.
		\end{numcases}
	\end{subequations}
	Ce sont les équations \eqref{eqSoDSKH} écrites pour \( p=1,\ldots, r\). Le déterminant de ce système est
	\begin{equation}
		\lambda_1\ldots\lambda_r\det\begin{pmatrix}
			1               & \ldots & 1               \\
			\lambda_1       & \ldots & \lambda_r       \\
			\vdots          &        & \vdots          \\
			\lambda_1^{r-1} & \ldots & \lambda_r^{r-1}
		\end{pmatrix}\neq 0,
	\end{equation}
	qui est un déterminant de Vandermonde (proposition~\ref{PropnuUvtj}) valant
	\begin{equation}
		0=\lambda_1\ldots\lambda_r\prod_{1\leq i\leq j\leq r}(\lambda_i-\lambda_j).
	\end{equation}
	Étant donné que les \( \lambda_i\) sont distincts et non nuls, nous avons une contradiction et nous devons conclure que \( (\alpha_1,\ldots, \alpha_r)\) était une solution triviale du système \eqref{EqDpvTnu}.
\end{proof}

\begin{proposition}[\cite{SVSFooIOYShq}]    \label{PropMWWJooVIXdJp}
	Soit un \( \eK\)-espace vectoriel \( E\). Un endomorphisme \( u\in\End(E)\) est nilpotent si et seulement si il existe une base de \( E\) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.
\end{proposition}

\begin{proof}
	\begin{subproof}
		\spitem[\( \Rightarrow\)]
		Nous faisons la démonstration par récurrence sur la dimension de \( E\). Lorsque \( n=1\) nous avons \( u=(a)\) avec \( a\in \eK\). Puisque \( a^k=0\) pour un certain \( k\) nous avons \( a=0\) parce qu'un corps est toujours un anneau intègre\footnote{Lemme~\ref{LEMooIKNMooMfvQnu}.}.

		Lorsque \( \dim(E)=n\) nous savons que \( u\) a un noyau non réduit au vecteur nul (parce qu'il est nilpotent). Soit donc un vecteur non nul \( x\in\ker(u)\) et une base
		\begin{equation}
			\{ x,e_2,\ldots, e_n \}
		\end{equation}
		donnée par le théorème de la base incomplète~\ref{ThonmnWKs}. La matrice de \( u\) dans cette base s'écrit
		\begin{equation}
			\begin{pmatrix}
				\begin{array}[]{c|c}
					0              & \begin{matrix}
						                 * & * & *
					                 \end{matrix} \\
					\hline
					\begin{matrix}
						0 \\
						0 \\
						0
					\end{matrix} &
					\begin{pmatrix}
						 &   & \\
						 & A & \\
						 &   &
					\end{pmatrix}
				\end{array}
			\end{pmatrix}.
		\end{equation}
		Un tout petit peu de calcul de produit de matrice montre que la matrice de \( u^k\) est de la forme
		\begin{equation}
			\begin{pmatrix}
				\begin{array}[]{c|c}
					0              & \begin{matrix}
						                 * & * & *
					                 \end{matrix} \\
					\hline
					\begin{matrix}
						0 \\
						0 \\
						0
					\end{matrix} &
					\begin{pmatrix}
						 &     & \\
						 & A^k & \\
						 &     &
					\end{pmatrix}
				\end{array}
			\end{pmatrix}.
		\end{equation}
		Étant donné que l'endomorphisme \( u\) est nilpotent, la matrice \( A\) l'est aussi. L'hypothèse de récurrence dit alors que \( A\) est strictement triangulaire supérieure (ou en tout cas peut le devenir par un changement de base adéquat).

		\spitem[\( \Leftarrow\)]
		Soit une base \( \{ e_1,\ldots, e_n \}  \) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.

		Alors \( u(e_1)=0\) et plus généralement, \( u(e_k)\in \Span\{ e_1,\ldots, e_{k-1} \}\). Voyez par récurrence que \( u^l(e_k)\in \Span\{ e_1,\ldots, e_{k-l} \}\). Donc \( u^l(e_k)=0\) dès que \( l\geq k\).
	\end{subproof}
\end{proof}

\begin{proposition}[Thème~\ref{THEMEooPQKDooTAVKFH}]     \label{PROPooWTFWooXHlmhp}
	Soit \( E\) un espace de Banach (espace vectoriel normé complet\footnote{Définition \ref{DefVKuyYpQ}.}). Si \( A\in\aL(E,E)\) est nilpotente, alors \( (\mtu-A)\) est inversible et son inverse est donné par
	\begin{equation}
		(\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k,
	\end{equation}
	où l'infini peut évidemment être remplacé par l'ordre de nilpotence de \( A\).
\end{proposition}

\begin{proof}
	En ce qui concerne la convergence de la somme, elle ne fait pas de doute parce que \( A\) étant nilpotente, la somme contient seulement une quantité finie de termes non nuls.

	Montrons à présent que la somme est l'inverse de \( \mtu-A\) en multipliant terme à terme :
	\begin{equation}
		\sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
	\end{equation}
	Par conséquent
	\begin{equation}
		\| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\to 0.
	\end{equation}
	La dernière limite est en réalité une égalité pour \( n\) assez grand.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Diagonalisable]  \label{DefCNJqsmo}
	Plusieurs notions.
	\begin{enumerate}
		\item
		      Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable\footnote{Définition~\ref{DefCQNFooSDhDpB}.} à une matrice diagonale.
		\item
		      Une matrice est \defe{diagonale}{matrice diagonale} si seuls ses éléments diagonaux sont non nuls.
		\item
		      Une application linéaire est diagonalisable si elle est semblable\footnote{Définition \ref{PROPooIXFSooZsFWHm}.} à une application linéaire diagonale.
		\item
		      Une application linéaire est diagonal si elle admet une base de vecteurs propres.
	\end{enumerate}
	Dans une matrice diagonale, certains éléments diagonaux peuvent être nuls. Pour un opérateur diagonal par contre, il faut de vrais vecteurs propres. Donc une matrice diagonale n'est pas toujours la matrice d'un opérateur diagonal.
\end{definition}

\begin{lemma}	\label{LEMooOCLHooBMwUtm}
	Un opérateur est diagonal si et seulement si il est diagonalisable.
	%TODOooBEUVooIAWQja. Prouver ça.
\end{lemma}

\begin{proposition}     \label{PROPooDEETooSOMiGO}
	Si \( A\) est un opérateur diagonalisable\footnote{Définition \ref{DefCNJqsmo}.} dont les valeurs propres sont \( \lambda_i\), alors il existe un opérateur inversible \( Q\) tel que
	\begin{equation}
		A=Q^{-1} DQ
	\end{equation}
	où \( D \) est l'opérateur diagonal contenant les \( \lambda_i\) sur sa diagonale.
\end{proposition}

\begin{proof}
	Commençons par montrer que si \( u\) est un vecteur propre de \( A\), alors \( Q^{-1}u\) est un vecteur propre de \( D\), pour la même valeur propre. En effet nous avons \( D=Q^{-1}AQ\), et donc
	\begin{subequations}
		\begin{align}
			D(Q^{-1}u)=Q^{-1}AQQ^{-1}u=Q^{-1}Au=\lambda Q^{-1} u.
		\end{align}
	\end{subequations}
	Donc si \( \{ e_i \}\) est une base de vecteurs propres de \( D\), alors \( \{ Qe_i \}\) est une base de vecteurs propres de \( A\). Pour les mêmes valeurs propres.
\end{proof}

\begin{lemma}
	Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est-à-dire si c'est la matrice unité).
\end{lemma}

\begin{proof}
	Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

\begin{lemma}       \label{LemgnaEOk}
	Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
	\begin{equation}
		\mu_u=P_1^{n_1}\ldots P_r^{n_r}
	\end{equation}
	où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
	\begin{equation}
		F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
	\end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
	Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
	\begin{enumerate}
		\item       \label{ItemThoDigLEQEXRiv}
		      L'endomorphisme \( u\) est diagonalisable.
		\item       \label{ItemThoDigLEQEXRi}
		      Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\), dont toutes les racines sont simples, tel que \( P(u)=0\).
		\item       \label{ItemThoDigLEQEXRii}
		      Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
			      \begin{equation}
				      \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
			      \end{equation}
			      mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
		\item       \label{ItemThoDigLEQEXRiii}
		      Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
		\item       \label{ITEMooZNJFooEiqDYp}
		      Dans une base adaptée, la matrice de \( u\) est diagonale et les éléments diagonaux sont ses valeurs propres.
	\end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
	Plein d'implications à prouver.
	\begin{subproof}
		\spitem[\ref{ItemThoDigLEQEXRi} implique~\ref{ItemThoDigLEQEXRii}] Étant donné que \( P(u)=0\), il est dans l'idéal des polynômes annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynômes annulateurs est généré par \( \mu_u\) par le théorème~\ref{ThoCCHkoU}.

		\spitem[\ref{ItemThoDigLEQEXRii} implique~\ref{ItemThoDigLEQEXRiv}] Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est-à-dire
		\begin{equation}
			\mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
		\end{equation}
		où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème~\ref{ThoDecompNoyayzzMWod}) nous enseigne que
		\begin{equation}
			E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
		\end{equation}
		Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

		\spitem[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRiii}] Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) et \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoMGQZooIgrXjy}\ref{ITEMooCJQGooXwjsfm}, nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est stable par \( u\).

		\spitem[\ref{ItemThoDigLEQEXRiii} implique~\ref{ItemThoDigLEQEXRiv}] En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

		\spitem[\ref{ItemThoDigLEQEXRiv} implique~\ref{ItemThoDigLEQEXRi}] Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
		\begin{equation}
			P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
		\end{equation}
		Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\),
		\begin{equation}
			P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
		\end{equation}
		par le lemme~\ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur la base \( \{ e_i \}\).

		\spitem[\ref{ITEMooZNJFooEiqDYp} implique~\ref{ItemThoDigLEQEXRi}]
		Si la matrice \( A\) est diagonale alors le polynôme \( P=\prod_{i=1}^n(X-A_{ii})\) est annulateur de \( A\). En effet,
		\begin{equation}
			P(A)e_k=\prod_{i=1}^n(A-A_{ii})x=\prod_{i=1}^n\big( u(e_k)-A_{ii}e_k \big)=\prod_{i=1}^n\big( A_{kk}e_k-A_{ii}e_k \big)=0
		\end{equation}
		parce que le facteur \( i=k\) est nul.
		\spitem[\ref{ItemThoDigLEQEXRii} implique~\ref{ITEMooZNJFooEiqDYp}]
		le polynôme minimal de \( u\) s'écrit
		\begin{equation}
			\mu=(X-\lambda_1)\ldots(X-\lambda_r),
		\end{equation}
		et les espaces \( E_i\) du lemme~\ref{LemgnaEOk} sont les espaces propres \( E_i=\ker(u-\lambda_i)\). Nous avons donc une somme directe
		\begin{equation}
			E=E_1\oplus\ldots\oplus E_r.
		\end{equation}
		Dans chacun des espaces propres, \( u\) a une matrice diagonale avec la valeur propre correspondante sur la diagonale. Une base de \( E\) constituée d'une base de chacun des espaces propres est donc une base comme nous en cherchons.
	\end{subproof}
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
	Si \( u\) est diagonalisable et si \( F\) est un sous-espace stable par \( u\), alors
	\begin{equation}
		F=\bigoplus_{\lambda}E_{\lambda}(u)\cap F
	\end{equation}
	où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
	Par le théorème~\ref{ThoDigLEQEXR}, le polynôme \( \mu_u\) est scindé et ne possède que des racines simples. Notons le
	\begin{equation}
		\mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
	\end{equation}
	Les espaces \( E_i\) du lemme~\ref{LemgnaEOk} sont maintenant les espaces propres.

	En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
	Soient \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
	Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme~\ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n=\dim(E)\) et \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme~\ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
	Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
	\begin{enumerate}
		\item       \label{ItemGqhAMei}
		      Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u_i)\big)\subset E_{\lambda}(u_i)\).
		\item
		      Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
	\end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
	Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_i(x)=\lambda x\). Nous montrons que \( u_j(x)\in E_{\lambda}(u_i)\). Nous avons
	\begin{equation}
		u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
	\end{equation}
	Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

	Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

	Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
	\begin{equation}
		E_{\lambda_k}(u_0)\neq E,
	\end{equation}
	sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
	\begin{equation}
		E=\bigoplus_{k}E_{\lambda_k}(u_0).
	\end{equation}
	Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point~\ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
	\begin{equation}
		\left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
	\end{equation}
	Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
	Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).

	Tant que \( 1\neq -1\), \( X^2-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

	Par exemple si le corps est de caractéristique \( 2\), nous avons
	\begin{subequations}
		\begin{align}
			A   & =\begin{pmatrix}
				       1 & 1 \\
				       0 & 1
			       \end{pmatrix}               \\
			A^2 & =\begin{pmatrix}
				       1 & 2 \\
				       0 & 1
			       \end{pmatrix}=\begin{pmatrix}
				                     1 & 0 \\
				                     0 & 1
			                     \end{pmatrix}.
		\end{align}
	\end{subequations}
	Cette matrice \( A\) représente donc une involution, mais n'est pas diagonalisable.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, pas toujours}
%---------------------------------------------------------------------------------------------------------------------------

Il n'est pas vrai qu'une matrice de \( \eM(n,\eC)\) soit toujours diagonalisable. En effet le théorème~\ref{ThoDigLEQEXR}\ref{ItemThoDigLEQEXRii} dit qu'une matrice est diagonalisable si et seulement si son polynôme minimal est scindé à racines simples. Certes sur \( \eC\) le polynôme minimal sera scindé, mais il ne sera pas spécialement à racines simples.

\begin{example}
	La matrice
	\begin{equation}
		A=\begin{pmatrix}
			0 & 1 \\
			0 & 0
		\end{pmatrix}
	\end{equation}
	a pour polynôme caractéristique \( \chi_A(X)=X^2\). C'est également son polynôme minimal, qui n'est pas à racine simple.

	Il est par ailleurs facile de voir que le seul espace propre de \( A\) est \( \Span\{ (1,0) \}\) (ici le span est sur \( \eC\)). Donc l'espace \( \eC^2\) ne possède pas de base de vecteurs propres de \( A\).
\end{example}

Ce qui est vrai, c'est que le polynôme caractéristique a des racines, et que ces racines correspondent à des vecteurs propres. Mais il n'y a pas toujours autant de vecteurs propres que la multiplicité des racines.

\begin{normaltext}
	Lorsque la diagonalisation n'est pas possible, il est souvent possible de trigonaliser. Les matrices triangulaires ne sont pas aussi faciles à manipuler que les matrices diagonales, mais c'est toujours ça de pris.

	Nous étudierons ça plus tard, en \ref{SUBSECooMCOGooEoQCsz} parce que ça va nécéssiter le théorème de d'Alembert.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]  \label{LemSchureRelnrqfiy}
	Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
	\begin{equation}        \label{EqMtrTSqRTA}
		Q^{-1}AQ=\begin{pmatrix}
			\lambda_1 & *      & *         & *               & *              \\
			0         & \ddots & \ddots    & \ddots          & \vdots         \\
			0         & 0      & \lambda_r & *               & *              \\
			0         & 0      & 0         & \begin{pmatrix}
				                                 a_1 & b_1 \\
				                                 c_1 & d_1
			                                 \end{pmatrix} & *                \\
			0         & 0      & 0         & 0               & \begin{pmatrix}
				                                                   a_s & b_s \\
				                                                   c_s & d_s
			                                                   \end{pmatrix}
		\end{pmatrix}.
	\end{equation}
	Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}
\index{lemme!Schur réel}

\begin{proof}
	Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
	\begin{equation}
		Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta u),
	\end{equation}
	et en égalisant les parties réelles et imaginaires,
	\begin{subequations}
		\begin{align}
			Au & =\alpha u-\beta v  \\
			Av & =\alpha v+\beta u.
		\end{align}
	\end{subequations}
	Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

	Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

	L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
	\begin{equation}
		Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
	\end{equation}
	La matrice \( Q^{-1}AQ\) est donc de la forme
	\begin{equation}
		Q^{-1} AQ=\begin{pmatrix}
			\begin{pmatrix}
				\cdot & \cdot \\
				\cdot & \cdot
			\end{pmatrix} & C_1    \\
			0                & A_1
		\end{pmatrix}
	\end{equation}
	où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

	Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjugués.

	En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( Q^{-1}AQ\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
	Une matrice symétrique réelle,
	\begin{enumerate}
		\item       \label{ITEMooJWHLooSfhNSW}
		      a un spectre contenu dans \( \eR\)
		\item       \label{ITEMooMWWRooXxGONW}
		      est diagonalisable par une matrice orthogonale.
	\end{enumerate}
	Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
	Soit \( A\) une matrice réelle symétrique. Elle agit sur l'espace \( \eC^n\) par la définition \ref{DEFooJVOAooUgGKme}, et en particulier la formule \ref{EQooQFVTooMFfzol}. Nous munissons de plus \( \eC^n\) de la forme sesquilinéaire définie en la proposition \ref{PROPooMWUCooMbJuaJ}.

	Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\), et \( \lambda\) est réelle.

	Le lemme de Schur réel~\ref{LemSchureRelnrqfiy} donne une matrice orthogonale \( Q\) qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( Q^{-1}AQ\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( Q^{-1}AQ\) est symétrique :
	\begin{equation}
		(Q^{-1}AQ)^t=Q^tA^t(Q^{-1})^t=Q^{-1}A^tQ=Q^{-1}AQ
	\end{equation}
	où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

	En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce qu'en utilisant la proposition \ref{PROPooNARVooEuhweD},
	\begin{equation}
		\langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
	\end{equation}
\end{proof}
Le théorème spectral pour les opérateurs autoadjoints sera traité plus bas parce qu'il a besoin de notions sur les formes bilinéaires, théorème~\ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchainement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
	Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

	En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
	\begin{equation}
		\begin{aligned}[]
			\begin{pmatrix}
				a & * & * \\
				b & * & * \\
				c & * & *
			\end{pmatrix}
			\begin{pmatrix}
				\lambda_1 & 0         & 0             \\
				0         & \lambda_2 & 0             \\
				0         & 0         & \lambda_3 & 0
			\end{pmatrix}
			\begin{pmatrix}
				a & b & c \\
				* & * & * \\
				* & * & *
			\end{pmatrix} & =
			\begin{pmatrix}
				a & * & * \\
				b & * & * \\
				c & * & *
			\end{pmatrix}
			\begin{pmatrix}
				\lambda_1a & \lambda_1b & \lambda_1c \\
				*          & *          & *          \\
				*          & *          & *
			\end{pmatrix}                                 \\
			                & =\begin{pmatrix}
				                   \lambda_1 a^2+* & \lambda_1ab+* & \lambda_1ac  +* \\
				                   \ldots          & \ldots        & \ldots          \\
				                   \ldots          & \ldots        & \ldots
			                   \end{pmatrix}.
		\end{aligned}
	\end{equation}
	Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}



\begin{proposition}     \label{PROPooQHHPooSqpgcb}
	Une forme bilinéaire est non-dénénérée\footnote{Définition \ref{DEFooNUBFooLfCqaK}.} si et seulement si sa matrice associée est inversible.
\end{proposition}

\begin{proof}
	Nous savons que la matrice associée est symétrique et qu'elle peut donc être diagonalisée (théorème~\ref{ThoeTMXla}). En nous plaçant dans une base de diagonalisation, nous devons prouver que la forme est non-dégénérée si et seulement si les éléments diagonaux de la matrice sont tous non nuls.

	Écrivons \( b(x,z)\) en choisissant pour \( z\) le vecteur de base \( e_k\) de composantes \( (e_k)_j=\delta_{kj}\) :
	\begin{equation}
		b(x,e_k)=\sum_{ij}x_i(e_k)_j
		=\sum_i b_{ik}x_i
		=b_{kk}x_k.
	\end{equation}
	Si \( b\) est dégénérée et si \( x\) est un vecteur non nul (disons que la composante \( x_i\) est non nulle) de \( E\) tel que \( b(x,z)=0\) pour tout \( z\in E\), alors \( b_{ii}=0\), ce qui montre que la matrice de \( b\) n'est pas inversible.

	Réciproquement si la matrice de \( b\) est inversible, alors tous les \( b_{kk}\) sont différents de zéro, et le seul vecteur \( x\) tel que \( b_{kk}x_k=0\) pour tout \( k\) est le vecteur nul.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice définie positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Matrice définie positive, opérateur défini positif]    \label{DefAWAooCMPuVM}
	Un opérateur sur un espace vectoriel sur \( \eC\) ou \( \eR\) est \defe{défini positif}{opérateur!défini positif} si toutes ses valeurs propres sont réelles et strictement positives.  Il est \defe{semi-défini positif}{semi-défini positif} si ses valeurs propres sont réelles positives ou nulles.

	Mêmes définitions pour une matrice.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} défini positif pour positif.

\begin{normaltext}      \label{NORMooAJLHooQhwpvr}
	Quelques ensembles de matrices symétriques.
	\begin{enumerate}
		\item
		      \( S(n,\eR)\) est l'ensemble des matrices symétriques réelles \( n\times n\).
		\item
		      \( S^+(n,\eR)\) l'ensemble des matrices réelles symétriques \( n\times n\) définies positives.
		\item
		      \( S^{++}(n,\eR)\) le sous-ensemble de \( S^+(n,\eR)\) des matrices strictement définies positives.
	\end{enumerate}
\end{normaltext}
\nomenclature[B]{\(  S^+(n,\eR)\)}{matrices symétriques définies positives}
\nomenclature[B]{\(  S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives}

\begin{remark}
	Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

\begin{proposition}     \label{PropcnJyXZ}
	Soit \( M\), une matrice symétrique. Nous avons
	\begin{enumerate}
		\item       \label{ITEMooTJVQooYmRkas}
		      \( \det(M)>0\) et \( \tr(M)>0\) implique \( M\) définie positive\footnote{Définition~\ref{DefAWAooCMPuVM}.},
		\item
		      \( \det(M)>0\) et \( \tr(M)<0\) implique \( M\) définie négative,
		\item       \label{ItemluuFPN}
		      \( \det(M)<0\) implique ni semi-définie positive, ni définie négative
		\item
		      \( \det(M)=0\) implique \( M\) semi-définie positive ou semi-définie négative.
	\end{enumerate}
\end{proposition}

Lorsqu'un énoncé parle d'une matrice symétrique, le premier réflexe est de la diagonaliser : considérer une matrice orthogonale \( Q\) telle que \( Q^tMQ=D\) avec \( D\) diagonale. Et les valeurs propres sur la diagonale : \( D_{kk}=\lambda_k\). Les matrices symétriques définies positives ont cependant des propriétés même en dehors de leur base de diagonalisation.

Pour rappel, \( \langle x, y\rangle \) est le produit scalaire dans \( \eR^n\) défini par la proposition \ref{PROPooSKVRooDGVCYj}.

\begin{lemma}   \label{LemWZFSooYvksjw}
	Soit une matrice symétrique \( M\).
	\begin{enumerate}
		\item       \label{ITEMooSKRAooOgHbGA}
		      Elle est strictement définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\) non nul dans \( \eR^n\).
		\item       \label{ITEMooMOZYooWcrewZ}
		      Elle est semi-définie positive si et seulement si \( \langle x, Mx\rangle \geq 0\) pour tout \( x\) non nul dans \( \eR^n\).
		\item        \label{ITEMooRRMFooHSOHxZ}
		      Si elle est seulement définie positive, alors \( \langle x, Mx\rangle \geq \lambda\| x \|^2\) dès que \( \lambda\geq 0\) minore toutes les valeurs propres.
	\end{enumerate}
\end{lemma}

\begin{proof}
	Démonstration en trois parties.
	\begin{subproof}
		\spitem[\ref{ITEMooSKRAooOgHbGA}]
		Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral~\ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
		\begin{equation}
			\langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
		\end{equation}
		où les \( \lambda_i\) sont les valeurs propres de \( M\).  Le produit\( \langle x,Mx \rangle\) est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont strictement positifs.

		\spitem[\ref{ITEMooMOZYooWcrewZ}]
		Nous avons encore
		\begin{equation}
			\langle x, Mx\rangle =\sum_{i}\lambda_ix_i^2
		\end{equation}
		qui est plus grand ou égal à zéro si et seulement si tous les \( \lambda_i\) sont plus grands ou égaux à zéro.

		\spitem[\ref{ITEMooRRMFooHSOHxZ}]

		Soit une matrice orthogonale \( T\) diagonalisant \( M\), c'est-à-dire telle que \( T^tMT=D\) avec \( D\) diagonale. Nous allons vérifier que si \( \lambda\leq\min\{ \lambda_i \}\), alors
		\begin{equation}        \label{EQooOSFEooCoPuuG}
			\langle Tx, MTx\rangle \geq \lambda\| Tx \|^2
		\end{equation}
		pour tout \( x\). Si nous considérons la base de diagonalisation \( \{ e_k \}\) pour les valeurs propres \( \lambda_k\), nous avons le calcul
		\begin{subequations}     \label{SUBEQSooHBIYooSpkYAl}
			\begin{align}
				\langle Tx, MTx\rangle & = \langle x, T^tMTx\rangle                                                                                     \\
				                       & = \langle x, Dx\rangle                                                                                         \\
				                       & = \sum_k\langle x, x_kDe_k\rangle                                                                              \\
				                       & = \sum_k\lambda_kx_k \underbrace{\langle x, e_k\rangle }_{=x_k}                                                \\
				                       & \geq \sum_k\lambda| x_k |^2                                     & \text{en posant} \lambda=\min\{ \lambda_i \}
			\end{align}
		\end{subequations}
		Nous avons donc
		\begin{equation}
			\langle Tx, MTx\rangle   \geq \sum_k\lambda| x_k |^2 = \lambda\| x \|^2 = \lambda\| Tx \|^2.
		\end{equation}
		Au dernier passage nous avons utilisé le fait que \( T\) est une isométrie (proposition~\ref{PropKBCXooOuEZcS}). L'inéquation \eqref{EQooOSFEooCoPuuG} est démontrée.

		Comme \( T\) est une bijection \footnote{Une matrice orthogonale a un déterminant qui vaut \( \pm 1\).}, cela implique le résultat pour tout \( x\).
	\end{subproof}
\end{proof}

Les personnes qui aiment les vecteurs lignes et colonnes écriront des inégalités comme
\begin{equation}
	x^tMx\geq x^tx.
\end{equation}
Tout à l'autre bout du spectre des personnes névrosées des notations, on trouvera des inégalités comme
\begin{equation}
	M(x\otimes x)\geq x\cdot x.
\end{equation}
Le penchant personnel de l'auteur de ces lignes est la notation avec le produit tensoriel. Si vous aimez ça, vous pouvez lire la section \ref{SECooUKRYooZjagcX} et en particulier ce qui suit \eqref{EQooUNRYooKBrXyK}.

La notation adoptée ici avec le produit scalaire \( \langle x, Mx\rangle \), qui peut aussi être écrite \( x\cdot Mx\) est entre les deux. Elle a l'avantage de n'être pas technologique comme le produit tensoriel (si vous y mettez les pieds, vous devez savoir ce que vous faites), tout en évitant de se casser la tête à savoir qui est un vecteur ligne ou un vecteur colonne.

\begin{proposition}     \label{PROPooUAAFooEGVDRC} \label{PROPooNQSXooVMFAtU}
	Une application bilinéaire est définie positive\footnote{Définition \ref{DEFooJIAQooZkBtTy}.} si et seulement si sa matrice symétrique associée\footnote{Par la proposition \ref{PropFSXooRUMzdb}.} l'est.
\end{proposition}

\begin{proof}
	La définition \ref{DEFooJIAQooZkBtTy} dit que \( b\) est strictement définie positive lorsque \( b(x,x)\geq 0\) et \( b(x,x)=0\) si et seulement si \( x=0\).

	D'autre part, le lemme \ref{LemWZFSooYvksjw} dit que la matrice \( B\) est strictement définie positive lorsque \( x\cdot Bx\geq 0\) et \( x\cdot Bx=0\) si et seulement si \( x=0\).

	Le lien entre les deux est que le lemme \ref{LEMooDCIOooTlVZMR} nous enseigne que pour tout \( x\) et \( y\),
	\begin{equation}
		b(x,y)=x\cdot By
	\end{equation}
	où \( B\) est la matrice de \( b\).
\end{proof}

\begin{proposition}     \label{PROPooCIEUooODqfwm}
	Soit une forme quadratique \( q\colon E\to \eK\) et sa matrice\footnote{Matrice associée à une forme quadratique, définition \ref{DEFooAOGPooXWXUcN}.} \( (q_{ij})\in \eM(n,\eK)\). Nous avons
	\begin{equation}		\label{EQooAJTUooBSAhKv}
		q(x)  =\sum_{i=1}^n\sum_{j=1}^nq_{ij}x_ix_j                         =\sum_{i=1}^nq_{ii}x_i^2+2\sum_{1\leq i <j\leq n}q_{ij}x_ix_j.
	\end{equation}
\end{proposition}

\begin{proof}
	La première égalité est seulement l'expression de la matrice de \( q\). Pour la seconde, on découpe la somme sur \( j\) en trois parties : \( j<i\), \( j=i\) et \( j>i\) :
	\begin{equation}		\label{EQooMKMDooGKqbJi}
		q(x)=\sum_i\sum_{j<i}q_{ij}x_ix_j+\sum_iq_{ii}x_i^2+\sum_i\sum_{j>i}q_{ij}x_ix_j.
	\end{equation}
	Nous pouvons utiliser le lemme \ref{LEMooGAMAooOAFhrc} en posant \( A=\{ (i,j)\tq j<i \}\), \( B=\{ (i,j)\tq j>i \}\), et en considérant la bijection
	\begin{equation}
		\begin{aligned}
			\varphi\colon A & \to b          \\
			(i,j)           & \mapsto (j,i).
		\end{aligned}
	\end{equation}
	Nous avons alors
	\begin{equation}
		\sum_{(i,j)\in A}q_{ij}x_ix_j=\sum_{(i,j)\in B}q_{ji}x_jx_i=\sum_{(i,j)\in B}q_{ij}x_ix_j.
	\end{equation}
	La première égalité est le lemme et la seconde est la symétrie de \( q\). Bref, dans \eqref{EQooMKMDooGKqbJi}, le premier et le dernier termes sont égaux.
\end{proof}

\begin{normaltext}
	De nombreux auteurs préfèrent écrire des choses comme \( x^tBy\) ou \( xB^ty\) ou \( xBy^t\) et se poser de longues questions sur qui est un «vecteur colonne» et qui est un «vecteur ligne», et si la matrice \( B\) soit être transposée ou non. Toutes ces notations servent(?) à cacher un bête produit scalaire.
\end{normaltext}

\begin{normaltext}
	Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire.
\end{normaltext}

\begin{corollary}
	Une matrice symétrique strictement définie positive est inversible.
\end{corollary}

\begin{proof}
	Si \( Ax=0\) alors \( \langle Ax, x\rangle =0\). Mais dans le cas d'une matrice strictement définie positive, cela implique \( x=0\) par le lemme~\ref{LemWZFSooYvksjw}.
\end{proof}

\begin{lemma}
	Pour une base quelconque, les éléments diagonaux d'une matrice symétrique semi-définie positive sont positifs. Si la matrice est strictement définie positive, alors les éléments diagonaux sont strictement positifs.
\end{lemma}

\begin{proof}
	Il s'agit d'une application du lemme~\ref{LemWZFSooYvksjw}. Si \( A\) est définie positive et que \( \{ e_i \}\) est une base, alors
	\begin{equation}
		A_{ii}=\langle Ae_i, e_i\rangle \geq \lambda\| e_i \|^2=\lambda\geq 0.
	\end{equation}
	Si \( A\) est strictement définie positive, alors \( \lambda\) peut être choisi strictement positif.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Réduction de Gauss}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Réduction de Gauss\cite{BIBooNUUEooJUjLpy,BIBooUULNooUtlrar}]     \label{THOooOMMFooKxqICS}
	Soit une forme quadratique non nulle \( q\) sur l'espace vectoriel \( E\) sur le corps \( \eK\). Il existe une base  \(\{ l_i \}_{i=1,\ldots, n}\) de \( E^*\) et des coefficients \( \alpha_i\in \eK\) tels que
	\begin{equation}
		q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
	\end{equation}
\end{theorem}

\begin{proof}
	Notre point de départ sont les formules \eqref{EQooAJTUooBSAhKv} pour la forme quadratique. Nous allons faire la preuve par récurrence sur la dimension de l'espace. Si \( n=1\), alors nous avons seulement
	\begin{equation}
		q(x)=\alpha x^2
	\end{equation}
	et donc le théorème est fait avec \( l(x)=x\).

	Nous supposons que le théorème est prouvé pour tout espace de dimension \( n\). Une forme quadratique pour un espace de dimension \( n+1\) s'écrit
	\begin{equation}
		q(x)=\sum_{i=1}^{n+1}m_{ii}x_i^2+2\sum_{1\leq i < j\leq n+1}m_{ij}x_ix_j.
	\end{equation}
	Vu que \( q\) est non nulle, un des \( m_{ij}\) est non nul. Nous allons diviser en plusieurs cas.
	\begin{itemize}
		\item
		      \( m_{11}\neq 0\)
		\item
		      \( m_{kk}\neq 0\) avec \( k\neq 1\)
		\item
		      \( m_{12}\neq 0\) et \( m_{ii}=0\) pour tout \( i\).
		\item
		      \( m_{kl}\neq 0\) avec \( (k,l)\neq (1,2)\) et \( m_{ii}=0\) pour tout \( i\).
	\end{itemize}
	Ces cas ne sont pas exclusifs, mais ils couvrent toutes les possibilités.

	\begin{subproof}
		\spitem[Si \( m_{11}\neq 0\)]
		Nous écrivons \( q\) sous la forme
		\begin{subequations}
			\begin{align}
				q(x) & =m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big)                     \\
				     & =m_{11}x_1^2+\sum_{i=2}^{n+1}m_{ii}x_i^2+2\sum_{j=2}^{n+1}m_{1j}x_1x_j+2\sum_{i=2}^n\sum_{j=i+1}^{n+1}(m_{ij}x_ix_j) \\
				     & =m_{11}x_1^2+2x_1\sum_{j=2}^{n+1}m_{1j}x_k+R(x_2,\ldots, x_{n+1})                                                    \\
				     & =m_{11}\left( x_1^2+2x_1\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j \right)+R(x_2,\ldots, x_{n+1})                  \\
				     & =m_{11}\big( x_1^2+2x_1f(x_2,\ldots, x_{n+1}) \big)+R(x_2,\ldots, x_{n+1})                                           \\
				     & =m_{11}\big( x_1+f(x_2,\ldots, x_{n+1}) \big)^2-m_{11}f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})
			\end{align}
		\end{subequations}
		où
		\begin{itemize}
			\item \( R\) est une forme quadratique de \( n-1\) variables;
			\item nous avons noté \( f(x_2,\ldots, x_{n+1})=\sum_{j=2}^{n+1}\frac{ m_{1j} }{ m_{11} }x_j\).
		\end{itemize}
		Maintenant, toute la partie \( -f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})\) est une forme quadratique de \( n\) variables. Par hypothèse de récurrence, il existe des coefficients \( \alpha_i\) et des formes linéairement indépendantes sur \( \eK^n\) \( l_i'(x_2,\ldots, x_{n+1})\) telles que
		\begin{equation}
			-f(x_2,\ldots, x_{n+1})^2+R(x_2,\ldots, x_{n+1})=\sum_{i=2}^{n+1}\alpha_il_i'(x_2,\ldots, x_{n+1})^2.
		\end{equation}
		En posant ensuite \( l_j(x_1,\ldots, x_{n+1})=l'_j(x_2,\ldots, x_{n+1})\), ainsi que \( l_1(x_1,\ldots, x_{n+1})=x_1+f(x_2,\ldots, x_{n+1})\), nous avons
		\begin{equation}
			q(x)=m_{11}l_1(x)^2+\sum_{j=2}^{n+1}\alpha_jl_j(x)^2.
		\end{equation}

		\spitem[Si \( m_{kk}\neq 0\) avec \( k\neq 1\)]

		Nous nommons \( k\) le plus petit entier pour lequel \( m_{kk}\neq 0\), et nous supposons que \( k\neq 1\), parce que nous avons déjà couvert ce cas. Dans ce cas, nous avons
		\begin{equation}
			q(x)=m_{kk}x_k^2+\sum_{j=k+1}^{n+1}m_{jj}x_j^2  +2\sum_{i=1}^n\big( \sum_{j=i+1}^{n+1}m_{ij}x_ix_j \big),
		\end{equation}
		et tout tourne comme dans le premier cas.
		\spitem[\( m_{ii}=0\) pour tout \( i\) et \( m_{12}\neq 0\)]
		Nous écrivons \( q\) en séparant les termes \( m_{1k}\) :
		\begin{subequations}
			\begin{align}
				q(x) & =2\sum_{1\leq i<j\leq n+1}m_{ij}x_ix_j                                                                                       \\
				     & =2m_{12}x_1x_2+2\sum_{2\leq j\leq n+1}m_{1j}x_1x_j+2\sum_{2\leq i<j\leq n+1}m_{ij}x_ix_j                                     \\
				     & =2m_{12}x_1x_2+2x_1\sum_{2\leq j\leq n+1}m_{1j}x_j+2\sum_{3\leq j\leq n+1}m_{2j}x_2x_j+2\sum_{3\leq i<j\leq n+1}m_{ij}x_ix_j \\
				     & =2m_{12}x_1x_2+x_1f(x_2,\ldots, x_{n+1})+x_2g(x_3,\ldots, x_{n+1})+T(x_3,\ldots, x_{n+1})      \label{SUBEQooLBXBooXoLyuw}
			\end{align}
		\end{subequations}
		où \( f\) et \( g\) sont linéaires et \( T\) est multilinéaire.

		À ce moment, nous tentons de factoriser toute la partie concernant \( x_1\) et \( x_2\). L'idée est d'utiliser ceci :
		\begin{equation}
			(x_1+g)(x_2+f)=x_1x_2+x_1f+x_2g+fg,
		\end{equation}
		mais en mettant les bons coefficients pour reproduire ce que nous avons dans \eqref{SUBEQooLBXBooXoLyuw} :
		\begin{equation}
			(2m_{12}+2g)(x_2+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }=2m_{12}x_1x_2+2x_1f+2x_2g.
		\end{equation}
		Cela pour dire que
		\begin{equation}
			q(x)=2(m_{12}x_1+g)(x_2+\frac{ f }{ m_{12} })-\frac{ 2fg }{ m_{12} }+T
		\end{equation}
		où \(-2fg/m_{12}+T\) est une forme quadratique de \( x_3,\ldots, x_{n+1}\), c'est-à-dire de \( n-1\) variables.

		L'hypothèse de récurrence nous donne des formes linéaires \( (l_i)_{i=3,\ldots, n+1}\) telles que
		\begin{equation}
			\frac{ 2fg }{ m_{12} }+T=\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
		\end{equation}
		Nous pouvons donc déjà écrire
		\begin{equation}
			q(x)=2l'_1(x)l'_2(x)+\sum_{i=3}^{n+1}\alpha_il_i(x)^2
		\end{equation}
		où
		\begin{itemize}
			\item Les forme \( l_i\) avec \( i\geq 3\) ne dépendent pas de \( x_1\) et \( x_2\), et sont donc indépendantes de \( l_1\) et \( l_2\).
			\item La forme \( l'_1\) ne dépend pas de \( x_2\),
			\item La forme \( l'_2\) ne dépend pas de \( x_1\).
		\end{itemize}
		Ce sont donc \( n+1\) formes linéaires indépendantes. Le seul problème résiduel est que les formes \( l'_1\) et \( l'_2\) arrivent en produit l'une de l'autre. Nous en définissons donc deux de plus :
		\begin{equation}
			\begin{aligned}[]
				l_1(x)=\frac{ 1 }{2}(l'_1+l'_2) \\
				l_2(x)=\frac{ 1 }{2}(l'_1-l'_2),
			\end{aligned}
		\end{equation}
		qui sont linéairement indépendantes l'une de l'autre et indépendantes des \( l_i\) (\( i\geq 3\)). Au final,
		\begin{equation}
			q(x)=l_1(x)^2+l_2(x)^2+\sum_{i=3}^{n+1}\alpha_il_i(x)^2.
		\end{equation}
		\spitem[Si \( m_{ii}=0\) et \( m_{12}=0\) et \( m_{kl}\neq 0\) avec \( k<l\)]
		Nous considérons la permutation
		\begin{equation}
			\begin{aligned}
				\sigma\colon \{ 1,\ldots, n+1 \} & \to \{ 1,\ldots, n+1 \}    \\
				i                                & \mapsto \begin{cases}
					                                           1 & \text{si } i=k \\
					                                           2 & \text{si } i=l \\
					                                           k & \text{si } i=1 \\
					                                           l & \text{si } i=2 \\
					                                           i & \text{sinon,}
				                                           \end{cases}
			\end{aligned}
		\end{equation}
		c'est-à-dire que \( \sigma\) permute \( 1\) et \( k\) ainsi que \( 2\) et \( l\). Ensuite nous posons
		\begin{equation}
			\begin{aligned}
				s\colon \eR^{n+1} & \to \eR^{n+1}          \\
				e_i               & \mapsto e_{\sigma(i)}.
			\end{aligned}
		\end{equation}
		Nous allons un peu considérer \( q\circ s\), pour changer :
		\begin{equation}        \label{EQooLVAWooAirEzP}
			(q\circ s)(x)=\sum_{i,j}m_{ij}s(x)_is(x)_j=\sum_{ij}x_{\sigma(i)}x_{\sigma(j)}.
		\end{equation}
		parce que \( s(x)_i=x_{\sigma(i)}\).

		Utilisons un petit abus de notation pour considérer
		\begin{equation}
			\begin{aligned}
				\sigma\colon \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \} & \to \{ 1,\ldots, n+1 \}\times \{ 1,\ldots, n+1 \} \\
				(i,j)                                                      & \mapsto \big(\sigma(i), \sigma(j)\big).
			\end{aligned}
		\end{equation}
		Cela est une bijection; nous pouvons utiliser le lemme \ref{DEFooLNEXooYMQjRo} pour permuter les termes dans \eqref{EQooLVAWooAirEzP} :
		\begin{subequations}
			\begin{align}
				(q\circ s)(x) & =\sum_{ij}m_{\sigma(i)\sigma(j)}x_{\sigma\sigma(i)}x_{\sigma\sigma(j)} \\
				              & =\sum_{ij}a_{ij}x_ix_j     \label{EQooPCTCooFnMWat}
			\end{align}
		\end{subequations}
		où nous avons posé \( a_{ij}=m_{\sigma(i)\sigma(j)}\) et utilisé le fait que \( \sigma=\sigma^{-1}\). Le point intéressant de l'histoire est que dans \eqref{EQooPCTCooFnMWat}, \( a_{12}=m_{kl}\neq 0\). La forme \( q\circ s\) est donc dans le cas déjà traité et il existe des formes linéaires \( l'_i\) telles que
		\begin{equation}
			(q\circ s)(x)=\sum_{i=1}^{n+1}\alpha_il'_i(x)^2.
		\end{equation}
		En évaluant cela en \( s(x)\), et en tenant compte de \( s=s^{-1}\), nous trouvons
		\begin{equation}
			q(x)=\sum_i\alpha_i(l_i\circ s)(x)^2,
		\end{equation}
		de telle sorte que \( l_i=l'_i\circ s\) soit la réponse à notre théorème.
	\end{subproof}
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Orthogonalité pour une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{BIBooUULNooUtlrar}]       \label{PROPooYXMMooYIuGRd}
	Soient un espace vectoriel \( (E,\eK)\) et une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\). Une base de \( E\) est \( q\)-orthogonale\footnote{Définition \ref{DEFooGECOooCCGVXG}.} si et seulement si la matrice de \( q\) dans cette base est diagonale.
\end{proposition}

\begin{proof}
	La matrice de \( q\) est donnée par \( Q_{ij}=b(e_i,e_j)\). Donc oui, cette matrice est diagonale si et seulement si les \( e_i\) sont orthogonaux.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]		\label{PROPooRERSooFHwWtB}
	Si \( b\) est une forme bilinéaire symétrique non dégénérée, et si \( q\) est la forme quadratique associée, alors il existe une base \( q\)-orthogonale.

	Les vecteurs de cette base ne sont pas isotropes\footnote{Définition \ref{DefVKMnUEM}.}
\end{proposition}

\begin{proof}
	Nous considérons une base \( \{e_i\}\) de \( E\). Vu que \( b\) est symétrique, elle est donnée par une matrice symétrique selon la formule \( b(x,y)=\sum_{kl}S_{kl}x_ky_l\). Le théorème \ref{ThoeTMXla} permet de diagonaliser \( S\) par une matrice orthogonale : il existe une matrice diagonale \( D\) et une matrice orthogonale \( A\) telles que \( S=A^tDA\).

	Nous posons alors \( s_i=Ae_i\), et nous vérifions que c'est bon. Nous avons :
	\begin{subequations}
		\begin{align}
			b(s_i,s_j) & =\sum_{kl}S_{kl}(s_i)_k(s_j)_l     \\
			           & =\sum_{kl}S_{kl}(Ae_i)_k(Ae_j)_l   \\
			           & =\sum_{kl}S_{kl}A_{ki}A_{lj}       \\
			           & = \sum_{kl} (A^t)_{ik}S_{kl}A_{lj} \\
			           & =D_{ij}.
		\end{align}
	\end{subequations}
	Vu que \( b\) n'est pas dégénérée, les éléments diagonaux de \( D\) ne sont pas nuls. Donc les vecteurs \( s_i\) vérifient \( b(s_i,s_i)\neq 0\).
\end{proof}

\begin{proposition}     \label{PROPooZYISooQZNgeo}
	Soit une forme quadratique \( q\). Si une base \( (e_i )\) de \( E\) est \( q\)-orthogonale, alors \( \mB=\{ e_i\tq q(e_i)=0 \}\) est une base de \( \ker(q)\).
\end{proposition}

\begin{proof}
	Nous considérons un vecteur de base \( e_j\), et nous montrons que \( q(e_j)=0\) si et seulement si \( e_j\in\ker(q)\). Nous savons par la proposition \ref{PROPooYXMMooYIuGRd} que la matrice de \( q\) dans la base \( (e_i)\) est diagonale et que les éléments diagonaux sont les \( q(e_i)\). Soit \( K=\{ i\tq q(e_i)=0 \}\).
	\begin{subproof}
		\spitem[\( \Span\{ e_i \}_{i\in K}\subset\ker(q)\)]
		Si \( x=\sum_{i\in K}x_ie_i\), alors
		\begin{equation}
			q(x)=b(x,x)=\sum_{i,j\in K}| x_i |^2b(e_i,e_j)=\sum_{i,j\in K}| x_i |^2\delta_{ij}q(e_i)=0
		\end{equation}
		parce que \( q(e_i)=0\) dès que \( i\in K\).
		\spitem[\( \ker(q)\subset\Span\{ e_i \}_{i\in K}\)]
		Soit \( x\in \ker(q)\) et écrivons-le sous la forme \( x=\sum_{i=1}^nx_ie_i\). Nous avons
		\begin{equation}
			0=q(x)=\sum_i| x_i |^2q(e_i).
		\end{equation}
		Mais \(    | x_i |^2\geq 0 \) et \( q(e_i)\geq 0\), donc si \( q(e_i)\neq 0\), alors \( x_i=0\). Donc les seules composantes non nulles de \( x\) sont celles sur lesquelles \( q\) s'annule. En d'autres termes \( x=\sum_ix_ie_i\in \Span\{ e_i \}_{i\in K}\).
	\end{subproof}
\end{proof}

\begin{theorem}[\cite{BIBooUULNooUtlrar,MonCerveau}]       \label{THOooIDMPooIMwkqB}
	Toute forme quadratique sur un espace vectoriel de dimension finie admet une base formée de vecteurs \( 2\) à \( 2\) orthogonaux (pour la forme considérée).
\end{theorem}

\begin{proof}
	Nous considérons la base \(  \{ l_i \}    \) de \( E^*\) donnée par la réduction de Gauss (théorème \ref{THOooOMMFooKxqICS}). La forme quadratique \( q\) s'écrit
	\begin{equation}
		q(x)=\sum_{i=1}^n\alpha_il_i(x)^2.
	\end{equation}
	La base préduale\footnote{Définition, existence, unicité dans la proposition \ref{PROPooDBPGooPagbEB}.} \( \{ e_i \}\) de \( \{ l_i \}\) répond aux conditions. Pour le vérifier, nous considérons la forme bilinéaire associée à \( q\) par l'identité de polarisation \ref{PROPooZLXVooOsXCcB} :
	\begin{equation}
		b(e_i,e_j)=\frac{ 1 }{2}\big( q(e_i)+q(e_j)-q(e_i-e_j) \big).
	\end{equation}
	Vu que \( l_k(e_i)=\delta_{ki}\), nous avons
	\begin{equation}
		q(e_i)=\sum_{k=1}^n\alpha_kl_k(e_i)^2=\alpha_i.
	\end{equation}
	En utilisant la linéarité,
	\begin{subequations}
		\begin{align}
			q(e_i-e_j) & =\sum_k\alpha_kl_k(e_i-e_j)^2                                    \\
			           & =\sum_k\alpha_k(\delta_{ki}-\delta_{kj})^2                       \\
			           & =\sum_k\alpha_k(\delta_{ki}+\delta_{kj}-2\delta_{ki}\delta_{kj}) \\
			           & =\alpha_i+\alpha_j-2\delta_{ij}\alpha_i.
		\end{align}
	\end{subequations}
	Donc
	\begin{equation}
		b(e_i,e_j)=\delta_{ij}\alpha_i.
	\end{equation}
	Les vecteurs \( \{ e_i \}\) sont donc bien deux à deux \( q\)-orthogonaux.
\end{proof}

Notons qu'en l'absence de notion de racine carrée sur \( \eK\), il n'est pas possible de considérer \( \sqrt{ \alpha_i }\) et donc de base \( q\)-orthonormée.


\begin{proposition}[\cite{MonCerveau}] \label{PROPooPMYCooAAtHsB}
	Si \( A\in \eM(n,\eK)\) est telle que \( \det(A)=0\), alors il existe des matrices de manipulation de lignes et de colonnes\footnote{Les opérations élémentaires sur les lignes sont résumées en \ref{DEFooKJNNooOcIJuu}.} \( G_1,\ldots, G_p\) et \( H_1,\ldots,H_q\) telles que \( G_1\ldots G_N\,A\, H_1\ldots H_q\) ait une colonne de zéros.
\end{proposition}

\begin{proof}
	Si la matrice \( A\) n'a pas de colonnes de zéros, il existe un \( i\) tel que \( A_{i1}\neq 0\). Nous utilisons une matrice de permutation de ligne \( 1\) et \( i\) :
	\begin{equation}
		\big( S(n,i,1)A \big)_{11}\neq 0.
	\end{equation}
	Nous notons \( s\) ce nombre. Alors en multipliant la première ligne par \( 1/s\) nous avons un \( 1\) dans la première case :
	\begin{equation}
		\big( T(n,1,1/s)S(n,i,1)A \big)_{11}=1.
	\end{equation}
	Pour la suite nous n'allons plus être aussi explicite.

	Maintenant, en faisant des combinaisons entre la première colonne et les autres, nous pouvons annuler toutes les autre entrées \( 1_{1i}\). Tout ça pour dire qu'il existe des matrices \( G_1,\ldots,G_{p'}\) et \( H_1,\ldots,H_{q'}\) telles que
	\begin{equation}
		G_1\ldots G_{p'}A H_1\ldots H_{q'}=
		\begin{pmatrix}
			1              & \begin{matrix}
				                 0 & \ldots & 0
			                 \end{matrix} \\
			\begin{matrix}
				0      \\
				\vdots \\
				0
			\end{matrix} & A^{(1)}
		\end{pmatrix}.
	\end{equation}
	Si \( A^{(1)}\) ne possède pas de colonnes de zéros, nous pouvons continuer.

	Si nous parvenons à faire \( n\) pas de la sorte, alors nous aurions
	\begin{equation}
		G_1\ldots G_{p}A H_1\ldots H_{q}=\delta,
	\end{equation}
	et donc, par le lemme \ref{LEMooDXSZooVpbCRj}, nous aurions \( \det(G_1\ldots G_{p})\det(A)\det(H_1\ldots H_{q})=1\), ce qui est impossible lorsque \( \det(A)=0\). Nous en concluons que le processus doit s'arrêter et qu'une des matrices \( A^{(k)}\) doit avoir une colonne de zéros\footnote{En réalité, le processus tel que nous l'avons décrit ne s'arrête que lorsque la première colonne est remplie de zéros.}.
\end{proof}

\begin{proposition}     \label{PROPooVUDJooLWjmSI}
	Une matrice dont le déterminant est nul n'est pas inversible.
\end{proposition}

\begin{proof}
	Par la proposition \ref{PROPooPMYCooAAtHsB}, il existe des matrices de manipulation de lignes et de colonnes \( G_1,\ldots, G_p\)  telles que la matrice \( G_1\ldots G_p\,A\, H_1\ldots H_q\) ait une colonne de zéros. De là, la proposition \ref{PROPooEOKBooKUROFg} implique que la matrice
	\begin{equation}        \label{EQooQGXBooXxFOtb}
		G_1\ldots G_pA H_1\ldots H_q
	\end{equation}
	n'est pas inversible. Vu les déterminants des matrices \( G_i\),  la proposition \ref{PROPooAVIXooMtVCet} implique que \( G_1\ldots G_p\) et \( H_1\ldots H_q\) sont inversibles. Si \( A\) était inversible, tout le produit serait inversible, ce qui est faux.
\end{proof}

\begin{theorem}     \label{THOooSNXWooSRjleb}
	Une matrice sur un corps commutatif est inversible si et seulement si son déterminant est non nul.
\end{theorem}

\begin{proof}
	Dans un sens c'est la proposition \ref{PROPooAVIXooMtVCet} et dans l'autre sens c'est la proposition \ref{PROPooVUDJooLWjmSI}.
\end{proof}


\begin{proposition}     \label{PROPooHQNPooIfPEDH}
	Soient des matrices \( A\) et \( B\) sur un corps commutatif. Alors
	\begin{equation}
		\det(AB)=\det(A)\det(B).
	\end{equation}
\end{proposition}

\begin{proof}
	Les propositions \ref{PROPooUCZVooPkloQp} et \ref{PROPooWVJFooTmqoec} ont déjà fait une grosse partie du travail. Il ne reste que le cas où \( \det(A)=\det(B)=0\).

	Dans ce cas, les matrices \( A\) et \( B\) ne sont pas inversibles (proposition \ref{THOooSNXWooSRjleb}). Le produit \( AB\) n'est alors pas inversible non plus\footnote{Citez le lemme \ref{LEMooZDNVooArIXzC} si vous voulez justifier ça.}. La proposition \ref{THOooSNXWooSRjleb}, utilisée dans le sens inverse, nous dit alors que \( \det(AB)=0\).

	Au final dans le cas \( \det(A)=\det(B)=0\) nous avons \( 0=\det(AB)=\det(A)\det(B)=0\).
\end{proof}

Faisons maintenant le cas général des manipulations de lignes et colonnes.

\begin{proposition}     \label{PROPooSLLGooSZjQrv}
	Soit une matrice carrée \( A\in \eM(n,\eK)\). La matrice \( B\) obtenue par la substitution simultanée
	\begin{equation}
		C_j\to \sum_ka_{kj}C_k
	\end{equation}
	a pour déterminant
	\begin{equation}
		\det(B)=\det(a)\det(A).
	\end{equation}
\end{proposition}

\begin{proof}
	L'élément \( B_{ij}\) de la matrice \( B\) est une combinaison linéaire de tous les éléments de sa ligne :
	\begin{equation}
		B_{ij}=\sum_ka_{kj}A_{ik}=(Aa)_{ij}.
	\end{equation}
	Donc \( B=Aa\). La proposition \ref{PROPooHQNPooIfPEDH} nous dit alors que \( \det(B)=\det(a)\det(A)\).
\end{proof}

\begin{theorem}[de Sylvester\cite{BIBooXOWGooAPWTfT}]   \label{ThoQFVsBCk}
	Soit \( Q\) une forme quadratique réelle de signature\footnote{Définition \ref{DEFooWDCLooDkRYLK}.} \( (p,q)\) sur \( \eR^n\) (\( n\geq p+q\)). Alors pour toute base \( Q\)-orthogonale \( \{ e_i \}\) de \( \eR^{p+q}\) nous avons les propriétés suivantes.
	\begin{enumerate}
		\item       \label{ITEMooCFQHooRWfmpT}
		      Les nombres \( p\) et \( q\) sont donnée par
		      \begin{subequations}
			      \begin{align}
				      p & =\Card\{ i\tq Q(e_i)>0 \}             \label{SUBEQooONWLooNsgmQY} \\
				      q & =\Card\{ i\tq Q(e_i)<0 \}.        \label{SUBEQooFKXMooOVwvKR}
			      \end{align}
		      \end{subequations}
		\item       \label{ITEMooWLPVooSTOOjL}
		      Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
		      \begin{equation}
			      P^tAP=\begin{pmatrix}
				      -\mtu_q &        &           \\
				              & \mtu_p &           \\
				              &        & 0_{n-p-q}
			      \end{pmatrix}.
		      \end{equation}
		\item       \label{ITEMooGOHCooPrNQwm}
		      Le rang de \( Q\) est \( p+q\).
	\end{enumerate}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}

\begin{proof}
	Soit  \( F\) un sous-espace de dimension maximale \( q\) sur lequel \( Q\) est définie négative. Le fait que la dimension de \( F\) soit \( q\) est la définition \ref{DEFooWDCLooDkRYLK} de la signature. Nous notons \( F^{\perp}\) sont \( Q\)-orthogonal, c'est-à-dire que
	\begin{equation}
		F^{\perp}=\{ v\in E\tq B(v,x)=0\,\forall x\in F \}.
	\end{equation}
	Le lemme \ref{LEMooUOZOooYvEcji} nous assure que \( E=F\oplus F^{\perp}\).

	Le théorème \ref{THOooIDMPooIMwkqB} sur l'existence de bases \( Q\)-orthogonales nous permet de considérer une base \( Q\)-orthogonale de \( F\) et une de \( F^{\perp}\). En réunissant les deux, nous avons une base de \( E\). Nous la notons \( \{ f_1,\ldots, f_n \}\) avec
	\begin{itemize}
		\item La partie \( \{ f_1,\ldots, f_q \}\) est une base de \( F\),
		\item La partie \( \{ f_{q+1},\ldots, f_n \}\) est une base de \( F^{\perp}\),
		\item Remarquez cependant qu'il n'est pas dit que \( n=q+p\).
	\end{itemize}
	Notons que pour \( i>q\), nous avons \( Q(f_i)\geq 0\), sinon la maximalité de \( F\) serait contredite par \( \Span\{ f_1,\ldots, f_q,f_i \}\).

	Cela prouve que
	\begin{equation}
		\Card\{ i\tq Q(f_i) >0\}=p.
	\end{equation}
	Le lemme \ref{LEMooISHCooVDJEKo} nous dit alors que
	\begin{equation}
		\Card\{ i\tq Q(e_i)>0 \}=\Card\{ i\tq Q(f_i) >0\}=p.
	\end{equation}
	C'est l'égalité \eqref{SUBEQooFKXMooOVwvKR}. L'égalité \eqref{SUBEQooONWLooNsgmQY} se prouve de la même façon, en prenant \( F\) maximal pour la propriété que \( Q\) y est strictement définie positive.

	Le point \ref{ITEMooCFQHooRWfmpT} est prouvé.

	Dans une base \( Q\)-orthogonale, la matrice de \( Q\) est diagonale, et contient sur la diagonale les valeurs de \( Q(e_i)\). Parmi celles-ci, on en a \( p\) strictement positives et \( q\) strictement négatives. Les \( n-p-q\) autres sont nulles. Vu que \( Q\) est à valeur réelle, nous avons une notion de racine carré, et nous pouvons considérer \( e_i/\sqrt{ | Q(e_i) | }\) au lieu de \( e_i\). De cette façon, \( Q(e_i)\) est normalisé. Avec ça, la matrice de \( Q\) est
	\begin{equation}        \label{EQooLQNRooCsgKVF}
		D=\begin{pmatrix}
			\mtu_p &         &   \\
			       & -\mtu_q &   \\
			       &         & 0
		\end{pmatrix}.
	\end{equation}
	Nous venons de prouver qu'il existe une base \( \{ e_i \}\) dans laquelle la matrice de \( Q\) est \eqref{EQooLQNRooCsgKVF}. Si \( A\) est la matrice de \( Q\) dans une base quelconque \( \{ f_i \}\) et si \( P\) est la matrice de changement de base \( f_j=\sum_iP_{ij}e_i\), la proposition \ref{PROPooLBIOooUpzxXA} donne \(D= P^tAP\).

	Le point \ref{ITEMooWLPVooSTOOjL} est prouvé.

	Pour \ref{ITEMooGOHCooPrNQwm}, la proposition \ref{PROPooLRZQooSfprff} nous permet de calculer le rang de \( Q\) par le rang de sa matrice dans n'importe quelle base. Nous choisissons la base qui donne la matrice \eqref{EQooLQNRooCsgKVF}. Le rang est alors bien \( p+q\).
\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équivalence de formes quadratiques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Équivalence de forme quadratique\cite{BIBooWVWZooZqliJt}]        \label{DEFooOLWYooMwhMJp}
	Deux formes quadratiques \( q\) et \( q'\) sont \defe{équivalentes}{équivalence de forme quadratiques} si il existe une application linéaire inversible \( \phi\) telle que \( q'=q\circ \phi\).
\end{definition}

\begin{proposition}[\cite{BIBooXOWGooAPWTfT}]       \label{PROPooBWXMooLsgyKm}
	Deux formes quadratiques sont équivalentes\footnote{Définition \ref{DEFooOLWYooMwhMJp}.} si et seulement si elles ont même signature\footnote{Définition \ref{DEFooWDCLooDkRYLK}.}.
\end{proposition}

\begin{proof}
	En deux parties, et en utilisant tout le temps le théorème de Sylvester \ref{ThoQFVsBCk}. Dans la suite nous allons toujours noter de la même façon les formes quadratiques, les applications linéaires et leurs matrices correspondantes.

	\begin{subproof}
		\spitem[\( \Rightarrow\)]
		%-----------------------------------------------------------
		Soient deux formes quadratiques équivalentes \( q\) et \( q'\). Nous avons une application linéaire \( \phi\) telle que \( q'=q\circ \phi\). Soit une matrice inversible \( P\) telle que
		\begin{equation}		\label{EQooTCIRooFtWiTz}
			P^tAP=\begin{pmatrix}
				-\mtu_{q'} &           &   \\
				           & \mtu_{p'} &   \\
				           &           & 0
			\end{pmatrix}.
		\end{equation}
		où \( (p',q')\) est la signature de \( q'\). Par équivalence et par le changement de base de la proposition \ref{PROPooOKTGooOYukoB}, nous avons \( q'=\phi^t q\phi\), et donc
		\begin{equation}
			P^tq'P =	(\phi P)^tq(\phi P).
		\end{equation}
		Dans la base des \( (\phi\circ P)e_i\), la matrice de \( q\) est \eqref{EQooTCIRooFtWiTz}. Donc le point \ref{ThoQFVsBCk}\ref{ITEMooCFQHooRWfmpT} du théorème de Sylvester montre que la signature de \( q\) est également \( (p',q')\).

		\spitem[\( \Rightarrow\)]
		%-----------------------------------------------------------
		Nous supposons que \( q\) et \( q'\) ont la même signature. Il existe donc des matrices inversibles \( P\) et \( Q\) telles que \( P^tq'P\) et \( Q^tqQ\) aient la même forme (celle de la matrice \eqref{EQooTCIRooFtWiTz}). Autrement dit,
		\begin{equation}
			q'\circ P=q\circ Q,
		\end{equation}
		d'où nous déduisons que \( q'=q\circ Q\circ P^{-1}\) parce que \( P\) est inversible. Comme l'application \( Q\circ P^{-1}\) est inversible, les formes quadratiques \( q\) et \( q'\) sont équivalentes.
	\end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DEFooGVGGooWQEIET}
	Soit une forme quadratique\footnote{Définition \ref{DefBSIoouvuKR}.} \( q\) sur l'espace vectoriel \( V\) sur \( \eK\). Soit \( A\) la matrice de \( q\) dans la base \( \{ e_i \}\) et \( B\) sa matrice dans la base \( \{f_{\alpha}  \}\). Nous supposons que le changement de base est orthogonal.

	Alors les valeurs propres de \( A\) et \( B\) sont les mêmes.

	Ces valeurs sont les \defe{valeurs propres}{valeur propre!forme quadratique} de \( q\).
\end{lemmaDef}

\begin{proof}
	Nous nous rappelons de la définition \ref{DEFooAOGPooXWXUcN} de la matrice associée à \( q\), et à la proposition \ref{PROPooLBIOooUpzxXA} qui parle de changement de base : \( B=Q^tAQ\) où \( Q\) est orthogonale.

	Soit un vecteur propre \( v\) de \(A \), de valeur propre \( \lambda\). Alors nous prouvons que \( Q^tv\) est un vecteur propre pour \( B\), de même valeur propre \( \lambda\). En effet,
	\begin{equation}
		BQ^tv=Q^tAQQ^tv=Q^tAv=\lambda Q^tv
	\end{equation}
	où nous avons utilisé \( QQ^t=\mtu\) et \( Av=\lambda v\).
\end{proof}

\begin{proposition}\label{PropFWYooQXfcVY}
	Dans la base de diagonalisation de sa matrice associée, une forme quadratique a la forme
	\begin{equation}
		q(x)=\sum_i\lambda_ix_i^2
	\end{equation}
	où les \( \lambda_i\) sont les valeurs propres de la matrice associée à \( q\).
\end{proposition}

\begin{proof}
	Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée. Si \( \{ f_i \}\) est une base de diagonalisation\footnote{Qui existe parce que la matrice est symétrique, théorème~\ref{ThoeTMXla}.} de la matrice de \( b\) alors dans cette base nous avons
	\begin{equation}
		q(x)=b(x,x)=\sum_{ij}x_ix_jb(f_i,f_j)=\sum_i\lambda_ix_i^2
	\end{equation}
	où les \( \lambda_i\) sont les valeurs propres de la matrice de \( b\).
\end{proof}

Notons que si nous choisissons une autre base de diagonalisation, les \( \lambda_i\) ne changement pas (à part l'ordre éventuellement).

Cela justifie la définition pour dire que nous nous permettrons de parler des \defe{valeurs propres}{valeur propre!d'une forme quadratique} d'une forme quadratique comme étant les valeurs propres de la matrice associée.


Le théorème \ref{THOooIDMPooIMwkqB} a déjà donné une base orthogonale pour toute forme quadratique sur un espace vectoriel \( (E,\eK)\) de dimension finie. Dans le cas de \( \eR^n\), nous pouvons en donner une preuve basée sur le théorème spectral, c'est la proposition \ref{PROPooUKRUooGRIDHt}.

\begin{proposition}     \label{PROPooUKRUooGRIDHt}
	Soit une forme bilinéaire symétrique \( b\) sur un \( \eR^n\). Il existe une matrice orthogonale \( Q\) telle que
	\begin{enumerate}
		\item
		      \( D=Q^tbQ\) est diagonale
		\item
		      \( D(x,y)=b(Qx,Qy)\) pour tout \( x,y\in \eR^n\).
	\end{enumerate}

	Il existe une base \( (f_i)_{i=1,\ldots, n}\) qui est \( b\)-orthogonale.

	Dans cet énoncé, nous mélangeons sans vergogne les formes et les matrices, en supposant qu'une base soit fixée\footnote{Autrement dit, si vous avez en tête d'utiliser cette proposition pour \( \eR^n\) c'est bon; mais sinon vous devez choisir une base et considérer toutes les matrices dans cette base.}. Par exemple
	\begin{equation}
		D(x,y)=\sum_{ij}D_{ij}x_iy_j.
	\end{equation}
\end{proposition}

\begin{proof}
	Pour la matrice diagonale, c'est le théorème spectral \ref{ThoeTMXla}\ref{ITEMooMWWRooXxGONW} qui joue parce que la matrice d'une forme bilinéaire symétrique est symétrique (c'est vu de la définition \eqref{EQooCUGFooRlKUtu}).

	Pour le reste c'est un calcul :
	\begin{subequations}
		\begin{align}
			D(x,y) & =\sum_{ijkl}Q^t_{ik}b_{kl}Q_{lj}x_iy_j   \\
			       & =\sum_{ijkl}b_{kl}(Q_{ki}x_i)(Q_{lj}y_j) \\
			       & =\sum_{kl}b_{kl}(Qx)_k(Qy)_l             \\
			       & =b(Qx,Qy).
		\end{align}
	\end{subequations}
	Nous avons utilisé le produit matrice fois vecteur donné par \eqref{EQooQFVTooMFfzol}.

	En ce qui concerne l'existence d'une base \( b\)-orthogonale, vu que \( D\) est diagonale, nous avons, pour \( i\neq j\) que \( D(e_i,e_j)=0\). Donc en posant \( f_i=Qe_i\), nous trouvons
	\begin{equation}
		0=D(e_i,e_j)=b(Qe_i,Qe_j)=b(f_i,f_j).
	\end{equation}
	La base \( (Qe_i)_{i=1,\ldots, n}\) est donc \( b\)-orthogonale.
\end{proof}
