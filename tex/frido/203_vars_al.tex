% This is part of Mes notes de mathématique
% Copyright (c) 2011-2013,2016-2018, 2023, 2025
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Résultats qui se démontrent avec des variables aléatoires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Nombres normaux}
%---------------------------------------------------------------------------------------------------------------------------

Tout nombre \( x\in \mathopen[ 0 , 1 \mathclose[\) admet un unique\footnote{Voir le théorème \ref{ThoRXBootpUpd} pour plus de détails.}
développement en base \( b\geq 2\) :
\begin{equation}
	x=\sum_{n=1}^{\infty}\frac{ \epsilon_n(x) }{ b^n }
\end{equation}
avec \( \epsilon(x)\in \eD_b\). Nous excluons \( x=1\) parce que son développement en puissances négatives de \( b\) est zéro.

Nous notons \( \mA=\{ 0,\ldots, b-1 \}\). Soit \( k\geq 1\) et \( r\in \mA^k\); nous posons
\begin{equation}
	N_x(r,n)=\Card\big\{   i\in\{ 1,\ldots, n-k+1 \}\tq \epsilon_1(x)=r_1,\ldots, \epsilon_{i+k-1}=r_k \big\}.
\end{equation}
C'est le nombre d'occurrences du motif \( r\) (de longueur \( k\)) dans les \( n\) premières décimales de \( x\).

\begin{definition}
	Un nombre \( x\in\mathopen[ 0 , 1 [\) est \defe{normal}{normal!nombre}\index{nombre!normal} en base \( b\) si pour tout \( r\in\{ 0,\ldots, b-1 \}^k\) nous avons
	\begin{equation}
		\frac{ N_x(r,n) }{ n }\to \frac{1}{ b^k }.
	\end{equation}
	Un nombre est normal si il est normal en toute base.
\end{definition}

\begin{proposition}[\cite{IMQidDv,KXjFWKA}]     \label{PropEEOXLae}
	Au sens de la mesure de Lebesgue, presque tous les nombres de \( \mathopen[ 0 , 1 [\) sont normaux.
\end{proposition}
\index{loi!des grands nombres!utilisation}
\index{indépendance!événements!utilisation}

\begin{proof}
	Pour \( x\in\mathopen[ 0 , 1 [\), nous notons \( \epsilon_n(x)\) son développement en base \( b\). Cela nous donne des variables aléatoires \( \epsilon_i\colon \mathopen[ 0 , 1 [\to \mA\) dont la loi de probabilité est donnée par
	\begin{equation}
		P(\epsilon_1=d)=P\big( \mathopen[ \frac{ d }{ b } , \frac{ d+1 }{ b } [ \big)=\frac{1}{ b }
	\end{equation}
	parce que l'intervalle \( \mathopen[ \frac{ d }{ b } , \frac{ d+1 }{ d } [\) est l'ensemble des nombres de \( \mathopen[ 0 , 1 [\) dont la première décimale est \( d\). Pour la loi des \( \epsilon_i\), il faut un peu plus découper, mais ça donne le même résultat : \( P(\epsilon_i=d)=1/b\). Ces variables aléatoires sont indépendantes et identiquement distribuées. Nous considérons aussi la variable aléatoire
	\begin{equation}
		\begin{aligned}
			N(r,n)\colon \mathopen[ 0 , 1 [ & \to \eN          \\
			x                               & \mapsto N_x(r,n)
		\end{aligned}
	\end{equation}
	Pour un \( r\in\mA\) fixé, nous définissons encore la variable aléatoire
	\begin{equation}
		\begin{aligned}
			X_j\colon \mA & \to \{ 0,1 \}                          \\
			x             & \mapsto \begin{cases}
				                        1 & \text{si } \epsilon_j(x)=b \\
				                        0 & \text{sinon.}
			                        \end{cases}.
		\end{aligned}
	\end{equation}
	Les variables aléatoires \( X_j\) sont des variables aléatoires de Bernoulli indépendantes et identiquement distribuées de paramètre \( E(X_j)=P(X_j=1)=P(\epsilon_1=b)=\frac{1}{ b }\). Nous pouvons utiliser dessus la loi forte des grands nombres (théorème~\ref{ThoefQyKZ}). Pour dire que
	\begin{equation}    \label{EqNALwzsh}
		\frac{1}{n }\sum_{i=1}^nX_i\stackrel{p.s.}{\longrightarrow}E(X_1)=\frac{1}{ b }.
	\end{equation}
	Mais en réalité nous avons aussi \( \sum_{j=1}^nX_j=N(r,n)\) parce que en appliquant à \( x\in\mathopen[ 0 , 1 [\) :
	\begin{equation}
		\sum_{j=1}^n\begin{cases}
			1 & \text{si } \epsilon_j(x)=r \\
			0 & \text{sinon}
		\end{cases}
		=
		\Card\big\{  i\in\{ 1,\ldots, n \}\tq \epsilon_i(x)=r \big\}=N_x(r,n),
	\end{equation}
	de sorte que l'équation \eqref{EqNALwzsh} nous dit exactement que pour tout \( r\in \mA\),
	\begin{equation}
		\lim_{n\to \infty} \frac{ N_x(r,n) }{ n }=\frac{1}{ b }
	\end{equation}
	pour presque tout \( x\in\mathopen[ 0 , 1 [\).

	Il reste à prouver la même chose pour tout \( r\in\mA^k\). Voyons avec \( k=2\) et \( r=(u,v)\in\mA^2\). Nous posons
	\begin{equation}
		Y_j=\mtu_{\{ \epsilon_j=u,\epsilon_{j+1}=v \}},
	\end{equation}
	et \( N(r,n)=\sum_{j=1}^{n-1}Y_j\). Les \( Y_i\) sont encore des binomiales de paramètre \( \frac{1}{ b^2 }\), mais elles ne sont pas indépendantes. En effet pour avoir \( Y_1(x)=Y_2(x)=1\), il faut que les trois premières décimales de \( x\) soit en même temps de la forme \( uv.\) et \( .uv\), donc
	\begin{equation}
		P(Y_1,Y_2=1)=\delta_{u,v}/b^3
	\end{equation}
	alors que \( P(Y_1=1)P(Y_2=2)=1/b^4\). Nous pouvons contourner ce problème en remarquant que les \( \epsilon_i\), eux, sont indépendants. Donc le lemme de regroupement~\ref{LemHOjqqw} nous dit que la famille \( \{ Y_{2n} \} \) est une famille de variables aléatoires indépendantes (et idem pour la famille \( Y_{2n-1}\)). En effet, les variables aléatoires \( Y_{2n}\) correspondent à la partition \( 23\), \( 45\), \( 67\), etc.

	Nous appliquons la loi des grands nombres sur les deux familles indépendamment :
	\begin{equation}
		\frac{1}{ n }\sum_{j=1}^nY_{2j-1}\stackrel{p.s.}{\longrightarrow}\frac{1}{ b^2 }
	\end{equation}
	et
	\begin{equation}
		\frac{1}{ n }\sum_{j=1}^nY_{2j}\stackrel{p.s.}{\longrightarrow}\frac{1}{ b^2 }
	\end{equation}
	Pour rappel, le but pour l'instant est d'établir la limite \( \lim_{n\to \infty} \frac{1}{ n }\sum_{j=1}^nY_j=\frac{1}{ b^2 }\). Nous allons l'établir séparément pour les termes pairs et impairs de la suite. Pour les pairs :
	\begin{equation}
		\frac{1}{ 2n }\sum_{j=1}^{2n}Y_j=\frac{ 1 }{2}\left( \frac{1}{ n }\sum_{j=1}^nY_{2j-1} \right)+\frac{ 1 }{2}\left( \frac{1}{ n }\sum_{j=1}^nY_{2j} \right)\stackrel{p.s.}{\longrightarrow}\frac{1}{ b^2 }
	\end{equation}
	Pour les impairs\quext{Ici dans \cite{KXjFWKA}, la seconde somme va jusqu'à \( n-1\) et je ne comprends pas pourquoi.},
	\begin{equation}
		\frac{1}{ 2n-1 }\sum_{j=1}^{2n-1}Y_j=\frac{ n }{ 2n-1 }\left( \frac{1}{ n }\sum_{j=1}^nY_{2j-1} \right)+\frac{ n }{ 2n-1 }\left( \frac{1}{ n }\sum_{j=1}^nY_{2j} \right)\to\frac{1}{ b^2 }
	\end{equation}
	parce que les deux parenthèses convergent vers \( \frac{1}{ b^2 }\) alors que les coefficients devant convergent vers \( \frac{ 1 }{2}\).

	Au final nous avons bien
	\begin{equation}
		\frac{ N(r,n) }{ n }=\frac{1}{ n }\sum_{j=1}^{n-1}Y_j=\frac{ n-1 }{ n }\left( \frac{1}{ n-1 }\sum_{j=1}^{n-1}Y_j \right)\to\frac{1}{ b^2 }
	\end{equation}
	tant que \( r\in\mA^2\).

	Pour prouver la même chose avec \( r\in \mA^k\), il suffit de faire le même raisonnement en divisant en plus de paquets : \( \{ Y_{kj+m} \}_{m=1,\ldots, k-1}\) sont indépendants et nous utilisons \( k\) fois la loi des grands nombres.

	Donc pour toute base \( b\) nous savons que les nombres non-normaux en base \( b\) forment un ensemble de mesure nulle dans \( \mathopen[ 0 , 1 [\). Il reste à voir que leur union reste de mesure nulle. Cela est vrai parce que nous avons une union dénombrable et qu'une union dénombrable d'ensembles de mesure nulle est de mesure nulle par le lemme~\ref{LemIDITgAy}.
\end{proof}

\begin{remark}  \label{RemUXAkcuH}
	Un nombre \( x\) est normal en base \( b\) si et seulement si la suite  \( u_k=xb^k\) est équirépartie modulo \( 1\) sur \( [0,1]\) (c'est-à-dire quel la suite des parties fractionnelles des \( u_k\) est équirépartie). Pour le nombre \( 0.2357873\ldots\), nous parlons de la suite \( 0.2357873\ldots\); \( 0.357873\ldots\); \( 0.57897\ldots\) etc. C'est la suite des queues de suites de la suite de ses décimales\footnote{C'est pas trop bien dit, mais on se comprend, non ?}.
\end{remark}


Deux vidéos de numberphile à propos de nombres normaux et calculables :
\begin{itemize}
	\item
	      \url{https://www.youtube.com/watch?v=5TkIe60y2GI},
	\item
	      \url{https://www.youtube.com/watch?v=LsBQhfkw5ag}
\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Bernstein}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Théorème de Bernstein\cite{KXjFWKA}]    \label{ThoDJIvrty}
	Soit \( f\in C^0\big( \mathopen[ 0 , 1 \mathclose],\eC \big)\) et son module de continuité
	\begin{equation}
		\begin{aligned}
			\omega\colon \mathopen[ 0 , 1 \mathclose] & \to \eR                                        \\
			h                                         & \mapsto \sup\{ | f(u)-f(v) |\tq | u-v |< h \}.
		\end{aligned}
	\end{equation}
	Pour \( n\geq 0\) nous définissons le \( n\)\ieme\ \defe{polynôme de Bernstein}{polynôme!de Bernstein} de \( f\) par
	\begin{equation}
		B_n(f)(x)=\sum_{k=0}^{n}\binom{ n }{ k }x^k(1-x)^{n-k}f\left( \frac{ k }{ n } \right).
	\end{equation}
	Alors il existe \( C\) tel que pour tout \( n\geq 1\) :
	\begin{enumerate}
		\item
		      \begin{equation}        \label{EqZQgnVqM}
			      \| f-B_n(f) \|_{\infty}\leq C\omega\left( \frac{1}{ \sqrt{n} } \right).
		      \end{equation}
		\item
		      \begin{equation}
			      B_n(f)\stackrel{unif}{\longrightarrow}f
		      \end{equation}
		      sur \( \mathopen[ 0 , 1 \mathclose]\).
		\item
		      L'inégalité \eqref{EqZQgnVqM} est optimale : il existe une fonction \( g\in C^{0}\big( \mathopen[ 0 , 1 \mathclose],\eC \big)\) et \( \delta>0\) tels que pour tout \( N\geq 1\), \( \| g-B_n(g) \|_{\infty}\geq\frac{ \delta }{ \sqrt{n} }\). Cette fonction peut être choisie lipschitzienne. Une telle fonction est donnée par exemple par \( g(x)=| x-\frac{ 1 }{2} |\).
		\item
		      Les polynômes forment une partie dense dans \( \Big( C^0\big( \mathopen[ 0 , 1 \mathclose] \big),\| . \|_{\infty} \Big)\).
	\end{enumerate}
\end{theorem}
\index{densité!des polynômes!dans \( C^0_c\mathopen[ 0 , 1 \mathclose]\)}
\index{approximation!de fonctions!par des polynômes}
\index{variable aléatoire!Bernoulli!utilisation}

\begin{proof}
	Soit \( x\in \mathopen[ 0 , 1 \mathclose]\) et une suite de variables aléatoires de Bernoulli indépendantes\footnote{Définition~\ref{DefNJUkotc}.} et identiquement distribuées \( (X_i)_{i\geq 1}\) de paramètre \( x\). Nous notons \( S_n=\sum_{k=1}^nX_k\).

	\begin{enumerate}
		\item

		      Pour cette histoire de convergence, il faut majorer la quantité \( \big| f(x)-B_n(f)(x) \big|\). Pour cela il y a trois astuces. La première est de se souvenir que \( E\big( f(x) \big)=f(x)\), et la seconde est que le théorème de transfert~\ref{PropintdPintdPXeR} appliqué à \( x\mapsto f(x/n)\) donne\footnote{Nous avons aussi utilisé la formule de l'espérance pour les variables aléatoires discrètes.}
		      \begin{equation}
			      E\left( f\big( \frac{ S_n }{ n } \big) \right)=\sum_{k=0}^nf\left( \frac{ k }{ n } \right)P(S_n=k)=\sum_{k=0}^nf\left( \frac{ k }{ n } \right)\binom{ n }{ k }x^k(1-x)^{n-k},
		      \end{equation}
		      c'est-à-dire que
		      \begin{equation}
			      B_n(f)(x)=E\left( f\big( \frac{ S_n }{ n } \big) \right).
		      \end{equation}
		      Et enfin la troisième astuce est d'utiliser le lemme~\ref{LemLUbgYeo} pour avoir
		      \begin{equation}
			      \omega\left( | x-\frac{ S_n }{ n } | \right)=\omega\left( \frac{1}{ \sqrt{n} }| \sqrt{n}-\frac{ S_n }{ \sqrt{n} } | \right)\leq
			      \left( \sqrt{n}| x-\frac{ S_n }{ n } |+1 \right)\omega\big( \frac{1}{ \sqrt{n} } \big).
		      \end{equation}
		      À partir de là nous pouvons un peu calculer :
		      \begin{subequations}
			      \begin{align}
				      \big| f(x)-B_n(f)(x) \big| & =\Big| E\left( f(x)-f\big( \frac{ S_n }{ n } \big) \right)    \Big|                              \\
				                                 & \leq E\left( | f(x)-f\big( \frac{ S_n }{ n } \big) | \right)                                     \\
				                                 & \leq E\left( \omega\Big( | x-\frac{ S_n }{ n } | \Big) \right)                                   \\
				                                 & \leq \omega\left( \frac{1}{ \sqrt{n} } \right)E\left( | \sqrt{n}x-\frac{ S_n }{ n } |+1 \right).
			      \end{align}
		      \end{subequations}
		      Le dernier facteur peut être récrit sous la forme
		      \begin{equation}
			      E\left( \sqrt{n}\big| x-\frac{ S_n }{ n } \big|+1 \right)=\sqrt{n}E\left( \big| x-\frac{ S_n }{ n } \big| \right)+1,
		      \end{equation}
		      et c'est là que nous pouvons utiliser l'inégalité de Hölder~\ref{ProptYqspT}\index{inégalité!Hölder!utilisation} :
		      \begin{equation}
			      E\big( | X | \big)=\| X \|_1\leq\| X \|_2
		      \end{equation}
		      où \( \| X \|_2\) désigne
		      \begin{equation}
			      \| X \|_2=\sqrt{ E\big( | X |^2 \big)  }.
		      \end{equation}
		      Nous pouvons donc écrire
		      \begin{equation}
			      \big| f(x)-B_n(f)(x) \big|\leq \omega\left( \frac{1}{ \sqrt{n} } \right)\left( \sqrt{n}\big\| x-\frac{ S_n }{ n } \big\|_2+1 \right).
		      \end{equation}
		      Nous étudions maintenant de plus près la quantité \( \| x-\frac{ S_n }{ n } \|_2\). D'abord
		      \begin{equation}
			      E\left( \big| x-\frac{ S_n }{ n } \big|^2 \right)=x^2-2\frac{ x }{ n }E(S_n)+\frac{1}{ n^2 }E(S_n^2).
		      \end{equation}
		      Ensuite nous savons l'espérance de \( S_n\) (qui vaut \( E(S_n)=nx\)) par \eqref{EqDGbBgrv} et le lemme~\ref{LemEXYEXEYprodindep} nous permet de calculer \( E(S_n^2)\) par indépendance des \( X_i\) qui composent \( S_n\). Nous avons alors
		      \begin{subequations}
			      \begin{align}
				      E\left( \big| x-\frac{ S_n }{ n } \big|^2 \right) & =x^2-2x^2+\frac{1}{ n^2 }\sum_{1\leq i\neq j\leq n}E(X_i)E(X_j)+\frac{1}{ n^2 }\sum_{i=1}^nE(X_i^2) \\
				                                                        & =-x^2+\frac{ n^2-n }{ n^2 }x^2+\frac{ nx }{ n^2 }                                                   \\
				                                                        & =\frac{ x(1-x) }{ n }.
			      \end{align}
		      \end{subequations}
		      Quelques justifications :
		      \begin{itemize}
			      \item \( E(X_i)=E(X_i^2)=x\) parce que \( X_i\) est une variable aléatoire de Bernoulli de paramètre \( x\).
			      \item La première somme contient tous les couples \( (i,j)\) sauf les diagonaux; il y en a donc \( n^2-n\).
		      \end{itemize}
		      En recombinant le tout,
		      \begin{subequations}    \label{subEqsRSuRoCJ}
			      \begin{align}
				      \big| f(x)-B_n(f)(x) \big| & \leq \omega\left( \frac{1}{ \sqrt{n} } \right)\left( \sqrt{n}\sqrt{\frac{ x(1-x) }{ n }}+1 \right) \\
				                                 & =\omega\left( \frac{1}{ \sqrt{n} } \right)\big( \sqrt{x(1-x)}+1 \big)                              \\
				                                 & \leq\frac{ 3 }{2}\omega\left( \frac{1}{ \sqrt{n} } \right).
			      \end{align}
		      \end{subequations}
		      La dernière majoration est une rapide étude de la fonction \( x(1-x)\).

		      Étant donné que les majorations \eqref{subEqsRSuRoCJ} sont valables pour tout \( x\), en passant au supremum nous avons
		      \begin{equation}
			      \| f-B_n(f) \|_{\infty}\leq \frac{ 3 }{2}\omega\left( \frac{1}{ \sqrt{n} } \right)\to 0.
		      \end{equation}

		      Ceci prouve les deux premiers points du théorème.

		\item

		      Fait.

		\item

		      Nous considérons la fonction
		      \begin{equation}
			      g(x)=| x-\frac{ 1 }{2} |
		      \end{equation}
		      et nous vérifions qu'elle vérifie toutes les conditions. D'abord si \( u,v\in\mathopen[ 0 , 1 \mathclose]\) alors
		      \begin{equation}
			      \big| g(u)-g(v) \big|\leq | u-v |
		      \end{equation}
		      et donc \( \omega(h)\leq h\), ce qui signifie que \( g\) est \( 1\)-lipschitzienne. Le principe de cette partie est de montrer que \( \| g-B_n(g) \|_{\infty}\) est plus grand que d'autres trucs (et non plus petit que d'autres trucs comme d'habitude). Nous commençons par
		      \begin{equation}
			      \| g-B_n(g) \|_{\infty}\geq g(\frac{ 1 }{2})-B_n(g)( \frac{ 1 }{2} ).
		      \end{equation}
		      Très vite nous nous rendons compte que \( g(1/2)=0\). Ensuite nous nous souvenons que
		      \begin{equation}
			      B_n(g)(\frac{ 1 }{2})=E\left( g(\frac{ S_n }{ n }) \right)=E\left( | \frac{ S_n }{ n }-\frac{ 1 }{2} | \right)=\frac{1}{ 2n }E\big( | 2S_n-n | \big).
		      \end{equation}
		      si nous posons \( \epsilon_i=2X_i-1\), alors les \( \epsilon_i\) sont des variables aléatoires de Rademacher indépendantes et identiquement distribuées qui satisfont à \( 2S_n-n=\sum_{i=1}^n\epsilon_i\). Nous utilisons la proposition~\ref{PropCZRNRsf} :
		      \begin{equation}
			      \| g-B_n(g) \|_{\infty}\geq \frac{1}{ 2n }E\big( | \sum_i\epsilon_i | \big)\geq\frac{1}{ 2n\sqrt{e} }\big\| \sum_{i=1}^n\epsilon_j \big\|_2.
		      \end{equation}
		      Calculons ce qui est dans la norme :
		      \begin{equation}
			      \big\| \sum_{j=1}^n\epsilon_j \big\|_2^2=E\left( \big( \sum_{j=1}^n\epsilon_j \big)^2 \right)=\sum_{1\leq i\neq j\leq n}E(\epsilon_i)E(\epsilon_j)+\sum_{i=1}^nE(\epsilon_i^2)=0+n=n.
		      \end{equation}
		      Nous finissons alors notre travail de majoration :
		      \begin{equation}
			      \| g-B_n(g) \|_{\infty}\geq\frac{1}{ 2n\sqrt{e} }\big\| \sum_{i=1}^n\epsilon_j \big\|_2\geq\frac{1}{ 2\sqrt{n}\sqrt{e} }\geq\frac{1}{ 2\sqrt{e} }\omega\left( \frac{1}{ \sqrt{n} } \right).
		      \end{equation}
		\item
		      Nous avons trouvé une suite de polynômes qui converge uniformément vers un élément arbitraire de \( C^0\big( \mathopen[ 0 , 1 \mathclose] \big)\). Cela prouve la densité.
	\end{enumerate}
\end{proof}

\begin{corollary}       \label{CORooCWLMooWwCOAP}
	Dans \( \eR\), si \( I=\mathopen[ a , b \mathclose]\) alors les polynômes forment une partie dense dans \( \big( C^0(I),\| . \|_{\infty} \big) \).
\end{corollary}

\begin{proof}
	Nous supposons que \( b>a\). Le cas \( a=b\) est assez facile parce que l'espace des fonctions sur \( \{ a \}\) est de dimension \( 1\).

	Nous considérons une bijection affine \( \varphi\colon \mathopen[ 0 , 1 \mathclose]   \to \mathopen[ a , b \mathclose]\) telle que \( \varphi(0)=a\) et \( \varphi(1)=b\). Soit \( f\in C^0(I)\).

	Si \( g=f\circ\varphi\), alors le théorème de Bernstein \ref{ThoDJIvrty} nous donne une suite de polynômes \( g_k\) sur \( \mathopen[ 0 , 1 \mathclose]\) tels que
	\begin{equation}
		g_k\stackrel{unif}{\longrightarrow}g.
	\end{equation}
	Nous considérons \( f_k=g_k\circ\varphi^{-1}\) qui est encore un polynôme parce que \( \varphi^{-1}\) est affine. Étant donné que \( \varphi^{-1}\) est une bijection, si \( h\) est une fonction sur \( \mathopen[ 0 , 1 \mathclose]\), nous avons
	\begin{equation}
		\sup_{x\in \mathopen[ a , b \mathclose]}\| (h\circ\varphi^{-1})(x) \|=\sup_{y\in \mathopen[ 0 , 1 \mathclose]}\| h(y) \|.
	\end{equation}
	Cela nous permt le calcul suivant :
	\begin{subequations}
		\begin{align}
			\| f_k-f \|_{\infty} & =\| g_k\circ\varphi^{-1}-g\circ\varphi^{-1} \|                                    \\
			                     & =\| (g_k-g)\circ\varphi^{-1} \|                                                   \\
			                     & =\sup_{x\in \mathopen[ a , b \mathclose]}\| (g_k-g)\big( \varphi^{-1}(x) \big) \| \\
			                     & =\sup_{y\in\mathopen[ 0 , 1 \mathclose]}\| (g_k-g)(y) \|                          \\
			                     & =\| g_k-g \|_{\infty}.
		\end{align}
	\end{subequations}
	Nous avons donc
	\begin{equation}
		\lim_{k\to \infty} \| f_k-f \|_{\infty}=0,
	\end{equation}
	ce qui prouve la densité.
\end{proof}
