% This is part of Mes notes de mathématique
% Copyright (c) 2011-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exponentielle sur une algèbre normée}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Définition}
%---------------------------------------------------------------------------------------------------------------------------

Dans ce qui suit, nous considérons une algèbre commutative.
\begin{propositionDef}[Exponentielle\cite{MonCerveau}]       \label{DEFooSFDUooMNsgZY}
    Soit \( (A,\| . \|)\) une algèbre\footnote{Définition~\ref{DefAEbnJqI}.} commutative de dimension finie sur \( \eC\) munie d'une norme d'algèbre. Pour \( x\in A\) nous définissons
    \begin{equation}        \label{EQooCUVTooGNOrFj}
        \exp(x)=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }.
    \end{equation}
    Cette définition a les propriétés suivantes :
    \begin{enumerate}
        \item
            C'est bien défini pour tout \( x\in A\). C'est-à-dire que pour chaque \( x\), la série \eqref{EQooCUVTooGNOrFj} converge.
        \item
            Cela donne une application continue \( \exp\colon A\to A\).
        \item       \label{ITEMooGGVAooVfhGuu}
            La fonction \( \exp\) est différentiable et
            \begin{equation}        \label{EQooKWBUooLUdBAw}
                (d\exp)_x(y)=\exp(x)y,
            \end{equation}
            le dernier produit étant la structure d'algèbre sur \( A\).
    \end{enumerate}
\end{propositionDef}

\begin{proof}
    Pour la différentiabilité de \( \exp\), nous voulons utiliser le théorème~\ref{ThoLDpRmXQ}. Pour cela nous posons
    \begin{equation}
        u_k(x)=\frac{ x^k }{ k! }
    \end{equation}

    \begin{subproof}
        \item[Convergence simple]
            Nous prouvons la convergence simple, c'est-à-dire pour chaque \( x\) séparément, de la série \eqref{EQooCUVTooGNOrFj} dans deux buts. D'abord de nous assurer que la définition posée de \( \exp\) a un sens, et ensuite pour commencer à vérifier les hypothèses du théorème~\ref{ThoLDpRmXQ}.

            Nous montrons que les sommes partielles forment une suite de Cauchy. Nous fixons \( x\in A\) et nous posons
            \begin{equation}
                s_n=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }.
            \end{equation}
            Soient \( p>q\), deux entiers. Nous avons :
            \begin{equation}        \label{EQooYNZNooDaiPhU}
                \| s_p-s_q \|=\| \sum_{k=q+1}^p\frac{ x^k }{ k! } \|\leq \sum_{k=q+1}^p\frac{ \| x^k \| }{ k! }\leq \sum_{k=q+1}^p\frac{ \| x \|^k }{ k! }
            \end{equation}
            où nous avons utilisé le fait que la norme sur \( A\) soit une norme d'algèbre.

            C'est le moment d'utiliser la série exponentielle donnée dans l'exemple~\ref{ExIJMHooOEUKfj} que nous appliquons avec \( t=\| x \|\). La série donnée par les coefficients \( a_k=\| x \|^k/k!\) converge et ses sommes partielles forment en particulier une suite de Cauchy. Donc ce que nous avons à droite dans \eqref{EQooYNZNooDaiPhU} peut être rendu arbitrairement petit lorsque \( p\) et \( q\) sont grands.

        \item[\( u_k\) est continue]
            Il s'agit de remarquer que \( (x+h)^k=x^k+hC(x,h)\) où \( C\) est une fonction bornée de \( h\) (lorsque \( h\) est dans un voisinage de \( 0\in A\)). Donc
            \begin{equation}
                \| (x+h)^k-x^k \|\leq \| h \|\| C(x,h) \|\to 0.
            \end{equation}
        \item[Candidat différentielle de \( u_k\)]
            Nous trouvons à présent un candidat à être différentielle de \( u_k\). Pour cela nous faisons le calcul suivant, sans trop nous soucier de la rigueur :
            \begin{equation}
                (du_k)_x(y)=\Dsdd{ u_k(x+ty) }{t}{0}=k\frac{1}{ k! }x^{k-1}y=u_{k-1}(x)y.
            \end{equation}
        \item[\( u_k\) est différentiable]
            Nous fixons \( x\in A\) et nous posons \( T(y)=u_{k-1}(x)y\). Ensuite nous vérifions que cela vérifie la définition de la différentielle : nous devons calculer
            \begin{equation}        \label{EQooNPKGooVmEYAV}
                \lim_{h\to 0} \frac{ u_k(x+h)-u_k(x)-T(h) }{ \| h \| }=\lim_{h\to 0} \frac{ (x+h)^k-x^k-kx^{k-1}h }{ k! \| h \| }=\clubsuit.
            \end{equation}
            Vous vous souvenez de la formule pour \( (x+h)^k\) ? Essayez de vous en souvenir. Le premier terme est \( x^k\), et le second est \( kx^{k-1}h\). Pour le reste c'est un polynôme dont tous les termes contiennent au moins \( h^2\). Nous avons donc
            \begin{equation}
                \clubsuit=\lim_{h\to 0} \frac{ h^2P(x,h) }{ k!\| h \| }=0.
            \end{equation}
            Nous en concluons que \( u_k\) est différentiable et que
            \begin{equation}
                (du_k)_x(y)=u_{k-1}(x)y.
            \end{equation}
        \item[\( u_k\) est de classe \( C^1\)]
            Nous devons démontrer que la différentielle est continue; cela est la continuité de l'application
            \begin{equation}
                \begin{aligned}
                    du_k\colon A&\to \aL(A,A) \\
                    x&\mapsto (du_k)_x.
                \end{aligned}
            \end{equation}
            La topologie sur \( A\) est celle de la norme, et celle sur \( \aL(A,A)\) est celle de la norme opérateur associée à la norme sur $A$. Nous avons\footnote{N'oubliez pas de faire à part le cas \( k=0\) parce que ce qui suit n'est correct que pour \( k\geq 1\).} :
            \begin{subequations}
                \begin{align}
                    \lim_{h\to 0} \| (du_k)_{x+h}-(du_k)_x \|&=\lim_{h\to 0} \sup_{\| y \|=1}\| u_{k-1}(x+h)y-u_{k-1}(x)y \|\\
                    &\leq\lim_{h\to 0} \sup_{\| y \|=1}\| u_{k+1}(x+h)-u_{k-1}(x) \|\| y \|\\
                    &=\lim_{h\to 0} \| u_{k+1}(x+h)-u_{k-1}(x) \|.
                \end{align}
            \end{subequations}
            Le fait que cette limite valle zéro est maintenant la continuité de \( u_{k-1}\).

        \item[Convergence normale sur tout compact]

            Soit un compact \( K\) de \( A\). Par le théorème de Borel-Lebesgue~\ref{ThoXTEooxFmdI}, \( K\) est fermé et borné. C'est pour ceci que nous avons supposé que \( A\) était de dimension finie sur \( \eR\). Soit donc \( R>0\) tel que \( \| y \|<R\) pour tout \( y\in K\). Nous avons
            \begin{equation}
                \| du_k \|_K=\sup_{x\in K}\| (du_k)_x \|=\sup_{x\in K}\frac{ \| x^{k-1} \| }{ (k-1)! }\leq \sup_{x\in K}\frac{ \| x \|^{k-1} }{ (k-1)! }\leq \frac{ R^{k-1} }{ (k-1)! }.
            \end{equation}
            Mais la série \( \sum_{k=0}^{\infty}\frac{ R^k }{k!}\) converge. Nous avons donc la convergence normale demandée.

        \item[Conclusion]

            Le théorème~\ref{ThoLDpRmXQ} conclu que l'exponentielle est de classe \( C^1\) et que sa différentielle est donnée par la formule
            \begin{equation}
                (d\exp)_x(y)=\sum_{k=0}^{\infty}(du_k)_x(y)=\sum_{k=1}^{\infty}(du_k)_x(y)=\sum_{k=0}^{\infty}u_k(x)y=\exp(x)y.
            \end{equation}
            Notez le jeu d'indices : \( du_k=0\) lorsque \( k=0\) (ce qui permet de faire commencer la somme à \( 1\)) et ensuite \( du_k\) fait intervenir \( u_{k-1}\) (ce qui fait revenir le départ de la somme à \( k=0\)).

    \end{subproof}
\end{proof}

\begin{normaltext}
    Lorsque nous disons que la différentielle de l'exponentielle est l'exponentielle elle-même, nous référons au point~\ref{DEFooSFDUooMNsgZY}\ref{ITEMooGGVAooVfhGuu} : la différentielle de \( \exp\) en \( x\) est l'opérateur de multiplication par \( \exp(x)\).

    Nous pouvons comprendre maintenant que \( \exp\) est même de classe \(  C^{\infty}\) parce qu'à chaque différentiation nous tombons sur la même fonction, laquelle est de classe au moins \( C^1\).

    Cependant, pour formaliser ça, il faut un peut travailler. Le cauchemar des différentielles successives d'une application \( A\to A\) est que les espaces en jeu sont des emboîtements terribles de \( \aL(A,\aL(A,\aL(A,A)))\).

    Ce qui nous sauve est que l'espace \( \aL(A,V)\) est un \( A\)-module, quel que soit \( V\). En particulier lorsque \( V\) est lui-même déjà un emboîtement. Faisons un lemme pour voir comment ça fonctionne.
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentielles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{MonCerveau}]
    Soient deux espaces vectoriels normés \( E\) et \( V\) tels que \( V\) soit un \( E\)-module\footnote{Définition~\ref{DEFooHXITooBFvzrR}.}. Nous supposons les normes soient telles que \( \| xv \|_{V}\leq \| x \|_E\| v \|_V\).

    Soit une fonction différentiable \( f\colon E\to V\) telle que la différentielle \( df\colon E\to \aL(E,V)\) soit de la forme
    \begin{equation}
        df_x(y)=yg(x)
    \end{equation}
    pour une certains fonction différentiable \( g\colon E\to V\).

    Alors \( f\) est \( C^1\), et deux fois différentiable telle que
    \begin{equation}
        \begin{aligned}
            d^2f\colon E&\to \aL\big( E,\aL(E,V) \big) \\
            (d^2f)_x(y)z&=z(dg_x)(y)
        \end{aligned}
    \end{equation}
    pour tout \( x,y,z\in E\).
\end{lemma}

\begin{proof}
    En plusieurs étapes.
    \begin{subproof}
        \item[\( f\) est \( C^1\)]
            Nous savons, par hypothèse, que \( f\) est différentiable. Il faut montrer que sa différentielle est continue, en remarquant déjà que \( g\) est continue parce que différentiable.

            Soit \( x_k\stackrel{E}{\longrightarrow}x\), et calculons \( \| df_{x_k}-df_x \|\) :
            \begin{equation}
                \begin{aligned}[]
                    \| df_{x_k}-df_x \|&=\sup_{\| y \|=1}\| df_{x_k}(y)-df_x(y) \|\\
                    &=\sup_{\| y \|=1}\| \big(g(x_k)-g(x)\big)y \|\\
                    &\leq\sup_{\| y \|=1}\| g(x_k)-g(x) \|\| y \|\\
                    &=\| g(x_k)-g(x) \|.
                \end{aligned}
            \end{equation}
            Donc nous avons bien \(df_{x_k}\stackrel{\aL(E,V)}{\longrightarrow}df_x\), ce qui signifie la continuité de \( df\). Donc \( f\) est de classe \( C^1\).

        \item[\( f\) est deux fois différentiable]

            Pour montrer que \( df\) est différentiable, nous mettons directement dans la définition \eqref{DefDifferentiellePta} le candidat
            \begin{equation}
                \begin{aligned}
                    T_x(h)\colon R&\to V \\
                    T_x(h)z&=zdg_x(y).
                \end{aligned}
            \end{equation}
            Nous devons vérifier la limite suivante :
            \begin{equation}        \label{EQooTBCKooRxBCum}
                \lim_{h\stackrel{E}{\longrightarrow} 0} \frac{ df_{x+h}-df_x-T_x(h) }{ \| h \| }=0.
            \end{equation}
            Étudions la norme du numérateur :
            \begin{subequations}
                \begin{align}
                    \| df_{x+h}-df_x-T_x(h) \|&=\sup_{\| y \|=1}\| df_{x+h}(y)-df_x(y)-T_x(h)y \|\\
                    &=\sup_{\| y \|=1}\| yg(x+h)-yg(x)-ydg_x(h) \|\\
                    &\leq \sup_{\| y \|=1}\| y \| \| g(x+h)-g(x)-dg_x(h) \|.
                \end{align}
            \end{subequations}
            La limite \eqref{EQooTBCKooRxBCum} se déduit donc de la différentiabilité de \( g\).
    \end{subproof}
    Note : la partie démontrant que \( f\) est \( C^1\) n'est pas strictement obligatoire parce qu'en vérifiant que \( f\) est deux fois différentiable, nous vérifions de facto que \( df\) est en particulier continue.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]   \label{LEMooTUWQooMCCDcm}
    Soient des algèbres normées \( A\) et \( V\) telles que \( V\) soit un \( A\)-module vérifiant \( \| xv \|\leq \| x \|\| v \|\) pour tout \( x\in A\) et \( v\in V\). Alors \( \aL(A,V)\) est un \( A\)-module vérifiant \( \| x\alpha \|\leq \| x \|\|\alpha  \|\) pour tout \( x\in A\) et \( \alpha\in \aL(A,V)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul utilisant la norme opérateur :
    \begin{equation}
            \| x\alpha \|=\sup_{\| y \|=1}\| (x\alpha)y \|
            =\sup_{\| y \|=1}\| x\alpha(y) \|
            \leq \sup_{\| y \|=1}\| x \|\| \alpha(y) \|
            =\| x \|\sup_{\| y \|=1}\| \alpha(y) \|
            =\| x \|\| \alpha \|.
    \end{equation}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooTBDAooQouzSk}
    La fonction \( \exp\colon A\to A\) est de classe \(  C^{\infty}\) et vérifie, pour tout \( k\geq 1\) la récurrence
    \begin{equation}
        (d^k\exp)_x(y)=y(d^{k-1}\exp)_x.
    \end{equation}
\end{proposition}

\begin{proof}
    La formule proposée fonctionne avec \( k=1\) :
    \begin{equation}
        (d\exp)_x(y)=y\exp(x).
    \end{equation}
    C'est la relation~\ref{EQooKWBUooLUdBAw}.

    Nous considérons \( k>1\), nous supposons que \( \exp\) est de classe \( C^{k-1}\) et \( k\) fois différentiable. Nous allons prouver que \( \exp\) est alors de classe \( C^k\) et \( k+1\) fois différentiable, et que la différentielle de \( d^k\exp\) est donné par la formule
    \begin{equation}
        (d^{k+1}\exp)_x(y)=y(d^{k}\exp)_x.
    \end{equation}

    Pour nous mettre au clair avec les espaces en présence, nous supposons que
    \begin{subequations}
        \begin{align}
            d^{k-1}\exp&\colon A\to \aL(A,V)\\
            d^{k}\exp&\colon A\to \aL\big( A,\aL(A,V) \big)
        \end{align}
    \end{subequations}
    pour un certain espace vectoriel normé \( V\), lequel est un de ces terrifiants emboîtement de type \( \aL\Big( A,\aL\big( A,\aL(A,A) \big) \Big)\). Il est bien un espace vectoriel normé, et également un \( A\)-module parce qu'on peut toujours définir la multiplication d'un élément \( v\in V\) par un élément \(x\in A\) comme étant la multiplication par \( x\) du résultat final de l'évaluation emboîtée, laquelle se termine par un élément de \( A\). Donc tout se met bien.

    Quoi qu'il en soit, nous posons
    \begin{equation}
        T_x(y)=y(d^{k}\exp)_x
    \end{equation}
    et nous vérifions ce que cela donne dans la définition de la différentielle. Si nous avons
    \begin{equation}
        \lim_{h\to 0} \frac{ (d^k\exp)_{x+h}-(d^k\exp)_x-T_x(h) }{ \| h \| }=0
    \end{equation}
    alors nous aurons prouvé tout ce qu'il nous faut.

    Le numérateur est une application \( A\to \aL(A,V)\); nous en écrivons la norme comme il se doit :
    \begin{subequations}
        \begin{align}
            \|   (d^k\exp)_{x+h}-(d^k\exp)_x-T_x(h) \|&=\sup_{\| y \|=1}\| (d^{k}\exp)_{x+h}(y)-(d^k\exp)_x(y)-h(d^{k}\exp)_xy \|\\
            &=\sup_{\| y \|=1}\| y(d^{k-1}\exp)_{x+h}-y(d^{k-1}\exp)_x-h(d^k\exp)_xy \|\\
            &=\sup_{\| y \|=1}\| y(d^{k-1}\exp)_{x+h}-y(d^{k-1}\exp)_x-hy(d^{k-1}\exp)_x \|\\
            &\leq \| (d^{k-1}\exp)_{x+h}-(d^{k-1}\exp)_x-h(d^{k-1}\exp)_x \|\\
            &=\| (d^{k-1}\exp)_{x+h}-(d^{k-1}\exp)_x-(d^{k}\exp)_x(h) \|.
        \end{align}
    \end{subequations}
    Dans ce calcul nous avons utilisé le lemme~\ref{LEMooTUWQooMCCDcm} et \( T_x(h)y=h(d^{k}\exp)_xy\).
    Maintenant, la limite
    \begin{equation}
        \lim_{h\to 0} \frac{  \| (d^{k-1}\exp)_{x+h}-(d^{k-1}\exp)_x-(d^{k}\exp)_x(h) \|.}{ \| h \| }
    \end{equation}
    n'est rien d'autre que la limite arrivant dans la définition du fait que \( d^k\exp\) est la différentielle de \( d^{k-1}\exp\). Cette limite est donc zéro comme nous voulions le prouver.
\end{proof}


Le théorème suivant est très important parce qu'il permet de définir l'exponentielle d'une matrice. Et les exponentielles de matrices sont utiles, entre très nombreuses autres choses pour résoudre certaines équations différentielles.
\begin{theoremDef}[\cite{MonCerveau}]      \label{THOooFGTQooZPiVLO}
    Soit une algèbre normée \( A\) (pas spécialement commutative). La formule
    \begin{equation}
        \exp(x)=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }
    \end{equation}
    définit une fonction différentiable dont la différentielle est donnée par\quext{La fonction exponentielle est, j'en suis quasiment certain, de classe \(  C^{\infty}\). Si vous connaissez un moyen pas trop douloureux de prouver cela, faites-le moi savoir.}
    \begin{equation}        \label{EQooFGPPooZKHeXU}
        (d\exp)_x(y)=\sum_{j,j\in \eN}\frac{ x^iyx^j }{ (i+j+1)! }
    \end{equation}
\end{theoremDef}

\begin{normaltext}
    Nous ne démontrons pas cela ici.

    Il s'agit d'une adaptation de la proposition~\ref{DEFooSFDUooMNsgZY}. Là où il faut faire attention, c'est dans l'équation \eqref{EQooNPKGooVmEYAV} : il n'y a pas \( k\) termes \( x^{k-1}h\) dans \( (x+h)^k\), mais \( k\) termes de la forme \( x^ihx\). C'est pour cela que la différentielle n'est pas donnée par \( T(y)=u_{k-1}(x)y\), mais bien par la somme \eqref{EQooFGPPooZKHeXU}.

    M'est avis en réalité que toute la démonstration du théorème~\ref{PropXFfOiOb} passe facilement au cas présent.
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Séries dans une algèbre normée}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons parler d'exponentielle de matrice. Avant cela, quelque propriétés qui sont valables sur des algèbres normées. Le principal exemple que nous avons en tête est \( \eA=\eM(n,\eC)\).

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooMZZQooEhQsgQ}
    Soit une algèbre normée \( \eA\). Soient une suite d'éléments \( A_k\in \eA\) et un élément \( B\). Nous supposons que la somme \( \sum_{k=0}^{\infty}A_k\) converge. Alors
    \begin{equation}
        B\sum_kA_k=\sum_k(BA_k).
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( N\in \eN\). Nous avons:
    \begin{subequations}
        \begin{align}
            \| \sum_{k=0}^NBA_k-B\sum_{k=0}^{\infty}A_k \|&=\| B\sum_{k=N+1}^{\infty}A_k \| \label{SUBEQooDTNAooWpXOKP}\\
            &\leq \| B \|\| \sum_{k=N+1}^{\infty}A_k \|     \label{SUBEQooJPSJooAqXtOJ}
        \end{align}
    \end{subequations}
    Justifications:
    \begin{itemize}
        \item Pour \eqref{SUBEQooDTNAooWpXOKP}, c'est la linéarité du produit matriciel.
        \item Pour \eqref{SUBEQooJPSJooAqXtOJ}, c'est que la norme est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}. Pour rappel, la norme opérateur en est une par le lemme \ref{LEMooFITMooBBBWGI}.}.
    \end{itemize}
    À droite, la limite \( N\to \infty\) donne zéro parce que \( \| B \|\) est un simple nombre, et \( \| \sum_{k=N+1}^{\infty}A_k \|\) est une queue de suite convergente par hypothèse.

    Nous avons donc bien convergence
    \begin{equation}
        \lim_{N\to \infty}\sum_{k=0}^{N}BA_k=B\sum_{k=0}^{\infty}A_k.
    \end{equation}
\end{proof}

Nous adaptons le produit de Cauchy du théorème \ref{ThokPTXYC} au cas d'une algèbre normée.
\begin{proposition}[\cite{MonCerveau}]      \label{PROPooFMEXooCNjdhS}
    Soient une algèbre normée \( \eA\), un élément \( A\in \eA\), ainsi que des séries convergentes \( \sum_{k=0}^{\infty}a_kA^k\) et \( \sum_{l=0}^{\infty}b_lA^l\). Alors
    \begin{equation}
        \left( \sum_ka_kA^k \right)\left( \sum_lb_lA^l \right)=\sum_{n=0}^{\infty}\big( \sum_{m=0}^na_mb_{n-m} \big)A^n.
    \end{equation}
\end{proposition}

\begin{proof}
    Un calcul :
    \begin{subequations}
        \begin{align}
            \left( \sum_ka_kA^k \right)\left( \sum_lb_lA^l \right) &=\sum_k\big( \sum_lb_lA^l \big)a_kA^k       \label{SUBEQooFAECooWFCaNW}\\
            & = \sum_k\big( \sum_lb_la_kA^{l+k} \big)   \label{SUBEQooDZTHooMwmKxJ}\\
            &=\lim_{K\to\infty} \sum_{k=0}^K\big( \lim_{L\to \infty} \sum_{l=0}^Lb_la_kA^{k+l} \big)\\
            &=\lim_{K\to \infty} \lim_{L\to \infty} \sum_{k=0}^K\sum_{l=0}^Lb_la_kA^{k+l}       \label{SUBEQooISSHooJsyMTv}\\
            &=\lim_{K\to \infty} \lim_{L\to \infty} \sum_{n=0}^{K+L}\sum_{m=0}^na_mb_{n-m}A^n       \label{SUBEQooAWUQooZCHIXH}\\
            &=\lim_{K\to \infty} \sum_{n=0}^{\infty}\sum_{m=0}^na_mb_{n-m}A^m       \label{SUBEQooUVOBooSPGjrA}\\
            &=\sum_{n=0}^{\infty}\sum_{m=0}^na_mb_{n-m}A^m      \label{SUBEQooCGRGooGIDCYv}
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooFAECooWFCaNW}, la proposition \ref{PROPooMZZQooEhQsgQ} nous permet d'entrer l'élément \( \sum_lb_lA^l\in \eA\) dans la somme sur \( k\).
        \item 
            Pour \eqref{SUBEQooDZTHooMwmKxJ}, c'est la même chose.
        \item
            Pour \eqref{SUBEQooISSHooJsyMTv}, la somme sur \( k\) étant finie (pour chaque \( K\)), elle commute avec la limite sur \( L\).
        \item
            Pour \eqref{SUBEQooAWUQooZCHIXH}, c'est une manipulation de sommes finies. On regroupe les termes selon les puissances de \( A\).
        \item
            Pour \eqref{SUBEQooUVOBooSPGjrA}, c'est effectuer la limite sur \( L\) pour \( K\) fixé.
        \item
            Pour \eqref{SUBEQooCGRGooGIDCYv}, l'expression dans la limite sur \( K\) ne dépend pas de \( K\). Donc nous pouvons simplement supprimer la limite.
    \end{itemize}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Exponentielle de matrice}
%---------------------------------------------------------------------------------------------------------------------------
\label{SECooBYQBooZifJsg}

\begin{proposition}[\cite{BIBooROFBooCcFjms}]
    Soient des matrices \( A,P\in \eM(n,\eC)\) telle que \( P\) soit inversible. Alors\footnote{La définition de l'exponentielle de matrice est \ref{DEFooSFDUooMNsgZY} où la convergence de la somme est celle de la norme opérateur \ref{DefNFYUooBZCPTr}.}
    \begin{equation}
        e^{P^{-1}AP}=P^{-1} e^{A}P.
    \end{equation}
\end{proposition}

\begin{proof}
    Une première chose à remarquer est que pour chaque\( m\in \eN\) nous avons \( (P^{-1}AP)^p=P^{-1} A^mP\). Ensuite,
    \begin{equation}
        e^{P^{-1}AP}=\sum_k\frac{(P^{-1}AP)^k}{ k! }=\sum_k\frac{ P^{-1}A^kP }{ k! }=P^{-1}\sum_k\frac{ A^k }{ k! }P.
    \end{equation}
    Nous avons utilisé la proposition \ref{PROPooMZZQooEhQsgQ} pour sortir \( P^{-1}\) à gauche et \( P\) à droite de la somme.
\end{proof}

\begin{proposition}[\cite{BIBooROFBooCcFjms}]       \label{PROPooFLHPooRhLiZE}
    L'exponentielle de matrice vérifie
    \begin{enumerate}
        \item       \label{ITEMooCVALooEfLQCyI}
            \( e^0=\id\)
        \item       \label{ITEMooNGPWooIyPEQt}
            \( A^m e^{A}= e^{A}A^m\)
        \item       \label{ITEMooEOSMooQWjcjA}
            \( ( e^{A})^t= e^{(A^t)}\)
        \item       \label{ITEMooROPJooMarenu}
            Si \( AB=BA\) alors \( A e^{B}= e^{B}A\) et \(  e^{A} e^{B}= e^{B} e^{A}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooCVALooEfLQCyI}]
            Juste substituer \( A=0\) dans la définition. Tous les termes tombent sauf le premier. Il faut utiliser le fait que \( A^0=\id\).
        \item[Pour \ref{ITEMooNGPWooIyPEQt}]
            Il faut utiliser la proposition \ref{PROPooMZZQooEhQsgQ} pour écrire
            \begin{equation}        \label{EQooLUUVooCtUtIC}
                A^m\sum_k\frac{ A^k }{ k! }=\sum_k\frac{ A^mA^k }{ k! }=\sum_{k}\frac{ A^kA^m }{ k! }=\sum_k\frac{ A^k }{ k! }A^m.
            \end{equation}
        \item[Pour \ref{ITEMooEOSMooQWjcjA}]
            Pour chaque \( k\) nous avons l'égalité \( (A^k)^t=(A^t)^k\). En utilisant encore le coup de la queue de suite qui converge vers zéro,
            \begin{equation}
                \| \sum_{k=0}^N\frac{ (A^t)^k }{ k! }-( e^{A})^t \|=\| \sum_{k=N+1}^{\infty}\frac{ (A^t)^k }{ k! } \|\to 0.
            \end{equation}
        \item[Pour \ref{ITEMooROPJooMarenu}]
            Pour prouver \( A e^{B}= e^{B}A\), c'est le même genre de manipulations que \eqref{EQooLUUVooCtUtIC}.

            Maintenant, vu que \( A\) et \( e^B\) commutent, l'égalité à peine prouvée montre que \(  e^{A}\) et \(  e^{B}\) commutent.
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{BIBooROFBooCcFjms}]       \label{PROPooKDKDooCUpGzE}
    Soient \( A\in \eM(n,\eC)\) ainsi que \( s,t\in \eC\). Alors
    \begin{equation}
         e^{sA} e^{tA} = e^{(s+t)A}.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous calculons le produit \( e^{sA} e^{tA}\) par le produit de Cauchy de la proposition \ref{PROPooFMEXooCNjdhS} :
    \begin{equation}
        \clubsuit = \left( \sum_k\frac{ t^k }{ k! }A^k \right)\left( \sum_l\frac{ s^l }{ l! }A^l \right)=\sum_{n=0}^{\infty}\sum_{m=0}^n\frac{ t^m }{ m! }\frac{ s^{n-m} }{ (n-m)! }A^n.
    \end{equation}
    À ce point, nous multiplions et divisons par \( n!\) et nous réarrangons la somme de la façon suivante :
    \begin{equation}
        \clubsuit = \sum_{n=0}^{\infty}\frac{ A^n }{ n! }\sum_{m=0}^n\frac{ n! }{ m!(n-m)! }t^ms^{n-m}.
    \end{equation}
    Nous reconnaissons la somme sur \( m\) comme étant un binôme de Newton\footnote{Proposition \ref{PropBinomFExOiL}.} pour \( (t+s)^n\). Nous avons donc finalement
    \begin{equation}
        \clubsuit = \sum_{n=0}^{\infty}\frac{ \big( (t+s)A \big)^n }{ n! }= e^{(t+s)A}.
    \end{equation}
\end{proof}

La proposition suivante dit que les exponentielles de matrices sont inversibles. Elle ne dit pas que toutes les matrices inversibles sont des exponentielles. Ce sera la proposition \ref{PropKKdmnkD}.
\begin{proposition}[\cite{BIBooROFBooCcFjms}]       \label{PROPooRERRooMutKcg}
    Si \( A\in \eM(n,\eC)\), alors \(  e^{A}\) est inversible et 
    \begin{equation}
        ( e^{A})^{-1}= e^{-A}.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de prendre \( s=1\) et \( t=-1\) dans la proposition \ref{PROPooKDKDooCUpGzE} et nous avons
    \begin{equation}
        e^{A} e^{-A}= e^{0}=\mtu.
    \end{equation}
    Cela prouve que \(  e^{A}\) est inversible et que son inverse est \(  e^{-A}\).
\end{proof}

\begin{proposition}[\cite{BIBooROFBooCcFjms}]       \label{PROPooSDNNooQtHkhA}
    Soit \( A\in \eM(n,\eC)\). Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eR&\to \GL(n,\eC) \\
            t&\mapsto  e^{tA}. 
        \end{aligned}
    \end{equation}
    Nous avons la formule de dérivation
    \begin{equation}
        f'(t)=A e^{tA}.
    \end{equation}
\end{proposition}

\begin{proof}
    Il s'agit de calculer la limite en utilisant la proposition \ref{PROPooKDKDooCUpGzE} :
    \begin{subequations}
        \begin{align}
            f'(t)&=\lim_{\epsilon\to 0}\frac{  e^{(t+\epsilon)A}- e^{tA} }{ \epsilon }\\
            &= e^{tA}\lim_{\epsilon\to 0}\frac{  e^{\epsilon A}-\mtu }{ \epsilon }\\
            &= e^{tA}\lim_{\epsilon\to 0}\frac{1}{ \epsilon }\big( \epsilon A+\sum_{k=2}^{\infty}\frac{ (\epsilon A)^k }{ k! } \big)\\
            &= e^{tA}\lim_{\epsilon\to 0}\big( A+\sum_{k=2}^{\infty}\frac{ \epsilon^{k-1}A }{ k! } \big)\\
            &=  e^{tA}A\\
            &= A e^{tA}
        \end{align}
    \end{subequations}
    où à la toute fin nous avons aussi utilisé la commutation de la proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooNGPWooIyPEQt}.
\end{proof}

Le théorème suivant montre que le produit d'exponentielle de matrices suit la règle usuelle tant que les matrices commutent. Cela est cependant plutôt l'exception que la règle. A priori nous avons \(  e^{A} e^{B}\neq  e^{A+B}\).
\begin{theorem}[\cite{BIBooROFBooCcFjms}]       \label{THOooXCPEooYGyLOp}
    Soient \( A,B\in \eM(n,\eC)\) telles que \( AB=BA\). Alors
    \begin{equation}
        e^{A+B}= e^{A} e^{B}.
    \end{equation}
\end{theorem}

\begin{proof}
    Vu que \( A\) et \( B\) commutent nous avons \( A e^{tB}= e^{tB}A\) (proposition \ref{PROPooFLHPooRhLiZE}\ref{ITEMooROPJooMarenu}). Ensuite nous posons
    \begin{equation}
        g(t)= e^{t(A+B)} e^{-tB} e^{-tA}.
    \end{equation}
    Nous calculons la dérivée de \( g\) en utilisant la règle de Leibnitz et la proposition \ref{PROPooSDNNooQtHkhA} :
    \begin{equation}
        \begin{aligned}[]
            g'(t)&=(A+B) e^{t(A+B)} e^{-tB} e^{-tA}\\
            &\quad + e^{t(A+B)}(-B) e^{-tB} e^{-tA}\\
            &\quad +  e^{t(A+B)} e^{-tB}(-A) e^{-tA}.
        \end{aligned}
    \end{equation}
    Vu que \( A\), \( B\) et \( A+B \) commutent, nous pouvons réarranger les facteurs en
    \begin{equation}
        \begin{aligned}[]
            g'(t)&=(A+B) e^{t(A+B)} e^{-tB} e^{-tA}\\
            &\quad -B e^{t(A+B)} e^{-tB} e^{-tA}\\
            &\quad -A  e^{t(A+B)} e^{-tB} e^{-tA}.
        \end{aligned}
    \end{equation}
    Enfin, cela fait
    \begin{equation}
        g'(t)=(A+B-B-A) e^{t(A+B)} e^{-tB} e^{-tA}=0.
    \end{equation}
    Donc \( g\) est constante et nous avons
    \begin{equation}
        e^{t(A+B)} e^{-tB} e^{-tA}=g(0)=\mtu.
    \end{equation}
    En multipliant à droite par \(  e^{tA} e^{tB}\) nous trouvons
    \begin{equation}
        e^{t(A+B)}= e^{tA} e^{tB}
    \end{equation}
    comme annoncé.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exponentielle et logarithme dans les réels}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Pour avoir une vue synthétique du plan, voir le thème \ref{THEMEooKXSGooCsQNoY}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{L'équation différentielle}
%---------------------------------------------------------------------------------------------------------------------------

Pour la suite nous notons \( y\) une solution de l'équation \( y'=y\), \( y(0)=1\), et nous allons en donner des propriétés indépendamment de l'existence, donnée par le théorème~\ref{ThoKRYAooAcnTut}.

\begin{proposition} \label{PropTLECooEiLbPP}
    Quelques propriétés de \( y\) (si elle existe) :
    \begin{enumerate}
        \item
            Pour tout \( x\in \eR\) nous avons \( y(x)y(-x)=1\).
        \item
            \( y(x)>0\) pour tout \( x\).
        \item
            \( y\) est strictement croissante.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons \( \varphi(x)=y(x)y(-x)\) et nous dérivons :
    \begin{equation}
        \varphi'(x)=y'(x)y(-x)-y(x)y'(-x)=0.
    \end{equation}
    Donc \( \varphi\) est constante\footnote{Proposition~\ref{PropGFkZMwD}.}. Vu que \( \varphi(0)=1\) nous avons automatiquement \( y(x)y(-x)=1\) pour tout \( x\).

Les deux autres allégations sont simples : si \( y(x_0)<0\) alors il existe \( t\in\mathopen] x_0 , 1 \mathclose[\) tel que \( y(t)=0\), ce qui est impossible parce que \( y(t)y(-t)=1\). La stricte croissance de \( y\) s'ensuit.
\end{proof}

\begin{proposition}     \label{PROPooGGUIooExVHPM}
    Quelques formules pour tout \( a,b\in \eR\) et \( n\in \eZ\) :
    \begin{enumerate}
        \item       \label{ITEMooMPSUooWQpVQJ}
            \( y(a+b)=y(a)y(b)\)
        \item
            \( y(na)=y(a)^n\)
        \item
            \( y\left( \frac{ a }{ n } \right)=\sqrt[n]{y(a)}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons \( h(x)=y(a+b-x)y(x)\) et nous avons encore \( h'(x)=0\) dont nous déduisons que $h$ est constante. De plus
    \begin{equation}
        h(0)=y(a+b)y(0)=y(a+b)
    \end{equation}
    et
    \begin{equation}
        h(b)=y(a)y(b).
    \end{equation}
    Vu que \( h\) est constante, ces deux expressions sont égales : \( y(a+b)=y(a)y(b)\).

    Forts de cette relation, une récurrence donne \( y(na)=y(a)^n\) pour tout \( n\in \eN\). De plus
    \begin{equation}
        y(a)=y\left( \frac{ a }{ n }\times n \right)=y\left( \frac{ a }{ n } \right)^n,
    \end{equation}
    ce qui donne \( y(a)=y(a/n)^n\) ou encore \( y(a/n)=\sqrt[n]{y(a)}\).

    Enfin pour les négatifs, si \( n\in \eN\),
    \begin{equation}
        y(-na)=\frac{1}{ y(na) }=\frac{1}{ y(a)^n }=y(a)^{-n}.
    \end{equation}
    Et de la même façon,
    \begin{equation}
        y\left( -\frac{ a }{ n } \right)=\frac{1}{ y\left( \frac{ a }{ n } \right) }=\sqrt[n]{\frac{1}{ y(a) }}=\sqrt[-n]{y(a)}.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Existence}
%---------------------------------------------------------------------------------------------------------------------------

Jusqu'ici nous avons donné des propriétés d'une éventuelle fonction \( y\) qui vérifierait l'équation différentielle. Il est temps de montrer qu'une telle fonction existe.

\begin{theorem} \label{ThoKRYAooAcnTut}
    La série entière
    \begin{equation}    \label{EqEIGZooKWSvPS}
        \exp(x)=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }
    \end{equation}
    définit une fonction dérivable solution de
    \begin{subequations}
        \begin{numcases}{}
            y'=y        \label{EQooSEIHooNmQKiC}\\
            y(0)=1.
        \end{numcases}
    \end{subequations}
\end{theorem}
\index{exponentielle!existence}

\begin{proof}
    La formule de Hadamard (théorème~\ref{ThoSerPuissRap}) donne le rayon de convergence de la série \eqref{EqEIGZooKWSvPS} par
    \begin{equation}
        \frac{1}{ R }=\lim_{k\to \infty} \frac{ \frac{1}{ (k+1)! } }{ \frac{1}{ k! } }=\lim_{k\to \infty} \frac{1}{ k+1 }=0.
    \end{equation}
    Donc nous avons un rayon de convergence infini. La fonction \( y\) est définie sur \( \eR\) et la proposition~\ref{ProptzOIuG} nous dit que \( y\) est dérivable. Nous pouvons aussi dériver terme à terme :
    \begin{equation}
            y'(x)=\sum_{k=0}^{\infty}\frac{ kx^{k-1} }{ k! }=\sum_{k=1}^{\infty}\frac{ kx^{k-1} }{ k! }=\sum_{k=1}^{\infty}\frac{ x^{k-1} }{ (k-1)! }=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }=y(x).
    \end{equation}
    Notez le petit jeu d'indice de départ de \( k\). Dans un premier temps, nous remarquons que \( k=0\) donne un terme nul et nous le supprimons, et dans un second temps nous effectuons la simplification des factorielles (qui ne fonctionne pas avec \( k=0\)).
\end{proof}

\begin{normaltext}
    Nous savons que la fonction \( y\) existe parce qu'une solution de l'équation différentielle \( y'=y\), \( y(0)=1\) est donnée par la fameuse série (théorème \ref{ThoKRYAooAcnTut}). À part cela, ce qui a été fait avec cette équation différentielle ne permet pas de prouver l'existence de \( y\). Donc, du point de vue de «définir l'exponentielle par son équation différentielle», c'est pas encore gagné. Notons au passage que le nombre \( e\) n'est pas encore bien défini via l'équation différentielle.
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Le nombre de Neper \( e\)}
%---------------------------------------------------------------------------------------------------------------------------

Nous savons par le théorème \ref{ThoKRYAooAcnTut} que \( x\mapsto \exp(x)\) est une solution de l'équation différentielle exponentielle (avec la bonne condition initiale). Or une telle solution est unique par la proposition \ref{PropDJQSooYIwwhy}.

\begin{definition}[Le nombre de Neper]
    Nous notons \( e\) le nombre \( \exp(1)\).
\end{definition}

\begin{proposition}     \label{PropCELWooLBSYmS}
    Pour tout \( x\in \eR\), nous avons
    \begin{equation}        \label{EQooBFIHooKopcmf}
        \exp(x)=e^x.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( y\) vérifiant la fameuse équation différentielle. Nous savons que \( y=\exp\) parce que c'est l'unique solution (proposition \ref{PropDJQSooYIwwhy}). Nous avons :
    \begin{equation}
        y(x)=y(1)^x.
    \end{equation}
    Si \( q\in \eQ\) alors \( q=a/b\) et
    \begin{equation}
        y(q)=y\left( \frac{ a }{ b } \right)=y\left( a\times \frac{1}{ b } \right)=y\left( \frac{1}{ b } \right)^a=\big( \sqrt[b]{y(1)} \big)^a=y(1)^{a/b}=y(1)^{q}.
    \end{equation}
    Le résultat est prouvé pour les rationnels.

    En ce qui concerne un élément général \( x\in \eR\), la fonction \( x\mapsto y(x)\) est continue sur \( \eR\), et la fonction \( x\mapsto e^x\) également (proposition \ref{DEFooOJMKooJgcCtq}). Ces deux fonctions étant égales sur \( \eQ\), elles sont égales sur \( \eR\) par la proposition  \ref{PropCJGIooZNpnGF}).
\end{proof}

Une conséquence des propositions \ref{PropCELWooLBSYmS} et \ref{PROPooGCBZooTcyGtO} est que 

\begin{subequations}    \label{EqLOIUooHxnEDn}
    \begin{align}
        \lim_{x\to -\infty}  e^{x}=0\\
        \lim_{x\to +\infty}  e^{x}=+\infty,
    \end{align}
\end{subequations}
et en particulier, 
\begin{equation}
    \begin{aligned}
    \exp\colon \eR&\to \mathopen] 0 , \infty \mathclose[ \\
        x&\mapsto  e^{x} 
    \end{aligned}
\end{equation}
est une bijection.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Application réciproque : logarithme}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef}    \label{DEFooELGOooGiZQjt}
    L'application \(\exp\colon \eR\to \mathopen] 0 , \infty \mathclose[\) est une bijection. L'application réciproque
    \begin{equation}
        \ln\colon \mathopen] 0 , \infty \mathclose[\to \eR
    \end{equation}
    est le \defe{logarithme}{logarithme!sur les réels positifs}.
\end{propositionDef}

\begin{proof}
Le fonction exponentielle est dérivable, toujours strictement positive, donc strictement croissante. Les limites en \( \pm \infty\) sont \( 0\) et \( +\infty\). Le théorème des valeurs intermédiaires~\ref{ThoValInter} nous dit que c'est une bijection. En effet, l'injectivité est la stricte croissance. En ce qui concerne la surjection, soit \( y\in \mathopen] 0 , \infty \mathclose[\). Vu que la limite en \( -\infty\) est zéro, il existe \( A\in \eR\) tel que \( \exp(x)<y\) pour tout \( x<A\), et de la même façon, il existe \( B\in \eR\) tel que \( \exp(x)>y\) pour tout \( x>B\). Si \( a<A\) et \( b>B\) alors \( \exp(a)<y\) et \( \exp(b)>y\), donc \( y\) est dans l'image de \( \mathopen[ a , b \mathclose]\) par l'exponentielle.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooCYGTooEjXEUu}
    Le logarithme est une fonction continue.
\end{lemma}

\begin{proof}
    C'est une conséquence du théorème de la bijection \ref{ThoKBRooQKXThd}\ref{ItemEJZooKuFoeFiv}, et de la continuité de l'exponentielle sur \( \eR\), qui est une partie du théorème \ref{ThoKRYAooAcnTut}.
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooLAOWooEYvXmI}
    Pour tout \( x,y\in \eR\) et pour \( a>0\) nous avons
    \begin{equation}
        \ln(\frac{1}{ x })=-\ln(x),
    \end{equation}
    et
    \begin{equation}        \label{EQooJVMUooVpUKyo}
        \ln(xy)=\ln(x)+\ln(y),
    \end{equation}
    et
    \begin{equation}        \label{EQooEJQSooWCczXy}
        \ln(a^x)=x\ln(a)
    \end{equation}
    et
    \begin{equation}
        a^x= e^{x\ln(a)}.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous avons, par la proposition \ref{PROPooVADRooLCLOzP},
    \begin{equation}
        e^{-\ln(x)}=\frac{1}{  e^{\ln(x)} }=\frac{1}{ x }.
    \end{equation}
    En prenant le logarithme des deux côtés nous trouvons
    \begin{equation}
        -\ln(x)=\ln\left( \frac{1}{ x } \right).
    \end{equation}

    Nous pouvons continuer avec la suivante.

    Par définition, \( \ln(xy)\) est donné par \( \exp\big( \ln(xy) \big)=xy\). Mais nous avons aussi, par la proposition \ref{PROPooVADRooLCLOzP} :
    \begin{equation}
        e^{\ln(x)+\ln(y)}=e^{\ln(x)}e^{\ln(y)}=xy.
    \end{equation}
    Nous avons donc démontré \eqref{EQooJVMUooVpUKyo}.

    La relation \eqref{EQooEJQSooWCczXy} de démontre d'abord pour \( x\in \eN\), puis pour \( x\in \eQ\) et enfin pour \( x\in\eR\). Si \( n\in \eN\) alors la relation \eqref{EQooJVMUooVpUKyo} donne immédiatement
    \begin{equation}
        \ln(a^n)=n\ln(a).
    \end{equation}
    pour tout \( x\in \eR\).

    Si \( m,n\in \eN\), le nombre \( a^{n/m}\) est par définition le \( x>0\) tel que
    \begin{equation}
        x^m=a^n.
    \end{equation}
    En prenant le logarithme des deux côtés : \( \ln(x^m)=\ln(a^n)\) et en utilisant la relation déjà démontrée pour \( \eN\) nous trouvons \( m\ln(x)=n\ln(a)\) et donc
    \begin{equation}
        \ln(a^{m/n})=\ln(x)=\frac{ m }{ n }\ln(a).
    \end{equation}
    La relation est donc démontré pour \( \ln(a^q)\) avec \( q\in \eQ^+\).

    Nous passons à \( q=-m/n\in \eQ^-\), c'est-à-dire toujours \( m,n\in \eN\). Nous avons, en utilisant la proposition \ref{PROPooLAOWooEYvXmI},
    \begin{equation}
        \ln(a^{-q})=\ln(\frac{1}{ a^q })=-\ln(a^q)=-q\ln(a).
    \end{equation}

    Enfin si \( x\in \eR\) nous considérons une suite de rationnels \( x_k\to x\). Pour chaque \( k\) nous avons
    \begin{equation}
        \ln(a^{x_k})=x_k\ln(a).
    \end{equation}
    Nous prenons la limite deux deux côtés. À droite nous avons tout de suite \( x\ln(a)\), et à gauche, par continuité de la fonction \( \ln\) (lemme \ref{LEMooCYGTooEjXEUu}) et de la fonction puissance (définition \ref{DEFooOJMKooJgcCtq}) nous trouvons \( \ln(a^x)\).
\end{proof}

La formule \eqref{EQooEJQSooWCczXy} en particulier est pratique pour réexprimer des fonctions puissances compliquées en écrivant
\begin{equation}        \label{EQooYEWCooKyravP}
    a^x= e^{\ln(a^x)}= e^{x\ln(a)}.
\end{equation}
Cela aide à calculer la dérivée de \( x\mapsto a^x\). 

Notons que certains prennent \eqref{EQooYEWCooKyravP} comme définition de la fonction puissance.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Approximations numériques de \( e\)}
%---------------------------------------------------------------------------------------------------------------------------

Nous donnons maintenant quelques approximations numériques de \( e\), particulièrement inefficaces.

\begin{lemma}
    Nous avons
    \begin{equation}
        2<e<3.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous savons que \( y(0)=1\) et \( y'(0)=1\). La fonction \( y\) est strictement croissante (et donc sa dérivée aussi). Nous avons donc \( y'(x)>1\) pour tout \( x\in\mathopen] 0 , 1 \mathclose]\), et donc
    \begin{equation}
        y(1)>1+1\times 1=2.
    \end{equation}
    Sachant que \( 2>y'(x)\) pour tout \( x\in \mathopen] 0 , 1 \mathclose[\) nous pouvons refaire le coup de l'approximation affine, cette fois en majorant :
        \begin{equation}
            y(1)<1+2\times 1=3.
        \end{equation}
\end{proof}

De la même façon nous savons que
\begin{equation}
    y(\frac{1}{ n })>1+\frac{1}{ n }
\end{equation}
parce que \( y'\) est minoré par \( 1\) sur \( \mathopen] 0 , \frac{1}{ n } \mathclose[\). Avec cela nous avons aussi la majoration
\begin{equation}
    y(\frac{1}{ n })<1+\frac{1}{ n }\times \left( 1+\frac{1}{ n } \right)=1+\frac{1}{ n }+\frac{1}{ n^2 }.
\end{equation}
Et enfin nous pouvons donner l'encadrement, valable pour tout \( n\) :
\begin{equation}
    \left( 1+\frac{1}{ n } \right)^n<y(1)<\left( 1+\frac{1}{ n }+\frac{1}{ n^2 } \right)^n.
\end{equation}
Pour \( n=10\) nous trouvons
\begin{equation}
    2.50<e<2.83.
\end{equation}

Bien que ce soit à mon avis humainement pas possible à faire à la main nous avons, pour \( n=100\) :
\begin{equation}
    2.70<e<2.7317
\end{equation}
Cela reste un encadrement très modeste.

Une méthode plus efficace consiste à calculer directement le développement de définition
\begin{equation}
    e=\exp(1)=\sum_{k=0}^{\infty}\frac{1}{ n! }.
\end{equation}
\lstinputlisting{tex/sage/sageSnip013.sage}

\begin{probleme}
    Comment trouver, avec cette méthode, un \emph{encadrement pour \( e\) ?}
\end{probleme}

Ce petit programme, avec \( 5\) termes donne \( e\simeq 65/24\simeq 2.708\). Avouez que c'est déjà bien mieux.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Résumé des propriétés de l'exponentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}  \label{ThoRWOZooYJOGgR}
    Les choses que nous savons sur l'exponentielle :
    \begin{enumerate}
        \item       \label{ITEMooEIKKooLNoaRD}
            Il y a unicité de la solution à l'équation différentielle
            \begin{subequations}    \label{subeqBKJNooJQtbBD}
        \begin{numcases}{}
            y'=y\\
            y(0)=1.
        \end{numcases}
    \end{subequations}
    \item
        L'équation différentielle \eqref{subeqBKJNooJQtbBD} possède une solution donnée par la série entière\nomenclature[Y]{\( \exp\)}{exponentielle}
        \begin{equation}    \label{EqUARSooKXnQxu}
        \exp(x)=\sum_{k=0}^{\infty}\frac{ x^k }{ k! }
    \end{equation}
\item
    Cette solution est une bijection \( y\colon \eR\to \mathopen] 0 , \infty \mathclose[\).
    \item   \label{ItemYTLTooSnfhOu}
        La fonction \( y\) ainsi définie est de classe \(  C^{\infty}\).
\item
    Elle est également donnée par la formule
    \begin{equation}
        \exp(x)=e^x
    \end{equation}
    où \( e\) est définit par \( e=\exp(1)\).
\item
    Elle vérifie
    \begin{equation}        \label{EQooVFXUooBfwjJY}
        e^{a+b}= e^{a} e^{b}
    \end{equation}
    \end{enumerate}
\end{theorem}
Nous nommons \defe{exponentielle}{exponentielle} cette fonction.

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
            C'est la proposition~\ref{PropDJQSooYIwwhy}.
        \item
            C'est le théorème~\ref{ThoKRYAooAcnTut}.
        \item
            Le rayon de convergence de la série \eqref{EqUARSooKXnQxu} est infini (théorème~\ref{ThoKRYAooAcnTut}); elle est donc définie sur \( \eR\). Le fait que ce soit une bijection est dû au fait qu'elle est strictement croissante (proposition~\ref{PropTLECooEiLbPP}) ainsi qu'aux limites \eqref{EqLOIUooHxnEDn}.
        \item
            Vu que \( y=y'\), \( y\) est dérivable. Mais comme \( y'\) est alors égale à une fonction dérivable, \( y'\) est dérivable. En dérivant l'égalité \( y'=y\) nous obtenons \( y''=y'\) et le jeu continue.
        \item
            C'est la proposition~\ref{PropCELWooLBSYmS}.
        \item
            C'est la proposition~\ref{PROPooGGUIooExVHPM}\ref{ITEMooMPSUooWQpVQJ}.
    \end{enumerate}
\end{proof}

\begin{example}[Un endomorphisme sans polynôme annulateur\cite{RombaldiO}]     \label{ExooLRHCooMYLQTU}
    l'exponentielle permet de donner un exemple d'un endomorphisme n'ayant pas de polynôme annulateur\footnote{Voir la définition~\ref{DefooOHUXooNkPWaB} et ce qui suit.} : l'endomorphisme de dérivation
    \begin{equation}
        \begin{aligned}
            D\colon C^{\infty}(\eR,\eR)&\to  C^{\infty}(\eR,\eR) \\
            f&\mapsto f'
        \end{aligned}
    \end{equation}
    n'a pas de polynôme annulateur. En effet supposons que \( P=\sum_{k=0}^{p}a_kX^k\) en soit un, et considérons les fonctions \( f_{\lambda}\colon t\mapsto  e^{\lambda t}\). Nous avons
    \begin{equation}
            0=P(D)f_{\lambda}
            =\sum_ka_kD^k(f_{\lambda})
            =\sum_ka_k\lambda^kf_{\lambda}
            =P(\lambda)f_{\lambda}.
    \end{equation}
    Par conséquent \( \lambda\) est une racine de \( P\) pour tout \( \lambda\in \eR\). Cela implique que \( P=0\).

    D'ailleurs si on y pense bien, cet exemple n'est qu'un habillage de l'exemple~\ref{ExooDTUJooIMqSKn}.
\end{example}

\begin{proposition}\label{ExZLMooMzYqfK}
    Quelque propriétés du logarithme.
    \begin{enumerate}
        \item
            Le logarithme est une application dérivable et strictement croissante.
        \item
            Le logarithme est la primitive de \( x\mapsto\frac{1}{ x }\) qui s'annule en \( x=1\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Elle est donc bijective, d'inverse continue et dérivable par le théorème~\ref{ThoKBRooQKXThd} et la proposition~\ref{PropMRBooXnnDLq}.

    La dérivée de la fonction logarithme peut être calculée en utilisant la formule \eqref{EqWWAooBRFNsv}, mais aussi de façon plus piettone en écrivant l'expression suivante, valable pour tout \( x\in \eR\) :
    \begin{equation}
        \ln\big( \exp(x) \big)=x,
    \end{equation}
    que nous pouvons dériver en utilisant le théorème de dérivation des fonctions composées :
    \begin{equation}
        \ln'\big( \exp(x) \big)\exp'(x)=1.
    \end{equation}
    Mais \( \exp'(x)=\exp(x)\), donc
    \begin{equation}
        \ln'(y)=\frac{1}{ y }
    \end{equation}
    pour tout \( y\) dans l'image de \( \exp\), c'est-à-dire pour tout \( y\) dans l'ensemble de définition de \( \ln\).

    Par ailleurs, \( \exp(0)=1\) donc
    \begin{equation}
        \ln(1)=\ln\big( \exp(0) \big)=0.
    \end{equation}

    En ce qui concerne l'unicité d'une primitive s'annulant en \( x=1\), c'est le corollaire~\ref{CorZeroCst}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dérivée de la fonction puissance}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}     \label{EXooGMRIooUucRez}
    Soit la fonction \( f(x,y)=x^y\), définie en \ref{DEFooOJMKooJgcCtq}. Nous allons en calculer les dérivées partielles au point \( (1,2)\). Notons que \( f\) n'est pas définie pour \( x<0\), mais que cela n'a pas d'importance parce que nous pouvons nous restreindre à un voisinage du point \( (1,2)\). La première dérivée partielle est facile :
    \[
        \partial_x f(1,2)=(yx^{y-1})_{(x,y)=(1,2)}=2.
    \]
    Pour la seconde, il faut utiliser les propriétés de l'exponentielle et du logarithme. D'abord le logarithme est par définition l'application réciproque de l'exponentielle (définition \ref{DEFooELGOooGiZQjt}), donc 
    \begin{equation}
        x^y=\exp\big( \ln(x^y) \big).
    \end{equation}
    Ensuite nous calculons en utilisant la proposition \ref{PROPooLAOWooEYvXmI} :
    \[
        \partial_y f(1,2)=\partial_y\left(e^{y\ln x}\right)_{(x,y)=(1,2)}=\left(\ln x e^{y\ln x}\right)_{(x,y)=(1,2)}=\ln\big( 1- e^{2\ln(1)} \big)=0.
    \]
\end{example}


Cet exemple est facilement généralisable aux fonctions de la forme \( x\mapsto u(x)^{v(x)}\). Voici une proposition qui dit comment faire.
\begin{proposition}[\cite{MonCerveau}]     \label{PROPooKUULooKSEULJ}
    Soit une fonction dérivable \( u\colon \eR\to \eR\) et \( a>0\). Nous avons
    \begin{equation}
        \left( a^u\right)'=u'\ln(a)a^u.
    \end{equation}

    Si de plus \( u(x)>0\) pour tout \( x\), nous avons
    \begin{equation}
        \left( u^a \right)'=au'u^{a-1}.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous considérons la fonction \( f(x)= a^{u(x)}\). Vu que \( f(x)>0\) pour tout \( x\), nous pouvons en prendre le logarithme et écrire l'égalité, valable pour tout \( x\) :
    \begin{equation}
        f(x)= e^{\ln(a^{u(x)})}=\exp\big( u(x)\ln(a) \big).
    \end{equation}
    Sachant la dérivée de l'exponentielle, cela n'est rien d'autre que la dérivée d'une fonction composée :
    \begin{equation}
        f'(x)=\ln(a) u'(x) e^{u(x)\ln(a)}.
    \end{equation}
    
    Pour l'autre, nous posons 
    \begin{equation}
        g(x)=u(x)^a,
    \end{equation}
    qui peut encore s'écrire sous la forme
    \begin{equation}
        g(x)= e^{a\ln\big( u(x) \big)}.
    \end{equation}
    Ici encore, c'est la dérivée de fonctions composées qui donne le résultat.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dérivée du logarithme}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooTGCBooJdkLpg}
Si \( u\colon \eR\to \mathopen] 0 , \infty \mathclose[\) est dérivable alors \( \ln(u)'=\dfrac{ u' }{ u }\).
\end{lemma}

\begin{proof}
    Cela est une conséquence du théorème de dérivation des fonctions composées : si \( g(x)=\ln(u(x))\) alors
    \begin{equation}
        g'(x)=\ln'\big( u(x) \big)u'(x)=\frac{1}{ u(x) }u'(x).
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Taylor pour l'exponentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Développement de l'exponentielle]       \label{PROPooQBRGooAhGrvP}
    Pour tout entier \( n\), il existe une fonction \( \alpha\colon \eR\to \eR\) telle que \( \lim_{t\to 0} \alpha(t)=0\) et
    \begin{equation}
        e^x=\sum_{k=0}^n\frac{ x^k }{ k! }+x^n\alpha(x).
    \end{equation}
\end{proposition}

\begin{proof}
    Il s'agit de la proposition \ref{PROPooQLHNooRsBYbe} appliquée à la série entière \eqref{DEFooSFDUooMNsgZY}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Analycité}
%---------------------------------------------------------------------------------------------------------------------------

Vu que \( \exp(x)\) est défini par une série entière (définition \ref{DEFooSFDUooMNsgZY}) et vu la proposition \ref{PROPooTRWVooETTtbP}, il n'est pas étonnant que \( \exp\) soit analytique. Traitons ce cas.

\begin{example}[Analycité de l'exponentielle]
   Soient \( a\in \eR\) et \( R>0\). Nous démontrons que \( \exp\) est analytique sur \( B(a,R)\). Si \( f(x)= e^{x}\), alors \( f^{(n)}(x)= e^{x}\) pour tout \( n\) (équation \eqref{EQooSEIHooNmQKiC}). Nous avons donc
   \begin{equation}
       | f^{(n)}(x) |< e^{a+R}
   \end{equation}
   pour tout \( x\in B(a,R)\). Nous partons de l'expression \eqref{THOooEUVEooXZJTRL} du reste :
   \begin{equation}
       | R_n(x) |\leq \frac{ M_n }{ (n+1)! }| x-a |^{n+1}\leq \frac{  e^{a+R} }{ (n+1)! }R^{n+1}.
   \end{equation}
   Mais nous avons la limite 
   \begin{equation}
       \lim_{n\to \infty} \frac{ R^{n+1} }{ (n+1)! }=0
   \end{equation}
   pour tout \( R\). 

   Donc avec les polynômes de Taylor \( P_n\) calculés en \( a\), nous avons \( P_n\to \exp\) simplement sur \( \eR\).

   Nous pouvons donc développer la fonction exponentielle autour de n'importe quel point, et avoir convergence des polynômes vers l'exponentielle sur tout \( \eR\). Vous accepterez cependant que si \( a\) et \( x\) sont éloignés, la convergence \( P_n(x)\to \exp(x)\) peut être extrêmement lente.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Autres propriétés et petits calculs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemPEYJooEZlueU}
Si \( a,b\in\mathopen] 0 , \infty \mathclose[\) alors
    \begin{equation}
        \ln(ab)=\ln(a)+\ln(b)
    \end{equation}
    et
    \begin{equation}    \label{EqOOZGooOWkGlA}
        \ln\left( \frac{1}{ b } \right)=-\ln(b).
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons \( f(x)=\ln(ax)\) qui est une fonction dérivable. Alors \( f'(x)=\frac{ a }{ ax }=\frac{1}{ x }\). Cette fonction \( f\) est donc une primitive de \( \frac{1}{ x }\) et il existe une constante \( K\) telle que
    \begin{equation}
        f(x)=\ln(x)+K.
    \end{equation}
    Vu que \( \ln(1)=0\) nous avons \( K=f(1)= \ln(a)\). Donc
    \begin{equation}
        \ln(ax)=\ln(x)+\ln(a).
    \end{equation}

    En ce qui concerne la seconde formule à démontrer, nous avons
    \begin{equation}
        \ln(1)=\ln\left( \frac{1}{ b }b \right)=\ln\left( \frac{1}{ b } \right)+\ln(b).
    \end{equation}
    Étant donné que $\ln(1)=0$ nous en déduisons la formule \eqref{EqOOZGooOWkGlA}.
\end{proof}

\begin{lemma}
    Si les suites \( (u_n)\) et \( (v_n)\) sont équivalentes\footnote{Définition \ref{DEFooEWRTooKgShmT}.} et si \( (v_n)\) admet une limite \( l\) différente de \( 1\), alors les suites \( (\ln u_n)\) et \( (\ln v_n)\) sont équivalentes.
\end{lemma}

\begin{proof}
    En effet si \( u_n=v_n\alpha(n)\) alors en utilisant la formule du lemme \ref{LemPEYJooEZlueU},
    \begin{equation}
        \ln(u_n)=\ln(v_n)+\ln\big( \alpha(n) \big)=\ln(v_n)\left( 1+\frac{ \ln\big( \alpha(n) \big) }{ \ln(v_n) } \right),
    \end{equation}
    et comme \( \alpha(n)\to 1\), la parenthèse tend vers \( 1\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Taylor pour le logarithme}
%---------------------------------------------------------------------------------------------------------------------------

Vu que \( \ln(0)\) n'existe pas, il n'est pas question de développer \( \ln\) autour de \( x=0\). À la place, nous allons le développer autour de \( x=1\) et plus précisément nous allons étudier Taylor pour la fonction \( f(x)=\ln(1+x)\). Les résultats seront résumés dans la proposition \ref{PROPooKPBIooJdNsqX}.

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooWCUEooJudkCV}
    Soit la fonction
    \begin{equation}
        \begin{aligned}
        f\colon \mathopen] -1 , \infty \mathclose[&\to \eR \\
            x&\mapsto \ln(1+x). 
        \end{aligned}
    \end{equation}
    Pour tout \( n\), il existe une fonction \( \alpha\colon \eR\to \eR\) telle que \( \lim_{t\to 0} \alpha(t)=0\) et
    \begin{equation}
        f(x)=\sum_{k=1}^n\frac{ (-1)^{k+1} }{ k }x^k+\alpha(x)x^n
    \end{equation}
    pour tout \( x\) dans le domaine de \( f\).

    Notez la somme qui part de \( k=1\) et non \( k=0\).
\end{proposition}

\begin{proof}
    Nous utilisons la formule de Taylor-Young (proposition \ref{PropVDGooCexFwy}). La première dérivée de \( f\) se calcule en utilisant le lemme \ref{LEMooTGCBooJdkLpg} :
    \begin{equation}
        f'(x)=\frac{1}{ 1+x }.
    \end{equation}
    Pour les dérivées suivantes, c'est juste du calcul et nous pouvons prouver par récurrence que
    \begin{equation}        \label{EQooKEAOooGmTLJF}
        f^{(k)}(x)=\frac{ (k-1)!(-1)^{k+1} }{ (1+x)^k }.
    \end{equation}
    En ce qui concerne l'évaluation en zéro :
    \begin{equation}
        f^{(k)}(0)=\begin{cases}
            0    &   \text{si } k=0\\
            (k-1)!(-1)^{k+1}    &    \text{sinon.}
        \end{cases}
    \end{equation}
    Du fait que \( f^{(0)}(0)=\ln(1)=0\), la somme commence à \( k=1\) et non \( k=0\). Nous avons
    \begin{equation}
        f(x)=\sum_{k=1}^{n}\frac{ f^{(k)}(0) }{ k! }x^k+\alpha(x)x^n=\sum_{k=1}^n\frac{ (-1)^{k+1} }{ k }x^k+\alpha(x)x^n.
    \end{equation}
\end{proof}

Nous étudions les polynômes de la série de Taylor pour
\begin{equation}
    \begin{aligned}
    f\colon \mathopen] -1 , \infty \mathclose[&\to \eR \\
        x&\mapsto \ln(1+x). 
    \end{aligned}
\end{equation}

Les dérivées successives de \( f\) ont déjà été calculées en \eqref{EQooKEAOooGmTLJF}. Nous développons autour de \( x=0\). Donc \( f(0)=\ln(1)=0\) et pour les autres,
\begin{equation}
    f^{(k)}(0)=(-1)^{k+1}(k-1)!.
\end{equation}
Pour les polynômes de Taylor, nous avons
\begin{equation}
    P_n(x)=\sum_{k=1}^n\frac{ (-1)^{k+1} }{ k }x^k
\end{equation}
où vous noterez la somme qui part de \( k=1\) et non de \( k=0\). Nous avons aussi la série de Taylor de \( f\) donnée par
\begin{equation}        \label{EQooTAREooKfpTPo}
    T(x)=\sum_{k=1}^{\infty}\frac{ (-1)^{k+1} }{ k }x^k.
\end{equation}
La somme est une limite ponctuelle, là où elle existe.

Jusqu'à présent, la seule certitude à props de \( T\) est que \( T(0)=f(0)=0\). Pour le reste :
\begin{itemize}
    \item Rien ne dit que \( T(x)\) existe pour d'autres \( x\) que \( x=1\).
    \item Et même si \( T(x)\) existait pour d'autres \( x\) (c'est-à-dire si le rayon de convergence de \eqref{EQooTAREooKfpTPo} était strictement plus grand que zéro), rien n'assurerait que la valeur serait celle de \( f\).
    \item Et même si \( T(x)\) convergeait vers \( f\) sur son disque de convergence, ce ne serait pas encore assez pour dire que \( f\) est analytique, parce que l'analycité demande que les séries de Taylor autour de \emph{chaque} point converge vers \( f\). Or ici nous ne parlons encore que de \( T\) qui est la série autour de \( x=0\).
\end{itemize}

\begin{lemma}       \label{LEMooWMGGooRpAxBa}
    La série de Taylor de \( x\mapsto \ln(1+x)\) autour de \( x=0\) converge sur \( \mathopen] -1 , 1 \mathclose]\). Elle ne converge pas pour \( x=-1\).
\end{lemma}

\begin{proof}
        
    En ce qui concerne le rayon de convergence de \( T\), nous utilisons la formule de Hadamard\footnote{Théorème \ref{ThoSerPuissRap}.} avec
    \begin{equation}
        a_k=\frac{ (-1)^{k+1} }{ k }.
    \end{equation}
    Ce que nous trouvons est
    \begin{equation}
        \frac{1}{ R }=\lim_{k\to \infty} | \frac{ a_{k+1} }{ a_k } |=\lim_{k\to \infty} \frac{ k }{ k+1 }=1.
    \end{equation}
    Le rayon de convergence de \( T\) est donc \( 1\). Nous avons donc que \( P_n\to T\) sur \( \mathopen] -1 , 1 \mathclose[\), et peut-être que \( P_n\to T\) en \( x=\pm 1\).

    Pour \( x=-1\). L'intuition nous dit que ce serait \( \ln(0)\) qui n'est pas défini. C'est le cas parce que 
    \begin{equation}
        P_n(-1)=\sum_{k=1}^n\frac{ (-1)^{k+1}(-1)^k }{ k }=-\sum_{k=1}^n\frac{1}{ k }.
    \end{equation}
    La limite \( n\to \infty\) diverge. Donc \( T\) n'est pas définie en \( x=-1\).

    Pour \( x=1\) par contre,
    \begin{equation}
        P_n(1)=\sum_{k=1}^n\frac{ (-1)^{k+1} }{ k }.
    \end{equation}
    Le critère des séries alternées\footnote{Théorème \ref{THOooOHANooHYfkII}.} nous donne la convergence de cette série.
\end{proof}

Nous savons maintenant que la série de Taylor \( T\) converge sur \( \mathopen] -1 , 1 \mathclose]\), et que \( T(0)=f(0)=\ln(1)=0\). Le premier gros morceau intéressant vient maintenant : nous allons prouver que \( T(x)\) converge vers ce que nous croyons, c'est-à-dire \( \ln(1+x)\) en personne.

\begin{proposition}     \label{PROPooKPBIooJdNsqX}
Pour tout \( x\in\mathopen] -1 , 1 \mathclose]\) nous avons
    \begin{equation}        \label{EqweEZnV}
        \ln(1+x)=\sum_{k=1}^{\infty}\frac{ (-1)^{k+1} }{ k }x^k
    \end{equation}
    De plus nous avons
    \begin{equation}    \label{EqKUQmOZ}
        \sum_{k=1}^{\infty}\frac{ (-1)^k }{ k }=\ln(2).
    \end{equation}
\end{proposition}

\begin{proof}
Il s'agit d'utiliser l'expression du reste fourni par le théorème \ref{THOooSIGRooJTLvlV}. Pour tout \( x\in \mathopen] -1 , \infty \mathclose[\), il existe un \( c\in\mathopen] 0 , x \mathclose[\) (le \( c\) dépend de \( x\)) tel que
    \begin{equation}
        P_n(x)-f(x)=\frac{ f^{(n+1)}(c) }{ (n+1)! }x^{n+1}.
    \end{equation}
    Cela est parce que \( f\) est de classe \(  C^{\infty}\). Calculons un peu : 
    \begin{subequations}
        \begin{align}
            P_n(x)-f(x)&=\frac{ f^{(n+1)}(c) }{ (n+1)! }x^{n+1}\\
            &=\frac{ (-1)^nn! }{ (1+c)^{n+1} }\frac{1}{ (n+1)! }x^{n+1}\\
            &=\frac{ (-1)^n }{ n+1 }\left( \frac{ x }{ 1+c } \right)^{n+1}.
        \end{align}
    \end{subequations}
Lorsque \( x>1\), il n'y a aucune garantie sur la convergence de cela pour \( n\to \infty\). Pour rappel, \( c\in\mathopen] 0 , x \mathclose[\). Si par contre \( x\in\mathopen] -1 , 1 \mathclose[\), alors nous savons que
    \begin{equation}
        \left| \frac{ x }{ 1+c } \right| <1,
    \end{equation}
    et donc convergence \( P_n(x)-f(x)\to 0\).

    Jusqu'ici nous avons prouvé que pour la série de Taylor converge vers \( \ln(1+x)\) pour \( x\in \mathopen] -1 , 1 \mathclose[\). Nous avons également vu que la série converge pour \( x=1\). Donc la fonction 
        \begin{equation}
            g(x)=\sum_{k=1}^{\infty}\frac{ (-1)^{k+1} }{ k }x^k
        \end{equation}
    est de continue sur \( \mathopen] -1 , 1 \mathclose]\) et égale à \( \ln(x+1)\) sur \( \mathopen] -1 , 1 \mathclose[\). Vu que \( f\colon x\mapsto \ln(x+1)\) est continue sur \( \mathopen] -1 , \infty \mathclose[\), nous avons également \( g(1)=f(1)=\ln(2)\).

    Ceci nous mène au dernier point de notre proposition : \( g(1)=\ln(2)\) s'écrit précisément
    \begin{equation}
        \sum_n\frac{ (-1)^n }{ n }=\ln(2).
    \end{equation}
\end{proof}

\begin{lemma}
    Soit la fonction\footnote{Pour la définition du logarithme, c'est la définition~\ref{DEFooELGOooGiZQjt}.}
    \begin{equation}
        f(x)=\frac{ \ln(1+x) }{ x }
    \end{equation}
    \begin{enumerate}
        \item
        Elle admet un prolongement de classe \(  C^{\infty}\) sur \( \mathopen] -1 , \infty \mathclose[\).
        \item
            \( f(0)=1\).
    \end{enumerate}
    La seconde condition étant évidemment avec un abus de notation entre \( f\) et son prolongement, parce que \( f\) n'est pas définie en zéro.
\end{lemma}

\begin{proof}
    La difficulté étant de voir que \( f\) a un prolongement en zéro et qu'elle y est de classe \(  C^{\infty}\).

    La \ref{PROPooKPBIooJdNsqX} nous donne l'égalité
    \begin{equation}
        \ln(1+x)=\sum_{k=1}^{\infty}\frac{ (-1)^{k+1} }{ k }x^k
    \end{equation}
    pour tout \( x\in \mathopen] -1 , 0 \mathclose]\); en particulier pour \( x=0\). Nous faisons le petit calcul suivant :
    \begin{subequations}        \label{SUBEQooRLQOooEzNFDp}
        \begin{align}
            \frac{1}{ x }\ln(1+x)&= \frac{1}{ x }\sum_{n=1}^{\infty}\frac{ (-1)^{n+1} }{ n }x^n\\
            &=\sum_{n=1}^{\infty}\frac{ (-1)^{n+1} }{ n }x^{n-1}\\
            &=\sum_{n=0}^{\infty}\frac{ (-1)^k }{ k+1 }x^k.
        \end{align}
    \end{subequations}
    Ce calcul n'est pas valable pour \( x=0\), mais ça ne nous empèche pas de poser
    \begin{equation}
        T(x)=\sum_{n=0}^{\infty}\frac{ (-1)^k }{ k+1 }x^k,
    \end{equation}
    qui, lui, est bien définie en zéro. Le rayon de convergence de la série \( T\) est égal à \( 1\), de telle sorte que
    \begin{equation}
        T\colon \mathopen] -1 , 1 \mathclose[\to \eR \\
    \end{equation}
    de classe \(  C^{\infty}\), et est égale à \( f\) sur \( \mathopen] -1 , 1 \mathclose[\setminus\{ 0 \}\).

    La série \( T\) est donc le prolongement demandé. En ce qui concerne \( f(0)\), c'est un abus pour écrire \( T(0)\) qui vaut immédiatement \( 1\).
\end{proof}

Notons qu'un calcul de limite
\begin{equation}
    \lim_{x\to 0} \frac{ \ln(1+x) }{ x }
\end{equation}
donnait la valeur \( f(0)=1\). Donc prolonger avec \( f(0)=1\) était la seule possibilité pour avoir une fonction continue. De là à dire que le prolongement ainsi créé est de classe \(  C^{\infty}\), c'est une autre histoire, qui est résoue par les séries entières.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Vitesses de $x^{\alpha}$, de l'exponentielle et du logarithme}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Un peu de théorie}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemSYHKooUiSMFJ}
    Pour tout \( \alpha>0\), il existe \( N\) tel que \( \ln(n)\leq n^{\alpha}\) pour tout \( n\geq N\).
\end{lemma}

\begin{proof}
En effet, nous avons
\begin{equation}
    \lim_{x\to\infty} \frac{ x^{\alpha} }{ \ln(x) }=\lim_{x\to\infty} \frac{ \alpha x^{\alpha-1} }{ 1/x }=\lim_{x\to\infty} \alpha x^{\alpha}=\infty
\end{equation}
quand $\alpha>0$.
\end{proof}
Cela tient également lorsque nous considérons $\ln(x)^p$ au lieu de $\ln(x)$. De cela, nous disons que le logarithme croit moins vite que n'importe quel polynôme.

\begin{example}
    Par exemple nous avons \( \ln(1-x)\sim -x\) pour \( x\to 0\) parce que
    \begin{equation}    \label{EqGICpOX}
        \lim_{x\to 0} -\frac{ \ln(1-x) }{ x }=\lim_{x\to 0} -\frac{ \frac{ -1 }{ 1-x } }{ 1 }=\lim_{x\to 0} \frac{1}{ 1-x }=1
    \end{equation}
    où nous avons utilisé la règle de l'Hospital (proposition~\ref{PROPooBZHTooHmyGsy}).
\end{example}

\begin{lemma}
    L'exponentielle croit plus vite que tout polynôme, et plus vite que que logarithme :
    \begin{equation}        \label{EqExpDecrtPlusVite}
        \lim_{t\to\infty} e^{-t}(\ln t)^{n}t^{\alpha}=0
    \end{equation}
    pour tout $n$ et pour tout $\alpha$.
\end{lemma}

% ne pas mettre de label à l'exemple suivant, parce que je ne suis pas certain que d'une façon ou d'une
% autre, il n'utilise pas le résultat lui-même.
\begin{example}     \label{EXooQNCJooFpnvnf}
   Le lemme \ref{LemLJOSooEiNtTs} a déjà prouvé la limite 
    \begin{equation}
        \lim_{n\to \infty} n^{\alpha}a^n
    \end{equation}
    pour tout \( \alpha>0\) et \( a<1\).

    L'utilisation de propriétés de l'exponentielle nous permet de donner une nouvelle preuve, plus courte\footnote{C'est toujours facile de prétendre qu'une preuve est plus courte qu'une autre lorsqu'on utilise en une ligne des très gros théorèmes qui ont mis dix pages à être démontrés.}.

    Le théorème \ref{ThoRWOZooYJOGgR} et la proposition \ref{PROPooLAOWooEYvXmI} nous permettent de passer à l'exponentielle. Pour chaque \( n\) nous avons :
    \begin{equation}        \label{EqLKLQooLIlWgm}
        n^{\alpha}a^n= e^{\alpha\ln(n)+n\ln(a)}.
    \end{equation}
    Ce qui est dans l'exponentielle est
    \begin{equation}
        \alpha\ln(n)+n\ln(a)=n\big(\alpha \frac{ \ln(n) }{ n }+\ln(a) \big).
    \end{equation}
    Dans la parenthèse, \( \ln(a)<0\) et \( \frac{ \ln(n) }{ n }\to 0\). Donc ce qui est dans l'exponentielle \eqref{EqLKLQooLIlWgm} tend vers \( -\infty\) et au final l'expression demandée tend vers zéro.
\end{example}

\begin{remark}
    Vous ne pouvez pas a priori considérer l'exemple \ref{EXooQNCJooFpnvnf} comme une preuve alternative au lemme \ref{LemLJOSooEiNtTs}, parce que vous n'êtes pas sûr que dans toute la théorie permettant de définir l'exponentielle (en particulier la convergence de \( \sum_kx^k/k!\)), le lemme n'est pas utilisé\quext{Faites la vérification et dites moi si c'est bon.}.
\end{remark}

\begin{proposition} \label{PropBQGBooHxNrrf}
    Pour tout polynôme \( P\) et pour tout \( a>0\) la fonction \( f(x)=P(x) e^{-ax}\) est intégrable\footnote{Définition~\ref{DefTCXooAstMYl}.} sur \( \mathopen[ 0 , \infty [\).
\end{proposition}

\begin{proof}
    Nous avons \( f(x)=P(x) e^{-ax/2} e^{-ax/2}\), et par la vitesse comparée des exponentielles et polynômes, pour un certain \( M>0\) nous pouvons affirmer que \( P(x) e^{-ax/2}<1\) sur \( \mathopen[ M , 0 [\). Dès lors
        \begin{equation}
            | f(x) |< e^{-ax/2},
        \end{equation}
        qui est intégrable.
\end{proof}

\begin{example}     \label{EXooAGEOooQdQkrS}
    La fonction logarithme (définition~\ref{DEFooELGOooGiZQjt}) n'est pas définie pour \( x\leq 0\). Par conséquent la fonction \( f(x)=x\ln(|x|)\) n'est pas définie en \( x=0\). Elle est bien définie pour \( x<0\) et vérifie
    \begin{equation}
        \lim_{x\to 0} x\ln(|x|)=0.
    \end{equation}
    Nous pouvons donc définir la fonction
    \begin{equation}
        \begin{aligned}
            \tilde f\colon \eR&\to \eR \\
            x&\mapsto \begin{cases}
                x\ln(| x |)    &   \text{si } x\neq 0\\
                0    &    \text{si } x=0.
            \end{cases}
        \end{aligned}
    \end{equation}
    Contrairement à la fonction initiale \( f\), cette fonction \( \tilde f\) est définie et continue en \( 0\).

    Notez que sur le graphe de la fonction \( \tilde f\), la courbe est bien régulière en \( x=0\).
    \begin{center}
       \input{auto/pictures_tex/Fig_XJMooCQTlNL.pstricks}
    \end{center}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Nombres premiers}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem} \label{ThonfVruT}
    Soit \( P\), l'ensemble des nombres premiers. Alors la somme \( \sum_{p\in P}\frac{1}{ p }\) diverge et plus précisément,
    \begin{equation}
        \sum_{\substack{p\leq x\\p\in P}}\frac{1}{ p }\geq \ln(\ln(x))-\ln(2).
    \end{equation}
\end{theorem}
\index{nombre!premier}
\index{convergence!rapidité}
\index{série!numérique}

\begin{proof}
    Nous posons
    \begin{equation}
        S_x=\{  q\leq x\text{ avec } q\text{ sans facteurs carrés} \}
    \end{equation}
    et
    \begin{equation}
        P_x=\{ p\in P\tq p\leq x \}.
    \end{equation}
    Si
    \begin{equation}
        K_x=\{  (q,m)\text{ tels que } q\text{ n'a pas de facteurs carrés et } qm^2\leq x \},
    \end{equation}
    alors nous avons
    \begin{equation}
        K_x=\bigcup_{q\in S_x}\bigcup_{m\leq \sqrt{x/q}}(q,m).
    \end{equation}
    Par définition et par le lemme~\ref{LemheKdsa} nous avons aussi
    \begin{equation}
        \{ n\leq x \}=\{ qm^2\tq (q,m)\in K_x \}.
    \end{equation}
    Tout cela pour décomposer la somme
    \begin{equation}        \label{EqpoJpuC}
        \sum_{n\leq x}\frac{1}{ n }=\sum_{q\in S_x}\sum_{m\leq\sqrt{x/q}}\frac{1}{ m^2 }\leq \sum_{q\in S_x}\frac{1}{ q }\underbrace{\sum_{m\geq 1}\frac{1}{ m^2 }}_{=C}.
    \end{equation}
    Nous avons aussi
    \begin{subequations}
        \begin{align}
            \prod_{p\in P_x}\left( 1+\frac{1}{ p } \right)&=1+\sum_{p\in P_x}\frac{1}{ p }+\sum_{\substack{p,q\in P_x\\p<q}}\frac{1}{ pq }+\sum_{\substack{p,q,r\in P_x\\p<q<r}}\frac{1}{ pqr }+\ldots\\
            &\geq 1+\sum_{p\in P_x}\frac{1}{ p }+\sum_{\substack{p,q\in P_x\\pq\leq x}}\frac{1}{ pq }+\sum_{\substack{p,q,r\in P_x\\pqr\leq x}}\frac{1}{ pqr }+\ldots
        \end{align}
    \end{subequations}
    Les sommes sont finies. Les sommes s'étendent sur toutes les façons de prendre des produits de nombres premiers distincts de telle sorte de conserver un produit plus petit que \( x\); c'est-à-dire que les sommes se résument en une somme sur les éléments de \( S_x\) :
    \begin{equation}        \label{EqooilOz}
        \exp\left( \sum_{p\in P_x}\frac{1}{ p } \right)\geq\prod_{p\in P_x}\left( 1+\frac{1}{ p } \right)\geq \sum_{q\in S_x}\frac{1}{ q }.
    \end{equation}
    La première inégalité est simplement le fait que \( 1+u\leq e^u\) si \( u\geq 0\) (directe de la définition~\ref{ThoRWOZooYJOGgR}). Les inégalités suivantes proviennent du fait que le logarithme est une primitive de la fonction inverse (proposition~\ref{ExZLMooMzYqfK}) :
    \begin{equation}
        \ln(x)\leq \sum_{n\geq x}\int_{n}^{n+1}\frac{dt}{ t }\leq \sum_{n\geq x}\frac{1}{ n }.
    \end{equation}
    Nous prolongeons ces inégalités avec les inégalités \eqref{EqpoJpuC} et \eqref{EqooilOz} :
    \begin{equation}
        \ln(x)\leq \sum_{n\geq x}\frac{1}{ n }\leq C\sum_{q\in S_x}\frac{1}{ q }\leq C\leq \exp\left( \sum_{p\in P_x}\frac{1}{ p } \right).
    \end{equation}
    En passant au logarithme,
    \begin{equation}
        \ln\big( \ln(x) \big)\leq\ln(C)+\sum_{p\in P_x}\frac{1}{ p }.
    \end{equation}
    Ceci montre la divergence de la série de droite. Nous cherchons maintenant une borne pour \( C\). Pour cela nous écrivons
    \begin{subequations}
        \begin{align}
            \sum_{n=1}^N\frac{1}{ n^2 }&\leq 1+\sum_{n=2}\frac{1}{ n(n-1) }\\
            &=1+\sum_{n=2}^N\left( \frac{1}{ n-1 }-\frac{1}{ n } \right)\\
            &=1+1-\frac{1}{ N }\\
            &\leq 2.
        \end{align}
    \end{subequations}
    Donc \( C\leq 2\).
\end{proof}
Ce théorème prend une nouvelle force en considérant le théorème de Müntz~\ref{ThoAEYDdHp} qui dit qu'alors l'ensemble \( \Span\{ x^p\tq  p\text{ est premier} \}\) est dense dans les fonctions continues sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme ou \( \| . \|_2\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques limites}
%---------------------------------------------------------------------------------------------------------------------------

Nous voyons à présent quelques calculs de limite et de développements mettant en scène des logarithmes et exponentielles.

\begin{example}\label{compose1}
    Pour trouver le développement de la fonction \( f(x)= e^{-2x}\), il suffit d'écrire celui de \( e^t\) et de remplacer ensuite $t$ par \( -2x\). Le développement à l'ordre \( 3\) de la fonction exponentielle est :
    \begin{equation}
        e^t=1+t+\frac{ t^2 }{2}+\frac{ t^3 }{ 6 }+t^3\alpha(t).
    \end{equation}
    Le développement de \( f(x)= e^{-2x}\) sera donc
    \begin{equation}
        f(x)=1-2x+\frac{ 4x^2 }{ 2 }-\frac{ 8x^3 }{ 6 }-8x^3\alpha(-2x).
    \end{equation}
    Donc le polynôme de degré \( 3\) partie régulière de \( g\) est :
    \begin{equation}
        1-2x+2x^2-\frac{ 4 }{ 3 }x^3,
    \end{equation}
    et la fonction reste correspondante est :
    \begin{equation}
        \alpha_g(x)=-8\alpha(-2x).
    \end{equation}
\end{example}

\begin{example}
    Nous savons les développements
    \begin{equation}
        f(x)=\ln(1+x)\sim x-\frac{ x^2 }{ 2 }+\frac{ x^3 }{ 3 }
    \end{equation}
    et
    \begin{equation}
        \sin(x)\sim x-\frac{ x^3 }{ 6 }.
    \end{equation}
    Nous obtenons le développement d'ordre \( 3\) de la fonction \( x\mapsto \ln\big( 1+\sin(x) \big)\) en écrivant
    \begin{equation}    \label{EqGXMooWKQkIL}
        \ln\big( 1+\sin(x) \big)\sim \big( x-\frac{ x^3 }{ 6 } \big)-\frac{ 1 }{2}\left( x-\frac{ x^3 }{ 6 } \right)^2+\frac{1}{ 3 }\left( x-\frac{ x^3 }{ 6 } \right)^3.
    \end{equation}
    Il s'agit maintenant de trouver les termes qui sont de degré inférieur ou égale à \( 3\).

    D'abord
    \begin{equation}
        \left( x-\frac{ x^3 }{ 6 } \right)^2=x^2-\frac{ x^4 }{ 3 }+\frac{ x^6 }{ 36 }\sim x^2
    \end{equation}
    Nous avons alors aussi
    \begin{equation}
        \left( x-\frac{ x^3 }{ 6 } \right)^6\sim x^2\left( x-\frac{ x^3 }{ 6 } \right)\sim x^3.
    \end{equation}
    En replaçant tout ça dans \eqref{EqGXMooWKQkIL} nous trouvons
    \begin{equation}
        \ln\big( 1+\sin(x) \big)\sim x-\frac{ x^2 }{2}+\frac{ x^3 }{ 6 }.
    \end{equation}
\end{example}

\begin{example}	\label{ExBCDookjljhjk}
    Calculer
    \begin{equation}\label{EqABCoolkjh}
        \lim_{x\to \infty}  e^{1/x}\sqrt{1+4x^2}-2x.
    \end{equation}
    Nous allons effectuer un développement asymptotique de la partie «difficile» de l'expression posant d'abord $x=1/h$. Si $f(x)=e^{1/x}\sqrt{1-4x^2}$ alors
    \begin{equation}
	g(h)=\frac{1}{|h|}e^h\sqrt{h^2+4}=\frac{1}{h}\big(  1+h+h\alpha(h) \big)\big( 2+h\beta(h) \big).
    \end{equation}
    La première parenthèse est le développement de $e^h$ et la seconde celui de $\sqrt{h^2+4}$. Nous nous apprêtons à faire la limite $x\to\infty$ qui correspond à $h\to 0^+$, nous pouvons donc supposer que $h>0$ et omettre la valeur absolue. En effectuant le produit et en regroupant tous les termes contenant $h^2$, $\alpha(h)$ ou $\beta(h)$ dans un seul terme $h\gamma(h)$,
    \begin{equation}
	f(h)=\frac{1}{h}\big(  2+2h+h\gamma(h) \big)=\frac{2}{h}+2+\gamma(h)=2x+2+\gamma(1/x)
    \end{equation}
    où $\gamma$ est une fonction vérifiant $\lim_{t\to 0}\gamma(t)=0$.

    Nous sommes maintenant en mesure de calculer la limite \eqref{EqABCoolkjh} :
    \begin{equation}
	\lim_{x\to\infty}e^{1/x}\sqrt{1+x^2}-2x= \lim_{x\to \infty}\big(  2x+2+\gamma(1/x)-2x \big)=2.
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Trigonométrie hyperbolique}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Les fonctions \defe{cosinus hyperbolique}{cosinus!hyperbolique} et \defe{sinus hyperbolique}{sinus!hyperbolique} sont les fonctions définies sur $\eR$ par les formules suivantes :
    \begin{subequations}
        \begin{align}
            \cosh(x)&=\frac{  e^{x}+ e^{-x} }{2}\\
            \sinh(x)&=\frac{  e^{x}- e^{-x} }{2}.
        \end{align}
    \end{subequations}
    Si vous ne vous rappelez plus la définition de \( e^x\), c'est \ref{DEFooSFDUooMNsgZY}.
\end{definition}

\begin{proposition}
    Quelque propriétés algébriques des fonctions trigonométriques hyperboliques.
    \begin{enumerate}
        \item
            \( \cosh(-x)=\cosh(x)\)
        \item
            \( \sinh(-x)=-\sinh(x)\)
        \item
            \( \cosh^2(x)-\sinh^2(x)=1\)
    \end{enumerate}
\end{proposition}

\begin{proof}
    Si s'agit simplement de remplacer les définitions.
\end{proof}

\begin{proposition}     \label{PROPooAOOHooXvLfrZ}
    Quelque propriétés analytiques des fonctions trigonométriques hyperboliques.
    \begin{enumerate}
        \item
            \( \cosh'(x)=\sinh(x)\)
        \item
            \( \sinh'(x)=\cosh(x)\).
        \item       \label{ITEMooZNZLooNMQFWr}
            \( \cosh(x)\geq 1\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Pour les dérivées, il s'agit d'utiliser la dérivation de l'exponentielle, laquelle est facile par le théorème \ref{ThoRWOZooYJOGgR}\ref{ITEMooEIKKooLNoaRD}.

    Pour \ref{ITEMooZNZLooNMQFWr}, nous commençons par les \( x\geq 0\). D'abord $\cosh(0)=1$. Ensuite \( \cosh'(x)=\sinh(x)=\frac{  e^{x}- e^{-x} }{ 2 }\). Vu que \( x> 0\) nous avons \(  e^{x}> e^{-x}>0\). Donc la dérivée de \( \cosh\) est strictement positive sur \( \mathopen] 0 , \infty \mathclose[\). La fonction y est donc partout plus grande que \( \cosh(0)=1\).

    Pour les \( x<0\), nous avons la fait que \( \cosh\) est paire.
\end{proof}

\begin{proposition}     \label{PROPooQLNYooIIOdvm}
    La fonction \( \sinh\colon \eR\to \eR\) est bijective.
\end{proposition}

\begin{proof}
    En deux parties.
    \begin{subproof}
        \item[Injective]
            Si \( \sinh(a)=\sinh(b)\), alors le théorème de Rolle \ref{ThoRolle} affirme qu'il existe \( c\in \mathopen] a , b \mathclose[\) tel que \( \sinh'(c)=0\). Mais la proposition \ref{PROPooAOOHooXvLfrZ} nous dit que \( \sinh'(x)=\cosh(c)\geq 1\). Donc impossible.

            \item[Surjective]
                Nous avons
                \begin{equation}
                    \lim_{x\to -\infty} \sinh(x)=-\infty
                \end{equation}
                et
                \begin{equation}
                    \lim_{x\to\infty } \sinh(x)=\infty.
                \end{equation}
                Soit \( y\in \eR\). Il existe \( m<0\) tel que \( \sinh(m)<y\) et \( M>0\) tel que \( \sinh(M)>y\). Le théorème des valeurs intermédiaires \ref{ThoValInter} nous enseigne qu'il existe \( x\in \mathopen[ m , M \mathclose]\) tel que \( \sinh(x)=y\).
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooWEHGooOBqSHY}
    Soient \( a,b\in \eR\) tels que \( a-b^2=1\). Il existe un unique \( (x,\sigma)\in \eR\times \{ \pm1 \}\) tel que
    \begin{subequations}        \label{SUBEQSooBIYDooIBuduV}
        \begin{numcases}{}
            \sinh(x)=b\\
            \sigma\cosh(x)=a.
        \end{numcases}
    \end{subequations}
\end{proposition}

\begin{proof}
    Vu que le sinus hyperbolique est une bijection\footnote{Proposition \ref{PROPooQLNYooIIOdvm}.}, il existe un unique \( x\in \eR\) tel que \( \sinh(x)=b\). Maintenant un petit calcul :
    \begin{equation}
        a^2=1+\sinh(x)^2=1+\frac{  e^{2x}+ e^{-2x}-2 }{ 4 }=\frac{  e^{2x}+ e^{-2x}+2 }{ 4 }=\cosh(x)^2.
    \end{equation}
    Vu que \( \cosh(x)^2=a^2\), il existe un unique \( \sigma\in\{ \pm1 \}\) tel que \( \sigma\cosh(x)=a\).
\end{proof}

Les représentations graphiques sont ceci :
\begin{center}
   \input{auto/pictures_tex/Fig_UNVooMsXxHa.pstricks}
\end{center}

La \defe{tangente hyperbolique}{tangente hyperbolique} est donnée par le quotient
\begin{equation}
    \tanh(x)=\frac{ \sinh(x) }{ \cosh(x) }.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Séries entières de matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Différentiabilité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropAMBXKgV}
    Soit \( (a_n)\) une suite dans \( \eC\) de rayon de convergence \( R\) et la fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eR)&\to \eM(n,\eR) \\
            A&\mapsto \sum_{k=0}^{\infty}a_kA^k
        \end{aligned}
    \end{equation}
    Alors
    \begin{enumerate}
        \item
            La différentielle de \( f\) sur \( B(0,R)\) est
            \begin{equation}    \label{EqRDVodDa}
                df_A(U)=\sum_{k=0}^{\infty}a_k\sum_{l=0}^{k-1}A^lUA^{k-1-l},
            \end{equation}
            c'est-à-dire que l'on peut différentier terme à terme. (Ici c'est \( A\) qui est dans \( B(0,R)\))
        \item
            La convergence de la somme~\ref{EqRDVodDa} est absolue.
        \item
            La convergence de la somme~\ref{EqRDVodDa} est normale sur tout compact.
        \item
            La fonction \( f\) est de classe \( C^1\) sur \( B(0,R)\), c'est-à-dire que la fonction \( A\mapsto df_A\) est continue.
    \end{enumerate}
\end{proposition}
Notons que \( df_A\) n'est pas tout à fait une série entière. Cependant, en ce qui concerne les normes, c'est tout comme si ça l'était.

\begin{proof}
    Nous posons \( u_k(A)=a_kA^k\), qui est une fonction de classe \(  C^{\infty}\) et dont la différentielle est donnée par
    \begin{equation}
        (du_k)_A(U)=\Dsdd{ u_k(A+tU) }{t}{0}=a_k\Dsdd{ (A+tU)^k }{t}{0};
    \end{equation}
    en distribuant le produit nous trouvons tout un tas de termes dont seuls ceux contenant exactement une fois \( tU\) ne vont pas s'annuler. Étant donné que \( U\) et \( A\) ne commutent pas nous avons l'expression un peu moche
    \begin{equation}
        (du_k)_A(U)=\sum_{l=0}^{k-1}a_kA^lUA^{k-1-l}.
    \end{equation}
    En ce qui concerne la norme, nous regardons celle de \( (du_k)_A\) pour un \( A\) fixé; c'est-à-dire que nous en regardons la norme opérateur :
    \begin{equation}
        \| (du_k)_A \|=\sup_{\| U \|=1}\| \sum_{l=0}^{k-1}a_kA^lUA^{k-1-l} \|\leq \sum_{l=0}^{k-1}| a_k |\| A \|^{l}\| A \|^{k-1-l}\leq k| a_k |\| A \|^{k-1}.
    \end{equation}
    Pour donner la convergence nous considérons un nombre \( r\) tel que \( \| A \|<r<R\), de telle sorte que la suite \( (a_nr^n)\) soit bornée par un nombre \( M\) et que nous puissions écrire
    \begin{equation}    \label{EqTGEwhnL}
        \| (du_k)_A \|\leq k| a_k |\| A \|^{k-1}=\frac{ k| a_k |\| A \|^k }{ \| A \| }=\frac{ k| a_k | }{ \| A \| }r^k\left( \frac{ \| A \| }{ r } \right)^k\leq \frac{ M }{ \| A \| }k\left( \frac{ \| A \| }{ r } \right)^k,
    \end{equation}
    dont la série converge. Nous avons donc convergence absolue de la série
    \begin{equation}
        \sum_{k=0}^{\infty}(du_k)_A.
    \end{equation}
    Passons à la convergence normale sur tout compact. Nous nous fixons \( r<R\) et nous nous intéressons à la norme de \( du_k\) sur \( \overline{ B(0,r) }\), c'est-à-dire
    \begin{equation}
        \| du_k \|_{\infty}=\sum_{x\in\overline{ B(0,r) }}\| (du_k)_A \|.
    \end{equation}
    Vu que \( \overline{ B(0,r) }\) est compact, ce supremum est un maximum et nous pouvons noter \( A_k\) la matrice qui le réalise. Nous réalisons alors les mêmes manipulations que pour \eqref{EqTGEwhnL} :
    \begin{equation}
        \| du_k \|_{\infty}=\| (du_k)_{A_k} \|\leq k| a_k |\| A_k \|^{k-1}\leq  k| a_k |r^{k-1}=\frac{1}{ r }k| a_k |r^k.
    \end{equation}
    Nous prenons maintenant \( r<r_0<R\) et \( M\), un majorant de \( (a_nr_0^n)\), de telle sorte qu'en multipliant et divisant par \( r_0^k\),
    \begin{equation}
        \| du_k \|_{\infty}\leq \frac{ k| a_k |r_0^k }{ r }\frac{ r^k }{ r_0^k }\leq \frac{ kM }{ r }\left( \frac{ r }{ r_0 } \right)^k,
    \end{equation}
    dont la série converge. Nous avons donc convergence normale sur tout compact. Par voie de \sout{fait} conséquences nous avons continuité de la série
    \begin{equation}
        \sum_{k=0}^{\infty}(du_k)_A
    \end{equation}
    et convergence vers \( df_A\) par le théorème~\ref{ThoLDpRmXQ}.
\end{proof}

\begin{proposition} \label{PropQIIURAh}
    Si le rayon de convergence de la série \( u(A)=\sum_{k=0}^{\infty}a_kA^k\) est \( R\), alors
    \begin{enumerate}
        \item
            elle converge normalement sur tout compact de \( B(0,R)\);
        \item
            la fonction \( u\) y est de classe \(  C^{\infty}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons
    \begin{equation}
        \begin{aligned}
            u_k\colon \eM(n,\eR)&\to \eM(n,\eR) \\
            A&\mapsto a_kA^k
        \end{aligned}
    \end{equation}
    qui est évidemment une fonction de classe \(  C^{\infty}\). Nous étudions la \( j\)\ieme\ différentielle en \( m\), pour \( k>j\) (dans une série, nous ne nous intéressons pas aux premiers termes). La \( j\)\ieme\ différentielle appliquée à \( v_1\) appliquée à \( v_2\), etc s'exprime de la façon suivante :
    \begin{equation}
        (d^ju_k)_m(v_1,\ldots, v_j)=\frac{ d  }{ d t_1 }\ldots\frac{ d  }{ d t_j }\Big( u_k(m+t_1v_1+\cdots +t_jv_j)    \Big)_{t_i=0}.
    \end{equation}
    Dans le produit \( (m+t_1v_1+\cdots +t_jv_j)^k\), seuls les termes contenant exactement une fois chacun des \( t_i\) ne s'annulera pas après avoir fait la dérivée et évalué en \( t_i=0\). Combien de termes cela fait ? Parmi les \( k\) facteurs, il faut en placer \( j\) qui ne sont pas \( m\) (cela fait \( \binom{ k }{ j }\) possibilités), et puis il faut ordonner ces \( j\) termes, cela fait encore \( j!\) possibilités. Au final,
    \begin{equation}
        \| (d^ju_k)_m \|\leq | a_k | \binom{ k }{ j }j!\| m \|^{k-j}=| a_k |P(k)\| m \|^{k-j}
    \end{equation}
    où \( P(k)=\frac{ k! }{ (k-j)! }\) est un polynôme de degré \( j\).

    Afin d'étudier la convergence normale sur tout compact de la série des \( d^ju_k\), nous considérons \( r<r_0<R\) et nous allons prouver la convergence normale sur \( \overline{ B(0,r) }\). Vu que c'est un compact, il existe une matrice \( m_k\in\overline{ B(0,r) }\) telle que
    \begin{subequations}
        \begin{align}
            \| d^ju_k \|_{\infty}&=\| (d^ju_k)_{m_k} \|\\
            &\leq | a_k |P(k)\| m_k \|^{k-j}\\
            &\leq | a_k |P(k)r^{k-j}\\
            &=\frac{ | a_k |P(k) }{ r^j }r^k\\
            &=\frac{ | a_k |r_0^kP(k) }{ r^j }\left( \frac{ r }{ r_0 } \right)^k\\
            &\leq \frac{ M }{ r^j }P(k)\left( \frac{ r }{ r_0 } \right)^k
        \end{align}
    \end{subequations}
    où \( M\) est un majorant de \( a_nr^n\). Vu que \( r_0/r<1\), la somme sur \( k\) converge et nous avons convergence normale sur tout compact de
    \begin{equation}
        d^j\sum_{k=0}^{\infty}a_kA^k=\sum_{k=0}^{\infty}d^j(a_kA^k)
    \end{equation}
    avec un peu d'abus de notation.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exponentielle de matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropKKdmnkD}
    Une matrice complexe est inversible si et seulement si elle est une exponentielle.

    Autrement dit :
    \begin{equation}
        \GL(n,\eC)= e^{\eM(n,\eC)}.
    \end{equation}
\end{proposition}
\index{exponentielle!de matrice}
\index{décomposition!Jordan!et exponentielle de matrice}

\begin{proof}
    Nous avons déjà prouvé dans la proposition \ref{PROPooRERRooMutKcg} que toutes les exponentielles étaient inversibles. Ici nous nous concentrons sur la réciproque.

    Soit \( A\in \GL(n,\eC)\); nous allons donner une matrice \( B\in \eM(n,\eC)\) telle que \( A=\exp(B)\). D'abord remarquons qu'il suffit de prouver le résultat pour une matrice par classe de similitude. En effet si \( A=\exp(B)\) et si \( M\) est inversible alors
    \begin{subequations}    \label{EqqACuGK}
        \begin{align}
            \exp(MBM^{-1})&=\sum_k\frac{1}{ k! }(MBM^{-1})^k\\
            &=\sum_k\frac{1}{ k! }MB^kM^{-1}\\
            &=M\exp(B)M^{-1}.
        \end{align}
    \end{subequations}
    Donc \( MAM^{-1}=\exp(MBM^{-1})\). Nous pouvons donc nous contenter de trouver un logarithme pour les blocs de Jordan. Nous supposons donc que \( A=(\mtu+N)\) avec \( N^m=0\).
    En nous inspirant de \eqref{EqweEZnV}, nous posons\footnote{Le logarithme d'un nombre n'est pas encore définit à ce moment, mais cela ne nous empêche pas de poser une définition ici pour une application des réels vers les matrices.}
    \begin{equation}
        D(t)=tN-\frac{ t^2 }{ 2 }N^2+\cdots +(-1)^m\frac{ t^{m-1} }{ m-1 }N^{m-1}
    \end{equation}
    et nous allons prouver que \(  e^{D(1)}=\mtu+N\). Notons que \( N\) étant nilpotente, cette somme ainsi que toutes celles qui viennent sont finies. Il n'y a donc pas de problèmes de convergences dans cette preuve (si ce n'est les passages des équations \eqref{EqqACuGK}).

    Nous posons \( S(t)= e^{D(t)}\) (la somme est finie), et nous avons
    \begin{equation}
        S'(t)=D'(t) e^{D(t)}
    \end{equation}
    Afin d'obtenir une expression qui donne \( S'\) en termes de \( S\), nous multiplions par \( (\mtu+tN)\) en remarquant que \( (\mtu+tN)D'(t)=N\) nous avons
    \begin{equation}
        (\mtu+tN)S'(t)=NS(t).
    \end{equation}
    En dérivant à nouveau,
    \begin{equation}    \label{EqKjccqP}
        (\mtu+tN)S''(t)=0.
    \end{equation}
    La matrice \( (\mtu+tN)\) est inversible parce que son noyau est réduit à \( \{ 0 \}\). En effet si \( (\mtu+tN)x=0\), alors \( Nx=-\frac{1}{ t }x\), ce qui est impossible parce que \( N\) est nilpotente. Ce que dit l'équation \eqref{EqKjccqP} est alors que \( S''(t)=0\). Si nous développons \( S(t)\) en puissances de \( t\) nous nous arrêtons au terme d'ordre \( 1\) et nous avons
    \begin{equation}
        S(t)=S(0)+tS'(0)=\mtu+tD'(0)=1+tN.
    \end{equation}
    En \( t=1\) nous trouvons \( S(1)=\mtu+N\). La matrice \( D(1)\) donnée est donc bien un logarithme de $\mtu+N$.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisabilité d'exponentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]      \label{PropCOMNooIErskN}
    Si \( A\in \eM(n,\eR)\) a un polynôme caractéristique scindé, alors \( A\) est diagonalisable si et seulement si \( e^A\) est diagonalisable.
\end{proposition}
\index{décomposition!Dunford!application}
\index{exponentielle!de matrice}
\index{diagonalisable!exponentielle}

\begin{proof}
    Si \( A\) est diagonalisable, alors il existe une matrice inversible \( M\) telle que \( D=M^{-1}AM\) soit diagonale (c'est la définition~\ref{DefCNJqsmo}). Dans ce cas nous avons aussi \( (M^{-1}AM)^k=M^{-1}A^kM\) et donc \( M^{-1}e^AM=e^{M^{-1}AM}=e^D\) qui est diagonale.

    La partie difficile est donc le contraire.

    \begin{subproof}
        \item[Qui est diagonalisable et comment ?]
            Nous supposons que \( e^A\) est diagonalisable et nous écrivons la décomposition de Dunford (théorème~\ref{ThoRURcpW}) :
            \begin{equation}
                A=S+N
            \end{equation}
            où \( S\) est diagonalisable, \( N\) est nilpotente, \( [S,N]=0\). Nous avons besoin de prouver que \( N=0\).

            Les matrices \( A\) est \( S\) commutent; en passant au développement nous en déduisons que \( A\) et \( e^S\) commutent, puis encore en passant au développement que \( e^A\) et \( e^S\) commutent. Vu que \( S\) est diagonalisable, \( e^S\) l'est et par hypothèse \( e^A\) est également diagonalisable. Donc \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables par la proposition~\ref{PropGqhAMei}.

            Étant donné que \( A\) et \( S\) commutent, nous avons \( e^N=e^{A-S}=e^Ae^{-S}\), et nous en déduisons que \( e^N\) est diagonalisable vu que les deux facteurs \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables.

        \item[Unipotence]

            Si \( r\) est le degré de nilpotence de \( N\), nous avons
            \begin{equation}    \label{EqQHjvLZQ}
                e^N-\mtu=N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! }.
            \end{equation}
            Donc
            \begin{equation}
                (e^N-\mtu)^k=\left( N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! } \right)^k
            \end{equation}
            où le membre de droite est un polynôme en \( N\) dont le terme de plus bas degré est de degré \( k\). Donc \( (e^N-\mtu)\) est nilpotente et \( e^N\) est unipotente.

            Si \( M\) est la matrice qui diagonalise \( e^N\), alors la matrice diagonale \( M^{-1}e^NM\) est tout autant unipotente que \( e^N\) elle-même. En effet,
            \begin{subequations}
                \begin{align}
                    (M^{-1}e^NM-\mtu)^r&=\sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}M^{-1}(e^N)^kM\\
                    &=M^{-1}\left( \sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}(e^N)^k \right)M\\
                    &=M^{-1}(e^N-\mtu)^rM\\
                    &=0.
                \end{align}
            \end{subequations}

            La matrice \( M^{-1}e^NM\) est donc une matrice diagonale et unipotente; donc \( M^{-1}e^NM=\mtu\), ce qui donne immédiatement que \( e^N=\mtu\).

        \item[Polynômes annulateurs]

            En reprenant le développement \eqref{EqQHjvLZQ} sachant que \( e^N=\mtu\), nous savons que
            \begin{equation}
                N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! }=0.
            \end{equation}
            Dit en termes pompeux (mais non moins porteurs de sens), le polynôme
            \begin{equation}
                Q(X)=X+\frac{ X^2 }{2}+\cdots +\frac{ X^{r-1} }{ (r-1)! }
            \end{equation}
            est un polynôme annulateur de \( N\).

            La proposition~\ref{PropAnnncEcCxj} stipule que le polynôme minimal d'un endomorphisme divise tous les polynômes annulateurs. Dans notre cas, \( X^r\) est un polynôme annulateur et donc le polynôme minimal de \( N\) est de la forme \( X^k\). Donc il est \( X^r\) lui-même.

            Nous avons donc \( X^r\divides Q\). Mais \( Q\) est un polynôme contenant le monôme \( X\) donc \( X^r\) ne peut diviser \( Q\) que si \( r=1\). Nous en concluons que \( X\) est un polynôme annulateur de \( N\). C'est-à-dire que \( N=0\).

        \item[Conclusion]

            Vu que Dunford\footnote{Théorème~\ref{ThoRURcpW}.} dit que \( A=S+N\) et que nous venons de prouver que \( N=0\), nous concluons que \( A=S\) avec \( S\) diagonalisable.

    \end{subproof}
\end{proof}

