% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{The group \texorpdfstring{$SL(2,\eR)$}{SL2R} and its algebra}	\label{SecToolSL}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\subsection{Iwasawa decomposition}
%----------------------------------
\index{Iwasawa!decomposition!of $SL(2,\eR)$}

Let $G=\SL(2,\eR)$ the group of $2\times 2$ matrices with unit determinant. The Lie algebra $\lG=\gsl(2,\eR)$ is the algebra of matrices with vanishing trace:
\begin{equation}
 \lG =  \{ X\in\End(\eR^2)\tq \tr(X) = 0\} 
=\left\{ \begin{pmatrix}
x & y \\
z & -x
\end{pmatrix}\textrm{ with }x,y,z\in\eR  \right\}. 
\end{equation}
The following elements will be intensively used:\label{PgBaseSLdeuxRHEF}
\[
H=\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
,\quad
  E=\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
,\quad
 F=\begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix},
\quad
T=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}
\]
where $T=E-F$ has been introduced for later convenience. The commutators are
\begin{subequations}\label{EqTableSLdR}
\begin{align}  
  [H,E]&=2E	&[T,H]&=-2T  \\
  [H,F]&=-2F	&[T,E]&=H   \\
  [E,F]&=H	&[T,F]&=H.
\end{align}
\end{subequations}
The exponentials can be easily computed and the result is
\begin{align}				\label{EqExpMatrsSLdeuxR}
 e^{tH}=
\begin{pmatrix}
   e^{t}	&	0	\\ 
  0	&	 e^{-t}	
\end{pmatrix},
&&
 e^{tE}=
\begin{pmatrix}
  1	&	t	\\ 
  0	&	1	
\end{pmatrix},
&&
 e^{tF}=
\begin{pmatrix}
  1	&	0	\\ 
  t	&	1	
\end{pmatrix}.
\end{align}
Notice that the sets $\{ H,E,F \}$, $\{ H,E,F \}$ and $\{ H,E+F,T \}$ are basis. A Cartan involution is given by $\theta(X)=-X^t$, and the corresponding Cartan decomposition is
\begin{align}
   \lK&=\Span\{ T \},
&\lP&=\Span\{ H,E+F \}.
\end{align}
\ifthenelse{\value{siTHZ}=1}{}{Indeed, we are in a matrix algebra, then $\tr(XY)$ is proportional to $\tr(\ad X\circ \ad Y)$.
 In order to see that $\theta$ is a Cartan involution, we have to prove that $B|_{\lK\times\lK}$ is negative definite and $B|_{\lP\times\lP}$ positive. It is true because for $X\in\lK$,
\[
    \tr(\ad X\circ \ad X)=\tr(XX)=\tr\begin{pmatrix}
-x^2 & 0 \\
0 & -x^2
\end{pmatrix}<0,
\]
and for $Y\in\lP$,
\[
    \tr(YY)=\tr\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix}\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix} =\tr\begin{pmatrix}
x^2+y^2 & 0 \\
0 & x^2+y^2
\end{pmatrix} >0.
\]
}			% Fin du siTHZ sur la vérification que $\theta$ est de Cartan

Up to some choices, the Iwasawa decomposition\label{pg_iwasldr} of the group $\SL(2,\eR)$ is given by the exponentiation of $\lA$, $\lN$ and~$\lK$
\begin{equation}
\begin{aligned}
  \lA&=\Span\{ H \}
&\lN&=\Span\{ E \}
&\lK&=\Span\{T\},
\end{aligned}
\end{equation}
so that
\begin{equation}\label{eq:expo_ANK}
A=\begin{pmatrix}
e^a & 0 \\
0 & e^{-a}
\end{pmatrix}\quad
N=\begin{pmatrix}
1 & l \\
0 & 1
\end{pmatrix}\quad
K=\begin{pmatrix}
\cos k & \sin k \\
-\sin k & \cos k
\end{pmatrix}.
\end{equation}

A common parametrization of $AN$ by $\eR^2$ is provided by
\begin{equation}   \label{EqParmalSL} 
(a,l)=
\begin{pmatrix}
  e^a&le^a\\
  0  &e^{-a}
\end{pmatrix}.
\end{equation}
One immediately has the following formula for the left action of $AN$ on itself:
\[
  L_{(a,l)}(a',l')=\begin{pmatrix}
e^{a+a'} & e^{a+a'}l'+e^{a-a'}l \\
0 & e^{-a-a'}
\end{pmatrix}=(a+a',l'+e^{-2a'}l).
\]
In this setting, the inverse is given by $(a,l)^{-1}=(-a,-l e^{2a})$.  
\ifthenelse{\value{siTHZ}=1}{}{

Let's give some formulas in $\SL(2,\eR)$. Using corollary \ref{Ad_e} and exponentiating commutation relations,
\begin{subequations}  \label{eq_eaHsldr}
\begin{align}
\Ad(e^{aH})E&=e^{2a}E,\\
\Ad(e^{aH})F&=e^{-2a}F,\\
\Ad( e^{aH})T&= e^{2a}E- e^{-2u}E+ e^{-2u}T\\
\Ad(e^{tE})H&=H-2tE,   							\label{eq_AdetE}\\
\Ad( e^{tE})T&=-tH+(t^2+1)E-E+T\\
\Ad( e^{tT})H&=\cos(2t)H+\sin(2t)(2E-T)\\
\Ad( e^{tT})E&=\frac{ 1 }{2}\Big( \sin(2t)H+\cos(2t)(2E-T)+T \Big)
\end{align}
\end{subequations}
where $z$ belongs to the center: $z=\pm\mtu$.} 


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{A companion : \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

We can consider the Iwasawa decomposition which is $\theta$-conjugated to the $AN$ that we just saw. That decomposition is generated by
\begin{equation}
	\bar\lN=\begin{pmatrix}
	0	&	0	\\ 
	1	&	0
\end{pmatrix}.
\end{equation}
The exponentiation produces
\begin{equation}
	\bar N=\begin{pmatrix}
	1	&	0	\\ 
	t	&	1	
\end{pmatrix},
\end{equation}
and the Iwasawa group is given by
\begin{equation}		\label{EqGeneANbarSLdeuxR}
	A\bar N=\begin{pmatrix}
	e^a	&	0	\\ 
	l e^{-a}	&	 e^{-a}	
\end{pmatrix}.
\end{equation}

\subsection{Killing form}
%------------------------

\ifthenelse{\value{siTHZ}=1}{The Killing form $B(X,Y)=\tr(\ad X\circ\ad Y)$ takes the following values:}{
In the basis $\{ H,E,T \}$, the adjoint operators are given by
\[
\ad H=\begin{pmatrix}
 0 & 0 &-2 \\
 0 & 0 &0 \\ 
 0 & 2 &2
\end{pmatrix},
\ad E=\begin{pmatrix}
 0 & 0 &0 \\
 0 & 0 &-1\\ 
-2 & 0&0
\end{pmatrix},\textrm{ and }
  \ad T=\begin{pmatrix}
 2 & 0 &0 \\
0 & 1 &0\\
-2 & 0&0
\end{pmatrix}.
\]
so that the Killing form can be computed directly from definition $B(X,Y)=\tr(\ad X,\ad Y)$. The result is}
\begin{subequations}
\begin{align}
B(T,H)&=0  & B(H,H)&=8\\
B(T,E)&=-4 & B(E,E)&=0\\
B(H,E)&=0  &  B(T,T)&=-4.
\end{align}
\end{subequations}
Expressed in the basis $\{H,E,F\}$, the matrix of the Killing form reads
\begin{equation}
B=
\begin{pmatrix}
8&&\\
&&4\\
&4&
\end{pmatrix}
\end{equation} 
while, in the basis  $\{H,E+F,T\}$, we find
\begin{equation}   \label{EqBHEFTsldR}
B=
\begin{pmatrix}
8\\
&8\\
&&-8
\end{pmatrix}.
\end{equation}
The latter is the reason of the name of the vector $T$: the sign of its norm is different, so that $T$ is candidate to be a time-like direction.

\subsection{Abstract root space setting}
%---------------------------------------

Looking on the table \eqref{EqTableSLdR} from an abstract point of view, we see that $E$ and $F$ are eigenvectors of $\ad(H)$ with eigenvalues $2$ and $-2$. So $\lA=\lG_0=\eR H$; $\lG_2=\eR E$; and $\lG_{-2}=\eR F$. Using a more abstract notation, the table of $\SL(2,\eR)$ becomes
\begin{subequations}  \label{subeq_rootSLR}
\begin{align}
  [A_{0},A_{2}]&=2A_{2}\\
	[A_{0},A_{-2}]&=-2A_{-2}\\
	[A_{2},A_{-2}]&=A_{0}.
\end{align}
\end{subequations}

\subsection{Isomorphism}
%-----------------------

As pointed out in the chapter II, \S6 of \cite{Knapp_reprez}, the map (seen as a conjugation in $\SL(2,\eC)$)
\begin{equation}
	\begin{aligned}
		\psi\colon \SU(1,1)&\to \SL(2,\eR) \\ 
		U&\mapsto AUA^{-1} 
	\end{aligned}
\end{equation}
with $A=\begin{pmatrix}
1&i\\i&1
\end{pmatrix}$ is an isomorphism between $\SL(2,\eR)$ and $\SU(1,1)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{Young tableau and diagrams}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We follow \cite{ModavePoincarre}. Let $n\in \eN$. A \defe{Young diagram}{Young!diagram} is a diagram of $n$ boxes arranged in rows of decreasing number of boxes. Here is an example with $n=8$:
%\begin{equation}
%	\input{image_Young_exemple.pstricks}
%\end{equation}

\begin{center}
   \input{Fig_TGdUoZR.pstricks}
\end{center}


A \defe{Young tableau}{Young!tableau} is a Young diagram in each box of which we placed a number between $1$ and $n$ such that
\begin{itemize}
	\item in each line, the numbers are increasing from left to right,
	\item in each line, the numbers are increasing from up to down.
\end{itemize}
It is \defe{standard}{Young!tableau!standard} if each of the integers $\{ 1,\ldots,n \}$ appears one and only one time. Here is an example:
%\begin{equation}
%	\input{image_Young_exempleStandard.pstricks}
%\end{equation}
\begin{center}
   \input{Fig_GBnUivi.pstricks}
\end{center}
Let $S_n$ be the symmetric group of all the permutations of $n$ objects. Let us see how does a Young tableau define symmetry properties of tensors. If $\lambda$ is a Young tableau with $n$ boxes, its applies on the tensor $T_{\mu_1,\ldots,\mu_n}$ by symmetrization over the set of indices in each line, and antisymmetrization over the sets of indices in each column. Now, the group $S_n$ can act on the so-obtained tensor in order to produce $n!$ new tensors, which are not independent.

\begin{lemma}
The space spanned by
\begin{equation}
	\{ g \tilde T\tq g\in S_n \}
\end{equation}
where $\tilde T$ is $\lambda T$ is an irreducible representation of $S_n$. Moreover, two representations coming from two different tableau with the same diagram are equivalent.
\end{lemma}
\begin{proof}
	No proof.
\end{proof}
As an example, consider the following tableau
%\begin{equation}
%	\input{image_Young_carre.pstricks}
%\end{equation}
\begin{center}
   \input{Fig_IYAvSvI.pstricks}
\end{center}
The corresponding symmetrization of the tensor $T_{abcd}$ is as follows. First, we symmetrize over $ac$ (i.e the row $1,3$):
\begin{equation}
\frac{ 1 }{2}(	T_{abcd}+T_{cbad}),
\end{equation}
and then we symmetrize over $bd$:
\begin{equation}
	\frac{1}{ 4 }(T_{abcd}+T_{cbad}+T_{adcb}+T_{cdab}).
\end{equation}
Then we have to take the ansisymmetrization over $ab$ ($8$ terms), and we finish by antisymmetrization over $cd$ to get the $16$ terms result.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Representations of \texorpdfstring{$\GL(V)$}{GL(V)} }
%---------------------------------------------------------------------------------------------------------------------------

If $v\in \eR^D$, then the action of all the non-singular $D\times D$ matrices on $v$ creates a representation space for $GL(D)$. In the same way, the $2$-indices tensors $T_{ab}$ form a $D^2$-dimensional representation of $GL(V)$ by acting separately on each index:
\begin{equation}		\label{EqYoungAgitIndices}
	(g\cdot T)_{ab}=g^{ac}g^{bd}T_{cd}.
\end{equation}
That representation is, however, reducible because a symmetric tensor remains symmetric; indeed
\begin{equation}
	(g\cdot T)_{ba}=g^{bc}g^{ad}T_{cd}=g^{bc}g^{ad}T_{dc}=g^{bd}g^{ac}T_{cd}=(g\cdot T)_{ab}.
\end{equation}
The same is true for the antisymmetric tensors. Thus,
\begin{equation}
	V\otimes V\simeq (V\odot V)\oplus(V\wedge V).
\end{equation}
More generally, when $GL(D)$ acts on $V^{\otimes n}$, the irreducible representations are the different combinations of (anti)symmetrizations, namely: the irreducible representations of $S_n$.



Let $\lambda$ be a Young tableau with $n$ boxes. The \defe{Schur module}{schur!module} $V_{\lambda}^{GL(D)}$\nomenclature{$V_{\lambda}^{GL(D)}$}{The Schur module, representation of $GL(D)$} is the vector space of tensor $\tilde T$ of rank $n$ such that
\begin{itemize}
	\item $\tilde T$ is completely antisymmetric for all the labels of each column of $\lambda$,
	\item the antisymmetrization of all the labels of one column of $\lambda$ with another label on the right vanishes.
\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Representations of \texorpdfstring{$O(p,q)$}{Opq}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{ModavePoincarre}]
If the sum of the length of the two first column of a Young diagram $\lambda$ is strictly bigger than $D$, then the irreducible representation of $O(D)$ given by $\lambda$ is identically vanishing.
\end{theorem}

Young diagrams such that the sum of the lengths of the first two columns does not exceed $D$ are said to be \defe{allowed}{allowed!Young diagram}.
\index{Young!diagram!allowed}

\begin{theorem}		\label{ThoOpqrepreTens}
Every non vanishing finite dimensional irreducible representation of $O(p,q)$ is isomorphic to a completely traceless tensor representation whose symmetry properties are given by an allowed Young diagram.
\end{theorem}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{The group \texorpdfstring{$\SO(3)$}{SO3} and its Lie algebra}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SubSecTheGroupSotrois}

We follow \cite{WormerAngular} in which more proofs can be found.

\begin{proposition}
An element of $\SO(3)$ has exactly one eigenvector with eigenvalue $1$. That vector is the \defe{rotation axis}{axis!of rotation in $\SO(3)$}.
\end{proposition}

The generator of rotation around the axis $n$ (unit vector) is given by the matrix
\begin{equation}
\begin{pmatrix}
  0	&	-n_3	&	n_2\\ 
  n_3	&	0	&	-n_1\\ 
 -n_2	&	n_1	& 0	  
\end{pmatrix}.
\end{equation}
That form results form the requirement that $Nr=n\times r$. If we denote by $R(n,\theta)$ the operator of rotation in $\eR^3$ by an angle $\theta$ around the axis $n$, one shows that
\begin{equation}
	R(b,\theta)=\mtu+\sin(\theta) N+\big(1-\cos(\theta)\big)N^2.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Rotations of functions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Consider any function $f\colon \eR^3\to \eC$; we define the \defe{rotation operator}{rotation!on functions} $U(n,\theta)$\nomenclature{$U(n,\theta)$}{Rotation operator on functions} by
\begin{equation}		\label{EqRotFunSOtrois}
	\big( U(n\theta)f \big)(r)=f\big( R(n,\theta)^{-1}r \big).
\end{equation}
These operators form a group, and we have in particular that
\[ 
	U(n,\theta_1)U(n,\theta_2)=U(n,\theta_1+\theta_2).
\]
We are interested in \emph{infinitesimal} rotations, that is rotations of angle $d\theta$ for which $(d\theta)^2\ll d\theta$, or in other words, we are interested in a development of equation \eqref{EqRotFunSOtrois} restricted to linear terms in $\theta$. What one obtains is
\begin{equation}
	\big( U(n,d\theta)f \big)(r)=\big( (1-i d\theta\, n\cdot l)f\big)(r)
\end{equation}
where the operator $l$ is defined by
\begin{equation}
	l=-ir\times\nabla.
\end{equation}
Its components $l_i=-i\epsilon_{ijk}r_j\partial_k $ satisfy commutation relations
\begin{equation}	\label{EqAldllepsl}
	[l_i,l_j]=i\epsilon_{ijk}l_k.
\end{equation}
The operator $n\cdot l$ is refereed as the \defe{generator of infinitesimal rotations}{generator!of infinitesimal rotations}. One can derive an expression of $U(n,\theta)$ in terms of $n\cdot l$ by the following:
\[ 
	U(n,\theta+d\theta)f=U(n,\theta)U(n,d\theta)f=U(n,\theta)(1-id\theta\, n\cdot l)f,
\]
so that we have the differential equation
\begin{equation}
	\frac{ dU }{ d\theta }(n,\theta)=-iU(n,\theta)n\cdot l
\end{equation}
with the initial condition $U(n,0)=1$. The solution is
\begin{equation}
	U(n,\theta)= e^{-i\theta\, n\cdot l}.
\end{equation}

\subsection{Representations of \texorpdfstring{$\SO(3)$}{SO3}}\label{pg:reprez_SOt}
%//////////////////////////////////////////////////////////////////////

The group $\SO(3)$ is strongly linked with $SU(2)$ by the following property:
\begin{equation}		\label{EqSOSUeZ}
   \SO(3)=\frac{SU(2)}{\eZ_2}.
\end{equation}

\begin{lemma}
A representation $\rho_j$ of $SU(2)$ is a representation of $\SO(3)$ if and only if $\rho_j(X)=\id$ for any $X$ in the kernel of the homomorphism $SU(2)\to \SO(3)$, namely: $\rho_j(\pm\mtu)=\id$.
\label{lem:SO_3}
\end{lemma}

\begin{proof}
We consider $\dpt{\rho_j}{SU(2)}{\End{V_j}}$ and $\dpt{\psi}{SU(2)}{\SO(3)}$. The latter fulfils $\psi(\mtu)=\psi(-\mtu)=\mtu$, which is an important equation because it ensures us that the rest of the expressions are well defined with respect to the class representative.

If $\rho_j(-\mtu)=\mtu$, we define $\dpt{d_j}{\SO(3)}{\End{V}}$ by $d_j([x])=\rho_j(x)$ (check that this is well defined). With this, 
\[
  d_j([x])d_j([y])=\rho_j(x)\rho_j(y)=\rho_j(xy)=d_j([xy]).
\]

Now let us suppose that $d_j([x])=\rho_j(x)$ is a representation. Thus 
\[
  \rho_j(x)=d_j([x])=d_j([-x])=\rho_j(-x)=\rho_j(-\mtu)\rho_j(x),
\]
so $\rho_j(-\mtu)=\id_{V_j}$.

\end{proof}

Moreover, any representation of $\SO(3)$ comes from a representation $\tilde\rho$ of $SU(2)$ by setting $\tilde\rho(-\mtu)=\id$ and $\tilde\rho(x)=\rho([x])$.

Now, we research the representations of $SU(2)$ for which the matrix $-\mtu$ is represented by the identity operator. These will be representations of $\SO(3)$. The spin $j$ representations of $SU(2)$ is given by
\[
   \rho_j(X)\phi_{pq}(\xi)=\phi_{pq}(X^{-1}\xi).
\]
With $X=-\mtu$, this gives: $\phi_{pq}(-\xi)=(-1)^{p+q}\phi_{pq}(\xi)$. If we want it to be equal to $\phi_{pq}(\xi)$, we need $p+q=2j$ even. This is true if and only if $j\in\eN$.

\label{pg:reprez_SO3}The conclusion is that the irreducible representations of $\SO(3)$ are the integer spin irreducible representations of $\SU(2)$. Note that the non relativistic mechanics has $\SO(3)$ as group of space symmetry. Thus there are no hope to find any half integer spin in a non relativistic theory.


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Representations of the algebra \texorpdfstring{$\gsu(2)=\so(3)$}{su2so3}}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Determination of the representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

In the case of $\so(3)$, the Cartan subalgebra is one dimensional, and one has only one simple root: $\alpha=J_{12}^*$. If $\Lambda=aJ_{12}^*$, one has $(\Lambda,\alpha)=a$, and theorem \ref{Thoirrepllamifffmor} says that $\Lambda$ is highest weight of an irreducible representation if and only if $a\in \eN/2$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Ladder operators}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We are now going to determine the irreducible representations in a more explicit way. From the relation \eqref{EqSOSUeZ}, we know that the study of $\gsu(2)$ and $\so(3)$ are the same. The algebra $\gsu(2)$ is the real algebra generated by the matrices of the form
$
\begin{pmatrix}
\alpha	&\beta\\
-\beta^*&-\alpha
\end{pmatrix}
$ with $\alpha$, $\beta\in\eC$. A convenient basis is given by
\begin{align}		\label{EqGenssudeux}
u_1&=\frac{ 1 }{2}
\begin{pmatrix}
  i	&	0	\\
  0	&	-i	
\end{pmatrix},
&u_2&=
\frac{ 1 }{2}
\begin{pmatrix}
  0	&	1	\\ 
  -1	&	0	
\end{pmatrix},
&u_3&=\frac{ 1 }{2}
\begin{pmatrix}
  0	&	i	\\ 
  i	&	0
\end{pmatrix}.
\end{align}
That algebra satisfies the commutation relations
\begin{equation}
	[u_i,u_j]=\epsilon_{ijk}u_k.
\end{equation}
The trick to build finite dimensional representations\index{representation!of $\gsu(2)$} of that algebra is common (see \cite{MQSenechal} for example). The first step is to perform a change of basis $J_k=iu_k$ that brings the algebra under the form (see section \ref{SubSecTheGroupSotrois} to understand why)
\begin{equation}		\label{EqAlgsuiepsijk}
	[J_i,J_j]=i\epsilon_{ijk}J_k.
\end{equation}
We are going to construct all the finite dimensional irreducible representations of the algebra \eqref{EqAlgsuiepsijk}. The key point of that new basis is that one can define the \defe{ladder operators}{ladder operators}
\begin{equation}
	J_{\pm}=J_1\pm iJ_2
\end{equation}
that have the property that 
\begin{equation}
	[J_3,J_{\pm}]=\pm J_{\pm}.
\end{equation}
Notice that for every $i$, we have $(J_i)^*=J_i$, so that $(L^{\pm})^*=L^{\mp}$. An other important property is that, defining $J^2=J_1^2+J_2^2+J_3^2$, we have
\begin{equation}
	[J_i,J^2]=0,
\end{equation}
which show that $J^2$ is a Casimir operator, and is thus by Schur's lemma a multiple of identity. Notice that we are using an abuse of notation between $J_i$ as element of $\gsu(2)$ and $J_i$ as the operator that represent $J_i$. In the first case, products like $J_iJ_j$ make no sense\footnote{In fact, one has to understand these products as elements of the universal enveloping algebra. What we are building is a reprensentation of that algebra, which, obviously, restricts to a representation of the algebra. When we use the Schur's lemma, in fact we invoke it in $\mU\big(\so(3)\big)$}, but it makes sense as operator composition.

The subalgebra $\{ J^2,J_3 \}$ being abelian, we can simultaneously diagonalise $J^2$ and $J_3$. Let $| m,\sigma \rangle $ be an orthonormal basis of the eigenspace of $J_3$ associated with the eigenvalue $m$. The index $\sigma$ is for a possible degenerateness to be studied later.  We have
\[ 
	J_3| m,\sigma \rangle =m| m,\sigma \rangle .
\]
Using the commutation relations between $J_3$ and the ladder operators, we have
\begin{equation}		\label{EqJtroisJpmmplusun}
	J_3J_{\pm}| m,\sigma \rangle =\big( \pm J_{\pm}+J_{\pm}J_3 \big)| m,\sigma \rangle =(m\pm 1)J_{\pm}| m,\sigma \rangle .
\end{equation}
Thus $J_{\pm}| m,\sigma \rangle $ is an eigenvector of $J_3$ with the eigenvalue $m\pm 1$, which means that $J_{\pm}| m,\sigma \rangle $ is a linear combination of the vectors $| m\pm 1,\sigma \rangle $ with different values of $\sigma$. This is the reason of the name of the \emph{ladder} operators: they raise and lower the eigenvalue of $J_3$.

We can now prove that one has to drop the index $\sigma$ because eigenvalues of $J_3$ cannot be degenerated. For, compute
\begin{equation}		\label{JpJmJcarrerelation}
	J_+J_-=(J_1+iJ_2)(J_1-iJ_2)=J^2-J_3^2+i[J_2,J_1]=J^2-J_3^2+J_3,
\end{equation}
so that
\[ 
	J_+J_-| m,\sigma \rangle =(\alpha-m^2+m)| m,\sigma \rangle 
\]
where $\alpha$ is defined by $J^2=\alpha\mtu$. That proves that the space generated by $| m,\sigma \rangle $ and the action of $J_3$, $J_+$ and $J_-$ is invariant under the representation, while one cannot obtain $\ket{m,\sigma'}$ by action of $J_{\pm}$ on $\ket{m,\sigma}$. Since we are looking for \emph{irreducible} representations, that space must actually be all the representation space. That rules out the possibility to have two different vectors $| m,\sigma_1 \rangle $ and $| m,\sigma_2 \rangle $.

The explicit matrix form of $J_{\pm}$ are:
\begin{align}
J_{+}&=
\begin{pmatrix}
0	&	0	&	0	&0	&\hdots\\
1	&	0	&	0	&0	&\hdots\\
0	&	1	&	0	&0	&\hdots\\
0	&	0	&	1	&0	&\hdots\\
\vdots	&	\vdots	&	\vdots	&\vdots	&\ddots
\end{pmatrix},
&J_{-}&=
\begin{pmatrix}
0	&	1	&	0	&0	&\hdots\\
0	&	0	&	1	&0	&\hdots\\
0	&	0	&	0	&0	&\hdots\\
0	&	0	&	0	&1	&\hdots\\
\vdots	&	\vdots	&	\vdots	&\vdots	&\ddots
\end{pmatrix},
\end{align}
Since we are searching for finite dimensional representations, there exists a maximal eigenvalue of $J_3$. Let us denote by $j$ that maximal eigenvalue and by $| j \rangle$ the corresponding eigenvector. The relation \eqref{EqJtroisJpmmplusun} shows that if $J_+| j \rangle\neq 0$, then $J_+| j \rangle$ is an eigenvector for $J_3$ with eigenvalue $j+1$, which contradicts maximality. Then we have $J_+| j \rangle=0$.

Since we know the action of $J_3$ and $J_+$ on $| j \rangle$, it is convenient to write $J^2$ in terms of these two operators. This is done in the same way as probing equation \eqref{JpJmJcarrerelation}:
\begin{equation}
	J^2=J_3^2+J_3+J_-J_+,
\end{equation}
so that
\begin{equation}		\label{EqJcarrejjplusun}
	J^2| j \rangle=j(j+1)| j \rangle.
\end{equation}
We know that $J^2=\alpha\mtu$ and that $\alpha$ is a characteristic of the representation. What equation \eqref{EqJcarrejjplusun} tells us is that the maximal eigenvalue of $J_3$ is related to $\alpha$ by $j(j+1)=\alpha$.

We are now able to determine the proportionality constant of relation $J_{\pm}| m \rangle\propto| m\pm 1 \rangle$. Since $(J_-)^*=J_+$, we have
\begin{equation}	\label{EqnormeJmoinm}
	\| J_-| m \rangle \|^2=\langle m| J_+J_- | m \rangle = j(j+1)-m^2+m.
\end{equation}
Then one has
\begin{subequations}	
	\begin{align}
		J_-| m \rangle	&=\sqrt{j(j+1)-m(m-1)}| m-1 \rangle,	\label{EqJmoinsmanglemmointun}		\\
		J_+| m \rangle	&=\sqrt{j(j+1)-m(m+1)}| m+1 \rangle.
	\end{align}
\end{subequations}
As expected, $J_-| -j \rangle=0$ and $J_+| j \rangle=0$. Notice that we avoid the possibility $J_-| m \rangle=-\sqrt{\cdots}| m-1 \rangle$ by a simple redefinition $| m-1 \rangle\to -| m-1 \rangle$.

Equation \eqref{EqnormeJmoinm} shows that the norm of $| m \rangle$ becomes negative for $m<-j$ and $m>j+1$. We conclude that the minimal eigenvalue of $J_3$ is $-j$. Since $| j \rangle$ has to be reached from $| -j \rangle$ by action of $J_+$, the difference $j-(-j)$ must be an integer. Thus $j\in\eN/2$. The number $j$ is the \defe{spin}{spin!of representation of $\so(3)$} of the representation.

Let us give the explicit example with spin one half.
When $j=\frac{ 1 }{2}$, the vector space is generated by the vectors $| 1/2 \rangle$ and $| -1/2 \rangle$, and the operators are given by
\begin{align}
	J_3&=\frac{ 1 }{2}
\begin{pmatrix}
  1	&	0	\\ 
  0	&	-1	
\end{pmatrix},
&J_-&=
\begin{pmatrix}
  0	&	0	\\ 
  1	&	0	
\end{pmatrix},
&J_+&=
\begin{pmatrix}
  0	&	1	\\ 
  0	&	0	
\end{pmatrix},
\end{align}
from which we deduce
\begin{align*}
J_1&=\frac{ 1 }{2}
\begin{pmatrix}
  0	&	1	\\ 
  1	&	0	
\end{pmatrix},
&J_2&=
\begin{pmatrix}
  0	&	-i	\\ 
  i	&	0	
\end{pmatrix}.
\end{align*}
Notice that we have $J_i=\frac{ 1 }{2}\sigma_i$ with the \defe{Pauli matrices}{pauli matrices},
\begin{align}
\sigma_1&=
\begin{pmatrix}
  0	&	1	\\ 
  1	&	0	
\end{pmatrix},
&\sigma_2&=
\begin{pmatrix}
  0	&	-i	\\ 
  i	&	0	
\end{pmatrix},
&\sigma_3&=
\begin{pmatrix}
  1	&	0	\\ 
  0	&	-1	
\end{pmatrix}.
\end{align}
These matrices fulfil the relation 
\begin{equation}
	\sigma_i\sigma_j=\delta_{ij}+i\epsilon_{ijk}\sigma_k.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsubsection{Weight vectors}	\label{subSubSecweightsotrois}
%---------------------------------------------------------------------------------------------------------------------------

The algebra $\so(3)$ does not contain abelian subalgebra of dimension bigger than one, so a Cartan subalgebra is generated by $J_3$. The unique (up to dilatation) element of $\hH^*$ is thus given by $\alpha(J_3)=1$. The relation $[J_z,J_{\pm}]$ provides the root spaces:
\begin{equation}
	\begin{aligned}
		\so(3)_1	&=\{ J_+ \}\\
		\so(3)_{-1}	&=\{ J_- \},
	\end{aligned}
\end{equation}
thus $\lN^{\pm}$ is generated by $J_{\pm}$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Two words about \texorpdfstring{$\gsu(3)$}{su3}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Using the Cartan matrix of $\gsu(3)$ and formula \eqref{EqCoefDynkMalpha}, we will determine the Dynkin coefficients of the representation 
%\input{image_su3Dynkin(1-).pstricks}
\input{Fig_RQsQKTl.pstricks} without even explicitly compute the weights. For that, we follow the construction of \cite{rncahn}. The Cartan matrix is
\begin{equation}
	A=
	\begin{pmatrix}
		2	&	-1	\\ 
		-1	&	2	
	\end{pmatrix}.
\end{equation}
The Dynkin coefficients of the highest weight is given by
\begin{equation}
	\Lambda_i=
	\begin{pmatrix}
		1	\\ 
		0	
	\end{pmatrix}.
\end{equation}
Since $\Lambda$ is highest weight, we have $q(\Lambda,\alpha_i)=1$, so that $\Lambda_1+q(\Lambda,\alpha_1)=1$ and $\Lambda_2+q(\Lambda,\alpha_2)=0$. Thus the only weight of the first layer is $M=\Lambda-\alpha_1$.
Using formula \eqref{EqCoefDynkMalpha}, we find
\begin{equation}
	(\Lambda-\alpha_1)_i=\Lambda_i-A_{1i}=
	\begin{pmatrix}
		1	\\ 
		0	
	\end{pmatrix}-
	\begin{pmatrix}
		2	\\ 
		-1	
	\end{pmatrix}=
	\begin{pmatrix}
		-1	\\ 
		1	
	\end{pmatrix}.
\end{equation}
We also have, by construction, $p(M,\alpha_1)=1$ and $p(M,\alpha_2)=0$, so that $M_1+p(M,\alpha_1)=-1+1=0$ and $M_2+p(M,\alpha_2)=1$. We conclude that $M-\alpha_2$ is a weight, and its Dynkin coefficients are given by
\begin{equation}
	(M-\alpha_2)_i=
	\begin{pmatrix}
		-1	\\ 
		1	
	\end{pmatrix}-
	\begin{pmatrix}
		-1	\\ 
		2	
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\ 
		-1	
	\end{pmatrix}.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Representations of \texorpdfstring{$\so(n)$}{son}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ladder operators}
%---------------------------------------------------------------------------------------------------------------------------

We follow \cite{Dolan_son}, and we restrict ourself to the case where $n$ is even. We will use two abuse of language. The first one is to denote by the same symbol the matrices $J_{ij}$ of $\so(n)$ and the endomorphisms of $V$ that represent them (and that we are searching for). The second one is to improperly speak about $J_{ij}^2$ as elements of $\so(3)$ when they are in fact in the universal covering. If we denote by $(ij)$ the rotation in the plane generated by the directions $i$ and $j$, the algebra has the block structure (here for $\so(8)$)
\begin{equation}
	\begin{matrix}
		\begin{matrix}
			(12)\\\phantom{(12)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(13)&(14)\\
			(23)&(24)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(15)&(16)\\
			(25)&(26)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(17)&(38)\\
			(27)&(28)
		\end{matrix}	
		}
		\\
		\begin{matrix}
			\phantom{(12)}\\\phantom{(33)}
		\end{matrix}
		&
		\begin{matrix}
			\phantom{(12)}&(34)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(35)&(36)\\
			(45)&(46)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(37)&(38)\\
			(47)&(48)
		\end{matrix}	
		}
		\\
		&&
		\begin{matrix}
			\phantom{(12)}&(56)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(57)&(58)\\
			(67)&(68)
		\end{matrix}	
		}
		\\
		&&&
		\begin{matrix}
			\phantom{(12)}&(78)
		\end{matrix}
	\end{matrix}
\end{equation}
The elements $A_p=J_{2p-1,2p}$ are simultaneously diagonalisable (because they form an abelian subalgebra). We denote by $| m_1,m_2,\ldots \rangle$ the eigenvector for the corresponding eigenvalue
\begin{equation}
	A_p| \overline{ m } \rangle=m_p| \overline{ m } \rangle.
\end{equation}
The remaining four operators in each block can be arranged in the following way (here in the block between $A_1$ and $A_2$):
\begin{subequations}		\label{subEqLppmmettouteca}
	\begin{align}
		L_{12}^{++}&=-J_{13}+iJ_{14}+iJ_{23}+J_{24}\\
		L_{12}^{+-}&=J_{13}+iJ_{14}-iJ_{23}+J_{24}\\
		L_{12}^{-+}&=J_{13}-iJ_{14}+iJ_{23}+J_{24}\\
		L_{12}^{--}&=-J_{13}-iJ_{14}-iJ_{23}+J_{24}.
	\end{align}
\end{subequations}
Using the fact that the $J_{ij}$ are Hermitian, we see that $(L_{pq}^{\pm\pm})^*=L_{pq}^{\mp\mp}$. These operators are build in such a way to have
\begin{subequations}		\label{SubEqCommsLppmmettoutca}
	\begin{align}
		[A_p,L_{pq}^{\pm\,.}]&=\pm L_{pq}^{\pm\,.}\\
		[A_q,L_{pq}^{.\,\pm}]&=\pm L_{pq}^{.\,\pm}
	\end{align}
\end{subequations}
These relations make that 
\begin{equation}
	L_{pq}^{\pm\pm}| \overline{ m } \rangle\,\propto\, | \ldots,m_p\pm 1,\ldots,m_q\pm 1,\ldots \rangle.
\end{equation}
To find the proportionality coefficient is one part of our work now. We denote by $j_p$ the maximal eigenvalue of $A_p$. Using the matricial expression $J_{ij}=i(E_{ij}-E_{ji})$ and the multiplication formula \eqref{EqFormMulEmtr}, we find
\begin{equation}			\label{EqJsqnmunmtu}
	J^2=\sum_{ij}(J_{ij})^2=(n-1)\mtu
\end{equation}
which is thus a Casimir operator that is represented as a multiple of identity\footnote{Take care that the expression \eqref{EqJsqnmunmtu} is an expression for $J^2$ in its definition representation. This is not the expression of $J^2$ acting on $| \overline{ m } \rangle$. Indeed we make a systematic abuse between elements $J_{ij}$ in $\so(n)$ and the operators which represent them on the vector space $V$ generated by the ket $| \overline{ m } \rangle$.}.

Since we are searching for finite dimensional representations, each of $m_p$ have minimal and maximal value. To find these values is the second part of our work now. We denote by $j_p$ the maximal eigenvalue of $A_p$ and by 

Using the commutation relations and the fact that products like $J_{13}J_{24}$ vanish, a simple computation yields
\begin{equation}	\label{EqLppLmmundeux}
	L_{12}^{++}L_{12}^{--}=\sum_{(i,j)\in b_{12}} J_{ij}^2+2(A_1+A_2)
\end{equation}
where $b_{12}$ denotes the block associated with $A_1$ and $A_2$, namely $\sum_{(i,j)\in b_{12}}=J_{13}^2+J_{23}^2+J_{14}^2+J_{24}^2$. In the case of even $n$, we have $n/2$ elements $A_p$, each of them generating $(\frac{ n }{ 2 }-1)$ blocks, so that formula \eqref{EqLppLmmundeux} can be summed up on all the blocks to have
\begin{equation}
	\sum_{b}L_b^{--}L_b^{++}=J^2+(n-2)\sum_pA_p-\sum_pA_p^2.
\end{equation}
On the other hand, notice that in the definition representation, for each block the matrix $\sum_{(i,j)\in b}J_{ij}^2$ is diagonal, so that its belongs to the center of $\so(n)$. From, its representative is a multiple of identity and we can in fact simultaneously diagonalise the operators
\begin{equation}
	\{ A_p,J_b^2,J^2 \}.
\end{equation}
If we take $J_a^2$ in the equation \eqref{EqLppLmmundeux}, and apply that to $| \overline{ \jmath } \rangle$ we obtain
\begin{equation}
	J_a^2| \overline{ \jmath } \rangle	=\Big( J^2-\sum_{b\setminus a}J_b^2-\sum_pA_p \Big)| \overline{ \jmath } \rangle
	=\Big( (n-2)\sum_pj_p-2\sum_pj_p^2-\sum_{b\setminus a}j_b^2 \Big)| \overline{ \jmath } \rangle,
\end{equation}
so that we have the following constrain over the $j_b^2$:
\begin{equation}		\label{EqCondCompajsqsumjdeux}
	\sum_bj_j^2=(n-2)\sum_pj_p-2\sum_pj_p^2.
\end{equation}
Once these $j_b^2$ are fixed, the representation is fixed because
\begin{equation}
	\| L_a^{++}| \overline{ m } \rangle \|^2=\langle\overline{ m }| L_a^{--}L_a^{++} | \overline{ m } \rangle=\langle \overline{ m }| \big( J_a^2+2(A_{a_1}+A_{a_2}) \big)=j_a^2+2(m_{a_1}+m_{a_2}),
\end{equation}
Thus we have
\begin{equation}			\label{EqLppketmsqrtjjjldots}
	L_a^{++}| \overline{ m } \rangle=\sqrt{  j_a^2+2(m_{a_1}+m_{a_2})  }| \ldots,m_{a_1}+1,\ldots,m_{a_2}+1,\ldots \rangle.
\end{equation}
We are now going to prove that one can choice the values of $j_a^2$.

\begin{proposition}
	Different choices of $j_a^2$ give rise to equivalent representations.
\end{proposition}

\begin{probleme}
	Je ne sais pas où est la faute dans la preuve qui suit, mais cette proposition me semble fausse. Doit y avoir un moyen de fixer ces $j_a^2$.
\end{probleme}

\begin{proof}
	Let $\pi_1$ and $\pi_2$ be two representations of $\so(n)$ that differ only by the value of $J_a$ (for every $a$). We have
	\begin{subequations}
		\begin{align}
			\pi_1(L_a^{++})| \overline{ m } \rangle&=k_a^{++}| \overline{ n } \rangle\\
			\pi_2(L_a^{++})| \overline{ m } \rangle&=l_a^{++}| \overline{ n } \rangle.
		\end{align}
	\end{subequations}
	Notice that the numbers $k$ and $l$ are not arbitrary: they have to be of the form of the square root of equation \eqref{EqLppketmsqrtjjjldots} for some $j_a^2$ that fulfil compatibility condition \eqref{EqCondCompajsqsumjdeux}. We are searching for an operator $A\colon V\to V$ such that 
	\begin{subequations}
		\begin{align}
			\pi_1(L_a^{++})A| \overline{ m } \rangle&= A\pi_2(L_a^{++})| \overline{ m } \rangle\\
			\pi_1(J_p)A| \overline{ m } \rangle&= A\pi_2(J_p)| \overline{ m } \rangle
		\end{align}
	\end{subequations}
	where $\pi_1(J_p)=\pi_2(J_p)$. The numbers $k_a^{\pm\pm}$ and $l_a^{\pm\pm}$ are given by the representations $\pi_i$ and are non zero. We write $A$ under the form
	\begin{equation}
		A| \overline{ m } \rangle=x(\overline{ m })|\overline{ m } \rangle,
	\end{equation}
	so that the second condition is automatically satisfied. We have
	\begin{equation}
		\pi_1(L_a^{++})A| \overline{ m } \rangle=x(\overline{ m })\pi_1(L_a^{++})| \overline{ m } \rangle=x(\overline{ m })k_a^{++}(\overline{ m })| \overline{ m }_a^{++} \rangle
	\end{equation}
	where $\overline{ m }_a^{++}=(\ldots, m_{a_1}+1,\ldots,m_{a_2}+1,\ldots)$. The constrain is
	\begin{equation}
		A\pi_2(L_a^{++})\overline{ m }=Al_a^{++}(\overline{ m })| \overline{ m }_a^{++} \rangle=l_a^{++}x(\overline{ m }_a^{++})| \overline{ m }_a^{++} \rangle.
	\end{equation}
	One can choose one of the $x(\overline{ m })$ and then construct the other with the law
	\begin{equation}
		x(\overline{ m }_a^{\pm\pm})=\frac{ k_a^{\pm\pm}(\overline{ m }) }{ l_a^{\pm\pm}(\overline{ m }) }x(\overline{ m }).
	\end{equation}
\end{proof}
One choice of $j_a^2$ that respect the condition \eqref{EqCondCompajsqsumjdeux} is
\begin{equation}
	j_a^2=\frac{ n-2 }{2}(j_{a_1}+j_{a_2})-(j_{a_1}^2+j_{a_2}^2).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Root spaces and Dynkin diagram for \texorpdfstring{$\so(2l)$}{so(21)}}
%---------------------------------------------------------------------------------------------------------------------------

The Cartan algebra of $\so(2l)$ is the abelian algebra generated by $\{ A_1,\ldots,A_l \}$ where $A_p=J_{2p-1,2p}$. The roots are of the form
\begin{equation}
	\pm A_p^*\pm A^*_q
\end{equation}
with $p\neq q$. Among of them, the positive roots are
\begin{equation}
	\alpha_p=A^*_p\pm A^*_q
\end{equation}
with $p<q$. The positive root $A_p+A_{p+k}$ is not simple, indeed
\begin{equation}
	\begin{aligned}[]
		A^*_p+A^*_{p+k}	&= (A^*_p-A^*_{p+1})+(A^*_{p+1}-A^*_{p+2})+\ldots+(A^*_{p+k-1}-A^*_{p+k})\\
		&\quad +2(A^*_{p+k}-A^*_{k-1})+\ldots+2(A^*_{l-2}-A^*_{l-1})\\
		&\quad +(A^*_{l-1}-A^*_l)+(A^*_{l-1}+A^*_l).
	\end{aligned}
\end{equation}
The simple roots of $\so(2l)$ are thus $A^*_p-A^*_{p+1}$ and $A^*_{l-1}+A^*_l$ with $p=1,\ldots,l-1$.

All the length are maximal (and equal to $1/\sqrt{2}$) and all the angles between two ``consecutive'' simple roots are the same, but the last one:
\begin{equation}
	(A^*_{l-1}-A^*_{l},A^*_{l-1}+A^*_{l})=0,
\end{equation}
thus the Dynkin diagram of $\so(2l)$ is given by
\begin{center}
   \input{Fig_FGWjJBX.pstricks}
\end{center}

%\begin{center}
%	\begin{pspicture}(-0.5,-0.5)(1.5,0.5)
%		%\psframe[linecolor=cyan](-0.5,-0.5)(1.5,0.5)
%		\psset{PointName=none,PointSymbol=none}
%		\pstGeonode(0,0){A}(1,0){B}(2,0){C}(3,0){D}(4,0.5){E}(4,-0.5){F}
%		\psline(A)(B)
%		\psline[linestyle=dotted](B)(C)
%		\psline(C)(D)
%		\psline(D)(E)
%		\psline(D)(F)
%		\pstMarquePoint[PointSymbol=o]{A}{0.3;-90}{$\alpha_1$}
%		\pstMarquePoint[PointSymbol=o]{B}{0.3;-90}{$\alpha_2$}
%		\pstMarquePoint[PointSymbol=o]{D}{0.4;-135}{$\alpha_{l-2}$}
%		\pstMarquePoint[PointSymbol=o]{E}{0.5;0}{$\alpha_{l-1}$}
%		\pstMarquePoint[PointSymbol=o]{F}{0.4;0}{$\alpha_{l}$}
%	\end{pspicture}
%\end{center}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Orthogonal algebra in the odd dimensional case}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{An odd dimensional example: \texorpdfstring{$\so(5)$}{so5}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecsocinq}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Root spaces}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Elements of $\so(5)$ are arranged in the following picture:
\begin{equation}
	\begin{matrix}
		\begin{matrix}
			(12)\\\phantom{(12)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(13)&(14)\\
			(23)&(24)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(15)\\
			(25)
		\end{matrix}	
		}
		\\
		\begin{matrix}
			\phantom{(12)}\\\phantom{(33)}
		\end{matrix}
		&
		\begin{matrix}
			\phantom{(12)}&(34)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(35)\\
			(45)
		\end{matrix}	
		}
	\end{matrix}
\end{equation}
Because of the fact that $J_{12}$ and $J_{34}$ commute and because of commutators \eqref{SubEqCommsLppmmettoutca}, the subalgebra
\begin{equation}
	\lH=\{ J_{12},J_{34} \}
\end{equation}
is a Cartan subalgebra of $\so(5)$.

Using the commutators \eqref{EqJJietaJcomm}, and the convention%
\footnote{One can take that convention because $J_{\alpha\beta}=-J_{\beta\alpha}$. One other possible convention would has been $X^{\alpha\beta}=-X^{\beta\alpha}$. In that case, the result is
\begin{equation}
	[J_{12},X^{\alpha\beta}J_{\alpha\beta}]=2i(X^{\beta 1}J_{2\beta}-X^{\beta 2}J_{1\beta}).
\end{equation}
}		% end of footnote.
that $X^{\alpha\beta}=0$ when $\beta\leq a$,  we find
\[ 
	[J_{12},X^{\alpha\beta}J_{\alpha\beta}]=i(X^{2\beta}J_{1\beta}-X^{1\beta}J_{2\beta}).
\]
Thus, if we are searching for vectors $X$ such that $[J_{12},X]=\lambda X$, we are lead to the equation
\begin{equation}
	i(X^{2\beta}J_{1\beta}-X^{1\beta}J_{2\beta})=\lambda X^{ab}J_{ab},
\end{equation}
and therms proportional to $J_{ab}$ with $a$ and $b$ different to $1$ and $2$ have to vanish. There only two possible values for $\lambda$, apart of zero,  are $\pm 1$. What we find is that $[A_1,X]=X$ implies that $X=\sum_{b=3,4}(J_{2b}+iJ_{1b})$. One finds similar results for $A_2$. One also finds that $[A_1,X]=0$ implies that $X^{2\beta}=X^{1\beta}=0$.

Putting all that together, one finds that
\begin{subequations}		\label{EqRootSOImpaircinq}
	\begin{align}
		\so(5)_{(\pm 1,0)}	&=\{ J_{25}\pm iJ_{15} \}\\
		\so(5)_{(0,\pm 1)}	&=\{ J_{45}\pm iJ_{35} \}\\
		\so(5)_{(1,\pm 1)}	&=\{ \mp J_{13}+iJ_{14}\pm iJ_{23}+J_{24} \}\\
		\so(5)_{(-1,\pm 1)}	&=\{ \pm J_{13}-iJ_{14}\pm iJ_{23}+J_{24} \}.
	\end{align}
\end{subequations}
The positivity of the root $(a,b)$ is defined by the positivity of $a$, or the one of $b$ when $a$ is vanishing. The positive roots of $\so(5)$ are $(1,0)$, $(1,\pm 1)$ and $(0,1)$:
\begin{equation}
	\begin{aligned}
		\alpha_1&=(1,0),	&\alpha_2&=(0,1),\\
		\alpha_3&=(1,1),	&\alpha_4&=(1,-1),
	\end{aligned}
\end{equation}
among which $\alpha_2$ and $\alpha_4$ are simple. Indeed, the root $(0,1)$ is obviously simple. The root $(1,1)$ is not simple, as sum of $(1,0)$ and $(0,1)$; the root $(1,0)$ is the sum of $(1,1)$ and $(1,-1)$ and is thus not simple. We are left with the simple roots $(1,-1)$ and $(0,1)$. So we have
\begin{subequations}
	\begin{align}
		\alpha_2	&=A_2^*,\\
		\alpha_4	&=A_1^*-A_2^*
	\end{align}
\end{subequations}
and the dual basis is
\begin{subequations}
	\begin{align}
		h_2	&=A_1+A_2.\\
		h_4	&=A_1
	\end{align}
\end{subequations}

One shows easily compute the inner product \eqref{EqDefInnprHestrar}:
\begin{equation}		\label{EqPridScalacbdroot}
	\big( (a,b),(c,d) \big)	= \frac{1}{ 4 }(ac+bd).
\end{equation}
The length of $(a,b)$ is thus $\frac{1}{ 4 }(a^2+b^2)$. The angle between $\alpha_1$ and $\alpha_2$ is given by
\begin{equation}
	\cos(\theta_{\alpha_1,\alpha_2})=\frac{ (\alpha_1,\alpha_2) }{ \sqrt{(\alpha_1,\alpha_2)(\alpha_4,\alpha_4)} }=\frac{ \frac{1}{ 4 }(-1) }{ \sqrt{ \frac{1}{ 4 }\frac{1}{ 4 }2  } }=-\frac{1}{  \sqrt{2} }.
\end{equation}
Thus we have $\theta_{\alpha_1,\alpha_2}=135\degree$, and the Dynkin diagram of $\so(5)$ is
%\begin{center}
%	\begin{pspicture}(-0.5,-0.5)(1.5,0.5)
%		%\psframe[linecolor=cyan](-0.5,-0.5)(1.5,0.5)
%		\psset{PointName=none,PointSymbol=none}
%		\pstGeonode(0,0){A}(1,0){B}
%		\psline[doubleline=true](A)(B)
%		\pstMarquePoint[PointSymbol=*]{A}{0.3;-90}{$\alpha_2$}
%		\pstMarquePoint[PointSymbol=o]{B}{0.3;-90}{$\alpha_4$}
%	\end{pspicture}
%\end{center}
\begin{center}
   \input{Fig_QPcdHwP.pstricks}
\end{center}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Irreducible representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We saw around equation \eqref{EqWnmoinvlambldarootmodul} that an irreducible representation of $\so(5)$ has to look like $f_1^{i_1}\cdots f_m^{i_m}v\lambda$. In our case, we consider $\lambda$, the highest weight and we denote by $| \lambda \rangle$ the cyclic vector which has the following properties
\begin{subequations}
	\begin{align}
		L^{+}_{1(5)}| \lambda \rangle=L^{+}_{2(5)}| \lambda \rangle&=L^{1\pm}_{12}| \lambda \rangle=0\\
		A_p| \lambda \rangle&=\lambda(A_p)| \lambda \rangle.
	\end{align}
\end{subequations}
Proposition \ref{PropoIrrrgenffflamble} says that a general element of the carrying space $W$ of an irreducible representation of $\so(5)$ is given by
\begin{equation}
	| i_1i_2i_3i_4 \rangle = \big( L^-_{1(5)} \big)^{i_1} \big( L_{2(5)}^- \big)^{i_2}\big( L^{--}_{12} \big)^{i_3}\big( L^{-+}_{12} \big)^{i_4}	| \lambda \rangle
\end{equation}
In order to know to which root space it belongs, we apply $A_p$ on that\footnote{In fact, equation \eqref{Eqfmlaphamoinsmouns} answers the question. The computation that we give here is more pedestrian, but still is independent of any explicit matricial choices, and is easily generalisable to others $\so(n)$ algebras.}. Using the relation \eqref{Eqhjfikplusun} and the fact that $\alpha_{1(5)}(A_1)=1$, we find
\begin{equation}
	A_1\big( L^-_{1(5)} \big)^{i_1}=-i_1\big( L^-_{1(5)} \big)^{i_1}+\big( L^-_{1(5)} \big)^{i_1}A_1.
\end{equation}
Since $\alpha_{2(5)}(A_1)=0$, we also have $A_1\big( L_{2(5)}^- \big)^{i_2}=\big( L_{2(5)}^- \big)A_1$. One found the other ones in the same way. The result is
\begin{equation}
	A_1| i_1i_2i_3i_4 \rangle=\big( \lambda(A_1)-i_1-i_3-i_4 \big)| i_1i_2i_3i_4 \rangle,
\end{equation}
which has to be seen as a straight generalisation of \eqref{EqJtroisJpmmplusun}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{List of irreducible representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let us determine the possible irreducible representations of $\so(5)$ using theorem \ref{Thoirrepllamifffmor}. The simple roots are $\alpha_2=(0,1)$ and $\alpha_4=(1,-1)$. A simple computation with equation \eqref{EqPridScalacbdroot} and $\Lambda=(a,b)$ gives
\begin{equation}
	\begin{aligned}[]
		\Lambda_{(1,0)}&=2b\\		
		\Lambda_{(1,-1)}&=a-b.
	\end{aligned}
\end{equation}
The requirement these two to be nonnegative integer produces the list of the irreducible representations:
\begin{equation}			\label{EqIrrepsso5}
	\begin{aligned}[]
		b&\in \eN/2\\
		a&=b,b+1,\ldots
	\end{aligned}
\end{equation}
In particular, $a$ and $b$ are both integer or half-integer.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Cartan matrix and Dynkin diagram of \texorpdfstring{$\so(2l+1)$}{so2l+1}}
%---------------------------------------------------------------------------------------------------------------------------

The Cartan subalgebra of $\lG=\so(2l+1)$ is generated by the elements $\lH=\{ J_{12},J_{34},\ldots,J_{2l-1,2l} \}$.  We denote them by
\begin{equation}
	B_p=J_{2p-1,2p}
\end{equation}
with $p=1,\ldots,l$. As explained in equations \eqref{EqRootSOImpaircinq}, the roots are $\pm B_p^*\pm B_q^*$ and $\pm B_p^*$ with $p=1,\ldots l$. The root spaces of the roots $\pm B^*_p$ are the combinations $J_{1,2l+1}\pm iJ_{2,2l+1}$, $J_{3,2l+1}\pm iJ_{4,2l+1},\ldots$

The positive roots are
\begin{equation}
	\begin{aligned}[]
		B_p^*\pm B_q^*	&&	\text{and}	&& B_p^*
	\end{aligned}
\end{equation}
with $p<q$. In order to determine the simple roots, first remark that $B_p^*+B_q^*$ is the sum of $B_p^*$ and $B_q^*$. The root $B_p^*$ (with $p\neq l$) is not simple because
\begin{equation}
	B^*_p=(B^*_p-B^*_q)+B^*_q.
\end{equation}
If $k>1$, we have
\begin{equation}
	B_p^*-B^*_{p+k} = (B_p^*-B^*_{p+k-1} )+(B_{p+k-1}^*-B^*_{p+k} ).
\end{equation}
Thus the simple roots for $\so(2l+1)$ are
\begin{equation}			\label{EqRacinesSimplessol}
	\begin{aligned}[]
		\alpha_i &=B^*_i-B^*_{i+1}	&	\text{and}	&&	\alpha_l &=B^*_l,
	\end{aligned}
\end{equation}
$i=1,\cdots,l-1$. The inversion of that is easy: 
\begin{equation}		\label{EqsonBenFnDesAlpha}
	B^*_i=\alpha_i+\alpha_{i+1}+\ldots+\alpha_l,
\end{equation}
and in particular, $B^*_l=\alpha_l$. Now we have to determine the basis $\{ t_{\alpha_i}\}$ defined by
\begin{equation}
	\alpha_i(h)=B(t_{\alpha_i},h)
\end{equation}
for every $h\in\lH$. In the dual basis of $\{\alpha_i\}$ if $h-h_j\alpha_j^*$, we have $\alpha_i(h)=h_j\delta_{ij}=h_i$. We have
\begin{equation}
	h_i=B(t_{\alpha},h)=h_jB_{kl}(t_{\alpha_i})_k\underbrace{(\alpha_j^*)_l}_{=\delta_{jl}}=h_jB_{kj}(t_{\alpha_i})_k,
\end{equation}
so we have
\begin{equation}
	(t_{\alpha_i})_l=B^{il}
\end{equation}
where $B^{ij}$ are the component of the inverse of the Killing form matrix in the basis $\{ \alpha_i^* \}$ of $\lH$. The inner product on $\lH^*$ is thus determined, in function of the Killing form, by
\begin{equation}
	B(t_{\alpha_i},t_{\alpha_j})=B_{kl}(t_{\alpha_i})_k(t_{\alpha_j})_l=B_{kl}B^{ik}B^{jl}=B^{ji},
\end{equation}
so that we write
\begin{equation}
	(\alpha_i,\alpha_j)=B^{ij}.
\end{equation}

We know that, for a vector space $V$, if $\xi_i=B_{ij} e^*_j$ is a basis of the dual $V^*$, then the dual basis of $V$ is given by $v_k=A_{kl}e_l$ with $A=(B^t)^{-1}$. In our case, $\alpha_i=B_i^*-B_{i+1}^*$, so that
\begin{equation}
	B =
	\begin{pmatrix}
		1	&	-1\\
		&	1	&	-1\\
		&	&	1	&	-1\\
		&	&	&	\ddots	&\ddots\\
		&	&	&		& 0&1
	\end{pmatrix}
\end{equation}
Inverting that matrix, one finds that
\begin{equation}
	\alpha_k = B_1^*+\ldots+B_k^*.
\end{equation}

Now, we have to find the matrix of the Killing for in the basis $\{ \alpha_i^* \}$. Since $B(B_k,B_l)=-2\delta_{kl}$, we have $B(\alpha_k^*,\alpha_l^*)=-2\min(k,l)$, and the inverse of that matrix is given by
\begin{equation}
	-B^ij=
	\begin{pmatrix}
		1		& -\frac{ 1 }{2}\\
		-\frac{ 1 }{2}	& 1			& -\frac{ 1 }{2}\\
		& -\frac{ 1 }{2}	& 1			&-\frac{ 1 }{2}\\
		&& \ddots		& \ddots		& \ddots\\
		&&			&			&	\frac{ 1 }{2}
	\end{pmatrix},
\end{equation}
so that
\begin{equation}		\label{EqProdsoimpairrac}
	\begin{aligned}[]
		(\alpha_k,\alpha_{k+1})&=\frac{ 1 }{2},	&	(\alpha_k,\alpha_k)&=-1,	&(\alpha_l,\alpha_l)&=-\frac{ 1 }{2}
	\end{aligned}
\end{equation}
when $k\leq l-1$. The Cartan matrix\index{Cartan!matrix!of $\so(2l+1)$} and the Dynkin diagram can now be written down easily:
\begin{equation}		\label{EqCartanSOimpair}
	A=
	\begin{pmatrix}
		2	& -1\\
		-1	& 2	& -1\\
		& -1	& 2		& -1\\
		&	& \ddots	& \ddots	& \ddots\\
		&	&		& -1		& 2	& -1\\
		&	&		& 		& -2	& 2
	\end{pmatrix}
\end{equation}
One sees form the products \eqref{EqProdsoimpairrac} that all the roots but $\alpha_l$ are of maximal length, so that the Dynkin diagram\index{Dynkin!diagram!of $\so(2l+1)$} has $l-1$ withe circles and one black one. Using proposition \ref{PropProdNbLignes}, we find that the Dynkin diagram of $\so(2l+1)$ is
%\begin{equation}
%	\input{image_Dynkin_so(2n+1)_toutes.pstricks}
%\end{equation}
\begin{equation}
   \input{Fig_MNICGhR.pstricks}
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Irreducible representations of \texorpdfstring{$\so(2,l-1)$}{so2l}}
%---------------------------------------------------------------------------------------------------------------------------

We use the same technique as the one from which we deduced the values \eqref{EqIrrepsso5}, using theorem \ref{Thoirrepllamifffmor}. The simple roots $\{ \alpha_k \}$ are given in \eqref{EqRacinesSimplessol} and the inner product is \eqref{EqProdsoimpairrac}. We are thus able to compute the numbers $\Lambda_{\alpha_k}$ for every $\Lambda=\sum_{i=1}^la_iB^*_i$. Let $\Lambda=\sum_{i=1}^la_iB^*_i$; taking \eqref{EqsonBenFnDesAlpha} into account, we compute first $\Lambda_{\alpha_j}$. With $B_k^*$ ($k<l$), we have
\begin{equation}
	(B^*_k,\alpha_l)=(\alpha_{l-1}+\alpha_l,\alpha_l)=\frac{ 1 }{2}-\frac{ 1 }{2}=0,
\end{equation}
so we have $\sum_{i=1}^la_i(B^*_i,\alpha_l)=-\frac{ a_l }{ 2 }$, and
\begin{equation}
	\Lambda_{\alpha_l}=2a_l.
\end{equation}
From that, we deduce $a_l\in\eN/2$. The computation of $\Lambda_{\alpha_1}$ is easy too because $(\alpha_k,\alpha_1)\neq 0$ only when $k=1$ or $k=2$. So $\Lambda_{\alpha_1}=-2(a_1B^*_1+a_2B^*_2,\alpha_1)=a_1-a_2$. That result generalises to
\begin{equation}
	\Lambda_{\alpha_k}=a_k-a_{k+1}.
\end{equation}
Since $a_l$ must belongs to $\eN/2$, we deduce that the irreducible representations of $\so(2l+1)$ are parametrized by
\begin{equation}
	a_l\geq a_{l-1}\geq\ldots\geq a_1
\end{equation}
which are all integer or half integer.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Elementary and basic representations of \texorpdfstring{$\so(2l+1)$}{so2l+1}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}
Attention : ici j'utilise la formule \eqref{EqCofDynMmoisAlpha}, et je crois que je l'utilise avec la transposée de la matrice de Cartan; j'ai peut-être une faute de convention qui invalide la fin de la récurrence.
\end{probleme}

The Dynkin diagram of $so(2l+1)$ is
\begin{equation}		\label{EqDynkSOimpair}
	%\input{image_Dynkin_so(2n+1).pstricks}
   \input{Fig_LEJNDxI.pstricks}
\end{equation}
We are looking at the elementary representation
\begin{equation}
	%\tau_1=\input{image_Dynkin_so(2n+1)_tau_1.pstricks}.
    \tau_1=\input{Fig_RGjjpwF.pstricks}
\end{equation}
Other basic representations on the same branch are given by
\begin{equation}		\label{EqReprBasTausoimpair}
	\tau_k\big( \tau_1^{\otimes k} \big)_1.
\end{equation}
Notice, however, that the last line in the diagram \eqref{EqDynkSOimpair} is not of the kind which is allowed to form a branch. Thus the process \eqref{EqReprBasTausoimpair} does not produce the representation
\begin{equation}
	%\sigma = \input{image_Dynkin_so(2n+1)_sigma.pstricks},
   \input{Fig_STdyNTH.pstricks}
\end{equation}
which is called the \defe{spinor representation}{spin!representation!of $\so(2l+1)$}. Let us now determine the list of weights for the representation $\tau_1$. As showed on the diagram \eqref{EqReprBasTausoimpair}, the Dynkin coefficients of the highest weight are
\begin{equation}
	\Lambda_i=
	\begin{pmatrix}
		1\\
		0\\
		\vdots
	\end{pmatrix},
\end{equation}
and we have $q(\Lambda,\alpha_i)=0$ for every $i$. Thus $\Lambda_i+q(\Lambda,\alpha_i)\geq 1$ only for $i=1$, and the only weight in the first layer is
\begin{equation}
	M^{(1)}=\Lambda-\alpha_1.
\end{equation}
Using formula \eqref{EqCoefDynkMalpha} and the first line of the Cartan matrix \eqref{EqCartanSOimpair}, we find
\begin{equation}
	M^{(1)}_i=
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}-
	\begin{pmatrix}
		2\\-1\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		-1\\1\\0\\\vdots
	\end{pmatrix},
\end{equation}
and, by construction,
\begin{equation}
	q(M^{(1)},\alpha_i)=
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}.
\end{equation}
In order to know what belongs to the second layer, we look at the lucky numbers of $M^{(1)}-\alpha_i$:
\begin{equation}
	M^{(1)}_i+q(M^{(1)},\alpha_i)=
	\begin{pmatrix}
		-1\\1\\0\\\vdots
	\end{pmatrix}+
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		0\\1\\0\\\vdots
	\end{pmatrix},
\end{equation}
and we deduce that $M^{(2)}=M^{(1)}-\alpha_2$ is the only weight in the second layer. Its Dynkin coefficients are given by
\begin{equation}
	M^{(2)}_i=
	\begin{pmatrix}
		-1\\1\\0\\0\\\vdots
	\end{pmatrix}-
	\begin{pmatrix}
		-1\\2\\-1\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		0\\-1\\1\\0\\\vdots
	\end{pmatrix}.
\end{equation}
We can now proceed by induction. Let us suppose that
\begin{equation}
	\begin{aligned}[]
		M^{(k)}_i&=
		\begin{pmatrix}
			0\\\vdots\\-1\\1\\0\\\vdots
		\end{pmatrix},&\text{and}&
		&q(M^{(j)},\alpha_i)=
		\begin{pmatrix}
			0\\\vdots\\1\\0\\0\\\vdots
		\end{pmatrix}
	\end{aligned}
\end{equation}
where in both, the first non zero element is on the $k$th line. The, we see that the only weight in the next layer is
\begin{equation}
	M^{(k+1)}=M^{(k)}-\alpha_{k+1},
\end{equation}
and, using the corresponding line of the Cartan matrix, we find
\begin{equation}
	M^{(k+1)}_i=
	\begin{pmatrix}
		0\\\vdots\\-1\\1\\0\\0\\\vdots
	\end{pmatrix}
	-
	\begin{pmatrix}
		0\\\vdots\\-1\\2\\-1\\0\\\vdots
	\end{pmatrix}
	=
	\begin{pmatrix}
		0\\\vdots\\0\\-1\\1\\0\\\vdots
	\end{pmatrix}.
\end{equation}
The induction finishes with
\begin{equation}
	M^{(l)}=M^{(l-1)}-\alpha_l
\end{equation}
which as the following Dynkin coefficients:
\begin{equation}
	M^{(l)}_i=M^{(l-1)}_i-A_{li}=
\begin{pmatrix}
\vdots\\0\\-1\\1
\end{pmatrix}
-
\begin{pmatrix}
\vdots\\0\\-2\\2
\end{pmatrix}
=
\begin{pmatrix}
\vdots\\0\\1\\-1
\end{pmatrix}.
\end{equation}
Notice that
\begin{equation}
	M^{(l)}_i+q(M^{(l)},\alpha_i)=
\begin{pmatrix}
\vdots\\0\\1\\0
\end{pmatrix},
\end{equation}
so that
\begin{equation}
	M^{(l+1)}=M^{(l)}-\alpha_{l-1}
\end{equation}
is a weight, and its Dynkin coefficients are
\begin{equation}
	\begin{pmatrix}
\vdots\\0\\1\\-1
\end{pmatrix}
-
\begin{pmatrix}
\vdots\\-1\\2\\-1
\end{pmatrix}
=
\begin{pmatrix}
\vdots\\1\\-1\\0
\end{pmatrix}
\end{equation}
with
\begin{equation}
	q(M^{(l+1)},\alpha_i)=
\begin{pmatrix}
\vdots\\0\\1\\0
\end{pmatrix}.
\end{equation}
At this point, a new induction can be done up to get
\begin{equation}
	M^{(2l-1)}=M^{(2l-2)}-\alpha_1.
\end{equation}
Thus we have

\begin{equation}
			M^{(2l-1)}_i=
\begin{pmatrix}
1\\-1\\0\\\vdots
\end{pmatrix}
-
\begin{pmatrix}
2\\-1\\0\\\vdots
\end{pmatrix}=
\begin{pmatrix}
-1\\0\\0\\\vdots
\end{pmatrix}
\end{equation}
and
\begin{equation}
q(M^{(2l-1)},\alpha_i)
=
\begin{pmatrix}
1\\0\\0\\\vdots
\end{pmatrix}
\end{equation}
We immediately check that there are no $M^{(2l)}$. Thus the Dynkin coefficients of the weights of the representation are
\begin{equation}
	\begin{aligned}[]
		\begin{pmatrix}
1\\0\\0\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
\vdots\\1\\-1\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
\vdots\\-1\\1\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
-1\\0\\0\\\vdots
\end{pmatrix}.
	\end{aligned}
\end{equation}
We have $l-1$ weights of each of the two middle types. Thus the representations is $2l$-dimensional.
