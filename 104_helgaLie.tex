% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{Representations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

References for Lie algebras and their modules are \cite{SternLieAlgebra,SamelsonNotesLieAlg,DirkEnvFiniteDimNilLieAlg,Wybourne,zelobenko,rncahn,SSLA_Modave2005}. 

Since $\lH$ is abelian, the operators $H_{\alpha_j}$ ($j=1,\ldots,l$) are simultaneously diagonalisable. In that basis of the representation space $W$, the basis vectors are denoted by $\ket{u_{\Lambda}}$ and have the property
\begin{equation}
    H_{\alpha_i}\ket{u_{\Lambda}}=\Lambda(H_{\alpha_i})\ket{u_{\Lambda}},
\end{equation}
and, as notation, we note $\Lambda_i=\Lambda(H_{\alpha_i})$. The root $\Lambda$ is a \defe{weight}{weight of a vector} of the vector $\ket{u_{\Lambda}}$. The vector $E_{\beta}\ket{u_{\Lambda}}$ is of weight $\beta+\Lambda$, indeed,
\begin{equation}
        H_{\alpha_i}E_{\beta}\ket{u_{\Lambda}}  =\big( [H_{\alpha_i},E_{\beta}]+E_{\beta}H_{\alpha_i} \big)\ket{u_{\Lambda}}
                            =\left( \frac{ 2(\alpha_i,\beta) }{ (\alpha_i,\alpha_i) }+\Lambda_i \right)E_{\beta}\ket{u_{\Lambda}}.
\end{equation}
Thus the eigenvalue of $E_{\beta}\ket{u_{\Lambda}}$ for $H_{\alpha_i}$ is, according to the relation, \eqref{EqbetaialphaiH},  $\beta(H_{\alpha_i})+\Lambda(H_{\alpha_i})$.

We suppose that the roots $\alpha_i$ are given in increasing order:
\begin{equation}
    \alpha_1\geq\alpha_2\geq\ldots\geq\alpha_l,
\end{equation}
and one says that a weight is \defe{positive}{positive!weight} if its first non vanishing component is positive. Then one choose a basis of $W$
\begin{equation}
    \ket{u_{\Lambda^{(1)}}},\ldots,\ket{u_{\Lambda^{(N)}}}
\end{equation}
of weight vectors. One say that this basis is \defe{canonical}{canonical!basis of a representation} if
\begin{equation}
    \Lambda^{(1)}\geq\ldots\geq\Lambda^{(N)}.
\end{equation}

\begin{theorem}
    A vector if weight $\Lambda$ which is a combination of vectors of weight $\Lambda^{(k)}$ all different of $\Lambda$ vanishes.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
A consequence of that theorem is that, if $W$ is a representation of dimension $N$ of $\lG$, there are at most $N$ different weights. When several vectors have the same weight, the number of linearly independent such vectors is the \defe{multiplicity}{multiplicity of a weight} of the weight. A weight who has only one weight vector is \defe{simple}{simple!weight}.

\begin{proposition}
    The weights $\Lambda$ and $\Lambda-2\alpha(\Lambda,\alpha)/(\alpha,\alpha)$ have the same multiplicity for every root $\alpha$.
\end{proposition}

\begin{theorem}
    Two representation are equivalent when they have the same highest weight.
\end{theorem}

\begin{proposition}     \label{PropMakalMmoinsinten}
For any weight $M$ and root $\alpha$,
\begin{equation}
    \frac{ 2(M,\alpha) }{ (\alpha,\alpha) }\in\eZ,
\end{equation}
and 
\begin{equation}
    M-\frac{ 2(M,\alpha) }{ (\alpha,\alpha) }\alpha
\end{equation}
is a weight.
\end{proposition}
Notice, in particular,  that for every weight $M$, the root $-M$ is also a weight.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{List of the weights of a representation}
%---------------------------------------------------------------------------------------------------------------------------

We consider a representation of highest weight $\Lambda$. For each weight $M$, we define
\begin{equation}
    \delta(M)=2\sum_{\alpha_i\in\Pi}M_{\alpha_i}
\end{equation}
where, as usual, $M_{\alpha}=2(M,\alpha)/(\alpha,\alpha)$. For any root $\alpha$, we define
\begin{equation}
    \gamma(\alpha)=\frac{ 1 }{2}\big( \delta(\Lambda)-\delta(\alpha) \big).
\end{equation}
Proposition \ref{PropMakalMmoinsinten} shows in particular that $\gamma(\alpha)$ is an integer. 

\begin{proposition}     \label{PropgammasubMLLweight}
When $M$ is a weight, $\gamma(M)$ is the number of simple roots that have to be subtracted from the highest weight $\Lambda$ in order to get $M$.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

Let us consider the sets
\begin{equation}
    \Delta_{\phi}^k=\{ M\tq\gamma(M)=k \}.
\end{equation}
That set is the \defe{layer}{layer} of order $k$. Of course, there exists a $T(\phi)$ such that
\begin{equation}
    \Delta_{\phi}=\Delta_{\phi}^0\cup\Delta_{\phi}^1\cup\ldots\cup\Delta_{\phi}^{T(\phi)}.
\end{equation}
That $T(\phi)$ is the \defe{height}{height of a representation} of the representation $\phi$. If $\Lambda$ is the highest weight and $\Lambda'$ is the lowest weight, then we have $\gamma(\Lambda)=0$ and $\gamma(\Lambda')=T(\phi)$.

A corollary of proposition \ref{PropgammasubMLLweight} is that, if $M\in\Delta_{\phi}^r$ and if $\alpha$ is a simple root, then $M+\alpha\in\Delta_{\phi}^{r-1}$, and $M-\alpha\in\Delta_{\phi}^{r+1}$. 

Let us denote by $S_k(\phi)$ the multiplicity of the layer of order $k$; we have
\begin{equation}
    S_0+S_1+\ldots+S_T=N,
\end{equation}
where $N$ is the dimension of the representation $\phi$. The number 
\begin{equation}
    III(\phi)=\max S_k(\phi)
\end{equation}
is the \defe{width}{width!of a representation} of the representation.

\begin{lemma}   
    If $\Lambda$ is the highest weight and $\Lambda'$ is the lowest weight, then $\delta(\Lambda)+\delta(\Lambda')=0$.
\end{lemma}
\begin{proof}
    No proof.
\end{proof}
From that lemma and the definition of $\gamma(M)$, we deduce that $\delta(\Lambda)-\delta(\Lambda')=2\gamma(\Lambda')=T(\phi)$, so that $\delta(\Lambda)=T(\phi)$ and
\begin{equation}
     \delta(M)=T(\phi)-2\gamma(M).
\end{equation}
In particular, $\delta(M)$ has a fixed parity for a given representation $\phi$. It is the \defe{parity}{parity!of a representation} (even or odd) of the representation.

\begin{theorem}     \label{ThoLLralphatablefo}
    If $\Lambda$ is the highest weight of the irreducible representation $\phi$, then
    \begin{equation}
        T(\phi)=\sum_{\alpha_i\in\Pi}r_{\alpha_i}\Lambda_{\alpha}
    \end{equation}
    where the coefficients $r_{\alpha_i}$ only depend on the algebra, and in particular not on the representation.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
The coefficients $r_{\alpha_i}$ are known for all the simple Lie algebra, see for example page 105 of \cite{Wybourne}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Finding all the weights of a representation}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The following can be found in \cite{rncahn,Wybourne}.

\begin{theorem}
    If $\Delta_{\phi}$ is the weight system of the irreducible representation $\phi$, then
    \begin{equation}
        S_k=S_{T-k}
    \end{equation}
    and
    \begin{equation}
        S_r\geq S_{r-1}\geq\ldots\geq S_2\geq S_1
    \end{equation}
    where $r=\frac{ T }{ 2 }+1$.
\end{theorem}
The theorem says that when $T(\phi)$ is even (let us say $T(\phi)=2r$), then $III(\phi)=S_r(\phi)$ and when $T(\phi)$ is odd (let us say $T(\phi)=2r+1$), then 
\begin{equation}
    III(\phi)=S_r(\phi)=S_{r+1}(\phi).
\end{equation}

Let $\alpha$ be a root. The \defe{$\alpha$-series}{series of weight}\index{$\alpha$-series of weight} trough the weight $M$ is the sequence of weights 
\begin{equation}
    M-r\alpha,\ldots,M+q\alpha
\end{equation}
such that $M-(r+1)\alpha$ and $M+(q+1)\alpha$ do not belong to $\Delta_{\phi}$.

\begin{proposition}     \label{PropweightCondprstring}
Let $M$ be a weight of the representation $\phi$ and $\alpha$, any root of $\lG$. If the $\alpha$-series trough $M$ begins at $M-r\alpha$ and ends at $M+q\alpha$, then
\begin{equation}
    \frac{ 2(M,\alpha) }{ (\alpha,\alpha) }=r-q,
\end{equation}
or, more compactly, $M_{\alpha}+q=r$.
\end{proposition}
Notice that, in that proposition, $q$ and $r$ are well defined functions of $M$ and $\alpha$.

We are now able to determine all the weights of the representation $\phi$. Let us suppose that we already know all the layers $\Delta_{\phi}^0,\ldots,\Delta_{\phi}^{r-1}$. We are going to determine the weights in the layer $\Delta_{\phi}^r$.

An element of $\Delta_{\phi}^r$ has the form $M-\alpha$ with $M\in\Delta_{\phi}^{r-1}$ and $\alpha$, a root. Thus, in order to determine $\Delta_{\phi}^r$, we have to test if $M-\alpha$ is a weight for each choice of $M\in\Delta_{\phi}^{r-1}$ and $\alpha\in\Pi$. Using proposition \ref{PropweightCondprstring}, if\quext{At page 104 of \cite{Wybourne}, that condition is (I think) wrongly written $M_{\alpha}+q\geq 0$; that mistake is repeated in the example of page 106.}
\begin{equation}
    M_{\alpha}+q \geq 1,
\end{equation}
then $M-\alpha\in\Delta_{\phi}$. The number $M_{\alpha}-q(M,\alpha)$ is the \defe{lucky number}{lucky number} of the root $M-\alpha$. The root is a weight if its lucky number is bigger or equal to $1$. Notice that $q(M,\alpha)$ depends on the representation we are looking at.

Since $M+k\alpha\in\Delta_{\phi}^{r-k}$, the value of $q$ is known when one knows the ``lower'' layers. We are thus able to determine, by induction, all the layers from $\Delta^0_{\phi}$ which only contains the highest weight. For this one, by definition, we always have $q=0$.

The Dynkin coefficients of one weights can be more easily computed using the following formula, which is a direct consequence of definition of the Cartan matrix:
\begin{equation}        \label{EqCoefDynkMalpha}
    (M-\alpha_j)_i=M_i-A_{ji}.
\end{equation}

As example, let us determine the weights of the representation \input{Fig_DynkinpWjUbE.pstricks} of \( \gsu(3)\).
%\input{image_su3Dynkin(-1).pstricks}  
%The result is on figure \ref{LabelFigDynkinpWjUbE}. % From file DynkinpWjUbE
%\newcommand{\CaptionFigDynkinpWjUbE}{<+Type your caption here+>}
The algebra $\gsu(3)$ has two simple roots $\alpha$ and $\beta$ whose inner products are $(\alpha,\alpha)=(\beta,\beta)=1$ and $(\alpha,\beta)=-1/2$. The highest weight of $\phi=$
\input{Fig_DynkinpWjUbE.pstricks}
is $\Lambda=(\alpha+2\beta)/3$.

We first test if $\Lambda-\alpha$ is a weight. Easy computations show that  $\Lambda_{\alpha}=0$ wile $q=0$; thus $\Lambda-\alpha$ is not a weight. The same kind of computations show that $\Lambda_{\beta}=1$, so that $\Lambda_{\beta}=q(\Lambda,\beta)=1$. That shows that $\Delta_{\phi}^1=\{ \Lambda-\alpha \}$.

Let now $M=\Lambda-\beta=(\alpha-\beta)/3$. Since $M+\alpha\notin\Delta_{\phi}$, we have $q(M,\alpha)=0$. On the other hand, $M_{\alpha}=1$, so that $M-\alpha\in\Delta_{\phi}^2$. The last one to have to be tested is $M-\beta$. Since $M+\beta=\Lambda$, we have $q(M,\beta)=1$, but $M_{\beta}=-1$. Thus $M_{\beta}+q(M,\beta)=0$ and $M-\beta$ is not a weight.

We can obviously continue in that way up to find $\Delta_{\phi}^r=0$, but there is an escape to be more rapid. Indeed, using theorem \ref{ThoLLralphatablefo} with coefficients $r_{\alpha}$ that can be found in tables (for example in \cite{Wybourne}), we find 
\begin{equation}
    T(\phi)=2\Lambda_{\alpha}+3\Lambda_{\beta}=2,
\end{equation}
thus we immediately know that $\Delta^3_{\phi}$ does not exist.

On the other hand, one knows the width $III(\phi)=\max S_k(\phi)$ because (since $T(\phi)=2r$, with $r=1$), we have $III(\phi)=S_1(\phi)$. Thus, once $\Delta^1(\phi)$ is determined, we know that the next ones will never have more elements.

In the example, when we know that $M-\alpha$ is a weight, we do not have to test $M-\beta$.

\label{LeTravail}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Tensor product of representations}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Tensor and weight}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Let $\phi$ and $\phi'$ be representations of $\lG$ on the vector spaces $R$ and $R'$ of dimensions $n$ and $m$. If $A\in\eM_n(R)$ and $B\in\eM_m(R')$, the \defe{tensor product}{tensor product!of matrices}, also know as the \defe{Kronecker product}{Kronecker product} of $A$ and $B$ is the matrix $A\otimes B\in\eM_{mn}(R\otimes R')$ whose elements are given by
\begin{equation}
    C_{ik,jl}=A_{ij}B_{kl}.
\end{equation}
The principal properties of that product are
\begin{subequations}
    \begin{align}
        (A_1A_2)\otimes(B_1B_2)&=(A_1\otimes B_1)(A_2\otimes B_2)\\
        (A\otimes B)^{-1}   &= A^{-1}\otimes B^{-1}\\
        \mtu_{R}\otimes\mtu_{R'}&=\mtu_{R\otimes R'}
    \end{align}
\end{subequations}
If $\varphi_1$ and $\varphi_2$ are two representations of a group $G$, the \defe{tensor product}{tensor product!of group representations} is defined by
\begin{equation}
    (\varphi_1\otimes\varphi_2)(g)=\varphi_1(g)\otimes\varphi_2(g).
\end{equation}
If $\phi$ and $\phi'$ are two representations of a Lie algebra $\lG$, the \defe{tensor product}{tensor product!of Lie algebra representations} representation is defined by
\begin{equation}
    (\phi\otimes\phi')(X)(v\otimes v')=\big( \phi(X)v\big)\otimes v'+v\otimes\big( \phi'(X)v' \big).
\end{equation}
If $\{ \phi_k \}$ are the irreducible representations, a natural question that arise is to determine the coefficients $\Gamma$ which decompose $\phi\otimes\phi'$ into irreducible representations:
\begin{equation}
    \phi\otimes\phi'=\sum_k\Gamma_k(\phi,\phi')\phi_k
\end{equation}

Let $W$ and $W'$ be the representation spaces and consider the following decompositions in weight spaces:
\begin{align}
    W&=\bigoplus_{\Lambda\in\Delta_1}W_{\Lambda},&      W'&=\bigoplus_{\Lambda\in\Delta_2}W'_{\Lambda}.
\end{align}
By definition, 
\begin{equation}
    (W\otimes W')_{\alpha}=\{ v\otimes v'\tq (\phi\otimes\phi')(h)(v\otimes v')=\alpha(h)(v\otimes v') \}.
\end{equation}
If $\big( \phi(h)v \big)\otimes v'+v\otimes\big( \phi'(h)v' \big)$ is a multiple of $v\otimes v'$, one requires that
\begin{subequations}
    \begin{align}
        \phi(h)v    &=\alpha_1(h)v,\\
        \phi'(h)v   &=\alpha_2(h)v'
    \end{align}
\end{subequations}
for the weights $\alpha_1$ and $\alpha_2$ of $\phi$ and $\phi'$. Thus we have
\begin{equation}
    (W\otimes W')_{\alpha_1+\alpha_2}=W_{\alpha_1}\otimes W_{\alpha_2}.
\end{equation}

We have in particular that the simple root system $\Delta_{\phi\otimes\phi'}$ of the representation $\phi\otimes\phi'$ is given by
\begin{equation}        \label{EqDeldelDElphitens}
    \Delta_{\phi\otimes\phi'}= \Delta_{\phi}+\Delta_{\phi'}.
\end{equation}

What we proved is\quext{The second part is not proved.}
\begin{proposition} \label{Propphihwrepplullllam}
    If $\phi$ is a representation of highest weight $\Lambda$ and $\phi'$ is a representation of highest weight $\Lambda'$, then $\phi\otimes\phi'$ is a representation of height weight $\Lambda+\Lambda'$.

    If, moreover, $\phi$ and $\phi'$ are irreducible, then $\phi\otimes\phi'$ is irreducible.
\end{proposition}

An irreducible representation that cannot be written under the form of a tensor product of irreducible representations is a \defe{basic representation}{basic!representation}.

\begin{lemma}
    A representation is basic if and only if its highest weight $\Lambda$ is such that the $\Lambda_{\alpha_i}$ are all zero but one which is $1$.
\end{lemma}
The basic representations of $\so(10)$ are given by the Dynkin diagrams of figure \ref{LabelFigDynkinNUtPJx}. All the irreducible representations are obtained by tensor products of the basic ones. An \defe{elementary}{elementary!representation} is a basic representation which has his ``1'' on a terminal point of the Dynkin diagram.
\newcommand{\CaptionFigDynkinNUtPJx}{Basic representations of $\so(10)$}
\input{Fig_DynkinNUtPJx.pstricks}
%L'autre truc est \ref{LabelFigDynkinNUtPJx}. % From file DynkinNUtPJx

% La ligne suivante Ã©tait l'ancien fichier contenant la figure.
%\input{fig_basic_sodix.pstricks} 

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Decomposition of tensor products of representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


%This is the picture of 
%                    1                    
%                    o-----o
%i.e. the old picture image_su3Dynkin(1-).pstricks
%The result is on figure \ref{LabelFigDynkinrjbHIu}. % From file DynkinrjbHIu
%\newcommand{\CaptionFigDynkinrjbHIu}{<+Type your caption here+>}
%\input{Fig_DynkinrjbHIu.pstricks}

% This is the picture of
%                          1
%                    o-----o
%The result is on figure \ref{LabelFigDynkinpWjUbE}. % From file DynkinpWjUbE
%\newcommand{\CaptionFigDynkinpWjUbE}{<+Type your caption here+>}
%\input{Fig_DynkinpWjUbE.pstricks} 

%This is the picture of 
%                          2                    
%                    o-----o
%i.e. the old picture \input{image_su3Dynkin(-2)
%The result is on figure \ref{LabelFigDynkinqlgIQl}. % From file DynkinqlgIQl
%\newcommand{\CaptionFigDynkinqlgIQl}{<+Type your caption here+>}
%\input{Fig_DynkinqlgIQl.pstricks}


Proposition \ref{Propphihwrepplullllam} allows us to decompose a tensor product of representations into irreducible representations. Let us do it on a simple example in $\gsu(3)$. We consider the representations $\phi=$\input{Fig_DynkinrjbHIu.pstricks} and $\phi'$=\input{Fig_DynkinpWjUbE.pstricks}. The first representation has weights
\begin{equation}
    \Delta_{\phi}=\left\{ \frac{ \alpha+2\beta }{ 3 },\frac{ \alpha-\beta }{ 3 },\frac{ -(2\alpha+\beta) }{ 3 } \right\},
\end{equation}
and the second one has
\begin{equation}
    \Delta_{\phi'}=\left\{ \frac{ \alpha+2\beta }{ 3 },\frac{ \alpha-\beta }{ 3 },\frac{ -(2\alpha+\beta) }{ 3 } \right\}.
\end{equation}

According to equation \eqref{EqDeldelDElphitens}, we have $9$ weights in the representation $\phi\otimes\phi'$ (all the sums of one element of $\Delta_{\phi}$ with a one of $\Delta_{\phi'}$). The highest one is
\[ 
    \frac{ 2\alpha+4\beta }{ 3 },
\]
which is the double of the highest weight in \input{Fig_DynkinpWjUbE.pstricks}, so $\phi\otimes\phi'$ contains the representation \input{Fig_DynkinqlgIQl.pstricks}. Now, we remove from the list of weights of $\phi\otimes\phi'$ the list of weight of \input{Fig_DynkinqlgIQl.pstricks}; the result is
\begin{equation}
    \frac{ 2\alpha+\beta }{ 3 },\frac{ -(\alpha-\beta) }{ 3 },\frac{ -(\alpha+2\beta) }{ 3 },
\end{equation}
which are the weights of \input{Fig_DynkinrjbHIu.pstricks}. The conclusion is that\quext{I guess the following line is a typo and should be 1--o times o--1.}
\begin{equation}
    \text{\input{Fig_DynkinpWjUbE.pstricks}}\otimes\text{\input{Fig_DynkinpWjUbE.pstricks}}=\text{\input{Fig_DynkinqlgIQl.pstricks}}\oplus\text{\input{Fig_DynkinrjbHIu.pstricks}}.
\end{equation}
That procedure of decomposition is quite long because it requires to compute the complete set of weights for some intermediate representations.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsubsection{Symmetrization and anti symmetrization}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let $\phi$ be a irreducible representation. We want to compute the symmetric and antisymmetric parts of the representation $\phi^{\otimes k}=\underbrace{\phi\otimes\ldots\otimes\phi}_{\text{$k$ times}}$. These symmetric and antisymmetric parts are denoted by $\phi^{\otimes k}_s$ and $\phi^{\otimes k}_a$ respectively.

\begin{proposition}
    If $\{ \xi_1,\ldots,\xi_N \}$ is a canonical basis of $\phi$ and if we denote by $\Lambda_i$ the weight of the vector $\xi_i$, the followings hold:
    \begin{enumerate}
        \item the weight system of $\phi^{\otimes k}_a$ is
            \begin{equation}
                \Lambda_{i_1}+\Lambda_{i_2}+\ldots+\Lambda_{i_k}
            \end{equation}
            with $i_k>\ldots>i_2>i_1$, and the highest weight is
            \begin{equation}
                \Lambda_1+\ldots+\Lambda_k.
            \end{equation}
            The dimension of the representation $\phi^{\otimes k}_a$ is
            \begin{equation}
                N\big( \phi^{\otimes k}_a \big)= {n\choose k}.
            \end{equation}

        \item The weight system of the representation $\phi^{\otimes k}_s$ is
            \begin{equation}
                \Lambda_{i_1}+\Lambda_{i_2}+\ldots+\Lambda_{i_k}
            \end{equation}
            with $i_k\geq \ldots\geq i_2\geq i_1$, and the highest weight is
            \begin{equation}
                k\Lambda_1
            \end{equation}
            The dimension of the representation $\phi^{\otimes k}_s$ is
            \begin{equation}
                N\big( \phi^{\otimes k}_s \big)= {n+k\choose k}.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    No proof.
\end{proof}
The representations $\phi^{\otimes k}_a$ and $\phi^{\otimes k}_s$ might be decomposable and we denote by $\phi^{\otimes k}_{s>}$ and $\phi^{\otimes k}_{a>}$ their highest weight parts.


Let $\alpha$ be a terminal point in a Dynkin diagram. The \defe{branch}{branch!in a Dynkin diagram} of $\alpha$ is the sequence of point of the Dynkin diagram $\alpha=\alpha_1,\alpha_2,\ldots,\alpha_k$ defined by the following properties.
\begin{itemize}
    \item The point $\alpha_i$ is connected with (and only with) the points $\alpha_{i-1}$ and $\alpha_{i+1}$,
    \item the connexion between $\alpha_i$ and $\alpha_{i+1}$ is of one of the following forms
        \begin{subequations}
            \begin{align}
            \input{Fig_ADUGmRRA.pstricks}\\
   \input{Fig_ADUGmRRB.pstricks}\\
   \input{Fig_ADUGmRRC.pstricks}
            \end{align}
        \end{subequations}
    \item the sequence $\alpha_1,\ldots,\alpha_k$ is maximal in the sense that no $\alpha_{k+1}$ can be added without violating one of the two first rules.
\end{itemize}

\begin{proposition}
    Let $\alpha$ be a terminal point in a Dynkin diagram and $\alpha_1,\ldots,\alpha_k$ be the corresponding branch. Then we have
    \begin{equation}
        \phi_{\alpha_r}\simeq \phi^{\otimes r}_{\alpha\,a>}
    \end{equation}
    for every $r=1,2,\ldots,k$.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{Verma module}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let us give the definition of \cite{VermaPiercey}. When $\lG$ is a semisimple Lie algebra, we have the usual decomposition
\begin{equation}
    \lG=\lN^-\oplus\lH\oplus\lN^+,
\end{equation}
where each of the three components are Lie algebras. In particular, the universal enveloping algebra $\mU(\lN^-)$ makes sense. Let $\mu\in\lH^*$. We build a representation $\pi_{\mu}$ of $\lG$ on $V_{\mu}=\mU(\lN^-)$ in the following way
\begin{itemize}
\item If $Y_{\alpha}\in\lN^-$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(Y_{\alpha})1  &=Y_{\alpha}\\
        \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_n})&=Y_{\alpha}Y_{\alpha_1}\ldots Y_{\alpha_n},
    \end{align}
\end{subequations}
\item if $H\in\lH$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(H)1   &=\mu(H)\\
        \pi_{\mu}(Y_{\alpha_1}\ldots Y_{\alpha_k})  &= \big( \mu(H)-\sum_{j=1}^k\alpha_j(H) \big)Y_{\alpha_1}\ldots Y_{\alpha_k},
    \end{align}
\end{subequations}
\item and if $X_{\alpha}\in\lN^+$, we define
\begin{subequations}
    \begin{align}
        \pi_{\mu}(X_{\alpha})1  &=0\\
        \pi_{\mu}(X_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k}    &=Y_{\alpha_1}\big( \pi_{\mu}(X_{\alpha})Y_{\alpha_2}\ldots Y_{\alpha_k} \big)\\
                                    &\quad  -\delta_{\alpha,\alpha_1}\sum_{j=1}^k\alpha_j(H_{\alpha})Y_{\alpha_1}\ldots Y_{\alpha_k}.
    \end{align}
\end{subequations}
\end{itemize}
In the last one, we do an inductive definition. 
\begin{lemma}
The couple $(\pi_{\mu},V_{\mu})$ is a representation of $\lG$ on $V_{\mu}$.
\end{lemma}
\begin{proof}
    No proof.
\end{proof}
That representation is one \defe{Verma module}{Verma module} for $\lG$. If the algebra $\lG$ is an algebra over the field $\eK$, the field $\eK$ itself is part of $\mU(\lN)^-$, so that the scalars are vectors of the representation. In that context, the multiplicative unit $1\in \eK$ is denoted by $v_0$.

\begin{theorem}
The representation $(\pi_{\mu},V_{\mu})$ of the semisimple Lie algebra $\lG$ is a cyclic module of highest weight, with highest weight $\mu$ and where $v_0$ is a vector of weight $\mu$.
\end{theorem}
\begin{proof}
    No proof.
\end{proof}
The Verma module is, \emph{a priori}, infinite dimensional and non irreducible, thus one has to perform quotients of the Verma module in order to build finite dimensional irreducible representations.
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{Cyclic modules and representations}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

An example over $\so(3)$ is given in subsection \ref{subSubSecweightsotrois}. The case of $\so(5)$ is treated in subsection \ref{SubSecsocinq}. Let $\lG$ be a semisimple Lie algebra with a Cartan subalgebra $\lH$ and a basis $\Delta$ for its roots $\Phi=\Phi^+\cup\Phi^-$. Let $W$ be a finite dimensional $\lG$-module.

\begin{lemma}
If $\lG$ is a nilpotent complex algebra and if $\gamma$ is a weight, then there exists a $v$ in $V_{\gamma}$ such that $c\cdot v=\gamma(x)v$ for every $x\in\lG$.
\end{lemma}
This is the proposition \ref{prop:trois_poids}. Notice that a Cartan algebra is nilpotent, thus one has at least one vector of $W$ which is a common eigenvector of every elements of $\lH$, in other words, $\exists\mu\in\lH^*$ and $\exists w\in W$ such that
\begin{equation}
    hw=\mu(h)w
\end{equation}
for every $h\in\lH$, and $w\neq 0$. If $w$ is such and if $x\in\lG_{\alpha}$, we have
\begin{equation}
    (hx)\cdot w=[h,x]\cdot w+(xh)\cdot w=\alpha(h)x\cdot w+x\mu(h)w=(\alpha+\mu)(h)x\cdot w.
\end{equation}
If we define 
\begin{equation}
    S=\{ w\in W\tq\exists\mu\in\lH^*\tq hw=\mu(h)w \},
\end{equation}
this is not a vector space, but the vector space $\Span S$ generated by $S$ is invariant under $\lG$ because $S$ itself is invariant under all the $\lG_{\alpha}$ with $\alpha\in\lG^*$.

On the other hand, we suppose that $\lG$ and $W$ are finite dimensional, so that their dual are isomorphic. Since a Cartan subalgebra is chosen, we have the decomposition
\begin{equation}
    \lG=\lH\oplus_{\alpha\in\lH^*}\lG_{\alpha}
\end{equation}
where $\lG_{\alpha}=\{ x\in\lG\tq [h,x]=\alpha(h)x\,\forall g\in\lH \}$. When $\alpha\in\lH^*$, the two following spaces are independent of the choice of the Cartan subalgebra $\lH$:
\begin{equation}
    \begin{aligned}
        W_{\alpha}  &=\{ v\in W \tq hv=\alpha(h)v\,\forall h\in\lH \}\\
        \lG_{\alpha}    &=\{ x\in\lG    \tq [h,x]=\alpha(h)x\,\forall h\in\lH \}.
    \end{aligned}
\end{equation}
If $v_{\alpha}\in W_{\alpha}$ and $x_{\beta}\in\lG_{\beta}$, we have
\begin{equation}
    h(x_{\beta}v_{\alpha})=\big( [h,x_{\beta}]+x_{\beta}h \big)v_{\alpha}=\big( \beta(h)+\alpha(h) \big)x_{\beta} v_{\alpha},
\end{equation}
so $x_{\beta}v_{\alpha}\in W_{\alpha+\beta}$. Thus $x_{\beta}$ is a map
\begin{equation}
    x_{\alpha}\colon W_{\alpha}\to W_{\alpha+\beta}.
\end{equation}
Since $W$ is finite dimensional, there exists a maximal $\alpha$ such that $W_{\alpha}\neq0$. We name it $\lambda$. For every $\beta\in\Phi^+$, we have $W_{\lambda+\beta}=\{ 0 \}$. In particular, if $v_{\lambda}\in W_{\lambda}$,
\begin{equation}
    x_{\alpha}x_{\lambda}=0
\end{equation}
for every $\alpha\in\Phi^+$, and, of course, 
\begin{equation}
    hv_{\lambda}=\lambda(h)v_{\lambda}.
\end{equation}
On the other hand, for every vector $v\in W$, and for $v_{\lambda}$ in particular, the space $\mU(\lG)v$ is invariant, so
\begin{equation}
    W=\mU(\lG)v_{\lambda}
\end{equation}
by irreducibility. One say that $W$ is the \defe{cyclic module}{cyclic!module} generated by $v_{\lambda}$.


%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Choice of basis}
%---------------------------------------------------------------------------------------------------------------------------



\begin{theorem}     \label{ThoBaseUGxxmono}
    Let $\lG$ be a Lia algebra on a field of characteristic zero. If $\{ x_i \}$ is an ordered basis of $\lG$, then
    \begin{equation}
        \{ x_{i_1}\cdots x_{i_n}\tq i_1\leq\ldots\leq i_n \}
    \end{equation}
    is a basis for the universal enveloping algebra $\mU(\lG)$ of $\lG$.
\end{theorem}
One can find a proof in \cite{DirkEnvFiniteDimNilLieAlg}. 

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Roots and highest weight vectors}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropoIrrrgenffflamble}
An irreducible cyclic module is generated by the elements of the form $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$.
\end{proposition}

\begin{proof}
    From theorem \ref{ThoBaseUGxxmono}, the monomials of the form
    \begin{equation}
        (f_1^{i_1}\cdots f_m^{i_m})\cdot (h_1^{j_1}\cdots h_l^{j_l})\cdot (e_1^{k_1}\cdots e_m^{k_m})
    \end{equation}
    form a basis of $\mU(\lG)$. When one act with such an element on $v_{\lambda}$, the $e_i$ kill it, while the $h_i$ do not act (a part of changing the norm). Thus, in fact, the module $W$ is generated by the only elements $f_1^{i_1}\cdots f_m^{i_m}v_{\lambda}$
\end{proof}
In very short, one can write
\begin{equation}        \label{EqWnmoinvlambldarootmodul}
    W=(\lN^-)^nv_{\lambda}.
\end{equation}
Since $f_kv_{\alpha}\in\lG_{\alpha-\alpha_k}$, we have
\begin{equation}        \label{Eqfmlaphamoinsmouns}
    f_1^{i_1}\cdot f_m^{i_m}v_{\lambda}\in\lG_{\lambda-(i_m\alpha_m-\ldots i_1\alpha_1)}.
\end{equation}
The set of roots is ordered by
\begin{equation}
    \begin{aligned}
        \mu_1&\prec\mu_2    &   \text{iff}  &&  \mu_2-\mu_1&=\sum_i k_i\alpha_i
    \end{aligned}
\end{equation}
with $\alpha_i>0$ and with $k_i\in\eN$. Equation \eqref{Eqfmlaphamoinsmouns} means that 
\begin{equation}
    \mu\prec\lambda
\end{equation}
for every weight $\mu$ of $W$.

\begin{definition}
Let $\lG$ be a finite dimensional Lia algebra. A \defe{cyclic module of highest weight}{module!highest weight} for $\lG$ is a module (not specially of finite dimension) in which there exists a vector $v_+$ such that $x_+v_+=0$ for every $x_+\in\lN^+$ and $hv_+=\lambda(h)v_+$ for every $h\in\lH$.
\end{definition}

\begin{proposition}
Every submodule of a cyclic highest weight module is a direct sum of weight spaces.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

From the relation $x_+v_+=0$, we know that all the weight spaces satisfy $V_{\mu}$ satisfy $\mu\prec\lambda$, and, since a module is the sum of all its submodules,
\begin{equation}        \label{EqVsumValpha}
    V=\bigoplus V_{\mu}.
\end{equation}
Notice that if $v_+$ is in a submodule, then that submodule is the whole $V$, thus the sum of two proper submodules is a proper submodule. We conclude that $V$ has an unique maximal submodule, and has thus an unique irreducible quotient.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dominant weight}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDomiunSei}

We know that every representation is defined by a highest weight. The following proposition\cite{Anupam} shows that every root cannot be a highest weight of an irreducible representation.

\begin{proposition}
    The highest weight of an irreducible representation of a simple complex Lie algebra is an integral dominant weight.
\end{proposition}

\begin{proof}
    Let \( \alpha_i\) be a simple root and consider the corresponding copy of \( \gsl(2,\eC)\) generated by \( \{ e_i,f_i,h_i \}\) (see proposition \ref{PropWEzZYzC}). The following part of \( L(\Lambda)\) is a \( \gsl(2,\eC)_i\)-module:
    \begin{equation}
        V(\alpha_i)=\bigoplus_{n\in\eZ}V_{\Lambda+n\alpha_i}=V_{\Lambda}\oplus V_{\Lambda-\alpha_i}\oplus V_{\Lambda-2\alpha_i}\oplus\ldots\oplus V_{\Lambda-r\alpha_i}
    \end{equation}
    for some positive integer \( r\). Notice that the sum over \( n\in\eZ\) does not contain terms with \( n<0\) because \( \Lambda\) being an highest weight, \( V_{\Lambda+k\alpha_i}=\emptyset\) when \( k>0\). We know that in a \( \gsl(2,\eC)\)-module the eigenvalues of \( h\) run from \( -m\) to \( m\) (see equations \eqref{EqReprezgsldeuxC} for example). Thus here
    \begin{equation}
        \Lambda(h_i)=-(\Lambda-r\alpha_i)(h_i).
    \end{equation}
    By construction \( \alpha_i(h_i)=2\), so \( \Lambda(h_i)=r\) and the proof is finished.
\end{proof}

\begin{proposition}
    If \( \Lambda\) is the highest weight of the representation \( L(\Lambda)\) of the complex simple Lie algebra \( \lG\) and if \( w_0\) is the longest elements of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{proposition}

\begin{proof}
    First remember that whenever \( \lambda\) is a weight of a representation and \( w\) is an element of the Weyl group, the root \( w\lambda\) is a weight\quext{To be proved.}; in particular \( w_0\Lambda\) is a weight of \( L(\Lambda)\).   Let \( v\in L(\Lambda)_{w_0\Lambda}\); we want to show that \( X_i^-v=0\).

    If \( X_i^-v\neq 0\), then \( w_0\Lambda-\alpha_i\) is a weight and \( w_0\big( w_0\Lambda-\alpha_i \big)=\Lambda-w_0\alpha_i\) is a weight too. Here we used the fact that \( w_0^2=\id\).
\end{proof}

\begin{probleme}
    Still to be shown:
    \begin{enumerate}
        \item
            \( w\lambda\) is a weight
        \item
            \( w_0^2=\id\)
    \end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Verma modules}
%---------------------------------------------------------------------------------------------------------------------------

Let us consider
\begin{equation}
    \lB=\lH\oplus\lN^+,
\end{equation}
and take $\alpha\in\lH^*$. Now, we define $\eC_{\alpha}$ as the vector space $\eC$ (one dimensional, generated by $z_+\in\eC$) equipped with the following action of $\lB$:
\begin{equation}
    \big( h+\sum_{\mu\prec 0}x_{\mu} \big)z_+=\alpha(h)z_+.
\end{equation}
The vector space $\eC_{\alpha}$ becomes a left $\mU(\lB)$-module. On the other hand, $\mU(\lG)$ is a free right $\mU(\lB)$-module because $\mU(\lB)\cup\mU(\lG)\subseteq\mU(\lG)$. As $\mU(\lB)$-module, a basis of $\mU(\lG)$ is given by $\lN^-$, i.e. by $\{ f_1^{i_1}\cdots f_m^{i_l} \}$. The \defe{Verma module}{Verma module} is the cyclic module
\begin{equation}
    \Verm(\alpha)=\mU(\lG)\otimes_{\mU(\lB)}\eC_{\alpha}
\end{equation}
which has a highest weight vector $v_{\lambda}=1\otimes z_+$. The tensor product over $\mU(\lB)$ beans that, when $X\in\mU(\lG)$, then
\begin{equation}
    \big( h+\sum_{\mu}x_{\mu} \big)X\otimes_{\mU(\lB)}zz_+=X\otimes\big( h+\sum_{\mu}x_{\mu} \big)zz_+=X\otimes_{\mU(\lG)}z\alpha(h)z_+=\alpha(h)X\otimes_{\mU(\lB)}zz_+.
\end{equation}
The Verma module is generated by $1\otimes z_+$ and the fact that
\begin{equation}
    zX(1\otimes z_+)=X\otimes zz_+.
\end{equation}

\begin{proposition}
Two irreducible cyclic modules with same highest weight are isomorphic.
\end{proposition}

\begin{proof}
Let $V$ and $W$ be two highest weight cyclic modules with highest weight $\lambda$ and highest weight vectors $v_{\lambda}$ and $w_{\lambda}$. In the module $V\oplus W$, the vector $v_{\lambda}\oplus w_{\lambda}$ is a highest weight vector of weight $\lambda$. Let us consider the module
\begin{equation}
    Z=\mU(\lG)(v_{\lambda}\oplus w_{\lambda}).
\end{equation}
That module is a highest weight cyclic module. The projections onto $V=Z/W$ and $W=Z/V$ are non vanishing surjective homomorphisms, so $V$ and $W$ are irreducible quotients of $Z$. But we saw bellow equation \eqref{EqVsumValpha} that $Z$ can only accept one irreducible quotient. Thus $V$ and $W$ are isomorphic.
\end{proof}
We denote by $\Irr_{\lG}(\lambda)$\nomenclature{$\Irr_{\lG}(\lG)$}{the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.} the unique cyclic highest weight $\lG$-module with highest weight $\lambda$.




\section{Semi-direct product}
%++++++++++++++++++++++++++++

\subsection{From Lie algebra point of view}\label{subsec:semi_Lie}
%----------------------------------------

Here, the matter comes from \cite{Knapp,newfromold}. When $\mfa$ and $\mfb$ are Lie algebras, one can consider $\mfg=\mfa\oplus\mfb$ as vector space, and define a Lie algebra structure on $\mfg$ by
\[
     [ (a,b),(a',b') ]=( [a,a'],[b,b'] ).
\]
This is the \defe{direct sum}{direct!sum of Lie algebras} of $\mfa$ and $\mfb$.

An endomorphism $\mD$ of the Lie algebra $\mfa$ is a \defe{derivation}{derivation!of a Lie algebra} when
\[
   \mD[X,Y]=[\mD X,Y]+[X,\mD Y].
\]
The set of the derivations of $\mfa$ is written $\Der\mfa$.

\begin{proposition}\label{prop:Lie_derr}
    Let $\mfa$ be a Lie algebra
    \begin{enumerate}
        \item $\Der\mfa$ is a Lie algebra for the usual commutator,
        \item $\dpt{\ad}{\mfa}{\Der\mfa\subseteq\End\mfa}$ is a Lie algebra homomorphism.
    \end{enumerate}
\end{proposition}

\begin{proof}
    For the first statement, we just have to compute to see that if $\mD,\mE\in\Der\mfa$,
    \[
        [\mD,\mE][X,Y]=(\mD\mE-\mE\mD)[X,Y]=[ [\mD,\mE]X,Y ]+[ X,[\mD,\mE]Y ].
    \]

    The second comes from the fact that $\ad X\in\Der\mfa$ for any $X\in\mfa$ and
    $\ad[X,Y]=\ad X\ad Y-\ad Y\ad X$.
\end{proof}

Let us now consider the vector space direct sum $\mfg=\mfa\oplus\mfb$. Let us suppose moreover that $\mfa$ is a Lie subalgebra of $\mfg$ and that $\mfb$ is an idea in $\mfg$. So we have that
\[
   \ad|_{\mfb}\in\Der\mfb.
\]
By proposition \ref{prop:Lie_derr}, we have a homomorphism $\dpt{\pi}{\mfa}{\Der\mfb}$, $\pi(A)=\ad A|_{\mfb}$. So if $A\in\mfa$ and $B\in\mfb$, $[A,B]=\pi(A)B$. The conclusion is that the Lie algebra structure of $\mfg$ is given by $\mfa$, $\mfb$ and $\pi$. In this case, we write   $\mfg=\mfa\oplus_{\pi}\mfb$,
and we say that $\mfg$ is the semidirect product of $\mfa$ and $\mfb$. The following theorem gives the general definition of semidirect product.

\begin{theorem}
    Let $\mfa$ and $\mfb$ be two Lie algebras, and $\dpt{\pi}{\mfa}{\Der\mfb}$, a Lie algebra homomorphism. There exists an unique Lie algebra structure on the vector space $\mfg=\mfa\oplus\mfb$ such that
    \begin{itemize}  
    \item the commutators on $\mfa$ and $\mfb$ are the old ones,
    \item $[A,B]=\pi(A)B$ for any $A\in\mfa$ and $B\in\mfb$.
    \end{itemize}
    In this case, in the so defined Lie algebra $\mfg$, $\mfa$ is a subalgebra and $\mfb$ is an ideal.
\end{theorem}

The vector space $\mfg=\mfa\oplus\mfb$ endowed with this Lie algebra structure is the 
\defe{semidirect product}{semi-direct product!of Lie algebras} of $\mfa$ and $\mfb$, it is denoted by 
\[
  \mfg=\mfa\oplus_{\pi}\mfb
\]
One also often speak about \defe{split extension}{split!extension} of $\lA$ by $\lB$, with the splitting map $\pi$.

\begin{proof}
    The unicity part is clear: the Lie algebra structure is completely defined by the two conditions and the condition of antisymmetry. The matter is just to see that this structure is a Lie algebra structure: we have to check Jacobi. If in $[[X,Y],Z]$, $X,Y,Z$ are all three in $\mfa$ or $\mfb$, it is trivial. The two other cases are  :
    \begin{itemize}
    \item $X$, $Y\in\mfa$ and $Z\in\mfb$. In this case, we use $\pi([X,Y])=\pi(X)\pi(Y)-\pi(Y)\pi(x)$ (because $\pi$ is a Lie algebra homomorphism) to find
    \[
    [[X,Y],Z]=\pi([X,Y])Z=-[[Y,Z],X]-[[Z,X],Y].
    \]

    \item The second case is $X$, $Y\in\mfb$ and $Z\in\mfa$. Here, we use the fact that $\pi(Z)$ is a derivation of $\mfb$. The computation is also direct.
    \end{itemize}

    It is clear that $\mfb$ is an ideal because for any $A\in\mfa$ and $B\in\mfb$, $[B,A]=-[A,B]=-\pi(A)B\in\mfb$.

\end{proof}

The theory of split extension is often used in the following sense. We have a Lie algebra $\mfg$ which decomposes (as vector space) into a direct sum $\mfa\oplus\mfb$. If in $\mfg$ the map $a\mapsto \ad(a)$ is an action of $\mfa$ on $\mfb$, we say that $\mfg$ is a split extension
\[ 
  \mfg=\mfa\oplus_{\ad}\mfb.
\]
This way to use split extensions is used for example in the proof of proposition \ref{PropRsurSglobgroup}.


\subsection{From a Lie group point of view}
%---------------------------------------
\dref{14.9}.


\begin{definition}
A subgroup $H$ is \defe{normal}{normal!subgroup} in the group $G$ if for any $g\in G$ and $a\in H$, $gag^{-1}\in H$.
\end{definition}

If $G$ is a group, $N$ a normal subgroup and $L$ a subgroup, we have $LN=NL$ where, by notation, if $A$ and $B$ are subsets of $G$, $AB=\{xy|x\in A,y\in B\}$. 

If $N$ and $L$ are groups, an \defe{extension}{extension!of group} of $N$ by $G$ is a short exact sequence
\begin{equation}
\xymatrix{ e \ar[r]& N \ar[r]^{\displaystyle i} & G \ar[r]^{\displaystyle \pi} & L \ar[r]^L  & e }
\end{equation}
which means that

\begin{enumerate}
\item $i$ is injective because only $e_N$ is sent to $e_G$,
\item $\pi$ is surjective because the whole $L$ is sent to $e$.
\end{enumerate}
One often say that $G$ is an extension of $N$ by $L$. In the most common case, $i$ is the inclusion, $L=G/N$ and $\pi$ is the natural projection.

We say that the extension is \defe{split}{split!extension} when there exists a \emph{split homomorphism} $\dpt{\rho}{L}{G}$ such that $\rho\circ\pi=\id_G$. 

\begin{definition}
We say that $G$ is the \defe{semidirect product}{semi-direct product!of Lie groups} of $N$ and $L$ when any $g\in G$ can be written in one and only one way as $g=nl$ with $n\in N$ and $l\in L$.
\end{definition}


\begin{definition}
A \defe{Lie group homomorphism}{homomorphism!of Lie group} between $G$ and $G'$ is a map $\dpt{u}{G}{G'}$ which is a group homomorphism and a morphism between $G$ and $G'$ as differentiable manifolds.
\end{definition}

\begin{lemma}
Any continuous (group) homomorphism between two Lie groups is a \emph{Lie} group homomorphism.
\end{lemma}
\dref{19.10.2} 

We consider $G$, a connected Lie group; $N$, a closed normal subgroup; and $L$, a connected immersed Lie group. Moreover, we suppose that $G$ is semidirect product of $N$ and $L$.

\begin{proposition}
The restriction to $L$ of the canonical projection $\dpt{\pi}{G}{G/N}$ is continuous for the induced topology from $G$ to $L$.
\end{proposition}
\begin{proof}
      The definition of an open set $\mU$ in $G/N$ is that $\pi^{-1}(\mU)$ is open in $G$. Then it is clear that $\pi$ is continuous. The matter is to check it for $\pi|_L$. Let $\mU$ be a subset of $\pi(L)$. It is unclear that $\pi^{-1}(\mU)\subset L$, but it is true that $\pi|_L^{-1}(\mU)\subset L$.
      
      As far as the induced topology on $L$ is concerned, $A\subset L$ is open when $A=\mO\cap L$ for a certain open set $\mO$ in $G$.
      
      Let $\mU$ be an open subset of $\pi|_L(L)$; this is $\pi^{-1}(\mU)$ is open in $G$. We have to compare $\pi^{-1}(\mU)$ and $\pi|_L^{-1}(\mU)$. Since
      
    \[
        \pi|_L^{-1}(\mU)=\{x\in L|\pi(x)\in\mU\},
    \]
    we have $\pi|_L^{-1}(\mU)=\pi^{-1}(\mU)\cap L$. But $\pi^{-1}(\mU)$ is open in $G$, then $\pi^{-1}(\mU)\cap L$ is open in $L$.
\end{proof}

\begin{proposition}
The group $G$ is the semidirect product of $N$ and $L$ if and only if $G=NL$ and $N\cap L=\{e\}$.
\end{proposition}

\begin{proof}

If $G$ is semidirect product of $N$ and $L$, $G=NL$ is clear. In this case, if $e\neq z\in N\cap L$, $z=ez=ze$, thus $z\in G$ can be written in two ways as $xy$ with $x\in N$ and $y\in L$.

For the converse, let us consider $n'l'=nl$. Then $x^{-1} x'=yy'{}^{-1}\in N\cap L=\{e\}$. Thus $x'=x$ and $y'=y$.
\end{proof}

Now, we consider $N$, a normal subgroup of $G$. If $\dpt{\pi}{G}{G/N}$ is the canonical homomorphism, the restriction $\dpt{\pi|_L}{L}{G/N}$ is an isomorphism. Indeed, on the one hand, this is surjective because $G=NL$ yields 
$[g]=[nl]=[l]=\pi|_L(l)$. On the other hand, $\pi|_L(l)=\pi|_L(l')$ implies that $l=nl'$ for a certain $n\in N$. Then $ll'{}^{-1}=n\in N\cap L=\{e\}$. So $n=e$ and $l=l'$.

\begin{remark}
If $N$ is any normal subgroup of $G$, there doesn't exist in general any subgroup $L$ of $G$ such that $G$ should be the semidirect product of $N$ and $L$.
\end{remark}

If $G$ is the semidirect product of $N$ and $L$, for any $y\in L$, $\dpt{\sigma_y}{x}{yxy^{-1}}$ is an automorphism of $N$. The point is that $\sigma_y(a)\in N$ for all $a\in N$ because $N$ is a normal subgroup.

It is also clear that $\forall\,u,v\in L$, $\sigma_{uv}=\sigma_u\circ\sigma_v$. Then $\dpt{\sigma}{L}{\Aut N}$\footnote{$\Aut N$ is the set of all the automorphism of $N$.} is a homomorphism. Moreover, the data of $\sigma$, $N$ and $L$ determines the law in $G$ (provided the fact that the product $NL$ is seen as formal) because any element of $G$ can be written as $nl$; thus a product $GG$ is $(nl)(n'l')=(n\sigma_y(n'))(ll')$

\begin{proposition}
Let $N$ and $L$ be two Lie groups and $\dpt{\sigma}{L}{\Aut N}$ a homomorphism. With the law 
\[
   (x,y)(x',y')=(x\sigma_y(x'),yy'),
\]
the set $S=N\times L$ is a group.

\end{proposition}
\dref{19.14.3} 
\begin{proof}
    If $e$ is the neutral of $N$ and $e'$ the one of $L$, it is cleat that $(e,e')$ is the neutral of $S$. It is also easy to check that the inverse of $(x,y)$ is $(\sigma_{y^{-1}}(x^{-1}),y^{-1})$. The associativity is just a computation using $\sigma_y(ab)=\sigma_y(a)\circ\sigma_y(b)$ and $\sigma_x\circ\sigma_y=\sigma_{xy}$.
\end{proof}

The set $N\times L$ endowed with this inner product is denoted 
\[
   N\times_{\sigma}L.
\]

\begin{proposition}
If $G$ is the semidirect product of $N$ and $L$, then $G$ is isomorphic to $N\times_{\sigma} L$.
\end{proposition}

\begin{proof}
    The isomorphism is $\dpt{T}{N\times_{\sigma}L}{G}$, $T(x,y)=xy$. On the one hand, it is bijective because an element of $G$ can be written as $nl$ with $n\in N$ and $l\in L$ in only one way. On the other hand, it is easy to check that $T( (x,y)(x',u') )=T(x,y)T(x',y')$.
\end{proof}

One can now give the final definition. Let us consider two connected Lie groups $N$, $L$ and a Lie group  homomorphism $\dpt{\sigma}{L}{\Aut N}$. By \dref{19.13.5}, the map $N\times L\to N$, $(x,y)\to\sigma_y(x)$ is $\Cinf$. So, the group structure on $N\times L$ given by
\begin{equation}\label{eq:prod_semi_direct}
   (x,y)(x',y')=(x\sigma_y(x'),yy')
\end{equation}
is compatible with the $\Cinf$ structure of $N\times L$ (seen as a Lie group). The manifold $N\times L$ endowed with the group structure \eqref{eq:prod_semi_direct} is the \defe{semidirect product}{semi-direct product!of Lie groups} on $N$ and $L$; this is denoted by 
\[
   N\times_{\sigma}L.
\]

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Introduction by exact short sequence}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{General setting}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let $G_0$, $G_1$ and $G_2$ be tree connected Lie groups. A \defe{short exact sequence}{exact sequence!short} between them is two group homomorphisms
\begin{equation}
    \begin{aligned}[]
        \iota&\colon G_0\to G_1\\
        \pi&\colon G_1\to G_2
    \end{aligned}
\end{equation}
such that $\Image(\iota)=\Kernel(\pi)$. In that case, one says that $G_1$ is an \defe{extension}{extension of Lie groups} of $G_2$ by $G_0$.

Since the group $\iota(G_0)$ is the kernel of an homomorphism, it is normal and we write $\iota(G_0)\lhd G_1$\nomenclature[G]{$A\lhd B$}{$A$ is a normal subgroup of $B$}. Moreover, $\iota(G_0)=\pi^{-1}(e_2)$ and is then closed in $G_1$. As group, we have
\begin{equation}
    G_2=G_1/\iota(G_0).
\end{equation}

The extension is \defe{split}{extension of Lie groups!split} if there exists a Lie group homomorphism  $j\colon G_2\to G_1$ such that
\begin{equation}
    \pi\circ j=\id|_{G_2}.
\end{equation}
This condition imposes $j$ to be injective. In that case we have an action of $G_2$ on $G_0$ defined by
\begin{equation}
    \begin{aligned}
        R\colon G_2&\to \Aut(G_0) \\
        R_{g_2}(g_0)&=\iota^{-1}\Big( \AD\big( j(g_2) \big)\iota(g_0) \Big). 
    \end{aligned}
\end{equation}
Notice that $\AD\big(j(g_2)\big)\iota(g_0)$ belongs to $\iota(G_0)$ because the latter is normal.

As manifold we consider 
\begin{equation}
    G=G_0\times G_2
\end{equation}
and we define the multiplication law
\begin{equation}
    \begin{aligned}
        \cdot\colon G\times G&\to G \\
        (g_0,g_2)\cdot(g'_0,g'_2)&=\big( g_0 R_{g_2}(g'_0),g_2g'_2 \big). 
    \end{aligned}
\end{equation}
For associativity we have
\begin{equation}
    (g_0,g_2)\cdot\big( (g'_0,g'_2)\cdot (g''_0,g''_2)) \big)=\Big(  g_0R_{g_2}\big( g'_0R_{g'_2}(g''_0) \big),g_2g'_2g''_2  \Big)
\end{equation}
while
\begin{equation}
    \begin{aligned}[]
        \big( (g_0,g_2)\cdot(g'_0,g'_2) \big)\cdot(g''_0,g''_2)=\big( g_0R_{g_2}(g'_0)R_{g_2g'_2}(g''_0),(g'_2g''_2) \big).
    \end{aligned}
\end{equation}
Thus the product is associative if and only if 
\begin{equation}
    g_0R_{g_2}\big( g'_0R_{g'_2}(g''_0) \big)=\big( g_0R_{g_2}(g'_0) \big)R_{g_2g'_2}(g''_0).
\end{equation}
That equality is in fact true because $R$ is a morphism from $G_2$ to $\Aut(G_0)$, so that $R_{g_2}R_{g'_2}=R_{g_2g'_2}$. 

The neutral in $G$ is $(e_0,e_2)$.

Since $R_{g_2}(g_0)$ is smooth with respect to both variables, the product is smooth. In that way, $G$ becomes a Lie group named the \defe{semi direct product}{semi direct product} of $G_2$ by $G_0$ and is denoted by
\begin{equation}
    G_0 \rtimes_RG_2.
\end{equation}
All the construction is still valid when $R$ is an homomorphism which does not comes from a split extension.

We define the product $G_0\times G_2\to G$ by
\begin{equation}
    g_0\cdot g_2=(g_0,e_2)\cdot(e_0,g_2)
\end{equation}

The diagram
\begin{equation}
    \xymatrix{%
            &       G_1 \ar[dr]^{\pi}           &\\
            G_0 \ar[ur]^{\iota}\ar[rd]_{\id\times\{ e \}}   &       & G_2\\
            &       G \ar@{.>}[uu]^{\varphi}\ar[ru]_{\pr_2}     &\\
       }
\end{equation}
suggests us to define the map
\begin{equation}
    \begin{aligned}
        \varphi\colon G_0\times G_2&\to G_1 \\
        (g_0,g_2)&\mapsto \iota(g_0)j(g_2) 
    \end{aligned}
\end{equation}
This is a Lie group homomorphism because on the one hand
\begin{equation}
    \varphi(g_0,g_2)\cdot \varphi(g'_0,g'_2)=\iota(g_0)j(g_2)\cdot \iota(g'_0)j(g'_2),
\end{equation}
while on the other hand
\begin{equation}
    \begin{aligned}[]
        \varphi\big( (g_0,g_2)\cdot (g'_0,g'_2) \big)&=\varphi\big( g_0R_{g_2}(g'_0),g_2g'_2 \big)\\
        &=\varphi\Big( g_0\iota^{-1}\big( \AD(j(g_2))\iota(g'_0) \big),g_2g'_2 \Big)\\
        &=\iota\Big(g_0\iota^{-1}\Big( \AD(j(g_2))\iota(g'_0) \Big))j(g_2g'_2)\\
        &=\iota(g_0)j(g_2)\iota(g'_0)j(g'_2)
    \end{aligned}
\end{equation}
because $\iota$ and $j$ are homomorphisms.

The Leibnitz rule on $\iota(g_0)j(g_2)$ provides the differential
\begin{equation}
    (d\varphi)_e=(d\iota)_{e_0}\oplus(dj)_{e_2}.
\end{equation}
This is injective because $j$ is injective. The kernel of $\varphi$ is the set
\begin{equation}
    \Kernel(\varphi)=\{ (g_0,g_2)\tq \iota(g_0)=j(g_2)^{-1} \}.
\end{equation}
Since $\iota(G_0)$ and $j(G_2)$ have no intersections\footnote{They are transverse because $j\circ \pi=\id|_{G_2}$.} (a part the identity), we have that the kernel reduces to the identity:
\begin{equation}
    \Kernel(\varphi)=\{ e \}.
\end{equation}
The differentials provide the diagram
\begin{equation}
    \xymatrix{
    \mG_0\ar[r]^{(d\iota)_{e_0}}    &   \mG_1\ar[r]^{(d\pi)_{e_1}}  &   \mG_2\ar@<2pt>[l]^{(dj)_{e_2}}.
    }
\end{equation}
We have $(d\pi)_{e_1}\circ (dj)_{e_2}=\id|_{\mG_2}$ and the map
\begin{equation}
    (d\varphi)_e\colon \mG_1\to \mG_0\oplus\mG_2
\end{equation}
is an algebra homomorphism (as differential of group homomorphism). It is also an isomorphism by dimension counting. The inverse theorem then shows that $\varphi$ is a local diffeomorphism: $\varphi(G)$ contains a neighborhood of the identity and then is surjective by proposition \ref{PropUssGpGenere}.

We conclude that $\varphi$ is a Lie group isomorphism.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Example: extensions of the Heisenberg algebra}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let $\pH(V,\Omega)=V\oplus\eR E$ be the Heisenberg algebra. A derivation is a map $D\colon \pH \to \pH$ such that
\begin{equation}        \label{EqderCOndHeis}
    D[X,Y]=[DX,YT]+[X,DY].
\end{equation}
Let us look at the derivations under the form
\begin{equation}
    D=\begin{pmatrix}
        X   &   v   \\ 
        \xi &   a   
    \end{pmatrix}
\end{equation}
where $a\in\eR$, $X\in\End(V)$, $v\in V$ and $\xi\in V^*$. The left hand side of the condition \eqref{EqderCOndHeis} reads
\begin{equation}    \label{EqderCOndHeisA}
    D[w+zE,w'+z'E]=D\big( \Omega(w,w')E \big)=\Omega(w,w')(v+aE).
\end{equation}
Now, using $Dw=Xw+\xi(w)E$ and $D(zE)=v+aE$, the right hand side is
\begin{equation}    \label{EqderCOndHeisB}
    \big( \Omega(Xw,v')+\Omega(zv,v')+\Omega(w,Xw')+\Omega(w,z'v) \big)E.
\end{equation}
Equating \eqref{EqderCOndHeisA} and \eqref{EqderCOndHeisB} we find $v=0$ and 
\begin{equation}        \label{EqPourEtreDerHein}
    \Omega(Xw,w')+\Omega(w,Xw')=a\Omega(w,w').
\end{equation}
If we write it as matrices, we find
\begin{equation}
    X^t\Omega+\Omega X=a\Omega.
\end{equation}

The derivations with $a=0$ form the algebra
\begin{equation}
    \Der(\pH)_0=\gsp(\Omega,V)\times V^*.
\end{equation}

If $a\neq 0$, we find the symplectic conform group
\begin{equation}
    \Conf(V,\Omega)=\{ A\colon V\to V\tq \Omega(Av,Aw)=\lambda\Omega(v,w)\text{ with }\lambda\in\eR_0^+ \}.
\end{equation}
Taking the derivative of the group condition, we find
\begin{equation}
    \Dsdd{ \Omega\big( A(t)v,A(t)w \big) }{t}{0}=\Dsdd{ \lambda(t)\Omega(v,w) }{t}{0},
\end{equation}
which produces the condition \eqref{EqPourEtreDerHein} with $X=\dot A$ and $a=\dot \lambda$.

\begin{enumerate}

    \item
        If $X=\id$ and $\xi=0$, then we must have $a=2$ and we have the derivation
        \begin{equation}
            H=\id|_V\oplus 2\id|_{\eR E}.
        \end{equation}
    \item
        If $\xi=0$, $a=0$ and $X$ if exchange the Lagrangian in the decomposition $V=W\oplus \bar W$.
\end{enumerate}

\subsection{Group algebra}
%-------------------------

Let $\mA$ and $\mB$ be abelian algebras and $\dpt{\rho}{\mA}{\Der\mB}$ be a homomorphism. We want to put a group structure on the set $\mA\times\mB$ in such a way that the Lie algebra of $\mA\times\mB$ has Lie bracket given by
\begin{equation}
\big[ (A+B),(A'+B')  \big]=[A,B']+[B,A']=\rho(A)B'-\rho(A')B.
\end{equation}
We claim that the group law should be 
\begin{equation}
(a,b)(a',b')=(a+a',e^{\rho(a)}b'+b)
\end{equation}
whose inverse is 
\begin{equation}
(a,b)^{-1}=(-a,-e^{-\rho(a)b})
\end{equation}
Indeed, the general form of the commutator is 
\[ 
  [X,Y]=\DDsdd{ \AD(X(t))Y(s) }{t}{0}{s}{0}
\]
with respect to the group law. A path in $\mA\times\mB$ with tangent vector $(a,b)$ is $(at,bt)$. Then
\begin{equation}
\begin{split}
\big[ (a,b),(a',b') \big]&=\DDsdd{ (at,bt)(a's,b's)(at,bt)^{-1} }{t}{0}{s}{0}\\
            &=\big(0,-\rho(a)b+\rho(a)b'\big).
\end{split}
\end{equation}
