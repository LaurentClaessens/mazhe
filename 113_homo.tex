% This is part of (almost) Everything I know in mathematics
% Copyright (c) 2013-2014,2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{The group \texorpdfstring{$\SO(2,1)$}{SO21} and its algebra}   \label{subsec_IwSOdu}
%-----------------------------------------------------------
\index{Iwasawa decomposition!of $\SO(2,1)$}

The condition for a matrix $A$ to belong to $\mathfrak{so}(2,1)$ is $A^tgA=g$ with $g=diag(1,1,-1)$. Hence considering the Cartan involution\label{pg:Cartan_SO} $\theta(X)=-X^t$, we find
\begin{equation}  \label{EqMtrsDansSOdu}
\lG=\mathfrak{so}(2,1)\leadsto\begin{pmatrix}
0&a&u_1\\-a&0&u_2\\u_1&u_2&0
\end{pmatrix},\quad
\lK\leadsto\begin{pmatrix}
0&1\\-1&0\\&&0
\end{pmatrix},
\quad
\lP\leadsto\begin{pmatrix}
&&u_1\\
&&u_2\\
u_1&u_2&0
\end{pmatrix}.
\end{equation}
As generator of $\lA$, we choose
\[ 
  J=\begin{pmatrix}
&0\\
0&0&1\\
&1
\end{pmatrix}.
\]
The condition $[J,X]=X$ which defines $\sN$ is satisfied by
\[ 
  L=\begin{pmatrix}
0&1&1\\-1\\1
\end{pmatrix}.
\]
If we denote by $X$ the general matrix of $\mathfrak{so}(2,1)$ given in equation \eqref{EqMtrsDansSOdu}, we have
\[ 
  \ad(J)X=
\begin{pmatrix}
0&-u_{1}&-a\\
u_{1}&0&0\\
-a&0&0
\end{pmatrix}
\]
which leads to the following root spaces\index{root!space!in $\mathfrak{so}(2,1)$}
\begin{equation}
S_{0}=
\begin{pmatrix}
0&0&0\\
0&0&1\\
0&1&0
\end{pmatrix},\quad
S_{1}=
\begin{pmatrix}
0&1&-1\\
-1&0&1\\
-1&1&0
\end{pmatrix},\quad
S_{-1}=
\begin{pmatrix}
0&1&1\\
-1&0&1\\
1&1&0
\end{pmatrix}
\end{equation}
with the relations
\begin{subequations}
\begin{align}
[S_{0},S_{1}]&=S_{1}\\
[S_{0},S_{-1}]&=-S_{-1}\\
[S_{1},S_{-1}]&=-2S_{0}.
\end{align}
\end{subequations}
Notice that the comparison with \eqref{subeq_rootSLR} shows that $SL(2,\eR)$ and $\SO(2,1)$ \emph{are not} isomorphic.

\subsection{Root spaces for \texorpdfstring{$\so(2,1)$}{so21}}
%--------------------------------------------------------------

\begin{proposition}
	The matrices of $\SO(p,q)$ satisfy the relation
	\[ 
		A^{-1}=\eta A^t\eta
	\]
	where $\eta$ is the diagonal matrix with $p$ times $1$ and $q$ times $-1$. That relation is often taken as the definition of $\SO(p,q)$.
\end{proposition}


The algebra $\so(2,1)$ is made up from $3\times 3$ matrices such that $X^t\eta+\eta X=0$ with vanishing trace. If we choice $\eta=diag(-,-,+)$, we find matrices of the form
\[ 
  \so(2,1)\leadsto\begin{pmatrix}
a	&u^t\\
u	&0
\end{pmatrix}
\]
where $a$ is an antisymmetric $2\times 2$ matrix and $u$ is any $1\times 2$ matrix. We find the Cartan decomposition
\[ 
  \iK\leadsto\begin{pmatrix}
\so(2)\\&0
\end{pmatrix},\qquad
\iP\leadsto\begin{pmatrix}
0&0&u_1\\
0&0&u_2\\
u_1&u_2&0
\end{pmatrix}.
\]
If one chooses
\[ 
  J=\begin{pmatrix}
0&0&1\\0\\1
\end{pmatrix}
\]
as generator for the abelian subalgebra of $\iP$, one finds
\[ 
	V_1=
	\begin{pmatrix}
		0&1&0\\
		-1&0&1\\
		0&1&0
	\end{pmatrix},\qquad
	V_{-1}=
	\begin{pmatrix}
		0&1&0\\
		-1&0&-1\\
		0&-1&0
	\end{pmatrix}
\]
as eigenvectors for $\ad(J)$ with eigenvalues $1$ and $-1$. The root space decomposition commutator table of $\so(2,1)$ is thus given by
\begin{subequations}
	\begin{align}
		 [V_0,V_1]&=V_1\\
		[V_0,V_{-1}]&=-V_{-1}\\
		[V_1,V_{-1}]&=-2V_0.
	\end{align}
\end{subequations}
Notice that the map $\phi(A_0)=2V_0$, $\phi(A_2)=V_1$, $\phi(A_{-2})=-V_{-1}$ provides an isomorphism between this table and the one of $\gsl(2,\eR)$, equations \eqref{subeq_rootSLR}. This fact assures a Lie algebra isomorphism $\gsl(2,\eR)\simeq\so(2,1)$. We actually have a stronger result: 

\begin{proposition}
The group $\SL(2,\eR)$ is a double-covering of the identity component $\SO_0(1,2)$.
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Position of \texorpdfstring{$ \SO(1,n)$}{SO(1,n)} inside \texorpdfstring{$ \SO(2,n)$}{SO(2,n)}}
%---------------------------------------------------------------------------------------------------------------------------

The quotient $\SO(2,n)/\SO(1,n)$ has a particular importance in section \ref{SecSymeStructAdS}. Hence, we will work out the Iwasawa decompositions of these groups imposing certain compatibility conditions. We already build the Cartan involution $\theta$ in such a way that $[\sigma,\theta]=0$. In this section, \( \sH\) is \( \SO(1,n)\) and we are searching for an Iwasawa decomposition \( \sA_{\sH}\oplus\sN_{\sH}\oplus\sK_{\sH}\). 

As we want compatibility between the Cartan decomposition of \( \SO(1,n)\), the Cartan decomposition of \( \SO(2,n)\) and the reductive decomposition of \( \SO(2,n)\) we start by giving the ``position'' of \( \SO(1,n)\) inside \( \SO(2,n)\). The in subsection \ref{SubSecIwaSOunn} we will proceed with the Iwasawa decomposition of \( \SO(1,n)\).



\begin{lemma} \label{lem:SO_pq_ss}
The Lie group $\SO(p,q)$ is semisimple.\nomenclature[G]{$\SO(p,q)$}{Isometry group of $\eR^{p,q}$}
\end{lemma}

\begin{proof}[Sketch of proof]
We are going to give some ideas of the proof. Remark that $\SO(p,q)$ essentially contains ``rotations''\ $J_{ij}(\lambda)$ of angle $\lambda$ in planes $(i,j)$. (If the coordinates $i$ and $j$ are not of the same type\footnote{i.e. if the metric in the plane $(i,j)$ has signature $(+,-)$.}, then it is not a true rotation but a boost and $\lambda$ is no more an angle but any real.) If one try to build an ideal $\mI$ containing $J_{ij}$, then the element $J_{kl}J_{ij}$ must also be in $\mI$. If we want $\mI$ to be abelian, we must have $[J_{ij},J_{ij}J_{kl}]$, or
\[
  J_{kl}J_{ij}=J_{ij}J_{kl}.
\]
It is really easy to find a $J_{kl}$ for which it is false. Thus $\SO(p,q)$ doesn't contain any abelian ideal, so that it is semisimple by lemma \ref{lem:ss_ideal}.
\end{proof}

\begin{probleme}
J'ai l'impression que c'est la simplicité de $\SO(p,q)$ que je démontre. À vérifier.
\end{probleme}

The Lie algebra $\sG=\mathfrak{so}(2,n)$ is the set 
\begin{equation}\label{def_sodn}
\big\{X\in M_{(2+n)\times (2+n)}\textrm{ such that }X^t\eta+\eta X=0\textrm{ and } \tr X=0\big\}
\end{equation}
where $\eta$ is the diagonal metric $\eta=diag(-,-,+,\ldots,+)$. An element of $\mathfrak{so}(2,n)$ can be written as
$
    X=\begin{pmatrix}
a & u ^t \\
v & B
\end{pmatrix}
$
with the matrices $a\in M_{2\times 2}$, $u\in M_{n\times 2}$, $v\in M_{n\times 2}$, and $B\in M_{n\times n}$. The conditions in \eqref{def_sodn} give: $a=-a^t$, $u=v$, and $B=-B^t$. Hence, a general matrix of $\sodn$ is given by
\begin{equation}	\label{eq:gene_sodn}
X=\begin{pmatrix}
a & u^t \\
u & B
\end{pmatrix}
\end{equation}
where $a,B$ are skew-symmetric.

A compatible reductive decomposition is build as follows. The matrices of $\so(1,n)$ have to be seen as matrices of $\so(2,n)$ with the condition $Y^t\sigma+\sigma Y=0$ for the  ``metric''\ $\sigma=diag(0,-,+,\ldots,+)$. Hence,
\begin{equation}		\label{eq:gene_H}
\sH=\soun\leadsto
  \begin{pmatrix}
     \begin{matrix}
       0&0\\
       0&0
     \end{matrix}
                       &  \begin{pmatrix}
		             \cdots 0\cdots\\
			    \leftarrow v^t\rightarrow
                          \end{pmatrix}\\
    \begin{pmatrix}	  
       \vdots & \uparrow\\
         0    & v \\
       \vdots & \downarrow
    \end{pmatrix} &  B
  \end{pmatrix}
\end{equation}
where  $v\in M_{n\times 1}$ and $B\in M_{n\times n}$ is skew-symmetric. Comparing this with the general form \eqref{eq:gene_sodn} of a matrix of $\sodn$ matrix, one immediately finds that, with the choice
\begin{equation}\label{EqGeneRedQ}
\sQ\leadsto
 \begin{pmatrix}
     \begin{matrix}
       0&a\\
       -a&0
     \end{matrix}
                       &  \begin{pmatrix}		             
			  \leftarrow w^t\rightarrow \\
			     \cdots 0\cdots\\
                          \end{pmatrix}\\
    \begin{pmatrix}	  
      \uparrow   & \vdots\\
          w      &  0\\
      \downarrow & \vdots 
    \end{pmatrix} & 0
  \end{pmatrix},
 \end{equation}
the decomposition $\sG=\sH\oplus\sQ$ is reductive:
\begin{align}		\label{EqDefRedHQ}
  [\sH,\sQ]&\subseteq\sQ,
 &[\sQ,\sQ]&\subseteq\sH,
\end{align}
and $B(\sH,\sQ)=0$. In the sequel, we will use the basis of $\sQ$ defined by 
\begin{align}		\label{EqDefBaseqi}
  q_0&=E_{12}-E_{21}, &q_i&=E_{1,(i-2)}+E_{(i-2),1}.
\end{align}
Notice, for later use that $q_1=J_2$ in the Iwasawa decomposition of $\SO(2,n)$.
%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Iwasawa decomposition for \texorpdfstring{$\SO(1,n)$}{SO1n}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecIwaSOunn}
\index{Iwasawa!decomposition!of $\SO(1,n)$}

We want our Cartan involution $\dpt{\theta}{\sG}{\sG}$, $\theta(X)=-X^t$ and to be such that $[\theta,\sigma]=0$ with $\dpt{\sigma}{\sG}{\sG}$ given by $\sigma=id|_{\sH}\oplus(-id)|_{\sQ}$, see theorem \ref{tho:sigma_theta}.\label{pg:calcul_sigma_theta}

Consider a general matrix $h$ in $\sH$ (from \eqref{eq:gene_H}); we have
\[
  (\sigma\theta)(h)=-\sigma
  \begin{pmatrix}
  0&0&&0\\
  0&0&&v^t\\
  0&v&&-B
  \end{pmatrix}=
  \begin{pmatrix}
  0&0&&0\\
  0&0&&-v^t\\
  0&-v&&B
  \end{pmatrix},
\]
while
\[
  \theta\sigma(h)=\theta(h)=-
  \begin{pmatrix}
  0&0&&0\\
  0&0&&v^t\\
  0&v&&-B
  \end{pmatrix},
\] 
then $[\sigma,\theta]|_{\sH}=0$. The same computation with a matrix in $\sQ$ (see \eqref{EqGeneRedQ}) gives $[\sigma,\theta]|_{\sQ}=0$.

Now we show that $\theta$ descent to a Cartan involution on $H$. It is clear that the restriction of $\theta$ is an involutive automorphism of $H$. Lemma \ref{lem:Killing_ss_descent} assures us that the restriction of the Killing form of $G$ to $H$ is the Killing form of $H$, so that the condition of positivity of $B_{\theta}$ holds on $H$ as well as on $G$. Last, $\theta$ leaves $\sH$ invariant. Indeed suppose that  $\theta X_{\sH}=X'_{\sH}+X_{\sQ}\in\sH\oplus\sQ$. Then $\sigma\theta X_{\sH}=h'-q$ and $\theta\sigma h=h'+q$; since $[\theta,\sigma]=0$, we have $q=0$.

The whole proves that we can use the same Cartan involution on $G$ as well as on $H$. Since $\theta=id|_{\iK}\oplus(-id)|_{\iP}$, it is clear that 
\begin{align}
   \iKH&=\iK\cap\sH,
   &\iPH&=\iP\cap\sH
\end{align}
is the Cartan decomposition of $\sH$. We can write explicit matrices as
\begin{equation}
\iKH=\so(n)\leadsto
\begin{pmatrix}
  0&0&\cdots  \\
  0&0&\cdots  \\
  \vdots&\vdots& B
\end{pmatrix},
\end{equation}
where $B$ is skew-symmetric, and
\begin{equation}
\iPH\leadsto
\begin{pmatrix}
  0&0&\cdots  \\
  0&0& u^t  \\
  \vdots&u& 0
\end{pmatrix}
\end{equation}
where $u\in\eM_{n\times 1}(\eR)$ is a line. One remark that there are no two-dimensional subalgebra of $\iP_{\sH}$. So $\iA_{\sH}$ reduces to the choice of any element $J_1\in\iP_{\sH}$.  A positivity notion is easy to find: the form $\omega\in\iAH^*$ such that $\omega(J_1)=1$ is positive while $-\omega$ is negative. We choose
\[
  J_1=\begin{pmatrix}&0\\0&0&0&1\\&0\\&1\end{pmatrix}.
\]
The computation of $\iNH=\{X\in\sH\tq (\ad J_1)X=X\}$ yields the following :
\begin{equation}\label{eq:re_N_H}
\iNH\leadsto
\begin{pmatrix}
     \begin{pmatrix} &&0&0\\&&a&0\\0&a&0&-a\\0&0&a&0\end{pmatrix}
     & \begin{pmatrix}\cdots&0&\cdots\\\leftarrow&\overline{v}&\rightarrow\\
        \cdots&0&\cdots\\\leftarrow&\overline{v}&\rightarrow\end{pmatrix}\\
     \begin{pmatrix}\vdots&\uparrow&\vdots&\uparrow\\
                    0& \overline{v}&0&-\overline{v}\\
		    \vdots&\downarrow& \vdots&\downarrow \end{pmatrix}
     &0		    
\end{pmatrix}.
\end{equation}
Finally, we consider the algebra $\iRH=\iAH\oplus\iNH$.

\section{Iwasawa decomposition for \texorpdfstring{$\SOdn$}{SO2n}} \label{subsecIwasawa_un}
%-------------------------------------------------
\index{Iwasawa decomposition!of $\SO(2,n)$}

As seen in the general construction and in previous examples, the Iwasawa decomposition of a group or an algebra depends on several choices. We will study two out of them in the case of $\SO(2,n)$ and see that some ``compatibility conditions'' with the decomposition of $\SO(1,n)$ and the symmetric space structure of $AdS$ (see section \ref{SecSymeStructAdS}) fix most of choices. When the two decompositions will be used together, the unadapted one will be tilded (see subsection \ref{subsec:precision}).

\subsection{Cartan decomposition and compatible reductive decomposition}		\label{SubSecCartandeuxN}
%//////////////////////////////

The Cartan decomposition of $\so(2,n)$ associated with the Cartan involution $\theta(X)=-X^t$ is
\begin{align}\label{K_et_P}
   \iK&\leadsto
\begin{pmatrix}
\mathfrak{so}(2) \\
 & \mathfrak{so}(n)
\end{pmatrix},
&\iP&\leadsto\begin{pmatrix}
0 & u^t \\
u & 0
\end{pmatrix}.
\end{align}
 Elements of $\SO(2)$ are represented by
\[ 
  \begin{pmatrix}
\cos\mu&\sin\mu\\
-\sin\mu&\cos\mu
\end{pmatrix}.
\]
A common abuse of notation in the text will be to identify the angle $\mu$ with the element of $\SO(2)$ itself. In the same spirit, when we speak about a matrix of $A\in \SO(2)$, we mean a matrix whose upper left corner is $A$ and the rest is the unit matrix. For example, for $AdS_3$, the matrix $-\mtu\in \SO(2)$ is
\[
\begin{pmatrix}
\begin{matrix}
-1&0\\
0&-1
\end{matrix}\\
&1\\
&&1
\end{pmatrix}.
\]
That matrix will be denoted\footnote{See also proposition \ref{LONGPropCartanExtExpo}.} by $k_{\theta}$ and has the property that
\begin{equation}
	\theta=\AD(k_{\theta}).
\end{equation}
Indeed, we have $k_{\theta}=\mtu$, so that $\AD(k_{\theta})^2=\id$. Since $k_{\theta}$ commutes with every element of $K$, $\AD(k_{\theta})|_{K}=\id$, and a simple computation shows that $\Ad(k_{\theta})|_{\sP}=-\id$.

Remark that $\iK$, the compact part of $\mG$ is made up from ``true'' rotations while $\iP$ contains boosts.  This remark allows us to guess a right choice of maximal abelian subalgebra in $\iP$. Indeed elements of $\iA$ must be boosts and the fact that there are only two time-like directions restricts $\sA$ to a two dimensional algebra. Up to reparametrization, it is thus generated by $t\partial_x+x\partial_t$ and $u\partial_y+y\partial_t$. Hence the following choice seems to be logical:
\begin{align}   \label{EqDevineA}
   J_1&=
\begin{pmatrix}
&0\\
0&0&0&1\\
&0\\
&1
\end{pmatrix}\in\sH,
&J_2&=q_1=
\begin{pmatrix}
0&0&1&0\\
0\\
1\\
0
\end{pmatrix}\in\sQ.
\end{align}

\subsection{Maximal abelian subalgebra}
%///////////////////////////////////////

We have to find an abelian subalgebra $\iA\subset\iP$. If we use the convention that the indices $a$, $b$ range from $1$ to $2$ and $i$, $j$ from $3$ to $n+2$, we can write
\begin{equation}
    u=\begin{pmatrix}
0 & u^t \\
u & 0
\end{pmatrix}=u^{ai}(E_{ai}+E_{ia}),
\end{equation}
where $E_{ij}$ denotes the matrix whose only non zero component is a $1$ in the place $ij$. Of course we use an abuse of notation between the $(n+2)\times(n+2)$ matrix of $\iP$ and $n\times 2$ matrix $u$ which defines it. We compute the commutator of two such matrices:
\begin{equation}
\begin{split}
[u,v]=u^{ai}v^{bj}(&\underline{\delta_{ib}E_{aj}}-\underline{\delta_{aj}E_{bi}}+\delE{ij}{ab}-\delE{ab}{ji}\\
&+ \delE{ab}{ij}-\delE{ij}{ba}+\underline{\delE{aj}{ib}}-\underline{\delE{ib}{ja}})
\end{split}
\end{equation}
Since $[\iP,\iP]\subseteq\iK$, the latter commutator takes the form $\begin{pmatrix}
a & 0 \\
0 & B
\end{pmatrix}$, thus the underlined terms must vanish. So we are left with:
\begin{eqnarray}
[u,v]&=&u^{ai}v^{bj}(\delta_{ij}(E_{ab}-E_{ba})+\delta_{ab}(E_{ij}-E_{ji}))\\
     &=&u^{ai}v^{bj} (E_{ab}-E_{ba})+u^{ai}v^{aj} (E_{ij}-E_{ji}).
\end{eqnarray}
Finally, $[u,v]=0$ if and only if 
\begin{equation} \label{EqCondSOdnsumia}
\sum_iu^{ai}v^{bi}=0\quad\text{and}\quad \sum_au^{ai}v^{aj}=0
\end{equation}
 with $a\neq b$, and $i\neq j$.

In order to build a maximal abelian subalgebra $\iA$ of $\iP$, we take one element in $\iP$ and then we extend it using the relations \eqref{EqCondSOdnsumia}. We choose $u^{ai}=\delE{a1}{i3}$.

If this $u$ lies in $\iA$, an other $v$ in $\iA$ must satisfy $v\in\iP$, $v^{23}=0$ and $v^{1j}=0$ for $j>3$. 
We choose the one with $u^{24}=1$. It is no difficult to see that these two are maximal. A basis of $\iA$ is thus $\tilde{H}_1\equiv u^{24}=1$ and
 $\tilde{H}_2\equiv u^{13}=1$. We immediately perform a change of basis by defining:
\begin{subequations}
\begin{align}
  H_1&=\tilde{H}_1-\tilde{H}_2\\
  H_2&=\tilde{H}_1+\tilde{H}_2,
\end{align}
\end{subequations}
and finally: $\iA=span\{H_1,H_2\}$. The matrices are given by
\begin{equation}
H_p=\left(\begin{array}{c|c}
N_p&0\\
\hline
0&0
\end{array}\right)
\end{equation}
with
\begin{equation}
N_p=\begin{pmatrix}
 & \begin{array}{cc}1&0\\0&(-1)^p\end{array} \\
\begin{array}{cc}1&0\\0&(-1)^p\end{array} & 
\end{pmatrix}.
\end{equation}
Up to a change of basis, they are exactly the ones guessed in equation \eqref{EqDevineA}. That matrix can be written under the more compact form

The generators of $\iA$ that we choose are the following linear combination of $J_1$ and $J_2$:
\begin{equation}\label{mtr_A}
       H_p=E_{13}+E_{31}+(-1)^p(E_{24}+E_{42}).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Compatibility conditions between \texorpdfstring{$\sH$}{H} and \texorpdfstring{$\sG$}{G}}
%---------------------------------------------------------------------------------------------------------------------------

The decomposition of $\SO(1,n)$ of section \ref{SubSecIwaSOunn} incites us to search for a decomposition of $\SOdn$ which satisfies certain compatibility conditions. If you want some explicit matrix, read subsection \ref{subsecIwasawa_deux} and \ref{SubSecANbarIwa}.

We go back to the choice of $\iA$. Remark that $\iP_{\sH}\subset\sP$. This suggests us to try to get $\iA_{\sH}\subset\sA$. So we will choose $J$ as first matrix in $\iA$, and then extend it to a maximal abelian subalgebra of $\iP$. Note in particular that this don't affect the choice of the Cartan involution. Now we consider $\Phi$ and $\Psi$, the restricted roots for $G$ and $H$; for recall:
\begin{equation}
\begin{split}
  \sH_{\alpha}  &=\{ X\in\sH\tq \forall H'\in\iAH,(\ad H')X=\alpha(H')X \},\\
  \sG_{\lambda} &=\{ X\in\sG\tq \forall H\in\iA,(\ad H)X=\lambda(H)X \},\\
  \Phi        &=\{ \lambda\in\iA^*\tq \lambda\neq 0, \sG_{\lambda}\neq 0 \},\\
  \Psi        &=\{ \lambda\in\sA_{\sH}^*\tq \lambda\neq 0, \sH_{\lambda}\neq 0\}.
\end{split}
\end{equation}

\begin{lemma}
The root spaces of $G$ and $H$ are related by
\[
    \Phi|_{\iAH}=\Psi,
\]
in other words, elements of $\Psi$ are the restriction to $\iAH$ of the ones of $\Phi$
\end{lemma}

\begin{proof}
Let us consider a $\alpha\in\Phi$: there exists a non-zero subspace $\sG_{\alpha}$ of $\sG$ such that for any $H\in\iA$, $[H,X]=\alpha(H)X$. Clearly, $\alpha$ also fulfils $[H',X]=\alpha(H')X=\alpha|_{\iAH}(H')X$ for any $H'\in\iAH\subset\iA$. Then $\alpha|_{\sA_{\sH}}\in\Psi$.

In order to prove the inverse sense, consider $\beta\in\Psi$ and $X\in\sH_{\beta}$. We have $[H',X]=\beta(H')X$ for every $H'\in\sA_{\sH}$. Let us consider $H\in\iA$: 
\begin{equation}
  [H',[H,X]]=[H,[H',X]]
            =\beta(H')[H,X].
\end{equation}
Thus for any $H\in\iA$ and $X\in\sH_{\beta}$, $[H,X]\in\sH_{\beta}$. In other words,
\[  
   [\iA,\sH_{\beta}]\subset\sH_{\beta}.
\]
Now, $\ad\iA$ is a set of semisimple\quext{Il faudra encore voir ça\ldots} commuting operators on $\sH_{\beta}$; hence there exists a basis of $\sH_{\beta}$ of common eigenvectors. Thus we can write a decomposition of $\sH_{\beta}$ under the form
\begin{equation}\label{eq:decomp_sH}
 \sH_{\beta}=\bigoplus_{\alpha\in\Phi}\sH_{\beta}^{\alpha},
\end{equation}
with $\sH_{\beta}^{\alpha}=\sH_{\beta}\cap\sG_{\alpha}$. In order to see this last point, consider an eigenvector $h_{\beta}\in\sH_{\beta}$ of all the elements of $\ad\sA$: there exists $\alpha\in\sA^*$ such that $\forall A\in\sA$, $[A,h_{\beta}]=\alpha(A)h_{\beta}$. In other words, $h_{\beta}\in\sG_{\alpha}$.

This decomposition allows us to write $X\in\sH_{\beta}$ as
\[
    X=\bigoplus_{\alpha\in\Phi}X^{\alpha}.
\]
On $X\in\sH_{\beta}^{\alpha}$, $\ad A$ acts as $(\ad A)X=\alpha(A)X$.  So, for $X\in\sH_{\beta}$, $H'\in\iAH$,
\[
  [H',X]=\beta(H')X=\bigoplus_{\alpha\in\Phi}\alpha(H')X^{\alpha}.
\]
Then $\alpha(H')=\beta(H')$, i.e. $\alpha|_{\iAH}=\beta$.

\end{proof}

Now, we choose a clever positivity notion on $\Phi$. The one of $\Psi$ is clear; we extend it to $\Phi$, so that $\Psi^+\subset\Phi^+$ and $\iNH\subset\iN$. Finally for our new Iwasawa decomposition of $\SOdn$,
\[
   \iRH\subset\iR.
\]

In the decomposition \eqref{eq:decomp_sH}, it is possible that, for certain $\alpha$ and $\beta$, the set $\sH_{\beta}^{\alpha}$ reduces to $\{0\}$. We define
\begin{equation}
  \phi(\beta)=\{ \alpha\in\Phi\tq \sH_{\beta}^{\alpha}\neq\{0\} \};
\end{equation}
it defines a map $\phi$ from $\Psi$ to the parts of $\Phi$. Moreover, $\phi(\Psi)$ gives a partition of $\Phi$ because $\phi(\beta)\cap\phi(\beta')=\emptyset$ if $\beta\neq\beta'$. Indeed, $\phi(\beta)\cap\phi(\beta')$ is the set of all the $\alpha\in\Phi$ such that $\sH_{\beta}\cap\sG_{\alpha}\neq\{0\}$ and $\sH_{\beta'}\cap\sG_{\alpha}\neq\{0\}$. But for $\alpha\in\phi(\beta)$, $\alpha_{\iAH}=\beta$, then $\alpha(H')=\beta(H')$; for the same reason, $\alpha(H')=\beta'(H')$, so that $\beta=\beta'$.

We know the $\pm 1$ eigenspaces decompositions $\sG\stackrel{\sigma}{=}\sH\oplus\sQ\stackrel{\theta}{=}\sK\oplus\sP$ with $[\sigma,\theta]=0$, and the Iwasawa decomposition $\sA_{\sH}\oplus\sN_{\sH}\oplus\sK_{\sH}$ of $\sH$. 

For compatibility and simplicity purposes, we want the Iwasawa decompositions $\sG=\sA\oplus\sN\oplus\sK$ of $\sG$ in such a way that $\sA_{\sH}\subset\sA$ and $\sN_{\sH}\subset\sN$.
We denote by $A$, $N$, $K$, $A_H$, $N_H$ and $K_H$ the analytic connected subgroups of $G$ whose Lie algebras are $\sA$, $\sN$, $\sK$, $\sA_{\sH}$, $\sN_{\sH}$, and $\sK_{\sH}$ respectively. 


 For the $\sA$-part, we just perform a change of basis
\begin{subequations}
\begin{align}
J_{1}&=\frac{ 1 }{2}(H_{2}-H_{1})\\
J_{2}&=\frac{ 1 }{2}(H_{1}+H_{2}) 
\end{align}
\end{subequations}
in order to have $J_{1}\in\sP\cap\sH$ and $J_{2}\in\sP\cap\sQ$. The involution $\sigma$ has a simpler expression in this basis.

In order to get $\sA\subset\sA_{\sH}$, we take the element of $\sA_{\sH}$ as first basis element of $\sA$:
\begin{equation}		\label{EqGeueuleJun}
   J_1=
\begin{pmatrix}
&0\\
0&0&0&1\\
&0\\
&1
\end{pmatrix}.
\end{equation}
One can check from from \eqref{EqGeneRedQ} and \eqref{K_et_P} that $J_{1}$ belongs to $\sP\cap\sH$. Now we want to extend it to a maximal abelian subalgebra of $\sP$. In order to build an Iwasawa decomposition of $\SOdn$ compatible with $\sQ$, we search a $J_2\in\sP\cap\sQ$. We know that
\[
  \sP\cap\sQ\leadsto
\begin{pmatrix}
0&0&u^t\\
0&0\\
u
\end{pmatrix},
\]
so that the matrix
\begin{equation}		\label{EqgueueleJdeux}
J_2=
\begin{pmatrix}
0&0&1&0\\
0\\
1\\
0
\end{pmatrix}
\end{equation}
belongs to $\sP\cap\sQ$ and commutes with $J_1$. 

\begin{remark}
The new $\sA$ for $\SOdn$ is the abelian Lie algebra spanned by $J_1$ and $J_2$. This is just a change of basis in $\sA$: $J_1=\frac{1}{2}(H_2-H_1)$, $J_2=\frac{1}{2}(H_1+H_2)$. 
\end{remark}
Note that $J_1\in\sH$ and $J_2\in\sQ$; then the expression of $\sigma$ on $\sA$ should be rather simple. Let us first show the action of $\sigma$ on the root spaces.

If $\alpha\in\sA^*$, the notation $\sigma^*\alpha$ means $f\circ\alpha$, and $\sG_{\alpha}$ denotes the root space associated with the form $\alpha$.

\begin{lemma}			\label{LemSigmaThetaRootSpaces}
	The involutions $\sigma$ and $\theta$ act on the root spaces by 
	\begin{align}		\label{eq:sig_G_alpha}
		\sigma\sG_{\varphi}&=\sG_{\sigma^*\varphi},
		&\theta\sG_{\varphi}&=\sG_{-\varphi}
	\end{align}
	for all $\varphi\in\sA^*$,
\end{lemma}

\begin{proof}
Let $H\in \sA$ and $X\in\sG_{\varphi}$; by definition: $[H,X]=\varphi(H)X$. We have
\[ 
  \varphi(H)\sigma X=\sigma[H,X]=[\sigma H,\sigma X]=\varphi(\sigma H)\sigma X.
\]
We conclude that $\sigma X\in\sG_{\sigma^*\varphi}$. For the second equality, we take $X\in\sG_{\varphi}$ and 
\[ 
  [H,\theta X]=\theta[\theta H,X]=\theta\big( \varphi(\theta H)X \big)=-\varphi(H)\theta(X).
\]
\end{proof}

\begin{lemma}		\label{LemSigmaChangeDeux}
The involution $\sigma$ changes the sign of the $J_{2}^*$-part of the root spaces:
\[
	\sigma\sG_{(x,y)}=\sG_{(x,-y)}
\]
where $(x,y)$ denote the coordinates of a root in the basis $\{ J_1^*,J_2^* \}$ of $\sA$.
\end{lemma}

\begin{proof}
Since $\sigma$ is an involutive automorphism, it satisfies $[\sigma X,Y]=\sigma[X,\sigma Y]$. So when $X\in\sG_{(x,y)}$, we have $(\ad(J_{1}))(\sigma X)=x\sigma X$ and $\ad(J_{2})(\sigma X)=-y\sigma X$.

\end{proof}

It is also clear that $\sigma^*\alpha=0$ implies $\alpha(J_2)=0$. Indeed, $(\sigma^*\alpha)(J_2)=\alpha(-J_2) =-\alpha(J_2)$, because $J_{2}\in\sQ$.
Now we can give a precision about the decomposition $\sH_{\beta}=\oplus_{\alpha\in\Phi'}\sH_{\beta}^{\alpha}$ given by \eqref{eq:decomp_sH}. In fact, $\Phi'=\Phi_{\sigma}=\{\alpha\tq \sigma^*\alpha=\alpha\}$. Indeed $\sigma$ fixes $\sH$ and thus fixes $\sH_{\beta}^{\alpha}$. If $h_{\beta}^{\alpha}\in\sH_{\beta}^{\alpha}$, it satisfies $\sigma h_{\beta}^{\alpha}=h_{\beta}^{\alpha}$. But from equation \eqref{eq:sig_G_alpha}, $\sigma h_{\beta}^{\alpha}\in \sG_{\sigma^*\alpha}$.

\begin{proposition}
	The spaces $\sP$ and $\sK$ are Killing-orthogonal. The spaces $\sH$ and $\sQ$ are Killing-orthogonal.
\end{proposition}

\begin{proof}
	Let consider $X\in\sH$ and $Y\in\sQ$. Since $\sigma$ is an automorphism of the algebra $\sG$, the Killing form is $\sigma$-invariant and we have
	\begin{equation}
		B(X,Y)=B(\sigma X,\sigma Y)=B(X,-Y)=-B(X,Y),
	\end{equation}
	which proves that $B(X,Y)=0$. Exactly the same holds for $X\in\sK$ and $Y\in\sP$, using $\theta$ instead of $\sigma$.
\end{proof}

\subsection{First choice: \texorpdfstring{$AN$}{AN}}\label{subsecIwasawa_deux}
%---------------------------------------------------------

We turn now our attention to the Iwasawa decomposition. As before we are searching for matrices under the form $E=\begin{pmatrix}A&B\\C&D\end{pmatrix}$ with dimensions $4+(n-2)$. The condition which determines the root spaces reads
\begin{equation}
  (\ad J_i)E=
\begin{pmatrix}
  [j_i,A]&j_iB\\
  -Cj_i&0
\end{pmatrix}
\stackrel{!}{=}\lambda_i
\begin{pmatrix}A&B\\C&D\end{pmatrix}.
\end{equation}

% !! Si tu modifies des trucs ici, tu dois aussi le faire dans l'appendice sur les choses explicites. 

The computations are rather the same as the ones of the first time. The result is\label{pg:root_n}
\begin{equation}
\sG_{(0,0)}\leadsto
\begin{pmatrix}
&&x&0\\
&&0&y\\
x&0\\
0&y\\
&&&& D
\end{pmatrix},
\end{equation}
where $D\in M_{(n-2)\times(n-2)}$ is skew-symmetric. Notice that all but the $D$-part of this space is spanned by $q_1$ and $J_1$, so the $\sQ$-component of that matrix is a multiple of $q_1$. Other root spaces are given by
\begin{subequations}
\begin{align}
\sG_{(1,0)}&\leadsto W_i=E_{2i}+E_{4i}+E_{i2}-E_{i4}\in\sH,\\
\sG_{(-1,0)}&\leadsto Y_i=-E_{2i}+E_{4i}-E_{i2}-E_{i4},\\
\sG_{(0,1)}&\leadsto V_i=E_{1i}+E_{3i}+E_{i1}-E_{i3},\\
\sG_{(0,-1)}&\leadsto X_i=-E_{1i}+E_{3i}-E_{i1}-E_{i3}
\end{align}
\end{subequations}
with\footnote{Let us remember that we are dealing with $\SO(2,n)$ and that $AdS_{l}$ is a quotient of $\SO(2,l-1)$, so in the case of $AdS_{l}$ the index $j$ runs from $5$ to $l+1$. The first anti de Sitter space which contains such root spaces is $AdS_{4}$. More generally, remark that the table \eqref{EqTableSOIwa} of $\mathfrak{so}(2,n)$ gives the feeling that if something works with $AdS_4$, it will work for $AdS_{l\geq 4}$.} $\dpt{i}{5}{n+2}$. For example,
\begin{equation}		\label{EqGeueuleVWXY}
	\begin{aligned}[]
		\sG_{(1,0)}\leadsto W_5&
		=\begin{pmatrix}
			&&&&0\\
			&&&&1\\
			&&&&0\\
			&&&&1\\
			0&1&0&-1&0
		\end{pmatrix}\in\sH
		&\sG_{(-1,0)}\leadsto Y_5&
		=\begin{pmatrix}
			 	&		&		&		&	0\\ 
			 	&		&		&		&	-1\\ 
			 	&		&		&		&	0\\ 
			 	&		&		&		&	1\\ 
			0	&	-1	&	0	&	-1	&	0 
 		\end{pmatrix}\in\sH\\
		\sG_{(0,1)}\leadsto V_{5}&=
		\begin{pmatrix}
			&&&&1\\
			&&&&0\\
			&&&&1\\
			&&&&0\\
			1&0&-1&0&0
		\end{pmatrix}
		&\sG_{(0,-1)}\leadsto X_5&=
		\begin{pmatrix}
			 	&		&		&		&	-1\\ 
			 	&		&		&		&	0\\ 
			 	&		&		&		&	1\\ 
			 	&		&		&		&	0\\ 
			-1	&	0	&	-1	&	0	&	0 
		\end{pmatrix}\\
		\sG_{(1,1)}\leadsto M&=
		\begin{pmatrix}
			0&1&0&-1\\
			-1&0&1&0\\
			0&1&0&-1\\
			-1&0&1&0
		\end{pmatrix}
		&\sG_{(-1,-1)}\leadsto F&=
		\begin{pmatrix}
			0&1&0&1\\
			-1&0&-1&0\\
			0&-1&0&-1\\
			1&0&1&0
		\end{pmatrix}\\
		\sG_{(1,-1)}\leadsto L&=
		\begin{pmatrix}
			0&1&0&-1\\
			-1&0&-1&0\\
			0&-1&0&1\\
			-1&0&-1&0
		\end{pmatrix}
		&\sG_{(-1,1)}\leadsto N&=
		\begin{pmatrix}
			0&1&0&1\\
			-1&0&1&0\\
			0&1&0&1\\
			1&0&-1&0
		\end{pmatrix}.
	\end{aligned}
\end{equation}
These are the same spaces as the previous ones, but the reasoning at page \pageref{pg:subt_tilde} shows that there must be a difference. The subtlety is that we will choose an other notion of positivity, so that the space $\sN$ will be different. 

%The result is on figure \ref{LabelFigHNxitLj}. % From file HNxitLj
\newcommand{\CaptionFigHNxitLj}{The root space}
\input{Fig_HNxitLj.pstricks}
 
Let us recall the aim of our new decomposition: we want to have $\sR_{\sH}\subset\sR$. For this purpose, the equation \eqref{eq:re_N_H} gives us a constraint on the choice of the positivity notion on $\sA^*$. First we must have $\sG_{(1,0)}\subset\sN$. The upper left $4\times 4$ corner of $\sN_{\sH}$ is spanned by $\sG_{(1,1)}-\sG_{(1,-1)}$. We complete our choice of $\sN$ with $\sG_{(0,1)}$. The underlying notion of positivity is that the element $\alpha(a,b)$ is positive in $\sA^*$ when $ (a>0) \LogOu(a=0 \LogEt b>0)$. 

The difference between decomposition and the previous one is the replacement of $N$ by $L\in\sG_{(1,-1)}$. Now\label{PgTablaIwa}
\begin{subequations}		\label{EqLeANEnDimAlg}
\begin{align}
	\sN&=\{W_i,V_j,M,L\}=\{ X_{1,0},X_{0,1},X_{1,1},X_{1,-1} \}\\
\sA&=\{ J_1, J_2\},
\end{align}
\end{subequations}
with the commutator table 
\begin{subequations}  \label{EqTableSOIwa}
\begin{align}
[V_i,W_j]&=\delta_{ij}M &[V_j,L]&=2W_j\\
[ J_1,W_j]&=W_j       &[ J_2,V_i]&=V_i\\
[ J_1,L]&=L           &[ J_2,L]&=-L\\
[ J_1,M]&=M           &[ J_2,M]&=M.
\end{align}
\end{subequations}
It is important to note that $W_{i}$, $J_{1}\in\sH$ and $J_{2}\in\sQ$. On the other hand, the vectors $V_i$ begin to appear in $\SO(2,3)$. The structure of $\SO(2,3)$ is thus slightly different to the one of $\SO(2,2)$, while the structures of $\SO(2,n)$ with $n\geq 3$ are more or less all the same. In the terminology of chapter \ref{ChapBHinAdS}, theses special vectors arrive with $AdS_4$.

The following change of basis in $\sA$ reveals to be useful in some circumstances:
\begin{subequations}  \label{EqChmHJ}
\begin{align}
  	H_1&=J_1-J_2			&H_2&=J_1+J_2\\
	J_1&=\frac{ 1 }{2}(H_1+H_2)	&J_2&=-\frac{ 1 }{2}(H_1-H_2)
\end{align}
\end{subequations}
which leads to the table
\begin{subequations}		\label{TableSeconde}
\begin{align}
{}[V_i,W_j]&=\delta_{ij}M		&[V_j,L]&=2W_j\\
[H_1,V_i]&=-V_i				&[H_2,V_i]&=V_i\\
[H_1,W_i]&=W_i				&[H_2,W_i]&=W_i\\
[H_1,L]&=2L				&[H_2,M]&=2M.
\end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{A companion : \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

That Iwasawa immediately induces a new one by conjugation by the Cartan automorphism $\theta$. That new decomposition will play a central role when we will be interested in black holes on $\SO(2,n)/\SO(1,n)$. See chapter \ref{ChapBHinAdS}, and more precisely the subsection \ref{SubSecGEneBHGrop} and the definition \ref{Singular}. 

The algebras $\sK$ and $\sA$ are not affected by $\theta$, but the algebra $\sN$ changes into a new algebra that we denote by $\bar\sN=\theta(\sN)$. The maximal parabolic algebra in that new Iwasawa decomposition is thus given by
\begin{equation}
	\begin{aligned}[]
		\sA&=\{ J_1,J_2 \}\\
		\bar\sN&=\{ X_{-1,0},X_{0,-1},X_{-1,1},X_{-1,-1} \}=\{ Y_i,X_i,N,F \}.
	\end{aligned}
\end{equation}
Indeed, using lemma \ref{LemSigmaThetaRootSpaces}, we have
\begin{equation}
	\begin{aligned}[]
		\theta(L)&\in\theta\big( \sG_{(1,-1)} \big)\in\sG_{(-1,1)}\leadsto N,\\
		\theta(M)&\in\theta\big( \sG_{(1,1)} \big)\in\sG_{(-1,-1)}\leadsto F,\\
		\theta(V_i)&\in\theta\big( \sG_{(0,1)} \big)\in\sG_{(0,-1)}\leadsto X_i,\\
		\theta(W_j)&\in\theta\big( \sG_{(1,0)} \big)\in\sG_{(-1,0)}\leadsto Y_j.
	\end{aligned}
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Second choice}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecANbarIwa}

Here, we show an other set of choices that can be done in order to get an Iwasawa decomposition. The one that we will create will not be compatible with the Iwasawa decomposition of $\SO(1,n)$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Nilpotent part}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We search the eigenvectors and eigenvalues of $\ad(H_p)$ under the form $E=\begin{pmatrix}A&B\\C&D\end{pmatrix}$ with $A\in M_{4\times 4}$, $B\in M_{(n-2)\times 4}$, $C\in M_{4\times (n-2)}$, $D\in M_{(n-2)\times (n-2)}$.  Remark that, thanks to \eqref{K_et_P}, the matrix $C$ is completely determined by $B$: 
\begin{equation}\label{C_de_B}
\left\{\begin{aligned}C^{i1}&=B^{1i},&C^{i2}&=B^{2i},\\
                    C^{i3}&=-B^{3i},&C^{i4}&=-B^{4i}.\end{aligned}\right.
\end{equation}
The equation to be solved is
\begin{equation}                                                                        \label{eq_gene}
(\ad H_p)E=\begin{pmatrix}[N_p,A]&N_pB\\-CN_p&0\end{pmatrix}\stackrel{!}{=}\lambda_pE,
\end{equation}
with the notation $\lambda_p=\lambda(H_p)$.


\subsubsection{Search for two non zero eigenvalues}

By ``two non zero eigenvalues'', we mean a $\lambda\in\iA^*$ such that $\lambda_{1}\neq 0$ and $\lambda_{2}\neq 0$. We immediately find $D=0$.

The next step is to determine $B$ by the condition $N_pB=\lambda_p B$. We change the range of the indices. Now, $a$, $b:1\rightarrow 4$, and $i$, $j:5\rightarrow n+2$ and a few computation give $\sum_{a}^{}N_p^{ca}B^{ai}=\lambda_pB^{ci}$ (with sum over $a$). Taking successively $c=1,2,3,4$ and taking into account $\lambda_p\neq 0$, we find:
\begin{subequations}\label{pour_B}
\begin{align}
B^{3i}&=\lambda_pB^{1i} \\
(-1)^pB^{4i}&=\lambda_pB^{2i}\\
\lambda_p&=\pm 1.
\end{align}
\end{subequations}
We can check that the equations obtained by $-CN_p=\lambda_pC$ are exactly the one that we can find directly using \eqref{C_de_B} and \eqref{pour_B}.

 Now, we determine $A$ by the condition $[N_p,A]=\lambda_pA$. We know that $A$ and $N_p$ are $4\times 4$ matrices. Again, we redefine the range of the indices: $a=1,2$ and $i=3,4$. Symmetry properties of $A$ are $A^{ai}=A^{ia}$, $A^{ij}=-A^{ji}$ and $A^{ab}=-A^{ba}$, so that
\[
 A=A^{12}(E_{12}-E_{21})+A^{ai}(E_{ai}+E_{ia})+A^{34}(E_{34}-E_{43})
\]
Using equation \eqref{mtr_A}, a quite tedious (but direct) computation give the following for $[N_p,A]$:
\begin{equation}\label{grosse_A}
\begin{pmatrix}
0&A^{23}-(-1)^p A^{14}&0&A^{34}-(-1)^p A^{12}\\
-A^{23}+(-1)^p A^{14}&0&A^{12}-(-1)^p A^{34}&0\\
0&A^{12}-(-1)^p A^{34}&0&A^{14}-(-1)^p A^{23}\\
A^{34}-(-1)^p A^{12}&0&A^{14}-(-1)^p A^{23}&0
\end{pmatrix}
\end{equation}
which has to be equated to $\lambda_pA$. We immediately have $A^{13}=A^{24}=A^{31}=A^{42}=0$. The others conditions are:
\begin{subequations}\label{pour_A}
\begin{align}
\lambda_pA^{12}=&A^{23}-(-1)^p A^{14}\\
\lambda_pA^{14}=&A^{34}-(-1)^p A^{12}
\end{align}
\end{subequations}
Since we are in the case $\lambda_p\neq 0$, using the fact that $\lambda_p=\pm 1$, we easily find $A=0$. Now, we define $\lambda_{\pm\pm}\in\iA^*$ by
\begin{eqnarray}
\begin{aligned}
\lambda_{++}(H_1)&=1 &\lambda_{++}(H_2)&=1,\\
\lambda_{+-}(H_1)&=1 &\lambda_{+-}(H_2)&=-1,\\
\lambda_{-+}(H_1)&=-1 &\lambda_{-+}(H_2)&=1,\\
\lambda_{--}(H_1)&=-1 &\lambda_{--}(H_2)&=-1.
\end{aligned}
\end{eqnarray}
Root spaces are (with $i:5\rightarrow n+2$):
\begin{eqnarray}
\begin{aligned}
\lambda_{++}&\leadsto V_i=E_{3i}+E_{1i}+E_{i1}-E_{i3},\\
\lambda_{-+}&\leadsto W_i=E_{4i}+E_{2i}-E_{i4}+E_{i2},\\
\lambda_{--}&\leadsto X_i=E_{3i}-E_{1i}-E_{i1}-E_{i3},\\
\lambda_{+-}&\leadsto Y_i=E_{4i}-E_{2i}-E_{i4}-E_{i2}.
\end{aligned}
\end{eqnarray}

\subsubsection{Search for eigenvalues with one zero}

We denote by $\lambda(a,b)$ the element of $\iA^*$ defined by $\lambda(x_1,x_2)(H_i)=x_i$. Equations \eqref{grosse_A} and \eqref{pour_A} with for example $a=0$ and $b\neq 0$ give $\lambda_2=\pm 2$. Serious computation give:
\begin{subequations}	\label{EqsTableRacinesSOdeuxn}
\begin{equation}
\lambda(0,-2)\leadsto F=
\begin{pmatrix}
                0&1&0&1\\
		-1&0&-1&0\\
		0&-1&0&-1\\
		1&0&1&0
              \end{pmatrix},\quad
\lambda(2,0)\leadsto N
=\begin{pmatrix}
                0&1&0&1\\
		-1&0&1&0\\
		0&1&0&1\\
		1&0&-1&0
              \end{pmatrix}
\end{equation}
\begin{equation}
\lambda(0,2)\leadsto M
=\begin{pmatrix}
                0&1&0&-1\\
		-1&0&1&0\\
		0&1&0&-1\\
		-1&0&1&0
              \end{pmatrix},\quad
\lambda(-2,0)\leadsto L=
\begin{pmatrix}
                0&1&0&-1\\
		-1&0&-1&0\\
		0&-1&0&1\\
		-1&0&-1&0
              \end{pmatrix}.
\end{equation}
\end{subequations}
In these expressions, we only wrote the upper left part of the matrices which are zero everywhere else.

\subsubsection{Two vanishing eigenvalues}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now search for a matrix $E$ of $\so(2,n)$ such that $(\ad H_p)E=0$ for $p=1,2$. Taking a look at \eqref{eq_gene}, we see that $D$ has no more constraints (apart the usual symmetries). The equation \eqref{grosse_A} gives us $A^{12}=A^{23}=A^{14}=A^{34}=0$, but $A^{13}$, $A^{24}$,$A^{31}$,$A^{42}$ are free. Therefore we can write:
\begin{equation}
  \mG_{\lambda(0,0)}=\{x(E_{13}+E_{31})+y(E_{42}+E_{24})+
    \left(\begin{array}{c|c}
    0&0\\
    \hline
    0&D
    \end{array}\right)
  \}.
\end{equation}
Remark that $\mG_{\lambda(0,0)}\cap\iP$ is spanned by matrices of the form
\[
\begin{pmatrix}
	0&0&x&0\\
	0&0&0&y\\
	x&0&0&0\\
	0&y&0&0
\end{pmatrix},
\]
so that $\mG_{\lambda(0,0)}\cap\iP=\iA$. This is a simple consequence of the very definition of $\iA$ as maximal abelian subalgebra of $\iP$.

\subsubsection{Choice of positivity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We are now able to write down the component $\iN$. We just have to ``elect''\ some $\mG_{\lambda_{(a,b)}}$ with a notion of positivity. Our choice is:
 \begin{equation}
     \iN=\{V_i,W_i,M,N\},\\
 \end{equation}
with $i$, $j:5\to n+2$. A basis of $\iK$ is given by $K_{rs}=E_{rs}-E_{sr}$, and $K_a=E_{12}-E_{21}$ with $\dpt{r,s}{3}{n+2}$.  The commutator with $\iA$ are:
\begin{equation}
\begin{split}
[H_p,K_a]&=-(-1)^p(E_{41}+E_{14})+E_{32}+E_{23}\\
[H_p,K_{rs}]&=\delta_{r3}(E_{1s}+E_{s1})-\delta_{s3}(E_{1r}+E_{r1})\\
           &\quad+(-1)^p(\delta_{4r}(E_{2s}+E_{s2})-\delta_{4s}(E_{2r}+E_{r2}))
\end{split}
\end{equation}

In the decomposition $\mG=\iA\oplus\iN\oplus\iK$, the matrix $E_{32}+E_{23}$ is obtained by
\[
   \frac{1}{2}(\lambda(0,2)+\lambda(2,0))=\begin{pmatrix}
0 & 1 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]
where the upper left corner $\begin{pmatrix}0&1\\-1&0\end{pmatrix}$ can be removed by use of $K_a$:
\[
               E_{32}+E_{23}=\frac{1}{2}(\lambda(0,2)+\lambda(2,0))-K_a
\]
In the same way,
\[
             E_{41}+E_{14}=\frac{1}{2}(\lambda(2,0)-\lambda(0,2))-K_{34}
\]
We also have:
\begin{equation}
\begin{split}
         E_{1i}-E_{i1}&=V_i-K_{3i}\\
         E_{2i}+E_{i2}&=W_i-K_{4i}.
\end{split}
\end{equation} 
Let us summarize the result obtained.  The Iwasawa decomposition of $\SOdn$ is given by
\begin{subequations}\label{eq:Iwasawa_explicite}
\begin{align}
  \mA&=\{H_1,H_2\}\\
  \mN&=\{V_i,W_j,M,N\}\\
  \mK&=\{\begin{pmatrix}a&0\\0&B\end{pmatrix}\}.
\end{align}
\end{subequations}
with $i$, $j:5\to n+2$, $a\in M_{2\times 2}$, $B\in M_{n\times n}$ skew-symmetric, and
$H_p=E_{13}+E_{31}+(-1)^p(E_{24}+E_{42})$.

\label{pg:exp_AN}
We are going to write down a general element of $AN$ (forgetting for the moment $W_i$ and $V_j$). Since $[H_1,H_2]=[M,N]=0$, the only product to be considered is $e^{tH_1}e^{uH_2}e^{aM}e^{bM}$.  Our first task is to exponentiate the matrices of $\mA$ and $\mN$. The first is quite easy: since $H_p^2=\mtu$, 
\[
  e^{tH_p}=\sum_k\frac{t^k}{k!}\left(  \mtu_{\text{if $k$ is even}}, {H_p}\,_{\text{if $k$ is odd}}  \right)
            =(\cosh t)\mtu+(\sinh t)H_p;
\]
then
\begin{equation}
e^{tH_1}e^{uH_2}=\begin{pmatrix}
  \cosh\xi &    0      & \sinh\xi &    0  \\
    0       & \cosh\eta &    0      & \sinh\eta \\
  \sinh\xi &    0      & \cosh\xi &    0     \\
    0       & \sinh\eta &    0      & \cosh\eta \\
                 \end{pmatrix}
\end{equation}
with the change of variable 
\begin{subequations}\label{eq:chm_xi_eta}
\begin{align}   
   \xi &=t+u,\\
   \eta &=u-t.
\end{align}   
\end{subequations}

For the second, we remark that $e^{aM}=\mtu+aM$ and $e^{bN}=\mtu+bN$ because $M^2=N^2=0$. The result is:
\begin{equation}
  e^{aM}e^{bN}=\begin{pmatrix}
                 1-2ab & b+a & 2ab   & b-a \\
 		 -b-a  & 1   & b+a   &  0   \\
		 -2ab  & b+a & 1+2ab &  b-a \\
		  b-a  &  0  &  a-b  &  1  
	      \end{pmatrix}.
\end{equation}
Remark that, by construction, matrices of $\sN$ are nilpotent. But a sum of nilpotent matrices is nilpotent and exponential on nilpotent matrices is easy to compute. Hence there are no obstructions to compute an explicit general matrix of $AN$, this is done at the page \pageref{PgExplAN}.

Now, we compute some commutators of the different matrices that we had seen. 
\begin{subequations}
\begin{align}
[V_i,W_j]&=\delta_{ij}A_{(0,2)}\\
[V_i,X_j]&=2\delta_{ij}A_{(0,0)}(-1,0)+2(E_{ij}-E_{ji})\\
[X_i,Y_j]&=\delta_{ij}\lambda(0,-2)\\
[W_i,X_j]&=\delta_{ij}\lambda(-2,0)\\
[V_i,Y_j]&=\delta_{ij}\lambda(2,0)\\
[W_i,Y_j]&=-2\delta_{ij}A_{(0,0)}(0,1)\\
[V_i,V_j]&=0\\
[\lambda(0,2),\lambda(0,-2)]&=4A_{(0,0)}(-1,-1)\\
[\lambda(2,0),\lambda(-2,0)]&=4A_{(0,0)}(-1,1)
\end{align}
\end{subequations}
\begin{subequations}
\begin{equation}
\begin{split}
         [H_p,K_{rs}]&=\delta_{r3}(V_s-K_{3s})-\delta_{s3}(V_r-K_{3r})\\
 	             &\quad+(-1)^p[\delta_{4r}(W_s-K_{4s})-\delta_{4s}(W_r-K_{4r})],
\end{split}
\end{equation}
\begin{equation}
         [H_p,K_a]=(-1)^{p+1}(\frac{1}{2}(N-M)-K_{34})+\frac{1}{2}(N+M)-K_a.
\end{equation}
\end{subequations}
The non-zero commutators in $\mA\oplus\mN$ are:
\begin{subequations}		\label{TabelPrem}
\begin{align}
[V_i,W_j]&=\delta_{ij}M &[W_i,N]&=-2V_i\\
[H_1,V_i]&=-V_i          &[H_2,V_i]&=V_i\\
[H_1,N]&=2N          &[H_2,M]&=2M\\
[H_1,W_i]&=W_i   &[H_2,W_i]&=W_i.
\end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Application: an isomorphism of Lie algebra}
%---------------------------------------------------------------------------------------------------------------------------
\label{sssIsomsoslplussl}

Root spaces are a powerful tool in proving isomorphisms of Lie algebras. Comparing the tables of $\gsl(2,\eR)$ and $\so(2,2)$ given in \ref{subeq_rootSLR} and \ref{EqsTableRacinesSOdeuxn}, it is easy to prove the following.

\begin{proposition}
We have
\begin{equation}
	\so(2,2)=\gsl(2,\eR)\oplus\gsl(2,\eR)
\end{equation}
as isomorphism of Lie algebras.
\end{proposition}

\begin{proof}
The algebra $\so(2,2)$ has the following root spaces:
\begin{align*}
\lambda(0,2)&&\lambda(2,0)\\
\lambda(0,-2)&&\lambda(-2,0)
\end{align*}
with $\iA=\{ H_1,H_2 \}$. So, by construction the vector space $\lG_1$ generated by $\{ H_1,\lambda(2,0),\lambda(-2,0) \}$ is a Lie subalgebra of $\so(2,2)$ with roots equal to $2$ and $-2$; the corresponding root spaces being one dimensional. Looking on the root spaces of $\gsl(2,\eR)$, we see that they are exactly the same. Thus $\lG_1=\gsl(2,\eR)$. The same is true for $\lG_2=\{ H_2,\lambda(0,2),\lambda(0,-2)  \}$ and we also have $[\lG_1,\lG_2]=0$ because of property \ref{enucii} of proposition \ref{PropPropRacincesReelles}. Thus we have
\[ 
	\so(2,2)=\lG_1\oplus\lG_2=\gsl(2,\eR)\oplus\gsl(2,\eR),
\]
as announced.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Openness of orbits in homogeneous spaces}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecDNqdJOp}

\begin{proposition} \label{pg:orbit_ssvar}
The orbits of $AN$ are submanifolds of $G/H$.
\end{proposition}

\begin{proof}
 Indeed proposition \ref{prop:orbit_N_ss_var} makes $R/(R\cap H)$ the orbit of $\pi(e)$ by $R$ and assure us that it is a submanifold of $G/H$. That proves the proposition for the orbit of $e$. 

For the other orbits, we consider the group $R_z=\AD(z^{-1})R$ which is also a Lie  subgroup of $G$. The space $R_z/(R_z\cap H)$ is isomorphic to the orbit of $\pi(e)$ under the action of $R_z$. Therefore $zR[z^{-1}]$ is a submanifold of $G/H$ and the very definition of a Lie group makes that  $R[z^{-1}]$ is a submanifold too.

\end{proof}

\begin{theorem} \label{tho:pr_ouvert}
If $R$ is a subgroup of $G$ with Lie algebra $\sR$, then the orbit $R\cdot \mfo$ is open in $G/H$ if and only if the projection $\dpt{\pr}{\sR}{\sQ}$ parallel to $\sH$ is surjective.
\end{theorem}

The projection is defined by $\pr(X)=X_{\sQ}$ if $X=X_{\sQ}+X_{\sH}$ is the decomposition of $X\in\sG$ with respect to the decomposition $\sG=\sH\oplus\sQ$. We need two lemmas before to prove the theorem.

\begin{lemma}
The orbit $R\cdot\mfo$ is open if and only if
\[
    \Span\{X^*_{\mfo}|X\in\mR\}=T_{\mfo}M
\]
where $X^*$ is the fundamental field defined by equation \eqref{EqDefChmpFonfOff}.
\label{lem:equiv_1}
\end{lemma}

\begin{proof}
From general theory of fundamental fields (lemma \ref{LemFundSpansTan}) we know that
\[
\Span\{X^*_{\mfo}|X\in\sG\}=T_{\mfo}M.
\]
The game is now to prove that one can replace $\sG$ by $\sR$ if and only if $R\cdot \mfo$ is open.

\subdem{Necessary condition}
If $R\cdot\mfo$ is open, we have a neighbourhood of $\mfo$ which is contained in $R\cdot\mfo$. Then for any $X\in\sG$, and for a small enough $t$, the element $e^{-tX}\cdot\mfo$ belongs to $R\cdot\mfo$. Hence we have a path $r_X(t)$ in $R$ such that $e^{-tX}\cdot\mfo=r_X(t)\cdot\mfo$:
\[
      \Dsdd{e^{-tX}\cdot\mfo}{t}{0}=\Dsdd{r_X(t)\cdot\mfo}{t}{0}.
\]
Since $r_X(t)$ is a path in $R$, we can replace it by a $e^{-tY}$ with a $Y\in\mR$ in the derivative. For this $Y$, we have $X^*_{\mfo}=Y^*_{\mfo}$.

\subdem{Sufficient condition} We have $\dim(R\cdot\mfo)=\dim\Span\{ X^*_{\mfo}\tq X\in\sR \}=\dim T_{\mfo}M$,
so $R\cdot\mfo$ has the same dimension as $M$. The conclusion follows from the fact that a submanifold is open if and only if it has maximal dimension.

\end{proof}

\begin{lemma}
The canonical projection is surjective from $\sR$ to the tangent space to identity:
\begin{equation}\label{eq:equiv_2}
    \Span\{X^*_{\mfo}|X\in\mR\}=d\pi_e(\mR).
\end{equation}

\label{XsdpiR}

\end{lemma}

\begin{proof}
 Consider the following computation when $X\in\mR=T_eR$ is given by the path $X(t)=e^{tX}$:
\begin{equation}
  d\pi_e X=\Dsdd{[X(t)]}{t}{0}
	=\Dsdd{e^{tX}\mfo}{t}{0}
	=Y^*_{\mfo}
\end{equation}
with $Y=-X$. Reading these lines from left to right shows that $d\pi_e(\mR)\subseteq\{X^*_{\mfo}:X\in\mR\}$ while reading it from right to left shows the inverse inclusion.
\end{proof}

%\begin{proposition}
%The orbit $R\cdot\mfo$ is open in $G/H$ if and only if $\dpt{\pr}{\mR}{\sQ}$ is surjective.
%\label{prop:ouvert_ssi}
%\end{proposition}

We are now able to prove the theorem.

\begin{proof}[Proof of theorem \ref{tho:pr_ouvert}]
From lemma \ref{lem:equiv_1} and lemma \ref{XsdpiR}, the orbit $R\cdot\mfo$ is open if and only if $\dpt{d\pi_e}{\mR}{T_{\mfo}M}$ is surjective. On the one hand any $X\in\mR$ can uniquely be written as $X=X_{\sH}+X_{\sQ}$ with $X_{\sH}\in\sH$ and $X_{\sQ}\in\sQ$. On the other hand it is clear that $d\pi_e X_{\sH}=0$, thus $R\cdot\mfo$ is open if and only if $\dpt{d\pi_e}{\RM}{T_{\mfo}M}$ is surjective.

Now, recall that $d\pi_e$ is surjective from $\sG$, hence it is surjective from $\sQ$. The first conclusion is that if $\dpt{\pr}{\mR}{\sQ}$ is surjective, then $R\cdot\mfo$ is open. The inverse implication remains to be proved.

We know that openness $R\cdot\mfo$ implies that $\dpt{d\pi_e}{\RM}{T_{\mfo}M}$ is bijective (surjective because $R\cdot\mfo$ is open and injective because $\dpt{d\pi_e}{\sQ}{T_{\mfo}M}$ is injective by lemma \ref{LemdpiisomMTM}). From all that, one concludes that $\RM=\sQ$. Indeed,  suppose that $X_{\sQ}\in\sQ$ and $X_{\sQ}\notin\RM$. Since $\dpt{d\pi_e}{\RM}{T_{\mfo}M}$ is surjective, there exists a $X_{\sQ}'\in\RM$ such that $d\pi_eX_{\sQ}'=d\pi_eX_{\sQ}'$. This is impossible because $d\pi_e$ is injective from the whole $\sQ$.

\end{proof}
