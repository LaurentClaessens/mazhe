% This is part of Mes notes de mathématique
% Copyright (c) 2011-2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Sur les espaces projectifs : \cite{ProjRolland}.

\begin{definition}
    Soit \( E\) un espace vectoriel de dimension finie sur le corps \( \eK\). Nous définissons sur \( E\setminus\{ 0 \}\) la relation d'équivalence \( u\sim v\) si et seulement si \( u=\lambda v\) pour un certain \( \lambda\in\eK\). Cette relation est la relation de \defe{colinéarité}{colinéarité}. L'ensemble des classes d'équivalence de \( \sim\) est l'\defe{espace projectif}{espace!projectif}\index{projectif!espace} de \( E\) et sera noté \( P(E)\)\nomenclature[G]{\( P(E)\)}{l'espace projectif de $E$}.
\end{definition}

Si \( \dim E=2\), l'ensemble \( P(E)\) est la \defe{droite projective}{droite!projective}\index{projectif!droite}, et si \( \dim E=3\) nous parlons du \defe{plan projectif}{plan!projectif}\index{projectif!plan}.

Étant donné que tous les \( \eK\)-espaces vectoriels de dimensions \( n+1\) sont isomorphes à\( \eK^{n+1}\), nous noterons \( P_n(\eK)\) ou \( P_n\) l'espace projectif \( P(\eK^{n+1})\). \label{PgNotimesjNtMoW}

\begin{example}
    Si \( n=1\) et \( \eK=\eR\), l'espace projectif est l'ensemble des droites vectorielles dans le plan usuel. Il y en a une pour chaque point du type \( (x,1)\) avec \( x\in\eR\) et ensuite une horizontale, passant par le point \( (1,0)\). Nous avons donc
    \begin{equation}
        P_1(\eR)=\{ (1,0) \}\cup\{ (x,1)\tq x\in \eR \}.
    \end{equation}
    Le point \( (1,0)\) est dit «point à l'infini».
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sous espaces projectifs}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Un \defe{sous-espace projectif}{projectif!sous-espace} de \( P(E)\) est une partie de la forme \( P(F)\) où \( F\) est un sous-espace vectoriel de \( E\).

\begin{proposition}     \label{PropuqpWVx}
    Si \( F\) et \( G\) sont des sous-espaces vectoriels de \( E\), alors
    \begin{equation}
        P(F)\cap P(G)=P(F\cap G)
    \end{equation}
    et nous avons
    \begin{equation}        \label{EqNAdWfN}
        \dim P(F)+\dim P(G)=\dim P(F+G)+\dim P(F\cap G).
    \end{equation}
\end{proposition}

\begin{proof}
    Nous avons 
    \begin{equation}
        P(F)=\{ [v]\tq v\in F \}
    \end{equation}
    où les crochets signifient la classe par rapport à la relation de colinéarité. Nous avons alors
    \begin{equation}
        P(F)\cap P(G)=\{ [v]\tq v\in F\cap G \}=P(F\cap G).
    \end{equation}
    Cela prouve le premier point.

    En ce qui concerne l'équation \eqref{EqNAdWfN}, en considérant \( \dim P(E)=\dim E-1\) nous devons prouver l'égalité
    \begin{equation}
        \dim F+\dim G=\dim (F+G)+\dim(F\cap G)
    \end{equation}
    concernant les dimensions des espaces vectoriels usuelles. Si nous considérons une base de \( E\) telle que \( B_1=\{ e_1,\ldots, e_{k_1} \}\) est une base de \( F\cap G\), \( B_2=\{ e_{k_1+1},\ldots, e_{k_2} \}\) complète \( B_1\) en une base de \( F\) et \( B_3=\{ e_{k_2+1},\ldots, e_n \}\) complète \( B_1\cup B_2\) en une base de \( G\).

    Nous avons alors
    \begin{subequations}
        \begin{align}
            \dim F+\dim G&=2\Card(B_1)+\Card(B_2)+\Card(b_3)\\
            \dim(F+G)&=\Card(B_1)+\Card(b_2)+\Card(B_3)\\
            \dim(F\cap G)&=\Card(B_1).
        \end{align}
    \end{subequations}
    De là la relation \eqref{EqNAdWfN} se déduit immédiatement.    
\end{proof}

\begin{theorem}[incidence]\index{théorème!incidence}
    Soient \( F\) et \( F\) deux sous-espaces vectoriels de \( E\) tels que 
    \begin{equation}
        \dim P(F)+\dim P(G)\geq \dim P(E).
    \end{equation}
    Alors \( P(F)\cap P(G)\neq \emptyset\).
\end{theorem}

\begin{proof}
    En utilisant les hypothèses et la proposition \ref{PropuqpWVx} nous avons
    \begin{equation}
        \dim P(E)+\dim P(G)=\dim P(F+G)+\dim P(F\cap G)\geq \dim P(E).
    \end{equation}
    En passant aux espaces vectoriels correspondants,
    \begin{equation}
        \dim(F+G)+\dim(F\cap G)\geq \dim(E)+1.
    \end{equation}
    Mais nous avons aussi \( \dim(F+G)\leq \dim(E)\) et par conséquent \( \dim(F\cap G)\geq 1\). Au final, \( \dim P(F\cap G)\geq 0\). Cela prouve que \( P(F\cap G)\) contient au moins un élément (nous rappelons que lorsqu'un espace projectif contient un seul élément, sa dimension est zéro).
\end{proof}

\begin{example}
    Soient les plans \( \Pi_1\equiv x=0\) et \( \Pi_2\equiv y=0\). Nous avons
    \begin{subequations}
        \begin{align}
            P(\Pi_1)&=\{ [0,y,1] \}\cup\{ [0,1,0] \}\\
            P(\Pi_2)&=\{ [x,0,1] \}\cup\{ [1,0,0] \}
        \end{align}
    \end{subequations}
    où le crochet signifie la classe pour la colinéarité. Ces deux droites projectives ont comme point d'intersection le point \( [0,0,1]\).
\end{example}

\begin{definition}
    Un \defe{hyperplan projectif}{projectif!hyperplan} est un sous-espace projectif de \( P(E)\) de la forme \( P(V)\) où \( V\) est un hyperplan de \( E\).
\end{definition}

\begin{proposition}
    Soit \( H=P(V)\) un hyperplan projectif de \( P(E)\) et soit \( m\) hors de \( H\). Alors toute droite projective passant par \( m\) coupe \( H\) en un et un seul point.
\end{proposition}

\begin{proof}
    Si \( \dim E=n\) nous avons \( \dim V=n-1\). Soit \( d=P(D)\) une droite projective passant par \( m\), c'est à dire que \( D\) est de dimension \( 2\) dans \( E\). Si \( D\subset V\) alors \( m\in P(D)\subset P(V)\); or nous avons demandé que \( m\) soit hors de \( P(V)\). Par conséquent \( D\) n'est pas inclus à \( V\) et en particulier \( \dim(D+V)=\dim(E)\).

    Nous recopions la formule \eqref{EqNAdWfN} pour notre cas :
    \begin{equation}
        \underbrace{\dim d}_{=1}+\underbrace{\dim H}_{=n-2}=\underbrace{\dim P(D+V)}_{=n-1}+\dim P(D\cap V).
    \end{equation}
    Nous avons donc \( \dim P(D\cap V)=0\), ce qui signifie que l'ensemble \( P(D\cap V)=P(D)\cap P(V)=d\cap H\) contient un et un seul point.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espace projectifs comme «complétés» d'espaces affines}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( E\) un espace vectoriel de dimension \( 2\) et \( P(E)\) la droite projective correspondante, et soit \( \{ e_1,e_2 \}\) une base de \( E\). Nous considérons la droite affine \( d\equiv y=1\). Nous avons la bijection
\begin{equation}        \label{EqvrfDLz}
    \begin{aligned}
        \phi\colon d\cup\{ \infty \}&\to P(E) \\
        (x,1)&\mapsto \text{la droite vectorielle passant par \( (x,1)\)} \\
        \infty&\mapsto \text{la droite vectorielle passant par \( (1,0)\)}.
    \end{aligned}
\end{equation}

\begin{lemma}
    Si nous munissons l'ensemble \( d\cup\{ \infty \}\) de la topologie compactifiée d'Alexandroff, la bijection \eqref{EqvrfDLz} est un homéomorphisme.
\end{lemma}

Soient maintenant les plans affines dans l'espace vectoriel \( E\) de dimension \( 3\)
\begin{subequations}
    \begin{align}
        \Pi_1\equiv z&=0\\
        \Pi_2\equiv z&=1.
    \end{align}
\end{subequations}
Une droite (vectorielle) de \( E\) coupe \( \Pi_2\) en un et un seul point, sauf si elle est contenue dans \( \Pi_1\). Nous avons donc une bijection
\begin{equation}
    \begin{aligned}
        \phi\colon P(E)&\to \Pi_2\cup P(\Pi_1) \\
        d&\mapsto \begin{cases}
            \Pi_2\cap d    &   \text{si cette intersection est non vide}\\
            d    &    \text{sinon.}
        \end{cases}
    \end{aligned}
\end{equation}
La droite projective \( P(\Pi_1)\) est la droite à l'infini du plan projectif \( P(E)\). Nous voyons que le plan projectif \( P(E)\) peut être vu comme un plan affine \( (\Pi_2)\) «complété»  par une droite affine \( P(\Pi_1)\). Cette dernière droite est elle-même une droite affine complétée par un point à l'infini.

Nous pouvons généraliser cette démarche en considérant un espace affine \( \affE\) de direction \( E\) sur le corps \( \eK\). Nous construisons \( F=E\times \eK\) et nous considérons un repère affine sur \( F\) tel que \( E\equiv x_{n+1}=0\). Nous pouvons donc identifier \( \affE\) à l'hyperplan affine d'équation \( x_{n+1}=1\) dans \( F\).

Une droite vectorielle de \( F\) non contenue dans \( E\) coupe \( \affE\) en un unique point; nous avons donc une bijection
\begin{equation}
    \affE\cup P(E)\to P(F).
\end{equation}
Dans ce cadre, \( P(E)\) est l'hyperplan à l'infini et nous disons que \( P(E)\) est la \defe{complétion projective}{complétion!projective}\index{projectif!complétion} de \( \affE\).

\begin{definition}
    Soit \( E\) un espace vectoriel de dimension \( 3\). Nous disons que \( d\subset P(E)\) est une \defe{droite projective}{projectif!droite} de \( P(E)\) si \( d=P(D)\) pour une plan vectoriel \( D\subset E\).
\end{definition}

\begin{example}
    Nous considérons les plans affines
    \begin{subequations}
        \begin{align}
            \Pi_1&\equiv z=0\\
            \Pi_2&\equiv z=1
        \end{align}
    \end{subequations}
    et nous avons la bijection
    \begin{equation}
        P(E)=\Pi_2\cup P(\Pi_1).
    \end{equation}
    Un plan affine \( D\) a deux possibilités : soit il coupe \( \Pi_2\) en une droite, soit il est égal à \( \Pi_1\). Si \( D\cap\Pi_2=d\) (\( d\) est une droite affine), alors nous avons
    \begin{equation}
        P(D)=d\cup\{ \infty_D \},
    \end{equation}
    ce qui justifie la terminologie comme quoi \( P(D)\) est une droite dans \( P(E)\).
\end{example}

Soit \( E\) un espace vectoriel de dimension \( 3\) et le plan projectif \( P(E)\). Nous avons deux types de droites projectives :
\begin{enumerate}
    \item
        D'abord nous avons la droite à l'infini, donnée\footnote{Dans notre représentation usuelle du plan projectif \( z=1\).} par \( P(z=0)\).
    \item
        Ensuite nous avons toutes les droites affines du plan \( z=1\). Chacune de ces droites est complétée par un point à l'infini. 
\end{enumerate}

\begin{example}     \label{ExempMyTmFp}
    Étudions un peu le second type de droites. D'abord si deux droites sont parallèles, leurs points à l'infini sont identiques. Prenons par exemple les droites \( d=\{ z=1,x=1 \}\) et \( d'=\{ z=1,x=2 \}\). Elles décrivent les directions des vecteurs
    \begin{equation}
        \begin{aligned}[]
            \begin{pmatrix}
                1    \\ 
                 y   \\ 
                1    
            \end{pmatrix}&&\text{et}&&
            \begin{pmatrix}
                2    \\ 
                y    \\ 
                1    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    En normalisant, ce sont les vecteurs
    \begin{equation}
        \begin{aligned}[]
            \frac{1}{ \sqrt{2+y^2} }\begin{pmatrix}
                1    \\ 
                y    \\ 
                1    
            \end{pmatrix}&&\text{et}&&
            \frac{1}{ \sqrt{5+y^2} }\begin{pmatrix}
                2    \\ 
                y    \\ 
                1    
            \end{pmatrix},
        \end{aligned}
    \end{equation}
    et toutes deux tendent vers le vecteur \( (0,1,0)\) pour \( y\to\infty\).
\end{example}

\begin{lemma}
    Deux droites d'un plan projectif ont toujours une intersection.
\end{lemma}

\begin{proof}
    Si les deux droites sont des droites affines non parallèles, le résultat est évident. Si elles sont parallèles, alors l'intersection est donnée par le point à l'infini comme indiqué dans l'exemple \ref{ExempMyTmFp}.

    Supposons que \( d\) est la droite à l'infini tandis que \( d'\) est une droite affine. Dans notre représentation usuelle du plan affine, la droite à l'infini \( d\) a contient les vecteurs \( (1,y,0)\) et le point à l'infini \( (0,1,0)\). La droite affine \( d'\) a pour équation paramétriques
    \begin{subequations}
        \begin{numcases}{}
            x=at+c\\
            y=bt+d\\
            z=1.
        \end{numcases}
    \end{subequations}
    Les directions données par la droite \( d'\) sont donc
    \begin{equation}
        \frac{1}{ a^2t^2+b^2t^2+c^2+d^2}\begin{pmatrix}
            at+c    \\ 
            bt+d    \\ 
            1
        \end{pmatrix}
    \end{equation}
    Son point à l'infini est la direction du vecteur \( (a,b,0)\), qui est bien un point de la droite à l'infini (éventuellement son point à l'infini\footnote{D'accord, aller chercher le point à l'infini de la droite à l'infini, c'est chercher loin, mais n'empêche que ça existe.}).
\end{proof}

La plupart du temps nous considérons le plan projectif comme étant le plan affine \( z=1\) de l'espace affine de dimension \( 3\) complété par la droite affine \( x=1,z=0\), elle-même complétée par le point \( (0,1,0)\). Ce n'est évidemment pas la seule manière. Tout plan peut être considéré comme le plan à l'infini et pour une droite projective, tout point peut être considéré comme point à l'infini.

Sur la figure \ref{LabelFigChoixInfinissLabelSubFigChoixInfini0}, le point à l'infini est la direction \( (1,0)\) tandis que la direction \( (1,1)\) n'a rien de spécial. À l'inverse sur la figure \ref{LabelFigChoixInfinissLabelSubFigChoixInfini1}, la direction à l'infini est \( (1,1)\) tandis que la direction \( (1,0)\) est une direction usuelle.

%The result is on figure \ref{LabelFigChoixInfini}.
\newcommand{\CaptionFigChoixInfini}{Deux façons de voir la droite projective. Étant donné que les points de la droite projective doivent être interprétés comme des directions (des classes d'équivallence), en réalité les deux dessins représentent les mêmes ensembles.}
\input{pictures_tex/Fig_ChoixInfini.pstricks}

\begin{remark}
    Du point de vue de la topologie, si nous mettons celle de la compactification d'Alexandroff, tous les points de la droite projective sont équivalents.

    Du point de vue de la géométrie différentielle, c'est la même chose. En effet nous pouvons mettre sur la droite projective un système de deux cartes en pensant aux angles. La première sur \( \mathopen] -a , a \mathclose[\) avec par exemple \( a<\pi/4\). La seconde carte serait \( \mathopen] a/2 , \pi \mathclose[\). Dans ce cas la direction \( \theta=0\) semble jouer un rôle spécial, mais il n'en est rien.

    Nous pouvons également considérer les cartes \( \mathopen] \pi/4-a , \pi/4+a \mathclose[\) et \( \mathopen] \pi/4+a/2 , 5\pi/4 \mathclose[\). Dans ces cartes, c'est plutôt le point \( \theta=\pi/4\) qui semble différent (encore qu'il soit bien centré dans une carte).
\end{remark}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Théorème de Pappus}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{theorem}     \index{théorème!Pappus!affine}
    Soient deux droites \( d\) et \( d'\) dans un plan affine. Soient \( A,B,C\in d\) et \( A',B',C'\in d'\) tels que \( AB'\parallel BA'\) et \( BC'\parallel B'C\). Alors \( AC'\parallel A'C\).
\end{theorem}

\begin{proof}
    Si \( d\) et \( d'\) ne sont pas parallèles nous considérons \( o\), le point d'intersection. Les relations de parallélisme des hypothèses impliquent qu'il existe \( \lambda_1\) et \( \lambda_2\) tels que
    \begin{subequations}
        \begin{numcases}{}
            A=\lambda_1 B\\
            B'=\lambda_1 A'
        \end{numcases}
    \end{subequations}
    et
    \begin{subequations}
        \begin{numcases}{}
            B'=\lambda_2 C'\\
            C=\lambda_2 B.
        \end{numcases}
    \end{subequations}
    En substituant nous trouvons
    \begin{subequations}
        \begin{numcases}{}
            C=\frac{ \lambda_2 }{ \lambda_1 }A\\
            A'=\frac{ \lambda_2 }{ \lambda_1 }C',
        \end{numcases}
    \end{subequations}
    ce qui implique que \( A'C\parallel AC'\).

    Si les droites \( d\) et \( d'\) sont parallèles, alors nous avons les translations
    \begin{subequations}
        \begin{numcases}{}
            B=A+x\\
            A'=B'+x
        \end{numcases}
    \end{subequations}
    et
    \begin{subequations}
        \begin{numcases}{}
            B=C+y\\
            C'=B'+y,
        \end{numcases}
    \end{subequations}
    ce qui montre que
    \begin{subequations}
        \begin{numcases}{}
            C=A+x-y\\
            A'=C'+x-y,
        \end{numcases}
    \end{subequations}
    et donc que \( A'C\parallel AC'\).
\end{proof}

Le théorème suivant est une version projective.
\begin{theorem}     \index{théorème!Pappus!projectif}
    Soient \( d\) et \( d'\) deux droites projectives d'un plan projectif. Soient \( A,B,C\in d\) et \( A',B',C'\in d'\). Alors les points \( B'C\cap C'B\), \( C'A\cap A'C\) et \( A'B\cap B'A\) sont alignés.
\end{theorem}

\begin{proof}
    Soient \( E=BC'\cap C'B\) et \( E'=C'A\cap A'C\). Ces deux points existent parce que deux droites projectives distinctes ont toujours un unique point d'intersection. Nous allons prendre \( EE'\) comme droite à l'infini et prouver que le point \( A'B\cap B'A\) est dessus. Étant donné que le point d'intersection de \( B'C\) et \( C'B\) est à l'infini nous avons \( B'C\parallel C'B\) (cela est un exemple de la flexibilité de la notion de parallélisme en géométrie projective). De la même façon nous avons \( C'A\parallel A'C\).

    Par le théorème de Pappus affine nous avons alors \( A'B\parallel B'A\) et par conséquent le point d'intersection est sur la droite à l'infini, c'est à dire sur la droite \( EE'\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Homographies}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Homographies}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soient \( E\) et \( F\) deux espaces vectoriels avec leurs projections naturelles
    \begin{subequations}
        \begin{align}
            \pi_E\colon E\setminus\{ 0 \}&\to P(E)\\
            \pi_F\colon F\setminus\{ 0 \}&\to P(F).
        \end{align}
    \end{subequations}
    Une application \( g\colon P(E)\to P(F)\) est une \defe{homographie}{homographie} si il existe un isomorphisme d'espaces vectoriels \( \tilde g\colon E\to F\) tel que le diagramme
    \begin{equation}
        \xymatrix{%
        E\setminus\{ 0 \} \ar[r]^{\tilde g}\ar[d]_{\pi_E}        &   F\setminus\{ 0 \}\ar[d]^{\pi_F}\\
           P(E) \ar[r]_{g}   &   P(F)
           }
    \end{equation}
    commute, c'est à dire si il existe \( \tilde g\colon E\to F\) telle que
    \begin{equation}
        \pi_F\big( \tilde g(v) \big)=g\big( \pi_E(v) \big)
    \end{equation}
    pour tout \( v\in E\).
\end{definition}

\begin{lemma}
    Si \( \tilde g\colon E\to F\) est linéaire et si \( \ker\tilde g=\{ 0 \}\) alors l'application \( g\) définie par
    \begin{equation}        \label{EqRlGIJW}
        g\big( \pi_E(v) \big)=\pi_F\big( \tilde g(v) \big)
    \end{equation}
    est une homographie.
\end{lemma}

\begin{proof}
    Nous devons simplement vérifier que l'équation \eqref{EqRlGIJW} définit bien une application. Soient \( v,w\in E\) tels que \( \pi_Ev=\pi_Ew\); nous devons montrer que 
    \begin{equation}        \label{EqmoIUkH}
        \pi_F\tilde gv=\pi_F\tilde gw.
    \end{equation}
    L'équation \eqref{EqmoIUkH} sera vérifiée si et seulement si il existe \( \lambda\in\eR\) tel que \( \tilde gv=\lambda\tilde gw\), c'est à dire si et seulement si \( \tilde g(v-\lambda w)=0\). Étant donné que nous supposons que le noyau de \( \tilde g\) est réduit à \( \{ 0 \}\), l'équation \eqref{EqmoIUkH} sera vérifiée si et seulement si \( v=\lambda w\), ce qui signifie exactement \( \pi_E(v)=\pi_E(w)\).
\end{proof}

La proposition suivante donne les premières propriétés des homographies.
\begin{proposition}
    Quelques propriétés des homographies.
    \begin{enumerate}
        \item
            Une homographie est bijective.
        \item
            Si deux espaces projectifs sont homographes, alors ils ont même dimension.
        \item
            L'ensemble des homographies \( P(E)\to P(F)\) est un groupe (pour la composition).
        \item
            Une homographie conserve l'alignement des points.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous considérons une homographie \( g\colon P(E)\to P(F)\), et \( \tilde g\) l'isomorphisme d'espaces vectoriels correspondant.
    \begin{enumerate}
        \item
            Pour l'injectivité, si \( g\big( [v] \big)=g\big( [w] \big)\) alors en utilisant la définition d'une homographie, \( \pi_F\tilde gv=\pi_F\tilde gw\), ce qui implique que \( \tilde gv=\lambda\tilde gw\), et donc \( v=\lambda w\), ce qui signifie \( [v]=[w]\).

            Pour la surjectivité, un élément général de \( P(F)\) prend la forme \( \pi_F\tilde gv\) pour un certain \( v\in E\). Nous avons \( g\big( \pi_Ev \big)=\pi_F\tilde gv\). Par conséquent l'élément \( \pi_F\tilde gv\) est bien dans l'image de \( g\).

        \item
            Une homographie \( P(E)\to P(F)\) n'existe que si il existe un isomorphisme \( E\to F\). Les dimensions sont donc automatiquement égales.
        \item
            Il suffit de vérifier que l'application
            \begin{equation}
                \begin{aligned}
                    \varphi\colon P(E)&\to P(E) \\
                    \pi_F\tilde gv&\mapsto \pi_Ev 
                \end{aligned}
            \end{equation}
            est bien définie et donne l'inverse de \( g\).
        \item
            Soient les points \( A,B,C\) alignés dans \( P(E)\); ils correspondent à des directions de \( E\) qui sont données par des vecteurs situés sur la même droite affine. Autrement dit, il existe trois points \( a,b,c\in E\) situés sur la même droite affine tels que \( A,B,C=\pi_E(a,b,c)\). Les images par \( g\) sont données par \( \pi_F\tilde ga\), \( \pi_F\tilde gb\), et \( \pi_F\tilde gc\).

            Étant donné qu'un isomorphisme d'espaces vectoriels conserve l'alignement affin, les points \( \tilde ga\), \( \tilde gb\) et \( \tilde gc\) sont alignés dans \( F\). Cela implique que les projections par \( \pi_F\) sont alignés dans \( P(F)\).
    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le groupe projectif}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Le groupe des homographies de l'espace \( P(E)\) est le \defe{groupe projectif}{groupe!projectif}\index{projectif!groupe}, noté \( \PGL(E)\).\nomenclature[G]{\( \PGL(E)\)}{groupe projectif}
\end{definition}

Nous avons une surjection naturelle
\begin{equation}        \label{EqpqNEfe}
    \begin{aligned}
         \GL(E)&\to \PGL(E) \\
        \tilde g&\mapsto g 
    \end{aligned}
\end{equation}
qui s'avère être un morphisme de groupes.

\begin{proposition}
    Nous avons l'isomorphisme de groupes
    \begin{equation}
        \frac{ \GL(E) }{\{  \text{homothéties} \}}\simeq \PGL(E).
    \end{equation}
    
\end{proposition}

\begin{proof}
    Nous devons prouver que le noyau de l'application \eqref{EqpqNEfe} est constitué des homothéties. Considérons un automorphisme d'espace vectoriel \( f\colon E\to E\) dont l'homographie associée est l'identité, et prouvons que \( f\) est une homothétie. Nous avons le diagramme commutatif suivant :
    \begin{equation}
        \xymatrix{%
        E\setminus\{ 0 \} \ar[r]^{f}\ar[d]_{\pi_E}        &   E\setminus\{ 0 \}\ar[d]^{\pi_E}\\
           P(E) \ar[r]_{\id}   &   P(E).
           }
    \end{equation}
    Pour tout vecteur \( v\in E\) nous avons \( \pi_E(v)=\pi_E\big( f(v) \big)\). Cela implique qu'il existe \( \lambda\in\eR\) tel que \( f(v)=\lambda v\). Tous les vecteurs de \( E\) sont donc des vecteurs propres de \( f\). Cela n'est possible que si toutes les valeurs propres sont identiques, c'est à dire que \( f\) est une homothétie.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Coordonnées homogènes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( E\) un espace vectoriel de dimension \( n+1\) et une base \( \{ e_0,\ldots, e_n \}\) de \( E\). Soit \( M\in P(E)\) et \( u\in E\) un élément engendrant \( M\). Au point \( M\) nous voudrions associer les coordonnées \( (x_0,\ldots, x_n)\) de \( u\) dans \( E\). Notons que toutes les coordonnées de \( u\) ne sont jamais nulles en même temps parce que \( u\) doit indiquer une direction. Nous savons par ailleurs que les coordonnées \( (x_0,\ldots, x_n)\) indiquent le même point de \( P(E)\) que les coordonnées \( (x'_0,\ldots, x'_n)\) si et seulement si \( x_i=\lambda x_i\).

\begin{definition}
    La classe d'équivalence de \( (x_0,\ldots, x_n)\) est la \defe{coordonnées homogène}{coordonnées!homogène} de \( M\). Nous la notons \( (x_0:\ldots :x_n)\).\nomenclature[G]{\( (x_0:\ldots:x_n)\)}{coordonnées homogènes dans un espace projectif}
\end{definition}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dualité}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\) un espace vectoriel de dimension \( n+1\). Une forme linéaire non nulle est un élément de \( E^*\), mais aussi un représentant d'un élément de \( P(E^*)\).

Le noyau d'une forme linéaire \( \omega\) est un hyperplan. Le noyau de la forme linéaire \( \lambda\omega\) étant le même hyperplan, l'hyperplan est donné par toute la classe de \( \omega\) dans \( P(E^*)\). Nous avons donc une bijection
\begin{equation}
    P(E^*)\leftrightarrow \{ \text{hyperplans vectoriels de \( E\)} \}.
\end{equation}

Soit \( E\) de dimension \( 3\) et une base \( \{ e_1,e_2,e_3 \}\). L'espace dual \( E^*\) possède la base duale \( \{ e_1^*,e_2^*,e_3^* \}\). À un élément \( m\in P(E^*)\) nous associons la droite
\begin{equation}
    H_m\{ (X:Y:T)\tq m(X,Y,T)=0 \}
\end{equation}
dans \( P(E)\). Si les coordonnées homogènes de \( m\) étaient \( (u:v:w)\) alors l'équation de la droite \( H_m\) est 
\begin{equation}    \label{Eqezgpmk}
    uX+vY+wT=0.
\end{equation}
En effet si \( \omega\in E^*\) est un représentant de \( m\) alors \( \omega=\lambda(ue_1^*+ve_2^*+we_3^*)\) et l'équation \eqref{Eqezgpmk} est indépendante de \( \lambda\) ainsi que du choix du représentant dans \( E\) du point \( (X:Y:T)\) dans \( P(E)\).

Si les points \( m_1\) et \( m_2\) sont distincts dans \( P(E^*)\), ils donnent deux droites \( m_1(X,Y,T)=0\) et \( m_2(X,Y,T)=0\). Les points de la droite qui joint \( m_1\) à \( m_2\) dans \( P(E^*)\) sont de la forme \( \lambda m_1+\mu m_2\) et ils sont associés à l'équation
\begin{equation}
    \lambda m_1(X,Y,T)+\mu m_2(X,Y,T)=0
\end{equation}
qui sont encore des droites dans \( P(E)\). Toutes ces droites passent par le point d'intersection des droits associées à \( m_1\) et \( m_2\). Nous avons donc
\begin{equation}
    \bigcap_{\lambda,\mu}H_{\lambda m_1+\mu m_2}=H_{m_1}\cap H_{m_2}.
\end{equation}

\begin{lemma}
    L'application
    \begin{equation}
        \begin{aligned}
            P(E^*)&\to \{ \text{droites dans \( P(E)\)} \} \\
            m&\mapsto H_m 
        \end{aligned}
    \end{equation}
    est une bijection.
\end{lemma}

\begin{proof}
    Une droite dans \( P(E)\) est donnée en coordonnées homogènes par une équation \( aX+bY+cT=0\). Cette droite est décrite par le point \( (a:b:c)\) dans \( P(E^*)\). Ce dernier correspond à la direction de la forme \( ae_1^*+be_2^*+ce_3^*\). Cela prouve que l'application est surjective.

    Pour l'injectivité, si \( m_1\neq m_2\) dans \( P(E^*)\), les formes \( \omega_1\) et \( \omega_2\) associées dans \( E^*\) ne sont pas multiples l'une de l'autre. Donc les équations
    \begin{equation}
        a_1X+b_1Y+z_1T=0
    \end{equation}
    et
    \begin{equation}
        a_2X+b_2Y+z_2T=0
    \end{equation}
    n'ont pas de solutions communes et décrivent donc des droites distinctes.
\end{proof}

\begin{lemma}   \label{LemjXywjH}
    Trois points distincts \( m_1\), \( m_2\) et \( m_3\) dans \( P(E^*)\) sont alignés si et seulement si les droites \( H_{m_1}\), \( H_{m_2}\) et \( H_{m_3}\) sont distinctes et concourantes.
\end{lemma}

\begin{proof}
    Supposons avoir trois points alignés, c'est à dire
    \begin{equation}    \label{EqXyfbmF}
        m_3=m_1+\mu(m_2-m_1).
    \end{equation}
    Soit \( X:Y:T\) le point d'intersection de \( H_{m_1}\) avec \( H_{m_2}\). Alors \( m_1(X,Y,T)=m_2(X,Y,T)=0\). En tenant compte de \eqref{EqXyfbmF} nous avons alors évidemment \( m_3(X,Y,T)=0\).

    Supposons maintenant que les trois droites \( H_{m_i}\) soient concourantes. Nous avons donc un point \( (X:Y:T)\) dans \( P(E)\) tel que \( m_i(X,Y,T)=0\). Si \( m_i\) est la classe de \( a_ie_1^*+b_ie_2^*+c_ie^*_3\) alors nous avons le système
    \begin{subequations}
        \begin{numcases}{}
            a_1X+b_1Y+c_1T=0\\
            a_2X+b_2Y+c_2T=0\\
            a_3X+b_3Y+c_3T=0.
        \end{numcases}
    \end{subequations}
    Afin que cela ait une solution non triviale nous devons avoir
    \begin{equation}
        \det\begin{pmatrix}
            a_1 &   b_1 &   c_1\\
            a_2 &   b_2 &   c_2\\
            a_3 &   b_3 &   c_3
        \end{pmatrix}\neq 0,
    \end{equation}
    c'est à dire que les points \( (a_i,b_i,c_i)\) soient alignés.
\end{proof}
 
En tenant compte de ce qui a été dit, une droite dans \( P(E^*)\) est constituée de points qui fournissent des droites concourantes dans \( P(E)\). Donc une droite de \( P(E^*)\) se caractérise par un point de \( P(E)\) (l'intersection) de la façon suivante. Un point \( M_d\in P(E)\) donne lieu à un \defe{faisceau de droites}{faisceau de droites} passant par \( M_d\). Chacune de ces droites donne lieu à un point de \( P(E^*)\) et tous ces points sont alignés. Nous avons ainsi construit la droite \( d\) dans \( P(E^*)\) correspondante au point \( M_d\) de \( P(E)\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes}
%---------------------------------------------------------------------------------------------------------------------------

Soit l'espace projectif de dimension \( n\) avec ses coordonnées homogènes \( (X_0:\ldots :X_n)\). Nous considérons l'espace affine \( H\equiv X_n=1\) dans l'espace vectoriel \( E\) de dimension \( n+1\). Nous considérons pour \( H\) un repère affine ayant pour origine le point \( (0,\ldots, 0,1)\). Considérons un polynôme homogène \( P\) sur le corps \( \eK\). L'équation 
\begin{equation}
    P(X_0,\ldots, X_n)=0
\end{equation}
sur l'espace vectoriel \( E\) descend immédiatement à l'espace projectif : étant donné que \( P\) est homogène nous avons \( P(u)=0\) si et seulement si \( P(\lambda u)=0\).

Nous essayons de décrire l'ensemble \( A\) des points de \( P(E)\) satisfaisant \( P(X_0,\ldots, X_n)=0\). Nous savons que les éléments de \( P(E)\) ont chacun un représentant soit dans \( H\) soit sur la droite à l'infini. Ceux de \( A\) ayant un représentant dans \( H\) sont d'équation
\begin{equation}
    Q(x_0,\ldots, x_{n-1})=0
\end{equation}
où \( Q\) est le polynôme donné par \( Q(X_0,\ldots, x_{n-1})=P(x_0,\ldots, x_{n-1},1)\). Les points de \( A\) ayant un représentant sur la droite à l'infini s'obtiennent par l'équation
\begin{equation}
    R(x_0,\ldots, x_{n-1})=0
\end{equation}
où \( R\) est le polynôme donné par \( R(x_0,\ldots, x_{n-1})=P(x_0,\ldots, x_{n-1},0)\).

\begin{example}
    Nous considérons la conique projective
    \begin{equation}    \label{EqpLeQIN}
        X^2-XT-Y^2-T^2=0.
    \end{equation}
    Elle est décomposée en deux partie : une dans l'espace affine «normale» et une à l'infini. La première s'obtient en posant \( T=1\) dans \eqref{EqpLeQIN} :
    \begin{equation}    \label{EqdGHzqJ}
        x^2-x-y^2-1=0.
    \end{equation}
    L'autre est obtenue en posant \( T=0\) :
    \begin{equation}
        x^2-y^2=0.
    \end{equation}
    La partie à l'infini est donc composée de deux points : \( (1:1:0)\) et \( (1:-1:0)\).

    Le graphique de l'équation \eqref{EqdGHzqJ} est donné à la figure \ref{LabelFigProjPoly}. Nous y voyons que les asymptotes sont effectivement données par les directions \( (1,1)\) et \( (1,-1)\) dans le plan.
    \newcommand{\CaptionFigProjPoly}{Le graphique de \( x^2-x-y^2-1=0\).}
    \input{pictures_tex/Fig_ProjPoly.pstricks}
\end{example}

Nous pouvons tenter de faire l'exercice inverse : considérer une conique dans \( \eR^2\), la voir comme une partie d'une conique dans l'espace projectif et trouver les points à l'infini qui la complètent.

\begin{example}
    La droite projective usuelle est donnée par la droite affine \( y-1=0\). L'homogénéisation donne \( y-z=0\) et par conséquent la partie à l'infini est donnée par \( y=0\), c'est à dire la direction \( (1,0)\) comme il se doit.
\end{example}

\begin{example}
    Prenons la conique
    \begin{equation}
        x^2+xy+y^3-2=0.
    \end{equation}
    D'abord nous homogénéisons cette équation pour la voir dans \( \eR^3\) :
    \begin{equation}
        x^2z+xyz+y^3-2z^3=0.
    \end{equation}
    Les points à l'infini sont ceux qui correspondent à \( z=0\), c'est à dire la droite donnée en coordonnées homogènes par \( (1:0:0)\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Repères projectifs}
%---------------------------------------------------------------------------------------------------------------------------

Si nous avons une base \( \{ e_i \}\) de \( \eR^n\) nous associons à \( M\in P(E)\) les coordonnées \( (X:Y:T)\). Mais si on prend la base \( \{ 2e_1,e_2,\ldots, e_n \}\), les coordonnées du même point deviennent \( (X/2:Y:T)\) alors que du point de vue de l'espace projectif, rien n'a été changé : la classe de \( e_1\) est la même que celle de \( 2e_1\). Les coordonnées homogènes ne sont donc pas intrinsèques.

\begin{definition}
    Soit \( E\) un espace vectoriel de dimension \( n+1\). Un \defe{repère projectif}{repère!projectif}\index{projectif!repère} de \( P(E)\) est la donnée de \( n+2\) points \( m_0,\ldots, m_{n+1}\) tels que
    \begin{enumerate}
        \item
            les vecteurs \( m_i\), \( i\neq 0\), sont les images d'une base \( \{ e_i \}\) de \( E\)
        \item
            \( m_0=\pi_E(e_1+e_2+\cdots +e_{n+1})\).
    \end{enumerate}
\end{definition}
Note que si \( m_k=\pi_E(v_k)\) (\( k=0,\ldots, n+1\)), alors tout choix de \( n+1\) vecteurs parmi les \( v_k\) est une base de \( E\).

\begin{lemma}
    Soit \( (m_0,\ldots, m_{n+1})\) un repère projectif de \( P(E)\). Soient \( \{ e_1,\ldots, e_{n+1} \}\) et \( \{ f_1,\ldots, f_{n+1} \}\) deux bases qui se projettent sur \( \{ m_i \}\) (c'est à dire \( \pi_E(e_i)=\pi_E(f_i)=m_i\)). Si \( \pi_E(e_1+\cdots +e_{n+1})=\pi_E(f_1+\cdots +f_{n+1})\) alors les deux bases sont proportionnelles : il existe un \( \lambda\) tel que \( f_i=\lambda e_i\).
\end{lemma}

\begin{theorem}
    Soient \( P(E)\) et \( P(F)\) deux espaces projectifs de dimensions \( n\).
    \begin{enumerate}
        \item
            Une homographie \( P(E)\to P(E)\) envoie un repère projectif sur un repère projectif.
        \item
            Si \( (m_0,\ldots, m_{n+1})\) est un repère projectif de \( P(E)\), si \( (m'_0,\ldots, m'_{n+1})\) est un repère projectif de \( P(F)\) alors il existe une unique homographie \( g\colon P(E)\to P(F)\) telle que \( g(m_i)=m'_i\) pour tout \( i=0,1,\ldots, n+1\)
    \end{enumerate}
\end{theorem}

Si nous avons une droite projective, trois points sont nécessaires pour créer un repère et donc pour construire une homographie de la droite sur elle-même. Soit \( E\) un espace vectoriel de dimension \( 2\) et \( P(E)\) la droite projective qui lui est associée. Soit une homographie \( f\colon P(E)\to P(E)\) et \( \tilde f\colon E\to E\),l'isomorphisme d'espaces vectoriels associé (par \( f\circ\pi_E=\pi_E\circ \tilde f\)). Si \( \{ e_1,e_2 \}\) est une base de $E$ alors l'application \( \tilde f\) a une matrice
\begin{equation}
    A=\begin{pmatrix}
        a_{11}    &   a_{12}    \\ 
        a_{21}    &   a_{22}    
    \end{pmatrix}\in \eM(2,\eK)
\end{equation}
avec \( \det A\neq 0\) parce que \( \tilde f\) est un isomorphisme.

La plupart des points de \( P(E)\) sont représentés par des points de la forme \( (z,1)\). Nous voudrions savoir quelle est la direction représentée par le point \( \tilde f(z,1)\); c'est à dire que nous voudrions savoir \( f([z,1])\) sous la forme \( [z',1]\) (si possible). Nous avons
\begin{equation}
    \tilde f(z,1)=(a_{11}z+a_{12},a_{21}z+a_{22}).
\end{equation}
Nous posons \( \lambda=a_{21}z+a_{22}\) et nous avons
\begin{equation}
    \tilde f(z,1)=\lambda\left( \frac{ a_{11}z+a_{12} }{ \lambda },1 \right).
\end{equation}
Il y a plusieurs possibilités suivant les valeurs de \( \lambda\) et de \( z\).

\begin{enumerate}
    \item
        Si \( \lambda=0\) c'est que nous avons \( \tilde f(z,1)=(a_{11}z+a_{12},0)\). L'application \( f\) envoie donc le point \( (z:1)\) sur le point à l'infini.
    \item
        Si \( \lambda\neq \), alors \( f\) envoie le point \( (z:1)\) vers un autre point «normal».
    \item
        Si le point de départ est le point à l'infini alors \( \tilde f(1,0)=(a_{11},a_{21})\). Cela peut être le point à l'infini ou non selon les valeurs des \( a_{ij}\).
\end{enumerate}

Dans tous les cas si nous posons
\begin{subequations}
    \begin{numcases}{}
        \varphi_f(z)=\frac{ a_{11}z+a_{12} }{ a_{21}z+a_{22} }\\
        \varphi_f(\infty)=\frac{ a_{11} }{ a_{21} }
    \end{numcases}
\end{subequations}
alors nous avons
\begin{equation}
    \tilde f(z,1)=\big( \varphi_f(z),1 \big).
\end{equation}
Si nous prenons la convention que \( \frac{1}{ 0 }=\infty\) et que \( (\infty,0)\) est le point à l'infini, alors cette application \( \varphi_f\) donne bien toutes les valeurs de \( f\), y compris les cas à l'infini.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Birapport}
%---------------------------------------------------------------------------------------------------------------------------

Soit une droite projective \( d\) et trois points distincts \( a\), \( b\) et \( c\) sur cette droite. Étant donné qu'ils forment un repère projectif, il existe une unique homographie 
\begin{equation}
    \begin{aligned}
        g\colon d&\to \eK\cup\{ \infty \} \\
        a&\mapsto \infty \\
        b&\mapsto 0 \\
        c&\mapsto 1. 
    \end{aligned}
\end{equation}
Si \( x\) est un point de \( d\) alors nous nommons le \defe{birapport}{birapport} de \( x\) par rapport à \( a\), \( b\) et \( c\) l'élément
\begin{equation}
    [a,b,c,x]=g(x)
\end{equation}
dans \( \eK\cup\{ \infty \}\).
\begin{enumerate}
    \item
        Les points \( a\), \( b\), \( c\) et \( x\) sont distincts si et seulement si \( [a,b,c,x]\in \eK\setminus\{ 0,1 \}\).
    \item
        Étant donné que les coordonnées homogènes sont bijectives, si \( k\in \eK\cup\{ \infty \}\) alors il existe un unique \( x\in d\) tel que \( [a,b,c,x]=k\).
\end{enumerate}

\begin{theorem}
    Une bijection entre deux droites projectives est une homographie si et seulement si elle conserve le birapport.
\end{theorem}

\begin{proposition}
    Soient \( a,b,c,d\in \eK\cup\{ \infty \}\). Alors
    \begin{equation}        \label{EqIYLFEJ}
        [a,b,c,x]=\frac{ (x-b)/(x-a) }{ (c-b)/(c-a) }
    \end{equation}
    où par convention nous prenons \( 1/0=\infty\).
\end{proposition}

\begin{proof}
    Soit \( D\) une droite projective et les coordonnées \( \{ (1,z) \}\cup\{ \infty \}\) dessus. Nous notons \( \varphi\) la fonction du membre de droite de \eqref{EqIYLFEJ}. L'homographie
    \begin{equation}
        \begin{aligned}
            \colon D&\to \eK\cup\{ \infty \} \\
            z&\mapsto \varphi(z) 
        \end{aligned}
    \end{equation}
    envoie \( a\) sur \( \infty\), \( b\) sur \( 0\) et \( c\) sur \( 1\). C'est donc bien cette homographie que définit le birapport et \( x\mapsto[a,b,c,x]\).
\end{proof}

\begin{lemma}
    Soient \( a\), \( b\), \( c\) distincts sur la droite projective \( D=P(E)\). Soient \( x,y\in E\) tels que \( \pi_E(x)=a\), \( \pi_E(y)=b\), \( \pi_E(x+y)=c\). Alors
    \begin{equation}
        d=\pi_E(\lambda x+\mu y)
    \end{equation}
    si et seulement si
    \begin{equation}
        [a,b,c,d]=\pi_{\eK^2}(\lambda,\mu).
    \end{equation}
    
\end{lemma}

\begin{proof}
    Étant donné que \( a\) et \( b\) sont distincts, les vecteurs \( x\) et \( y\) forment une base de \( E\). Soit \( f\colon E\to \eK^2\) un isomorphisme qui envoie \( (x,y)\) sur \( e_1,e_2\) où \( e_i\) sont les vecteurs de base de \( \eK^2\). Ensuite nous considérons  \( g\colon P(E)\to P(\eK^2)\), l'homographie associée à \( f\). Par définition \( f\big( \pi_Ez \big)=\pi_{\eK^2}\big( f(z) \big)\). Par \( f\) nous avons
    \begin{equation}
        \begin{aligned}[]
            a&\mapsto \begin{pmatrix}
                1    \\ 
                0    
            \end{pmatrix}
            &b&\mapsto\begin{pmatrix}
                0    \\ 
                1    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Donc par \( g\) nous avons
    \begin{equation}
        \begin{aligned}[]
            a&\mapsto\infty&
            b\mapsto 0.
        \end{aligned}
    \end{equation}
    Nous avons aussi \( f(\lambda x+\mu y)=(\lambda,\mu)\) et 
    \begin{subequations}
        \begin{align}
            g(c)&=g\big( \pi_E(x+y) \big)\\
            &=\pi_Ff(x+y)\\
            &=\pi_F(f(x)+f(y))\\
            &=\pi_F\begin{pmatrix}
                1    \\ 
                1    
            \end{pmatrix}\\
            &=1.
        \end{align}
    \end{subequations}
    La dernière inégalité est le fait que la direction \( (1,1)\) dans \( \eR^2\) est représentée par le point \( x=1\) sur la droite \( y=1\) qui est notre «représentation» de la droite affine. L'application \( g\) a donc toutes les propriétés qu'il faut pour être l'application qui définit le birapport. Nous avons donc bien \( g(d)=[a,b,c,d]\).

    D'une part si \( d=\pi_E(\lambda x+\mu y)\) alors
    \begin{equation}
        g(d)=\pi_{\eK^2}f(\lambda x+\mu y)=\pi_{\eK^2}(\lambda,\mu).
    \end{equation}
    Dans l'autre sens si \( [a,b,c,d]=\pi_{\eK^2}(\lambda,\mu)\) alors supposons que \( g(d)=\pi_{\eK^2}(\lambda,\mu)\) avec \( d=\pi_E(v)\) alors
    \begin{equation}
        g\pi_Ev=\pi_{\eK^2}f(v),
    \end{equation}
    ce qui implique \( f(v)=\alpha(\lambda,\mu)\) pour un certain \( \alpha\in \eK\). Par conséquent \( v=\alpha(\lambda x+\mu y)\) et \( d=\pi_E(\lambda x+\mu y)\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sphère de Riemann}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La \defe{sphère de Riemann}{sphère!de Riemann} est l'espace projectif modelé sur \( \eC^2\) : en vertu des notations données à la page \pageref{PgNotimesjNtMoW}, c'est\nomenclature[G]{\( P_1(\eC)\)}{sphère de Poincarré}
\begin{equation}
    P_1(\eC)=P(\eC^2).
\end{equation}
Une homographie de cet espace est une application \( g\) qui vérifie
\begin{equation}
    g\big( \pi(z_1,z_2) \big)=\pi\big( \tilde g(z_1,z_2) \big)
\end{equation}
pour un certain automorphisme de \( \eC^2\). En ce qui concerne \( \pi\), nous pouvons prendre la convention
\begin{equation}
    P_1(\eC)=\{ (z,1),z\in\eC \}\cap\{ (1,0) \}
\end{equation}
et alors avoir la projection
\begin{equation}
    \pi\begin{pmatrix}
        z_1    \\ 
        z_2    
    \end{pmatrix}=\begin{cases}
        \left( \frac{ z_1 }{ z_2 },1 \right)    &   \text{si \( z_2\neq 0\)}\\
        (1,0)=\infty    &    \text{si \( z_2=0\)}.
    \end{cases}
\end{equation}
Par ailleurs un automorphisme de \( \eC^2\) est de la forme
\begin{equation}
    \tilde g\begin{pmatrix}
        z_1    \\ 
        z_2    
    \end{pmatrix}=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\begin{pmatrix}
        z_1    \\ 
        z_2
    \end{pmatrix}
\end{equation}
avec \( ad-bc\neq 0\). Cela induit
\begin{equation}
    g\left( \frac{ z_1 }{ z_2 },1 \right)=\pi\begin{pmatrix}
        az_1+bz_2    \\ 
        cz_1+dz_2    
    \end{pmatrix}=
    \begin{cases}
        \left( \frac{ az_1+bz_2 }{ cz_1+dz_2 },1 \right)    &   \text{si \( cz_1+dz_2\neq 0\)}\\
        \infty    &    \text{sinon}.
    \end{cases}
\end{equation}
Nous avons aussi
\begin{equation}
    g(\infty)=\pi(\tilde g(1,0))=\pi\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\begin{pmatrix}
        1    \\ 
        0    
    \end{pmatrix}=\pi\begin{pmatrix}
        a    \\ 
        c    
    \end{pmatrix}=\begin{cases}
        \frac{ a }{ c }    &   \text{si $ c\neq 0$}\\
        \infty    &    \text{si c=0}.
    \end{cases}
\end{equation}

Étant donné que les coordonnées peuvent être multipliées (\( (a,b)=(\lambda a,\lambda b)\)) nous pouvons récrire l'homographie sous la forme
\begin{equation}
    g(z_1,z_2)=(az_1+bz_2,cz_1+dz_2).
\end{equation}
Nous avons aussi
\begin{equation}
    g^{-1}(\infty)=\{ \pi(z_1,z_2)\tq cz_1+dz_2=0 \}.
\end{equation}

% TODO : Regarder si c'est possible de mettre les représentations de SL(2,C) et SO(1,3)

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Action du groupe modulaire}
%---------------------------------------------------------------------------------------------------------------------------
\index{groupe!modulaire}

Le \defe{demi-plan de Poincaré}{Poincaré (demi-plan)} est l'ensemble
\begin{equation}
    P=\{ z\in \eC\tq \Im(z)>0 \}.
\end{equation}
Le \defe{groupe modulaire}{groupe!modulaire}\index{modulaire (groupe)} est le quotient de groupes
\begin{equation}
    \PSL(2,\eZ)=\frac{ \SL(2,\eZ) }{ \eZ_2 }.
\end{equation}
Ce sont donc les matrices au signe près de la forme
\begin{equation}
    \begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}
\end{equation}
où \( a\), \( b\), \( c\) et \( d\) sont entiers tels que \( ad-cb=1\).

\begin{theorem}[\cite{NsoHIL}] \label{ThoItqXCm}
    Le groupe modulaire agit fidèlement (définition \ref{DefuyYJRh}) sur le demi-plan de Poincaré par
    \begin{equation}    \label{EqVXvwlB}
        \begin{pmatrix}
            a    &   b    \\ 
            c    &   d    
        \end{pmatrix}*z=\frac{ az+b }{ cz+d }.
    \end{equation}
    L'ensemble \(D= D_1\cup D_2\) avec
    \begin{subequations}
        \begin{align}
            D_1&=\{ z\in P\tq | z |>1,\,-\frac{ 1 }{2}\leq \Re(z)<\frac{ 1 }{2} \}\\
            D_2&=\{ z\in P\tq | z |=1,\,-\frac{ 1 }{2}\leq \Re(z)\leq0 \}
        \end{align}
    \end{subequations}
    est un domaine fondamental (définition \ref{DefcSuYxz}) de cette action.

    De plus si nous notons 
    \begin{equation}
        \begin{aligned}[]
            S&=\begin{pmatrix}
                0    &   -1    \\ 
                1    &   0    
            \end{pmatrix},&T&=\begin{pmatrix}
                1    &   1    \\ 
                0    &   1    
            \end{pmatrix},
        \end{aligned}
    \end{equation}
    alors pour tout \( z\in P\), il existe \( A\in \gr(S,T)\) telle que \( A*z\in D\).
    %TODO : faire un dessin
\end{theorem}
\index{groupe!action}
\index{groupe!partie génératrice}
\index{groupe!et géométrie}
\index{racine!de l'unité}
\index{matrice}
\index{homographie}
\index{géométrie!avec nombres complexes}
\index{géométrie!avec des groupes}

\begin{proof}

    Nous divisions la preuve en plusieurs étapes.
    \begin{subproof}
        \item[Bien définie]

            D'abord il faut remarquer que l'action \eqref{EqVXvwlB} est bien définie par rapport au quotient : \( A*z=(-A)*z\). La vérification est immédiate.

        \item[Interne]

            Montrons que si \( A\in \PSL(2,\eZ)\) et \( z\in P\) alors \( A*z\in P\). Nous avons
            \begin{equation}
                A*z=\frac{ az+b }{ cz+d }=\frac{ (az+b)(c\bar z+d) }{ | cz+d |^2 }=\frac{ a| z |c+azd+bc\bar z+bd }{ | cz+d |^2 },
            \end{equation}
            et donc en décomposant \( z=\Re(z)+i\Im(z)\),
            \begin{equation}
                \Im(A*z)=\Im\left( \frac{ azd+bc\bar z }{ | cz+d |^2 } \right)=\frac{ ad-bc }{ | cz+d |^2 }\Im(z)=\frac{ \Im(z) }{ | cz+d |^2 }
            \end{equation}
            où nous avons tenu compte de \( ad-bc=1\). Donc l'action respecte la (stricte) positivité de la partie imaginaire.

        \item[Action]

            Nous vérifions maintenant que la formule donne bien une action : \( A*(B*z)=(AB)*z\). Cela est un bon calcul :
            \begin{subequations}
                \begin{align}
                    A*(B*z)&=A*\left( \frac{ a'z+b }{ c'z+d' } \right)\\
                    &=\frac{ a\left( \frac{ a'z+b }{ c'z+d' } \right)+b }{ c\left( \frac{ a'z+b }{ c'z+d' } \right)+d }\\
                    &=\frac{ a(a'z+b')+b(c'z+d') }{ c(a'z+b')+d(c'z+d') }\\
                    &=\frac{ (aa'+bc')z+(ab'+bd') }{ (ca'+dc')z+(cb'+dd') }\\
                    &=\begin{pmatrix}
                        aa'+bc'    &   ab'+bd'    \\ 
                        a'c+dc'    &   cb'+dd'    
                    \end{pmatrix}*z\\
                    &=(AB)*z.
                \end{align}
            \end{subequations}
        \item[Fidèle]

            Soit \( A\in\PSL(2,\eZ)\) tel que pour tout \( z\in P\) nous ayons
            \begin{equation}
                \frac{ az+b }{ cz+d }=z.
            \end{equation}
            Alors nous avons
            \begin{equation}
                cz^2+(d-a)z+b=0.
            \end{equation}
            Cela est donc un polynôme en \( z\) qui s'annule sur un ouvert\footnote{On ne peut pas dire que \( b=0\) simplement en justifiant qu'on l'obtient en posant \( z=0\) parce que \( z=0\) n'est pas dans le demi-plan de Poincaré.} (le demi-plan de Poincaré). Il doit donc être identiquement nul, donc \( c=b=a-d=0\). Si vous n'y croyez pas, écrivez pour \( z=\epsilon i\) (avec \( \epsilon>0\)) :
            \begin{equation}
                -c\epsilon^2+\epsilon(d-a)i+b=0
            \end{equation}
            pour tout \( \epsilon\). Le fait d'avoir \( c\epsilon^2=b\) pour tout \( \epsilon\) implique que \( c=b=0\). Donc \( A\) est de la forme
            \begin{equation}
                A=\begin{pmatrix}
                    a    &   0    \\ 
                    0    &   d    
                \end{pmatrix},
            \end{equation}
            avec la contrainte supplémentaire que \( ad=1\), les nombres \( a\) et \( b\) étant entiers. Nous avons donc soit \( a=d=1\) soit \( a=d=-1\). Étant donné le quotient par \( \eZ_2\), ces deux possibilités donnent le même élément de \( \PSL(2,\eZ)\).
            

        \item[Les orbites intersectent \( D\)] 

            Soit \( z\in P\). Nous devons trouver \( A\in\PSL(2,\eZ)\) tel que \( A*z\in D\). Nous savons déjà que
            \begin{equation}
                \Im(A*z)=\frac{ \Im(z) }{ | cz+d |^2 }.
            \end{equation}
            Nous notons \( \mO_z\) l'orbite de \( z\) sous le groupe modulaire et nous posons
            \begin{equation}
                I_z=\{ \Im(u)\tq u\in \mO_z \}=\{ \Im(A*z)\tq A\in\PSL(2,\eZ) \},
            \end{equation}
            l'ensemble des parties imaginaires des éléments de l'orbite de \( z\). Nous allons montrer que cet ensemble est borné vers le haut en montrant que la quantité \( | cz+d |\) ne peut, ) \( z\) donné, prendre qu'un nombre fini de valeurs plus grandes que \( \Im(z)\)\footnote{Bien que cela ne soit pas indispensable pour la preuve, remarquons que \( I_z\) ne comprend qu'une quantité au plus dénombrable de valeurs. Le fait que, à \( z\) donné, la quantité \( | cz+d |^2\) puisse être rendue aussi grande que l'on veut est évident. Donc \( I_z\) est borné vers le bas par zéro (qui n'est pas atteint, mais qui est une valeur d'adhérence).}. Nous cherchons donc les couples \( (c,d)\in \eZ^2\) tels que \( | cz+d |<1\). 

            Nous avons \( \Im(cz+d)=c\Im(z)\), donc \( | cz+d |\geq |c \Im(z) |\), mais il n'y a qu'un nombre fini de \( c\in \eZ\) tels que \( | c\Im(z) |<1\). De la même façon, pour la partie réelle nous avons
            \begin{equation}
                \Re(cz+d)=c\Re(z)+d,
            \end{equation}
            et pour chaque \( c\),  il n'y a qu'un nombre fini de \( d\in \eZ\) qui laissent cette quantité plus petite que \( 1\) (en valeur absolue).

            Donc \( I_z\) possède un maximum. Soit \( A_1\in\PSL(2,\eZ)\) tel que \( \Im(A_1*z)=\max I_z\). Nous notons \( z_1=A_1*z\), et que nous n'avons a priori pas l'unicité. Nous allons maintenant agir sur \( z_1\) avec l'élément
            \begin{equation}
                T=\begin{pmatrix}
                    1    &   1    \\ 
                    0    &   1    
                \end{pmatrix}
            \end{equation}
            pour ramener \( z_1\) dans le domaine \( D\). Si \( u\in P\) nous avons \( T*u=u+1\) et donc
            \begin{equation}
                T^n*u=u+n.
            \end{equation}
            Vu que \( D\) est de largeur \( 1\), il existe un \( n\) (éventuellement négatif) tel que 
            \begin{equation}
                \Re(T^n*z_1)\in\mathopen[ -\frac{ 1 }{2} , \frac{ 1 }{2} [.
            \end{equation}
            Notons qu'ici le fait d'être ouvert d'un côté et fermé de l'autre joue de façon essentielle (pour l'unicité aussi). Nous notons \( z_2=T^n*z_1\).
            
            Supposons un instant que \( | z_2 |<1\). Nous considérons l'élément
            \begin{equation}
                S=\begin{pmatrix}
                    0    &   -1    \\ 
                    1    &   0    
                \end{pmatrix}
            \end{equation}
            qui fait
            \begin{equation}
                \Im(S*z)=\frac{ \Im z }{| z |^2}.
            \end{equation}
            Donc si \( | z_2 |<1\) alors \( \Im(S*z_2)>\Im(z_2)\), ce qui contredit la maximalité de \( \Im(z_2)\) dans \( I_z\). Nous en déduisons que \( | z_2 |\geq 1\). Nous en déduisons que \( | z_2 |\geq 1\).

            Si \( | z_2 |>1\), alors \( z_2\in D_1\) et c'est bon. Si \( | z_2 |=1\), alors il faut encore un peu travailler. Si \( z_2\pm 1\) est à l'intérieur du disque, alors en agissant avec \( T\) ou \( T^{-1}\) nous retrouvons la même contradiction que précédemment. En écrivant \( z_2= e^{i\theta}\), nous devons donc avoir \( 2\cos(\theta)\leq 1\) ou encore \( |\Re(z_2)|\leq \frac{ 1 }{2}\). Donc si \( \Re(z_2)\leq 0\) alors \( z_2\in D_2\).

            Le dernier cas à traiter est \( \Re(z_2)\in\mathopen] 0 , \frac{ 1 }{2} \mathclose]\), c'est à dire \( \theta\in \mathopen[ \frac{ \pi }{ 3 } , \frac{ \pi }{2} [\). Dans ce cas l'action avec \( S\) ramène l'angle dans la bonne zone parce que \( S*z=-\frac{1}{ z }\) et donc \( S*(\rho e^{-i\theta})=-\frac{1}{ \rho } e^{-i\theta}\).

            \item[Unicité]

                Nous voulons à présent montrer que si \( z\in D\), alors \( A*z\) n'est plus dans \( D\) (sauf si \( A=\pm\mtu\)). Nous supposons que \( z\in D\) et \( A\in \PSL(2,\eZ)\) soient tels que \( A*z\in D\), et nous prouvons qu'alors soit nous arrivons à une contradiction soit nous arrivons à \( A=\mtu\). Pour cela nous allons décomposer en de nombreux cas.

                \begin{enumerate}
                    \item
                        Nous commençons par \( \Im(A*z)\geq \Im(z)\). Dans ce cas nous avons \( | cz+d |\leq 1\) et en particulier \( | c | |\Im(z) |\leq 1\). Étant donné que le point de \( D\) qui a la partie imaginaire la plus petite est \( -\frac{ 1 }{2}+\frac{ 2 }{ \sqrt{3} }i\), nous trouvons \( | c |\leq 2/\sqrt{3}\). Vu que \( c\) doit être entier, nous avons trois cas : \( c=-1,0,1\).
                        \begin{enumerate}
                            \item
                                Soit \( c=0\). Alors \( A=\begin{pmatrix}
                                    a    &   b    \\ 
                                    0    &   d    
                                \end{pmatrix}\) et la condition de déterminant est \( ad=1\), ce qui signifie \( a=d=1\) (la possibilité \( a=b=-1\) est «éliminée» le quotient par \( \eZ_2\) définissant \( \PSL(2,\eZ)\)). La matrice \( A\) doit alors être de la forme
                                \begin{equation}
                                    A=\begin{pmatrix}
                                        1    &   b    \\ 
                                        0    &   1    
                                    \end{pmatrix}
                                \end{equation}
                                et \( A*z=z+b\). Si \( z\in D\), alors le seul \( z+b\) à être (peut-être) encore dans \( D\) est \( b=0\), mais alors \( A\) est l'identité.

                            \item
                                Soit \( c=1\). Alors la condition \( | cz+d |\leq 1\) nous donne trois possibilités\footnote{Je ne rigolais pas quand je disais qu'on allait avoir de nombreux cas.} : \( d=-1,0,1\).

                                \begin{enumerate}
                                    \item
                                        Si \( d=-1\), alors nous devons avoir \( | z-1 |\leq 1\). Il est instructif de faire un dessin, mais le point d'intersection entre les cercles \( | z |=1\) et \( | z-1 |=1\) est le point \( \frac{ 1 }{2}+\frac{ \sqrt{3} }{2}i\), qui n'est pas dans \( D\). Bref, il n'y a pas de points dans \( D\) vérifiant \( | z-1 |\leq 1\).

                                    \item
                                        Si \( d=1\), alors (et c'est maintenant que la dissymétrie de \( D\) intervient) nous avons le point
                                        \begin{equation}
                                            z=-\frac{ 1 }{2}+\frac{ \sqrt{3} }{2}i
                                        \end{equation}
                                        qui est dans \( D\) et qui vérifie \( | z+1 |\leq 1\). Voyons à quoi ressemble la matrice \( A\) dans ce cas. Son déterminant est \( a-b=1\). Nous écrivons donc
                                        \begin{equation}
                                            A=\begin{pmatrix}
                                                b+1    &   b    \\ 
                                                1    &   1    
                                            \end{pmatrix},
                                        \end{equation}
                                        et en tenant compte du fait que \( z\bar z=| z+1 |=1\), nous calculons
                                        \begin{subequations}
                                            \begin{align}
                                                A*z&=\frac{ (b+1)z+b }{ z+1 }\\
                                                &=\frac{ (bz+z+b)(\bar z+1) }{ | z+1 |^2 }\\
                                                &=z+b+1.
                                            \end{align}
                                        \end{subequations}
                                        La seule façon de ne pas quitter \( D\) est d'avoir \( b=-1\), mais alors nous avons
                                        \begin{equation}
                                            A=\begin{pmatrix}
                                                0    &   -1    \\ 
                                                1    &   1    
                                            \end{pmatrix}
                                        \end{equation}
                                        et \( A*z=z\). Donc au final \( z\) est quand même le seul de son orbite à être dans \( D\).

                                        Notons au passage cette très intéressante propriété du point
                                        \begin{equation}
                                            z_0=-\frac{ 1 }{2}+\frac{ \sqrt{3} }{2}i.
                                        \end{equation}
                                        C'est un point de qui vérifie \( z_0=A*z_0\) pour un élément non trivial \( A\) de \( \PSL(2,\eZ)\). L'existence d'un tel élément est ce qui va nous coûter un peu de sueur pour prouver que \( PSL(2,\eZ)\) est engendré par \( S\) et \( T\).

                                    \item
                                        Le cas \( d=0\) nous fait écrire \(1= \det A=-b\), donc \( b=-1\) et 
                                        \begin{equation}
                                            A=\begin{pmatrix}
                                                a    &   -1    \\ 
                                                1    &   0    
                                            \end{pmatrix}.
                                        \end{equation}
                                        Nous avons alors \( A*z=a-\frac{1}{ z }\). De plus la condition \( | z |\leq 1\) revient à \( | z=1 |\). Pour les nombres complexes de module \( 1\), l'opération \( z\to -1/z\) est la symétrie autour de l'axe des imaginaires purs. Le seul à ne pas sortir de \( D\) est le fameux \( z=-\frac{ 1 }{2}+\frac{ \sqrt{3} }{2}i\), qui revient sur lui-même avec \( a=-1\).
                                \end{enumerate}
                                
                        \end{enumerate}
                        
                        Nous passons à la possibilité \( c=-1\). Dans ce cas la matrice est de la forme
                        \begin{equation}
                            A=\begin{pmatrix}
                                a    &   b    \\ 
                                -1    &   d    
                            \end{pmatrix},
                        \end{equation}
                        et nous revenons au cas \( c=1\) en prenant $-A$ au lieu de \( A\).


                    \item
                        Nous passons au cas \( \Im(A*z)<\Im(z)\). Nous récrivons cette condition avec
                        \begin{equation}
                            \Im(A*z)<\Im\big( A^{-1}*(A*z) \big).
                        \end{equation}
                        Si nous supposons que \( z\) et \( A\) sont tels que \( z\) et \( A*z\) soient tous deux dans \( D\), alors \( z'=A*z\) est un élément de \( D\) tel que
                        \begin{equation}
                            \Im(z')<\Im(A^{-1}*z').
                        \end{equation}
                        Or nous avons vu qu'aucun élément de \( D\) vérifiant cette condition n'existait sans être trivial (celui qui ne bouge pas). Pour cela il suffit d'appliquer tout ce que nous venons de dire avec \( A^{-1}\) au lieu de \( A\).
                \end{enumerate}

            \item[Quelque conclusions]

                Après avoir passé tous les cas en revue, le fameux point \( z_0=-\frac{ 1 }{2}+\frac{ \sqrt{3} }{2}i\) est l'unique point de \( D\) à accepter une matrice non triviale \( A\in \PSL(2,\eZ)\) telle que \( z_0=A*z_0\).

                Nous remarquons aussi que tous les points de \( P\) sont ramenés dans \( D\) par une matrice obtenue comme produit de \( T\), \( S\), \( T^{-1}\) et \( S^{-1}\).

    \end{subproof}
\end{proof}

\begin{corollary}[\cite{SjxoHK}]    \label{CorJQwgNp}
    Les matrices \( S\) et \( T\) génèrent le groupe modulaire au sens où toute matrice de \( \PSL(2,\eZ)\) s'écrit comme
    \begin{equation}
        T^{m_1}S^{p_1}\ldots T^{m_k}S^{p_k}
    \end{equation}
    pour un certain \( k\) et des nombres \( m_i,p_i\in \eZ\). Autrement dit, \( \PSL(2,\eZ)=\gr(S,T)\).
\end{corollary}

\begin{proof}
    Soit \( z\), un point de \( D\) autre que \( z_0\). Alors si \( A\in \PSL(2,\eZ)\) est non trivial nous avons \( A*z\) hors de \( D\). Du coup, comme vu dans la démonstration du théorème \ref{ThoItqXCm}, il existe \( B\in \gr(S,T)\) tel que \( B*(A*z)\in D\). Vu que \( D\) ne contient qu'un seul point de chaque orbite, nous avons
    \begin{equation}
        B*A*z=z,
    \end{equation}
    et donc \( BA=\pm\mtu\), ce qui prouve que\footnote{Dans \( \PSL(2,\eZ)\), nous n'avons pas besoin de mettre \( \pm\) parce qu'il est compris dans la définition.} \( A=B^{-1}\), c'est à dire que \( A\in\gr(S,T)\).
\end{proof}
