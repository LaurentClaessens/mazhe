% This is part of Mes notes de mathématique
% Copyright (c) 2008-2009,2011-2015
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Propriété des solutions}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Dans cette section nous étudions les équations différentielles du type
\begin{subequations}
    \begin{numcases}{}
        y'(t)=f\big( t,y(t) \big)\\
        y(t_0)=y_0
    \end{numcases}
\end{subequations}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Fuite des compacts et explosion en temps fini}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Fuite des compacts\cite{GPRooZkclFA,ZPNooLNyWjX}]
Nous considérons l'équation différentielle 
\begin{subequations}
    \begin{numcases}{}
        y'(t)=f\big( t,y(t) \big)\\
        y(t_0)=y_0
    \end{numcases}
\end{subequations}
où \( f\colon I\times \Omega\to \eR^n\) est continue et \( \Omega\) ouvert dans \( \eR^n\). Soit la solution maximale \( y_M\colon J_M=\mathopen] t_{min} , t_{max} \mathclose[\to \Omega\). Si \( t_{max}<\sup(I)\) alors \( y_M(t)\) sort de tout compact de \( \Omega\) lorsque \( t\to t_{max}\).
\end{theorem}
\index{théorème!fuite des compacts}

\begin{proof}
Soit \( K\) un compact de \( \Omega\) et nous considérons une suite \( (t_m)\) dans \( \mathopen] t_{min} , t_{max} \mathclose[\) telle que \( t_m\to t_{max}\). Si nous supposons que \( y_M(t)\) ne sort pas de \( K\) alors nous avons \( y_M(t_m)\in K\), c'est à dire une suite dans un compact. Quitte à passer à une sous-suite, nous supposons qu'elle est convergente. Soit \( x_1\in K\) la limite \( \lim_{m\to \infty}y_M(t_m)=x_1\).

    Vu que \( t_{max}\in I\), la condition initiale \( y(t_{max})=x_1\) est valide et le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} nous donne une unique solution maximale \( y_P\) définie sur un ouvert \( J_P\) autour de \( t_{max}\).

    Nous allons maintenant construire une solution au problème initial qui contredit la maximalité de \( y_M\). Attention : il n'est pas évident a priori que \( y_P(t)=y_M(t)\) sur l'intersection des domaines. Si c'était évident, la proposition serait démontrée.

    Soit \( \tilde J=J_M\cup J_P\cap\mathopen] t_{min} , +\infty \mathclose[\) et la fonction
        \begin{equation}
            \tilde y(t)=\begin{cases}
                y_M(t)    &   \text{si \( t<t_{max}\)}\\
                y_P(t)    &    \text{si \( t\geq t_{max}\)}.
            \end{cases}
        \end{equation}
        La fonction \( \tilde y\) est continue par construction parce que
        \begin{equation}
            \lim_{t\to t_{max}} y_M(t)=x_1=y_P(t_{max}).
        \end{equation}
        Nous vérifions à présent que \( \tilde y\) est une solution : \( \tilde y'(t_{max})=f\big( t_{max},y(t_{max}) \big)\) :
        \begin{subequations}
            \begin{align}
                \lim_{\epsilon\to 0}\frac{ \tilde y(t_{max}-\epsilon)-\tilde y(t_{max}) }{ \epsilon }&=\lim_{\epsilon\to 0}\frac{ y_M(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=\lim_{\epsilon\to 0}\frac{ y_M(t_{max}-\epsilon)-y_P(t_{max}-\epsilon)+y_P(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=\lim_{\epsilon\to 0}\frac{ y_P(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=y'_P(t_{max}).
            \end{align}
        \end{subequations}
        Donc \( \tilde y\) est solution pour la condition initiale \( \tilde y(t_{max})=x_1\) et coïncide avec \( y_P\) en \( t_{max}\) et avec \( y_M\) avant \( t_{max}\). Donc en réalité \( y_P\), \( y_M\) et \( \tilde y\) sont identiques et cela contredit la maximalité de \( y_M\).
\end{proof}

\begin{corollary}[Explosion en temps fini]      \label{CorGDJQooNEIvpp}
    Soit \( (y_m,J)\) la solution maximale du problème de Cauchy \eqref{XtiXON} :
    \begin{subequations}      
        \begin{numcases}{}
            y'=f(t,y)\\
            y(t_0)=y_0
        \end{numcases}
    \end{subequations}
    avec \( f\colon U=I\times \Omega\to \eR^n\) où \( I\) est ouvert dans \( \eR\) et \( \Omega\) ouvert dans \( \eR^n\). Nous supposons que \( f\) est continue sur \( U\) et localement Lipschitz par rapport à \( y\).
    
    Si \( J=\mathopen] t_{min} , t_{max} \mathclose[\) alors nous avons l'alternative suivante :
    \begin{enumerate}
        \item   \label{ItemOLYYooJVkRfj}
            Soit \( t_{max}=\sup(I)\),
        \item
            soit \( t_{max}<\sup(I)\) et \( \lim_{t\to t_{max}}  \| y(t) \|= \infty\).
    \end{enumerate}

    Le résultat tient aussi \emph{mutatis mutandis} pour \( t_{\min}\).
\end{corollary}

\begin{proof}
    Attention : ceci n'est pas une simple paraphrase de la fuite des compacts. L'information supplémentaire que ce corollaire donne est que la solution sort de tout compact \emph{pour ne plus y retourner}.

    L'hypothèse \( t_{max}<\sup(I)\) signifie que la solution finit d'exister avant que les hypothèses sur \( f\) cessent d'être vraies. C'est à dire que la solution maximale est moindre que ce que nous aurions pu espérer.

Soit \( K\) compact et supposons que que pour tout \( t_0<t_{max}\) il existe \( t\in\mathopen] t , t_{max} \mathclose[\) tel que \( y_M(t)\in K\). Alors cela crée une suite \( t_k\) dans \( J\) telle que \( y_M(t_k)\) est dans \( K\). Comme dans le théorème de la fuite des compacts nous concluons l'impossibilité de la chose.

    Donc pour tout compact \( K\) de \( \Omega\), il existe \( T<t_{max}\) tel que \( y_M(t)\in \Omega\setminus K\) pour tout \( t\in\mathopen[ T , t_{max} [\). En prenant des boules fermées de plus en plus grandes en guise de compacts nous concluons que
        \begin{equation}
            \lim_{t\to t_{max}} \| y_M(t) \|=\infty.
        \end{equation}
\end{proof}

\begin{example}
    Soit l'équation différentielle
    \begin{subequations}
        \begin{numcases}{}
            y'=y(y-1)\sin(yt)\\
            y(0)=\frac{ 1 }{2}.
        \end{numcases}
    \end{subequations}
    La fonction \( f(t,y)=y(y-1)\sin(yt)\) ayant une dérivée bornée partout, est localement Lipschitz et le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} s'applique. Pour toute condition initiale, une solution maximale unique existe.

    Si nous oublions la condition initiale, il est facile de trouver des solutions constantes : \( y'=0\) avec \( y(t)=k\) donne l'équation
    \begin{equation}
        0=k(k-1)\sin(kt).       
    \end{equation}
    Les solutions \( y_1(t)=0\) et \( y_2(t)=1\) sont des solutions existant pour tout \( t\).

    Le graphe de la solution correspondante à la condition initiale \( y(0)=\frac{ 1 }{2}\) ne pouvant pas croiser les graphes de \( y_1\) et \( y_2\), elle est obligée d'exister pour tout \( t\) parce qu'elle ne peut pas exploser en temps fini.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Stabilité de Lyapunov}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefKMCGooOeFKlA}
    Dans le cas de l'équation différentielle \( y'(t)=f\big( y(t),t \big)\) pour \( y\colon \eR\to \eR^n\), un point \( a\in \eR^n\) est un \defe{point d'équilibre}{équilibre!point point une équation différentielle} lorsque la fonction constante \( y(t)=a\) est une solution.

    Le point d'équilibre \( a\in \eR^n\) est \defe{stable}{point!d'équilibre!stable}\index{stabilité!d'un point d'équilibre} si pour tout \( \epsilon>0\), il existe \( \delta>0\) tel que \( \| y(0)-a \|<\delta\) implique \( \| y(t)-a \|<\epsilon\) pour tout \( t\).
\end{definition}

\begin{theorem}[Théorème de stabilité de Lyapunov\cite{MJEooXxBFFY,PAXrsMn,GPRooZkclFA}]    \label{ThoBSEJooIcdHYp}
    Soit l'équation différentielle
    \begin{subequations}    \label{EqZZLBooKBkZkG}
        \begin{numcases}{}
            y'(t)=f(y)\\
            y(0)=y_0
        \end{numcases}
    \end{subequations}
    avec une fonction \( f\colon \eR^n\to \eR^n\) de classe \( C^1\) vérifiant \( f(0)=0\) et \( y_0\in \eR^n\). Nous supposons que l'application linéaire \( df_0\) n'a que des valeurs propres dont la partie réelle est strictement négative. 
    
    Alors
    \begin{enumerate}
        \item   \label{ItemGZEAooAhxuDQi}
            Il existe \( k>0\) tel que si \( \| y_0 \|<k\) alors la solution maximale est définie sur \( \eR\),
        \item   \label{ItemGZEAooAhxuDQii}
            pour le même nombre \( k>0\), si \( \| y_0 \|<k\) alors \( y(t)\stackrel{t\to\infty}{\longrightarrow} 0\) exponentiellement vite,
        \item
            la solution \( y=0\) est un point d'équilibre attractif.
    \end{enumerate}

\end{theorem}
\index{théorème!stabilité de Lyapunov}
\index{stabilité!Lyapunov}

\begin{proof}
    Placer ici une phrase intelligente\footnote{Parce que sinon l'environnement \info{description} qui suit donne un mauvais effet.}.
    \begin{subproof}
        \item[Prolégomène]

    Le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} nous enseigne que l'équation différentielle considérée possède une unique solution maximale (entre autres parce qu'une fonction de classe \( C^1\) est localement Lipschitz) et nous nommons \( J\) l'intervalle sur lequel elle est définie.

\item[Système linéarisé]

    Nous posons \( A=df_0\). La fonction \( y_L(t)= e^{tA}y_0\) est solution du système linéarisé
    \begin{subequations}
        \begin{numcases}{}
            y'(t)=Ay(t)\\
            y(0)=y_0.
        \end{numcases}
    \end{subequations}
    Pour évaluer la norme de \( y_L\) nous utilisons le lemme \ref{LemQEARooLRXEef} : il existe un polynôme \( P\) tel que
    \begin{equation}
        \| y_L(t) \|\leq P\big( | t | \big)\sum_{i=1}^r e^{\real{\lambda_i}t}\| y_0 \|.
    \end{equation}
    Mais par hypothèse, \( \real(\lambda_i)<0\) et si nous posons \( \lambda=\max\{ \real(\lambda_i) \}\) nous avons \( \lambda<0\) et 
    \begin{equation}
        \| y_L(t) \|\leq P\big( | t | \big) e^{\lambda t}\| y_0 \|.
    \end{equation}
    Donc quel que soit \( y_0\) nous avons \( \lim_{t\to \infty} \| y_L(t) \|=0\) c'est à dire \( \lim_{t\to \infty} y_L(t)=0\).

\item[Une forme linéaire]

    Nous définissons la forme bilinéaire suivante sur \( \eR^n\) :
    \begin{equation}
        b(x,y)=\int_0^{\infty}\langle  e^{tA}x,  e^{tA}y\rangle dt.
    \end{equation}
    D'abord cela est bien défini pour tout \( x,y\in \eR^n\) parce que
    \begin{equation}
        \big| \langle  e^{tA}x,  e^{tA}y\rangle  \big|\leq \|  e^{tA}x \|\|  e^{tA}y \|\leq P_1\big( | t | \big)P_2\big( | t | \big) e^{2\lambda t}\| x \|\| y \|,
    \end{equation}
    qui est intégrable entre \( 0\) et \( \infty\) à cause de la décroissance exponentielle\footnote{Proposition \ref{PropBQGBooHxNrrf}.}. Montrons que \( b\) est définie positive. Soit donc \( x\neq 0\) et calculons
    \begin{equation}
        b(x,x)=\int_0^{\infty}\|  e^{tA}x \|^2dt.
    \end{equation}
    Ce qui est dans l'intégrale est forcément (pas strictement) positif pour tout \( t\). Mais si \( x\neq 0\) alors \( \| x \|^2\) est strictement positif et sur un voisinage de \( t=0\) nous avons aussi \( \|  e^{tA}x \|^2\) qui est strictement positif. Ergo \( b(x,x)>0\) dès que \( x\neq 0\), ce qui signifie que \( b\) est strictement définie positive (lemme \ref{LemWZFSooYvksjw}).

    Nous notons \( q\colon V\to \eR\) la forme quadratique associée à \( b\) et aussi la norme qui va avec : \( \| x \|_q=\sqrt{q(x)}\). En ce qui concerne le gradient \( \nabla q\colon V\to V\), nous avons la formule \( \nabla q(x)\cdot y=2b(x,y)\)\cite{MJEooXxBFFY}. En effet, nous utilisons une des nombreuses formules du lemme \ref{LemdfaSurLesPartielles}\footnote{Le fait que \( q\) soit différentiable est simplement le fait que \( b\) soit bilinéaire.} :
    \begin{subequations}
        \begin{align}
            \nabla q(x)\cdot y&=\Dsdd{ q(x+ty) }{t}{0}\\
            &=\Dsdd{ q(x)+t^2q(y)+2tb(x,y) }{t}{0}\\
            &=2b(x,y).
        \end{align}
    \end{subequations}
    Nous avons aussi
    \begin{subequations}
        \begin{align}
            \nabla q(x)\cdot Ax&=2b(x,Ax)\\
            &=2\int_0^{\infty}\langle  e^{tA}x,  e^{tA}Ax\rangle \\
            &=\int_0^{\infty}\frac{ \partial  }{ \partial t }\Big( \langle  e^{tA}x,  e^{tA}x\rangle  \Big)(t)dt\\
            &=\lim_{T\to \infty} \Big[ \langle  e^{tA}x,  e^{tA}x\rangle  \Big]_{t=0}^{t=T}.
        \end{align}
    \end{subequations}
    Mais vu que \( \|  e^{tA}x \|\to 0\), pour \( t\to \infty\) il ne reste que terme \( t=0\) de la différence, c'est à dire
    \begin{equation}    \label{EqUCOGooEFxZSO}
        \nabla q(x)\cdot Ax=2b(x,Ax)=-\| x \|^2.
    \end{equation}
    Étant donné que \( \nabla q(x)\) est le vecteur dirigé vers l'extérieur de l'ellipsoïde de la courbe de niveau de \( q\) au point \( x\), le vecteur \( Ax\) est dirigé vers l'intérieur.

\begin{center}
   \input{Fig_FNBQooYgkAmS.pstricks}
\end{center}


\item[Majoration de \(  q\big( y(t) \big)'  \)]
    Nous posons 
    \begin{equation}
        \begin{aligned}
            r\colon \eR^n&\to \eR^n \\
            x&\mapsto f(x)-Ax. 
        \end{aligned}
    \end{equation}

    Soit \( y\) la solution maximale au problème \eqref{EqZZLBooKBkZkG} que nous pouvons aussi écrire sous la forme
    \begin{equation}
        y'(t)=r\big( y(t) \big)+Ay(t).
    \end{equation}
    Calculons un peu \ldots
    \begin{subequations}    \label{subeqsZCOLooOzTBLr}
        \begin{align}
            q\big( y(t) \big)'&=b\big( y(t),y(t) \big)'\\
            &=2b(y,y')\\
            &=2b\big( y,Ay \big)+2b\big( y,r(y) \big)\\
            &=-\| y \|^2+2b\big( y,r(y) \big)       &\text{\eqref{EqUCOGooEFxZSO} avec \( x=y(t)\)}\\
            &\leq -\| y \|^2+2\| y(t) \|_q\| r\big( y(t) \big) \|_q &\text{Cauchy-Schwarz : \( | b(a,b) |\leq \| a \|_q\| b \|_q\).}
        \end{align}
    \end{subequations}
    Chacun des deux termes peut encore être majoré. En ce qui concerne le premier, par équivalence des normes\footnote{Définition \ref{DefEquivNorm} et théorème \ref{ThoNormesEquiv}.}, il existe une constante \( C\) telle que \( \| y \|\geq C \| y \|_q\). En renommant immédiatement $C^2$ en \( C\), \( \| y \|^2\geq C\| y \|_q^2=Cq(y)\).

    Pour le second, nous allons utiliser la différentiabilité de \( r\) et le théorème des accroissements finis. Vu que \( df_0=A\) nous avons \( dr_0=df_0-A=0\) et de plus \( r\) est de classe \( C^1\) parce que \( f\) l'est. Toutes les normes étant équivalentes\footnote{Théorème \ref{ThoNormesEquiv}.} sur \( \eR^n\) nous pouvons exprimer la continuité de \( dr\) pour la norme \( \| . \|_q\) : si \( \epsilon>0\) est fixé alors il existe \( \alpha>0\) tel que \( \| x \|<\alpha\) implique \( \| dr_x \|_q<\epsilon\). Nous pouvons écrire les accroissements finis\footnote{Théorème \ref{ThoNAKKght}.} pour la fonction \( r\) :
    \begin{equation}    \label{EqIDTHooCsMSVs}
        \| r(x)-r(0) \|_q\leq \sup_{a\in\mathopen[ 0 , x \mathclose]}\| df_a \|\| x \|_q.
    \end{equation}
    La chose facile à remarquer est que \( r(0)=f(0)=0\). En ce qui concerne les choses difficiles, vu que \( dr\) est continue (parce que \( r\) est \( C^1\)) il existe un \( \delta>0\) tel que \( \| dr_a \|_q<\epsilon\) dès que \( a\in B_q(0,\delta)\). Si nous prenons \( \| x \|_q<\delta\) alors cette majoration est valable pour tous les éléments sur lequel est pris le supremum dans la formule \eqref{EqIDTHooCsMSVs}. Donc
    \begin{equation}
        \| r(x) \|_q\leq \epsilon\| x \|_q
    \end{equation}
    tant que \( \| x \|_q\leq \delta\). Par conséquent, tant que \(  \| y(t) \|_q\leq \delta\) nous avons \( \| r\big( y(t) \big) \|\leq \epsilon\| y(t) \|_q\). Nous continuons le calcul \eqref{subeqsZCOLooOzTBLr} :
    \begin{subequations}
        \begin{align}
            q\big( y(t) \big)'&\leq Cq(y)+2\epsilon\| y(t) \|_q^2\\
            &=-(C-2\epsilon)q(y).
        \end{align}
    \end{subequations}
    Si \( \epsilon\) est petit on a \( C-2\epsilon >0 \) et on pose \( \beta=C-2\epsilon\) pour écrire
    \begin{equation}    \label{EqEYJIooHvSBic}
        q\big( y(t) \big)'\leq -\beta q\big( y(t) \big)
    \end{equation}
    tant que \( \| y(t) \|_q<\delta\).
    
\item[Si \( q(y_0)<\delta  \) alors \( q\big( y(t) \big) < \delta \)]

    Nous posons\footnote{\( t_1\) est bien définit et est bien un minimum. J'en veux pour preuve que si \( q\big( y(t_s) \big)=\delta\), on peut prendre le minimum seulement sur les \( t\in\mathopen[ 0 , t_s \mathclose]\); or par continuité \( q\big( y(t) \big)=\delta\) définit un fermé. Bref \( t_1\) est un infimum sur un compact (fermé borné) et donc bien un minimum atteint.}
    \begin{subequations}    \label{subeqsFNPJooERJkxO}
        \begin{align}
            t_1=\min\{ t>0\tq q\big( y(t) \big)=\delta \}\\
            t_2=\max\{ t<0\tq q\big( y(t) \big)=\delta \}.
        \end{align}
    \end{subequations}
    L'inégalité \eqref{EqEYJIooHvSBic} est valable pour \( t=0\), \( t=t_1\) et \( t=t_2\); nous l'écrivons pour \( t_1\) :
    \begin{equation}
        q\big( y(t) \big)'_{t=t_1}\leq-\beta q\big( y(t_1) \big)\leq-\beta \delta<0
    \end{equation}
    Nous avons donc \( q\big( y(t_1) \big)=\delta\) et \( q\big( y(t) \big)'_{t=t_1}<0\). Par conséquent pour tout \( t\) proche de \( t_1\) avec \( 0<t<t_1 \) il y a \( q\big( y(t) \big)>\delta\).


Pour la même raison, prise en \( t=0\) nous avons pour tout \( t\) proche de \( 0\) avec \( t>0\) que \( q\big( y(t) \big)<\delta\). Par continuité de \( t\mapsto q\big( y(t) \big)\) cette fonction doit passer par la valeur \( \delta\) dans \( \mathopen] t_2 , 0 \mathclose[\) et \( \mathopen] 0 , t_1 \mathclose[\), ce qui contredit la maximalité de \( t_2\) et la minimalité de \( t_1\).

    Ci-dessous, une partie de ce à quoi ressemble le graphe de \( t\mapsto q\big( y(t) \big)\) :
\begin{center}
   \input{Fig_ASHYooUVHkak.pstricks}
\end{center}

    Deux conclusions :
    \begin{itemize}
        \item
            Vu que \( q\big( y(t) \big)\) est borné pour tout \( t\in \eR\), nous sommes dans le cas \ref{ItemOLYYooJVkRfj} de l'alternative du théorème d'explosion en temps fini \ref{CorGDJQooNEIvpp}. Donc la solution \( y(t)\) existe sur tout \( \eR\) pourvu que \( \| y_0 \|\) soit assez petit. Plus précisément par équivalence des normes, il existe un nombre \( D>0\) tel que \( \| x \|\geq D\| x \|_q\) pour tout \( x\). Si \( \| y_0 \|\leq D\delta\) alors 
            \begin{equation}
                D\| y_0 \|_q\leq \| y_0 \|\leq D\delta,
            \end{equation}
            qui donne immédiatement \( \| y_0 \|_q\leq \delta\), ce qui faut pour faire fonctionner l'existence de \( y(t)\) pour tout \( t\).
        \item 
            Nous pouvons maintenant d'utiliser l'inégalité \eqref{EqEYJIooHvSBic} pour tout \( t\in \eR\) sous la seule hypothèse que \( q(y_0)<\delta\) au lieu de \( q\big( y(t) \big)<\delta\).
    \end{itemize}

    La partie \ref{ItemGZEAooAhxuDQi} de ce théorème est prouvée; nous passons au reste à la partie \ref{ItemGZEAooAhxuDQii}. Pour cela nous supposons que \( q(y_0)<\delta\).

\item[À propos de \(  e^{\beta t}q(y)\)]

    En sous-entendant la dépendance en \( t\) dans \( y\) nous avons
    \begin{equation}
        \Big(  e^{\beta t}q(y) \Big)'=\beta e^{\beta t}q(y)+ e^{\beta t}q(y)'= e^{\beta t}\big( \beta q(y)+q(y)' \big),
    \end{equation}
    mais nous avons déjà prouvé que \( q(y)'\leq -\beta q(y)\) (équation \eqref{EqEYJIooHvSBic}), donc
    \begin{equation}    \label{EqEJMEooFKuxTv}
        \Big(  e^{\beta t}q(y) \Big)'\leq 0
    \end{equation}

\item[Décroissance exponentielle]
    Si \(t\geq 0\), l'inégalité \eqref{EqEJMEooFKuxTv} donne
    \begin{equation}
        e^{\beta t}q\big( y(t) \big)\leq q(y_0),
    \end{equation}
    c'est à dire
    \begin{equation}
        q\big( y(t) \big)\leq  e^{-\beta t}q(y_0)
    \end{equation}
    lorsque \( t\geq 0\). Par équivalence des normes, nous avons des nombres \( D_1\) et \( D_2\) tels que
    \begin{equation}
        D_1\| x \|_q\leq \| x \|\leq D_2\| x \|_q
    \end{equation}
    pour tout \( x\in \eR^n\). Nous avons donc pour tout \( t\geq 0\) que
    \begin{equation}
        \| y(t) \|\leq D_2\| y(t) \|_q\leq D_2\| y_0 \|_q e^{-\beta t}.
    \end{equation}
    Pour rappel, \( \beta>0\), ce qui prouve la partie \ref{ItemGZEAooAhxuDQii} du théorème.

\item[Point d'équilibre]

    Le point \( y=0\) est point d'équilibre (définition \ref{DefKMCGooOeFKlA}) parce que \( f(0)=0\), donc \( y(t)=0\) fonctionne. Dans ce cas, \( y_0=0\).

\item[Stabilité]

La stabilité est le fait que \( \| y(t) \|_q\leq \delta\) dès que \( \| y_0 \|_q\leq \delta\).
    
\end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Système proie et prédateurs : Lokta-Voltera}
%---------------------------------------------------------------------------------------------------------------------------

Le système de \defe{Lokta-Voltera}{Lokta-Voltera} est l'équation différentielle suivante :
\begin{subequations}
    \begin{numcases}{}
        x'=ax-bxy\\
        y'=-cy+dxy
    \end{numcases}
\end{subequations}
où \( a,b,c,d\) sont des constantes positives et avec la condition \( x(t_0)>0\), \( y(t_0)>0\).

En ce qui concerne l'interprétation des équations\cite{QUMHooCSloAC},
\begin{enumerate}
    \item
        \( x(t)\) est le nombres proies
    \item
        \( y(t)\) est le nombres prédateurs
    \item 
        Les proies ont une reproduction rapide qui mène à une croissance exponentielle en absence de prédation (d'où le terme \( ax\)).
    \item
        Au contraire, les prédateurs meurent (ou migrent) rapidement lorsqu'ils n'ont pas de proies et nous supposons une décroissance exponentielle du nombre de prédateurs en l'absence de proies. D'où le terme \( -cy\) avec le signe négatif.
    \item
        Les termes \( -bxy\) et \( dxy\) sont les termes d'interaction entre le proies et les prédateurs. Ils sont proportionnels à la fréquence de leurs rencontres, lesquelles sont avantageuses pour les prédateurs et problématiques pour les proies.
\end{enumerate}

\begin{theorem}[Lokta-Voltera\cite{PAXrsMn}]            \label{ThoJHCLooHjeCvT}
    Soient des constantes positives \( a,b,c,d\) et le système équations différentielles
    \begin{subequations}
        \begin{numcases}{}
            x'=ax-bxy\\
            y'=-cy+dxy\\
            x(t_0)>0, y(t_0)>0.
        \end{numcases}
    \end{subequations}
 
    Alors
    \begin{enumerate}
        \item
            Les solutions sont positives sur leur domaines.
        \item
            Les solutions existent sur \( \eR\).
        \item
            Les solutions sont périodiques.
    \end{enumerate}
\end{theorem}
\index{théorème!Lokta-Voltera}

\begin{proof}
    Nous divisions la preuve.
    \begin{subproof}
    \item[Comment théorème de Cauchy-Lipschitz s'applique]
        Tel quel, le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} ne s'applique pas parce qu'il demande une condition initiale pour avoir unicité. En ce qui concerne les notations, ce qui est noté «y» dans le théorème est ici le couple \( x,y\) et la fonction \( f\) est alors
        \begin{equation}
            f\big( t,\begin{pmatrix}
                x    \\ 
                y    
            \end{pmatrix}\big)=\begin{pmatrix}
                ax-bxy    \\ 
                -cy+dxy    
            \end{pmatrix}.
        \end{equation}
        C'est une fonction continue localement Lipschitz partout par le lemme \ref{LemCFZUooVqZmpc} et la proposition \ref{PropGIBZooVsIqfY}.
        
        Nous savons cependant que les solutions sont de classe \( C^1\) et que moyennant la donnée d'une condition initiale, la solution est unique.
    \item[Les solutions restent positives]
        Supposons \( x(s)=0\) pour un certain \( s>t_0\). Alors le solution 
        \begin{subequations}
            \begin{numcases}{}
                x(t)=0\\
                y(t)=\exp(-ct)
            \end{numcases}
        \end{subequations}
        est une solution pour \( \mathopen[ t_0 , s+\epsilon \mathclose]\). Par unicité de la solution avec condition initiale \( s(s)=0\), nous avons aussi \( x(t_0)=0\) pour toutes les solutions, ce qui contredit notre condition.

        De la même façon, avoir \( y(s)=0\) donne une solution avec \( y(t)=0\) pour tout \( t\) et donc une contradiction.

    \item[Solutions sur \( \eR\)]

        Nous montrons maintenant que les solutions sont définies sur \( \eR\).

        Nous avons \( x'<ax\), donc pour tout \( t\) où la solution est définie,
        \begin{equation}
            0<x(t)<x(t_0) e^{a(t-t_0)},
        \end{equation}
        c'est à dire que la solution ne peut pas exploser en temps fini\footnote{Voir le corollaire \ref{CorGDJQooNEIvpp}.} : elle est bornée par le haut et le bas. Elle doit donc exister pour tout \( t\in \eR\). Par ailleurs, \( y'<dxy\) donc
        \begin{equation}
            0<y(t)<y(t_0) e^{d\int_{t_0}^{t}x(s)ds}
        \end{equation}
        qui est également contraire à l'explosion en temps fini.

    \item[4 zones : monotonie]

        Nous divisons \( \eR^2\) en quatre zones d'après les signes de \( a-by\) et \( c-dx\). Nous montrons que dans chacune de ces zones, les solutions sont monotones. Prenons par exemple la partie
        \begin{equation}
            \{  (x,y)\in \eR^2\tq   a-by>0 \}\times\{ c-dx<0 \}.
        \end{equation}
        Vu l'équation \( x'=x(a-by)\), tant que \( \big( x(t),y(t) \big)\) est dans cette zone, la fonction \( x'\) a le signe de \( x\) et est donc positive. Donc \( x\) est croissante dans cette zone.

        De la même façon, \( y'=-y(c-dx)\) est \( y'\) a un signe constant dans la zone.

    \item[4 zones : on bouge]

        Nous prouvons à présent qu'une solution ne reste pas dans une zone. 

        \begin{enumerate}
            \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by>0 \}&&\times&& \{ c-dx>0 \}\\
                x'>0&&&&y'<0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Nous avons en particulier \( x'>0\), donc \( x\) est croissante tout en ayant la borne supérieure \(  x<c/d \). Par conséquent \( x\) a une limite que nous appelons \( x_1\in \mathopen[ 0 , \frac{ c }{ d } \mathclose]\).

        De la même façon,; \( y\) est décroissante et bornée vers le bas par zéro. Donc $y$ a une limite que nous notons \( y_1\in\mathopen[ 0 , y(t_0) \mathclose]\). 

        Vu que \( x\) est bornée et de classe \( C^1\) nous avons forcément \( \lim_{t\to \infty} x'(t)=0\). Mais vu que \( x'=ax-bxy\) nous devons avoir
        \begin{equation}
            ax_1-bx_1y_1=0.
        \end{equation}
        Mais ni \( x_1>0\) donc \( a-by_1=0\), ce qui donne \( y_1=\frac{ a }{ b }\) et aussi \( x_1=\frac{ c }{ d }\). Bref, \( y\) est décroissante et tend vers \( a/b\); donc \( y(t_0)>a/b\), ce qui contredit que \( y(t_0)\) soit dans la zone considérée.

        Étant donné que \( x'>0\) et \( y'<0\), la solution sort de la zone pour entrer dans la zone \ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by>0 \}&&\times&& \{ c-dx<0 \}\\
                x'<0&&&&y'>0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Les fonctions \( x\) et \( y\) sont convergentes. Par conséquent \( \ln(y)\) converge aussi et vu que \( x\) est croissante,
        \begin{equation}
            \frac{ y' }{ y }=-c+dx\geq -x+dx(t_0)>0
        \end{equation}
        Cela signifie que \( \ln(y)'\) est toujours positive et bornée par le bas. Cela est impossible si \( y\) est borné. 

        Donc on sort de la zone pour entrer dans \ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by<0 \}&&\times&& \{ c-dx<0 \}\\
                x'<0&&&&y'>0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). 

        Le même type de raisonnement fait passer à la zone\ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by<0 \}&&\times&& \{ c-dx>0 \}\\
                x'<0&&&&y'<0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Encore une fois, cela nous fait sortir de la zone et retourne vers la première zone.
        \end{enumerate}

       À ce moment nous voyons déjà que la relation entre proie et prédateurs, c'est un peu le mythe de Sisyphe

   \item[Une intégrale première]

       Posons la fonction
       \begin{equation}
           H(x,y)=by+dx-a\ln(y)-c\ln(x).
       \end{equation}
       Une simple dérivation montre que  \(  x\mapsto H\big( x(t),y(t) \big) \) est constante. Nous considérons la fonction
       \begin{equation}
           \begin{aligned}
               f\colon \eR&\to \eR \\
               s&\mapsto H\big( \frac{ c }{ d },s \big)
           \end{aligned}
       \end{equation}
       dont la dérivée n'est autre que \( f'(s)=b-\frac{ a }{ s }\). La fonction \( f\) est donc décroissante sur l'intervalle \( \mathopen[ \frac{ a }{ b } , \infty [\) et donc injective. Sur les changements de zones, il existe un \( t_0\) tel que
           \begin{subequations}
               \begin{align}
                   x(t_0)&=\frac{ d }{ c }\\
                   y(t_0)&>0.
               \end{align}
           \end{subequations}
            Pour cette valeur \( t_0\) nous avons alors \( H\big( x(t_0),y(t_0) \big)=  f\big( y(t_0) \big)  \). En posant \( s_0=y(t_0)>0\) nous avons
            \begin{equation}
                H(x_0,y_0)=f(s_0)
            \end{equation}
            et \( f\) étant injective, ce \( s_0\) est la seule valeur de \( s\) à vérifier \( H(x_0,y_0)=f(s)\).

        \item[Conclusion]

            La fonction \( x\) passant d'une zone à l'autre, il existe un \( t_1>t_0\) tel que \( x(t_1)=a/b\). Nous avons évidemment
            \begin{equation}
                H\big( x(t_1),y(t_1) \big)=H(x_0,y_0)
            \end{equation}
            parce que \( H\) est constante le long du mouvement. Cela se traduit par
            \begin{equation}
                H\big( \frac{ a }{ b },y(t_1) \big)=f(s_0),
            \end{equation}
            et donc \( y(t_1)=f(s_0)=y(t_0)\). Avec tout cela nous avons
            \begin{subequations}
                \begin{numcases}{}
                    y(t_1)=y(t_0)\\
                    x(t_1)=x(t_2)=\frac{ a }{ b }.
                \end{numcases}
            \end{subequations}
            Cela est donc un point par lequel la solution repasse. Par unicité de la solution, elle est donc périodique.
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équation du second ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Wronskien}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons ici une équation différentielle de la forme
\begin{equation}    \label{EqJDAAnWY}
    y''(t)+q(t)y(t)=0
\end{equation}
Dans ce point nous allons considérer la fonction \( q\) sans hypothèse de périodicité. L'équation de Hill (sous-section \ref{SubSecDWwVVPa}) sera la même équation, mais en supposant que \( q\) est périodique.

Nous commençons par argumenter que si \( q\) est continue, alors l'ensemble des solutions de l'équation \eqref{EqJDAAnWY} est un espace vectoriel de dimension deux. Pour cela il suffit d'appliquer la méthode de réduction de l'ordre (section \ref{SecWGdleRM}) puis le théorème de dimension pour les systèmes linéaires (théorème \ref{ThoNYEXqxO}). En effet si la fonction \( y_1\) est solution de \eqref{EqJDAAnWY} si et seulement si le vecteur \(Y= \begin{pmatrix}
    y_1    \\ 
    y_2    
\end{pmatrix}\) est solution du système linéaire
\begin{equation}
    Y'(t)=\begin{pmatrix}
        0    &   1    \\ 
        -q(t)    &   0    
    \end{pmatrix}Y(t).
\end{equation}

Soient deux solutions \( y_1\) et \( y_2\) de l'équation différentielle. Le \defe{Wronskien}{Wronskien} de ces deux solutions est le déterminant
\begin{equation}
    W(t)=\begin{vmatrix}
        y_1    &   y_2    \\ 
        y'_1    &   y'_2    
    \end{vmatrix}.
\end{equation}
Si nous considérons l'équation différentielle
\begin{equation}
    y''+py'+qy=0,
\end{equation}
le Wronskien peut être déterminé sans savoir explicitement \( y_1\) et \( y_2\) parce que \( W=y_1y'_2-y'_1y_2\), et en dérivant,
\begin{subequations}
    \begin{align}
        W'&=y_1y_2''+y'_1y'_2-y''_1y_2-y'_1y'_2\\
        &=y_1(-py'_2-qy_2)-(-py'_1-qy_1)y_2\\
        &=-p\begin{vmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{vmatrix},
    \end{align}
\end{subequations}
c'est à dire
\begin{equation}    \label{EqHEMRgM}
    W'=-pW.
\end{equation}
Il suffit donc de savoir une condition initiale pour obtenir une équation différentielle pour \( W\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Avec second membre}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle du second ordre avec un second membre se présente sous la forme
\begin{equation}
	ay''(t)+by'(t)+cy(t)=v(t)
\end{equation}
où $v(t)$ est une fonction donnée. Le truc est de commencer par résoudre l'équation différentielle sans second membre, c'est à dire trouver la fonction $y_H(t)$ telle que
\begin{equation}
	ay''_H(t)+by_H'(t)+cy_H(t)=0.
\end{equation}
Cela se fait en utilisant la méthode du polynôme caractéristique.

Ensuite, il faut trouver une solution particulière $y_P(t)$ de l'équation avec le second membre. Une seule. Pour y parvenir, il faut du doigté et un peu de technique. Il faut faire des essais en fonction de ce à quoi ressemble le $v(t)$ :
\begin{enumerate}

	\item
		Si $v(t)$ est un polynôme, alors il faut essayer un polynôme,

	\item
		Si $v(t)=\cos(\omega t)$ ou bien $v(t)=\sin(\omega t)$, alors essayer $y_P(t)=A\cos(t)+B\sin(\omega t)$,

	\item
		Si $v(t)= e^{\omega t}$, alors essayer $y_P(t)=A e^{\omega t}$.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Équation \texorpdfstring{$y''+q(t)y=0$}{y''+q(t)y=0}}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSyTwyM}


Nous allons donner quelque propriété des solutions de l'équation
\begin{equation}
    y''+qy=0
\end{equation}
en fonction de telle ou telle hypothèse sur \( q\).

\begin{proposition}
    Si \( q\colon \eR^+\to \eR\) est continue et si
    \begin{equation}
        \int_0^{\infty}| q(t) |dt
    \end{equation}
    converge, alors
    \begin{enumerate}
        \item
            toute solution bornée de \( y''+qy=0\) vérifie \( \lim_{t\to \infty} y'(t)=0\),
        \item
            l'équation \( y''+qy=0\) admet des solutions non bornées.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit \( y\) une solution bornée, et intégrons l'équation différentielle entre \( 0\) et \( \infty\) :
    \begin{equation}
        \int_0^{\infty}y''(t)dt=-\int_0^{\infty}q(t)y(t)dt.
    \end{equation}
    La fonction \( y\) étant bornée, l'hypothèse sur \( q\) permet de dire que l'intégrale de droite existe. Par ailleurs,
    \begin{equation}
        \int_0^{\infty}y''=\lim_{a\to \infty}\int_0^ay''=\lim_{a\to \infty}y'(a)-y'(0).
    \end{equation}
    Cela justifie que la limite \( \lim_{t\to \infty} y'(t)\) existe. Posons \( \alpha=\lim_{t\to \infty} y'(t)\) et supposons par l'absurde que \( \alpha\neq 0\). Soit \( \epsilon>0\) et \( \lambda\) assez grand pour que
    \begin{equation}
        \| y'-\alpha \|_{\mathopen[ \lambda , \infty [}<\epsilon.
    \end{equation}
    Soit aussi \( x>\lambda\). Nous avons
    \begin{subequations}
        \begin{align}
            y(x)&=y(\lambda)+\int_{\lambda}^xy'(t)dt\\
            &\geq y(\lambda)\int_{\lambda}^x(\alpha-\epsilon)\\
            &=y(\lambda)+\alpha x-\epsilon\lambda.
        \end{align}
    \end{subequations}
    En prenant la limite des deux côtés on voit que \( y(x)\to \infty\) dès que \( \alpha\neq 0\), ce qui est contraire aux hypothèses. Donc \( \alpha=0\).

    Pour la seconde partie de la proposition, nous devons prouver que l'équation \( y''+qy=0\) possède des solutions non bornées. Si l'équation a seulement des solutions bornées et si \( \{ u,v \}\) est une base de solutions, alors nous avons \( u',v'\to 0\). Si nous reprenons l'équation \eqref{EqHEMRgM} avec \( p=0\) nous savons que dans notre cas le Wronskien satisfait à \( W'=0\), c'est à dire qu'il est constant. Mais vu que \( u\) et \( v\) sont bornées et que les dérivées tendent vers zéro, nous avons \( W(t)\to 0\) et donc \( W(t)=0\).

    Or l'annulation identique du Wronskien contredit que \( \{ u,v \}\) serait une base de solutions. Donc il existe des solutions non bornées.
\end{proof}

\begin{proposition} \label{PropMYskGa}
    Soit l'équation différentielle \( y''+qy=0\). Si \( q\) est \( C^1\), strictement positive et croissante, alors toutes les solutions sont bornées.
\end{proposition}
\index{monotonie}

\begin{proof}
    Soit \( y\) une solution et multiplions l'équation par \( 2y'\) (qui est non nulle par hypothèse) :
    \begin{equation}
        2y'y''+2qy'y=0.
    \end{equation}
    Nous allons intégrer cela en nous souvenant que \( 2y'y''\) est la dérivée de \( (y')^2\). Pour tout \( t>0\) nous avons
    \begin{subequations}
        \begin{align}
            0&=y'(t)^2-y'(0)^2+2\underbrace{\int_0^tq(t)y'(t)y(t)dt}_{\text{par partie}}\\
            &=y'(t)^2-y'(0)^2+2\left( [qy^2]_0^t-\int_0^tq'y^2 \right)\\
        \end{align}
    \end{subequations}
    Le terme qui nous intéresse est celui qui contient \( y(t)\) :
    \begin{equation}
        2q(t)y(t)^2=-y'(t)^2+y'(0)^2+2q(0)y(0)^2+2\int_0^t q'y^2
    \end{equation}
    Nous pouvons majorer \( -y'(t)^2\) par zéro et remplacer toutes les constantes par \( K\) :
    \begin{equation}
        q(t)y(t)^2\leq\int_0^tq'y^2+K=\int_0^t\frac{ q' }{ q }qy^2.
    \end{equation}
    C'est le moment d'utiliser le lemme de Grönwall \ref{LemuBVozy} avec \( \phi=qy^2\) et \( \psi=q'/q\). Les hypothèses de croissance et de positivité ont été posées exprès. Bref, on a
    \begin{subequations}
        \begin{align}
            qy^2&\leq K\exp\left( \int_0^t\frac{ q'(s) }{ q(s) }ds \right)\\
            &=K\exp\left( \ln\frac{ q(t) }{ q(0) } \right)\\
            &=K\frac{ q(t) }{ q(0) }.
        \end{align}
    \end{subequations}
    Notons que \( q(0)\) est strictement positif. Nous déduisons que
    \begin{equation}
        y^2\leq \frac{ K }{ q(0) }
    \end{equation}
    et donc \( y\) est bornée.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Hill}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDWwVVPa}

L'équation \defe{de Hill}{équation!différentielle!Hill} est une équation différentielle de la forme 
\begin{equation}    \label{EqPQMvzEZ}
    y''+qy=0
\end{equation}
où
\begin{enumerate}
    \item
        \( q\in C^1(\eR,\eR)\),
    \item
        \( q\) est paire et \( \pi\)-périodique
\end{enumerate}
Nous nous intéressons aux solutions complexes de cette équation différentielle.

Nous nommons \( W\subset C^2(\eR,\eC)\) l'espace des solutions complexes de l'équation \eqref{EqPQMvzEZ}. Nous savons par ce qui a été dit en \ref{subsecSyTwyM} que cet espace est de dimension deux. De plus avec le hypothèses faites ici sur \( q\), nous savons que les solutions sont de classe $C^3$ parce que si \( y\) est une solution, alors l'équation \( y''=qy\) nous indique que \( y\) est \( C^1\) parce que \( y''\) existe (\( y'\) est dérivable et donc continue). Mais si \( y\) est de classe \( C^1\), alors le membre de droite \( qy\) est \( C^1\) et donc \( y''\) est \( C^1\), ce qui prouve que \( y\) est de classe \( C^3\). La récurrence ne va pas plus loin parce que \( q\) est seulement de classe \( C^1\).

Nous considérons l'application de translation
\begin{equation}
    \begin{aligned}
        T\colon C^2(\eR,\eC)&\to C^2(\eR,\eC) \\
        (Ty)(x)&=y(x+\pi). 
    \end{aligned}
\end{equation}
En utilisant la règle de dérivation de fonctions composées, \( (Ty)'=Ty'\) et \( (Ty)''=Ty''\), de telle sorte que si \( u\) est solution de l'équation \eqref{EqPQMvzEZ}, alors \( Tu\) est également solution. Donc \( W\) est un espace stable par \( T\).

Le théorème \ref{ThoNYEXqxO} nous permet de choisir une base de \( W\) en imposant des conditions. Nous choisissons une base \( \{ y_1,y_2 \}\) telles que
\begin{equation}
    \begin{aligned}[]
        y_1(0)&=1       &&  y_2(0)=0\\
        y'_1(0)&=0      &&  y_2'(0)=1.
    \end{aligned}
\end{equation}
Le théorème \ref{ThoNYEXqxO} nous assure que deux telles solutions existent et qu'elles forment une base de \( W\) parce que \( W\) est de dimension \( 2\).

\begin{lemma}[\cite{WNxwuWc}]   \label{IVLzNaU}
    Avec ce choix de base \( \{ y_1,y_2 \}\) la matrice de \( T\) est donnée par
    \begin{equation}
        T=\begin{pmatrix}
            y_1(\pi)    &   y_2(\pi)    \\ 
            y'_1(\pi)    &   y'_2(\pi)    
        \end{pmatrix}.
    \end{equation}
    De plus la fonction \( y_1\) est paire et la fonction \( y_2\) est impaire.
\end{lemma}

\begin{proof}

Cherchons la matrice de \( T\) dans cette base en associant \( \begin{pmatrix}
    1    \\ 
    0    
\end{pmatrix}\) à \( y_1\) et \( \begin{pmatrix}
    0    \\ 
    1    
\end{pmatrix}\) à \( y_2\). Si \( T=\begin{pmatrix}
    a    &   b    \\ 
    c    &   d    
\end{pmatrix}\), alors
\begin{equation}    \label{EqSZhBPGy}
    Ty_1=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\begin{pmatrix}
        1    \\ 
        0    
    \end{pmatrix}=\begin{pmatrix}
        a    \\ 
        c    
    \end{pmatrix}=ay_1+cy_2.
\end{equation}
En évaluant cela en \( t=0\),
\begin{equation}
    (Ty_1)(0)=ay_1(0)+cy_2(0)=a,
\end{equation}
donc \(a=(Ty_1)(0)=y_1(\pi)\). En dérivant \eqref{EqSZhBPGy}, en tenant compte du fait que \( (Ty_1)'=Ty_1'\) et en évaluant en \( t=0\), nous trouvons de même \( c=y'_1(\pi)\). Puis le même cinéma avec \( y_2\) donne
\begin{equation}
    T=\begin{pmatrix}
        y_1(\pi)    &   y_2(\pi)    \\ 
        y'_1(\pi)    &   y'_2(\pi)    
    \end{pmatrix}.
\end{equation}
    
Passons maintenant à la parité de \( y_1\) et \( y_2\). Nous posons \( \psi(t)=y_1(-t)\). Alors \( \psi'(t)=-y_1'(-t)\) et \( \psi''(t)=y_1''(t)\), tant et si bien que
\begin{equation}
    \psi''(t)+q(t)\psi(t)=y_1''(-t)+q(t)y_1(-t)=0.
\end{equation}
donc \( \psi\) est une solution de l'équation. Mais
\begin{subequations}
    \begin{numcases}{}
        \psi(0)=y_1(0)\\
       \psi'(0)=-y'_1(0)=0,
    \end{numcases}
\end{subequations}
donc \( \psi\) a les mêmes conditions initiales que \( y_1\). Par conséquent \( \psi=y_1\) (par le l'unicité donnée dans le théorème de Cauchy-Lipschitz \ref{ThokUUlgU}) et \( y_1\) est paire. Nous procédons de même en partant de \( \varphi(t)=-y_2(-t)\) pour trouver que \( \varphi=y_2\) et que donc que \( y_2\) est impaire.


\end{proof}
Remémorons nous toutefois, pour calmer toute enthousiasme excessif, que \( T\) dépend de deux solutions et donc de la fonction \( q\) donnée dans l'équation.

\begin{proposition}[\cite{KXjFWKA}] \label{PropGJCZcjR}
    Nous considérons l'équation \( y''+qy=0\) et sa base de solutions \( \{ y_1,y_2 \}\) en suivant les notations données plus haut.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors toutes les solutions de l'équation sont bornées.
        \item
            Si \( | \tr(T) |=2\) alors nous avons une solution non bornée.
        \item
            Si \( |\tr(T)|>2\) alors toutes les solutions de l'équation sont non bornées.
        \item
            Le cas \( | \tr(T) |=2\) se présente si et seulement si \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proposition}
\index{endomorphisme!sous-espace stable}
\index{endomorphisme!diagonalisable}
\index{équation!différentielle!étude qualitative}
\index{équation!différentielle!système}


\begin{proof}
    Remarquons que le déterminant de la matrice \( T\) est égal au Wronskien des solutions \( y_1\) et \( y_2\) calculé en \( t=\pi\). Calculons sa valeur :
    \begin{equation}
        W(y_1,y_2)=\det\begin{pmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{pmatrix}=y_1y'_2-y'_1y'_2.
    \end{equation}
    En dérivant et en remplaçant \( y''_i\) par \( -qy_i\), nous trouvons tout de suite \( W(y_1,y_2)'=0\). Donc le Wronskien est constant et il est facile de le calculer en \( t=0\) :
    \begin{equation}
        W(y_1,y_2)(0)=1-0=1.
    \end{equation}
    Donc pour tout \( t\) nous avons \( W(y_1,y_2)(t)=1\). En particulier
    \begin{equation}
        \det(T)=W(y_1,y_2)(\pi)=1,
    \end{equation}
    et notons au passage que \( T\) est inversible.

    Nous écrivons le polynôme caractéristique de \( T\) sous la forme \( \chi_T=X^2-\tr(T)X+\det(T)\), c'est à dire
    \begin{equation}
        \chi_T=X^2-\tr(T)X+1,
    \end{equation}
    dont le discriminant est \( \Delta=\tr(A)^2-4\).

    Nous passons à présent aux différents points de la proposition.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors \( \Delta<0\) et \( \chi_T\) a deux racines complexes conjuguées que nous notons \( \rho\) et \( \bar\rho\). De plus le produit des racines étant le terme indépendant, \( \rho\bar\rho=1\); en particulier \( | \rho |=| \bar \rho |=1\). Notons \( \{ u,v \}\) une base de vecteurs propres : \( Tu=\rho u\) et \( Tv=\bar \rho v\). Il est vite vu que la fonction \( | u |\) est \( \pi\)-périodique :
            \begin{equation}
                | u |(t+\pi)=| u(t+\pi) |=| (Tu)(t) |=| (\rho u)(t) |=| \rho | | u |(t)=| u |(t).
            \end{equation}
            La fonction \( | u |\) est continue\footnote{La fonction \( u\) elle-même n'est cependant pas garantie d'être périodique.} et périodique ergo bornée. La fonction \( | v |\) est bornée pour la même raison et par linéarité, toutes les fonctions de \( W\) sont bornées.

        \item

            Si \( \tr(T)=\pm 2\), alors \( \Delta=0\) et \( \chi_T\) a une racine réelle double\footnote{Ce qui n'implique pas le fait d'avoir deux vecteurs propres pour cette valeur propre, mais tout de même au moins un, voir l'exemple \ref{ExICOJcFp}.} qui doit être \( \pm 1\). Soit \( u\) un vecteur propre de \( T\) pour la valeur propre \( \pm 1\). Nous avons
            \begin{equation}
                | u |(t+\pi)=| Tu(t) |=| \pm u(t) |,
            \end{equation}
            ce qui prouve encore que \( | u |\) est périodique et donc bornée.

            Notons que nous n'avons pas d'informations sur le fait qu'une autre solution soit ou non bornée.

        \item

            Si \( | \tr(T) |>2\), alors \( \chi_T\) a deux racines réelles distinctes \( r\) et \( r'\) avec \( rr'=1\) (toujours les relations coefficients-racines). En raison de quoi \( r'=r^{-1}\) et quitte à échanger \( r\) et \( r'\) nous supposons \( | r |>1\). L'opérateur est maintenant diagonalisable et nous considérons \( \{ u,v \}\) une base de vecteurs propres pour les valeurs propres \( r\) et \( r'\). Une solution non nulle de l'équation s'écrit donc sous la forme
            \begin{equation}
                y=\alpha u+\beta v
            \end{equation}
            avec \( (\alpha,\beta)\neq (0,0)\).

            \begin{itemize}
                \item Si \( \alpha=0\), alors \( \beta\neq 0\) et nous choisissons une valeur \( t\) telle que \( v(t)\neq 0\). Dans ce cas,
                    \begin{equation}
                        y(t+n\pi)=\beta v(t+n\pi)=\beta(T^nv)(t)=\beta (r')^n v(t),
                    \end{equation}
                    et en faisant \( n\to -\infty\) nous obtenons \( \pm \infty\) suivant le signe de \( \beta\).

                \item Si \( \alpha\neq 0\), alors nous fixons\footnote{Mais pas trop hein; nous aurons encore besoin d'assigner à \( t\) d'autres valeurs dans d'autres théorèmes.} \( t\) tel que \( u(t)\neq 0\). Alors
                    \begin{equation}
                        y(t+n\pi)=\alpha r^nu(t)+\beta (r')^n(t).
                    \end{equation}
                    En faisant \( n\to \infty\), nous avons \( (r')^n\to 0\) tandis que le premier terme tend vers \( \pm\infty\) suivant le signe de \( \alpha\).
            \end{itemize}

        \item

            D'abord le théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ} nous indique que \( \chi_T(T)=0\), c'est à dire que
            \begin{equation}    \label{EqFHVSsUO}
                T^2-\tr(T)T+1=0.
            \end{equation}
            Nous avons déjà mentionné le fait que \( T\) était inversible. Multiplions donc \eqref{EqFHVSsUO} par \( T^{-1}\) :
            \begin{equation}    \label{EqPNyjBOy}
                T+T^{-1}=\tr(T)\mtu_2.
            \end{equation}
            Vu que \( T^{-1}\) est l'endomorphisme \( T^{-1}u(t)=u(t-\pi)\), sa matrice est donnée par
            \begin{equation}
                T^{-1}=\begin{pmatrix}
                    y_1(-\pi)    &   y_2(-\pi)    \\ 
                    y'_1(-\pi)    &   y'_2(-\pi)    
                \end{pmatrix}=\begin{pmatrix}
                    y_1(\pi)    &   -y_2(\pi)    \\ 
                    -y'_1(\pi)    &   y'_2(\pi)    
                \end{pmatrix}
            \end{equation}
            où nous avons utilisé le fait que \( y_1\) était paire et \( y_2\) impaire (lemme \ref{IVLzNaU}). Si nous notons \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   d    
            \end{pmatrix}\), alors \( T^{-1}=\begin{pmatrix}
                a    &   -b    \\ 
                -c    &   d    
            \end{pmatrix}\) et
            \begin{equation}
                T+T^{-1}=\begin{pmatrix}
                    2a    &     0  \\ 
                       0 &   2b    
                \end{pmatrix}.
            \end{equation}
            L'équation \eqref{EqPNyjBOy} donne alors, vu que \( \tr(T)=a+d\),
            \begin{equation}
                \begin{pmatrix}
                    2a    &   0    \\ 
                    0    &   2b    
                \end{pmatrix}=\begin{pmatrix}
                    a+d    &   0    \\ 
                    0    &   a+d    
                \end{pmatrix},
            \end{equation}
            ce qui donne immédiatement \( a=d\). La matrice de \( T\) a donc comme forme \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   a    
            \end{pmatrix}\) et \( \tr(T)=2a\).

            Donc \( \tr(T)=\pm 2\) si et seulement si \( a=\pm 1\) et vu que \( 1=\det(T)=a^2-bc\), nous avons \( a=\pm 1\) si et seulement si \( bc=0\), ce qui signifie exactement \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Différents types d'équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation homogène}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffHomo}

Une équation différentielle \defe{homogène}{équation!différentielle!homogène} est une équation de la forme
\begin{equation}
	y'=f(t,y)
\end{equation}
où $f(\lambda t,\lambda y)=f(t,y)$ pour tout $\lambda\neq 0$.

Elle se présente sous la forme
\begin{equation}
	y'=\frac{ \text{degré $n$ en $t,y$} }{  \text{degré $n$ en $t,y$}  },
\end{equation}
avec pas de $y'$ à droite : juste du $y$ et du $t$.

\begin{lemma}
L'équation $y'=f(t,y)$ est homogène si et seulement si $f(t,y)$ est une fonction de $y/t$ seulement.
\end{lemma}
Pour résoudre l'équation homogène, on pose
\begin{equation}		\label{EqDiffHomoPoser}
	z(t)=\frac{ y(t) }{ t },
\end{equation}
donc $tz=y$, et 
\begin{equation}
	y'(t)=tv'(t)+v(t),
\end{equation}
à remettre dans l'équation de départ.
%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de Bernoulli}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecBernh}

C'est une équation du type
\begin{equation}	\label{EqBerNDiffalp}
	y'=a(t)y+b(t)y^{\alpha}
\end{equation}
où $\alpha\neq 0$ ou $1$. Pour la résoudre, on divise l'équation par $y^{\alpha}$, et on pose $u=y^{1-\alpha}$, et on tombe sur une équation linéaire
\begin{equation}
	u'=(1-\alpha)\big( a(t)u+b(t) \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de \href{http://fr.wikipedia.org/wiki/Jacopo_Riccati}{Riccati}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecRicatti}

C'est une équation de la forme 
\begin{equation}		\label{EqDiffGFeneRicatti}
	y'=a(t)y^2+b(t)y+c(t).
\end{equation}
\index{équation!de Riccati}

En général, on ne peut pas la résoudre, mais si on en connaît \emph{a priori} des solutions particulières, alors on peut s'en sortir.

\begin{enumerate}

\item 
Si on sait que $y_1(t)$ est une solution, alors on pose
\begin{equation}
	y(t)=y_1(t)+\frac{1}{ u(t) },
\end{equation}
et on obtient une équation linéaire
\begin{equation}
	u'=-\big( 2y_1(t)a(t)+b(t) \big)u-a(t).
\end{equation}

\item
Si $y_1$ et $y_2$ sont solutions, alors nous avons $y$ sous forme implicite
\begin{equation}
	\frac{ y-y_1 }{ y-y_2 }=K e^{\int a(t)\big( y_1(t)-y_2(t) \big)dt}.
\end{equation}
\end{enumerate}

Pour résoudre une équation de Ricatti, il faut donc d'abord deviner une ou deux solutions.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation différentielle exacte}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffExacte}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Résolution lorsque tout va bien}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Avant de vous lancer dans les équations différentielles exacte, vous devez lire la section sur les formes différentielles \ref{SecFormDiffRappel}. Une équation différentielle exacte est de la forme $P(t,y)+Q(t,y)y'=0$ que nous allons écrire sous la forme
\begin{equation}		\label{EqExacteDiff}
	P(t,y)dt+Q(t,y)dy=0.
\end{equation}
Nous savons que si $\partial_yP=\partial_tQ$, alors il existe une fonction $f(t,y)$ telle que $Pdt+Qdy=df$. Pour trouver une telle fonction, nous pouvons simplement intégrer la forme $Pdt+Qdy$. En effet, si $\gamma\colon [0,1]\to \eR^2$ est un chemin tel que $\gamma(0)=(0,0)$ et $\gamma(1)=(t,y)$, alors en définissant
\begin{equation}
	f(t,y)=\int_{\gamma}[Pdt+Qdt]=\int_{0}^1\big[ (P\circ\gamma)(u)dt+(Q\circ\gamma)(u) \big]\big( \gamma'(u) \big)du,
\end{equation}
nous avons $df=Pdt+Qdy$. N'importe quel chemin fait l'affaire. Calculons avec $\gamma(u)=(tu,yu)$. La dérivée de ce chemin est donnée par
\begin{equation}
	\gamma'(u)=t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix}.
\end{equation}
Étant donné que $dt\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=a$ et $dy\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=b$, nous avons
\begin{equation}
	\begin{aligned}[]
	f(t,y)&=\int_0^1[Pdt+Qdy]\big( \gamma(u) \big)\left( t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix} \right)du\\
		&=\int_0^1P\big( \gamma(t) \big)tdu+\int_0^1Q\big( \gamma(t) \big)ydu\\
		&=\int_0^1\big[ tP(tu,uy)+yQ(tu,yu) \big]du.
	\end{aligned}
\end{equation}
Nous retrouvons exactement la formule \eqref{EqIMFormI33Fffdd}. Si ça t'étonne, c'est que tu n'as pas compris ;) Dans le cas où nous avons la fonction $f$ qui vérifie $P=\partial_tf$ et $Q=\partial_yf$, l'équation \eqref{EqExacteDiff} devient
\begin{equation}
	\frac{ \partial f }{ \partial t }+\frac{ \partial f }{ \partial y }\frac{ dy }{ dt }=0,
\end{equation}
c'est à dire 
\begin{equation}
	\frac{ d }{ dt }\Big[ f\big( t,y(t) \big) \Big]=0,
\end{equation}
dont la solution
\begin{equation}
	f\big( t,y(t) \big)=C
\end{equation}
donne la solution $y(t)$ sous forme implicite.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Facteur intégrant (quand tout ne va pas bien)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si la forme $Pdt+Qdy$ n'est pas exacte, il n'existe pas de fonction $f$ qui résolve l'affaire. Nous pouvons toutefois essayer de trouver un \defe{facteur intégrant}{facteur!intégrant}. Nous cherchons une fonction $M$ telle que
\begin{equation}
	(MP)dt+(MQ)dy
\end{equation}
soit exacte. Nous cherchons donc $M(t,y)$ telle que $\partial_y(MP)=\partial_t(MQ)$. En utilisant la règle de Leibnitz, nous trouvons l'équation suivante pour $M$ :
\begin{equation}		\label{EqDuFacteurIntegrant}
	M(\partial_yP-\partial_tQ)=Q(\partial_tM)-P(\partial_yM).
\end{equation}
Cette équation est en générale extrêmement difficile à résoudre, mais dans certains cas particuliers, il est possible d'en trouver une solution à tâtons.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Distributions pour les équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTNgeNms}

Nous commençons par définir l'espace \(  C^{\infty}\big( \eR,\swS'(\eR^d) \big)\)\nomenclature[Y]{$C^{\infty}\big( \eR,\swS'(\eR^d) \big)$}{Fonctions à valeurs dans les distributions.} en disant que \( t\mapsto u_t\) est dans cet espace si
\begin{enumerate}
    \item
        pour tout \( t\in \eR\) nous avons \( u_t\in \swS'(\eR^d)\),
    \item
        l'application \( t\mapsto u_t\) est de classe \(  C^{\infty}\).
\end{enumerate}
Pour définir ce que nous entendons par une fonction de classe \( C^k\) à valeurs dans \( \swS'(\eR^d)\) nous nous souvenons de la proposition \ref{PropQAuJstI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Schrödinger}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Équation de Schrödinger\cite{KXjFWKA}]    \label{ThoLDmNnBR}
    Soit \( g\in\swS'(\eR^d)\) et le problème
    \begin{subequations}
        \begin{numcases}{}
            \partial_t\tilde u-i\Delta \tilde u=0   \label{EqIKhGuiq}\\
            u_0=g
        \end{numcases}
    \end{subequations}
    où \( \tilde u\in C^{\infty}\big( \eR,\swD'(\eR^d) \big)\) est lié à \( u\) par la remarque  \ref{RemZYVkHRT}. Alors
    \begin{enumerate}
        \item   \label{ItemVFracYji}
            Il existe une unique solution dans \( C^{\infty}\big( \eR,\swS'(\eR^d) \big)\).
        \item   \label{ItemVFracYjiii}
            Cette solution \( u\) vérifie de plus \( \tilde u\in\swS'(\eR\times \eR^d)\).
    \end{enumerate}
\end{theorem}
\index{Schrödinger}
\index{distribution!équation de Schrödinger}

\begin{proof}
    Nous allons donner explicitement une fonction \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et nous allons vérifier l'équation \eqref{EqIKhGuiq} en testant sur une fonction \( \psi\in\swS'(\eR\times \eR^d)\). Cela prouvera le point \ref{ItemVFracYjiii} ainsi que la partie existence de \ref{ItemVFracYji}. Dans ce qui suit toutes les transformées de Fourier seront par rapport à la variable \( x\in \eR^d\) ou par rapport à \( \xi\). Jamais par rapport à \( t\in \eR\).

    \begin{subproof}
    \item[Existence]
        Pour \( t\in \eR\) nous posons\footnote{En utilisant la définition \eqref{DefTDkrqkA} du produit d'une distribution par une fonction.}
        \begin{equation}
            u_t=\TF^{-1}(f_t\hat g)
        \end{equation}
        où \( f_t\in\swS(\eR^d)\) est la fonction \( f_t(x)= e^{-it\| x \|^2}\). Pour toute fonction \( \varphi\in\swS(\eR^d)\) nous avons
        \begin{equation}
            u_t(\varphi)=(f\hat g)\big( \TF^{-1}(\varphi) \big)=\hat g\big( f\TF^{-1}(\varphi) \big)=g\Big( \TF\big( f\TF^{-1}(\varphi) \big) \Big).
        \end{equation}
        Le fait que \( \TF^{-1}(\varphi)\) soit une fonction Schwartz fait partie de la proposition \ref{PropKPsjyzT}. Pour chaque \( t\) nous avons bien \( u_t\in\swS'(\Omega)\).

        De plus la fonction \( h(t,x)= e^{-it\| x \|^2}(\TF^{-1}\varphi)(x)\) est dans \(  C^{\infty}(\eR\times \eR^d)\), et par conséquent l'application
        \begin{equation}
            t\mapsto \hat g\big( h(t,.) \big)
        \end{equation}
        est également \(  C^{\infty}\) par la proposition \ref{PropBQUOcyw}. Ceci pour dire que \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\). Il faut encore vérifier que cette fonction est bien une solution de notre problème. Nous testons cette équation sur \( \psi\in\swS(\eR\times\eR^d)\). Pour alléger les notations nous posons \( \psi_t\colon x\mapsto \psi(t,x)\) et par conséquent aussi \( (\partial_t\psi_t)(x)=(\partial_t\psi)(t,x)\). Nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=(\partial_t\tilde u-i\Delta\tilde u)(\psi)\\
                &=-\tilde u(\partial_t\psi)-i\tilde u(\Delta\psi)\\
                &=-\int_{\eR}u_t\big( (\partial_t\psi_t)+i(\Delta\psi_t) \big)dt
            \end{align}
        \end{subequations}
        Ici nous nous souvenons du lemme \ref{LemYYjFZSa} qui nous dit que nous pouvons permuter \( \TF^{-1}\) et \( \partial_t\). Et pour l'autre terme il faut utiliser le lemme \ref{LemQPVQjCx} avec \( | \alpha |=2\) et une somme pour obtenir que
        \begin{equation}
            \widehat{\Delta\varphi}(x)=-\| x \|^2\hat\varphi(x),
        \end{equation}
        qui dans notre cas s'écrit sous la forme
        \begin{equation}
            \TF^{-1}\Big( (\Delta\psi_t) \Big)(x)=-\| x \|^2\TF^{-1}\psi(t,x).
        \end{equation}
        En remettant bout à bout,
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}(f_t\hat g)\Big( (\partial_t-i\| . \|^2)\TF^{-1}\psi_t \Big)dt\\
                &=-\int_{\eR}\hat g\Big( x\mapsto  e^{-it\| x \|^2}(\partial_t-i\| x \|^2)(\TF^{-1}\psi)(t,x) \Big)dt
            \end{align}
        \end{subequations}
        Pour alléger les notations nous notons \( \check{\psi_t}(x)=(\TF^{-1}\psi)(t,x)\). Nous avons
        \begin{equation}
            \partial_t\left(  e^{-it\| x \|^2}\check\psi_t(x) \right)=-i\| x \|^2 e^{-it\| x \|^2}\check{\psi_t}(x)+ e^{-it\| x \|^2}(\partial_t\check{\psi_t}),x)= e^{-it\| x \|^2}\big( \partial_t-i\| x \|^2 \big)\check{\psi_t}(x);
        \end{equation}
        cela nous permet d'un peu factoriser une dérivée dans \( \heartsuit\) :
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}\hat g\left( \partial_t\Big(  e^{-it\| . \|^2}\check{\psi_t}(.) \Big) \right)dt\\
                &=-\int_{\eR}\partial_t\hat g\left(  e^{-it\| . \|^2}\check{\psi_t}(.) \right)dt\\
                &=-\lim_{N\to \infty} \left[ \hat g\Big(  e^{-i\| . \|^2}\check{\psi_t}(.) \Big) \right]_{t=-N}^{t=N}.
            \end{align}
        \end{subequations}
        Histoire de bien comprendre les notations, il ne s'agit pas de calculer \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) général et de remplacer ensuite \( t\) par \( N\) et \( -N\). En effet la valeur de \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) donné est celle qu'on obtient en calculant \( \hat g(\ldots)\) après avoir remplacé \( t\) par ce que l'on veut. Par conséquent, en posant \( \varphi(t,\xi)= e^{-i\| \xi \|^2}\check\psi_t(\xi)\) nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=\lim_{N\to \infty} \left[ g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(t,\xi )d\xi \right) \right]_{t=-N}^{t=N}\\
                &=\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(N,\xi )d\xi \right)-\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(-N,\xi )d\xi \right)
            \end{align}
        \end{subequations}
        La limite commute avec \( g\) parce que cette dernière est une distribution (continue). De plus la limite commute avec l'intégrale parce que ce qui est dedans est Schwartz. La fonction \( \varphi\) étant Schwartz, la limite est nulle. Donc
        \begin{equation}
            \heartsuit=0
        \end{equation}
        et la fonction \( u\) proposée est bien une solution de l'équation de Schrödinger dans \(  C^{\infty}(\eR,\swS'(\eR^d))\).

    \item[Unicité]

        Nous considérons deux solutions \( u_1,u_2\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et la fonction \( u=u_1-u_2\) doit satisfaire au problème
        \begin{subequations}
            \begin{numcases}{}
                (\partial_t\tilde u-i\Delta\tilde u)(\psi)=0\\
                u_0=0.
            \end{numcases}
        \end{subequations}
        Nous allons montrer que seule la fonction \( u_t=0\) peut satisfaire à cela pour tout \( \psi\in\swS(\eR\times \eR^d)\). Nous allons même montrer qu'en imposant ces équations seulement sur la partie de \( \swS(\eR\times\eR^d)\) qui est à support compact par rapport à \( \eR\), la seule solution est \( u_t=0\). Soit donc \( \psi\in\swS(\eR\times \eR^d)\) à support compact vis-à-vis de sa variable \( t\). Alors
        \begin{equation}
            0=-\tilde u(\partial_t\psi+i\Delta\psi)=-\int_{\eR}u_t\Big( (\partial_t\psi_t)+i(\Delta\psi_t) \Big)dt
        \end{equation}
        où encore une fois \( \partial_t\psi_t\) est la fonction \( x\mapsto (\partial_t\psi)(t,x)\). Maintenant nous utilisons la proposition \ref{PropUDkgksG} pour dire que 
        \begin{equation}
            \frac{ d }{ dt }\Big( u_t(\psi_t) \Big)=u^{(1)}_t(\psi_t)+u_t\left( \frac{ \partial \psi }{ \partial t }(t,.) \right)
        \end{equation}
        pour écrire
        \begin{equation}
            0=-\int_{\eR}\frac{ d }{ dt }\big( u_t(\psi_t) \big)-u_{t}^{(1)}(\psi_t)+u_t\big( i(\Delta\psi)(t,.) \big)dt
        \end{equation}
        Le premier terme est facile :
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\Big( u_t(\psi_t) \Big)dt=\lim_{N\to \infty} \Big[ u_t(\psi_t) \Big]_{t=-N}^{t=N}=0
        \end{equation}
        parce que \( \psi\) est à support compact par rapport à \( t\). Nous restons donc avec
        \begin{equation}
            \int_{\eR}u_{t}^{(1)}(\psi_t)-iu_t\big( (\Delta\psi)(t,.) \big)dt=0
        \end{equation}
        Nous traitons le terme en \( u_t^{(1)}\) en utilisant le fait évident \( T(\varphi)=(\TF T)(\TF^{-1}\varphi)\) et en remarquant le lemme \ref{LemWRoRPIX} :
        \begin{equation}
            u_t^{(1)}(\psi_t)=(\TF u_t^{(1)})(\TF^{-1}\psi_t)=(\TF u)_t^{(1)}(\TF^{-1}\psi_t).
        \end{equation}
        Pour l'autre terme on fait un peu la même chose en nous souvenant ce que fait la transformée de Fourier en traversant le laplacien :
        \begin{equation}
            u_t(\Delta\psi_t)=(\TF u_t)(\TF^{-1}\Delta\psi_t)=(\TF u_t)\big( x\mapsto -\| x \|^2(\TF^{-1}\psi_t)(x) \big).
        \end{equation}
        En recollant encore :
        \begin{equation}    \label{EqHOGaGpt}
            \int_{\eR}(\TF u)^{(1)}_t(\TF^{-1}\psi_t)+i(\TF u_t)\big( \| . \|^2\TF^{-1}\psi_t \big)dt=0.
        \end{equation}
        Cette équation est valable tant que \( \psi\in \swS(\eR\times\eR^d)\) avec support compact en \( t\). Nous allons nous en créer une super cool. D'abord nous choisissons \( \varphi\in\swS(\eR^d)\) et \( \chi\in\swD(\eR)\) et nous considérons\footnote{Le candidat qui parvient à effectivement présenter ça comme développement, il est fort.}
        \begin{equation}    \label{EqEVtJcnz}
            \psi(t,x)=\TF\Big( \xi\mapsto  e^{it\| \xi \|^2}\varphi(\xi)\chi(t) \Big)(x).
        \end{equation}
        Notons que la transformée de Fourier conserve le fait qu'une fonction soit Schwartz, mais pas le fait d'avoir support compact. Cependant nous ne prenons que la transformée de Fourier par rapport à \( x\). Le résultat est donc une fonction \( \psi\) qui est Schwartz par rapport à \( x\) et support compact par rapport à \( t\). Nous pouvons donc écrire \eqref{EqHOGaGpt} en utilisant la fonction \eqref{EqEVtJcnz} :
        \begin{equation}    \label{EqHPUyZFz}
            0=\int_{\eR}(\TF u)_t^{(1)}\Big( x\mapsto e^{it\| x \|^2}\varphi(x)\chi(t) \Big)+i(\TF u_t)\Big( x\mapsto\| x \|^2 e^{it\| x \|^2}\varphi(x)\chi(t) \Big)dt.
        \end{equation}
        Là dedans, \( \chi(t)\) peut sortir à la fois de la transformée de Fourier et de l'application des distributions; il doit seulement rester dans l'intégrale. Dans le second terme nous allons utiliser l'égalité (due entre autre à la proposition \ref{PropUDkgksG}) :
        \begin{subequations}    \label{EqCRGfbLU}
            \begin{align}
            \frac{ d }{ dt }\big( \hat u_t( e^{it\| . \|^2}\varphi) \big)&=\frac{ d }{ dt }\left( u_t\big( \TF e^{it\| . \|^2}\varphi \big) \right)\\
            &=u_t^{(1)}\big( \TF  e^{it\| . \|^2}\varphi \big)+u_t\left( \frac{ \partial  }{ \partial t }\TF e^{it\| . \|^2}\varphi \right)\\
            &=(\TF u_t^{(1)})\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big)\\
            &=(\TF u)_t^{(1)}\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big).
            \end{align}
        \end{subequations}
        Et là, magie c'est exactement ce qui est dans \eqref{EqHPUyZFz}. Donc
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\hat u_t\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)\chi(t)dt=0
        \end{equation}
        pour toute fonctions à support compact \( \chi\). Donc la proposition \ref{PropAAjSURG} nous dit que
        % 13107277
        \begin{equation}
            \partial_t\hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=0.
        \end{equation}
        C'est zéro partout et non seulement presque partout parce qu'en plus nous avons la continuité. Par conséquent pour tout \( t\in \eR\) nous avons
        \begin{equation}
            \hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=\hat u_0\big( x\mapsto \varphi(x)\big)=0.
        \end{equation}
        Et cela est vrai pour toute fonction \( \varphi\in\swS(\eR^d)\). Nous considérons donc \( t_0\in \eR\) et une fonction \( \theta\in\swS(\eR^d)\) pour construire
        \begin{equation}
            \varphi(x)= e^{-it_0\| x \|^2}\theta(x).
        \end{equation}
        Nous avons alors \( \hat u_{t_0}\big( x\mapsto\theta(x) \big)=0\), ce qui signifie que \( \hat u_{t_0}=0\). Du coup pour tout \( \theta\in\swS(\eR^d)\) nous avons \( u_{t_0}(\TF\theta)=0\), mais comme la transformée de Fourier est une bijection de \( \swS(\eR^d)\) (proposition \ref{PropKPsjyzT}) nous avons en fait \( u_{t_0}(\theta)=0\) pour tout \( \theta\in\swS(\eR^d)\), c'est à dire \( u_{t_0}=0\) pour tout \( t_0\in \eR\) et au final \( u=0\).
    \end{subproof}
\end{proof}
