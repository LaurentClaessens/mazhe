% This is part of Mes notes de mathématique
% Copyright (c) 2011-2013,2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Action de groupe et connexité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Sources : \cite{MneimneLie} et \wikipedia{fr}{Matrice_normale}{wikipédia}.

\begin{theorem}     \label{ThojrLKZk}
    Soit \( G\) un groupe topologique localement compact et dénombrable à l'infini\footnote{Cela signifie qu'il est une réunion dénombrable de compacts} agissant continument et transitivement sur un espace topologique localement compact \( E\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon G/G_x&\to E \\
            [g]&\mapsto g\cdot x 
        \end{aligned}
    \end{equation}
    est un homéomorphisme.
\end{theorem}

\begin{lemma}       \label{LemkLRAet}
    Si \( G\) et \( H\) sont des groupes topologiques tels que $G/H$ et \( H\) sont connexes\footnote{Définition \ref{DefIRKNooJJlmiD}.}, alors \( G\) est connexe.
\end{lemma}

\begin{proof}
    Soit \( f\colon G\to \{ 0,1 \}\) une fonction continue. Considérons l'application
    \begin{equation}
        \begin{aligned}
            \tilde f\colon G/H&\to \{ 0,1 \} \\
            [g]&\mapsto f(g). 
        \end{aligned}
    \end{equation}
    D'abord nous montrons qu'elle est bien définie. En effet si \( h\in H\) nous aurions \( \tilde f([gh])=f(gh)\), mais étant donné que \( H\) est connexe, l'ensemble \( gH\) est également connexe; la fonction continue \( f\) est donc constante sur \( gH\). Nous avons donc \( f(gh)=f(g)\).

    Étant donné que \( G/H\) est également connexe, la fonction \( \tilde f\) doit être constante. Si \( g_1\) et \( g_2\) sont deux éléments du groupe, nous avons \( f(g_1)=\tilde f([g_1])=\tilde f([g_2])=f(g_2)\). Nous en déduisons que \( f\) est constante et que \( G\) est connexe.
\end{proof}

\begin{theorem}
    Le groupe \( \SO(n)\) est connexe, le groupe \( \gO(n)\) a deux composantes connexes.
\end{theorem}

\begin{proof}
    La seconde assertion découle de la première parce que les matrices de déterminant \( 1\) et celles de déterminant \( -1\) ne peuvent pas être reliées par un chemin continu tandis que l'application
    \begin{equation}
        M\mapsto \begin{pmatrix}
            -1    &       &       \\
                &   1    &       \\
                &       &   1
        \end{pmatrix}M
    \end{equation}
    est un homéomorphisme entre les matrices de déterminant \( 1\) et celles de déterminants \( -1\). Montrons donc que \( G=\SO(n)\) est connexe par arcs pour \( n\geq 2\) en procédant par récurrence sur la dimension.
    
    Nous acceptons le résultat pour $G=\SO(2)$. Notons que nous en avons besoin pour prouver que la sphère \( S^{n-1}\) est connexe.
    
    Le groupe \( \SO(n)\) agit, par définition, de façon transitive sur la sphère \( S^{n-1}\). Soit \( a\in S^{n-1}\), nous avons
    \begin{subequations}
        \begin{align}
            G\cdot a&=S^{n-1}\\
            G_a&\simeq \SO(n-1)
        \end{align}
    \end{subequations}
    où \( G_a\) est le fixateur de \( a\) dans \( G\). Pour montrer le second point, nous considérons \( \{ e_i \}\), la base canonique de \( \eR^n\) et \( M\in G\) telle que \( Ma=e_1\). Le fixateur de \( e_1\) est évidemment isomorphe à \( \SO(n-1)\) parce qu'il est constitué des matrices de la forme
    \begin{equation}
        \begin{pmatrix}
             1   &   0    &   \ldots    &   0    \\
             0   &   a_{11}    &   \ldots    &   a_{1,n-1}    \\
             \vdots   &   \vdots    &   \ddots    &   \vdots    \\ 
             0   &   a_{n-1,1}    &   \ldots    &   a_{n-1,n-1}     
         \end{pmatrix}
    \end{equation}
    où \( (a_{ij})\in \SO(n-1)\). L'application 
    \begin{equation}
        \begin{aligned}
            \alpha\colon G_{e_1} &\to G_{a} \\
            A&\mapsto M^{-1}A M
        \end{aligned}
    \end{equation}
    est un isomorphisme entre \( G_a\) et \( \SO(n-1)\). Le théorème \ref{ThojrLKZk} nous montre alors que, en tant qu'espaces topologiques,
    \begin{equation}
        G/G_a=S^{n-1}.
    \end{equation}
    L'hypothèse de récurrence montre que \( G_a=\SO(n-1)\) est connexe tandis que nous savons que \( S^{n-1}\) est connexe. Le lemme \ref{LemkLRAet} conclut que \( G=\SO(n)\) est connexe.
\end{proof}

\begin{lemma}       \label{LemIbrsFT}
    Une bijection continue entre un espace compact et un espace séparé est un homéomorphisme.
\end{lemma}

\begin{proposition}
    Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes.
\end{proposition}

\begin{proof}
    Soit \( G(n)\) le groupe \( \SU(n)\) ou \( \gU(n)\). Ce groupe opère transitivement sur la sphère complexe
    \begin{equation}
        S_{\eC}^{n-1}=\{ z\in \eC^n\tq \langle z, z\rangle=\sum_k| z_k |^2 =1 \}.
    \end{equation}
    Cet ensemble est le même que \( S^{2n-1}\) parce que \( |z_k|=x_k^2+y_k^2\). Nous avons une bijection continue entre \( S^{n-1}\) et \( S^{n-1}_{\eC}\) et donc un homéomorphisme (lemme \ref{LemIbrsFT}). Soit \( a\in S^{n-1}_{\eC}\), nous avons
    \begin{subequations}
        \begin{align}
            G\cdot a&=S^{n-1}_{\eC}\\
            G_a&\simeq G(n-1).
        \end{align}
    \end{subequations}
    La seconde ligne est un isomorphisme de groupe et un homéomorphisme. Il est donné de la façon suivante. D'abord le fixateur de \( e_1\) dans \( G(n)\) est donné par les matrices de la forme
    \begin{equation}
        \begin{pmatrix}
             1   &   0    &   \ldots    &   0    \\
             0   &   a_{11}    &   \ldots    &   a_{1,n-1}    \\
             \vdots   &   \vdots    &   \ddots    &   \vdots    \\ 
             0   &   a_{n-1,1}    &   \ldots    &   a_{n-1,n-1}     
         \end{pmatrix}
    \end{equation}
    où \( (a_{ij})\in G(n-1)\). Par ailleurs si \( M\) est une matrice de \( G(n)\) telle que \( Ma=e_1\), nous avons l'homéomorphisme
  
    \begin{equation}
        \begin{aligned}
            \alpha\colon G_{e_1}&\to G_a \\
            A&\mapsto M^{-1} AM. 
        \end{aligned}
    \end{equation}
    Encore une fois, cela est un homéomorphisme par le lemme \ref{LemIbrsFT}. Par composition nous avons \( G_a\simeq G(n-1)\) et un homéomorphisme
    \begin{equation}
        G(n)/G_a=S^{n-1}_{\eC}.
    \end{equation}
    Le groupe \( G_a\) et l'ensemble \( S^{n-1}_{\eC}\) étant connexes, le groupe \( G(n)\) est connexe par le lemme \ref{LemkLRAet}.
\end{proof}

\begin{lemma}[\cite{PAXrsMn}]
    Si \( G\) est un sous-groupe connexe de \( \GL(n,\eC)\) alors son groupe dérivé\footnote{Définition \ref{DefVUFBooNQjEdn}.} l'est également.
\end{lemma}
\index{groupe dérivé!de \( \GL(n,\eC)\)}

\begin{proof}
    Soit \( S_m\) l'ensemble des produits de \( m\) commutateurs de \( G\) :
    \begin{equation}
        S_m=\{ g_1,\ldots, g_m\,\text{où les \( g_i\) sont des commutateurs} \}.
    \end{equation}
    La partie \( S_m\) est l'image de \( G\) par l'application continue
    \begin{equation}
        \begin{aligned}
            \underbrace{G\times \ldots\times G}_{\text{\( 2m\) facteurs}}&\to G \\
            (g_1,h_1,g_2,h_2,\ldots, g_m,h_m)&\mapsto [g_1,h_1]\ldots [g_m,h_m] 
        \end{aligned}
    \end{equation}
    En tant qu'image d'un connexe par une application continue, \( S_m\) est connexe par la proposition \ref{PropGWMVzqb}. Vu que les \( S_m\) ont l'identité en commun, le groupe dérivé
    \begin{equation}
        D(G)=\bigcup_{m=1}^{\infty}S_m
    \end{equation}
    est également connexe.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

L'ensemble des matrices est un espace vectoriel. Nous identifions $\eM(n,\eR)$ avec $ \eR^{n^2}$; plus précisément, nous identifions une matrice 
\begin{equation}
    A = (a_{i,j})_{1\leq i \leq n, 1 \leq j \leq n}
\end{equation}
avec le vecteur $x = (x_1, x_2, \dots, x_{n^2}) \in \eR^{n^2}$, où $ a_{i,j} = x_{(n-1)i + j}$. 

\begin{definition}  \label{DefWQNooKEeJzv}
    Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} si il commute avec son adjoint.
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dilatations et transvections}
%---------------------------------------------------------------------------------------------------------------------------

Soit un corps commutatif \( \eK\) et \( n\geq 2\).

\begin{theoremDef}[\cite{PAXrsMn}]     \label{ThoooAZKDooNDcznv}
    Soit une application linéaire \( u\colon E\to E\) dont les points fixes forment un hyperplan noté \( H\) d'équation \( H=\ker(f)\) avec \( f\in E^*\).
    \begin{enumerate}
        \item     \label{ITEMooGTKRooQSPNoI}
            Les affirmations suivantes sont équivalentes :
            \begin{enumerate}
                \item  \label{ITEMooZHYRooFGKaifi}
                    \( \det(u)\neq 1\)
                \item       \label{ooXKLWooTfUMzV}
                    L'application \( u\) est diagonalisable et a une valeur propre qui vaut \( \det(u)\neq 1\).
                \item       \label{ooMZPTooCLylbh}  
                    \( \Image(u-\id)\nsubseteq H\).
                \item   \label{ITEMooZHYRooFGKaifiv}
                    Il existe une base de \( E\) dans laquelle la matrice de \( u\) est \( \diag(1,\ldots, 1,\lambda)\) avec \( \lambda\neq 1\).
            \end{enumerate}
        \item       \label{ITEMooMSJXooUsLCHx}
            Les affirmation suivantes sont équivalentes :
            \let\oldthenumii\theenumi
            \renewcommand{\theenumii}{\roman{enumii}}
            \begin{enumerate}
                \item       \label{ITEMooRTIEooOoWCFsa}
                    Il existe \( a\in H\) tel que pour tout \( x\in E\), \( u(x)=x+f(x)a\).
                \item       \label{ITEMooRTIEooOoWCFsb}
                    Dans une base adaptée, la matrice de \( u\) est donnée par
                    \begin{equation}        \label{EQooFXBDooTgZwMv}
                        \begin{pmatrix}
                             1   &       &       &       \\
                                &   \ddots    &       &       \\
                                &       &   1    &   1    \\ 
                                &       &       &   1     
                         \end{pmatrix}.
                    \end{equation}
            \end{enumerate}
            \let\theenumii\oldtheenumii
        \item
            Les conditions \ref{ITEMooZHYRooFGKaifi}-\ref{ITEMooZHYRooFGKaifiv} sont respectées si et seulement si les conditions \ref{ITEMooRTIEooOoWCFsa}-\ref{ITEMooRTIEooOoWCFsb} ne sont pas respectées (elles sont les négations l'une de l'autre.).
    \end{enumerate}
    Un endomorphisme qui est soit l'identité soit respecte les conditions \ref{ITEMooGTKRooQSPNoI} est une \defe{dilatation}{dilatation}. Un endomorphisme qui est soit l'identité soit qui vérifie les conditions \ref{ITEMooMSJXooUsLCHx} est une \defe{transvection}{transvection} (dans les deux cas il faut que les points fixes forment un hyperplan).
\end{theoremDef}

Notons que selon cette terminologie, l'application \( x\mapsto \lambda x\) n'est pas une dilation mais un produit de dilations.

\begin{proof}
    Nous allons prouver plein d'implications \ldots
    \begin{subproof}
    \item[\ref{ITEMooZHYRooFGKaifi} implique \ref{ooXKLWooTfUMzV}]
        Le théorème de la base incomplete (voir remarque \ref{REMooYGJEooEcZQKa}) permet de considérer une base \( \{ e_1,\ldots, e_n \}\) de \( E\) telle que \( \{ e_1,\ldots, e_{n-1} \} \) soit une base de \( H\). Dans cette base, la matrice de \( u\) est de la forme suivante (les cases non remplies sont nulles et les étoiles correspondent à des valeurs inconnues mais pas spécialement nulles) :
        \begin{equation}        \label{EqooPQOEooGUyIwa}
        \begin{pmatrix}
             1   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   1    &   *    \\ 
                &       &       &   \lambda     
         \end{pmatrix}
        \end{equation}
        Le fait que le déterminant de \( u\) ne soit pas \( 1\) implique que \( \lambda\neq 1\). Par conséquent le polynôme caractéristique
        \begin{equation}
            \chi_u(X)=(1-X)^{n-1}(\lambda-X)
        \end{equation}
        possède une racine \( \lambda\neq 1\), et donc \( u\) possède un vecteur propre \( v\) pour cette valeur\footnote{Proposition \ref{PropooBYZCooBmYLSc}.}. Le vecteur \( v\) est linéairement indépendant de \( \{ e_1,\ldots, e_{n-1} \}\) (parce que vecteur propre de valeur propre différente). Par conséquent l'ensemble \( \{ e_1,\ldots, e_{n-1},v \}\) est une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ItemHIVAooPnTlsBi}. Cela est une base de vecteurs propres et donc une base de diagonalisation\footnote{Nous pourrions en dire à peine plus et prouver le point \ref{ITEMooZHYRooFGKaifiv}, mais cela ne servirait à rien parce que nous voulons prouver les équivalences et qu'il faudra quand même prouver que \ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}.}.
    \item[\ref{ooXKLWooTfUMzV} implique \ref{ooMZPTooCLylbh}]
        Nous nommons maintenant \( \{ e_1,\ldots, e_{n} \}\) la base de diagonalisation. Nous avons \( u(e_n)=\lambda e_n\) avec \( \det(u)=\lambda\neq 1\). Nous avons
        \begin{equation}
            (u-\id)(e_n)=(\lambda-1)e_n\notin H,
        \end{equation}
        ce qui prouver que l'image de \( e_n\) par \( u-\id\) n'est pas dans \( H\).
    \item[\ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}]
        Reprenons une base \( \{ e_1,\ldots, ,e_n \}\) donnant la matrice \eqref{EqooPQOEooGUyIwa}. Il existe \( x\in E\) tel que \( u(x)-x\) n'est pas dans \( H\), c'est à dire tel que \( u\big( u(x)-x \big)\neq u(x)-x\). Nous en déduisons que
        \begin{equation}
            u^2(x)-2u(x)+x\neq 0
        \end{equation}
        ou encore que 
        \begin{equation}
            (X-1)^2(u)x\neq 0.
        \end{equation}
        C'est à dire que \( (X-1)^2\) n'est pas un polynôme annulateur de \( u\). Or ce serait le cas si \( X-1\) était le polynôme minimal (proposition \ref{PropAnnncEcCxj}). Le polynôme caractéristique étant \( (X-1)^{n-1}(X-\lambda)\) (et étant annulateur\footnote{Théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ}.}), le polynôme minimal est de la forme 
        \begin{equation}
            \mu_u(X)=\begin{cases}
                (X-1)(X-\lambda)    &   \text{si \( \lambda\neq 1\)}\\
                X-1    &    \text{si \( \lambda=1\)}.
            \end{cases}
        \end{equation}
        Dans notre cas nous venons de voir que ce n'est pas \( X-1\) et donc c'est \( (X-1)(X-\lambda)\) avec \( \lambda\neq 1\).

        Nous devons trouver une base de diagonalisation \ldots Supposons
        \begin{equation}
            u(e_n)=\sum_{k=1}^{n-1}a_ke_k+\lambda e_n,
        \end{equation}
        dans lequel nous venons de prouver que \( \lambda\neq 1\), et cherchons
        \begin{equation}
            e'_n=\sum\_{j=1}^np_je_j
        \end{equation}
        de telle sorte à avoir \( u(e'_n)=\lambda e_n\). Nous avons
        \begin{subequations}
            \begin{align}
                u(e'_n)&=\sum_{j=1}^{n-1}p_ju(e_j)+p_nu(e_n)\\
                &=\sum_{j=1}^{n-1}(p_j+p_na_j)e_j+p_n\lambda e_n.
            \end{align}
        \end{subequations}
        En égalisant à \( \lambda\sum_{j=1}^np_je_j\), il vient
        \begin{equation}
            p_j+p_na_j=\lambda p_j
        \end{equation}
        pour tout \( j=1,\ldots, n-1\) et la condition triviale \( p_n\lambda=\lambda p_n\) pour \( j=n\). Nous en déduisons que le choix
        \begin{equation}
            p_j=\frac{ p_na_j }{ \lambda-1 }
        \end{equation}
        fonctionne (parce que \( \lambda\neq 1\) comme nous l'avons démontré plus haut). En bref, il suffit de poser
        \begin{equation}
            e'_n=\sum_{j=1}^{n-1}\frac{ p_na_j }{ \lambda-1 }e_j+p_ne_n
        \end{equation}
        avec \( p_n\) au choix pour avoir une base \( \{ e_1,\ldots, e_{n-1},e'_n \}\) de diagonalisation de \( u\) avec \( \lambda\neq 1\) comme dernière valeur propre.
    \item[\ref{ITEMooZHYRooFGKaifiv} implique \ref{ITEMooZHYRooFGKaifi}] Évident \ldots encore qu'il faut invoquer l'invariance du déterminant par changement de base.
    \end{subproof}
    Nous avons terminé la première série d'équivalences. Nous continuons avec la seconde.
       \begin{subproof}
        \item[\ref{ITEMooRTIEooOoWCFsa} implique \ref{ITEMooRTIEooOoWCFsb}]
            Nous prenons \( e_{n-1}=a\) et nous complétons en une base de \( H\). Pour \( e_n\) il suffit de prendre n'importe quel vecteur \( v\) tel que \( f(v)\neq 0\) (qui existe parce que \( f=0\) est seulement un hyperplan), et de le normaliser.

            Dans cette base, la matrice de \( u\) a la forme désirée parce que \( u(e_n)=e_n+f(e_n)a=e_n+e_{n-1}\) du fait que \( e_{n-1}=a\) et \( f(e_n)=1\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique \ref{ITEMooRTIEooOoWCFsa}]
            Soit \( \{ e_1,\ldots, e_n \}\) cette base. En prenant \( a=e_{n-1}\) et en posant \( x=\sum_kx_ke_k\) nous avons
            \begin{equation}
                u(x)=\sum_{k=1}^{n-1}x_ke_k+x_n(e_{n-1}+e_n)=x+x_ne_{n-1}=x_na.
            \end{equation}
            Mais vu que \( f(x)=\sum_if_ix_i\), et que \( f(e_i)=0\) pour tout \( i=1,\ldots, n-1\) nous avons \( f(x)=f_nx_n\). Il n'y a cependant pas de raisons d'avoir \( f_n=1\). Cependant en définissant
            \begin{equation}
                e'_i=\frac{1}{ f_n }e_i
            \end{equation}
            nous avons bien \( u(e'_n)=\frac{1}{ f_n }(e_{n-1}+e_n)=e'_{n-1}+e'_n\). Donc dans cette base nous avons encore la matrice de \( u\) de la forme
            \begin{equation}
                \begin{pmatrix}
                     1   &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                 \end{pmatrix},
            \end{equation}
            mais cette fois avec \( f(e'_n)=1\).
    \end{subproof}
    Nous avons terminé avec la seconde série d'équivalences. Il nous reste à prouver que la première est équivalente à la négation de la seconde.
    \begin{subproof}
        \item[non \ref{ooMZPTooCLylbh} implique \ref{ITEMooRTIEooOoWCFsa}]
            Considérons \( x_0\in E\) tel que \( f(x_0)=1\) et posons \( a=u(x_0)-x_0\in\Image(u-\id)\). Par la négation de \ref{ooMZPTooCLylbh} nous avons \( a\in H\). De plus \( x_0\notin H\) (sinon \( f(x_0)=0\)) donc \( u(x_0)\neq x_0\) et \( a\neq 0\).

            Nous montrons que ce choix de \( a\) fonctionne : \( u(x)=x+f(x)a\) pour tout \( x\in E\). Nous faisons cela séparément pour \( x\in H\) et pour \( x=x_0\). 

            Si \( h\in H\) alors \( u(h)=h\) et \( f(h)=0\) donc \( h+f(h)a=h=u(h)\). Si \( x=x_0\) alors \( u(x_0)=a+x_0\) (cela est la définition de \( a\)) et\( x_0+f(x_0)a=x_0+a\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique non \ref{ITEMooZHYRooFGKaifi}]
           Dans une base adaptée nous avons 
           \begin{equation}
               \begin{pmatrix}
                    1    &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                \end{pmatrix},
           \end{equation}
           et donc \( \det(u)=1\), ce qui contredit \ref{ITEMooZHYRooFGKaifi}.
    \end{subproof}
\end{proof}

\begin{remark}
    Nous notons \( E_{ij}\) la matrice qui possède uniquement \( 1\) en position \( (i,j)\). C'est à dire que \( \big( E_{ij} \big)_{kl}=\delta_{ik}\delta_{jl}\). Soit \( H\) l'hyperplan des points fixes de \( f\). Dans une base contenant une base de $H$, la matrice d'une transvection a pour forme type :
    \begin{equation}        \label{EqooZAKHooBjKlTd}
        T_{ij}(\lambda)=\mtu+\lambda E_{ij}
    \end{equation}
    avec \( i\neq j\) et \( \lambda\in \eK\), et une dilatation a pour forme type la matrice diagonale
    \begin{equation}
        D_i(\alpha)=\mtu+(\alpha-1)E_{ii}
    \end{equation}
    avec \( \alpha\in \eK^*\).

    Bien entendu, en choisissant une base quelconque, les matrices des dilatations et des translations peuvent avoir des formes différentes.
\end{remark}

\begin{lemma}       \label{LemooTQJXooGoIxsI}
    Quelque manipulations de lignes et de colonnes pour les matrices.
    \begin{enumerate}
        \item       \label{ITEMooRWANooPAVjkm}
            La multiplication à gauche par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de ligne
            \begin{equation}
                L_i\to L_i+\lambda L_j.
            \end{equation}
        \item       \label{ITEMooHPSMooWBrSXP}
            La multiplication à droite par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de colonne
            \begin{equation}
                C_j\to C_j+\lambda C_i.
            \end{equation}
        \item       \label{ITEMooXUGFooKcbrxs}
            La multiplication à gauche par \( T_{ij}(1)T_{ji}(-1)T_{ij}(1)\) revient à la substitution de lignes
            \begin{subequations}
                \begin{numcases}{}
                    L_i\to L_j\\
                    L_j\to -L_i.
                \end{numcases}
            \end{subequations}
    \end{enumerate}
\end{lemma}
Note qu'il n'est pas possible d'inverser deux lignes à l'aide de transvections sans changer un signe parce que les transvections sont de déterminant \( 1\) alors que l'inversion de lignes change le signe du déterminant.

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooRWANooPAVjkm}]
            Nous devons prouver que
            \begin{equation}
                \big( T_{ij}(\lambda)A \big)_{kl}=\begin{cases}
                    A_{kl}    &   \text{si \( k\neq i\)}\\
                    A_{il}+\lambda A_{jl}    &    \text{si \( k=i\)}.
                \end{cases}
            \end{equation}
            Un peu de calcul matriciel avec utilisation modérée des indices donner :
            \begin{subequations}
                \begin{align}
                    \big( T_{ij}(\lambda)A \big)_{kl}&=\sum_s\big( T_{ij}(\lambda) \big)_{ks}A_{sl}\\
                    &=\sum_s\delta_{ks}A_{sl}+\lambda\delta_{ik}\delta_{js}A_{sl}\\
                    &=A_{kl}+\lambda\delta_{ik}A_{jl}.
                \end{align}
            \end{subequations}
        \item[Pour \ref{ITEMooHPSMooWBrSXP}] C'est la même chose.
        \item[Pour \ref{ITEMooXUGFooKcbrxs}] Si nous appliquons successivement ces trois matrices (de droite à gauche) nous effectuons les substitutions :
            \begin{equation}
                \begin{aligned}[]
                \begin{cases}
                    L'_i=L_i+L_j\\
                    L'_j=L_j
                \end{cases}
                \text{suivit de }
                \begin{cases}
                    L''_i=L'_i\\
                    L''_j=L'_j-L'_i
                \end{cases}
                \text{et de}
                \begin{cases}
                    L'''_i=L''_i+L''_j\\
                    L'''_j=L''_j.
                \end{cases}
                \end{aligned}
            \end{equation}
            En effectuant ces substitutions,
            \begin{equation}
                L'''_i=L''_i+L''_j=L'_i+(L'_j-L'_i)=L'_j=L_j
            \end{equation}
            et
            \begin{equation}
                L'''_j=L''_j=L'_j-L'_i=L_j-(L_i+L_j)=-L_i,
            \end{equation}
            ce qu'il fallait.
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{ooUWTWooPKySTQ}]      \label{PropooFDNRooWFfUDd}
    Soit \( n\geq 2\) et un corps commutatif \( \eK\).
    \begin{enumerate}
        \item
            Si \( A\in \GL(n,\eK)\), il existe des transvections \( U_1,\ldots, U_r\), \( V_1,\ldots, V_s\) telles que
                \begin{equation}        \label{EQooKSQVooIpkdIE}
                    A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
                \end{equation}
        \item       \label{ITEMooLRYXooSoKRiA}
            L'ensemble des transvections engendre le groupe spécial linéaire \( \SL(n,\eK)\).
        \item
            L'ensemble des transvections et des dilatations engendre le groupe linéaire \( \GL(n,\eK)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous allons montrer que toutes les matrices de \( \SL(n,\eK)\) peuvent être écrites comme produits de matrices de la forme \eqref{EqooZAKHooBjKlTd}. Cela montrera qu'étant donné un endomorphisme \( f\) et une base \emph{pas spécialement liée à \( f\)}, il est possible décrire la matrice de \( f\) comme produit de transvections dont les hyperplans invariants sont «contenus» dans cette base. Cela suffit à prouver que les transvections engendrent \( \SL(n,\eK)\) grâce au lemme \ref{LemFUIZooBZTCiy}.

    Toutes les transvections ont un déterminant égal à \( 1\). Donc le groupe engendré par les transvections est inclus à \( \SL(2,\eK)\). Soit \( A\in\GL(n,\eK)\); nous allons utiliser le pivot de Gauss pour la diagonaliser. Étant donné que \( A\) est inversible, sa première colonne n'est pas nulle. Si \( A_{i1}\neq 0\) alors une multiplication à gauche par \( L_{1i}\big(   (A_{11}-1)/A_{i1}  \big)\) effectue la substitution
    \begin{equation}
        L_1\to L_1-\frac{ A_{11}-1 }{ A_{i1} }L_i
    \end{equation}
    qui met un \( 1\) en la position \( (1,1)\). Notons que si la première colonne est de la forme 
    \begin{equation}
        \begin{pmatrix}
            s    \\ 
            0    \\ 
            \vdots    \\ 
            0    
        \end{pmatrix}
    \end{equation}
    avec \( s\neq 0\) alors il faut plutôt faire les substitutions \( L_2\to L_2+L_1\) et ensuite \( L_1\to L_1-\frac{1}{ s }L_2\) pour obtenir le même résultat. En effectuant le pivot avec \( A_{11}\), une suite d'opérations sur les lignes et les colonnes donnent
    \begin{equation}
        M_1\ldots M_pAN_1\ldots N_q=\begin{pmatrix}
            1    &   0    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( A_1\in\GL(n-1,\eK)\) et \( \det(A_1)=\det(A)\). En continuant de la sorte nous arrivons sur une matrice diagonale\footnote{Attention : les opérations sur les lignes et le colonnes ne sont pas des opérations de similitude. Il n'est pas question de prétendre ici que toutes les matrices de \( \GL(n,\eK)\) sont diagonales, voir la définition \ref{DefBLELooTvlHoB}.}
    \begin{equation}
        M_1\ldots M_{p'}AN_1\ldots N_{q'}=
        \begin{pmatrix}
             1   &       &       &       \\
                &   \ddots    &       &       \\
                &       &   1    &       \\ 
                &       &       &   \alpha     
         \end{pmatrix}
    \end{equation}
    avec \( \alpha=\det(A)\). En d'autres termes nous avons prouvé qu'il existe des transvections \( U_1,\ldots, U_r\) et \( V_1,\ldots, V_s\) telles que
    \begin{equation}        \label{EQooZYYFooQGCgxU}
        A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
    \end{equation}
    Cela prouve que les transvections et les translations engendrent \( \GL(n,\eK)\). Si \( A\in \SL(n,\eK)\) alors \( D_n\big( \det(A) \big)=1\) et l'équation \eqref{EQooZYYFooQGCgxU} est un produit de transvections.
\end{proof}

\begin{proposition}
    Le groupe \( \GL(n,\eR)\) est engendré par les endomorphismes inversibles diagonalisables.
\end{proposition}

\begin{proof}
    Par la proposition \ref{PropooFDNRooWFfUDd}, le groupe \( \GL(n,\eR)\) est engendré par les dilatations et les transvections. Il suffit donc de montrer qu'à leur tour, ces deux types d'endomorphismes sont engendrés par les endomorphismes inversibles et diagonalisables.

    Les dilatations sont diagonalisables et inversibles. Soit une transvection \( u\), et une base \( \{ e_i \}_{i=1,\ldots, n}\) dans laquelle \( u\) est de la forme \eqref{EQooFXBDooTgZwMv}. Nous considérons l'endomorphisme \( d\colon E\to E\) défini par \( d(e_k)=ke_k\). Cet endomorphisme est diagonalisable parce que son polynôme minimal, \( \mu_d=\prod_{k=1}^n(X-k)\), est scindé à racines simples (voir le théorème \ref{ThoDigLEQEXR}).

    Nous avons évidemment \( u= d^{-1}\circ(d\circ u) \) où \( d^{-1}\) est diagonalisable et inversible. Voyons que \( d\circ u\) est également diagonalisable en montrant que \( \mu_d\) est son polynôme minimal (qui est scindé à racines simples).

    Il suffit de montrer que \( \mu_d(d\circ u)(e_k)=0\) pour tout \( k\). Ainsi \( \mu_d\) sera un polynôme annulateur de \( d\circ u\) de degré \( n\), et donc minimal.
    \begin{subproof}
        \item[Si \( k\leq n-1\)]
            Alors \( u(e_k)=e_k\) et \( (d\circ u-n)e_k=(k-n)e_k\). En tout :
            \begin{equation}
                \mu_d(d\circ u)(e_k)=(d\circ u-1)(d\circ u-2)\ldots (d\circ u-n)e_k=(k-1)(k-2)\ldots (k-n)e_k=0
            \end{equation}
            parce que dans le produit des \( k-i\), il y en a forcément un de nul.
        \item[Si \( k=n\)]
            Dans un premier temps,
            \begin{equation}
                (d\circ u-n)e_n=d(e_n+e_{n-1})-ne_n=ne_n+(n-1)e_{n-1}-ne_n=(n-1)e_{n-1}.
            \end{equation}
            Ensuite 
            \begin{subequations}
                \begin{align}
                    \big( d\circ u-(n-1) \big)e_{n-1}&=d(e_{n-1})-(n-1)e_{n-1}\\
                    &=d(e_{n-1})-(n-1)e_{n-1}\\
                    &=(n-1)e_{n-1}-(n-1)e_{n-1}\\
                    &=0
                \end{align}
            \end{subequations}
    \end{subproof}
    Le polynôme \( \mu_d\) est donc un polynôme scindé à \( n\) racines simples annulateur de \( d\circ u\), qui est alors diagonalisable et inversible (parce que \( u\) et \( d\) le sont).

    Donc sous la forme \( u=d^{-1}(du)\), la transvection \( u\) est écrite comme produit de diagonalisables inversibles.
\end{proof}

\begin{proposition}[\cite{LoFdlw}]
    Soit \( n\geq 3\) et \( \eK\) un corps de caractéristique différente de \( 2\). Alors
    \begin{enumerate}
        \item
            le groupe dérivé de \( D(\GL(n,\eK))\) est \(\SL(n,\eK)\);  \index{groupe!dérivé!de \( \GL(n,\eK)\)}
        \item
            le groupe dérivé de \( \SL(n,\eK)\) est \( \SL(n,\eK)\).\index{groupe!dérivé!de \( \SL(n,\eK)\)}
    \end{enumerate}
\end{proposition}
La preuve utilise le fait que les transvections engendrent \( \SL(n,\eK)\) et que les transvections avec les dilatations engendrent \( \GL(n,\eK)\). Voir la proposition \ref{PropooFDNRooWFfUDd}.
%TODO : faire une preuve de cela. C'est dans l'écrit d'algèbre de 2013.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Connexité  de certains groupes}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}           \label{LEMooIPOVooZJyNoH}
    Le groupe \( \gO(n),\eR\) n'est pas connexe.
\end{lemma}

\begin{proof}
    La non connexité par arcs est facile parce que les éléments de déterminant \( 1\) ne peuvent pas être reliés aux éléments de déterminant \( -1\) par un chemin continu restant dans \( \gO(n)\) à cause du théorème des valeurs intermédiaires \ref{ThoValInter}.

    En ce qui concerne la connexité, il faut en dire un peu plus.

    Les éléments de \( \gO(n,\eR)\) ont des déterminants égaux à \( 1\) ou à \( -1\). Ces deux parties sont des ouverts (pour la topologie induite de \( \eM(n,\eR)\)). En effet soit \( A\in\SO(n,\eR)\) (la partie contenant les déterminants \( 1\); ce que l'on va dire tient pour l'autre partie). Alors, vu que le déterminant est une fonction continue sur \( \eM(n,\eR)\) il exist un voisinage \( \mO\) de \( A\) dans \( \\eM(n,\eR)\) dans lequel le déterminant reste entre \( \frac{ 1 }{2}\) et \( \frac{ 3 }{2}\) (c'est la définition de la continuité avec \( \epsilon=1/2\)). L'ensemble \( \mO\cap\gO(n,\eR)\) est par définition un ouvert de \( \gO(n,\eR)\) et ne contient que des éléments de déterminant \( 1\).

    La partie \( \gO(n,\eR)\) de \( \\eM(n,\eR)\) est donc non-connexe selon la définition \ref{DefIRKNooJJlmiD}.
\end{proof}

\begin{lemma}       \label{LEMooQMXHooZQozMK}
    Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes par arcs.
\end{lemma}

\begin{proof}
    Soit \( A\), une matrice unitaire et \( Q\) une matrice unitaire qui diagonalise \( A\). Étant donné que les valeurs propres arrivent par paires complexes conjuguées,
    \begin{equation}
        QAQ^{-1}=\begin{pmatrix}
            e^{i\theta_1}    &       &       &       &   \\  
            &    e^{-i\theta_1}    &       &       &   \\  
            &       &    \ddots    &       &   \\  
            &       &       &    e^{i\theta_r}    &   \\  
            &       &       &       &        e^{-i\theta_r}
        \end{pmatrix}.
    \end{equation}
    Le chemin \( U(t)\) obtenu en remplaçant \( \theta_i\) par \( t\theta_i\) avec \( t\in\mathopen[ 0 , 1 \mathclose]\) joint \( QAQ^{-1}\) à l'identité. Par conséquent \( Q^{-1}U(t)Q\) joint \( A\) à l'unité.
\end{proof}

\begin{proposition}     \label{PROPooYKMAooCuLtyh}
    Le groupe \( \SO(n)\) est connexe.
\end{proposition}

\begin{theorem}
    Les matrices \wikipedia{fr}{Endomorphisme_normal}{normales}\footnote{Définition \ref{DefWQNooKEeJzv}.} forment un espace connexe par arc.
\end{theorem}

\begin{proof}
    Soit \( A\) une matrice normale, et \( U\) une matrice unitaire qui diagonalise \( A\). Nous considérons \( U(t)\), un chemin qui joint \( \mtu\) à \( U\) dans \( \gU(n)\). Pour chaque \( t\), la matrice
    \begin{equation}
        A(t)=U(t)^{-1} AU(t)
    \end{equation}
    est normale. Nous avons donc trouvé un chemin dans les matrices normales qui joint \( A\) à une matrice diagonale. Il est à présent facile de la joindre à l'identité.

    Toutes les matrices normales étant connexes à l'identité, l'ensemble des matrices normales est connexe.
\end{proof}

\begin{proposition}     \label{PROPooALQCooLZCKrH}
    Le groupe \( \SL(n,\eK)\) est connexe par arcs.
\end{proposition}

\begin{proof}
    Soit \( A\in \SL(n,\eK)\); par la proposition \ref{PropooFDNRooWFfUDd}\ref{ITEMooLRYXooSoKRiA} nous pouvons écrire
    \begin{equation}
        A=\prod_{c\in X}T_c(\lambda_c)
    \end{equation}
    où \( X\) est une partie de l'ensemble des couples \( (i,j)\) dans \( \{ 1,\ldots, n \}\). En posant
    \begin{equation}
        \begin{aligned}
            \varphi\colon \mathopen[ 0 , 1 \mathclose]&\to \SL(n,\eK) \\
            t&\mapsto \prod_{c\in X}T_c(t\lambda_c) 
        \end{aligned}
    \end{equation}
    nous avons une application continue de \( A\) vers \( \mtu\), dont pour tout \( t\) la matrice \( \varphi(t)\) est inversible de déterminant\( 1\).

    Donc tous les éléments de \( \SL(n,\eK)\) peuvent être reliés à \( \mtu\). Donc \( \SL(n,\eK)\) est connexe par arcs.
\end{proof}

\begin{proposition}[\cite{ooGKOIooXKUQKk}]\label{PROPooVJNIooMByUJQ}
    Le groupe \( \GL(n,\eC)\) est connexe par arcs.
\end{proposition}

\begin{proof}
    Soit \( A\in\GL(n,\eC)\) et sa décomposition \eqref{EQooKSQVooIpkdIE}. Comme fait précédemment, chacune des transvections peut être reliée à \( \mtu\) par un chemin continu dans \( \SL(n,\eC)\). En ce qui concerne le facteur de translation,  nous ne pouvons pas simplement prendre le chemin donné par \( t\mapsto D_n\big( t\det(A) \big)\) parce que le résultat n'est pas inversible en \( t=0\).

    Vu que \( C^*\) il existe une application continue \( \alpha\colon \mathopen[ 0 , 1 \mathclose]\to \eC^*\) telle que \( \alpha(0)=\det(A)\in \eC^*\) et \( \alpha(1)=1\). Il suffit alors de prendre \( D_n\big( \alpha(t) \big)\) et nous avons un chemin continu de \( A\) vers \( \mtu\) restant dans \( \GL(n,\eC)\).
\end{proof}

\begin{proposition} \label{PROPooBIYQooWLndSW}
    Le groupe \( \GL(n,\eR)\) a exactement deux composantes connexes par arcs.
\end{proposition}
\index{connexité!le groupe \( \GL^+(n,\eR)\)}

\begin{proof}
    Nous notons \( \GL^+(n,\eR)\) et \( \GL^-(n,\eR)\) les parties de \( \GL(n,\eR)\) formées des applications de déterminant \( \pm1\) respectivement. Vu le théorème des valeurs intermédiaires (théorème \ref{ThoValInter}), il n'existe pas d'applications continues dans \( \GL(n,\eR)\) reliant \( \GL^+(n,\eR)\) à \( \GL^-(n,\eR)\) tout en restant dans les applications de déterminant non nul\footnote{Si \( \varphi\colon \mathopen[ 0 , 1 \mathclose]\to \GL(n,\eR)\) est le chemin, la fonction à mettre dans le théorème des valeurs intermédiaires est la fonction \( f\colon \mathopen[ 0 , 1 \mathclose]\to \eR\) \(t\mapsto \det\big( \varphi(t) \big)\).}.

    Montrons que \( \GL^{\pm}(n,\eR)\) sont connexes par arcs. Si \( A\in\GL^+(n,\eR)\) alors grâce à la décomposition \eqref{EQooKSQVooIpkdIE}, il existe un chemin continu de \( A\) vers \( D_n\big( \det(A) \big)\). Vu que \( \eR^{\pm}\) sont connexes par arc, il est possible de relier \( D_n\big( \det(A) \big)\) à \( D_n(\pm 1)\) par un chemin continu.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Densité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropDigDensVxzPuo}
    Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\).
\end{proposition}
\index{densité!matrices diagonalisables dans \( \eM(n,\eC)\)}

\begin{proof}
    D'après le lemme de Schur \ref{LemSchurComplHAftTq}, une matrice de \( \eM(n,\eC)\) est de la forme
    \begin{equation}
        A=Q\begin{pmatrix}
            \lambda_1    &   *    &   *    \\
              0  &   \ddots    &   *    \\
            0    &   0    &   \lambda_n
        \end{pmatrix}Q^{-1}.
    \end{equation}
    Les valeurs propres sont sur la diagonale. La matrice est diagonalisable si les éléments de la diagonales sont tous différents. Il suffit maintenant de considérer \( n\) suites \( (\epsilon^{(r)}_k)_{k\in\eN}\) convergentes vers zéro telles que pour chaque \( k\) les nombres \( \lambda_r+\epsilon^{(r)}_k\) soient tous différents. La suite de matrices
    \begin{equation}
        A_k=Q\begin{pmatrix}
            \lambda_1+\epsilon^{(1)}_k    &   *    &   *    \\
              0  &   \ddots    &   *    \\
              0    &   0    &   \lambda_n+\epsilon^{(n)}_k
        \end{pmatrix}Q^{-1}.
    \end{equation}
    est alors diagonalisable pour tout \( k\) et nous avons \( \lim_{k\to \infty} A_k=A\).
\end{proof}

\begin{proposition} \label{PropQGUPooVudelJ}
    Les matrices inversibles sont denses dans l'ensemble des matrices. C'est à dire que \( \GL(n,\eR)\) est dense dans \( \eM(n,\eR)\).
\end{proposition}
\index{densité!de \( \GL(n,\eR)\) dans \( \eM(n,\eR)\)}

\begin{proof}
    Soit \( A\in \eM(n,\eR)\); le lemme de Schur réel \ref{LemSchureRelnrqfiy} nous permet d'écrire
    \begin{equation}
        A=Q
        \begin{pmatrix}
            \lambda_1    &       &       &       &   \\  
                &   \ddots    &       &       &   \\  
                &       &  \lambda_r     &       &   \\  
                &  &     &   \begin{pmatrix}
                    a    &   b    \\ 
                    c    &   d    
                \end{pmatrix}&          \\  
                &       &       &       &   \ddots    
        \end{pmatrix}
        Q^{-1}
    \end{equation}
    avec \( Q\) orthogonale.

    Pour définir \( A_k\) nous remplaçons \( \lambda_i\) par \( \lambda_i+\epsilon^{(i)_k}\) de façon à avoir \( \epsilon^{(i)}_k\to 0\) et \( \lambda_i+\epsilon^{(i)}_k\neq 0\). En ce qui concerne les blocs, ceux dont le déterminant est non nul, nous n'y touchons pas, et ceux dont le déterminant est nul, nous remplaçons \( a\) par \( a+\epsilon_k\).

    Avec cela, \( QA_kA^{-1}\) est une suite dans \( \GL(n,\eR)\) qui converge vers \( A\).
\end{proof}

\begin{proposition}
    Si \( A\in\eM(n,\eC)\) alors
    \begin{equation}
        e^{\tr(A)}=\det( e^{A}).
    \end{equation}
\end{proposition}

\begin{proof}
    Ici, \( e^A\) est l'exponentielle soit d'endomorphisme soit de matrice définie par la proposition \ref{PropPEDSooAvSXmY}.

    Le résultat est un simple calcul pour les matrices diagonalisable. Si \( A\) n'est pas diagonalisable, nous considérons une suite de matrices diagonalisables \( A_k\) dont la limite est \( A\) (proposition \ref{PropDigDensVxzPuo}). La suite
    \begin{equation}
        a_k= e^{\tr(A_k)}
    \end{equation}
    converge vers \(  e^{\tr(A)}\) tandis que la suite 
    \begin{equation}
        b_k=\det( e^{A_k})
    \end{equation}
    converge vers \( \det( e^{A})\). Mais nous avons \( a_k=b_k\) pour tout \( k\); les limites sont donc égales.
\end{proof}

\begin{theorem}[Cayley-Hamilton\cite{QATooFIHVMw,MOSooRVRrHw}]  \label{ThoHZTooWDjTYI}
    Tout endomorphisme d'un espace vectoriel de dimension finie sur un corps commutatif quelconque annule son propre polynôme caractéristique
\end{theorem}
\index{théorème!Cayley-Hamilton}
% position EYRooJkxiFf

Une autre démonstration est donnée en le théorème \ref{ThoCalYWLbJQ}.
\begin{proof}
    La preuve est divisée en plusieurs étapes.
    \begin{subproof}
        \item[Endomorphisme diagonalisable]
            Soit \( u\) un endomorphisme sur un espace vectoriel \( V\) de dimension \( n\) sur un corps \( \eK\) et \( \chi_u\) sont polynôme caractéristique. Nous savons que si \( \lambda\) est une valeur propre de \( u\) alors \( \chi_u(\lambda)=0\) le théorème \ref{ThoWDGooQUGSTL}\ref{ItemeXHXhHii}. En combinant avec le lemme \ref{LemVISooHxMdbr}, si \( x\) est vecteur propre pour la valeur propre \( \lambda\) de \( u\) nous avons
            \begin{equation}
                \chi_u(u)x=\chi_u(\lambda)x=0.
            \end{equation}
            Donc tant que \( u\) possède une base de vecteurs propres nous avons \( \chi_u(u)=0\).

        \item[Le cas complexe]

            Nous nous restreignons à présent (et provisoirement) au cas \( \eK=\eC\), ce qui nous donne \( u\in \eM(n,\eC)\). Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\) par la proposition \ref{PropDigDensVxzPuo}. Si \( A\in \eM(n,\eC)\) nous considérons une suite de matrices diagonalisables \( A_k\stackrel{\eM(n,\eC)}{\longrightarrow}A\). Pour chaque \( k\) nous avons par le point précédent 
            \begin{equation}
                \chi_{u_k}(u_k)=0.
            \end{equation}
            Chacune des composantes de \( \chi_{u_k}(u_k)\) est un polynôme en les composantes de \( u_k\), ce qui légitime le passage à la limite :
            \begin{equation}
                \chi_u(u)=0.
            \end{equation}
            Le théorème est établi pour toutes les matrices de \( \eM(n,\eC)\) et donc aussi pour tous les sous-corps de \( \eC\) comme \( \eR\) ou \( \eZ\).

        \item[La cas général]

            Par définition, \( \chi_u(X)=\det(u-X\mtu)\); les coefficients de \( X\) sont des polynômes à coefficients entiers en les composantes de \( u\). En substituant \( u\) à \( X\) nous obtenons une matrice dont chacune des entrées est un polynôme à coefficients entiers en les coefficients de \( u\). Pour chaque \( i\) et \( j\) entre \( 1\) et \( n\) il existe donc un polynôme \( P_{ij}\in \eZ(X_1,\ldots, X_{n^2})\) tel que
            \begin{equation}
                \chi_u(u)_{ij}=P(u_{11},\ldots, u_{nn}).
            \end{equation}
            Ces polynômes ne dépendent pas de \( u\) ni du corps sur lequel on travaille. Notre but est maintenant de prouver que \( P_{ij}=0\).

            Étant donné que le cas complexe (et a fortiori entier) est déjà prouvé nous savons que pour tout \( u\in \eM(n,\eZ)\) nous avons \( P(u_{11},\ldots, u_{nn})=0\). La proposition \ref{PropTETooGuBYQf} nous donne effectivement \( P=0\), en conséquence de quoi l'endomorphisme \( \chi_u(u)\) est nul.

    \end{subproof}
\end{proof}

\begin{example}
    Pour montrer que chaque composante \( \chi_u(u)\) est bien un polynôme à coefficients entiers en les coefficients de \( u\), voyons l'exemple \( 2\times 2\) : \( u=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\). D'abord
    \begin{equation}
        \chi_u(X)=\det\begin{pmatrix}
            a-X    &   b    \\ 
            c    &   d-X    
        \end{pmatrix}=X^2-(a+d)X+ad-cb.
    \end{equation}
    Le coefficient de \( X^2\) est \( 1\), celui de \( X\) est \( -a-d\) et le terme indépendant est \( ad-cb\); tout trois sont des polynômes à coefficients entiers en \( a,b,c,d\). Après substitution de \( X\) par \( u\), 
    \begin{equation}
        \chi_u(u)_{ij}=(u^2)_{ij}-(a+d)u_{ij}+ad-cb.
    \end{equation}
    Cela est bien un polynôme à coefficients entiers en les entrées de la matrice \( u\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carré d'une matrice hermitienne positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropVZvCWn}
    Si \( A\in \eM(n,\eC)\) est une matrice hermitienne\footnote{Définition \ref{DEFooKEBHooWwCKRK}.} positive, alors il existe une unique matrice hermitienne positive \( R\) telle que \( A=R^2\). De plus \( R\) est un polynôme (de \( \eR[X]\)) en \( A\).
\end{proposition}
\index{matrice!semblable}
\index{polynôme!d'endomorphisme}
\index{endomorphisme!diagonalisable}
\index{matrice!hermitienne!racine carré}
\index{racine!carré!de matrice hermitienne}

La matrice \( R\) ainsi définie est la \defe{racine carré de}{matrice!racine carré}\index{racine!carré de matrice!hermitienne positive} de \( A\), et est notée \( \sqrt{A}\)\nomenclature[A]{\( \sqrt{A}\)}{racine d'une matrice hermitienne positive}. Une des applications usuelles de cette proposition est la décomposition polaire.

\begin{proof}
    \begin{subproof}
    \item[Existence]
        Étant donné que \( A \) est hermitienne, elle est diagonalisable par une unitaire (proposition \ref{ThogammwA}), et ses valeurs propres sont réelles et positives (parce que \( A\) est positive). Soit donc \( P\) une matrice unitaire telle que
        \begin{equation}
            P^*AP=\begin{pmatrix}
                \alpha_1    &       &       \\
                    &   \ddots    &       \\
                    &       &   \alpha_n
            \end{pmatrix}
        \end{equation}
        avec \( \alpha_i>0\). Si on pose
        \begin{equation}
            R=P\begin{pmatrix}
                \sqrt{\alpha_1}    &       &       \\
                    &   \ddots    &       \\
                    &       &   \sqrt{\alpha_n}
            \end{pmatrix}P^*,
        \end{equation}
        alors \( R^2=A\) parce que \( P^*P=\mtu\).
    \item[Hermitienne positive]
        La matrice \( R\) est hermitienne parce que, avec un peu de notation raccourcie, \( R=P^*\sqrt{\alpha}P\) et \( R^*=P^*\sqrt{\alpha}P\). D'autre part, elle est positive parce que ses valeurs propres sont les \( \sqrt{\alpha_i}\) qui sont positives.
        
    \item[Polynôme]
        Nous montrons maintenant que la matrice \( R\) est un polynôme en \( A\). Pour cela nous considérons un polynôme \( Q\) tel que \( A(\alpha_i)=\sqrt{\alpha_i}\) pour tout \( i\). Soit \( \{ e_i \}\) une base de diagonalisation de \( A\) : \( Ae_i=\alpha_ie_i\). Alors c'est encore une base de diagonalisation de \( Q(A)\). En effet si \( Q=\sum_ka_kX^k\), alors
        \begin{equation}
            Q(A)e_i=(\sum_ka_kA^k)e_i=(\sum_ka_k\alpha_i^k)e_i=Q(\alpha_i)e_i=\sqrt{\alpha_i}e_i.
        \end{equation}
        Les valeurs propres de \( Q(A)\) sont donc \( \sqrt{\alpha_i}\). Nous savons maintenant que \( Q(A)\) a la même base de diagonalisation de \( A\) (et donc la même matrice unitaire \( P\) qui diagonalise), c'est à dire que
        \begin{equation}
            Q(A)=P^*\begin{pmatrix}
                \sqrt{\alpha_1}    &       &       \\
                    &   \ddots    &       \\
                    &       &   \sqrt{\alpha_n}
            \end{pmatrix}=R.
        \end{equation}
        Donc oui, \( R\) est un polynôme en \( A\).

        Notons que ce \( Q\) n'est pas du tout unique; il existe une infinité de polynômes qui envoient \( n\) nombres donnés sur \( n\) nombres donnés.

    \item[Unicité]
        Soit \( S\) une matrice hermitienne positive telle que \( R^2=S^2=A\). D'abord \( S\) commute avec \( A\) parce que
        \begin{equation}
            SA=S^3=S^2S=AS.
        \end{equation}
        Donc \( S\) commute aussi avec \( Q(A)=R\). Étant donné que \( S\) et \( R\) commutent et sont diagonalisables, ils sont simultanément diagonalisables par le corollaire \ref{CorQeVqsS}. Soient \( D_R=PRP^*\) et \( D_S=PSP^*\) les formes diagonales de \( R\) et \( S\) dans une base de simultanée diagonalisation. Les carrés des valeurs propres de \( R\) et \( S\) étant identiques (ce sont les valeurs propres de \( A\)) et les valeurs propres de \( R\) et \( S\) étant positives, nous déduisons que \( D_R=D_S\) et donc que \( R=P^*D_RP=P^*D_SP=S\).
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carré d'une matrice symétrique positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{JJdQPyK}]   \label{LemTLlTAAf}
    Le groupe orthogonal \( \gO(n,\eR) \) est compact.
\end{lemma}

\begin{proof}
    Nous avons \( O(n)=f^{-1}\big( \{ \mtu_n \} \big)\) où \( f\) est l'application continue \( A\mapsto A^tA\). En tant qu'image inverse d'un fermé par une application continue, le groupe \( O(n)\) est fermé.

    De plus il est borné parce que tous les coefficients d'une matrice orthogonale sont \( \leq 1\), donc \( \| A \|_{\infty}\) pour tout \( A\in O(n)\).
\end{proof}

\begin{proposition} \label{PropPEMDqVT}
    Une matrice symétrique semi (ou pas) définie positive admet une unique racine carré symétrique. Le spectre de la racine carré est la racine carré du spectre de la matrice de départ.
\end{proposition}

\begin{proof}
    Ceci est une phrase pour que les titres se mettent bien.
    \begin{subproof}
        \item[Existence]
            Soit \( T\) une matrice symétrique et \( Q\) une matrice orthogonale qui diagonalise\footnote{Théorème \ref{ThoeTMXla}.} \( T\) : \( QTQ^{-1}=D\) avec \( D=\diag(\lambda_i)\) et \( \lambda_i\geq 0\). En posant \( R=Q^{-1}\sqrt{D}Q\), il est vite vérifié que \( R^2=T\) et que \( R\) est symétrique. En ce qui concerne le spectre, \( R\) a pour valeurs propres les \( \sqrt{\lambda_i}\).
        \item[Unicité]

            Soit \( R\) une matrice symétrique de \( T\) : \( R^2=T\). Du coup \( R\) et \( T\) commutent : \( RT=R^3=TR\). Par conséquent les espaces propres de \( T\) sont stables sous \( R\). Soit \( E_{\lambda} \) l'un d'eux de dimension \( d\), et \( T_F\), \( R_F\) les restrictions de \( T\) et \( R\) à \( E_{\lambda}\). L'application \( T_F\) est une homothétie et \( R_F^2=T_F=\lambda\mtu\). Mais \( R_F\) est encore une matrice symétrique définie positive, donc nous pouvons considérer une base \( \{ e_1,\ldots, e_d \}\) de \( E_{\lambda}\) qui diagonalise \( R_F\) avec les valeurs propres \( \mu_i\); nous avons donc en même temps
            \begin{subequations}
                \begin{align}
                    R_f^2(e_i)&=\mu_i^2 e_i\\
                    T_F(e_i)&=\lambda e_i,
                \end{align}
            \end{subequations}
            de telle sorte que \( \mu_i^2=\lambda\). Mais les valeurs propres de \( R_F\) sont positives, sont \( \mu_i=\sqrt{\lambda}\) pour tout \( i\). En conclusion \( R_F\) est univoquement déterminé par la donnée de \( T\). Vu que cela est valable pour tous les espaces propres de \( T\) et que ces espaces propres engendrent tout \( E\), l'opérateur \( R\) est déterminé de façon univoque par \( T\).
    \end{subproof}
\end{proof}
Notons que nous n'avons démontré l'unicité qu'au sein des matrices symétriques.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Décomposition polaires : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( S^+(n,\eR)\) l'ensemble des matrices \( n\times n\) symétriques réelles définies positives et \( S^{++}(n,\eR)\) le sous-ensemble de \( S^+(n,\eR)\) des matrices strictement définies positives.
\nomenclature[B]{\( S^+(n,\eR)\)}{matrices symétriques définies positives}
\nomenclature[B]{\( S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives}

\begin{lemma}   \label{LemMGUSooPqjguE}
    La partie \( S^+(n,\eR)\) est fermée dans \( \eM(n,\eR)\).
\end{lemma}

\begin{proof}
    En effet si \( S_k\) est une suite de matrices symétriques convergeant dans \( \eM(n,\eR)\) vers la matrice \( A\), les suites \( (S_k)_{ij}\) et \( (S_k)_{ji}\) des composantes \( ij\) et \( ji\) sont des suites égales, et donc leurs limites sont égales\footnote{Ici nous utilisons le critère de convergence composante par composante et le fait que nous ne sommes pas trop inquiétés par la norme que nous choisissons parce que toutes les normes sont équivalentes par le théorème \ref{ThoNormesEquiv}.}. Donc la limite est symétrique.

    En ce qui concerne le spectre, le théorème \ref{ThoeTMXla} nous permet de diagonaliser : \( S_k=Q_kD_kQ_k^{-1}\) où les \( D_k\) sont des matrices diagonales remplies de nombres positifs ou nuls. Vu que \( O(n)\) est compact\footnote{Lemme \ref{LemTLlTAAf}.}, nous avons une sous-suite \( Q_{\varphi(k)}\) convergente : \( Q_{\varphi(k)}\to Q\). Pour chaque \( k\), nous avons
    \begin{equation}
        S_{\varphi(k)}=Q_{\varphi(k)}D_{\varphi(k)}Q^{-1}_{\varphi(k)},
    \end{equation}
    dont la limite existe et vaut \( A\). Vu que pour tout \( k\), \( D_{\varphi(k)}=Q^{-1}_{\varphi(k)}S_{\varphi(k)}Q_{\varphi(k)}\) et que le produit matriciel est continu, la suite \( k\mapsto D_{\varphi(k)}\) est une suite convergente dans \( \eM(n,\eR)\). Nous notons \( D\) sa limite qui est encore une matrice diagonale contenant des nombres positifs ou nuls sur la diagonale.
    \begin{equation}
        A=\lim_{k\to \infty } S_{\varphi(k)}=QDQ^{-1},
    \end{equation}
    et donc le spectre de \( A\) est la limite de ceux des matrices \( D_{\varphi(k)}\). Chacun étant positif, la limite est positive. Donc \( A\in S^+(n,\eR)\).
\end{proof}

\begin{lemma}   \label{LemZKJWqIP}
    La fermeture de l'ensemble des matrice symétriques strictement définies positives est l'ensemble des matrices définies positives : \( \overline{ S^{++}(n,\eR) }=S^+(n,\eR)\).
\end{lemma}
\index{densité!de \( S^+(n,\eR)\) dans \( S^{++}(n,\eR)\)}

\begin{proof}
    Le lemme \ref{LemMGUSooPqjguE} nous a à peine dit que \( S^+(n,\eR)\) était fermé. Nous devons prouver que pour tout élément de \( S^+(n,\eR)\), il existe une suite \( (S_k)\) dans \( S^{++}(n,\eR)\) convergeant vers \( S\).

    Si \( S\in S^+(n,\eR)\) alors nous avons la diagonalisation
    \begin{equation}
        S=QDQ^{-1} =Q
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n
        \end{pmatrix}
        Q^{-1}
    \end{equation}
    où \( \lambda_i\geq 0\) pour tout \( i\). Nous définissons
    \begin{equation}
        D_k=
        \begin{pmatrix}
            \lambda_1+\epsilon^{(1)}_k    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n+\epsilon^{(n)}_k
        \end{pmatrix}
    \end{equation}
    où \( \epsilon^{i}_k\) est une suite convergent vers \( 0\) telle que \( \lambda_i+\epsilon^{(i)}_n>0\) pour tout \( n\). Typiquement si \( \lambda_i>0\) alors \( \epsilon^{(i)}_k=0\) et sinon \( \epsilon^{(i)}_k=1/k\).

    Pour tout \( k\) nous avons \( QD_kQ^{-1}\in S^{++}(n,\eR)\) et de plus \( QD_kQ^{-1}\to QDQ=S\).
\end{proof}

\begin{theorem}[Décomposition polaire de matrices symétriques définies positives\cite{JJdQPyK,AABkVai,WWBTooITOwEn}] \label{ThoLHebUAU}
   En ce qui concerne les matrices inversibles :
   \begin{equation}
       \begin{aligned}
           f\colon O(n,\eR)\times S^{++}(n,\eR)&\to \GL(n,\eR) \\
           (Q,S)&\mapsto SQ 
       \end{aligned}
   \end{equation}
   est un homéomorphisme\footnote{Cela est en réalité en difféomorphisme, voir la remarque \ref{RemBJCBooGLiRmG}.}.

   En ce qui concerne les matrices en général :
   \begin{equation}
       \begin{aligned}
           g\colon O(n,\eR)\times S^+(n,\eR)&\to \eM(n,\eR) \\
           (Q,S)&\mapsto SQ 
       \end{aligned}
   \end{equation}
   est une surjection mais pas une injection.

   De plus les mêmes conclusions tiennent si nous regardons \( (Q,S)\mapsto QS\) au lieu de \( SQ\).
\end{theorem}
\index{groupe!linéaire!décomposition polaire}
\index{endomorphisme!décomposition!polaire}
\index{décomposition!polaire}

%TODO : prouver le difféomorphisme.
%TODO : je crois qu'on doit pouvoir prouver que les éléments de la décomposition polaire sont des polynômes en M.

\begin{proof}
    Nous commençons par prouver les résultats concernant les matrices inversibles.
    \begin{subproof}
        \item[Existence et unicité]

            Si \( M=SQ\), alors \( MM^t=SQQ^tS^t=S^2\), donc \( S\) doit être une racine carré symétrique de la matrice définie positive \( MM^t\). La proposition \ref{PropPEMDqVT} nous dit que ça existe et que c'est unique. Donc \( S\) est univoquement déterminé par \( M\). Maintenant avoir \( Q=MS^{-1}\) est obligatoire (unicité) et fonctionne :
            \begin{equation}
                Q^tQ=(S^{-1})^tM^tMS^{-1}=S^{-1}S^2S^{-1}=\mtu,
            \end{equation}
            donc \( Q\) ainsi défini est orthogonale.

            Notons que ceci ne fonctionne pas lorsque \( M\) n'est pas inversible parce qu'alors \( S\) n'est pas inversible.
        
        \item[Homéomorphisme]

            Le fait que \( f\) soit continue n'est pas un problème : c'est un produit de matrice. Nous devons vérifier que \( f^{-1}\) est continue. Soit une suite convergente \( M_k\to M\) dans \( \GL(n,\eR)\). Si nous nommons \( (Q_k,S_k)\) la décomposition polaire de \( M_k\) et \( (Q,S)\) celle de \( M\), nous devons prouver que \( Q_k\to Q\) et \( S_k\to S\). En effet dans ce cas nous aurions
            \begin{equation}    \label{EqJIkoaJv}
                \lim_{k\to \infty} f^{-1}(M_k)=\lim_{k\to \infty} (Q_k,S_k)=(Q,S)=f^{-1}(M).
            \end{equation}
            
            Étant donné que \( O(n)\) est compact (lemme \ref{LemTLlTAAf}), la suite \( (Q_k)\) admet une sous-suite convergente (Bolzano-Weierstrass, théorème \ref{ThoBWFTXAZNH}) que nous nommons
            \begin{equation}
                Q_{\varphi(k)}\to F\in O(n).
            \end{equation}
            Vu que la suite \( (M_k)\) converge, sa sous-suite converge vers la même limite : \( M_{\varphi(k)}\to M\) et vu que pour tout \( k\) nous avons \( S_k=M_kQ_k^{-1}\),
            \begin{equation}
                S_{\varphi(k)}\to G=MF^{-1}.
            \end{equation}
            Vu que chacune des matrices \( S_{\varphi(k)}\) est symétrique définie positive, la limite est symétrique et semi-définie positive\footnote{Lemme \ref{LemZKJWqIP}}. Donc \( G\in S^+(n,\eR)\cap \GL(n,\eR)\) parce que de plus \( M\) et \( F\) étant inversibles, \( G\) est inversible. En ce qui concerne la sous-suite nous avons
            \begin{equation}
                M_{\varphi(k)}=S_{\varphi(k)}Q_{\varphi(k)}\to GF=M
            \end{equation}
            où \( F\in O(n)\) et \( G\in S^+(n,\eR)\). Par unicité de la décomposition polaire de \( M\) (partie déjà démontrée), nous avons \( G=S\) et \( F=Q\).

            Nous avons prouvé que toute sous-suite convergente de \( Q_k\) a \( Q\) pour limite. Donc la suite elle-même converge\footnote{Proposition \ref{PropHNylIAW}, pas difficile.} vers \( Q\). Donc \( Q_k\to Q\). Du coup vu que \( S_k=M_kQ_k^{-1}\) est un produit de suites convergentes, \( S_k\) converge également, vers \( S\) :  \( S_k\to S\).

            Au final l'application \( f^{-1}\) est bien continue parce que les égalités \eqref{EqJIkoaJv} ont bien lieu.
    \end{subproof}

    Nous passons maintenant à la preuve dans le cas des matrices en général.

    Soit \( A\in \eM(n,\eR)\); par densité (lemme \ref{PropQGUPooVudelJ}), il existe une suite \( (A_k)\) dans \( \GL(n,\eR)\) telle que \( A_k\to A\). Pour chacun des \( k\) nous appliquons la décomposition polaire déjà prouvée : \( A_k=Q_kS_k\). D'abord \( (Q_k)\) est une suite dans le compact\footnote{Lemme \ref{LemTLlTAAf}.} \( \gO(n,\eR)\) et accepte donc une sous-suite convergente. Quitte à redéfinir la suite de départ, nous supposons pour alléger les notations que \( Q_k\to Q\in \gO(n,\eR)\). Vu que \( Q_k\) est inversible, 
    \begin{equation}
        S_k=Q^{-1}_kA_k
    \end{equation}
    Le produit matriciel étant continu nous avons \( S_k\to S\) dans \( \eM(n,\eR)\). Mais \( S^+(n,\eR)\) étant fermé (lemme \ref{LemMGUSooPqjguE}) nous avons aussi \( S\in S^+(n,\eR)\).
\end{proof}

\begin{remark}  \label{RemBJCBooGLiRmG}
    Pour démontrer que \( f\) est différentiable, nous devons utiliser le théorème d'inversion locale \ref{ThoXWpzqCn}; cela est fait dans la proposition \ref{PropWCXAooDuFMjn}.
\end{remark}

\begin{corollary}       \label{CorAWYBooNCCQSf}
    Toute matrice peut être écrite sous la forme \( Q_1DQ_2\) où \( Q_1\) et \( Q_2\) sont orthogonale et \( D\) est diagonale.
\end{corollary}

\begin{proof}
    Si \( A\in\eM(n,\eR)\) alors la décomposition polaire \ref{ThoLHebUAU} nous donne \( A=SQ\) où \( S\) est symétrique définie positive et \( Q\) est orthogonale. La matrice \( S\) peut ensuite être diagonalisée par le théorème \ref{ThoeTMXla} : \( S=RDR^{-1}\) où \( D\) est diagonale et \( R\) est orthogonale. Avec ces deux décompositions en main, \( A=SQ=RDR^{-1}Q\). La matrice \( R^{-1}Q\) est orthogonale.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Enveloppe convexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Sur \( C\) est un ensemble convexe, un point \( x\in C\) est un \defe{point extrémal}{extrémal!point dans un convexe} si \( C\setminus\{ x \}\) est encore convexe.
\end{definition}

\begin{theorem}[\cite{KXjFWKA}] \label{ThoBALmoQw}
    Soit \( E\) un espace euclidien de dimension \( n\geq 1\) et \( \aL(E)\) l'espace des opérateurs linéaires sur \( E\) sur lequel nous considérons la norme subordonnée\footnote{Définition \ref{DEFooYDFVooSMgvsD}.} à celle sur \( E\). L'ensemble des points extrémaux de la boule unité fermée de \( \aL(E)\) est le groupe orthogonal \( O(n,\eR)\).
\end{theorem}
\index{densité!points extrémaux dans \( \aL\)}

\begin{proof}
    Nous notons \( \mB\) la boule unité fermée de \( \aL(E)\). Montrons pour commencer que les éléments de \( O(n)\) sont extrémaux dans \( \mB\). D'abord si \( A\in O(E)\) alors \( \| A \|=1\) parce que \( \| Ax \|=\| x \|\). Supposons maintenant que \( A\) n'est pas extrémal, c'est à dire qu'il est le milieu d'un segment joignant deux points (distincts) de la boule unité de \( \aL(E)\). Soient donc \( T,U\in\mB\) tels que \( A=\frac{ 1 }{2}(T+U)\). Pour tout \( x\in E\) tel que \( \| x \|=1\) nous avons 
    \begin{equation}    \label{EqKTuAIIE}
        1=\| x \|=\| Ax \|=\frac{ 1 }{2}\| Tx+Ux \|\leq \frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)\leq\frac{ 1 }{2}\big( \| T \|+| U | \big)\leq 1
    \end{equation}
    Toutes les inégalités sont en réalité des égalités. En particulier nous avons
    \begin{equation}
        \| Tx+Ux \|=\| Tx \|+\| Ux \|,
    \end{equation}
    mais alors nous sommes dans un cas d'égalité dans l'inégalité de Cauchy-Schwartz (théorème \ref{ThoAYfEHG}) et donc il existe \( \lambda\geq 0\) tel que \( Tx=\lambda Ux\). Mais de plus les \sout{inégalité} égalités \eqref{EqKTuAIIE} nous donnent
    \begin{equation}
        \frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)=1
    \end{equation}
    alors que nous savons que \( \| Tx \|,\| Ux \|\leq 1\), donc \( \| Tx \|=\| Ux \|=1\). La seule possibilité est d'avoir \( \lambda=1\) et donc que \( U=T\) parce que nous avons \( Tx=Ux\) pour tout \( x\) de norme \( 1\). Au final \( A\) n'est pas le milieu d'un segment dans \( \mB\).

    Nous passons donc à l'inclusion inverse : nous prouvons que les points extrémaux de \( \mB\) sont dans \( O(E)\). Pour cela nous prenons \( U\in\mB\setminus O(E)\) et nous allons montrer que \( U\) n'est pas un point extrémal : nous allons l'écrire comme milieu d'un segment dans \( \mB\).

    Par la seconde partie du théorème de décomposition polaire \ref{ThoLHebUAU}, il existe \( Q\in O(n,\eR)\) et \( S\in S^+(n,\eR)\) tels que \( U=QS\). Nous diagonalisons \( S\) à l'aide de la matrice orthogonale \( P\) :
    \begin{equation}
        S=PDP^{-1}
    \end{equation}
    avec \( D=\diag(\lambda_i)\). En termes de normes, nous avons
    \begin{equation}
        \| U \|=\| S \|=\| S \|.
    \end{equation}
    En effet vu que \( Q\) est orthogonale, \( \| Ux \|=\| QSx \|=\| Sx \|\) pour tout \( x\), donc \( \| U \|=\| S \|\). De plus pour tout \( x\) nous avons
    \begin{equation}
        \| Sx \|=\| PDP^{-1} x \|=\| DP^{-1}x \|.
    \end{equation}
    Étant donné que \( P^{-1}\) est une bijection, le supremum des \( \| Sx \|\) sera le même que celui des \( \| Dx \|\) et donc \( \| S \|=\| D \|\). Étant donné que par définition \( \| U \|\leq 1\), nous avons aussi \( \| D \|\leq 1\) et donc \( 0\leq\lambda_i\leq 1\) (pour rappel, les valeurs propres de \( D\) sont positives ou nulles parce que \( S\) est ainsi). 

    Comme \( U\notin O(E)\), au moins une des valeurs propres n'est pas \( 1\), supposons que ce soit \( \lambda_1\). Alors nous avons \( \alpha,\beta\in\mathopen[ -1 , 1 \mathclose]\) avec \( -1\leq \alpha<\beta\leq 1\) et \( \lambda_1=\frac{ 1 }{2}(\alpha+\beta)\). Nous posons alors
    \begin{subequations}
        \begin{align}
            D_1=\diag(\alpha,\lambda_2,\ldots, \lambda_n)\\
            D_2=\diag(\beta,\lambda_2,\ldots, \lambda_n).
        \end{align}
    \end{subequations}
    Nous avons bien \( D_1\neq D2\) et \( D_1+ D_2=D\). Par conséquent
    \begin{equation}
        U=\frac{ 1 }{2}\big( QPD_1P^{-1}+QPD_2P^{-1} \big)
    \end{equation}
    avec \( QPD_1P^{-1}\neq QPD_2^{-1}\). La matrice \( U \) est donc le milieu d'un segment. Reste à montrer que ce segment est dans \( \mB\). Pour ce faire, prenons \( x\in E\) et calculons :
    \begin{equation}
        \| QPD_iP^{-1}x \|=\| D_iP^{-1}x \|\leq\| P^{-1}x \|=\| x \|
    \end{equation}
    parce que \( \| D_i \|\leq 1\) et \( P^{-1}\) est orthogonale. Au final la norme de \( QPD_iP\) est plus petite que \( 1\) et donc \( U\) est bien le milieu d'un segment dans \( \mB\), et donc non extrémal.
\end{proof}

\begin{theorem}[\cite{NHXUsTa}] \label{ThoVBzqUpy}
    L'enveloppe convexe de \( O(n)\) dans \( \eM_n(\eR)\) est la boule unité pour la norme induite de \( \| . \|_2\) sur \( \eR^n\).
\end{theorem}
\index{convexité!enveloppe de $O(n)$}
\index{groupe!linéaire!enveloppe convexe de $\Omega(n)$}

\begin{proof}
    Nous notons \( \mB\) la boule unité fermée de \( \eM(n,\eR)\) et \( \Conv\big( O(n,\eR) \big)\) l'enveloppe convexe de \( O(n,\eR)\). Vu que \( \mB\) est convexe nous avons \( \Conv\big( O(n) \big)\subset\mB\).


    Maintenant nous devons prouver l'inclusion inverse. Pour ce faire nous supposons avoir un élément \( A\in \mB\setminus\Conv\big( O(n) \big)\) et nous allons dériver une contradiction.
    
    Remarquons que \( O(n)\) est compact par le lemme \ref{LemTLlTAAf} et que par conséquent \( \Conv(O(n))\) est compacte par le corollaire \ref{CorOFrXzIf} et donc fermée. Nous considérons un produit scalaire \( (X,Y)\mapsto X\cdot Y\) sur \( \eM\). Vu que \( \Conv\big( O(n) \big)\) est un fermé convexe nous pouvons considérer la projection\footnote{Le théorème de projection : théorème \ref{ThoWKwosrH}.} sur \( \Conv(A)\) relativement au produit scalaire choisis.

    Nous notons \( P=\pr_{\Conv\big( O(n) \big)}(A)\). En vertu du théorème de projection, nous avons
    \begin{equation}    \label{EqYSisLTL}
        (A-P)\cdot (M-P)\leq 0
    \end{equation}
    pour tout \( M\in\Conv O(n)\). Notons \( B=A-P\) pour alléger les notations. L'équation \eqref{EqYSisLTL} s'écrit
    \begin{equation}    \label{EqQDLZqXQ}
        B\cdot M\leq B\cdot P.
    \end{equation}
    D'autre par vu que \( B \neq 0\) nous avons \( B\cdot B> 0\), c'est à dire \( B\cdot (A-P)>0\) et donc
    \begin{equation}
        B\cdot A>B\cdot P.
    \end{equation}
    En combinant avec \eqref{EqQDLZqXQ},
    \begin{equation}        \label{EqIQNlwql}
        B\cdot M\leq B\cdot P<B\cdot A.
    \end{equation}
    Nous utilisons maintenant la décomposition polaire, théorème \ref{ThoLHebUAU}, pour écrire \( B=QS\) avec \( Q\in O(n)\) et \( S\in S^+(n,\eR)\). Vu que l'inégalité \eqref{EqIQNlwql} tient pour tout \( M\in\Conv(O(n))\), elle tient en particulier pour \( Q\in O(n)\). Donc
    \begin{equation}
        B\cdot Q=B\cdot A.
    \end{equation}
    Nous nous particularisons à présent au produit scalaire \( (X,Y)\mapsto\tr(X^tY)\) de la proposition \ref{PropMAQoKAg}. D'abord
    \begin{equation}    \label{EaHVxWdau}
        B\cdot Q=\tr(B^tQ)=\tr(S^tQ^tQ)=\tr(S^t)=\tr(S),
    \end{equation}
    et ensuite l'inégalité \eqref{EaHVxWdau} devient
    \begin{equation}
        \tr(S)<B\cdot A=\tr(S^tQ^tA).
    \end{equation}
    Nous choisissons une basse \( \{ e_i \}\) diagonalisant \( S\) : \( Se_i=\lambda_ie_i\) vérifiant automatiquement \( \lambda_i\geq 0\) parce que \( S\) est semi-définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}. Alors
    \begin{subequations}
        \begin{align}
            \tr(S)&<\tr(S^tQ^tA)\\
            &=\sum_i\langle S^tQ^tAe_i, e_i\rangle \\
            &=\sum_i\langle Ae_i, QSe_i\rangle \\
            &\leq \sum_i \| Ae_i \| | \lambda_i | \underbrace{\| Qe_i \|}_{=1} \\
            &\leq \sum_i\lambda_i   & A\in\mB\Rightarrow\| Ae_i \|\leq 1\\
            &=\tr(S).
        \end{align}
    \end{subequations}
    Il faut noter que la première inégalité est stricte, et donc nous avons une contradiction.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition de Bruhat}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Décomposition de Bruhat]\index{Bruhat (décomposition)}\index{décomposition!Bruhat}    \label{ThoizlYJO}
    Soit \( \eK\) un corps; un élément \( M\in\GL(n,\eR)\) s'écrit sous la forme
    \begin{equation}
        M=T_1P_{\sigma}T_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont des matrices triangulaires supérieures inversibles et où \( P_{\sigma}\) est une matrice de permutation \( \sigma\in S_n\). De plus il y a unicité de \( \sigma\).
\end{theorem}
\index{groupe!permutation}
\index{groupe!linéaire}
\index{matrice}

\begin{proof}
    Afin de rendre les choses plus visuelles, nous nous permettons de donner des exemples au fur et à mesure de la preuve. Nous prenons l'exemple de la matrice
    \begin{equation}
        \begin{pmatrix}
            1    &   3    &   4    \\
            2    &   5    &   6    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    \begin{subproof}
    \item[Existence]
        Soit \( M\in \GL(n,\eR)\); vu qu'elle est inversible, on a un indice \( i_1\) maximum tel que \( M_{i_1,1}\neq 0\). Nous changeons toutes les lignes jusque là, c'est à dire que nous faisons, pour \( 1\leq i< i_1\),
        \begin{equation}        \label{EqGHUbwR}
            L_i\to L_i-\frac{ M_{i1} }{ M_{i_11} }L_{i_1}.
        \end{equation}
        Voir le lemme \ref{LemooTQJXooGoIxsI}\ref{ITEMooXUGFooKcbrxs}.

        Nous avons donc obtenu une matrice dont la première colonne est nulle sauf la case numéro \( i_1\). L'opération \eqref{EqGHUbwR} revient à considérer la multiplication par la matrice de transvection
        \begin{equation}
            T_1^{(i)}=T_{ii_1}\left( -\frac{ M_{i1} }{ M_{i_11} } \right)
        \end{equation}
        pour tout \( i<i_1\). Pour rappel nous ne changeons que les lignes \emph{au-dessus} de la \( i_1\). Du coup les matrices \( T^{(i)}_1\) sont triangulaires supérieures. Nous avons donc la nouvelle matrice \( M_1=\left( \prod_{i<i_1}T_1^{(i)} \right)M\) pour laquelle toute la première colonne est nulle sauf un élément.

        Dans le cas de l'exemple, le «pivot» sera la ligne \( (2,5,6)\) et la matrice se transforme à l'aide de la matrice \( T_1=T_{12}(-1/2)\) :
        \begin{equation}    \label{EqyjXIYf}
            \begin{pmatrix}
                1    &   -1/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}
            \begin{pmatrix}
                1    &   3    &   4    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
        \end{equation}

    
    Maintenant nous faisons de même avec les colonnes (en renommant \( M\) la matrice obtenue à l'étape précédente) :
    \begin{equation}
        C_j\to C_j-\frac{ M_{i_1j} }{ M_{i_11} }C_1,
    \end{equation}
    qui revient à multiplier à droite par les matrices \( T_{1j}(\frac{ M_{i_1i} }{ M_{i_11} })\) avec \( j>1\). Encore une fois ce sont des matrices triangulaires supérieures.

    Dans l'exemple, pour traiter la seconde colonne, nous multiplions \eqref{EqyjXIYf} à droite par la matrice \( T_{12}(-5/2)\) :
    \begin{equation}
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}
            \begin{pmatrix}
                1    &   -5/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   0    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
    \end{equation}
    Appliquer encore la matrice \( T_{13}(-6/2)\) apporte la matrice
    \begin{equation}
        \begin{pmatrix}
            0    &   1/2    &   1    \\
            2    &   0    &   0    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    Enfin nous multiplions la matrice obtenue par \( \frac{1}{ M_{i_11} }\mtu\) pour normaliser à \( 1\) l'élément «pivot» que nous avions choisit. Dans notre exemple nous multiplions par \( 1/2\) pour trouver
    \begin{equation}        \label{Eqduglwu}
        \begin{pmatrix}
            0    &   1/4    &   1/2    \\
            1    &   0    &   0    \\
            0    &   7/2    &   4
        \end{pmatrix}.
    \end{equation}

    La matrice obtenue jusqu'ici possède une ligne et une colonne de zéros avec un \( 1\) à leur intersection, et elle est de la forme
    \begin{equation}
        M'=T_1MT_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont triangulaires supérieures et inversibles, produits de matrices de transvection (et d'une matrice scalaire pour la normalisation).

    Il reste à recommencer l'opération avec la seconde colonne (qui n'est pas toute nulle parce que le déterminant est encore non nul) puis la suivante etc. Dans notre exemple de l'équation \eqref{Eqduglwu}, nous éliminerions le \( 1/4\) et le \( 4\) en utilisant le \( 7/2\).

    Encore une fois tout cela se fait à l'aide de matrice supérieures parce qu'à chaque étape, les colonnes précédent le pivot sont déjà nulles (saut un \( 1\)) et ne doivent donc pas être touchées.

    À la fin de ce processus, ce qui reste est une matrice \( TMT'\) qui ne contient plus que un seul \( 1\) sur chaque ligne et chaque colonne, c'est à dire une matrice de permutation : \( P_{\sigma}=TMT'\) et donc
    \begin{equation}
        M=T^{-1}_{\sigma}(T')^{-1}.
    \end{equation}

        \item[Unicité]

            Soient \( \sigma,\sigma\in S_n'\) tels que \( T_1P_{\sigma}T_2=S_1P_{\tau}S_2\) avec \( T_i\) et \( S_i\) triangulaires supérieures et inversibles. En posant \( T=T_2S_2^{-1}\) et \( S=T_1^{-1}S_1\), nous avons
            \begin{equation}
                P_{\sigma}T=SP_{\tau}
            \end{equation}
            où \( S\) et \( T\) sont des matrices triangulaires supérieures et inversibles. Par les calculs de la preuve du lemme \ref{LemyrAXQs},
            \begin{subequations}
                \begin{numcases}{}
                    (P_{\sigma}T)_{kl}=T_{\sigma^{-1}(k)l}\\
                    (SP_{\tau})_{kl}=S_{k\tau(l)},
                \end{numcases}
            \end{subequations}
            et donc
            \begin{equation}    \label{EqKlmgOT}
                T_{\sigma^{-1}(k)l}=S_{k\tau(l)}.
            \end{equation}
            En écrivant cette équation avec \( k=\sigma(i)\) (nous rappelons que \( \sigma\) est bijective),
            \begin{equation}
                T_{il}=S_{\sigma(i)\tau(l)}.
            \end{equation}
            Nous savons que les termes diagonaux de \( T\) sont non nuls parce que \( T\) est triangulaire supérieure et inversible (donc pas de colonnes entières nulles). Nous avons donc, en prenant \( i=l=k\),
            \begin{equation}
                0\neq T_{kk}=S_{\sigma(k)\tau(k)}.
            \end{equation}
            La matrice étant triangulaire supérieure, cela implique 
            \begin{equation}    \label{EqEmiBTX}
                \sigma(k)\leq\tau(k).
            \end{equation}
            De la même manière en écrivant \eqref{EqKlmgOT} avec \( l=\tau^{-1}(i)\),
            \begin{equation}
                S_{ki}=T_{\sigma^{-1}(k)\tau^{-1}(i)}
            \end{equation}
            et donc
            \begin{equation}
                \sigma^{-1}(k)\leq \tau^{-1}(k).
            \end{equation}
            En écrivant cela avec \( k=\sigma(j)\), nous avons \( j\leq \tau^{-1}\sigma(j)\) et en appliquant enfin \( \tau\),
            \begin{equation}
                \tau(j)\leq \sigma(j).
            \end{equation}
            En comparant avec \eqref{EqEmiBTX}, nous avons \( \sigma=\tau\).
    \end{subproof}
\end{proof}
