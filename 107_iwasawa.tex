% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Connectedness of some groups}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The following is a general result about Lie groups:
\begin{lemma} \label{LemConnSpecMo}
If $G$ is a Lie group and $G_0$ is its identity component, the connected components of $G$ are lateral classes of $G_0$. More specifically, if $x\in G_1$, then $G_1=xG_0=G_0x$.
\end{lemma}
%TODO : a proof

\begin{lemma}[\cite{HelgasonSym}]   \label{LemOHjzfsL}
Connectedness of some usual groups:
\begin{itemize}
\item 
    The groups $\SU(p, q)$, $\SU^*(2n)$, $\SO^*(2n)$, $p(n, R)$, and $\SP(p, q)$ are connected.
\item 
    The group $\SO(p, q)$ ($0<p<p+q$) have exactly two connected components.
\end{itemize}
\end{lemma}
%TODO : a proof.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{Young tableau and diagrams}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We follow \cite{ModavePoincarre}. Let $n\in \eN$. A \defe{Young diagram}{Young!diagram} is a diagram of $n$ boxes arranged in rows of decreasing number of boxes. Here is an example with $n=8$:
%\begin{equation}
%	\input{image_Young_exemple.pstricks}
%\end{equation}

\begin{center}
   \input{pictures_tex/Fig_TGdUoZR.pstricks}
\end{center}


A \defe{Young tableau}{Young!tableau} is a Young diagram in each box of which we placed a number between $1$ and $n$ such that
\begin{itemize}
	\item in each line, the numbers are increasing from left to right,
	\item in each line, the numbers are increasing from up to down.
\end{itemize}
It is \defe{standard}{Young!tableau!standard} if each of the integers $\{ 1,\ldots,n \}$ appears one and only one time. Here is an example:
%\begin{equation}
%	\input{image_Young_exempleStandard.pstricks}
%\end{equation}
\begin{center}
   \input{pictures_tex/Fig_GBnUivi.pstricks}
\end{center}
Let $S_n$ be the symmetric group of all the permutations of $n$ objects. Let us see how does a Young tableau define symmetry properties of tensors. If $\lambda$ is a Young tableau with $n$ boxes, its applies on the tensor $T_{\mu_1,\ldots,\mu_n}$ by symmetrization over the set of indices in each line, and antisymmetrization over the sets of indices in each column. Now, the group $S_n$ can act on the so-obtained tensor in order to produce $n!$ new tensors, which are not independent.

\begin{lemma}
The space spanned by
\begin{equation}
	\{ g \tilde T\tq g\in S_n \}
\end{equation}
where $\tilde T$ is $\lambda T$ is an irreducible representation of $S_n$. Moreover, two representations coming from two different tableau with the same diagram are equivalent.
\end{lemma}
\begin{proof}
	No proof.
\end{proof}
As an example, consider the following tableau
%\begin{equation}
%	\input{image_Young_carre.pstricks}
%\end{equation}
\begin{center}
   \input{pictures_tex/Fig_IYAvSvI.pstricks}
\end{center}
The corresponding symmetrization of the tensor $T_{abcd}$ is as follows. First, we symmetrize over $ac$ (i.e the row $1,3$):
\begin{equation}
\frac{ 1 }{2}(	T_{abcd}+T_{cbad}),
\end{equation}
and then we symmetrize over $bd$:
\begin{equation}
	\frac{1}{ 4 }(T_{abcd}+T_{cbad}+T_{adcb}+T_{cdab}).
\end{equation}
Then we have to take the ansisymmetrization over $ab$ ($8$ terms), and we finish by antisymmetrization over $cd$ to get the $16$ terms result.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Representations of \texorpdfstring{$\GL(V)$}{GL(V)} }
%---------------------------------------------------------------------------------------------------------------------------

If $v\in \eR^D$, then the action of all the non-singular $D\times D$ matrices on $v$ creates a representation space for $GL(D)$. In the same way, the $2$-indices tensors $T_{ab}$ form a $D^2$-dimensional representation of $GL(V)$ by acting separately on each index:
\begin{equation}		\label{EqYoungAgitIndices}
	(g\cdot T)_{ab}=g^{ac}g^{bd}T_{cd}.
\end{equation}
That representation is, however, reducible because a symmetric tensor remains symmetric; indeed
\begin{equation}
	(g\cdot T)_{ba}=g^{bc}g^{ad}T_{cd}=g^{bc}g^{ad}T_{dc}=g^{bd}g^{ac}T_{cd}=(g\cdot T)_{ab}.
\end{equation}
The same is true for the antisymmetric tensors. Thus,
\begin{equation}
	V\otimes V\simeq (V\odot V)\oplus(V\wedge V).
\end{equation}
More generally, when $GL(D)$ acts on $V^{\otimes n}$, the irreducible representations are the different combinations of (anti)symmetrizations, namely: the irreducible representations of $S_n$.



Let $\lambda$ be a Young tableau with $n$ boxes. The \defe{Schur module}{schur!module} $V_{\lambda}^{GL(D)}$\nomenclature{$V_{\lambda}^{GL(D)}$}{The Schur module, representation of $GL(D)$} is the vector space of tensor $\tilde T$ of rank $n$ such that
\begin{itemize}
	\item $\tilde T$ is completely antisymmetric for all the labels of each column of $\lambda$,
	\item the antisymmetrization of all the labels of one column of $\lambda$ with another label on the right vanishes.
\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Representations of \texorpdfstring{$O(p,q)$}{Opq}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{ModavePoincarre}]
If the sum of the length of the two first column of a Young diagram $\lambda$ is strictly bigger than $D$, then the irreducible representation of $O(D)$ given by $\lambda$ is identically vanishing.
\end{theorem}

Young diagrams such that the sum of the lengths of the first two columns does not exceed $D$ are said to be \defe{allowed}{allowed!Young diagram}.
\index{Young!diagram!allowed}

\begin{theorem}		\label{ThoOpqrepreTens}
Every non vanishing finite dimensional irreducible representation of $O(p,q)$ is isomorphic to a completely traceless tensor representation whose symmetry properties are given by an allowed Young diagram.
\end{theorem}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Two words about \texorpdfstring{$\gsu(3)$}{su3}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Using the Cartan matrix of $\gsu(3)$ and formula \eqref{EqCoefDynkMalpha}, we will determine the Dynkin coefficients of the representation 
%\input{image_su3Dynkin(1-).pstricks}
\input{pictures_tex/Fig_RQsQKTl.pstricks} without even explicitly compute the weights. For that, we follow the construction of \cite{rncahn}. The Cartan matrix is
\begin{equation}
	A=
	\begin{pmatrix}
		2	&	-1	\\ 
		-1	&	2	
	\end{pmatrix}.
\end{equation}
The Dynkin coefficients of the highest weight is given by
\begin{equation}
	\Lambda_i=
	\begin{pmatrix}
		1	\\ 
		0	
	\end{pmatrix}.
\end{equation}
Since $\Lambda$ is highest weight, we have $q(\Lambda,\alpha_i)=1$, so that $\Lambda_1+q(\Lambda,\alpha_1)=1$ and $\Lambda_2+q(\Lambda,\alpha_2)=0$. Thus the only weight of the first layer is $M=\Lambda-\alpha_1$.
Using formula \eqref{EqCoefDynkMalpha}, we find
\begin{equation}
	(\Lambda-\alpha_1)_i=\Lambda_i-A_{1i}=
	\begin{pmatrix}
		1	\\ 
		0	
	\end{pmatrix}-
	\begin{pmatrix}
		2	\\ 
		-1	
	\end{pmatrix}=
	\begin{pmatrix}
		-1	\\ 
		1	
	\end{pmatrix}.
\end{equation}
We also have, by construction, $p(M,\alpha_1)=1$ and $p(M,\alpha_2)=0$, so that $M_1+p(M,\alpha_1)=-1+1=0$ and $M_2+p(M,\alpha_2)=1$. We conclude that $M-\alpha_2$ is a weight, and its Dynkin coefficients are given by
\begin{equation}
	(M-\alpha_2)_i=
	\begin{pmatrix}
		-1	\\ 
		1	
	\end{pmatrix}-
	\begin{pmatrix}
		-1	\\ 
		2	
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\ 
		-1	
	\end{pmatrix}.
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Representations of \texorpdfstring{$\so(n)$}{son}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ladder operators}
%---------------------------------------------------------------------------------------------------------------------------

We follow \cite{Dolan_son}, and we restrict ourself to the case where $n$ is even. We will use two abuse of language. The first one is to denote by the same symbol the matrices $J_{ij}$ of $\so(n)$ and the endomorphisms of $V$ that represent them (and that we are searching for). The second one is to improperly speak about $J_{ij}^2$ as elements of $\so(3)$ when they are in fact in the universal covering. If we denote by $(ij)$ the rotation in the plane generated by the directions $i$ and $j$, the algebra has the block structure (here for $\so(8)$)
\begin{equation}
	\begin{matrix}
		\begin{matrix}
			(12)\\\phantom{(12)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(13)&(14)\\
			(23)&(24)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(15)&(16)\\
			(25)&(26)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(17)&(38)\\
			(27)&(28)
		\end{matrix}	
		}
		\\
		\begin{matrix}
			\phantom{(12)}\\\phantom{(33)}
		\end{matrix}
		&
		\begin{matrix}
			\phantom{(12)}&(34)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(35)&(36)\\
			(45)&(46)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(37)&(38)\\
			(47)&(48)
		\end{matrix}	
		}
		\\
		&&
		\begin{matrix}
			\phantom{(12)}&(56)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(57)&(58)\\
			(67)&(68)
		\end{matrix}	
		}
		\\
		&&&
		\begin{matrix}
			\phantom{(12)}&(78)
		\end{matrix}
	\end{matrix}
\end{equation}
The elements $A_p=J_{2p-1,2p}$ are simultaneously diagonalisable (because they form an abelian subalgebra). We denote by $| m_1,m_2,\ldots \rangle$ the eigenvector for the corresponding eigenvalue
\begin{equation}
	A_p| \overline{ m } \rangle=m_p| \overline{ m } \rangle.
\end{equation}
The remaining four operators in each block can be arranged in the following way (here in the block between $A_1$ and $A_2$):
\begin{subequations}		\label{subEqLppmmettouteca}
	\begin{align}
		L_{12}^{++}&=-J_{13}+iJ_{14}+iJ_{23}+J_{24}\\
		L_{12}^{+-}&=J_{13}+iJ_{14}-iJ_{23}+J_{24}\\
		L_{12}^{-+}&=J_{13}-iJ_{14}+iJ_{23}+J_{24}\\
		L_{12}^{--}&=-J_{13}-iJ_{14}-iJ_{23}+J_{24}.
	\end{align}
\end{subequations}
Using the fact that the $J_{ij}$ are Hermitian, we see that $(L_{pq}^{\pm\pm})^*=L_{pq}^{\mp\mp}$. These operators are build in such a way to have
\begin{subequations}		\label{SubEqCommsLppmmettoutca}
	\begin{align}
		[A_p,L_{pq}^{\pm\,.}]&=\pm L_{pq}^{\pm\,.}\\
		[A_q,L_{pq}^{.\,\pm}]&=\pm L_{pq}^{.\,\pm}
	\end{align}
\end{subequations}
These relations make that 
\begin{equation}
	L_{pq}^{\pm\pm}| \overline{ m } \rangle\,\propto\, | \ldots,m_p\pm 1,\ldots,m_q\pm 1,\ldots \rangle.
\end{equation}
To find the proportionality coefficient is one part of our work now. We denote by $j_p$ the maximal eigenvalue of $A_p$. Using the matricial expression $J_{ij}=i(E_{ij}-E_{ji})$ and the multiplication formula \eqref{EqFormMulEmtr}, we find
\begin{equation}			\label{EqJsqnmunmtu}
	J^2=\sum_{ij}(J_{ij})^2=(n-1)\mtu
\end{equation}
which is thus a Casimir operator that is represented as a multiple of identity\footnote{Take care that the expression \eqref{EqJsqnmunmtu} is an expression for $J^2$ in its definition representation. This is not the expression of $J^2$ acting on $| \overline{ m } \rangle$. Indeed we make a systematic abuse between elements $J_{ij}$ in $\so(n)$ and the operators which represent them on the vector space $V$ generated by the ket $| \overline{ m } \rangle$.}.

Since we are searching for finite dimensional representations, each of $m_p$ have minimal and maximal value. To find these values is the second part of our work now. We denote by $j_p$ the maximal eigenvalue of $A_p$ and by 

Using the commutation relations and the fact that products like $J_{13}J_{24}$ vanish, a simple computation yields
\begin{equation}	\label{EqLppLmmundeux}
	L_{12}^{++}L_{12}^{--}=\sum_{(i,j)\in b_{12}} J_{ij}^2+2(A_1+A_2)
\end{equation}
where $b_{12}$ denotes the block associated with $A_1$ and $A_2$, namely $\sum_{(i,j)\in b_{12}}=J_{13}^2+J_{23}^2+J_{14}^2+J_{24}^2$. In the case of even $n$, we have $n/2$ elements $A_p$, each of them generating $(\frac{ n }{ 2 }-1)$ blocks, so that formula \eqref{EqLppLmmundeux} can be summed up on all the blocks to have
\begin{equation}
	\sum_{b}L_b^{--}L_b^{++}=J^2+(n-2)\sum_pA_p-\sum_pA_p^2.
\end{equation}
On the other hand, notice that in the definition representation, for each block the matrix $\sum_{(i,j)\in b}J_{ij}^2$ is diagonal, so that its belongs to the center of $\so(n)$. From, its representative is a multiple of identity and we can in fact simultaneously diagonalise the operators
\begin{equation}
	\{ A_p,J_b^2,J^2 \}.
\end{equation}
If we take $J_a^2$ in the equation \eqref{EqLppLmmundeux}, and apply that to $| \overline{ \jmath } \rangle$ we obtain
\begin{equation}
	J_a^2| \overline{ \jmath } \rangle	=\Big( J^2-\sum_{b\setminus a}J_b^2-\sum_pA_p \Big)| \overline{ \jmath } \rangle
	=\Big( (n-2)\sum_pj_p-2\sum_pj_p^2-\sum_{b\setminus a}j_b^2 \Big)| \overline{ \jmath } \rangle,
\end{equation}
so that we have the following constrain over the $j_b^2$:
\begin{equation}		\label{EqCondCompajsqsumjdeux}
	\sum_bj_j^2=(n-2)\sum_pj_p-2\sum_pj_p^2.
\end{equation}
Once these $j_b^2$ are fixed, the representation is fixed because
\begin{equation}
	\| L_a^{++}| \overline{ m } \rangle \|^2=\langle\overline{ m }| L_a^{--}L_a^{++} | \overline{ m } \rangle=\langle \overline{ m }| \big( J_a^2+2(A_{a_1}+A_{a_2}) \big)=j_a^2+2(m_{a_1}+m_{a_2}),
\end{equation}
Thus we have
\begin{equation}			\label{EqLppketmsqrtjjjldots}
	L_a^{++}| \overline{ m } \rangle=\sqrt{  j_a^2+2(m_{a_1}+m_{a_2})  }| \ldots,m_{a_1}+1,\ldots,m_{a_2}+1,\ldots \rangle.
\end{equation}
We are now going to prove that one can choice the values of $j_a^2$.

\begin{proposition}
	Different choices of $j_a^2$ give rise to equivalent representations.
\end{proposition}

\begin{probleme}
	Je ne sais pas où est la faute dans la preuve qui suit, mais cette proposition me semble fausse. Doit y avoir un moyen de fixer ces $j_a^2$.
\end{probleme}

\begin{proof}
	Let $\pi_1$ and $\pi_2$ be two representations of $\so(n)$ that differ only by the value of $J_a$ (for every $a$). We have
	\begin{subequations}
		\begin{align}
			\pi_1(L_a^{++})| \overline{ m } \rangle&=k_a^{++}| \overline{ n } \rangle\\
			\pi_2(L_a^{++})| \overline{ m } \rangle&=l_a^{++}| \overline{ n } \rangle.
		\end{align}
	\end{subequations}
	Notice that the numbers $k$ and $l$ are not arbitrary: they have to be of the form of the square root of equation \eqref{EqLppketmsqrtjjjldots} for some $j_a^2$ that fulfil compatibility condition \eqref{EqCondCompajsqsumjdeux}. We are searching for an operator $A\colon V\to V$ such that 
	\begin{subequations}
		\begin{align}
			\pi_1(L_a^{++})A| \overline{ m } \rangle&= A\pi_2(L_a^{++})| \overline{ m } \rangle\\
			\pi_1(J_p)A| \overline{ m } \rangle&= A\pi_2(J_p)| \overline{ m } \rangle
		\end{align}
	\end{subequations}
	where $\pi_1(J_p)=\pi_2(J_p)$. The numbers $k_a^{\pm\pm}$ and $l_a^{\pm\pm}$ are given by the representations $\pi_i$ and are non zero. We write $A$ under the form
	\begin{equation}
		A| \overline{ m } \rangle=x(\overline{ m })|\overline{ m } \rangle,
	\end{equation}
	so that the second condition is automatically satisfied. We have
	\begin{equation}
		\pi_1(L_a^{++})A| \overline{ m } \rangle=x(\overline{ m })\pi_1(L_a^{++})| \overline{ m } \rangle=x(\overline{ m })k_a^{++}(\overline{ m })| \overline{ m }_a^{++} \rangle
	\end{equation}
	where $\overline{ m }_a^{++}=(\ldots, m_{a_1}+1,\ldots,m_{a_2}+1,\ldots)$. The constrain is
	\begin{equation}
		A\pi_2(L_a^{++})\overline{ m }=Al_a^{++}(\overline{ m })| \overline{ m }_a^{++} \rangle=l_a^{++}x(\overline{ m }_a^{++})| \overline{ m }_a^{++} \rangle.
	\end{equation}
	One can choose one of the $x(\overline{ m })$ and then construct the other with the law
	\begin{equation}
		x(\overline{ m }_a^{\pm\pm})=\frac{ k_a^{\pm\pm}(\overline{ m }) }{ l_a^{\pm\pm}(\overline{ m }) }x(\overline{ m }).
	\end{equation}
\end{proof}
One choice of $j_a^2$ that respect the condition \eqref{EqCondCompajsqsumjdeux} is
\begin{equation}
	j_a^2=\frac{ n-2 }{2}(j_{a_1}+j_{a_2})-(j_{a_1}^2+j_{a_2}^2).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Root spaces and Dynkin diagram for \texorpdfstring{$\so(2l)$}{so(21)}}
%---------------------------------------------------------------------------------------------------------------------------

The Cartan algebra of $\so(2l)$ is the abelian algebra generated by $\{ A_1,\ldots,A_l \}$ where $A_p=J_{2p-1,2p}$. The roots are of the form
\begin{equation}
	\pm A_p^*\pm A^*_q
\end{equation}
with $p\neq q$. Among of them, the positive roots are
\begin{equation}
	\alpha_p=A^*_p\pm A^*_q
\end{equation}
with $p<q$. The positive root $A_p+A_{p+k}$ is not simple, indeed
\begin{equation}
	\begin{aligned}[]
		A^*_p+A^*_{p+k}	&= (A^*_p-A^*_{p+1})+(A^*_{p+1}-A^*_{p+2})+\cdots+(A^*_{p+k-1}-A^*_{p+k})\\
		&\quad +2(A^*_{p+k}-A^*_{k-1})+\cdots+2(A^*_{l-2}-A^*_{l-1})\\
		&\quad +(A^*_{l-1}-A^*_l)+(A^*_{l-1}+A^*_l).
	\end{aligned}
\end{equation}
The simple roots of $\so(2l)$ are thus $A^*_p-A^*_{p+1}$ and $A^*_{l-1}+A^*_l$ with $p=1,\ldots,l-1$.

All the length are maximal (and equal to $1/\sqrt{2}$) and all the angles between two ``consecutive'' simple roots are the same, but the last one:
\begin{equation}
	(A^*_{l-1}-A^*_{l},A^*_{l-1}+A^*_{l})=0,
\end{equation}
thus the Dynkin diagram of $\so(2l)$ is given by
\begin{center}
   \input{pictures_tex/Fig_FGWjJBX.pstricks}
\end{center}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Orthogonal algebra in the odd dimensional case}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{An odd dimensional example: \texorpdfstring{$\so(5)$}{so5}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecsocinq}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Root spaces}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Elements of $\so(5)$ are arranged in the following picture:
\begin{equation}
	\begin{matrix}
		\begin{matrix}
			(12)\\\phantom{(12)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(13)&(14)\\
			(23)&(24)
		\end{matrix}	
		}
		&
		\boxed{
		\begin{matrix}
			(15)\\
			(25)
		\end{matrix}	
		}
		\\
		\begin{matrix}
			\phantom{(12)}\\\phantom{(33)}
		\end{matrix}
		&
		\begin{matrix}
			\phantom{(12)}&(34)\\\phantom{(45)}
		\end{matrix}
		&
		\boxed{
		\begin{matrix}
			(35)\\
			(45)
		\end{matrix}	
		}
	\end{matrix}
\end{equation}
Because of the fact that $J_{12}$ and $J_{34}$ commute and because of commutators \eqref{SubEqCommsLppmmettoutca}, the subalgebra
\begin{equation}
	\lH=\{ J_{12},J_{34} \}
\end{equation}
is a Cartan subalgebra of $\so(5)$.

Using the commutators \eqref{EqJJietaJcomm}, and the convention%
\footnote{One can take that convention because $J_{\alpha\beta}=-J_{\beta\alpha}$. One other possible convention would has been $X^{\alpha\beta}=-X^{\beta\alpha}$. In that case, the result is
\begin{equation}
	[J_{12},X^{\alpha\beta}J_{\alpha\beta}]=2i(X^{\beta 1}J_{2\beta}-X^{\beta 2}J_{1\beta}).
\end{equation}
}		% end of footnote.
that $X^{\alpha\beta}=0$ when $\beta\leq a$,  we find
\[ 
	[J_{12},X^{\alpha\beta}J_{\alpha\beta}]=i(X^{2\beta}J_{1\beta}-X^{1\beta}J_{2\beta}).
\]
Thus, if we are searching for vectors $X$ such that $[J_{12},X]=\lambda X$, we are lead to the equation
\begin{equation}
	i(X^{2\beta}J_{1\beta}-X^{1\beta}J_{2\beta})=\lambda X^{ab}J_{ab},
\end{equation}
and therms proportional to $J_{ab}$ with $a$ and $b$ different to $1$ and $2$ have to vanish. There only two possible values for $\lambda$, apart of zero,  are $\pm 1$. What we find is that $[A_1,X]=X$ implies that $X=\sum_{b=3,4}(J_{2b}+iJ_{1b})$. One finds similar results for $A_2$. One also finds that $[A_1,X]=0$ implies that $X^{2\beta}=X^{1\beta}=0$.

Putting all that together, one finds that
\begin{subequations}		\label{EqRootSOImpaircinq}
	\begin{align}
		\so(5)_{(\pm 1,0)}	&=\{ J_{25}\pm iJ_{15} \}\\
		\so(5)_{(0,\pm 1)}	&=\{ J_{45}\pm iJ_{35} \}\\
		\so(5)_{(1,\pm 1)}	&=\{ \mp J_{13}+iJ_{14}\pm iJ_{23}+J_{24} \}\\
		\so(5)_{(-1,\pm 1)}	&=\{ \pm J_{13}-iJ_{14}\pm iJ_{23}+J_{24} \}.
	\end{align}
\end{subequations}
The positivity of the root $(a,b)$ is defined by the positivity of $a$, or the one of $b$ when $a$ is vanishing. The positive roots of $\so(5)$ are $(1,0)$, $(1,\pm 1)$ and $(0,1)$:
\begin{equation}
	\begin{aligned}
		\alpha_1&=(1,0),	&\alpha_2&=(0,1),\\
		\alpha_3&=(1,1),	&\alpha_4&=(1,-1),
	\end{aligned}
\end{equation}
among which $\alpha_2$ and $\alpha_4$ are simple. Indeed, the root $(0,1)$ is obviously simple. The root $(1,1)$ is not simple, as sum of $(1,0)$ and $(0,1)$; the root $(1,0)$ is the sum of $(1,1)$ and $(1,-1)$ and is thus not simple. We are left with the simple roots $(1,-1)$ and $(0,1)$. So we have
\begin{subequations}
	\begin{align}
		\alpha_2	&=A_2^*,\\
		\alpha_4	&=A_1^*-A_2^*
	\end{align}
\end{subequations}
and the dual basis is
\begin{subequations}
	\begin{align}
		h_2	&=A_1+A_2.\\
		h_4	&=A_1
	\end{align}
\end{subequations}

One shows easily compute the inner product \eqref{EqDefInnprHestrar}:
\begin{equation}		\label{EqPridScalacbdroot}
	\big( (a,b),(c,d) \big)	= \frac{1}{ 4 }(ac+bd).
\end{equation}
The length of $(a,b)$ is thus $\frac{1}{ 4 }(a^2+b^2)$. The angle between $\alpha_1$ and $\alpha_2$ is given by
\begin{equation}
	\cos(\theta_{\alpha_1,\alpha_2})=\frac{ (\alpha_1,\alpha_2) }{ \sqrt{(\alpha_1,\alpha_2)(\alpha_4,\alpha_4)} }=\frac{ \frac{1}{ 4 }(-1) }{ \sqrt{ \frac{1}{ 4 }\frac{1}{ 4 }2  } }=-\frac{1}{  \sqrt{2} }.
\end{equation}
Thus we have $\theta_{\alpha_1,\alpha_2}=135\degree$, and the Dynkin diagram of $\so(5)$ is
\begin{center}
   \input{pictures_tex/Fig_QPcdHwP.pstricks}
\end{center}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Irreducible representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We saw around equation \eqref{EqWnmoinvlambldarootmodul} that an irreducible representation of $\so(5)$ has to look like $f_1^{i_1}\cdots f_m^{i_m}v\lambda$. In our case, we consider $\lambda$, the highest weight and we denote by $| \lambda \rangle$ the cyclic vector which has the following properties
\begin{subequations}
	\begin{align}
		L^{+}_{1(5)}| \lambda \rangle=L^{+}_{2(5)}| \lambda \rangle&=L^{1\pm}_{12}| \lambda \rangle=0\\
		A_p| \lambda \rangle&=\lambda(A_p)| \lambda \rangle.
	\end{align}
\end{subequations}
Proposition \ref{PropoIrrrgenffflamble} says that a general element of the carrying space $W$ of an irreducible representation of $\so(5)$ is given by
\begin{equation}
	| i_1i_2i_3i_4 \rangle = \big( L^-_{1(5)} \big)^{i_1} \big( L_{2(5)}^- \big)^{i_2}\big( L^{--}_{12} \big)^{i_3}\big( L^{-+}_{12} \big)^{i_4}	| \lambda \rangle
\end{equation}
In order to know to which root space it belongs, we apply $A_p$ on that\footnote{In fact, equation \eqref{Eqfmlaphamoinsmouns} answers the question. The computation that we give here is more pedestrian, but still is independent of any explicit matricial choices, and is easily generalisable to others $\so(n)$ algebras.}. Using the relation \eqref{Eqhjfikplusun} and the fact that $\alpha_{1(5)}(A_1)=1$, we find
\begin{equation}
	A_1\big( L^-_{1(5)} \big)^{i_1}=-i_1\big( L^-_{1(5)} \big)^{i_1}+\big( L^-_{1(5)} \big)^{i_1}A_1.
\end{equation}
Since $\alpha_{2(5)}(A_1)=0$, we also have $A_1\big( L_{2(5)}^- \big)^{i_2}=\big( L_{2(5)}^- \big)A_1$. One found the other ones in the same way. The result is
\begin{equation}
	A_1| i_1i_2i_3i_4 \rangle=\big( \lambda(A_1)-i_1-i_3-i_4 \big)| i_1i_2i_3i_4 \rangle,
\end{equation}
which has to be seen as a straight generalisation of \eqref{EqJtroisJpmmplusun}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{List of irreducible representations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let us determine the possible irreducible representations of $\so(5)$ using theorem \ref{Thoirrepllamifffmor}. The simple roots are $\alpha_2=(0,1)$ and $\alpha_4=(1,-1)$. A simple computation with equation \eqref{EqPridScalacbdroot} and $\Lambda=(a,b)$ gives
\begin{equation}
	\begin{aligned}[]
		\Lambda_{(1,0)}&=2b\\		
		\Lambda_{(1,-1)}&=a-b.
	\end{aligned}
\end{equation}
The requirement these two to be nonnegative integer produces the list of the irreducible representations:
\begin{equation}			\label{EqIrrepsso5}
	\begin{aligned}[]
		b&\in \eN/2\\
		a&=b,b+1,\ldots
	\end{aligned}
\end{equation}
In particular, $a$ and $b$ are both integer or half-integer.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Cartan matrix and Dynkin diagram of \texorpdfstring{$\so(2l+1)$}{so2l+1}}
%---------------------------------------------------------------------------------------------------------------------------

The Cartan subalgebra of $\lG=\so(2l+1)$ is generated by the elements $\lH=\{ J_{12},J_{34},\ldots,J_{2l-1,2l} \}$.  We denote them by
\begin{equation}
	B_p=J_{2p-1,2p}
\end{equation}
with $p=1,\ldots,l$. As explained in equations \eqref{EqRootSOImpaircinq}, the roots are $\pm B_p^*\pm B_q^*$ and $\pm B_p^*$ with $p=1,\ldots l$. The root spaces of the roots $\pm B^*_p$ are the combinations $J_{1,2l+1}\pm iJ_{2,2l+1}$, $J_{3,2l+1}\pm iJ_{4,2l+1},\ldots$

The positive roots are
\begin{equation}
	\begin{aligned}[]
		B_p^*\pm B_q^*	&&	\text{and}	&& B_p^*
	\end{aligned}
\end{equation}
with $p<q$. In order to determine the simple roots, first remark that $B_p^*+B_q^*$ is the sum of $B_p^*$ and $B_q^*$. The root $B_p^*$ (with $p\neq l$) is not simple because
\begin{equation}
	B^*_p=(B^*_p-B^*_q)+B^*_q.
\end{equation}
If $k>1$, we have
\begin{equation}
	B_p^*-B^*_{p+k} = (B_p^*-B^*_{p+k-1} )+(B_{p+k-1}^*-B^*_{p+k} ).
\end{equation}
Thus the simple roots for $\so(2l+1)$ are
\begin{equation}			\label{EqRacinesSimplessol}
	\begin{aligned}[]
		\alpha_i &=B^*_i-B^*_{i+1}	&	\text{and}	&&	\alpha_l &=B^*_l,
	\end{aligned}
\end{equation}
$i=1,\cdots,l-1$. The inversion of that is easy: 
\begin{equation}		\label{EqsonBenFnDesAlpha}
	B^*_i=\alpha_i+\alpha_{i+1}+\cdots+\alpha_l,
\end{equation}
and in particular, $B^*_l=\alpha_l$. Now we have to determine the basis $\{ t_{\alpha_i}\}$ defined by
\begin{equation}
	\alpha_i(h)=B(t_{\alpha_i},h)
\end{equation}
for every $h\in\lH$. In the dual basis of $\{\alpha_i\}$ if $h-h_j\alpha_j^*$, we have $\alpha_i(h)=h_j\delta_{ij}=h_i$. We have
\begin{equation}
	h_i=B(t_{\alpha},h)=h_jB_{kl}(t_{\alpha_i})_k\underbrace{(\alpha_j^*)_l}_{=\delta_{jl}}=h_jB_{kj}(t_{\alpha_i})_k,
\end{equation}
so we have
\begin{equation}
	(t_{\alpha_i})_l=B^{il}
\end{equation}
where $B^{ij}$ are the component of the inverse of the Killing form matrix in the basis $\{ \alpha_i^* \}$ of $\lH$. The inner product on $\lH^*$ is thus determined, in function of the Killing form, by
\begin{equation}
	B(t_{\alpha_i},t_{\alpha_j})=B_{kl}(t_{\alpha_i})_k(t_{\alpha_j})_l=B_{kl}B^{ik}B^{jl}=B^{ji},
\end{equation}
so that we write
\begin{equation}
	(\alpha_i,\alpha_j)=B^{ij}.
\end{equation}

We know that, for a vector space $V$, if $\xi_i=B_{ij} e^*_j$ is a basis of the dual $V^*$, then the dual basis of $V$ is given by $v_k=A_{kl}e_l$ with $A=(B^t)^{-1}$. In our case, $\alpha_i=B_i^*-B_{i+1}^*$, so that
\begin{equation}
	B =
	\begin{pmatrix}
		1	&	-1\\
		&	1	&	-1\\
		&	&	1	&	-1\\
		&	&	&	\ddots	&\ddots\\
		&	&	&		& 0&1
	\end{pmatrix}
\end{equation}
Inverting that matrix, one finds that
\begin{equation}
	\alpha_k = B_1^*+\cdots+B_k^*.
\end{equation}

Now, we have to find the matrix of the Killing for in the basis $\{ \alpha_i^* \}$. Since $B(B_k,B_l)=-2\delta_{kl}$, we have $B(\alpha_k^*,\alpha_l^*)=-2\min(k,l)$, and the inverse of that matrix is given by
\begin{equation}
	-B^ij=
	\begin{pmatrix}
		1		& -\frac{ 1 }{2}\\
		-\frac{ 1 }{2}	& 1			& -\frac{ 1 }{2}\\
		& -\frac{ 1 }{2}	& 1			&-\frac{ 1 }{2}\\
		&& \ddots		& \ddots		& \ddots\\
		&&			&			&	\frac{ 1 }{2}
	\end{pmatrix},
\end{equation}
so that
\begin{equation}		\label{EqProdsoimpairrac}
	\begin{aligned}[]
		(\alpha_k,\alpha_{k+1})&=\frac{ 1 }{2},	&	(\alpha_k,\alpha_k)&=-1,	&(\alpha_l,\alpha_l)&=-\frac{ 1 }{2}
	\end{aligned}
\end{equation}
when $k\leq l-1$. The Cartan matrix\index{Cartan!matrix!of $\so(2l+1)$} and the Dynkin diagram can now be written down easily:
\begin{equation}		\label{EqCartanSOimpair}
	A=
	\begin{pmatrix}
		2	& -1\\
		-1	& 2	& -1\\
		& -1	& 2		& -1\\
		&	& \ddots	& \ddots	& \ddots\\
		&	&		& -1		& 2	& -1\\
		&	&		& 		& -2	& 2
	\end{pmatrix}
\end{equation}
One sees form the products \eqref{EqProdsoimpairrac} that all the roots but $\alpha_l$ are of maximal length, so that the Dynkin diagram\index{Dynkin!diagram!of $\so(2l+1)$} has $l-1$ withe circles and one black one. Using proposition \ref{PropProdNbLignes}, we find that the Dynkin diagram of $\so(2l+1)$ is
%\begin{equation}
%	\input{image_Dynkin_so(2n+1)_toutes.pstricks}
%\end{equation}
\begin{equation}
   \input{pictures_tex/Fig_MNICGhR.pstricks}
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Irreducible representations of \texorpdfstring{$\so(2,l-1)$}{so2l}}
%---------------------------------------------------------------------------------------------------------------------------

We use the same technique as the one from which we deduced the values \eqref{EqIrrepsso5}, using theorem \ref{Thoirrepllamifffmor}. The simple roots $\{ \alpha_k \}$ are given in \eqref{EqRacinesSimplessol} and the inner product is \eqref{EqProdsoimpairrac}. We are thus able to compute the numbers $\Lambda_{\alpha_k}$ for every $\Lambda=\sum_{i=1}^la_iB^*_i$. Let $\Lambda=\sum_{i=1}^la_iB^*_i$; taking \eqref{EqsonBenFnDesAlpha} into account, we compute first $\Lambda_{\alpha_j}$. With $B_k^*$ ($k<l$), we have
\begin{equation}
	(B^*_k,\alpha_l)=(\alpha_{l-1}+\alpha_l,\alpha_l)=\frac{ 1 }{2}-\frac{ 1 }{2}=0,
\end{equation}
so we have $\sum_{i=1}^la_i(B^*_i,\alpha_l)=-\frac{ a_l }{ 2 }$, and
\begin{equation}
	\Lambda_{\alpha_l}=2a_l.
\end{equation}
From that, we deduce $a_l\in\eN/2$. The computation of $\Lambda_{\alpha_1}$ is easy too because $(\alpha_k,\alpha_1)\neq 0$ only when $k=1$ or $k=2$. So $\Lambda_{\alpha_1}=-2(a_1B^*_1+a_2B^*_2,\alpha_1)=a_1-a_2$. That result generalises to
\begin{equation}
	\Lambda_{\alpha_k}=a_k-a_{k+1}.
\end{equation}
Since $a_l$ must belongs to $\eN/2$, we deduce that the irreducible representations of $\so(2l+1)$ are parametrized by
\begin{equation}
	a_l\geq a_{l-1}\geq\ldots\geq a_1
\end{equation}
which are all integer or half integer.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Elementary and basic representations of \texorpdfstring{$\so(2l+1)$}{so2l+1}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}
Attention : ici j'utilise la formule \eqref{EqCofDynMmoisAlpha}, et je crois que je l'utilise avec la transposée de la matrice de Cartan; j'ai peut-être une faute de convention qui invalide la fin de la récurrence.
\end{probleme}

The Dynkin diagram of $so(2l+1)$ is
\begin{equation}		\label{EqDynkSOimpair}
	%\input{image_Dynkin_so(2n+1).pstricks}
   \input{pictures_tex/Fig_LEJNDxI.pstricks}
\end{equation}
We are looking at the elementary representation
\begin{equation}
	%\tau_1=\input{image_Dynkin_so(2n+1)_tau_1.pstricks}.
    \tau_1=\input{pictures_tex/Fig_RGjjpwF.pstricks}
\end{equation}
Other basic representations on the same branch are given by
\begin{equation}		\label{EqReprBasTausoimpair}
	\tau_k\big( \tau_1^{\otimes k} \big)_1.
\end{equation}
Notice, however, that the last line in the diagram \eqref{EqDynkSOimpair} is not of the kind which is allowed to form a branch. Thus the process \eqref{EqReprBasTausoimpair} does not produce the representation
\begin{equation}
	%\sigma = \input{image_Dynkin_so(2n+1)_sigma.pstricks},
   \input{pictures_tex/Fig_STdyNTH.pstricks}
\end{equation}
which is called the \defe{spinor representation}{spin!representation!of $\so(2l+1)$}. Let us now determine the list of weights for the representation $\tau_1$. As showed on the diagram \eqref{EqReprBasTausoimpair}, the Dynkin coefficients of the highest weight are
\begin{equation}
	\Lambda_i=
	\begin{pmatrix}
		1\\
		0\\
		\vdots
	\end{pmatrix},
\end{equation}
and we have $q(\Lambda,\alpha_i)=0$ for every $i$. Thus $\Lambda_i+q(\Lambda,\alpha_i)\geq 1$ only for $i=1$, and the only weight in the first layer is
\begin{equation}
	M^{(1)}=\Lambda-\alpha_1.
\end{equation}
Using formula \eqref{EqCoefDynkMalpha} and the first line of the Cartan matrix \eqref{EqCartanSOimpair}, we find
\begin{equation}
	M^{(1)}_i=
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}-
	\begin{pmatrix}
		2\\-1\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		-1\\1\\0\\\vdots
	\end{pmatrix},
\end{equation}
and, by construction,
\begin{equation}
	q(M^{(1)},\alpha_i)=
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}.
\end{equation}
In order to know what belongs to the second layer, we look at the lucky numbers of $M^{(1)}-\alpha_i$:
\begin{equation}
	M^{(1)}_i+q(M^{(1)},\alpha_i)=
	\begin{pmatrix}
		-1\\1\\0\\\vdots
	\end{pmatrix}+
	\begin{pmatrix}
		1\\0\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		0\\1\\0\\\vdots
	\end{pmatrix},
\end{equation}
and we deduce that $M^{(2)}=M^{(1)}-\alpha_2$ is the only weight in the second layer. Its Dynkin coefficients are given by
\begin{equation}
	M^{(2)}_i=
	\begin{pmatrix}
		-1\\1\\0\\0\\\vdots
	\end{pmatrix}-
	\begin{pmatrix}
		-1\\2\\-1\\0\\\vdots
	\end{pmatrix}=
	\begin{pmatrix}
		0\\-1\\1\\0\\\vdots
	\end{pmatrix}.
\end{equation}
We can now proceed by induction. Let us suppose that
\begin{equation}
	\begin{aligned}[]
		M^{(k)}_i&=
		\begin{pmatrix}
			0\\\vdots\\-1\\1\\0\\\vdots
		\end{pmatrix},&\text{and}&
		&q(M^{(j)},\alpha_i)=
		\begin{pmatrix}
			0\\\vdots\\1\\0\\0\\\vdots
		\end{pmatrix}
	\end{aligned}
\end{equation}
where in both, the first non zero element is on the $k$th line. The, we see that the only weight in the next layer is
\begin{equation}
	M^{(k+1)}=M^{(k)}-\alpha_{k+1},
\end{equation}
and, using the corresponding line of the Cartan matrix, we find
\begin{equation}
	M^{(k+1)}_i=
	\begin{pmatrix}
		0\\\vdots\\-1\\1\\0\\0\\\vdots
	\end{pmatrix}
	-
	\begin{pmatrix}
		0\\\vdots\\-1\\2\\-1\\0\\\vdots
	\end{pmatrix}
	=
	\begin{pmatrix}
		0\\\vdots\\0\\-1\\1\\0\\\vdots
	\end{pmatrix}.
\end{equation}
The induction finishes with
\begin{equation}
	M^{(l)}=M^{(l-1)}-\alpha_l
\end{equation}
which as the following Dynkin coefficients:
\begin{equation}
	M^{(l)}_i=M^{(l-1)}_i-A_{li}=
\begin{pmatrix}
\vdots\\0\\-1\\1
\end{pmatrix}
-
\begin{pmatrix}
\vdots\\0\\-2\\2
\end{pmatrix}
=
\begin{pmatrix}
\vdots\\0\\1\\-1
\end{pmatrix}.
\end{equation}
Notice that
\begin{equation}
	M^{(l)}_i+q(M^{(l)},\alpha_i)=
\begin{pmatrix}
\vdots\\0\\1\\0
\end{pmatrix},
\end{equation}
so that
\begin{equation}
	M^{(l+1)}=M^{(l)}-\alpha_{l-1}
\end{equation}
is a weight, and its Dynkin coefficients are
\begin{equation}
	\begin{pmatrix}
\vdots\\0\\1\\-1
\end{pmatrix}
-
\begin{pmatrix}
\vdots\\-1\\2\\-1
\end{pmatrix}
=
\begin{pmatrix}
\vdots\\1\\-1\\0
\end{pmatrix}
\end{equation}
with
\begin{equation}
	q(M^{(l+1)},\alpha_i)=
\begin{pmatrix}
\vdots\\0\\1\\0
\end{pmatrix}.
\end{equation}
At this point, a new induction can be done up to get
\begin{equation}
	M^{(2l-1)}=M^{(2l-2)}-\alpha_1.
\end{equation}
Thus we have

\begin{equation}
			M^{(2l-1)}_i=
\begin{pmatrix}
1\\-1\\0\\\vdots
\end{pmatrix}
-
\begin{pmatrix}
2\\-1\\0\\\vdots
\end{pmatrix}=
\begin{pmatrix}
-1\\0\\0\\\vdots
\end{pmatrix}
\end{equation}
and
\begin{equation}
q(M^{(2l-1)},\alpha_i)
=
\begin{pmatrix}
1\\0\\0\\\vdots
\end{pmatrix}
\end{equation}
We immediately check that there are no $M^{(2l)}$. Thus the Dynkin coefficients of the weights of the representation are
\begin{equation}
	\begin{aligned}[]
		\begin{pmatrix}
1\\0\\0\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
\vdots\\1\\-1\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
\vdots\\-1\\1\\\vdots
\end{pmatrix},&&
\begin{pmatrix}
-1\\0\\0\\\vdots
\end{pmatrix}.
	\end{aligned}
\end{equation}
We have $l-1$ weights of each of the two middle types. Thus the representations is $2l$-dimensional.
