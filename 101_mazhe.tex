% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Chevalley basis}
%---------------------------------------------------------------------------------------------------------------------------

The Chevalley basis corresponds to an other choice of normalization of the element \( e_{\alpha}\), \( h_{\alpha}\). If we set
\begin{subequations}
    \begin{numcases}{}
        H_{\alpha}=K_{\alpha}t_{\alpha}\\
        E_{\alpha}=N_{\alpha}e_{\alpha}
    \end{numcases}
\end{subequations}
with
\begin{equation}
    \begin{aligned}[]
        K_{\alpha}&=\frac{ 2 }{ (\alpha,\alpha) }\\
        N_{\alpha}&=\sqrt{\frac{ 2 }{ B(e_{\alpha},e_{-\alpha})(\alpha,\alpha) }},
    \end{aligned}
\end{equation}
then we have the \defe{Chevalley relations}{Chevalley!basis}:
\begin{subequations}        \label{EqsChevalleuRels}
    \begin{align}
        [E_{\alpha},E_{-\alpha}]&=H_{\alpha}\\
        [H_{\alpha},E_{\beta}]&=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }E_{\beta}\\
        [H_{\alpha},H_{\beta}]&=0.
    \end{align}
\end{subequations}
The last relation is nothing else than the fact that the Cartan subalgebra \( \lH\) is abelian. Notice that we don't give relations between \( E_{\alpha}\) and \( E_{\beta}\). Of course \( [E_{\alpha},E_{\beta}]\sim E_{\alpha+\beta}\) but the spaces \( \lG_{\alpha}\) and \( \lG_{\beta}\) being Killing orthogonal, the Killing does not provides a natural relative normalisation between \( E_{\alpha}\) and \( E_{\beta}\).

\begin{definition}  \label{DefORftFjP}
    If \( \{ \alpha_i \}_{i=1,\ldots,l}\) is the set of simple roots, we consider the notation \( X_i^+=E_{\alpha_i}\), \( X^-_i=E_{-\alpha_i}\), \( H_i=H_{\alpha_i}\) and we introduce the \defe{Cartan matrix}{Cartan!matrix}
    \begin{equation}
        A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }.
    \end{equation}
\end{definition}
Reduced to the simple roots the relations \eqref{EqsChevalleuRels} become
\begin{equation}        \label{EqChevalleySimple}
    \begin{aligned}[]
        [X^+_i,X^-_j]&=\delta_{ij}H_i\\
        [H_i,X^{\pm}_j]&=\pm A_{ij}X^{\pm}_j\\
        [H_i,H_j]&=0.
    \end{aligned}
\end{equation}
The first relation comes from the fact that \( \alpha_i-\alpha_j\) is not a root when \( \alpha_i\) and \( \alpha_j\) are simple roots. 

\begin{remark}
    The idea behind the Chevalley relations  is that the algebra \( \lG\) is generated by the elements \( X^{\pm}_i\), \( H_i\) and the commutation relations \eqref{EqChevalleySimple}. Even if these elements do not form a basis (while the elements \( E_{\alpha}\), \( H_{\alpha}\) do), one can define a function on \( \lG\) by giving its values on \( X^{\pm}_i\) and \( H_i\) providing one has a canonical way to extend it on commutators.

    The definition \ref{EqDefCobrackStandard} of standard cobracket works in this way.
\end{remark}

\begin{example}
    The basis \( \{ H,E,F \}\) given by \eqref{EqsXPdnZlG}:
    \begin{equation} 
    H=\begin{pmatrix}
    1 & 0 \\
    0 & -1
    \end{pmatrix}
    ,\quad
      E=\begin{pmatrix}
    0 & 1 \\
    0 & 0
    \end{pmatrix}
    ,\quad
     F=\begin{pmatrix}
    0 & 0 \\
    1 & 0
    \end{pmatrix},
    \quad
    T=\begin{pmatrix}
    0&1\\
    -1&0
    \end{pmatrix}
    \end{equation}
    satisfy the relations \eqref{subEqsSBhuAWx}
    \begin{subequations}    
        \begin{align}
            [H,E]&=2E\\
            [H,F]&=-2F\\
            [E,F]&=H.
        \end{align}
    \end{subequations}
    that are nothing else than the relations \eqref{EqChevalleySimple}.

    The only positive root is \( \alpha(H)=2\). The Cartan matrix\footnote{Definition \ref{DefORftFjP}.} reduces to one number: 
    \begin{equation}
        A=A_{11}=\frac{ 2(\alpha,\alpha) }{ (\alpha,\alpha) }=2.
    \end{equation}

    The Killing form, in the basis \( \{ H,E,F \}\) is given by
    \begin{equation}
        B=\begin{pmatrix}
            8    &   0    &   0    \\
            0    &   0    &   4    \\
            0    &   4    &   0
        \end{pmatrix}
    \end{equation}
    and the element \( t_{\alpha}\) is then
    \begin{equation}
        t_{\alpha}=\frac{1}{ 4 }H.
    \end{equation}
    The inner product on \( \lH^*\) is then
    \begin{equation}        \label{Eqinnerhstarsldc}
        (\alpha,\alpha)=B(t_{\alpha},t_{\alpha})=\frac{ 1 }{2}.
    \end{equation}
\end{example}

\begin{remark}
    Notice that these relations do not give the value of
    \begin{equation}
        [E_{\alpha},E_{\beta}]=N_{\alpha,\beta}E_{\alpha+\beta}
    \end{equation}
    when \( \alpha+\beta\) is a root.
\end{remark}

\begin{probleme}
    It has to be possible to compute \( N_{\alpha,\beta}\), but I do not know how. The answer is given in equation \eqref{EqChevalleyBasis} but I don't know where I got them. Maybe there are some hints in \cite{Cornwell} (Il faut ajouter Cornwell à la bibliographie et enlever le problème \ref{ProbAvecCorwell}).
\end{probleme}

\begin{probleme}
    It seems that \( A_{ij}\) is the larger integer \( k\) such that \( \alpha_i+k\alpha_j\) is a root. This is the justification of the other Serre's relations that read
    \begin{equation}
        \ad^{1-A_{ij}}(X^{\pm}_i)X^{\pm}_j=0.
    \end{equation}
    That relation has to be written with the Chevalley's ones.
\end{probleme}

One can choose the coefficients in a more scientific way\cite{SerreSSAlgebres}. Let \( \alpha\) be a positive root, let \( H_{\alpha}\) be the inverse root of \( \alpha\) and \( e_{\alpha}\in\lG_{\alpha}\). We have
\begin{equation}
    [e_{\alpha},e_{\beta}]=\begin{cases}
        N_{\alpha,\beta}e_{\alpha+\beta}    &   \text{if \( \alpha+\beta\) is a root}\\
        0    &    \text{if \( \alpha+\beta\) is not a root}.
    \end{cases}
\end{equation}
We are going to find a multiple \( E_{\alpha}\) of \( e_{\alpha}\) in such a way to have in the same time
\begin{subequations}
    \begin{numcases}{}
        [E_{\alpha},E_{-\alpha}]=H_{\alpha}\\
        N_{\alpha,\beta}=-N_{-\alpha,-\beta}.
    \end{numcases}
\end{subequations}

Let \( \sigma\) be an involutive automorphism of \( \lG\) such that \( \sigma|_{\lH}:-\id\). First we have \( \sigma(\lG_{\alpha})=\lG_{-\alpha}\) because
\begin{equation}
    [h,\sigma(e_{\alpha})]=\sigma[\sigma(h),e_{\alpha}]=-\sigma\alpha(h)e_{\alpha}=-\alpha(h)\sigma(e_{\alpha})
\end{equation}
for every \( h\in\lH\) and \( e_{\alpha}\in\lG_{\alpha}\). From corollary \ref{CorrExistInverseRoot} there exist a number \( a_{\alpha}\) such that
\begin{equation}
    [e_{\alpha},\sigma(e_{\alpha})]=a_{\alpha} H_{\alpha}.
\end{equation}
We pose 
\begin{subequations}
    \begin{numcases}{}
        E_{\alpha}=\frac{1}{ \sqrt{-a_{\alpha}} }e_{\alpha}\\
        E_{-\alpha}=-\sigma(E_{\alpha}).
    \end{numcases}
\end{subequations}
With that choice we immediately have \( [E_{\alpha},E_{-\alpha}]=H_{\alpha}\). We also have \( N_{\alpha,\beta}=-N_{-\alpha,-\beta}\); in order to see it, consider
\begin{equation}
    [\sigma E_{\alpha},\sigma E_{\beta}]=\sigma[E_{\alpha},E_{\beta}]=N_{\alpha,\beta}\sigma(E_{\alpha+\beta})=-N_{\alpha,\beta}E_{-\alpha-\beta}.
\end{equation}
But the same is also equal to
\begin{equation}
    [-E_{-\alpha},-E_{-\beta}]=[E_{-\alpha},E_{-\beta}]=N_{-\alpha,-\beta}E_{-\alpha,-\beta}.
\end{equation}

\begin{proposition}
    With these choices we have
    \begin{equation}
        N_{\alpha,\beta}=\pm(p+1)
    \end{equation}
    where \( p\) is the largest integer \( j\) such that \( \beta-j\alpha\) is a root.
\end{proposition}

\begin{probleme}
    I don't know a proof of that, but \cite{SerreSSAlgebres} gives a reference.
\end{probleme}

From proposition \ref{Propoxalphaymoinaalpha} we know that \( t_{\alpha}\in\lH_{\alpha}\), so that \( H_{\alpha}\) is a multiple of \( H_{\alpha}\). The proportionality factor is easy to fix since
\begin{equation}        \label{EqRealsHalpatalphaNorsmd}
    \begin{aligned}[]
        \alpha(H_{\alpha})&=2 &\text{definition of $H_{\alpha}$}\\
        \alpha(t_{\alpha})&=(\alpha,\alpha) &\text{definition \eqref{EqDefInnprHestrar}}.
    \end{aligned}
\end{equation}
Thus \( H_{\alpha}=\frac{ 2 }{ (\alpha,\alpha) }t_{\alpha}\) and 
\begin{equation}
    [H_{\alpha},E_{\beta}]=\beta(H_{\alpha})E_{\beta}=\frac{ 2 }{ (\alpha,\alpha) }\beta(t_{\alpha})=\frac{ 2(\alpha,\beta) }{ (\alpha,\beta) }
\end{equation}
again by the definition \eqref{EqDefInnprHestrar}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Coefficients in the Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

In this section we search to give the form of the coefficients in the Cartan matrix. We will show that the values of \( (\alpha,\beta)\) are quite restricted.

\begin{remark}
    The notations are not standard. Here the symbol \( \Delta\) denotes the set of \emph{simple} roots while the set of all roots is denoted by \( \Phi\). In the book \cite{Cornwell}, the symbol \( \Delta\) is the set of all roots. This makes quite a difference !
\end{remark}

\begin{definition}
    If \( \alpha\) and \( \beta\) are roots of the complex semisimple Lie algebra \( \lG\), then the \defe{\( \alpha\)-string}{string!of roots} containing \( \beta\) is the set of roots of the form \( \alpha+k\beta\) with \( k\in\eZ\).
\end{definition}

Among other things, the following proposition shows that a string has no gap.
\begin{proposition}     \label{Proppqasbabaa}
    Let \( \alpha,\beta\in\Phi\). Then there exits integers \( p,q\) such that \( \{ \beta+k\alpha \}_{-p\leq k\leq q} \) is the \( \alpha\)-string containing \( \beta\). The numbers \( p\) and \( q\) satisfy
    \begin{equation}        \label{Eq2qbaaapmq}
        p-q=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }
    \end{equation}
    and the form
    \begin{equation}
        \beta-\frac{ 2(\beta,\alpha) }{ (\alpha,\alpha) }\alpha
    \end{equation}
    is a nonzero root.
\end{proposition}

\begin{proof}
    We consider the vector space
    \begin{equation}
        V=\bigoplus_{k\in\eZ}\lG_{\beta+k\alpha}
    \end{equation}
    and the Lie algebra \( \gsl(2)_{\alpha}=\langle e_{\alpha},f_{\alpha},h_{\alpha}\rangle\) defined in subsection \ref{SubSecCopiedeSLdansGi}. The latter acts on \( V\). Simple computation using the fact that \( \beta(h_{\alpha})=2(\alpha,\beta)/(\alpha,\alpha)\) shows that
    \begin{equation}
        [\frac{ 1 }{2}h_{\alpha},e_{\beta+k\alpha}]=\left( \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+k \right)e_{\beta+k\alpha}.
    \end{equation}
    Thus the matrix of \( \ad(\frac{ 1 }{2}h_{\alpha})\) is diagonal and has no multiplicity in its eigenvalues. We deduce that the representation if irreducible. From general theory of irreducible representations of \( \gsl(2)\) we know that there exists an half-integer number \( j\) such that the diagonal entries of \( \ad(\frac{ 1 }{2}h_{\alpha})\) take \emph{all} the values from \( -j\) to \( j\) by integer steps. Thus the \( \alpha\)-string containing \( \beta\) has the form \( \{ \beta+k\alpha \}_{-p\leq k\leq q}\) where \( p\) and \( q\) satisfy
    \begin{subequations}
        \begin{numcases}{}
            \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }-p=-j\\
            \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+q=j.
        \end{numcases}
    \end{subequations}
    Summing we get
    \begin{equation}       
        p-q=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }.
    \end{equation}
    If \( \lambda\) is an eigenvalue of \( \ad(\frac{ 1 }{2}h_{\alpha})\), then \( -\lambda\) is also an eigenvalue (this is still from the irreducible representation theory of \( \gsl(2)\)). The number \( (\alpha,\beta)/(\alpha,\beta)\) is obviously an eigenvalue (with \( k=0\)), thus the string contains a \( k\) such that
    \begin{equation}
        \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+k=-\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }.
    \end{equation}
    The solution is \( k=-2(\alpha,\beta)/(\alpha,\alpha)\) and we deduce that
    \begin{equation}
        \beta-2\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }\alpha
    \end{equation}
    is a root of \( \lG\).
\end{proof}

\begin{proposition}
    Let \( \alpha,\beta\) be two roots. Then we have
    \begin{equation}
        \frac{2(\alpha,\beta)}{(\alpha,\alpha)}=0,\pm 1,\pm 2,\pm 3.
    \end{equation}
\end{proposition}

\begin{proof}
    First, equation \eqref{Eq2qbaaapmq} shows that \( \frac{2(\alpha,\beta)}{(\alpha,\alpha)}\) is integer. If \( \alpha=\pm\beta\), the result is \( 2\). If \( \alpha\neq\pm\beta\), the vectors \( t_{\alpha}\) and \( t_{\beta}\) are linearly independent and the Schwarz inequality shows
    \begin{equation}        \label{EqprofmqxqCqrtmai}
        (\alpha,\beta)^2=| B(t_{\alpha},t_{\beta}) |< B(t_{\alpha},t_{\alpha})B(t_{\beta},t_{\beta})=(\alpha,\alpha)(\beta,\beta).
    \end{equation}
    Thus
    \begin{equation}        \label{EqprofmqxqCqrtmaii}
        \left| \frac{2(\alpha,\beta)}{(\alpha,\alpha)} \right| \left| \frac{2(\alpha,\beta)}{(\beta,\beta)} \right| <\frac{ 4| (\alpha,\beta)(\alpha,\beta) | }{ (\alpha,\beta)^2 }=4.
    \end{equation}
    Consequently the number \( | 2(\alpha,\beta)/(\alpha,\alpha) |\) being integer can only take the values \( 0\), \( 1\), \( 2\) and \( 3\). Notice that the inequality in \eqref{EqprofmqxqCqrtmai} and \eqref{EqprofmqxqCqrtmaii} are strict since \( \alpha_i\) is not collinear to \( \alpha_j\).
\end{proof}

\subsection{Simple roots}
%------------------------
As seen before, $\Phi$ admits an order inherited from $\lHeR^*$. A root $\alpha>0$ is \defe{simple}{simple!root} if it cannot be written as a sum of two positive roots.

\begin{theorem}      \label{ThoposrootnjajnZ}
    Let \( \{ \alpha_1,\ldots,\alpha_l \}\) be the set of simple roots. Then every root \( \beta\in\Phi\) can be decomposed into
    \begin{equation}
        \beta=\sum_{i=1}^ln_i\alpha_i
    \end{equation}
    where non vanishing the numbers \( n_i\in\eZ\) are either all positive or all negative.
\end{theorem}

\begin{proof}
    Let \( \beta\) be positive. If it is not simple, the one can decompose it into two positive roots:
    \begin{equation}
        \beta=\gamma+\delta
    \end{equation}
    with \( \gamma,\delta>0\). If \( \gamma\) and/or \( \delta\) are not simple, they can be decomposed further. This process has to be finite, indeed if the process is not finite, the decomposition of at least one positive root has to contains itself (because there are finitely many of them) while it is impossible to have \( \gamma=\gamma+\alpha\) with \( \alpha>0\).
\end{proof}

Two corollaries: a root is either positive or negative (this is part of the definition of positivity) and when a root is positive, its decomposition into simple roots has only positive coefficients.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl group}
%---------------------------------------------------------------------------------------------------------------------------
References about Weyl group: \cite{Knapp_reprez}. See also \cite{Cornwell}, page 530.

If \( \alpha\) is a root of \( \lG\) we define the \defe{symmetry}{symmetry!of a root} of \( \alpha\) as
\begin{equation}
    \begin{aligned}
        s_{\alpha}\colon \lH^*&\to \lH^* \\
        \beta&\mapsto \beta-\beta(H_{\alpha})\alpha
    \end{aligned}
\end{equation}
where \( H_{\alpha}\in\lH\) is the inverse root of \( \alpha\). Since \( \alpha(H_{\alpha})=2\) we have \( s_{\alpha}(\alpha)=-\alpha\). The group generated by the symmetries and the identity is the \defe{Weyl group}{Weyl!group}.

From what is said around equation \eqref{EqRealsHalpatalphaNorsmd} and the definition \( (\alpha,\beta)=\alpha(t_{\beta})\), we have
\begin{equation}
    s_{\alpha}(\beta)=\beta-\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }\alpha.
\end{equation}

We know from proposition \ref{Proppqasbabaa} that \( s_{\alpha}(\beta)\) is a root while there are only finitely many roots; thus the Weyl group is finite since there are only a finite number of maps from a finite set to itself.

The symmetries associated to roots are involutive:
\begin{equation}
    s_{\alpha}^2=\id.
\end{equation}
Indeed
\begin{equation}
    \begin{aligned}[]
        s^2_{\alpha}(\beta)&=s_{\alpha}\big( \beta-\beta(H_{\alpha})\alpha \big)\\
        &=\beta-\beta(H_{\alpha})-\big( \beta-\beta(H_{\alpha})\alpha \big)(H_{\alpha})\alpha\\
        &=\beta
    \end{aligned}
\end{equation}
if we take into account \( \alpha(H_{\alpha})=2\).

Relative to the symmetry \( s_{\alpha_i}\) we have the symmetry \( s_i\) on \( \lH\) defined by
\begin{equation}        \label{EqSymsiReltosalphai}
    s_i(h)=h-\alpha_i(h)H_i
\end{equation}
where \( h\in\lH\) and \( H_i\) is the inverse root of \( \alpha_i\).

\begin{remark}
    The simple roots \( \alpha_i\) {\bf are not} orthogonal.
\end{remark}

Let $\Delta$ be a reduced abstract root system on a real finite dimensional vector space $V$. The group $W$ generated by the $s_{\alpha}:\alpha\in\Delta$ is the \defe{Weyl group}{Weyl!group}\index{group!Weyl}.

\begin{proposition}     \label{PropWeylIsomalphai}
    The elements \( s_{\alpha_i}\) are isometries of \( \lH^*\), i.e.
    \begin{equation}
        \big( s_{\alpha_i}(\alpha),s_{\alpha_i}(\beta) \big)=(\alpha,\beta).
    \end{equation}
\end{proposition}

\begin{proof}
    For the sake of shortness, let us write
    \begin{equation}
        n_{i,\alpha}=\frac{ 2(\alpha_i,\alpha) }{ (\alpha_i,\alpha_i) }.
    \end{equation}
    We have \( t_{s_{\alpha_i}(\alpha)}=t_{\alpha}-n_{i,\alpha}t_{\alpha_i}\). Thus
    \begin{equation}
        B\big( t_{s_{\alpha_i}(\alpha)}, t_{s_{\alpha_i}(\beta)} \big)=B(t_{\alpha}-n_{i,\alpha}t_{\alpha_i},t_{\beta}-n_{i,\beta}t_{\alpha_i})
    \end{equation}
    distributing and taking into account the fact all the relations like \( B(t_{\alpha},t_{\alpha_i})=(\alpha,\alpha_i)\), the right hand side reduces to \( B(t_{\alpha},t_{\beta})=(\alpha,\beta)\).
\end{proof}

When \( \Phi\) is the root system, one can chose many different notions of positivity; each of them bring to different simple systems. It turns out that the action of the Weyl group on a simple system produces the simple system of an other choice of positivity on \( \Phi\).

\begin{lemma}       \label{LemalphajsPhipinjsasbab}
    If \( s_{\alpha_i}\alpha=s_{\alpha_i}\beta\), then \( \alpha=\beta\).
\end{lemma}

\begin{proof}
    The hypothesis \( s_{\alpha_j}(\alpha-\beta)=0\) provides
    \begin{equation}
        0=\alpha-\beta-\frac{ 2(\alpha-\beta,\alpha_j) }{ (\alpha_j,\alpha_j) }\alpha_j
    \end{equation}
    so that \( \alpha=\beta+z\alpha_j\) for some \( z\in\eC\). Thus we have
    \begin{equation}
        s_{\alpha_j}(\alpha)=s_{\alpha_j}(\beta)+zs_{\alpha_j}(\alpha_j)=s_{\alpha_j}(\alpha)-z\alpha_j.
    \end{equation}
    Thus \( z=0\) and \( \alpha=\beta\).
\end{proof}

\begin{proposition}     \label{PropsalphaisurjPhipmaj}
    Let \( \alpha_i\) a simple root. The set \( \Phi^+\setminus\{ \alpha_i \}\) is stable under \( s_{\alpha_i}\), i.e.
    \begin{equation}
        s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)=\Phi^+\setminus\{ \alpha_i \}.
    \end{equation}
\end{proposition}

\begin{proof}
    Let \( \lambda\in\Phi^+\) be a positive root. By theorem \ref{ThoposrootnjajnZ} we have
    \begin{equation}        \label{Eqllamsumajajajpos}
        \lambda=\sum_ja_j\alpha_j
    \end{equation}
    with \( a_j\geq 0\). Since \( \lambda\neq\alpha_i\) we have \( a_j>0\) for some \( j\neq i\). Indeed the only multiple of \( \alpha_i\) to be a root are \( 0\) and \( \pm\alpha_i\). Since \( \lambda\in\Phi^+\) and \( \lambda\neq \alpha_i\), none of these three solutions are taken into consideration.
    
    Let's apply \( s_{\alpha_i}\) on both sides of \eqref{Eqllamsumajajajpos}:
    \begin{equation}        \label{eqalialphaillamfracalphai}
        \begin{aligned}[]
            s_{\alpha_i}(\lambda)&=s_{\alpha_i}\big( \sum_ja_j\alpha_j \big)\\
            &=\sum_{j\neq i}a_js_{\alpha_i}(\alpha_j)+a_i\underbrace{s_{\alpha_i}(\alpha_i)}_{-\alpha_i}\\
            &=\sum_{j\neq i}a_j\alpha_j-\sum_{j\neq i}a_j\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\alpha_i-a_i\alpha_i
        \end{aligned}
    \end{equation}
    Since a root is either positive or negative, the coefficients are either \emph{all} positive or \emph{all} negative. Since all the coefficients (apart for the one of \( \alpha_i\)) are the same as the ones of \( \lambda\), the root \eqref{eqalialphaillamfracalphai} is positive.

    We still have to prove that \( s_{\alpha_i}(\lambda)\neq \alpha_i\). Indeed if \( s_{\alpha_i}(\lambda)=\alpha_i\) we have
    \begin{equation}
        \lambda=s_{\alpha_i}s_{\alpha_i}(\lambda)=s_{\alpha_i}(\alpha_i)=-\alpha_i,
    \end{equation}
    which contradicts the positivity of \( \lambda\).

    Up to now we proved that \( s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)\subset\Phi^+\setminus\{ \alpha_i \}\). If \( \lambda\in\Phi^+\setminus\{ \alpha_i \}\), then
    \begin{equation}
        \sigma=s_{\alpha_i}(\lambda)\in s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)\subset\Phi^+\setminus\{ \alpha_i \}
    \end{equation}
    and \( s_{\alpha_i}(\sigma)=\lambda\), so that \( \lambda\) is the image by \( s_{\alpha_i}\) of \( \sigma\in\Phi^+\setminus\{ \alpha_i \}\).
\end{proof}

\begin{theorem}      \label{ThosajBijSurpPpsmaj}
    The map \( s_{\alpha_j}\colon \Phi^+\setminus\{ \alpha_j \}\to \Phi^+\setminus\{ \alpha_j \}\) is bijective.
\end{theorem}

\begin{proof}
    Surjectivity is proposition \ref{PropsalphaisurjPhipmaj} while injectivity is lemma \ref{LemalphajsPhipinjsasbab}.
\end{proof}

\begin{lemma}[\cite{Cornwell}, page 533]
    We consider the half sum of the positive roots:
    \begin{equation}
        \delta=\frac{ 1 }{2}\sum_{\alpha\in\Phi^+}\alpha.
    \end{equation}
    We have
    \begin{enumerate}
        \item
            If \( \alpha_j\) is a simple root, \( s_{\alpha_j}\delta=\delta-\alpha_j\).
        \item
            If \( \alpha_j\) is a simple root, \( (\delta,\alpha_j)=\frac{ 1 }{2}(\alpha_j,\alpha_j)\).
    \end{enumerate}    
\end{lemma}

\begin{proof}
    We compute \( s_{\alpha_j}\delta\) dividing the sum into two parts:
    \begin{subequations}
        \begin{align}
            s_{\alpha_j}\delta&=\frac{ 1 }{2}\sum_{\substack{\alpha\in\Phi^+\\\alpha\neq\alpha_j}}s_{\alpha_j}(\alpha)+\frac{ 1 }{2}s_{\alpha_j}(\alpha_j)\\
            &=\frac{ 1 }{2}\sum_{\substack{\alpha\in\Phi^+\\\alpha\neq\alpha_j}}\alpha-\frac{ 1 }{2}\alpha_j.
        \end{align}
    \end{subequations}
    The second inequality is from the fact that \( s_{\alpha_j}\) is bijective on \( \Phi^+\setminus\{ \alpha_j \}\) by theorem \ref{ThosajBijSurpPpsmaj}. Adding a subtracting \( \frac{ \alpha_j }{2}\) we get
    \begin{equation}
        s_{\alpha_j}\delta=\frac{ 1 }{2}\sum_{\alpha\in\Phi^+}\alpha-\frac{ \alpha_j }{2}-\frac{ \alpha_j }{2}=\delta-\alpha_j
    \end{equation}

    Using the proposition \ref{PropWeylIsomalphai}, we have
    \begin{equation}
        (\delta,\alpha_j)=(s_{\alpha_j}\delta,s_{\alpha_j}\alpha_j)=(\delta-\alpha_j,-\alpha_j)=-(\delta,\alpha_j)+(\alpha_j,\alpha_j),
    \end{equation}
    consequently, \( 2(\delta,\alpha_j)=(\alpha_j,\alpha_j)\) and the result follows.
\end{proof}

\subsection{Abstract root system}
%--------------------------------

The material about abstract root system  mainly comes from \cite{Knapp_reprez}. 

\begin{definition}      \label{DefAbsRootSystSerre}
    An \defe{abstract root system}{abstract!root system}\index{root!abstract} in a finite dimensional vector space $V$ endowed with an is a subset $\Phi$ of $V$ such that
    \begin{itemize}
        \item \( \Phi\) is finite and $\Span\Phi=V$,
        \item
            For every \( \alpha\in \Phi\), there is a symmetry \( s_{\alpha}\) of vector \( \alpha\) leaving \( \Phi\) stable.
        \item 
            For every \( \alpha,\beta\in\Phi\), the vector \( s_{\alpha}(\beta)-\beta\) is an integer multiple of \( \alpha\).
    \end{itemize}
\end{definition}
The abstract system is \defe{reduced}{reduced abstract root system} when $\alpha\in\Phi$ implies $2\alpha\notin\Phi$. It is \defe{irreducible}{irreducible!abstract root system} is $\Phi$ doesn't admits non trivial decomposition as $\Phi=\Phi'\cup\Phi''$ with $(\alpha,\beta)=0$ for any $\alpha\in\Phi'$ and $\beta\in\Phi''$. We use the notation $\Phi:=\Phi\cup\{0\}$. 


The following is a consequence of all we did up to now.
\begin{theorem}
    The root system of a complex semisimple Lie algebra is a reduced abstract root system.
\end{theorem}

The \defe{Weyl group}{Weyl group!abstract setting} of \( \Phi\) is the subgroup of \( \GL(V)\) generated by the transformations \( s_{\alpha}\) with \( \alpha\in\Phi\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Link with other definitions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The definition \ref{DefAbsRootSystSerre} is not the ``usual'' one (in \cite{Wisser}, page 14 for example). We show now that we retrieve the usual features of an abstract.

\begin{lemma}
    An abstract root system admits a bilinear positive symmetric non degenerate form which is invariant under its Weyl group.
\end{lemma}

\begin{proof}
    If \( (.,.)_1\) is a bilinear positive non degenerate symmetric form on the vector space \( V\), the form
    \begin{equation}
        (\alpha,\beta)=\sum_{w\in W}(w\alpha,w\beta)_1
    \end{equation}
    is invariant under the Weyl group. This construction is possible since the Weyl group is finite.
\end{proof}

\begin{definition}
    Let \( V\) be a vector space and \( v\in V\) a non vanishing vector. A symmetry of vector \( v\) is an automorphism \( s\colon V\to V\) such that
    \begin{enumerate}
        \item
            \( s(v)=-v\);
        \item
            the set \( H=\{ w\in V\tq \alpha(w)=w \}\) is an hyperplane in \( V\).
    \end{enumerate}
\end{definition}
A symmetry of vector \( v\) induces the decomposition \( V=H\oplus\eR v\). The symmetries are of order \( 2\): \( s^2=\id\).

\begin{lemma}
    let \( v\) be a nonzero vector of \( V\) and \( A\) be a finite part of \( V\) such that \( \Span(A)=V\). Then there exists at most one symmetry of vector \( v\) leaving \( A\) invariant.
\end{lemma}

\begin{proof}
    Let \( s\) and \( s'\) be two such symmetries and consider \( u=ss'\). We immediately have \( u(A)=A\) and \( u(v)=v\). Let us prove that \( u\) induce the identity on the quotient \( V/\eR v\). A general vector in \( V\) can be written (in a non unique way) under the form
    \begin{equation}
        h+h'+v
    \end{equation}
    with \( h\in H\) and \( h'\in H'\). Let \( h=h'_1+\beta v\) be the decomposition of \( h\) in \( H'\oplus \eR v\) and \( h'=h_1+\gamma v\) be the decomposition of \( h'\) with respect to the direct sum \( V=H\oplus\eR v\).  Then we have
    \begin{subequations}
        \begin{align}
            ss'(h+h'+\alpha v)&=ss'\big( (h'_1+\beta v)+h'+\alpha v \big)\\
            &=s\big( (h'_1-\beta v)+h'+\alpha v \big)\\
            &=s(h-2\beta v+h_1+\gamma v+\alpha v)\\
            &=h+2\beta v+h_1-\gamma v+\alpha v\\
            &=h+h'+(\alpha-2\gamma+2\beta)v.
        \end{align}
    \end{subequations}
    Thus at the level of the quotient, $u$ leaves invariant \( h+h'\).

    It is not guaranteed that \( u\) is the identity, but the eigenvalues of \( u\) are \( 1\). For each \( x_i\in A\), there exists \( n_i\in\eN\) such that \( u^{n_i}x_i=x\). If \( n\) is a common multiple of all the \( n_i\) (these are finitely many), we have \( u^n(x)=x\) for every \( x\in A\). Since \( A\) generates \( V\), we have \( u^n=\id\) and then \( u\) is diagonalizable.

    We already mentioned the fact that the eigenvalues of \( u\) are \( 1\). Since \( u\) is diagonalizable, it is the identity and \( s=s'\).
\end{proof}

The invariant form give to \( V\) a structure of euclidian vector space for which the elements of the Weyl group are orthogonal matrices. Thus the symmetries read
\begin{equation}    \label{EqSymparnnusul}
    s_{\alpha}(x)=x-2\frac{ (x,\alpha) }{ (\alpha,\alpha) }\alpha.
\end{equation}
This is the only transformation which makes \( s_{\alpha}(\alpha)=-\alpha\) in the same time as being implemented by an orthogonal matrix. The symmetry \( s_{\alpha}\) is nothing else than the orthogonal symmetry with respect to the hyperplane orthogonal to \( \alpha\).

The expression \eqref{EqSymparnnusul} has the consequence that
\begin{equation}
    s_{\alpha}(\beta)-\beta=-\frac{ (\beta,\alpha) }{ (\alpha,\alpha) }\alpha.
\end{equation}
By the definition of an abstract root system, the latter has to be an integer multiple of \( \alpha\), so
\begin{equation}
    \frac{ 2(\beta,\alpha) }{ (\alpha,\alpha) }\in\eZ.
\end{equation}

\begin{definition}
    Two abstract root systems \( \Phi\) on \( V\) and \( \Phi'\) on \( V'\) are \defe{isomorphic}{isomorphism!of abstract root system} is there exists an isomorphism of vector space \( \psi\colon V\to V'\) such that \( \psi(\Phi)=\Phi'\) and
    \begin{equation}
        2\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }=2\frac{ \big( \psi(\alpha),\psi(\beta) \big) }{ \big( \psi(\alpha),\psi(\alpha) \big) }
    \end{equation}
    for every \( \alpha,\beta\in \Phi\).
\end{definition}


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Basis of abstract root system}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
The part about basis of abstract root system comes from \cite{SerreSSAlgebres}.

\begin{definition}      \label{DefbasisabsRoot}
    Let \( \Phi\) be an abstract root system. A part \( S\subset \Phi\) is a \defe{basis}{basis!of an abstract root system} of \( \Phi\) if 
    \begin{enumerate}
        \item
            \( S\) is a basis of \( V\) as vector space;
        \item
            every \( \beta\in\Phi\) can be written under the form
            \begin{equation}
                \beta=\sum_{\alpha\in S}m_{\alpha}\alpha
            \end{equation}
            where \( m_{\alpha}\) are all integers of the same sign.
    \end{enumerate}
\end{definition}
The set \( \Delta\) of simple roots of the root system of a complex semisimple Lie algebra is a basis.

We are going to build a basis of an abstract root system. Let \( h\in V^*\) be such that \( \alpha(h)\neq 0\) for every  \( \alpha\in\Phi\) and define
\begin{equation}
    \Phi_h^+=\{ \alpha\in\Phi\tq \alpha(h)>0 \}.
\end{equation}
We have \( \Phi=\Phi^+_h\cup -\Phi_h^+\). We say that an element \( \alpha\in\Phi^+_h\) is \defe{decomposable}{decomposable!in an abstract root system} if there exist \( \beta,\gamma\in\Phi_h^+\) such that \( \alpha=\beta+\gamma\). We write \( S_h\) the set of undecomposable elements in \( \Phi^+_h\).

\begin{lemma}       \label{LemShPhihpCBLSh}
    Any element in \( \Phi^+_h\) is a linear combination with positive coefficients of elements of \( S_h\).
\end{lemma}

\begin{probleme}
    It seems to me that Serre's book\cite{SerreSSAlgebres} has a misprint here. At page V-11 he writes :
    \begin{quote}
        Tout élément de \( R^+_t\) est combinaison linéaire, à coefficients entiers \( \geq 0\) des éléments de S.
    \end{quote}
    Shouldn't he have written \( S_t\).
\end{probleme}

\begin{proof}
    Let \( I\) be the set of \( \alpha\in\Phi^+_h\) that cannot be written under such a decomposition. We choose \( \alpha\in I\) such that \( \alpha(h)\) is minimal. If \( \alpha\) is undecomposable, then \( \alpha\in S_h\) and the condition \( \alpha\in I\) is contradicted. Thus \( \alpha\) is decomposable. Let \( \beta,\gamma\in\Phi^+_h\) be such that \( \alpha=\beta+\gamma\). Since \( \alpha(h)\) is minimal,
    \begin{equation}
        \begin{aligned}[]
            \beta(h)&\leq \alpha(h)\\
            \gamma(h)&\leq \alpha(h).
        \end{aligned}
    \end{equation}
    Thus we have \( \beta(h)=\alpha(h)-\gamma(h)<0\) which contradicts \( \beta\in\Phi^+\). We conclude that \( I\) is empty.
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Properties}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The main properties of an abstract root system are given in the  following proposition.
\begin{proposition}     \label{PropPropAbstrRootviiiikl}
If $\Phi$ is an abstract root system in a vector space $V$, one has the following properties:

\begin{enumerate}
\item\label{enubi} If $\alpha\in\Phi$ then $-\alpha\in\Phi$.

\item\label{enubii} If $\alpha\in\Phi$, the multiples of $\alpha$ which could also be in $\Phi$ are either $\pm\alpha$, or $\pm\alpha$ and $\pm 2\alpha$ or $\pm\alpha$ and $\pm\frac{1}{2}\alpha$.

\item\label{enubiii} If $\alpha\beta\in\Phi$ then $\frZ{\alpha}{\beta}$ can take the nonzero values $\pm 1$, $\pm 2$, $\pm 3$ or $\pm 4$. The case $\pm 4$ can only arise if $\beta=\pm 2\alpha$.

\item\label{enubiv} If $\alpha,\beta\in\Phi$ are not proportional each other and if $|\alpha|\leq|\beta|$, then $\frZ{\beta}{\alpha}$ equals $0$ or $\pm 1$.

\item\label{enubv} If $\alpha,\beta\in\Phi$ and $(\alpha,\beta)>0$, then $\alpha-\beta\in\Phi$ and if $(\alpha,\beta)<0$, the $\alpha+\beta\in\Phi$.

\item\label{enubvi} If $\alpha,\beta\in\Phi$ and neither $\alpha+\beta$ neither $\alpha-\beta$ belongs to $\Phi$, then $(\alpha,\beta)=0$.

\item\label{enubvii} If $\alpha\in\Phi$ and $\beta\in\Phi$, the $n\in\eZ$ such that $\beta+n\alpha\in\Phi$ fulfils $-p\leq n\leq q$ for certain $p,q\geq 0$. Moreover there are no gap,
\[
   p-q=\frZ{\alpha}{\beta},
\]
and there are at most four roots in the set $\{\beta+n\alpha\}_{-p\leq n\leq q}$.

\item\label{enubviii} If $\Phi$ is reduced, 

\begin{enumerate}
\item\label{enubviiia} If $\alpha\in\Phi$, the only multiples of $\alpha$ to lies in $\Phi$ are $\pm\alpha$,
\item\label{enubviiib} If $\alpha\in\Phi$ and $\beta\in\Phi$, then $\frZ{\alpha}{\beta}$can be equal to $0$, $\pm 1$, $\pm 2$ or $\pm 3$.
\end{enumerate}
\end{enumerate} \label{prop:Cartan_matr}
\end{proposition}
The proof will not use the fact that $\Phi$ spans $V$.

\begin{proof}
\ref{enubi} $s_{\alpha}\alpha=-\alpha$.

\ref{enubii} If $\beta=c\alpha$ with $|c|<1$, then 
\[
\frZ{\alpha}{\beta}=2c
\]
must belongs to $\eZ$, then $c=0,\pm\frac{1}{2}$. If $|c|>1$, we use exactly the same with $\alpha=\us{c}\beta$, so that $\us{c}=0;\pm\frac{1}{2}$. Now if $2\alpha$ is a root, it is clear that $\frac{1}{2}\alpha$ can't be.

If $\Phi$ is reduced, the fact that $\frac{1}{2}\alpha\in\Phi$ implies that $\alpha\notin\Phi$, so that $\pm\frac{1}{2}\alpha$ is excluded if $\alpha\in\Phi$, under the same assumption, $2\alpha$ is also excluded. This proves \ref{enubviiia}.

\ref{enubiii} The Schwartz inequality $|(\alpha,\beta)|\leq|\alpha||\beta|$ gives
\[
\left|   \frZ{\alpha}{\beta}\frZ{\beta}{\alpha}     \right|\leq 4.
\]
The equality only holds for $\beta=c\alpha$. In this case, we just saw that $\frZ{\alpha}{\beta}=2c$ with $c=2$ at most. If the equality is strict, then $\frZ{\alpha}{\beta}$ and $\frZ{\beta}{\alpha}$ are two integers whose product is $\leq 3$. The possibilities are $0$, $\pm 1$, $\pm 2$, $\pm 3$.

\ref{enubiv} If $|\alpha|\leq|\beta|$, then the following integer inequality holds:
\[
\left|\frZ{\alpha}{\beta}   \right|\leq\left|\frZ{\beta}{\alpha}   \right|.
\]
Since the product of the two  is $\leq 3$, the smallest is $0$ or $1$.

\ref{enubv}
If $\beta=c\alpha$, then $c=\pm\frac{1}{2},\pm 2,\pm 1$. All the cases are easy. If $(\alpha,\beta)>0$, then $c>0$ and $\alpha-\beta= \alpha-\frac{1}{2}\alpha=\frac{1}{2}\alpha$ or $\alpha-\beta=\alpha-2\alpha=-\alpha$.

Then we can suppose that $\alpha$ and $\beta$ are not proportional each other. We consider $\alpha,\beta\in\Phi$ and $(\alpha,\beta)>0$ (the other case is proved in much the same way). We just saw in \ref{enubiv} that $\frZ{\beta}{\alpha}$ could be equals to $0$ or $\pm 1$, then the fact that $(\alpha,\beta)>0$ imposes $\frZ{\beta}{\alpha}=1$, so that $s_{\beta}(\alpha)=\alpha-\beta$.

If $|\beta|\leq|\alpha|$, we use
\begin{equation}
s_{\alpha}(\beta)=\beta-\frZ{\beta}{\alpha}\alpha
               =\beta-\alpha,
\end{equation}

\ref{enubvi} is an immediate consequence of the previous point.

\ref{enubvii} Let $-p$ and $q$ be the smallest and the largest values of $n$ such that $\beta+n\alpha \in\Phi$. They exist because $\Phi$ is a finite set. Suppose that there is a gap between $r$ and $s$ ($r<s-1$), i.e. $\beta+r\alpha\in\Phi$, $\beta+s\alpha\in\Phi$, but $\beta+(r+1)\alpha,\beta+(s-1)\alpha\notin\Phi$.

By the point \ref{enubv}, $(\beta+r\alpha,\alpha)\geq 0$ and $(\beta+s\alpha,\alpha)\leq 0$. Making the difference between these two inequalities,
\[
   (r-s)|\alpha|^2\geq 0,
\]
then $r\geq s$, which contradict the definition of $r$ and $s$. So there is no gap. Now let us compute
\begin{equation}
\begin{split}
   s_{\alpha}(\beta+n\alpha)&=\beta+n\alpha-\frZ{\alpha}{\beta+n\alpha}\alpha\\
                          &=\beta+n\alpha-\left(    \frZ{\alpha}{\beta}+2n    \right)\alpha\\
              &=\beta-n\alpha-\frZ{\alpha}{\beta}\alpha\in\Phi.
\end{split}
\end{equation}
Then for any $n$ in $-p\leq n\leq q$, 
\[
   -q\leq n+\frZ{\alpha}{\beta}\leq p.
\]
With $n=q$, the second inequality gives $\frZ{\alpha}{\beta}\leq p-q$ while the first one with $n=-p$ gives  $p-q\leq\frZ{\alpha}{\beta}$.

The last point is to check the length of the string of root. We can suppose $q=0$ (i.e to look the string of $\beta-q\alpha$ instead of the one of $\alpha$; of course this is the same), then the length is $p+1$ and
\[
   p=\frZ{\alpha}{\beta}.
\]
If $\alpha$ and $\beta$ are not proportional, the point \ref{enubiii} makes it equals at most to $3$. If they are proportional, then the possibilities are $\alpha=\pm\beta,\pm\frac{1}{2}\beta,\pm 2\beta$. The string $\beta+n\alpha$ with $\alpha=\beta$ is at most $\{\beta,2\beta\}$, if $\alpha=\frac{1}{2}\beta$, this is just $\{\beta\}$ and if $\alpha=2\beta$, this is $\{\beta,-\beta\}$.

The proof is complete.
\end{proof}

\begin{lemma}       \label{LemShabShablesz}
    If \( \alpha,\beta\in S_h\), then \( (\alpha,\beta)\leq 0\).
\end{lemma}

\begin{proof}
    If \( (\alpha,\beta)\geq 0\), then proposition \ref{PropPropAbstrRootviiiikl}\ref{enubv} shows that \( \gamma=\alpha-\beta\) is a root. There are two possibilities: \( \gamma\in\pm\Phi^+_h\). If \( \gamma\in\Phi^+_h\), then \( \alpha=\gamma+\beta\) is decomposable; contradiction. If \( \gamma\in -\Phi^+_h\), then \( \beta=\alpha-\gamma\) is decomposable; contradiction.
\end{proof}

\begin{lemma}[Lemme 4 page V-12]        \label{LemIndepAhVstar}
    Let \( h\in V^*\) and \( A\subset V\) be a subset satisfying
    \begin{enumerate}
        \item
            \( \alpha(h)>0\) for every \( \alpha\in A\);
        \item
            \( (\alpha,\beta)\leq 0\) for every \( \alpha,\beta\in A\).
    \end{enumerate}
    Then the elements in \( A\) are linearly independent.
\end{lemma}

\begin{proof}
    Let us consider a vanishing linear combination of elements in \( A\):
    \begin{equation}        \label{EqNullCombinsumAmAuAd}
        \sum_{\alpha\in A}m_{\alpha}\alpha=0.
    \end{equation}
    We can sort the terms following that \( m_{\alpha}\) is positive or negative and cut the sum in two parts:
    \begin{equation}
        \sum_{\beta\in A_1}y_{\beta}\beta=\sum_{\gamma\in A_2}z_{\gamma}\gamma
    \end{equation}
    with \( y_{\beta},z_{\gamma}\geq 0\) and where \( A_1\) and \( A_2\) are disjoint subsets of \( A\). Let us consider \( \lambda=\sum_{\beta\in A_1}y_{\beta}\beta\) and compute
    \begin{equation}        \label{EqllamllamnprofAunAdeuxsom}
        (\lambda,\lambda)=\sum_{\substack{\beta\in A_1\\\gamma\in A_2}}y_{\beta}z_{\gamma}(\beta,\gamma).
    \end{equation}
    By hypothesis \( (\beta,\gamma)\) is lower than zero and by construction the product \( y_{\beta},z_{\gamma}\) is positive. Thus the right hand side of equation \eqref{EqllamllamnprofAunAdeuxsom} is negative. We conclude that \( \lambda=0\). Thus
    \begin{equation}
        0=\lambda(h)=\sum_{\beta\in A_1}y_{\beta}\beta(h).
    \end{equation}
    Since all the terms in the sum are larger than zero we have \( y_{\beta}=0\). In the same way we get \( z_{\gamma}=0\). The vanishing linear combination \eqref{EqNullCombinsumAmAuAd} is then trivial and the elements of \( A\) are linearly independent.
\end{proof}

\begin{proposition}\label{PropSestShsi}
    The elements of \( S_h\) form a basis of \( \Phi\) in the sense of definition \ref{DefbasisabsRoot}. Conversely, if \( S\) is a basis of \( \Phi\) and if \( h\in V^*\) is such that \( \alpha(h)>0\) for every \( \alpha\in S\),we have \( S=S_h\).
\end{proposition}

\begin{proof}
    The set \( S_h\) satisfies the conditions of lemma \ref{LemIndepAhVstar} since by definition \( \alpha(h)>0\) for every \( \alpha\in S_h\) and by lemma \ref{LemShabShablesz} the inner products are all negative. Thus \( S_h\) is a free set. It is generating by lemma \ref{LemShPhihpCBLSh}. Again by lemma \ref{LemShPhihpCBLSh}, every element in \( \Phi\) can be written as sum of elements of \( S_h\) with all coefficients of the same sign. Here we use the fact that \( v\) is positive if and only if \( -v\) is negative and that every vector is either positive or negative.

    For the second part, let \( S\) be a basis and \( h\in V^*\) such that \( \alpha(h)>0\) for all \( \alpha\in S\). Let
    \begin{equation}
        \Phi^+=\{ \sum_{\alpha\in S}m_{\alpha}\alpha \text{ with \( m_{\alpha}\in\eN\)} \}.
    \end{equation}
    We have \( \Phi^+\subset\Phi_h^+\) and \( -\Phi^+\subset -\Phi_h^+\). Since \( \Phi=\Phi^+\cup-\Phi^+\) we also have \( \Phi^+=\Phi_h^+\). Since elements of \( S\) are indecomposable in \( \Phi^+\), they are indecomposable in \( \Phi^+_h\) and we have \( S\subset S_h\).

    The sets \( S\) and \( S_h\) have the same number of elements because they both are basis of \( V\), thus \( S=S_h\).
\end{proof}

\begin{lemma}\label{LemwShShpahwahp}
    If \( h\) and \( h'\) are elements of \( V^*\) related by \( \alpha(h)=(w\alpha)h'\), then \( w(S_h)=S_{h'}\) (if these space can be defined).
\end{lemma}

\begin{proof}
    Let \( \alpha\in S_h\). The element \( w(\alpha)\) belongs to \( \Phi_{h'}^+\) because
    \begin{equation}
        w(\alpha)h'=\alpha(h)>0
    \end{equation}
    because \( \alpha\in\Phi_h^+\). We still have to check that \( w(\alpha)\) is undecomposable in \( \Phi_{h'}^+\). If \( w(\alpha)=\beta+\gamma\) with \( \beta,\gamma\in\Phi_{h'}^+\), we have \( \alpha=w^{-1}\beta+w^{-1}\gamma\). From the link between \( h\) and \( h'\) we have
    \begin{equation}
        (w^{-1}\beta)(h)=(ww^{-1}\beta)h'=\beta(h')>0.
    \end{equation}
    Thus \( w^{-1}\beta\in \Phi_h^+\) which is a contradiction because we supposed that \( \alpha\) is undecomposable.
\end{proof}

\begin{lemma}\label{Lemswwsbwemucirc}
    If \( \alpha,\beta\in\Phi\) and if \( w\in W_S\), then \( s_{w(\beta)}=w\circ s_{\beta}\circ w^{-1}\).
\end{lemma}

\begin{proof}
    Using the fact that the symmetries are isometries of the inner product,
    \begin{equation}
        s_{w(\beta)}(\alpha)=\alpha-\frac{ \big( w(\beta),\alpha \big) }{ \big( w(\beta),w(\beta) \big) }w(\beta)=\alpha-\frac{ (\beta),w^{-1}\alpha }{ (\beta,\beta) }w\beta.
    \end{equation}
    Applying that to \( w(\alpha)\) instead of \( \alpha\) and applying \( w^{-1}\), we have
    \begin{subequations}
        \begin{align}
            w^{-1}s_{w(\beta)}\big(w(\alpha)\big)&=w^{-1}\left( w\alpha-\frac{ (\beta,w^{-1}w\alpha) }{ (\beta,\beta) }w\beta \right)\\
            &=\alpha-\frac{ (\beta,\alpha) }{ (\beta,\beta) }w^{-1}w\beta\\
            &=s_{\beta}(\alpha).
        \end{align}
    \end{subequations}
\end{proof}

\begin{theorem}[\cite{SerreSSAlgebres}]     \label{ThoWeylGenere}
    Let \( W\) be the Weyl group of the abstract root system \( \Phi\). Let \( S\) a basis of \( \Phi\) and \( W_S\) the subgroup of \( W\) generated by \( s_{\alpha}\) with \(\alpha\in S\). Then
    \begin{enumerate}
        \item   \label{ItemThoWeylGenerei}
            for every \( h\in V^*\), there exists \( w\in W_S\) such that \( (w\alpha)(h)\geq 0\) for every \( \alpha\in S\).
        \item   \label{ItemThoWeylGenereii}
            If \( S'\) is a basis of \( \Phi\), the there exists a \( w\in W_S\) such that \( w(S')=S\).
        \item\label{ItemThoWeylGenereiii}
            For every \( \beta\in\Phi\) there exists \( w\in W_S\) such that \( w(\beta)\in S\).
        \item\label{ItemThoWeylGenereiv}
            The group \( W\) is generated by the symmetries \( s_{\alpha}\) with \( \alpha\in S\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    For item \ref{ItemThoWeylGenerei}, consider \( h\in V^*\) and \( \delta=\frac{ 1 }{2}\sum_{\gamma\in S}\gamma\). Let \( w\in W_S\) be such that \( w(\delta)h\) is the largest possible\footnote{We can consider that \( w\) because \( W\) is finite.}. If \( \alpha\in S\) we have
    \begin{equation}
        w(\delta)h\geq ws_{\alpha}(\delta)h=w(\delta)h-w(\alpha)h,
    \end{equation}
    so that \( w(\alpha)h\geq 0\) for every \( \alpha\in S\). This proves our first assertion.

    We pass to point \ref{ItemThoWeylGenereii}. Let \( h'\in V^*\) be such that \( \alpha'(h')> 0\) for every \( \alpha'\in S'\). By the first item there exists \( w\in W_S\) such that
    \begin{equation}
        (w\alpha)(h')\geq 0
    \end{equation}
    for every \( \alpha\in S\). In fact we even have \( w\alpha h'>0\) for every \( \alpha\in S\). Indeed \( w\alpha\) can be decomposed as \( \sum_{\alpha'\in S'}m_{\alpha'}\alpha'\) where all the \( m_{\alpha'}\) have the same sign. In this case
    \begin{equation}        \label{Eqwapsummapaphp}
        (w\alpha)h'=\sum_{\alpha'}m_{\alpha'}\alpha'(h')\neq 0
    \end{equation}
    because each of \( \alpha'(h')\) is strictly positive while all the terms of the sum have the same sign. This means, by the way, that \( S'=S_{h'}\) following the proposition \ref{PropSestShsi}.
    
    We define \( h\in V^*\) by the relation
    \begin{equation}
        \alpha(h)=(w\alpha)(h').
    \end{equation}
    By what we said in equation \eqref{Eqwapsummapaphp} we have \( \alpha(h)>0\) for every \( \alpha\in S\), so that we have \( S=S_h\). Finally by lemma \ref{LemwShShpahwahp}, \( w(S_h)=S_{h'}\).

    We prove now the point \ref{ItemThoWeylGenereiii}. For \( \gamma\in\Phi\) we consider the hyperplane
    \begin{equation}
        L_{\gamma}=\{ h\in V^*\tq \gamma(h)=0 \}.
    \end{equation}
    Consider a particular \( \beta\in\Phi\) the hyperplanes \( L_{\gamma}\) with \( \gamma\neq\pm\beta\) do not coincide with \( L_{\beta}\) and there is only finitely many of them, so there exists a \( h_0\in L_{\beta}\) such that \( h_0\) do not belong to any \( L_{\gamma}\) for any \( \gamma\neq \pm\beta\).

    In particular we have \( \beta(h_0)=0\) and \( \gamma(h_0)\neq 0\) for every \( \gamma\in\Phi\), \( \gamma\neq\pm\beta\). If we choose \( \epsilon\) small enough, there exists \( h\) near from \( h_0\) such that
    \begin{subequations}
        \begin{numcases}{}
            \beta(h)=\epsilon>0\\
            | \gamma(h) |>\epsilon&\text{if \( \gamma\neq \pm\beta\).}
        \end{numcases}
    \end{subequations}
    Let \( S_{h}\) be the basis associated with this \( h\). We have \( \beta\in S_h\). Indeed first \( \beta(h)=\epsilon>0\) and if \( \beta=\gamma+\rho\), we would have
    \begin{equation}
        \gamma(h)=\beta(h)-\rho(h)=\epsilon-\rho(h)<0,
    \end{equation}
    so that \( \beta\) is undecomposable in \( \Phi_h^+\). Now from point \ref{ItemThoWeylGenereii} there exists \( w\in W_S\) such that \( w(S_h)=S\). In particular \( w(\beta)\in S\).

    We turn our attention to the item \ref{ItemThoWeylGenereiv}. We are going to prove that \( W=W_S\). Since \( W\) is generated by the symmetries \( s_{\beta}\) (\( \beta\in\Phi\)), it is sufficient to prove that \( W_S\) generates the symmetries \( s_{\beta}\).

    Let \( \beta\in\Phi\) and consider the element \( w\in W_S\) such that \( \alpha=w(\beta)\in S\). From lemma \ref{Lemswwsbwemucirc} we have
    \begin{equation}
        s_{\alpha}=s_{w(\beta)}=w\circ s_{\beta}\circ w^{-1},
    \end{equation}
    so that
    \begin{equation}
        s_{\beta}=w^{-1}\circ s_{\alpha}\circ w\in W_S.
    \end{equation}
\end{proof}

What this theorem says in the case of complex semisimple Lie algebras is that if \( \{ \alpha_1,\ldots,\alpha_l \}\) is the set of simple roots, the symmetries \( s_{\alpha_i}\) generate the Weyl group. Now, since any root can be mapped on a simple one using the Weyl group, any root can be recovered from a simple one acting with the Weyl group that is generated by the simple ones.

Thus one can determine all the roots from the data of the simple ones by computing \( s_{\alpha_i}\alpha_j\) and then acting again with the \( s_{\alpha_i}\) on the results and again and again. This is the fundamental reason from which the root system can be recovered for the Cartan matrix.


When we have the Cartan matrix \( A \) of a semisimple complex Lie algebra, the first point is to find the norm of the roots by finding the diagonal matrix \( D\). We have \( (\alpha_i,\alpha_i)=D_{ii}\). For the other products we write
\begin{equation}
    A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ D_{ii} },
\end{equation}
thus
\begin{equation}
    (\alpha_i,\alpha_j)=\frac{ D_{ii}A_{ij} }{ 2 }.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Abstract Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

The following proposition summarize the properties of the of the Cartan matrix.
\begin{definition}      \label{DeabstrCartanmatr}
    A matrix \( (A_{ij})_{1\leq i,j\leq l}\) satisfying the following conditions is an \defe{abstract Cartan matrix}{Cartan!matrix!abstract}\index{abstract!Cartan matrix}
    \begin{enumerate}
        \item
            \( A_{ij}\in\eZ\),
        \item
            \( A_{ii}=2\),
        \item   \label{ItempoprCartaniii}
            \( A_{ij}\leq 0\) if \( i\neq j\),
        \item
            \( A_{ij}=0\) if and only if \( A_{ji}=0\),
        \item\label{ItempoprCartanv}
            there exists a diagonal matrix \( D\) with positive coefficients such that \( DAD^{-1}\) is symmetric and positive defined.
    \end{enumerate}
\end{definition}
The classification of abstract Cartan matrix will be performed in subsection \ref{SubsecDynkindiam}. The data of an abstract Cartan matrix defines an abstract root system. For a proof, see \cite{CartanRootProject}.

\begin{proposition}
    The Cartan matrix of a semisimple complex Lie algebra is an abstract Cartan matrix.
\end{proposition}

\begin{proof}
    The first two points are already done. For the point \ref{ItempoprCartaniii}, note that the sign of \( (\alpha,\beta)\) is not sure when \( \alpha\) is any root. However here we are speaking of simple roots. Let us consider the root
    \begin{equation}
        \lambda=\alpha_i-\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\alpha_i
    \end{equation}
    Since it is a root, proposition \ref{ThoposrootnjajnZ} says that the coefficients in the decomposition in simple roots have to be all integer and of the same sign. Thus the combination \( (\alpha_i,\alpha_j)/(\alpha_i,\alpha_i)\) has to be negative.

    The point \ref{ItempoprCartanv} is also non trivial. Consider the diagonal matrix \( D=\diag\big( (\alpha_i,\alpha_i) \big)_{i=1,\ldots,l}\). We have
    \begin{subequations}
        \begin{align}
            (DAD^{-1})_{ij}&=\sum_{kl}D_{ik}A_{kl}(D^{-1})_{lj}\\
            &=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i)^{1/2}(\alpha_j,\alpha_j)^{1/2} }.
        \end{align}
    \end{subequations}
    This is a symmetric matrix. In order to proof that this is positive defined, we are going to provide a matrix \( B\) such that \( DAD^{-1}=BB^t\). Let \( \{ \lambda_i \}\) be an orthonormal basis of \( \lH^*\) and consider the matrix \( b\) given by the decomposition of the simple roots in this basis:
    \begin{equation}
        \alpha_i=\sum_j b_{ij}\lambda_j.
    \end{equation}
    In particular we have \( (\alpha_i,\alpha_j)=\sum_kb_{ik}b_{jk}\). Then we consider the matrix
    \begin{equation}
        B_{ij}=\frac{ b_{ij} }{ (\alpha_i,\alpha_i)^{1/2} }
    \end{equation}
    which is non degenerate since the \( \alpha_i\) are simple and thus a re linearly independent. Small computation shows that 
    \begin{subequations}
        \begin{align}
            (BB^t)_{ij}&=\sum_k\frac{ b_{ik} }{ (\alpha_i,\alpha_i)^{1/2} }\frac{ b_{jk} }{ (\alpha_j,\alpha_j)^{1/2} }\\
            &=\frac{ (\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i)^{1/2}(\alpha_j,\alpha_j)^{1/2} }\\
            &=(DAD^{-1})_{ij}.
        \end{align}
    \end{subequations}
    But \( BB^t\) is positive defined, then \( DAD^{-1}\) is.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dynkin diagrams}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubsecDynkindiam}

The sources for Dynkin diagrams is \cite{SternLieAlgebra,Wisser}.

We are going to associate to each abstract Cartan matrix, a diagram that will uniquely correspond to an abstract root system. In other words what we are going to do is to classify the matrix satisfying the conditions of definition \ref{DeabstrCartanmatr}.

If \( A\) is an abstract Cartan matrix we build the \defe{Dynkin diagram}{Dynkin diagram} of \( A\) with the following rules.
\begin{enumerate}
    \item
        We put \( l\) vertices (one for each root)
    \item
        The vertex \( i\) and \( j\) are joined with \( A_{ij}A_{ji}\) lines.
\end{enumerate}
A step by step construction is available in \cite{Wisser}.

In the following we are considering an abstract Cartan matrix \( A\) and its associated abstract root system \( \{ \alpha_i \}\).

\begin{lemma}   \label{LesmabsCartDynk}
    A abstract Cartan matrix with its abstract root system and its Dynkin diagram have the following properties.
    \begin{enumerate}
        \item\label{ItemLesmabsCartDynki}
            If one remove the \( i\)th line an column of an abstract Cartan matrix, one still has an abstract Cartan matrix. In other words, each subdiagram of a Dynkin diagrm is a Dynkin diagram.
        \item\label{ItemLesmabsCartDynkii}
            Two vertices are linked by \emph{at most} three lines.
        \item\label{ItemLesmabsCartDynkiii}
            Each Dynkin diagram has more vertices than linked pairs.
        \item\label{ItemLesmabsCartDynkiv}
            A Dynkin diagram has no loop.
        \item\label{ItemLesmabsCartDynkv}
            A vertex in a Dynkin diagram has at most three lines attached (including multiplicities). Note: this is a generalization of point \ref{ItemLesmabsCartDynkii}.
        \item\label{ItemLesmabsCartDynkvi}
            Two root linked by a simple edge have equal \defe{weight}{weight!in a Dynkin diagram}, that is \( (\alpha_i,\alpha_i)=(\alpha_j,\alpha_j)\).
        \item\label{ItemLesmabsCartDynkvii}
            If the two roots \( \alpha_i\), \( \alpha_i\) are connected by a simple edge, we can collapse them, removing the connecting edge and conserving all the other edges.

    \end{enumerate}
\end{lemma}

\begin{proof}
    For point \ref{ItemLesmabsCartDynkii} we have
    \begin{equation}
        A_{ij}A_{ji}=4\frac{ (\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\frac{ (\alpha_j,\alpha_i) }{ (\alpha_j,\alpha_j) }<4
    \end{equation}
    by Cauchy-Schwarz inequality. We insist on the fact that the inequality is strict since \( \alpha_i\) and \( \alpha_j\) are not collinear: they are simple roots.

    For point \ref{ItemLesmabsCartDynkiii} consider the form
    \begin{equation}
        \gamma=\sum_{i=1}^l\alpha_i(\alpha_i,\alpha_i)^{1/2}.
    \end{equation}
    Since the simple roots are linearly independent, this sum is nonzero and we have \( 0<(\gamma,\gamma)\). We have
    \begin{equation}
        \begin{aligned}[]
            0<(\gamma,\gamma)&=\sum_{ij}\frac{ (\alpha_i,\alpha_j) }{ \sqrt{(\alpha_i,\alpha_i)(\alpha_j,\alpha_j)} }\\
            &=2\sum_{i<j} \frac{ (\alpha_i,\alpha_j) }{ \sqrt{(\alpha_i,\alpha_i)(\alpha_j,\alpha_j)} }+\text{number of nodes}\\
            &=-\sum_{i<j}(A_{ij}A_{ji})^{1/2}+\text{number of nodes}.
        \end{aligned}
    \end{equation}
    Since for each linked pair \( (i,j)\) we have a term \( A_{ij}A_{ji}\geq 1\), we have \( -\sum_{i<j}(A_{ij}A_{ji})^{1/2}\leq \text{number of pairs}\) and the positivity of the sum shows that
    \begin{equation}
        \text{number of nodes}>\sum_{ij}A_{ij}A_{ji}\geq\text{number of pairs}.
    \end{equation}

    For item \ref{ItemLesmabsCartDynkiv}, suppose that a loop is given by the roots \( \alpha_1,\ldots,\alpha_n\). Since any sub-Dynkin diagram is a Dynkin diagram (from point \ref{ItemLesmabsCartDynki}), we can consider only the loop. This is a diagram with \( n\) vertices and \( n\) pairs, which contradicts point \ref{ItemLesmabsCartDynkiii}.

    We pass to item \ref{ItemLesmabsCartDynkv}. Let \( \alpha_0\) be a root linked to \( n\) simple lines, \( m\) double lines and \( p\) triple lines. For notational convenience, we write \( v_i=\alpha_i/(\alpha_i,\alpha_i)\), \( \{ v_i \}_{1\leq i\leq n}\) is the set of ``simply'' linked roots to \( \alpha_0\), \( \{ v'_i \}_{1\leq i\leq m}\) the set of ``doubly'' linked and \( \{ v''_i \}_{1\leq i\leq p}\) the set of ``triply'' ones. Consider the vector
    \begin{equation}
        \gamma=v_0+\sum_{i=1}^nf_iv_i+\sum_{i=1}^mg_iv'_i+\sum_{i=1}^ph_iv''_i
    \end{equation}
    where \( f_i\), \( g_i\) and \( h_i\) are constant to be determined. In order to compute the norm of \( \gamma\), notice that since there are no loops, no lines join \( v_i\), \( v'_i\) and \( v''_i\) together, so we have \( (v_i,v'_j)=(v_i,v''_j)=(v'_i,v''_j)=0\) and from the number of lines, \( (v_0,v_i)=-1/2\), \( (v_0,v'_i)=-1/\sqrt{2}\) and \( (v_0,v''_i)=-\sqrt{3}/2\). Thus we have
    \begin{equation}
        (\gamma,\gamma)=1+\sum_{i=1}^m(f_i^2-f_i)+\sum_{i=1}^m(g_i^2-\sqrt{2}g_i)+\sum_{i=1}^p(h_i^2-\sqrt{3}h_i).
    \end{equation}
    The minimum is realised with \( f_i=1/2\), \( g_i=\sqrt{2}/2\) and \( h_i=\sqrt{3}/2\) and for these values we have
    \begin{equation}
        (\gamma,\gamma)=1-\frac{ n+2m+3p }{ 4 }.
    \end{equation}
    Since the inner product has to be positive we must have \( n+2m+3p<4\), the is the number of lines issued from \( \alpha_0\) has to be lower or equal to \( 3\).
   
    In order to proof \ref{ItemLesmabsCartDynkvi}, remark that if \( \alpha_i\) and \( \alpha_j\) are connected by a simple edge, then \( A_{ij}A_{ji}=1\), which is only possible with \( A_{ij}=A_{ji}=-1\). In particular we have \( 2(\alpha_i,\alpha_j)/(\alpha_i,\alpha_i)=2(\alpha_j,\alpha_i)/(\alpha_j,\alpha_j)\), which proves that \( (\alpha_i,\alpha_i)=(\alpha_j,\alpha_j)\).

    Proof of item \ref{ItemLesmabsCartDynkvii}. Since the two roots have same weight, the item \ref{ItemLesmabsCartDynkvi} says that up to permutation the Cartan matrix has a block \( 2\times 2\) looking like
    \begin{equation}
        \begin{pmatrix}
            2    &   -1    \\ 
            -1    &   2    
        \end{pmatrix}.
    \end{equation}
    The proposed move consist to replace that block with the \( 1\times 1\) matrix \( (2)\). As an example,
    \begin{equation}
        \begin{pmatrix}
             2   &   -1    &   0    &   0    \\
             -1   &   2    &   -1    &   -1    \\
             0   &   -1    &   2    &   0    \\ 
             0   &   -1    &   0    &   2     
         \end{pmatrix}\mapsto
         \begin{pmatrix}
             2   &   -1    &   -1    \\
             -1   &   2    &   0    \\
             -1   &   0    &   2
         \end{pmatrix}.
    \end{equation}
    It is clear that the obtained matrix is still an abstract Cartan matrix.
\end{proof}

From these properties we can deduce much constrains on the Dynkin diagrams. First, the only diagram containing a triple edge is
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@3{-}[r]        &   \alpha_2
       }
\end{equation}

Let pass to the diagrams with only simple and double edges. If there is a double, there cannot be a triple point: the following is impossible
\begin{equation}
    \xymatrix{%
         &                          &           &               &       \alpha_5\\
        \alpha_1 \ar@2{-}[r]   &    \alpha_2 \ar@{-}[r] & \alpha_3\ar@{-}[r]&  \alpha_4 \ar@{-}[ru]\ar@{-}[rd]\\
        &&&&\alpha_6
       }
\end{equation}
since collapsing the roots \( \alpha_2\), \( \alpha_3\) and \( \alpha_4\) should create a point with four edges. Thus a diagram with a double edge is only possible inside a straight chain. Let us study the diagram
\begin{equation}        \label{EqdiaguduuuDy}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4\ar@{-}[r]&  \alpha_5
       }
\end{equation}
Once again we denote \( v_i=\alpha_i/| \alpha_i |\) and we consider the (non vanishing) vector
\begin{equation}
    \gamma=v_1+bv_2+cv_3+dv_4+ev_5
\end{equation}
whose norm is given by
\begin{equation}
    (\gamma,\gamma)=1+b^2+c^2+d^2+e^2-b-\sqrt{2}bc=cd=de.
\end{equation}
Equating all the partial derivative to zero provides the point 
\begin{equation}
    \begin{aligned}[]
        b&=2&c&=\frac{ 3 }{ \sqrt{2} }&d&=\sqrt{2}&e&=\frac{1}{ \sqrt{2} }.
    \end{aligned}
\end{equation}
One check that with these values \( (\gamma,\gamma)=0\) which is impossible. The diagram \eqref{EqdiaguduuuDy} is thus impossible. By the collapsing principle, all the diagrams of the form
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4\ar@{-}[r]& \ldots \ar@{-}[r]&  \alpha_l
       }
\end{equation}
are impossible. The only possible diagrams with double edge are thus
\begin{subequations}
    \begin{align}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4
       }\\
    \xymatrix{%
    \alpha_1 \ar@2{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_l
    }    \label{subEqDynkdspds}\\
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{-}[r]&\alpha_l.
    }    \label{subEqdunksspd}
    \end{align}
\end{subequations}
The diagrams \eqref{subEqDynkdspds} and \eqref{subEqdunksspd} are the same. They however do not completely determine the abstract Cartan matrix because the diagram \eqref{subEqdunksspd} induces an asymmetry between \( \alpha_1\) and \( \alpha_2\). The so written Dynkin diagram cannot distinguish between the matrices
\begin{equation}
    \begin{aligned}[]
        \begin{pmatrix}
            2    &   -2    &   0    \\
            -1    &   2    &   -1    \\
            0    &   -1    &   2
        \end{pmatrix}&&
        \text{and}&&
        \begin{pmatrix}
            2    &   -1    &   0    \\
            -2    &   2    &   -1    \\
            0    &   -1    &   2
        \end{pmatrix}&
    \end{aligned}
\end{equation}
Thus we split the diagram \eqref{subEqdunksspd} into
\begin{subequations}        \label{suBeqdfynkabGP}
    \begin{align}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{->}[r]&\alpha_l.
    }   \\
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{<-}[r]&\alpha_l.
    }    
    \end{align}
\end{subequations}
In which the arrow points to the biggest root. The first one means that \( | \alpha_1 |=\ldots=| \alpha_{l-1} |=1\), \( \alpha_{l}=2\) while the second diagram means \( | \alpha_1 |=\ldots=| \alpha_{l-2} |=| \alpha_l |=1\), \( \alpha_{l-1}=2\).

We'll have to come back on this point later in subsection \ref{subsecRecbyhanfd}. Notice that this is the only diagram on which that problem occurs.

We are left to study the diagrams with only single edge. The following diagram is the simplest possible one:
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l}.
       }
\end{equation}
We have to know under what conditions one can have a triple point. We already know that there can be only one triple point.

If a diagram has a triple point, then one of the branch is of length \( 1\). Indeed if not we would have the following diagram:
\begin{equation}
    \xymatrix{%
    &                          &                       &       \alpha_2\ar@{-}[r]&\alpha_5\\
        \alpha_7 \ar@{-}[r]   &    \alpha_4 \ar@{-}[r] & \alpha_1 \ar@{-}[ru]\ar@{-}[rd]\\
        &&&\alpha_6\ar@{-}[r]&\alpha_6
       }
\end{equation}
Looking at the vector \( \gamma=3v_1+2(v_2+v_3+v_4)+v_5+v_6+v_7\) provides \( (\gamma,\gamma)=-3\) which is impossible. Thus the diagrams with a branch are straight chains with one unique triple point which has a branch of length one. The question is: where can happen that branch ? The diagram
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]&\alpha_4  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]&\alpha_6  \ar@1{-}[r]&\alpha_7\\
    &&&\alpha_8
       }
\end{equation}
cannot happen since the corresponding vector \( v_1+2v_2+3v_3+4v_4+3v_5+2v_6+v_7+2v_8\) has norm zero. Thus on a triple point, one branch has one branch of length \( 1\) and at least one other to be of length \( 1\) or \( 2\). It turns out that all the diagrams of the form
\begin{equation}
    \xymatrix{%
    &                          &                       &       \alpha_{l-1}\\
    \alpha_1 \ar@{-}[r]   &    \ldots \ar@{-}[r] & \alpha_{l-2} \ar@{-}[ru]\ar@{-}[rd]\\
        &&&\alpha_l
       }
\end{equation}
are possible. We are thus left with diagrams with a triple point with a branch of length \( 1\) and a branch of length \( 2\) :
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]&\ldots  \ar@1{-}[r]&\alpha_l \\
    &&\alpha_4
       }
\end{equation}
The diagram with a branch of length \( 5\)
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]   &\alpha_6  \ar@1{-}[r]   &\alpha_7  \ar@1{-}[r]    &\alpha_8 \ar@1{-}[r]&\alpha_9  \\
    &&\alpha_4
       }
\end{equation}
does not exist. We achieve the proof of that fact using for example this code for \href{http://www.sagemath.org}{sage}:
\begin{verbatim}
----------------------------------------------------------------------
| Sage Version 4.7.1, Release Date: 2011-08-11                       |
| Type notebook() for the GUI, and license() for information.        |
----------------------------------------------------------------------
sage: a=[var('a'+str(i-1)) for i in range(1,11)]
sage: l=9
sage: a[1]=1
sage: squares = sum( [a[i]**2 for i in range(1,l+1)] )     # The sum goes to l
sage: lines = sum(  [a[i]*a[i+1] for i in range(1,l-1)  ]   )+a[3]*a[9]  # The sum goes up to l-2
sage: f=symbolic_expression(squares - lines)
sage: X = solve( [f.diff(a[i])==0 for i in range(2,l+1)],[ a[i] for i in range(2,l+1)  ]  )   
sage: print X[0]
[a2 == 2, a3 == 3, a4 == (5/2), a5 == 2, a6 == (3/2), a7 == 1, a8 == (1/2), a9 == (3/2)]
sage: f(   *tuple(  [  X[0][i].rhs() for i in range(0,l-1)  ]  )   )
0
\end{verbatim}
This proves that the vector \( v_1+2v_2+3a_3+\frac{ 5 }{2}v_4+2v_5+\frac{ 3 }{2}v_6+v_7+\frac{ 1 }{2}v_8+\frac{ 3 }{2}v_9\) has vanishing norm, which is impossible.

\begin{probleme}
    This code raises a deprecation warning that I'm not able to solve.
\end{probleme}

We are finally left with the diagrams with one triple point with one branch of length \( 1\), one branch of length \( 2\) and the third branch with length \( 1\), \( 2\), \( 3\) or \( 4\) :
\begin{subequations}
    \begin{align}
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4   \\
        &&\alpha_5
           }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]& \alpha_5\\
        &&\alpha_6
        }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]&\alpha_5  \ar@1{-}[r]& \alpha_6\\
        &&\alpha_7
        }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]&\alpha_5  \ar@1{-}[r]& \alpha_6  \ar@1{-}[r]&\alpha_7\\
        &&\alpha_8
        }
    \end{align}
\end{subequations}

In order to list all the possible complex semisimple Lie algebra, we have to check each of the left Dynkin diagrams if they give rise to an abstract Cartan matrix.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Example of reconstruction by hand}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecRecbyhanfd}

We turn now our attention on the difference between the two diagrams \eqref{suBeqdfynkabGP}. The Cartan matrix of the diagram $
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2 \ar@2{->}[r]&\alpha_3
        }   $ is given by
        \begin{equation}
        A=\begin{pmatrix}
            2    &   -1    &   0    \\
            -1    &   2    &   -2    \\
            0    &   -1    &   2
       \end{pmatrix}.
        \end{equation}
The diagonal matrix \( D\) of definition \ref{DeabstrCartanmatr} is
\begin{equation}
    D=\begin{pmatrix}
        1    &       &       \\
            &   1    &       \\
            &       &   2
    \end{pmatrix}
\end{equation}
and the length of the roots are \( \| \alpha_1 \|=\| \alpha_2 \|=1\) and \( | \alpha_3 |=2\). Let us compute the angles between the roots. In order to compute \( (\alpha_1,\alpha_2)\) we look at \( A_{12}\):
\begin{equation}
    A_{12}=-1=2\frac{ (\alpha_1,\alpha_2) }{ (\alpha_1,\alpha_1) },
\end{equation}
and the same computation with \( A_{23}\) provides
\begin{subequations}
    \begin{align}
        (\alpha_1,\alpha_2)&=-\frac{ 1 }{2}\\
        (\alpha_2,\alpha_3)&=-1
    \end{align}
\end{subequations}
We compute all the roots using the theorem \ref{ThoWeylGenere} which basically says that acting with the ``simple'' Weyl group \( W_S\) on the simple roots generates all the roots. On the first strike we have
\begin{equation}
    \begin{aligned}[]
        s_1(\alpha_2)&=\alpha_2+\alpha_1&s_2(\alpha_1)&=\alpha_1+\alpha_2&s_3(\alpha_1)&=\alpha_1\\
        s_1(\alpha_3)&=\alpha\alpha_3 &s_2(\alpha_3)&=\alpha_3+2\alpha_2&s_3(\alpha_2)&=\alpha_2+\alpha_3.
    \end{aligned}
\end{equation}
We discovered the roots \( \alpha_2+\alpha_1\), \( \alpha_3+2\alpha_2\) and \( \alpha_2+\alpha_3\). Acting again on these roots by \( s_{\alpha_1}\), \( s_{\alpha_2}\) and \( s_{\alpha_3}\) the only new results are
\begin{equation}
    \begin{aligned}[]
        s_1(\alpha_3+\alpha_2)&=\alpha_1+\alpha_2+\alpha_3\\
        s_1(\alpha_3+2\alpha_2)&=2\alpha_1+2\alpha_2+\alpha_3.
    \end{aligned}
\end{equation}
Acting again we find only one new root: 
\begin{equation}
    s_{\alpha_2}(\alpha_1+\alpha_2+\alpha_3)=\alpha_1+2\alpha_2+\alpha_3. 
\end{equation}
We check that acting once again with the three simple roots on this last one does not brings new roots. Thus we have \( 9\) positive roots. Adding the negative ones, we are left with \( 18\) root spaces of dimension one. The Cartan algebra has dimension \( 3\), so the algebra we are looking at has dimension \( 21\).

Now take a look at the similar Dynkin diagram and its Cartan matrix:
\begin{subequations}
    \begin{align}
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2 \ar@2{<-}[r]&\alpha_3
        }   &
        &A&=\begin{pmatrix}
            2    &   -1    &   0    \\
            -1    &   2    &   -1    \\
            0    &   -2    &   2
       \end{pmatrix}
    \end{align}
\end{subequations}
The inner products are
\begin{equation}
    \begin{aligned}[]
        | \alpha_1 |=|\alpha_3|=1, | \alpha_2 |=2 \\
        (\alpha_1,\alpha_2)=-1/\sqrt{2},(\alpha_2,\alpha_3)=-1
    \end{aligned}
\end{equation}
and the roots are
\begin{subequations}
    \begin{align}
        \alpha_1\\
        \alpha_2\\
        \alpha_3\\
        \alpha_1+\alpha_2\\
        \alpha_2+\alpha_3\\
        \alpha_2+2\alpha_3\\
        \alpha_1+\alpha_2+\alpha_3\\
        \alpha_1+\alpha_2+2\alpha_3\\
        \alpha_1+2\alpha_2+2\alpha_3.
    \end{align}
\end{subequations}
We see that the inner products are already not the same. Notice that the roots are really different: it is not simply a renaming \( \alpha_2\leftrightarrow \alpha_3\).

Thus the two Dynkin diagrams \eqref{subEqdunksspd} are describing two different Lie algebras.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Reconstruction}
%---------------------------------------------------------------------------------------------------------------------------

The construction theorem is the following.
\begin{theorem}
    Let \( R\) be an abstract root system in a complex vector space \( V^*\) and \( \{ \alpha_1,\ldots,\alpha_n \}\) be a basis of \( R\). We denote by \( H_i\in V\) the \defe{inverse root}{inverse!root}\index{root!inverse} of \( \alpha_i\)(i.e. \( \alpha(H_{\alpha})=2\)). We define the Cartan matrix
    \begin{equation}
        A_{ij}=\alpha_j(H_i).
    \end{equation}
    Let \( \lG\) be the Lie algebra defined by the \( 3n\) generators \( X_i,Y_i,H_i\) and the relations
    \begin{subequations}
        \begin{align}
            [H_i,H_j]&=0\\
            [X_i,Y_j]&=\delta_{ij}H_i\\
            [H_i,X_j]&=A_{ij}X_j\\
            [H_i,Y_j]&=-A_{ij}Y_j
        \end{align}
    \end{subequations}
    and, for \( i\neq j\),
    \begin{subequations}
        \begin{align}
            \ad(X_i)^{-A_{ij}+1}(X_j)&=0        \label{EqSerrea}\\
            \ad(Y_i)^{-A_{ij}+1}(Y_j)&=0.
        \end{align}
    \end{subequations}
    Then \( \lG\) is a semisimple Lie algebra in which a Cartan subalgebra is generated by \( H_1,\ldots,H_n \) and its root system is \( R\).
\end{theorem}
A complete proof can be found in \cite{SerreSSAlgebres} at page VI-19. We are going to give some ideas.

We consider \( \lG\), the Lie algebra generated by the elements \( H_i\), \( X_i\) and \( Y_i\). We denote by \( \lH\) the abelian Lie algebra generated by the elements \( H_i\).
\begin{lemma}       \label{LemadXiNilpotent}
    The endomorphism \( \ad(X_i)\) and \( \ad(Y_i)\) are nilpotent.
\end{lemma}

\begin{proof}
    Let \( V_i\) the subspace of \( \lG\) of elements \( z\) such that \( \ad(X_i)^kz=0\) for some \( k\in\eN\). The space \( V_i\) is a Lie subalgebra of \( \lG\) because
    \begin{equation}
        \ad(X_i)[z,z']=-[z,\ad(X_i)z']+[z',\ad(X_i)z].
    \end{equation}
    Acting with \( \ad(X_i)^n\) we get terms of the form \( [\ad(X_i)^kz,\ad(X_i)^lz']\) with \( k+l=n\). If \( n\) is large enough, all the terms vanish.

    From the relation \eqref{EqSerrea} we see that \( X_j\in V_i\) for every \( j\). Since \( [X_i,H_j]\) is proportional to \( X_i\) we also have \( H_j\in V_i\) and then \( Y_j\in V_i\) because \( [X_i,Y_j]=\delta_{ij}H_i\in V_i\). Thus the Lie algebra \( V_i\) contains all the Chevalley generators and then \( V_i=\lG\).
\end{proof}

For \( \lambda\in\lH^*\) we define
\begin{equation}
    \lG_{\lambda}=\{ z\in\lG\tq\ad(h)z=\lambda(h)z\forall h\in\lH \}.
\end{equation}

Then one prove that \( \dim\lG_{\alpha_i}=1\) and \( \dim\lG_{m\alpha_i}=0\) if \( m\neq \pm 1,0\). This corresponds to the fact that we have a reduced root system, which is always the case in complex semisimple Lie algebras\footnote{However, at this point we have not proved yet that \( \lG\) is semisimple and has that root system.}. We denote by \( \Phi\) the subset of \( \lambda\in\lH^*\) such that \( \lG_{\lambda}\neq 0\).

It turns out that we have the direct sum decomposition
\begin{equation}
    \lG=\lH\oplus\bigoplus_{\alpha\in\Phi}\lG_{\alpha}.
\end{equation}

One of the key ingredients in this building is the following lemma.
\begin{lemma}
    If \( \lambda\) and \( \mu\) are related by an element of the Weyl group, then \( \dim\lG_{\lambda}=\lG_{\mu}\).
\end{lemma}


\begin{proof}
    Lemma \ref{LemadXiNilpotent} allows us to introduce the automorphism
    \begin{equation}
        \theta_i= e^{\ad(X_i)} e^{-\ad(Y_i)} e^{\ad(X_i)}
    \end{equation}
    of \( \lG\). We see that the restriction of \( \theta_i\) to \( \lH\) is the symmetry associated to \( \alpha_i\) (see \eqref{EqSymsiReltosalphai}). Indeed the first exponential reduces to
    \begin{equation}
        e^{\ad(X_i)}H_k=H_k-A_{ki}X_i
    \end{equation}
    where \( A_{ki}=\alpha_i(H_k)\). The second exponential gives
    \begin{equation}
        \begin{aligned}[]
            e^{\ad(-Y_i)}(H_k-A_{ki}X_i)&=H_k-A_{ki}X_i+(-A_{ki}Y_i-A_{ki}H_i)+\frac{ 1 }{2}(2A_{ki}Y_i)\\
            &=H_k-A_{ki}H_i-A_{ki}X_i.
        \end{aligned}
    \end{equation}
    Notice the simplification of \( A_{ki}Y_i\). The third exponential then provides the result (after some simplifications):
    \begin{equation}
        e^{\ad(X_i)}(H_k-A_{ki}H_i-A_{ki}X_i)=H_k-A_{ki}H_i=H_k-\alpha_i(H_k)H_i.
    \end{equation}
    We proved that \( \theta_i(H_k)=s_I(H_k)\).  We deduce that \( \theta_ie_{\alpha}\in\lG_{s_{\alpha_i}(\alpha)}\) whenever \( e_{\alpha}\in\lG_{\alpha}\). Since \( \theta_i\) is an automorphism of \( \lG\) we have
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=\theta_i[\theta_i^{-1}H_k,e_{\alpha}].
    \end{equation}
    Since \( \theta_i\) reduces to the involutive automorphism \( s_i\) on \( \lH\) we have \( \theta_i^{-1}H_k=\theta_iH_k=s_i(H_k)\). Then we have
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=\theta_i[s_i(H_k),e_{\alpha}]=\theta_i\alpha\big( s_i(H_k) \big)e_{\alpha}.
    \end{equation}
    The eigenvalue of \( \theta_ie_{\alpha}\) for \( \ad(H_k)\) is thus \( \alpha\big( s_i(H_k) \big)\). Using the definition and \( A_{ki}=\alpha_i(H_k)\) we have
    \begin{equation}
        \begin{aligned}[]
            \alpha\big( s_i(H_k) \big)&=\alpha(H_k)-\alpha_i(H_k)\alpha(H_i)\\
            &=\big( \alpha-\alpha(H_i)\alpha_i \big)H_k\\
            &=s_{\alpha_i}(\alpha)H_k.
        \end{aligned}
    \end{equation}
    At the end we got
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=s_{\alpha_i}(H_k)\theta_ie_{\alpha}
    \end{equation}
    and then \( \theta_ie_{\alpha}\in\lG_{s_{\alpha_i}(\alpha)}\). Thus the automorphism \( \theta_i\) transforms \( \lG_{\lambda}\) into \( \lG_{\mu}\) when \( \mu=s_i(\lambda)\) and
    \begin{equation}
        \dim\lG_{\lambda}=\dim\lG_{s_i(\lambda)}.
    \end{equation}
\end{proof}
From here we prove that \( \dim\lG_{\alpha}=1\) for every root \( \alpha\)\footnote{\cite{SerreSSAlgebres} page VI-23. Be careful: this is not the statement of page VI-2.}. 

Now if \( \alpha+\beta=\gamma+\mu\), the elements \( [E_{\alpha},E_{\beta}]\) and \( [E_{\gamma},E_{\mu}]\) are proportional since they belong to the one-dimensional space \( \lG_{\alpha+\beta}\).


\begin{remark}      \label{RemChevDefmapCommXH}
    A linear map \( \phi\colon \lG\to V\) from \( \lG\) to a vector space \( V\) can be defined on the generators \( X_i\), \( Y_i\) and \( H_i\) among with a formula giving \( \phi([X,Y])\) in terms of \( \phi(X)\) and \( \phi(Y)\).
\end{remark}

\begin{probleme}
    This remark could be made more precise. I'm thinking to the proposition \ref{PropStandardBialgStruct} giving the standard bialgebra structure on a Lie algebra.
\end{probleme}

The classification of complex semisimple Lie algebras is the following:
\begin{subequations}
    \begin{align}
        A_l&&\gsl(l+1,\eC)&&\dim=l(l+2)&&l=1,2,\ldots&&
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\ldots  \ar@1{-}[r]&\alpha_l  
           }\\
        B_l&&\go(2l+1,\eC)&&\dim=l(2l+1)&&l=2,3,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@2{->}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@{-}[r]&\alpha_l.
    }   \\
    C_l&&\gsp(l,\eC)&&\dim=l(2l+1)&&l=3,4,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@{->}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{->}[r]&\alpha_l.
    }   \\
    D_l&&\go(2l,\eC)&&\dim=l(2l-1)&&l=4,5,\ldots&&
    \xymatrix{%
    &                       &                       &                          &\alpha_{l-1}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-2}\ar@{-}[dr]\ar[ur]\\
    &               &                               &                              &\alpha_l
    }   \\
    E_6&&&&\dim=78&&l=7,\ldots&&
    \xymatrix{%
    &                   &   \alpha_{6}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \alpha_3 \ar@{-}[r]\ar@{-}[u] & \alpha_4\ar@{-}[r] &\alpha_5
    }   \\
    E_7&&&&\dim=133&&l=7,\ldots&&
    \xymatrix{%
    &                  & &   \alpha_{7}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3\ar@{-}[r]   & \alpha_4 \ar@{-}[r]\ar@{-}[u] & \alpha_5\ar@{-}[r] &\alpha_6
    }   \\
    E_8&&&&\dim=248&&l=8,\ldots&&
    \xymatrix{%
    &                &  & &   \alpha_{8}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3\ar@{-}[r]&\alpha_4\ar@{-}[r]  & \alpha_5 \ar@{-}[r]\ar@{-}[u] & \alpha_6\ar@{-}[r] &\alpha_7
    }   \\
    F_4&&&&\dim=52&&l=4,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@2{->}[r]   & \alpha_3 \ar@{-}[r] & \alpha_4
    }   \\
    G_2&&&&\dim=14&&l=2,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@3{-}[r]&\alpha_2
    }  
    \end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Cartan-Weyl basis}
%---------------------------------------------------------------------------------------------------------------------------

Let us study the eigenvalue equation
\begin{equation}        \label{EqvalpradAprho}
    \ad(A)X=\rho X.
\end{equation}
The number of solutions with $\rho=0$ depends on the choice of $A\in\lG$.

\begin{lemma}
    If $A$ is chosen in such a way that $\ad(A)X=0$ has a maximal number of solutions, then the number of solutions is equal to the rank\index{rank of a Lie algebra} of $\lG$ and the eigenvalue $\alpha=0$ is the only degenerated one in equation \eqref{EqvalpradAprho}.
\end{lemma}

We suppose $A$ to be chosen in order to fulfill the lemma. Thus we have linearly independent vectors $H_i$ ($i=1,\ldots l$) such that
\begin{equation}
    [A,H_i]=0
\end{equation}
where $l$ is the rank of $\lG$. Since $[A,A]=0$, the vector $A$ is a combination $A=\lambda^iH_i$. Since $\ad(A)$ is diagonalisable, one can find vectors $E_{\alpha}$ with
\begin{equation}
    [A,E_{\alpha}]=\alpha E_{\alpha},
\end{equation}
and such that $\{ H_i,E_{\alpha} \}$ is a basis of $\lG$. Using the fact that $\ad(A)$ is a derivation, we find
\begin{equation}
    [A,[H_i,E_{\alpha}]]=\alpha[H_i,E_{\alpha}],
\end{equation}
The eigenvalue $\alpha=0$ being the only one to be degenerated, one concludes that $[H_i,E_{\alpha}]$ is a multiple of $E_{\alpha}$:
\begin{equation}
    [H_i,E_{\alpha}]=\alpha_i E_{\alpha}.
\end{equation}
Replacing $A=\lambda^iH_i$, we have
\begin{equation}
    \alpha E_{\alpha}=[\lambda^iH_i,E_{\alpha}]=\lambda^i\alpha_iE_{\alpha},
\end{equation}
thus $\alpha=\lambda^i\alpha_i$ (with a summation over $i=1,\ldots,l$).

Before to go further, notice that the space spanned by $\{ H_i \}_{i=1,\ldots,l}$ is a maximal abelian subalgebra of $\lG$, so that it is a Cartan subalgebra that we,  naturally denote by $\lH^*$. Thus, what we are doing here is the usual root space construction. In order to stick the notations, let us associate the form $\sigma_{\alpha}\in\lH^*$ defined by $\sigma_{\alpha}(H_i)=\alpha_i$. In that case,
\begin{equation}
    \sigma_{\alpha}(A)=\sigma_{\alpha}(\lambda^iH_i)=\lambda^i\alpha_i=\alpha
\end{equation}
and we have
\begin{equation}
    [A,E_{\alpha}]=\sigma_{\alpha}(A)E_{\alpha}.
\end{equation}
On the other hand, we have $[H_i,E_{\alpha}]=\alpha_iE_{\alpha}=\sigma_{\alpha}(H_i)E_{\alpha}$, so that the eigenvalue $\alpha$ is identified to the root $\alpha$, and we have $E_{\alpha}\in\lG_{\alpha}$.

Let us now express the vectors $t_{\alpha}$ in the basis of the $H_i$. The definition property is $B(t_{\alpha},H_i)=\alpha(H_i)=\alpha_i$. If $t_{\alpha}=(t_{\alpha})^iH_i$, we have
\begin{equation}
    \alpha_i=B(t_{\alpha},H_i)=B_{kl}(t_{\alpha})^k\underbrace{(H_i)^l}_{=\delta^l_i}=B_{ki}(t_{\alpha})^k.
\end{equation}
If $(B^{ij})$ are the matrix elements of $B^{-1}$, we have
\begin{equation}
    (l_{\alpha})^l=\alpha_iB^{il}=\alpha^l
\end{equation}
where $\alpha^l$ is defined by the second equality. Using proposition \ref{Propoxalphaymoinaalpha}, we have
\begin{equation}
    [E_{\alpha},E_{-\alpha}]=B(E_{\alpha},E_{-\alpha})\alpha^lH_l.
\end{equation}
Thus one can renormalise $E_{\alpha}$ in such a way to have
\begin{equation}
    \begin{aligned}[]
        [H_i,H_j]       &=0,\\
        [E_{\alpha},E_{-\alpha}]    &=\alpha^iH_i\\
        [H_i,E_{\alpha}]    &=\alpha_iE_{\alpha}=\alpha(H_i)E_{\alpha}\\
        [E_{\alpha},E_{\beta}]  &=N_{\alpha\beta}E_{\alpha+\beta}
    \end{aligned}
\end{equation}
where the constant $N_{\alpha\beta}$ are still undetermined. A basis $\{ H_i,E_{\alpha} \}$ of $\lG$ which fulfill these requirements is a basis of \defe{Cartan-Weyl}{Cartan-Weyl basis}.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

We follow \cite{Wybourne}. We denote by $\Pi$ the system of simple roots of $\lG$. All the positive roots have the form
\begin{equation}
    \sum_{\alpha\in\Pi}k_{\alpha}\alpha
\end{equation}
with $k_{\alpha}\in\eN$.

\begin{theorem}
    Let $\alpha$ and $\beta$ be simple roots Thus
    \begin{enumerate}
        \item
            $\alpha-\beta$ is not a simple root
        \item
            we have
            \begin{equation}        \label{EqabSuraaStrictNEf}
            \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }=-p
        \end{equation}
        where $p$ is a strictly positive integer.
    \end{enumerate}
\end{theorem}

\begin{proof}[Partial proof]
    We are going to prove that $\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }$ is an integer. Let $\alpha$ and $\gamma$ be non vanishing roots such that $\alpha+\gamma$ is not a root, and define
\begin{equation}
    E'_{\gamma-j\alpha}=\ad(E_{-\alpha})^kE_{\gamma}\in\lG_{\gamma-k\alpha}.
\end{equation}
Since there are a finite number of roots, there exists a minimal positive integer $g$ such that $\ad(E_{-\alpha})^{g+1}E_{\gamma}=0$. We define the constants $\mu_k$ (which depend on $\gamma$ and $\alpha$) by
\begin{equation}
    [E_{\alpha},E'_{\gamma-k\alpha}]=\mu_kE'_{\gamma-(k-1)\alpha}.
\end{equation}
Using the definition of $E'_{\gamma-k\alpha}$ and Jacobi, one founds
\begin{equation}
    \mu_kE'_{\gamma-(k-1)\alpha}=\big[E'_{\alpha},[E_{-\alpha},E'_{\gamma-(k-1)\alpha}]\big]=\alpha^i[H_i,E'_{\gamma-(k-1)\alpha}]+\mu_{k-1}E'_{\gamma-(k-a)\alpha},
\end{equation}
so that $\mu_k=\alpha^i\gamma_i-(k-1)\alpha^i\alpha_i+\mu_{k-1}$, and we have the induction formula
\begin{equation}
    \mu_k=(\alpha,\gamma)-(k-1)(\alpha,\alpha)+\mu_{k-1}
\end{equation}
for $k\geq 2$. If we define $\mu_0=0$, that relation is even true for $k=1$. The sum for $k=1$ to $k=j$ is easy to compute and we get
\begin{equation}
    \mu_j=j(\alpha,\gamma)-\frac{ j(j-1) }{ 2 }(\alpha,\alpha).
\end{equation}
Since $\mu_{g+1}=0$, we have 
\begin{equation}        \label{Eqalphagammapargdeux}
    (\alpha,\gamma)=g(\alpha,\alpha)/2,
\end{equation}
and thus
\begin{equation}
    \mu_j=\frac{ j(g-j+1)(\alpha,\alpha) }{ 2 }.
\end{equation}
Let $\beta$ be any root and look at the string $\beta+j\alpha$. There exists a maximal $j\geq 0$ for which $\beta+j\alpha$ is a root while $\beta+(j+1)\alpha$ is not a root. Now we consider $\gamma=\beta+j\alpha$ with that maximal $j$. Putting $\gamma=\alpha+j\beta$ in \eqref{Eqalphagammapargdeux}, one finds
\begin{equation}
    (\alpha,\beta)=\frac{ (g-2j)(\alpha,\alpha) }{ 2 }, 
\end{equation}
and finally,
\begin{equation}
    \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }=g-2j,
\end{equation}
which is obviously an integer.


\end{proof}

From the inner product on $\lH^*$, we deduce a notion of \defe{angle}{angle between roots}:
\begin{equation}
    \cos(\theta_{\alpha,\beta})=\frac{ (\alpha,\beta) }{ \sqrt{(\alpha,\alpha)(\beta,\beta)} }.
\end{equation}
The \defe{length}{length of a root} of the root $\alpha$ is the number $\sqrt{(\alpha,\alpha)}$.

\begin{lemma}
    If $\alpha$ and $\beta$ are roots, then
    \begin{equation}
        \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }\in\eZ,
    \end{equation}
    and
    \begin{equation}
        \beta-\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }
    \end{equation}
    is a root too.

    If $\alpha$ and $\beta$ are non vanishing, then the $\alpha$-string which contains $\beta$ contains at most $4$ roots. Finally, the ratio
    \begin{equation}
        \frac{ 2(\alpha,\beta) }{ (\alpha,\beta) }
    \end{equation}
    takes only the values $0$, $\pm 1$, $\pm 2$ or $\pm 3$.
\end{lemma}

Let $\Pi=\{ \alpha_1,\ldots,\alpha_l \}$ be a system of simple roots. The \defe{Cartan matrix}{Cartan!matrix} is the $l\times l$ matrix with entries
\begin{equation}        \label{EqDefMatriceCartan}
    A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }.
\end{equation}
Notice that, in the literacy, one find also the convention $A_{ij}=2(\alpha_i,\alpha_j)/(\alpha_j,\alpha_j)$, as in \cite{rncahn}, for example.

\begin{lemma}       \label{LemRatdjaijdjaji}
    There exist positive rational numbers \( d_i\) such that 
    \begin{equation}        \label{EqdiAijdjAji}
        d_i A_{ij}=d_jA_{ji}
    \end{equation}
    where \( A\) is the Cartan matrix.
\end{lemma}

\begin{proof}
    The numbers are given by
    \begin{equation}
        d_i=\frac{ (\alpha_i,\alpha_i) }{ (\alpha_1,\alpha_1) }.
    \end{equation}
    The relations \eqref{EqdiAijdjAji} are easy to check using the definition \eqref{EqDefMatriceCartan}. The fact that \( d_i\) is a strictly positive rational number comes from \eqref{EqabSuraaStrictNEf}.
\end{proof}

\begin{probleme}
    I think that there is a property saying (something like) that \( A_{ij}\) is the larger integer \( k\) such that \( \alpha_i+k\alpha_j\) is  a root.
\end{probleme}
