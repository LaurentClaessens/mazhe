% This is part of (almost) Everything I know in mathematics
% Copyright (c) 2015-2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{The conformal group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Preliminary discussion}
%---------------------------------------------------------------------------------------------------------------------------

If one looks at \cite{ooIYOHooMRMfXl,ooDPRUooOFPyPH}, one sees that a conformal transformation is a transformation of a (pseudo)riemannian manifold that leaves the metric unchanged up to a positive scalar function : \( g'_{\mu\nu}(x')=\Omega(x)g_{\mu\nu}(x)\). Thus we are looking for the maps \( \phi\colon M\to M\) that realise that condition.

Since our objective is to do differential geometry, we cannot follow the computations in \cite{ooIYOHooMRMfXl,ooDPRUooOFPyPH} because there are too much «infinitesimal» in that and we don't understand anything\footnote{No pun intended : these books are very okay, but as far as we are interested in bundles, there are some works to do.}.

In order to determine the Poincaré group that leaves the metric invariant, we were fortunate because of theorem \ref{ThoDsFErq} that ensured linearity of the map \( \phi\). Our search for the Poincaré group\footnote{By the way given by theorem \ref{THOooQJSRooMrqQct}.} was thus simplified by two circumstances :
\begin{itemize}
    \item \( \phi\) and \( d\phi\) are the same.
    \item the condition \( g_{\mu\nu}=g'_{\mu\nu}\) does not involves a specific point, so that we had not to ask ourself questions about the «base point» of the vectors.
\end{itemize}
Here we are in a more complicated situation. 

\begin{definition}
Let \( (M,g)\) be a (pseudo)riemannian manifold. A conformal map will be \( \phi\colon M\to M\) such that
\begin{equation}        \label{EQooCCMMooXVbTAd}
    g_{\phi(x)}\big( d\phi_xv,d\phi_xw \big)=\Omega(x)g_x(v,w)
\end{equation}
for a function \( \Omega\in C^{\infty}(M)\). The condition \eqref{EQooCCMMooXVbTAd} has to hold for every \( x\in M\) and \( v,w\in T_xM\).
\end{definition}

We particularise ourself to the case where the manifold \( M\) is a vector space \( V\) of dimension \( d\) with constant metric \( \eta\). If \( \{ e_i \}_{i=1,\ldots, d}\) is a basis of \( V\), we consider the same basis on each \( T_xV\) and for \( v,w\in T_xV\) we have
\begin{equation}
    v\cdot w=\sum_{ij}\eta_{ij}v_iw_j.
\end{equation}
All the sums are intended from \( 1\) to \( d\).

\begin{definition}      \label{DEFooVKNBooFBWQQM}
    A \defe{conformal map}{conformal map} is a \(  C^{\infty}\) map \( \phi\colon V\to V\) for which there exists a function \( \Omega\in C^{\infty}(V)\) satisfying
    \begin{equation}        \label{EQooOZDUooCDaIrh}
        v\cdot w=\Omega(x) d\phi_x(v)\cdot d\phi_x(w)
    \end{equation}
    for every \( x\in V\) and every \( v,w\in V_x\).
\end{definition}
    

\begin{normaltext}  \label{NorooVEVOooRBpvXF}
    This is not the group of ``angles preserving'' transformations, but rather the group of \emph{locally} preserving the angles.

In order to understand that, let us imagine a deforming material. A vector at point \( A\) is an arrow joining point \( A\) to a point \( B\). The image of the vector \( \vect{ AB }\) has to be \( \vect{ \phi(A)\phi(B) }\) when the material is deformed. Thus instead of moving the vector by \( d\phi\), we should move both extremities by \( \phi\) itself. We could speak about affine spaces as described around definition \ref{DEFooQELZooEXvxgw}, but instead we will describe our subject with a vector bundle.

We will discuss this point in \ref{sebsecooCBKEooQOWqFo}.
\end{normaltext}

\begin{remark}
    We are going not to determine all the conformal transformations\quext{Can you answer the question \url{http://math.stackexchange.com/questions/1549670/rigorous-definition-of-a-generator-for-a-transformation-group} ?}, but the (connected to the identity) Lie group of conformal transformations. That is : we are searching for a Lie group \( G\) of diffeomorphisms \( M\to M\) satisfying the condition \eqref{EQooCCMMooXVbTAd}.
\end{remark}

We will use the strategy presented in \ref{NORMooMGAUooIoLtjW}, as in the proof of proposition \ref{PROPooDVIWooAFDNPy}
% When its done, give a more precise reference at position 10906-29466

Since \( G\) is a Lie group, each element can be written under the form \( g= e^{X}\) for some \( X\in\lG\) (here \( \lG\) is the Lie algebra, that is \( T_eG\)). Following the definition \ref{DEFooUYOZooWdcClz} we define \( X\colon V\to V\) by
\begin{equation}
    X(v)=\Dsdd{  \exp(-tX)(v) }{t}{0}
\end{equation}
The exponential here is the one from the Lie algebra to the Lie group. We also define
\begin{equation}
    \phi_t(x)=\Dsdd{  e^{-tX}x }{t}{0}
\end{equation}
for \( t\in \eR\) and \( x\in V\) (\( t\) being restricted to an open set around \( 0\)). 

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Generators}
%---------------------------------------------------------------------------------------------------------------------------

We consider the case in which the metric is flat an \( g_x=\eta\) for every \( x\).  The starting point is the condition \eqref{EQooOZDUooCDaIrh}. We write it for the map
\begin{equation}
    \phi_t(x)= e^{-tX}(x)
\end{equation}
and we search the function \( X\colon V\to V\). We derive with respect to \( t\) the equality (for fixed \( x\in V\))
\begin{equation}        \label{EQooJZDTooVJEUyo}
    (d\phi_t)_x(v)\cdot (d\phi_t)_x(w)=\Omega_t(x)v\cdot w.
\end{equation}
Permuting two derivatives, we have
\begin{subequations}
    \begin{align}
        \Dsdd{ (d\phi_t)_x(v) }{t}{0}&=\Dsdd{ \Dsdd{   \phi_t(x+uv)  }{u}{0} }{t}{0}\\
        &=\Dsdd{\Dsdd{ \phi_t(x+uv) }{t}{0} }{u}{0}\\
        &=\Dsdd{ \Dsdd{  e^{-tX}(x+uv) }{t}{0} }{u}{0}\\
        &=\Dsdd{ -X(x+uv) }{u}{0}\\
        &=-dX_x(v).
    \end{align}
\end{subequations}
If \( t=0\) we also have
\begin{equation}
    (d\phi_0)_x(w)=d(\id)_x(w)=w.
\end{equation}
Thus the derivative of \eqref{EQooJZDTooVJEUyo} produces the following differential equation for \( X\) :
\begin{equation}
    dX_x(v)\cdot w+dX_x(w)\cdot v=\omega(x)v\cdot w.
\end{equation}
Let \( \{ e_i \}\) be a basis of \( V\). We have \( dX_x(e_i)=\frac{ \partial X }{ \partial x_i }(x)\) and
\begin{equation}
    dX_x(e_i)\cdot e_j=\sum_{kl}\big( dX_x(e_i) \big)(e_j)_l\eta_{kl}=\sum_k\eta_{kj}\frac{ \partial X_k }{ \partial x_i }=\partial_i\tilde X_j
\end{equation}
where \( \tilde X=\eta X\), this is \( \tilde X_i=\sum_k\eta_{ik}X_k\). The equation for \( X\), if \( v=e_i\) and \( w=e_j\) is
\begin{equation}        \label{EQooAKZWooOVIdur}
    \frac{ \partial \tilde X_j }{ \partial x_i }+\frac{ \partial \tilde X_i }{ \partial x_j }=\omega(x)\eta_{ij}.
\end{equation}

\begin{lemma}[\cite{ooDPRUooOFPyPH}]
    Let \( V\) be a vector space of dimension \( d\geq 3\) endowed with the constant metric \( \eta\). The solutions of the differential equation
    \begin{equation}    \label{EQooVBIMooOBKAKQ}
        \partial_if_j+\partial_jf_i=\omega(x)\eta_{ij}
    \end{equation}
    for a function \( f\colon V\to V\) of class \(  C^{\infty}\) are
    \begin{equation}    \label{EQooEFOMooUhcgfT}
        f(x)=a+\sum_{km}b_{km}x_ke_m+\sum_{klm}q_{klm}x_kx_le_m
    \end{equation}
    where
    \begin{enumerate}
        \item
            \( a\in V\) is any vector.
        \item
            the antisymmetric part of \( b\) is free
        \item
            the symmetric part of \( b\) is proportional to \( \eta\).
        \item
            \( q_{ijk}=-d_j\eta_{ik}+d_k\eta_{ij}-d_i\eta_{jk}\) and \( d\) is arbitrary.
    \end{enumerate}
    In this case, \( \omega\) is given by \( \omega(x)=\frac{ 2 }{ d }(\nabla\cdot f)(x)\) where the divergence is the one of proposition \ref{PROPooLIJTooKFTwPY}.
\end{lemma}

\begin{proof}
    We can initiate our work by expression \( \omega\) in terms of \( f\). For that, multiply both sides by \( (\eta^{-1})_{ij}\) and make the sum ove \( i\) and \( j\). On the right hand side we have
    \begin{equation}
        \omega(x)\sum_ij(\eta^{-1})_{ij}\eta_{ij}=\omega(x)\sum_j(\eta^{-1}\eta)_{jj}=\omega(x)d.
    \end{equation}
    On the left hand side we have (reaming the summation indices \( i\leftrightarrow j\) in one of the two terms) :
    \begin{equation}
        2\sum_{ij}(\eta^{-1})_{ij}\frac{ \partial  f_j }{ \partial x_i }=2\nabla\cdot  f.
    \end{equation}
    For the divergence, see the proposition \ref{PROPooLIJTooKFTwPY}. Thus \( \omega(x)=\frac{ 2 }{ d }\nabla\cdot f\) and the equation is
    \begin{equation}    \label{EQooAPOPooBdKskD}
        \partial_i f_j+\partial_j f_i=\frac{ 2 }{ d }(\nabla\cdot f)\eta_{ij}.
    \end{equation}
    We apply on both sides the operator\footnote{Since the function \( f\) is supposed to be of class \(  C^{\infty}\), we derivatives commute.}
    \begin{equation}
        \sum_{ijkl}(\eta^{-1})_{ik}(\eta^{-1})_{jl}\partial_k\partial_l
    \end{equation}
    which is nothing else than the contraction with \( \partial^i\partial^j\) when one uses the uppper-lower index notation.
    The first term is
    \begin{equation}
        \sum_{ijkl}(\eta^{-1})_{ik}(\eta^{-1})_{jl}\partial_k\partial_l\partial_i f_j=\sum_{jl}(\eta^{-1})_{jl}\partial_l(\Box  f_j)=\nabla\cdot(\Box f)
    \end{equation}
    where we introduced the operator
    \begin{equation}
        \Box=\sum_{kl}(\eta^{-1})_{kl}\partial_k\partial_l.
    \end{equation}

    The second term gives the same result. On the right hand side,
    \begin{equation}
        \frac{ 2 }{ d }\sum_{ijkl}(\eta^{-1})_{ik}(\eta^{-1})_{jl}\partial_k\partial_l\eta_{ij}\nabla\cdot  f=\frac{ 2 }{ d }\sum_{kl}(\eta^{-1})_{kl}\partial_k\partial_l\nabla\cdot f=\frac{ 2 }{ d }\nabla\cdot\Box f.
    \end{equation}
    If \( d\neq 1\) we are left with\footnote{By the way, if \( d=1\), the question about the angle preserving diffeomorphisms does not make much sense.}
    \begin{equation}
        \Box(\nabla\cdot  f)=0.
    \end{equation}
    Now we go back to equation \eqref{EQooAPOPooBdKskD}.
    \begin{equation}
        \underbrace{\partial_i f_j}_{A}+\underbrace{\partial_j f_i}_{B}=\underbrace{\frac{ 2 }{ d }(\nabla\cdot f)\eta_{ij}}_C.
    \end{equation}
    We apply the operator
    \begin{equation}
        \sum_{im}\partial_l(\eta^{-1})_{im}\partial_m,
    \end{equation}
    which is the contraction with \( \partial_l\partial^i\).

    On \( A\), we recognize the \( \Box\) operator :
    \begin{equation}
        \partial_l\Box f_j.
    \end{equation}
    On \( B\) we recognize the divergence operator :
    \begin{equation}
        \partial_j\partial_l\nabla\cdot  f.
    \end{equation}
    On \( C\) we have \( \eta^{-1}\eta=\mtu\) and
    \begin{equation}
        \frac{ 2 }{ d }\partial_l\partial_j\nabla\cdot f.
    \end{equation}
    Putting the whole together,
    \begin{equation}    \label{EQooCHJAooVzpeos}
        \partial_l\Box f_j+\big( 1-\frac{ 2 }{ d } \big)\partial_j\partial_l\nabla\cdot f=0.
    \end{equation}
    Let us write the same equation with \( l\leftrightarrow j\) :
    \begin{equation}    \label{EQooSMQVooBqKvdO}
        \partial_j\Box f_l+\big( 1-\frac{ 2 }{ d } \big)\partial_l\partial_j\nabla\cdot f=0.
    \end{equation}
    We sum \eqref{EQooCHJAooVzpeos} with \eqref{EQooSMQVooBqKvdO}  and use the equation \eqref{EQooAPOPooBdKskD} :
    \begin{equation}
        \Box\big( \underbrace{\partial_l f_j+\partial_j f_l}_{=\frac{ 2 }{ d }\eta_{lj}\nabla\cdot f} \big)+2\big( 1-\frac{ 2 }{ d } \big)\partial_l\partial_j\nabla\cdot  f=0.
    \end{equation}
    Using the fact that \( \Box\nabla\cdot f=0\), the first term vanishes. If \( d\neq 2\) we have
    \begin{equation}
        \partial_l\partial\j\nabla\cdot  f=0.
    \end{equation}

    Applying the operation \( \partial_k\partial_l\) on the equation \eqref{EQooAPOPooBdKskD},
    \begin{equation}
        \partial_i\partial_k\partial_l\nabla\cdot f_j+\partial_k\partial_l\partial_j f_i=\frac{ 2 }{ d }\eta_{ij}\underbrace{\partial_k\partial_l\nabla\cdot f}_{=0}=0
    \end{equation}
    If we write \( \Delta_{ijkl}=\partial_i\partial_k\partial_k f_l\) we have
    \begin{equation}    \label{EQooFBXSooLncZtv}
        \Delta_{ijkl}=-\Delta_{ijlk}
    \end{equation}
    while \( \Delta\) is symmetric with respect to its first three indices. Starting from \( \Delta_{ijlk}\) we permute the two inner indices and apply \eqref{EQooFBXSooLncZtv}; the whole two time :
    \begin{equation}
        \Delta_{i\,jk\,l}=\Delta_{i\,kj\,l}=-\Delta_{iklj}=-\Delta_{il\,kj}=\Delta_{il\,jk}=\Delta_{ijlk}.
    \end{equation}
    Thus \( \Delta_{ijlk}=0\), and
    \begin{equation}
        \partial_i\partial_j\partial_kf_l=0
    \end{equation}
    for every \( i,j,k,l\). Thus each component of the function \( f\) is a polynomial of degree \( 2\) with respect to \( x\). This leaves us with
    \begin{equation}
        f(x)=a+\sum_{km}b_{km}x_ke_m+\sum_{klm}q_{klm}x_kx_le_m.
    \end{equation}
    We get constrains on \( a,b,q\) putting that solution into the equation \eqref{EQooVBIMooOBKAKQ} with \( \omega(x)=\frac{ 2 }{ d }\nabla\cdot f\). Using the fact that \( \partial_ie_m=\delta_{im}\) we get
    \begin{equation}
        \partial_if_j=\sum_kb_{kj}\delta_{ki}+\sum_{kl}q_{klj}(\delta_{ki}x_l+x_k\delta_{li})=b_{ij}+2\sum_kq_{kij}x_k,
    \end{equation}
    and
    \begin{equation}
        \nabla\cdot f=\sum_{ij}(\eta^{-1})_{ij}b_{ij}+2\sum_{ijk}q_{kij}(\eta^{-1})_{ij}x_k=\tr(b\eta^{-1})+2\sum_{ijk}q_{kij}(\eta^{-1})_{ij}x_k.
    \end{equation}
    Putting all together we have the condition
    \begin{equation}
        b_{ij}+b_{ji}+2\sum_{k}q_{kij}x_k+2\sum_kq_{kji}x_k=\frac{ 2 }{ d }\eta_{ij}\big( \tr(b\eta^{-1})+2\sum_{klm}q_{klm}(\eta^{-1})_{lm}x_k \big).
    \end{equation}
    Since that condition has to hold for every choice of \( x\) we can separate the part with and without \( x\).

    \begin{subproof}
    \item[Part without \( x\)]
        We have 
        \begin{equation}
            b_{ij}+b_{ji}=\frac{ 2 }{ d }\eta_{ij}\tr(b\eta^{-1}).
        \end{equation}
        The antisymmetric part of \( b\) gives zero on left, but also zero on right because \( \eta^{-1}\) is symmetric. Let us be more specific; if \( A\) is antisymmetric and \( S\) be symmetric. Then \( (AS)^t=-SA\) and \( \tr(AS)=\tr\big( (AS)^t \big)=-\tr(SA)=-\tr(AS)\) by the cyclic invariance of the trace. We conclude that the antisymmetric part of \( b\) is free.

        The symmetric part of \( b\) is proportional to \( \eta\), and the coefficient of proportionality is \( \frac{ 2 }{ d }\tr(b\eta^{-1})\). We can check that this is not a new constraint by putting directly \( b=\alpha\eta\) into the equation :
        \begin{equation}
            \alpha\eta_{ij}+\alpha\eta_{ji}=\frac{ 2 }{ d }\eta_{ij}\tr(\alpha\eta\eta^{-1}),
        \end{equation}
        after simplifications and taking into account \( \tr(\alpha)=\alpha d\), we see that it is valid for every \( \alpha\).
    \item[Path with \( x\)]

        We can factorize \( x_k\) and write the condition as
        \begin{equation}
            2q_{kij}+2q_{kji}=\frac{ 4 }{ d }\eta_{ij}\sum_{lm}(\eta^{-1})_{lm}q_{klm}.
        \end{equation}
        Thus there exists a vector \( d\) such that \( q_{ijk}=-q_{ikj}-2d_i\eta_{jk}\) (the coefficient \( 2\) is for later convenience). Maybe each vector \( d\) will not produce a solution; we'll have to check it later. Using three times the symmetry of \( q\) with respect to the first two indices and the latter formula,
        \begin{subequations}        
            \begin{align}
                q_{kij}&=-q_{kji}-2d_k\eta_{ij}\\
                &=-q_{jki}-2d_k\eta_{ij}\\
                &=q_{jik}+2d_j\eta_{ik}-2d_k\eta_{ij}\\
                &=q_{ijk}+2d_j\eta_{ik}-2d_k\eta_{ij}\\
                &=-q_{ikj}-2d_i\eta_{kj}+2d_j\eta_{ik}-2d_k\eta_{ij}.
            \end{align}
        \end{subequations}
        Thus 
        \begin{equation}    \label{EQooKZVTooWSMmyM}
            q_{ijk}=-d_j\eta_{ik}+d_k\eta_{ij}-d_i\eta_{jk}.
        \end{equation}
    \end{subproof}
    
    At this point we proved that every solution of \eqref{EQooVBIMooOBKAKQ} are of the given form \eqref{EQooEFOMooUhcgfT} with the constrains. We still have to check that for every choice of antisymmetric \( b\) and every choice of \( d\) (giving \( q\) by the formula \eqref{EQooKZVTooWSMmyM}), the corresponding function actually is a solution.

    \begin{subproof}
    \item[Antisymmetric]
        Let us check for \( f(x)=\sum_{km}b_{km}x_ke_m\) when \( b\) is antisymmetric. We have
        \begin{equation}
            \partial_if_j=\sum_kb_{kj}\underbrace{\partial_ix_k}_{=\delta_{ik}}=b_{ij}
        \end{equation}
        and
        \begin{equation}
            \nabla\cdot f=\sum_{kl}(\eta^{-1})_{kl}\partial_kf_l=\sum_{kl}(\eta^{-1})_{kl}b_{kl}=0.
        \end{equation}
        The lase zero is because of the contraction of a symmetric matrix (\( \eta^{-1}\)) with an antisymmetric matrix. Now \( \partial_if_j+\partial_jf_i=b_{ij}+b_{ji}=0\) and the equation is satisfied.

    \item[The quadratic part]
        We have to check that the function
        \begin{equation}
            f(x)=\sum_{klm}\big( -d_l\eta_{km}+d_m\eta_{kl}-d_k\eta_{lm} \big)x_kx_lem
        \end{equation}
        is a solution of \( \partial_if_j+\partial_jf_i=\frac{ 2 }{ d }\eta_{ij}\nabla\cdot f\). Few computations provide on the one hand
        \begin{equation}
            \partial_if=2\sum_k\big( -d_i\eta_{kj}+d_j\eta_{ki}-d_k\eta_{ij} \big)x_k,
        \end{equation}
        and on the other hand,
        \begin{equation}
            \nabla\cdot f=-2d\sum_kd_kx_k.
        \end{equation}
        A few more computations show that the equation is satisfied for every choice of vector \( d\).
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Exponentiation}
%---------------------------------------------------------------------------------------------------------------------------

So we have \( X(x)=\eta^{-1}\tilde X(x)\) with
\begin{equation}
    \tilde X(x)=a+\sum_{km}b_{km}x_ke_m+\sum_{klm}q_{klm}x_kx_le_m.
\end{equation}
We are now going to exponentiate each part of that ``infinitesimal'' transformation.

\begin{description}
        \item[Translation] 
    For the constant part we have \( X(x)=\eta^{-1} a\) and up to redefinition of \( a\) we suppose \( X(x)=a\). Thus
    \begin{equation}
        e^{tX}(x)=\sum_{k=0}^{\infty}=x+\sum_{k=1}^{\infty}\frac{ t^ka }{ k! }=x+(e^t-1)a.
    \end{equation}
    One can check that with \( \phi_t(x)=x+(e^t-1)a\) we have \( \Dsdd{ \phi_t(x) }{t}{0}=a\). 

    The corresponding global transformation is the translation \( \phi_1\), that is \( \phi(x)=x+(e-1)a\). This is an arbitrary translation.

        \item[Dilatation] 

    For the symmetric part of \( b\) we have
    \begin{equation}
        \tilde X=\sum_{km}(\alpha\eta)_{km}x_ke_m
    \end{equation}
    and then
    \begin{equation}
        X(x)=\alpha\sum_{kml}\eta_{km}x_k(\eta^{-1})_{ml}e_l=\alpha\sum_lx_le_l=\alpha x.
    \end{equation}
    Thus \( X(x)=\alpha x\) for every \( k\) (included \( k=0\)) and
    \begin{equation}
        e^{tX}(x)=\sum_{k=0}^{\infty}\frac{ t^k\alpha^k }{ k! }x= e^{t\alpha}x.
    \end{equation}
    The corresponding global transformation is
    \begin{equation}
        \phi(x)= e^{\alpha}x,
    \end{equation}
    that is a dilatation with positive coefficient.

\item[Rotation]

    For the antisymmetric part of \(b\)  we have \( \tilde X(x)=bx\), that is
    \begin{equation}
        X(x)=\eta^{-1}bx.
    \end{equation}
    We check that \(  e^{tX}\) is an \( \eta\)-isometry, that is
    \begin{equation}
        e^{tX}(x)\cdot  e^{tX}(y)=x\cdot y.
    \end{equation}
    That equality is obviously true with \( t=0\). For checking other values of \( t\), we take the derivative with respect to \( t\) and we check that this is zero :
    \begin{equation}
        X(x)\cdot y+x\cdot X(y)\stackrel{?}{=}0,
    \end{equation}
    that is
    \begin{equation}    \label{EQooGKSSooBxzEzx}
        \eta^{-1}bx\cdot y\stackrel{?}{=}-x\cdot \eta^{-1}by.
    \end{equation}
    Before to make the (simple) computation, we could want to use something like \( \langle Ax, y\rangle =\langle x, A^ty\rangle \). Since the product here is given by the metric \( \eta\), the notion of «antisymmetric» has to be given by \( \eta^{-1}A\) where \( A\) is a regular antisymmetric matrix, and search for some notion of \( \eta\)-transposition. But \ldots We know from \ref{NooMZVRooExWVKJ} that the notion of transpose of a linear map is incredibly bad defined. So let's make the computation by brute force.

    Computing the two sides of \eqref{EQooGKSSooBxzEzx} with the full indices techniques we find
    \begin{subequations}
        \begin{align}
            \eta^{-1}bx\cdot y=\sum_{ij}b_{ij}x_jy_i\\
            x\cdot \eta^{-1}by=\sum_{ij}b_{ij}x_iy_j,
        \end{align}
    \end{subequations}
    and the yes we have 
    \begin{equation}
        X(x)\cdot y+x\cdot X(y)=0.
    \end{equation}

    In the global conformal group, the infinitesimal transformation
    \begin{equation}
        X(x)=\eta^{-1}bx
    \end{equation}
    with an antisymmetric matrix \( b\) correspond to the global isometries of \( \eta\).

\item[Special conformal transformations]

    We work on the exponentiation of the transformation
    \begin{equation}
        \tilde X(x)=\sum_{ijk}q_{ijk}x_ix_je_k=\sum_{ijk}(-d_j\eta_{ik}+d_k\eta_{ij}-d_j\eta_{jk})x_ix_je_k.
    \end{equation}
    The first work is to write down \( X(x)=\eta^{-1}\tilde X(x)\), that is to replace \( e_k\) by \( \sum_l(\eta^{-1})_{lk}e_k\). We have~:
    \begin{equation}
        X(x)=-2\sum_{ij}d_ix_ix_je_j+\sum_{kl}d_k(\eta^{-1})_{kl}e_l(x\cdot x).
    \end{equation}
    Now we define \( d'=\eta^{-1}d\) and we write
    \begin{equation}
        X(x)=-2(d'\cdot x)x+x^2d'
    \end{equation}
    where \( x^2\) stands for \( x\cdot x\) (which is not specially positive since the metric \( \eta\) is not positive defined). Notice that the vector \( d'\) is arbitrary in the sens that, since \( d\) can take any value, \( d'\) can also take any value. For that reason, we will drop the prime.

    In order to determine \(  e^{X}(x)\) we will solve the differential equation for \( x(t)= e^{tX}(x_0)\) (with \( x(0)=x_0\)). We have :
    \begin{subequations}
        \begin{align}
            x'(s)=\Dsdd{  e^{tX}(x_0) }{t}{s}&=\Dsdd{  e^{(t-s)X}   e^{sX(x_0)}  }{t}{s}\\
            &=\Dsdd{  e^{(t-s)X}x(s) }{t}{s}\\
            &=\Dsdd{  e^{uX}x(s) }{u}{0}\\
            &=X\big( x(s) \big).
        \end{align}
    \end{subequations}
    Thus for every \( t\) we have
    \begin{equation}
        x'(t)=X\big( x(t) \big)
    \end{equation}
    and dropping the dependence in \( t\) on \( x\) and \( x'\) the differential equation to solve reads\cite{ooUCSPooYwxHzL}
    \begin{equation}
        x'=-2(d\cdot x)x+(x\cdot x)d.
    \end{equation}

    In order to solve it we pose \( y(t)=\frac{ x(t) }{ x(t)\cdot x(t) }\). We have
    \begin{equation}
        y'=\frac{ x'x^2-x(2x\cdot x') }{ (x\cdot x)^2 }
    \end{equation}
    Then we substitute \( x'\) by \( -2d\cdot x+x^2d\) in the right hand side and after some computations we get
    \begin{equation}
        y'(t)=d.
    \end{equation}
    Thus \( y'(t)=y_{0}+td\) and knowing that \( y_0=y(0)=\frac{ x_0 }{ x_0^2 }\) we can write
    \begin{equation}
        \frac{ x }{ x^2 }=y=\frac{ x_0 }{ x_0^2 }+td.
    \end{equation}
    We are left with the algebraic equation
    \begin{equation}
        \frac{ x }{ x^2 }=A
    \end{equation}
    with \( x,A\in(V,\eta)\). Since \( x^2\) in only a numerical coefficient we have \( x=\lambda A\) for some \( \lambda\in \eC\). Replacing :
    \begin{equation}
        \frac{ A }{ A^2 }=\lambda A
    \end{equation}
    and thus \( \lambda=\frac{1}{ A^2 }\), so that
    \begin{equation}
        x=\frac{ A }{ A^2 }.
    \end{equation}
    In our case, \( A=\frac{ x_0 }{ x_0^2+td }\), and few computations provide
    \begin{equation}
        x(t)=\frac{ x_0+tx_0^2d }{ 1+tx_0\cdot d+t^2x_0^2d^2 }.
    \end{equation}
    As fas as \( x_0^2\neq 0\), this is the integral curve of the special conformal transformation of parameter \( d\). It always exists on an open set around \( t=0\).

    The generic form of the special conformal transformation is got with \( t=1\), which provides
    \begin{equation}        \label{EQooPMIGooVLXpYt}
        e^{X}(x_0)=\frac{ x_0+x_0^2d }{ 1+x_0\cdot d+x_0^2d^2 }.
    \end{equation}
\end{description}

The whole derivation of the special conformal transformation \eqref{EQooPMIGooVLXpYt} raises the question of the well-definiteness. Let us provide an answer in the euclidian case. We suppose that \( \eta\) is strictly positive defined. Let \( (x_0,d)\) such that the denominator in \eqref{EQooPMIGooVLXpYt} is negative. Taking a rotation \( R\) that brings \( x_0\) to \( e_1\) (in the canonical basis). The value of \( x_0\cdot d\) and \( x_0^2d^2 \) are not changed if we substitute \( x_0\) by \( Rx_0\) and \( d\) by \( Rd\). Thus we have a vector \( d\) such that with \( (x_0,d)=(e_1,d)\), the denominator is zero.o

We have \( 1+x_0\cdot d+x_0^2d^2=1+d_1+d^2\), with \( b^2=b_1^2+\ldots b_n^2>|b_1|\). Thus the denominator is not only positive but larger than \( 1\). This shows (by contradiction) that in the euclidian case, the expression \eqref{EQooPMIGooVLXpYt} is always well defined.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{What about really preserving the angles ?}
%---------------------------------------------------------------------------------------------------------------------------
\label{sebsecooCBKEooQOWqFo}

We continue the discussion of \ref{NorooVEVOooRBpvXF}. If one wants a map \( \phi\colon M\to M\) that preserves the angles in the sense that for any three points \( A,B,C\in M\) the angle formed by \( \vect{ AB }\) and \( \vect{ AC }\) is the same as the angle formed by the vectors \( \vect{ \phi(A)\phi(B) }\) and \( \vect{ \phi(A)\phi(C) }\), we need more structure, since there are no notion of ``vector from one point to another'' in a general manifold. So we particularize ourself to the case in which \( M\) is a vector space. Thus we can write differences like \( \phi(x)-\phi(y)\).

Let \( V\) be a finite dimensional vector space and \( E=V\times V\) be the trivial vector bundle with fibre \( V\). We denote \( V_x=\{ (x,v)\tq v\in V \}\) and for each \( x\in V\) we have a non-degenerate bilinear form \( g_x\colon V\to V\). We define
\begin{equation}
    g\big( (x,v),(x,w) \big)=g_x(v,w)
\end{equation}
and we will often directly write \( g_x(v,w)\) or \( v\cdot w\) when \( v,w\) belong to \(V_x\) instead of \( V\).

A map \( \phi\colon V\to V\) also acts on the vector bundle as
\begin{equation}        \label{EQooFICEooQACFoU}
    \phi(x,v)=\big( \phi(x),\phi(x+v)-\phi(x) \big).
\end{equation}
This way to act translates the fact that for a vector, we displace the ending point as well as the starting point with \( \phi\). This is not the same as displacing the vector by \( d\phi_x\).

\begin{remark}
    We consider a vector bundle with fibre \( V\) in order to make sense to the definition \eqref{EQooFICEooQACFoU}. One can also working on the more intuitive setting of the tangent bundle \( TM\), but in this case, on has to deal with a linear bijection between \( T_xM\) and \( V\).
\end{remark}

With no abuse of notations, the condition \eqref{EQooOZDUooCDaIrh} reads, for \( v,w\in V\) :
\begin{equation}\label{EQooFZUFooTGWpBn}
    g_x(v,w)=\Omega(x)g_{\phi(x)}\big(  \phi(x+v)-\phi(x),\phi(x+w)-\phi(x)  \big).
\end{equation}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Two dimensional conformal group}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let us consider the euclidian two dimensional space, that is \( d=2\) and \( \eta=\delta\). The equations \eqref{EQooAKZWooOVIdur} reduce to
\begin{subequations}
    \begin{numcases}{}
        \partial_1X_1=\partial_2X_2\\
        \partial_1X_2=-\partial_2X_1
    \end{numcases}
\end{subequations}
that are the Cauchy-Riemann equations \eqref{PropkwIQwg}. Theorem \ref{PropkwIQwg} says that the map \( Z\colon \eC\to \eC\) defined by
\begin{equation}
    Z(x+iy)=X_1(x,y)+iX_2(x,y)
\end{equation}
is holomorphic.

For the sake of generality, let us note that \( Z\) has to be holomorphic on the domain where we want the global transformation to be conformal : a differential equation is something local. Thus, if we want to check also for transformations that are only conformal on an open set, we want to study maps \( Z\) that are holomorphic on an open set. We can suppose \( Z\) to be meromorphic outside that open set. Thus we say that the infinitesimal generators of the conformal transformations are meromorphic maps and can be written as\footnote{Theorem \ref{THOooMKJOooVghZyG}.}
\begin{equation}
    Z(z)=\sum_{k\in \eZ}a_kz^k.
\end{equation}

\begin{remark}
    The map \( Z\) has to be seen as the ``component'' maps of the vector field that is tangent to a global transformation. That is the vector field which is everywhere tangent to the integral curves.
\end{remark}

\begin{normaltext}  \label{NORMooHDLPooQBfEif}
    The map \( Z\) being seen as an element of the Lie algebra of the conformal group\footnote{The extended conformal group in the sense that some of the transformations are not globally well defined.}, it acts on an holomorphic\quext{In the context of complex manifold we are dealing with holomorphic functions like we are dealing with \(  C^{\infty}\) function in real differential geometry ?} function \( f\colon \eC\to \eC \) by the definition \ref{DEFooUYOZooWdcClz} and its formula \eqref{EQooQDLAooZXnWta} :
    \begin{equation}
        Z_{z_0}(f)=-df_{z_0}\big( Z(z_0) \big)=-Z(z_0)(\partial_zf)(z_0)
    \end{equation}
    where we used the proposition \ref{PROPooCHUEooYsGcQK}.
\end{normaltext}

Thus for each meromorphic function \( Z\) we have a generator of the conformal group that is written as \( Z(z)\partial_z\). But we know a basis of the meromorphic functions (at least the ones that have only one pole at zero) : these are the functions \( l_n=z^n \) with \( n\in \eZ\). So we set
\begin{equation}
    l_n=-z^{n+1}\partial_z.
\end{equation}
These operators are a basis of the Lie algebra of conformal transformations that are defined on an open set \( B(0,r)\setminus \{ 0 \}\). The commutation relations are easy to compute :
\begin{equation}
    [l_m,l_n]=(m-n)l_{m+n}.
\end{equation}

Let us determine what are the operators \( l_n\) which are globally defined\cite{ooPEEYooCndpvc}. First the operator \( z^k\partial_z\) has a pole at \( z=0\) for every \( k<0\). In order to determine the values of \( k\) for which \( z^k\partial_z\) has a pole at \( z=\infty\) we follow the definition \ref{DEFooPXYYooOMZYOT}\quext{We follow in fact the \emph{idea} of that definition, but we should have a more precise definition of an holomorphic vector field and a pole of a vector field.} and perform the change of variable \(w=1/z\).

Let \( U\) be a neighbourhood of \( 0\) in \( \eC\) and consider the chart 
\begin{equation}
    \begin{aligned}
        z\colon U&\to \hat \eC \\
        w&\mapsto \frac{1}{ w }. 
    \end{aligned}
\end{equation}
This is a chart for a neighbourhood of \( \infty\). The definition of \( \partial_w\) is \( \partial_wf\big( z(w_0) \big)=\Dsdd{ f\big( z(w_0+t) \big) }{t}{0}  \). Here the limit is with \( t\in U\). Let us compute, using the fact that the differential of an holomorphic function \( f\) is the multiplication by \( \partial_z\) (proposition \ref{PROPooCHUEooYsGcQK})
\begin{subequations}        \label{EQooOCHQooFVHtYV}
    \begin{align}
        \frac{ \partial f }{ \partial w }\big( z(w_0) \big)&=\Dsdd{ f\big( z(w_0+t) \big) }{t}{0}\\
        &=df_{z(w_0)}z'(w_0)\\
        &=(\partial_zf)(\frac{1}{ w_0 })\frac{ -1 }{ w_0^2 }
    \end{align}
\end{subequations}
Thus 
\begin{equation}
    \frac{ \partial f }{ \partial z }\big( z(w_0) \big)=-w_0^2\frac{ \partial f }{ \partial w }\big( z(w_0) \big).
\end{equation}
In that sense we write
\begin{equation}
    \partial_w=-z^2\partial_z.
\end{equation}

\begin{remark}
    This computation \eqref{EQooOCHQooFVHtYV} is nothing else than the usual chain formula
    \begin{equation}
        \frac{ \partial f }{ \partial w }=\frac{ \partial f }{ \partial w }\frac{ \partial w }{ \partial z }.
    \end{equation}
\end{remark}

We have 
\begin{subequations}
    \begin{align}
        z^k(\partial_zf)&=z^k\frac{-1}{ z^2 }\partial_wf\\
        &=-z^{k-2}\partial_wf\\
        &=-w^{2-k}\partial_wf.
    \end{align}
\end{subequations}
For the limit \( w\to 0\) to exist we need \( k\leq 2\).

At the end of the day, \( z^kpartial_z\) has no pole at \( 0\) and \( \infty\) when \( k=0,1,2\). The operators \( l_n\) are thus well defined with
\begin{equation}
    n=-1,0,1.
\end{equation}
