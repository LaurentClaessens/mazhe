% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Other results}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Abstract Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

As before if we chose a basis $\{\varphi_1\ldots\varphi_l\}$ of $V$, we can consider a lexicographic ordering\index{lexicographic ordering} on $V$. A root is \defe{simple}{simple!abstract root} when it is positive and can't be written as as sum of two positive roots. As in a non abstract case, abstract simple root also have the following property:

\begin{proposition}
If $\dim V=l$, one has only $l$ simple roots $\alpha_1,\ldots,\alpha_l$; they are linearly independent and if $\beta\in\Phi$ expands into $\beta=\sum c_j\alpha_j$, the $c_j$'s all are integers and the non zero ones all have the same sign.
\end{proposition}

An ordering on $V$ gives a notion of simple roots. The $l\times l$ matrix whose entries are
\[
   A_{ij}=\frZ{\alpha_i}{\alpha_j}
\]
is the \defe{abstract Cartan matrix}{abstract!Cartan matrix}\index{Cartan!abstract matrix} of the abstract root system and the given ordering. 

\begin{theorem}
    The main properties are
    \begin{enumerate}
        \item $A_{ij}\in\eZ$,
        \item $A_{ii}=2$,
        \item if $i\neq j$, then $A_{ij}\leq 0$ and $A_{ij}$ can only take the values $0,-1,-2$ or $-3$,
        \item if $i\neq j$, $A_{ij}A_{ji}<4$ (no sum),
        \item $A_{ij}=0$ is and only if $A_{ji}=0$,
        \item $\det A$ is integer and positive.
    \end{enumerate}
\end{theorem}

    \begin{proof}
    The last point is the only non immediate one. The matrix $A$ is the product of the diagonal matrix with entries $2/|\alpha_i|^2$ and the matrix whose entries are $(\alpha_i,\alpha_j)$. The fact that the latter is positive definite is a general property of linear algebra. If $\{e_i\}$ is a basis of a vector space $V$, the matrix whose entry $ij$ is given by $(e_i,e_i)$ is positive definite. Indeed one can consider an orthonormal basis $\{f_i\}$ and a nondegenerate change of basis $e_i=B_{ik}f_k$. Then $(e_i,e_j)=(BB^t)_{ij}$. It is easy to see that for all $v\in V$, we have $(BB^t)_{ij}v^iv^j=\sum_k(v^iB_{ik})^2>0$.

    The fact that the determinant is integer is simply the fact that this is a polynomial with integer variables.
\end{proof}

If we have an ordering on $V$ we define $\Phi^+$, the set of positive roots. From there, one can consider $\Pi$, the set of simple roots. Any element of $\Phi$ expands to a sum of elements of $\Pi$. Note that the knowledge of $\Pi$ is sufficient to find $\Phi^+$ back because $\alpha>0$ implies $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

We can make this reasoning backward. Let us consider $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a basis of $V$ such that any $\alpha\in\Phi$ expands as a sum of $\alpha_i$ with all coefficients of the same sign. Such a $\Pi$ is a \defe{simple system}{simple!system}. From such a $\Pi$, we can build a $\Phi^+$ as the set of elements of the form $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

\begin{proposition}
The so build $\Phi^+$ is the set of positive roots for a certain ordering.
\end{proposition}

\begin{proof}
If we consider on $V$ the lexicographic ordering with respect to the basis $\Pi$, a positive element $\alpha=\sum c_i\alpha_i$ has at least one positive coefficient among the $c_i$. If $\alpha\in\Phi$, we can say (by definition of $\Pi$) that in this case \emph{all} the coefficients are positive, then the positive roots exactly form the set $\Phi^+$.
\end{proof}

From now when we speak about a $\Phi^+$, it will always be with respect to a simple system. The advantage is the fact that there are no more implicit ordering.

\begin{lemma}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system and $\alpha\in\Phi^+$. Then
\[
   s_{\alpha_i}=\begin{cases}
                           -\alpha_i & \text{if $\alpha=\alpha_i$}\\
                    >0       & \text{if $\alpha\neq\alpha_i$}.
                          \end{cases}
\]
\end{lemma}

\begin{proof}
The first case is well know from a long time. For the second, compute
\begin{equation}
\begin{split}
  s_{\alpha_i}( \sum c_j\alpha_j )&=\sum_{j\neq i}c_j\alpha_j+c_i\alpha_i-2c_i\alpha_i
                                         -\sum_{j\neq i}\frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)\alpha_i\\
                              &=\sum_{j\neq i}+\left(
        -\sum_{i\neq j}       \frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)+c_i
                                    \right)\alpha_i.
\end{split}
\end{equation}
We see that between $\sum c_k\alpha_k$ and $s_{\alpha_i}(\sum c_k\alpha_k)$, there is just the coefficient of $\alpha_i$ which changes. Then if $\alpha\neq \alpha_i$, the positivity is conserved.

\end{proof}

\begin{proposition}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system. Then $W$ is generate by the $s_{\alpha_i}$'s. If $\alpha\in\Phi$, then there exists a $\alpha_i\in\Pi$ and $s\in W$ such that $s\alpha_j=\alpha$.
\end{proposition}

\begin{proof}
We denote by $W'$ the group generate by the $s_{\alpha_i}$'s; the purpose is to show that $W=W'$. We begin to show that if $\alpha>0$, then $\alpha=s\alpha_j$ for certain $s\in W'$ and $\alpha_j\in\Pi$. For this, we write $\alpha=\sum c_j\alpha_j$ and we make an induction with respect to $\niv(\alpha)=\sum c_j$. If $\niv(\alpha)=1$, then $\alpha-\alpha_j$ and $s=\id$ works.  Now we suppose that it works for $\niv<\niv(\alpha)$. We have
\[
   0<(\alpha,\alpha)=\sum c_i(\alpha,\alpha_i).
\]
Since all the $c_i$ are positive, it assures the existence of a $i_0$ such that $(\alpha,\alpha_{i_0)}>0$. Then from the lemma, $\beta=s_{\alpha_{i_0}}(\alpha)>0$ ($\alpha\neq \alpha_{i_0}$ because $\niv(\alpha)>1$). The root $\beta$ can be expanded as
\begin{equation}
\beta=\sum_{j \neq i_0}c_j\alpha_j+\left(  c_{i_0}-\sum_{j\neq i_0}\frac{c_j}{|\alpha_{i_0}|^2}(\alpha,\alpha_{i_0})   \right)\alpha_{i_0}.
\end{equation}
Since $(\alpha,\alpha_{i_0})>0$, it implies $\niv(\beta)<\niv(\alpha)$ and thus $\beta=s'\alpha_j$ for a certain $s'\in W'$. So $\alpha=s_{\alpha_{i_0}}s'\alpha_j$ with $s_{\alpha_{i_0}}s'\in W'$. This conclude the induction. For $\alpha<0$, the same result holds by writing $-\alpha=s\alpha_j$ and $\alpha=ss_{\alpha_j}\alpha_j$.

Now it remains to prove that $W'\subseteq W$. For a $\alpha\in\Phi$, we write $\alpha=s\alpha_j$ with $s\in W'$. Then
\[
   s_{\alpha}=ss_{\alpha_j}s^{-1}\in W'.
\]
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{About group representations}
%---------------------------------------------------------------------------------------------------------------------------

Let $\pi$ be a representation of a group $G$. The \defe{character}{character!of a representation} of $\pi$ is the function
\begin{equation}
    \begin{aligned}
        \chi_{\pi}\colon G&\to\eC \\ 
        g&\mapsto \tr\big( \pi(g) \big). 
    \end{aligned}
\end{equation}
From the cyclic invariance of trace, it fulfils $\chi_{\pi}(gxg^{-1})=\chi_{\pi}(x)$, so that the character is a central function.

Let $G$ be a Lie group with Lie algebra $\lG$. We denote by $Z_{\pm}$ the subgroup of $G$ generated by $\lN^{\pm}$. The \defe{Cartan subgroup}{Cartan!subgroup} $D$ of $G$ is the maximal abelian subgroup of $G$ which has $\lH$ as Lie algebra.

A \defe{character}{character!of an abelian group} of an abelian group is a representation of dimension one.

 Let $T$ be a representation of $G$ on a complex vector space $V$. One say that $\xi\in V$ is a \defe{highest weight}{highest weight!for group representation} if
\begin{itemize}
\item $T(z)\xi=\xi$ for every $z\in Z_+$,
\item $T(g)\xi=\alpha(g)\xi$ for every $g\in D$.
\end{itemize}
The function $\alpha\colon D\to \eC$ is the \defe{highest weight}{highest weight} of the representation $T$.

\begin{lemma}
    The function $\alpha$ is a character of the group $D$.
\end{lemma}

\begin{proof}
    The number $\alpha(gg')$ is defined by $T(gg')\xi=\alpha(gg')\xi$. Using the fact that $T$ is a representation, one easily obtains $T(gg')\xi=\alpha(g)\alpha(g')\xi$. 
\end{proof}

\subsection{Modules and reducibility}
%------------------------------------

As far as terminology is concerned, one can sometimes find the following definitions. A $\lG$-module is \defe{simple}{simple!module}\index{module!simple} when the only submodules are $\lG$ and $0$. It is \defe{semisimple}{semisimple!module}\index{module!semisimple} when it is isomorphic to a direct sum of simple modules. The module is \defe{indecomposable}{indecomposable module}\index{module!indecomposable} if it is not isomorphic to the direct sum of two non trivial submodules.

An vector space endomorphism $\dpt{a}{V}{V}$ is \defe{semisimple}{semisimple!endomorphism} if $V$ is semisimple as module for the associative algebra spanned by $A$. In this text, we will not use this terminology but the one in terms of reducibility. It is clear that $\lG$ is itself a $\lG$-module for the adjoint representation. From this point of view, a $\lG$-submodule is an ideal. Then a simple Lie algebra is an irreducible $\lG$-module and a semisimple Lie algebra is completely reducible by corollary \ref{cor:decomp_ideal}. This explains the terminology correspondence

\begin{center}
\begin{tabular}{ccc}
\emph{simple} & $\leftrightarrow$ & \emph{irreducible} \\ 
\emph{semisimple}& $\leftrightarrow$ &\emph{completely reducible}.\\
\end{tabular} 
\end{center}

\begin{theorem}[Weyl's theorem]
A representation of a semisimple Lie algebra is completely reducible.
\end{theorem}

\subsection{Weight and dual spaces}
%---------------------------------

In general, when $\dpt{T}{V}{V}$ is an endomorphism of the vector space $V$ and $\lambda\in\eK$ ($\eK$ is the base field of $V$), we define
\begin{equation}
V_\lambda=\{ v\in V\tq (T-\lambda\mtu)^nv=0\textrm{ for a $n\in\eN$} \}).
\end{equation}
If $V(\lambda)\neq0$, we say that $\lambda$ is a \defe{weight}{weight!for endomorphism} and $V(\lambda)$ is a weight space.

Let now particularize to the case where $\lG$ is a Lie algebra, and $\lG^*$ its dual space (the space of all the complex linear forms on $\lG$). Let $\rho$ be a representation of $\lG$ on a complex vector space $V$ (seen as a $\lG$-module\index{module}), and $\gamma\in\lG^*$. For each $x\in\lG$, we have $\dpt{\rho(x)}{V}{V}$ and $\gamma(x)\in\eC$; then it makes sense to speak about the operator $\dpt{\rho(x)-\gamma(x)}{V}{V}$ and to define
\begin{equation}
V_{\gamma}=\{ v\in V\tq\forall x\in\lG,\exists n\in\eN \tq\big(\rho(x)-\gamma(x)\big)^nv=0 \}.
\end{equation}
If $V_{\gamma}\neq 0$, we say that $\gamma$ is a \defe{weight}{weight!for representation} for the representation $\rho$ while $V\bgamma$ is the corresponding \defe{weight space}{weight!space}. 

Notice that a root is a weight space for the adjoint representation, see definition \ref{DefRootSpace}. We denote by $\Phi$ the set of non empty root spaces. 

\begin{lemma}

Let $\End(V)$ be the algebra of linear endomorphism of a vector space $V$. Let $x_1,\ldots,x_k,y_1,\ldots,y_k\in\End(V)$ and 
\[
    e=\sum_i[x_i,y_i].
\]
If $e$ commutes with all $x_i$, then it is nilpotent.
\label{lem:EndV_e}
\end{lemma}

A proof of this lemma can be found in \cite{Hochschild}

\begin{theorem}
Let $\lG$ be a Lie algebra of linear endomorphisms of a finite dimensional vector space $V$. We suppose that $V$ is a completely reducible $\lG$-module and we denote the center of $\lG$ by $\mZ$. Then 
\begin{enumerate}
\item $[\lG,\lG]\cap\mZ=0$,
\item $L/\mZ$ has a non zero abelian ideal,
\item any element of $\mZ$ is a semisimple endomorphism.
\end{enumerate}
\end{theorem}

\begin{probleme}
    The following proof seems me to be quite wrong.
\end{probleme}

\begin{proof}
Let $A$ be the associative algebra spanned by $\lG$ and the identity on $V$. It is clear that the $A$-stable subspaces are exactly the $\lG$-stable ones. Then $V$ is a completely reducible $A$-module and it has no non zero nilpotent left ideal. Indeed let $B$ be a left ideal in $A$ such that $BB=0$. In this case, $B\cdot V$ is a $A$-submodule of $V$ (because $B$ is an ideal) and $V=B\cdot V\oplus W$ for a certain $A$-submodule $W$. Since $B\cdot V$ is a $A$-submodule, 
\[
B\cdot W\subset(B\cdot V)\cap W
\]
(because $W$ is stable under $A$) which implies $B\cdot W=0$ and $B\cdot V=B\cdot(BV+W) 0$. Consequently, $B=0$.

Let $T$ be the center of $A$; this is an ideal, so that $T$ has no non zero nilpotent elements. To see it, consider a nilpotent element $z\in T$. Remark that $T=Az$  is a nilpotent ideal because $AzAz=Az^2A$. Now, we prove that $z$ is a semisimple linear endomorphism of $V$. By lemma \ref{lem:A_n_stabilise}, with large enough $n$, $z^n(V)$ finish to stabilize. Let $q=\sum_{v\in V}$  The space $V_s=z^n(V)$ is not zero because $z$ is not nilpotent. Let $W$ be the set of elements of $V$ which are annihilated by a certain power of $z$. Equation \eqref{eq:ApoplusW} makes $z$ semisimple because $V_s$ and $W$ are $z$-stables.

By lemma \ref{lem:EndV_e}, any element of $[A,A]\cap T$ is nilpotent; but we just saw that it has no non zero nilpotent elements then $[A,A]\cap T=0$, so that 
\[
[\lG,\lG]\cap\mZ=0.
\]
This proves the first point. 

Now we consider  an ideal $J$ such that $[J,J]\subset\mZ$. Then $[J,J]=[J,J]\cap \mZ=0$. We looks at the abelian ideal $[\lG,J]$ of $\lG$. This is an ideal because $[[g,j],h]=-[[j,h],g]-[[h,g],j]$. By the lemma, the elements of $[\lG,J]$ are nilpotent and the associative algebra generated by $[G,J]$ is also nilpotent because $[\lG,J]$ is abelian.

The elements of $B$ are polynomials with respect to elements of $[\lG,J]$, then $AB\subset BA+B$ because $AB$ is made up with elements of the form $a(hj-jh)^n$ which itself is made up with elements $ah^kj^l$. By commutating $j^l$, we get 
\[
j^lah^k+\text{elements of } [\lG,J],
\]
but $J$ is an ideal and $j^l\in\ J$. By induction,
\begin{equation}
(AB)^k\subset B^kA+B^k.
\end{equation}
Since $B$ is nilpotent, $AB$ is a nilpotent left ideal. Then $AB=0$ which in turn implies $B=0$. In particular $[\lG,J]=0$, so that $J\subset\mZ$. But any abelian ideal in $\lG/\mZ$ is the canonical projection of an ideal $J$ of $\lG$ such that $[J,J]\in\mZ$. We conclude that $\lG/\mZ$ has no non zero abelian ideal.

\end{proof}

Now we are able to prove a third version of Lie's theorem:
\begin{theorem}[Lie]\label{tho:Lie_trois}
If $\lG$ is a solvable ideal, then any completely reducible $\lG$-module  is annihilated by $[\lG,\lG]$.
\end{theorem}

\begin{proof}
Let $V$ be such a $\lG$-module, $\rho$ the representation of $\lG$ on $V$ and $\mA=\rho(\lG)\subset\End(V)$. By assumption, $\lA$ is a solvable subalgebra of $\End(V)$; let $\mZ$ be the center of $\lA$. It is clear that $\lA/\mZ$ is solvable, so that it has no non zero abelian ideal. But the fact that $\lA/\mZ$ is solvable makes one of the $\dD^k(\lA/\mZ)$ an abelian ideal. The conclusion is that $\lA/\mZ=0$, or $\lA=\mZ$. Clearly this makes $[\lA,\lA]=0$.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Dynkin diagram}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    If $\alpha$ and $\beta$ are simple roots, then the angle $\theta_{\alpha,\beta}$ can only take the values $90\degree$, $120\degree$, $135\degree$ or $150\degree$.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

In order to draw the \defe{Dynkin diagram}{Dynkin!diagram} of a Lie algebra, one draws a circle for each simple root, and one joins the roots with $1$, $2$ or $3$ lines, following that the value of the angle is $120\degree$, $135\degree$ or $150\degree$. If the roots are orthogonal (angle $90\degree$), they are not connected. If the length of a root is maximal, the circle is left empty. If not, it is filled.

One easily determines the number of lines between two roots by the following proposition.
\begin{proposition}         \label{PropProdNbLignes}
    If $\alpha$ and $\beta$ are two simple roots with $(\alpha,\alpha)\leq(\beta,\beta)$, then
    \begin{equation}
        \frac{ (\alpha,\alpha) }{ (\beta,\beta) }=
    \begin{cases}
        1   &\text{if $\theta_{\alpha,\beta}=120\degree$}\\
        2   &\text{if $\theta_{\alpha,\beta}=135\degree$}\\
        3   &\text{if $\theta_{\alpha,\beta}=150\degree$}.
    \end{cases}
    \end{equation}
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

If $M$ is a weight of a representation, its \defe{Dynkin coefficients}{Dynkin!coefficient} are
\begin{equation}
    M_i=\frac{ 2(M,\alpha_i) }{ (\alpha_i,\alpha_i) },
\end{equation}
and we can compute the Dynkin coefficients from one weight to another by the simple formula
\begin{equation}        \label{EqCofDynMmoisAlpha}
    (M-\alpha_j)_i=M_i-A_{ij}.
\end{equation}
A weight is \defe{dominant}{weight!dominant}\index{dominant weight} if all its Dynkin coefficients are strictly positive.

\begin{proposition}\label{prop:trois_poids}
    Let $\lG$ be a nilpotent complex Lie algebra and $\rho$, a representation of $\lG$ on a finite dimensional vector space $V$. Then
    \begin{enumerate}
        \item $\forall \gamma\in\lG^*$, the space $V_{\gamma}$ is a $\lG$-submodule of $V$,
        \item if $\gamma$ is a weight, then there exists a nonzero vector $v\in V_{\gamma}$ such that $\forall x\in\lG$, $x\cdot v=\gamma(x)v$,
        \label{prop:trois_poids:deux}
        \item\label{ItemVSEZlhviii} $V=\bigoplus_{\gamma}V_{\gamma}$ where the sum is taken over the set of weight.
    \end{enumerate}
\end{proposition}

\begin{proof}
Since $\rho$ is a representation,
\[
\big(\rho(x)-\gamma(x)\big)\rho(y)=\rho(y)\big( \rho(x)-\gamma(x) \big)+\rho([x,y]).
\]
Now let us suppose that $\big(\rho(x)-\gamma(x)\big)^m\rho(y)$ is a sum of endomorphism of the form 
\[
\rho( (\ad x)^py )\rmg{x}^q
\]
with $p+q=m$. We just saw that it was true for $m=1$. Let us check for $m+1$:
\begin{equation}
\begin{split}
\rho(x)\rho( (\ad x)^py )\rmg{x}^q&=\rho( [x,(\ad x)^py] )\rmg{x}^q\\
&\quad+\rho((\ad x)^py)\rho(x)\rmg{x}^q.
\end{split}
\end{equation}
Then, since $\lG$ is nilpotent, the space $V_{\gamma}$ is a submodule of $V$ because for large enough $m$ and for all $y$, $\rmg{x}^m \rho(y)v=0$ if $v\in V_{\gamma}$.

Any nilpotent algebra is solvable, then from Lie theorem \ref{tho:Lie_trois}, the restrictions of $\rho(x)$ (with $x\in\lG$) to irreducible submodules commute. By Schur's lemma \ref{lem:Schur}, they are multiples of identity. But if all $\lG$ is the identity on an irreducible module, then the module has dimension one. In particular, \emph{any irreducible submodule of $V_{\gamma}$ has dimension one}\quext{Encore que soit pas bien clair pourquoi un tel module existerait... donc l'affiramation suivante ne me semble pas trop justifi\'ee}.

Then, in the weight space $V_{\gamma}$, there is a $v$ which fulfils  $\rho(x)v=\lambda(x)v$ for all $x\in\lG$. It is rather clear that it will only works for $\lambda=\gamma$. Our conclusion is that there exists a $v\in V_{\gamma}$ such that $\rho(x)v=\gamma(x)v$.

Now we consider $\gamma_1,\ldots,\gamma_k$, distinct weights. They are linear forms; then there exists a $x\in\lG$ such that $\gamma_1(x),\ldots,\gamma_k(x)$ are distinct numbers. In fact, the set $\{h\in\lH\tq \alpha_i(h)=\alpha_j(h)\text{ for a certain pair $(i,j)$}\}$ is a finite union of hyperplanes in $\lH$; then the complementary is non empty.


With this fact we can see that the sum $V_{\gamma_1}+\ldots+V_{\gamma_k}$ is direct. Indeed let $v\in V_{\gamma_i}\cap V_{\gamma_j}$; for the chosen $x\in\lG$ and for suitable $m$,
\begin{equation}
\big(  \rho(x)-\gamma_i(x) \big)^mv=\big(  \rho(x)-\gamma_j(x) \big)^mv=0
\end{equation}
which implies $\gamma_i(x)=\gamma_j(x)$ or $v=0$. In particular one has only a finitely many roots and we can suppose that our choice of $\gamma_i$ is complete.

For $a\in\eC$, we define $V_a$ as the set of elements in $V$ which are annihilated by some power of $\rho(x)-a$ with our famous $x$. By the first lines of the proof, $V_a$ is a $\lG$-submodule of $V$.

For the same reasons as before\quext{Celles que je n'ai pas bien comprises}, if $V_a\neq 0$, there exists a $v\in V_a$ and a weight $\gamma_i$ such that $\forall y\in\lG$, 
\[
\rho(y)v=\gamma_i(y)v.
\]
But as $v$ is annihilated by a power of $\big( \rho(x)-a \big)$, it is clear that $a=\gamma_i(x)$, and some theory of linear endomorphism\quext{th\'eorie que je ne connais pas trop} shows that $V$ is the sum of the $V_a$'s:
\begin{equation}
V=\sum_{i=1}^kV_{\gamma_i(x)}.
\end{equation}
It remains to be proved that $V_{\gamma_i(x)}\subset V_{\gamma_i}$. Let $y\in\lG$ and
\[
V_{i,a}=\{ v\in V_{\gamma_i(x)}\tq\exists n: (\rho(y)-a)^nv=0 \}.
\]
As usual\quext{et comme d'hab, l'argument que je ne saisit pas} if $V_{i,a}\neq 0$, there exists a $v\in V_{i,a}$ and a weight $\gamma_j$ such that $\rho(z)v=\gamma_j(z)v$ for any $z\in\lG$. Then $a=\gamma_j(y)=\gamma_i(y)$. But $V_{\gamma_i(x)}$ being the sum of the $V_{i,a}$'s, we have $V_{\gamma_i(x)}=V_{i,\gamma_i(y)}$ for any $y\in\lG$. This makes $V_{\gamma_i(x)\subset V_{\gamma_i}}$.

\end{proof}

From proposition \ref{prop:trois_poids}\ref{ItemVSEZlhviii}, an element $y\in\lG$ can be decomposed as
\begin{equation}\label{eq:decomp_racine}
    y=\sum_{\beta\in\Phi}y_{\beta}
\end{equation}
with $y_{\beta}\in\lG_{\beta}$.

\begin{theorem} \label{tho:Killing_Cartan}
    Let $\lG$ be a Lie algebra, $\lH$ a Cartan subalgebra of $\lG$ and $B$ the Killing\index{Killing!form} form of $\lG$. Then for all $x$, $y\in\lH$,
    \begin{equation}
    B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y)
    \end{equation}
    where $g_{\gamma}=\dim\lG\bgamma$.
\end{theorem}

\begin{proof}
    We are seeing $\lG$ as a $\lH$-module for the adjoint representation. In particular, proposition \ref{prop:trois_poids} makes $\lG$ a direct sum of the $\lH$-submodules $\lG_{\gamma}$. Then
    \begin{equation}
    B(x,y)=\tr({\ad x}^2)\\
            =\sum_{\gamma\in\Phi}\tr(\ad x|_{\gamma}^2)
    \end{equation}
    where $\ad x|_{\gamma}$ means the restriction of $\ad x$ to $\lG\bgamma$. It is clear that $\ad x|\bgamma-\gamma(x)$ is nilpotent, then $\ad x|\bgamma^2-\gamma(x)^2$ is also nilpotent because
    \[
    \ad x|_{\gamma}^2-\gamma(x)^2=(\ad x|\bgamma+\gamma(x))(\ad x|\bgamma-\gamma(x))
    \]
    and the fact that these two terms commute. The trace of a nilpotent endomorphism is zero, then $\tr(\ad x|_{\gamma}^2-\gamma(x)^2)=0$ or for all $x\in\lG$,
    \begin{equation}\label{eq:Bxx}
    B(x,x)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)^2.
    \end{equation}
    on the other hand, we know that a quadratic form determines only one bilinear form. Here the form \eqref{eq:Bxx} gives
    \[
    B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y).
    \]
\end{proof}

\begin{theorem}\label{tho:six_Cartan}
    If $\alpha,\beta$ are roots of a semisimple Lie algebra $\lG$ with respect to a Cartan subalgebra $\lH$, then
    \begin{enumerate}
        \item if $x_{\alpha}\neq0\in\lG_{\alpha}$ fulfils $[h,x_{\alpha}]=\alpha(h)x_{\alpha}$ for all $h\in\lH$, then $\forall y\in\lG_{-\alpha}$
        \[
        [x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha},
        \] 
        \item\label{ite:six_deux} $\alpha(h_{\alpha})$ is rational and positive. Moreover 
        \[
        \alpha(h_{\alpha})\sum_{\gamma\in\Phi}(\gamma_{\alpha}-\gamma^{\alpha})^2=4,
        \]
        \item $2\beta(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$,
        \label{ite:six_trois}
        \item the forms $0,\alpha,-\alpha$ are the only integer multiples of $\alpha$ which are roots,
        \label{ite:six_quatre}
        \item $\dim\lG_{\alpha}$=1,
        \label{ite:six_cinq}
        \item any $k$ which makes $\beta+k\alpha$ a root lie between $-\beta_{\alpha}$ and $\beta^{\alpha}$. In other words, $\beta+k\alpha\in\Phi$ is only true with $-\beta_{\alpha}\leq k\leq\beta^{\alpha}$.
        \label{ite:six_six}
    \end{enumerate}

\end{theorem}

\begin{proof}
The fact that $y\in\lG_{-\alpha}$ and that $x\in\lG_{\alpha}$ make $[x,y]\in\lG_0=\lH$. Now we consider $h\in\lH$ and the invariance formula \eqref{eq:Killing_invariant}. We find:
\begin{equation}
B(h,[x_{\alpha},y])=-B([x_{\alpha},h],y)\\
        =\alpha(h)B(x_{\alpha},y)\\
        =B(h,h_{\alpha})B(x_{\alpha},y)\\
        =B(h,B(x_{\alpha},y)h_{\alpha}).
\end{equation}
Since it is true for any $h\in\lH$ and $B$ is nondegenerate on $\lH$ we find the first point. In order to prove \ref{ite:six_deux}, we consider
\[
U=\bigoplus_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}\lG_{\beta+m\alpha}.
\]
By definition of $\alpha_{\beta}$ and $\alpha^{\beta}$, each term of the sum is a root space. If $z\in\lG_{\alpha}\oplus\lG_{-\alpha}$, then $U$ is stable under $\ad z$ because the terms in $\ad z U$ are of the form $[z,x_{\beta+m\alpha}]\in\lG_{\beta+m\alpha\pm\alpha}$. Note however that this $\ad z U$ is not \emph{equal} to $U$.

Let $x_{\alpha}\neq 0\in\lG_{\alpha}$. There exists a $y\in\lG_{-\alpha}$ such that $[x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha}$ (here we use semi-simplicity). By fitting the norm of $y$, we can choose it in order to get  $[x_{\alpha},y]=h_{\alpha}$, so that 
\[
\ad h_{\alpha}=[\ad x_{\alpha},\ad y].
\]
Now we look at the restriction of $\ad h_{\alpha}$ to $U$:
\begin{equation}
\tr(\ad h_{\alpha})=\tr(\ad x_{\alpha}\circ\ad y)-\tr(\ad y\circ\ad x_{\alpha})=0.
\end{equation}
Since $h_{\alpha}\in\lH=\lG_0$, we have $\dpt{\ad h_{\alpha}}{U}{U}$, so that the annihilation of the trace of $\ad h_{\alpha}$ can be particularised to 
\[
\tr(\ad h_{\alpha}|_U)=0.
\]
On the other hand, by definition $\ad h_{\alpha}-(\beta+m\alpha)(h_{\alpha})$ is nilpotent on $\lG_{\beta+m\alpha}$. Then it has a vanishing trace:
\[
\sum_m\tr( \ad h_{\alpha}-(\beta+m\alpha)h_{\alpha}  )=0.
\]
But we had yet seen that the term with $\ad h_{\alpha}$ is zero; then
\begin{equation}\label{eq:trace_U}
\sum_{-\beta_{\alpha}\leq m\leq \beta^{\alpha}} (\beta+m\alpha)h_{\alpha}\dim\lG_{\beta+m\alpha}=0.
\end{equation}
If we suppose that $\alpha(h_{\alpha})=0$ this gives $\beta(h_{\alpha})=0$. Since this conclusion is true for any root $\beta$, we find $B(h,h_{\alpha})=0$ for any $h\in\lH$. In other words, $\alpha(h)=0$ for any $h\in\lH$. This contradicts the assumption, so that we conclude $\alpha(h_{\alpha})\neq 0$.


Let $V=\lH+(x_{\alpha})+\sum_{m<0}\lG_{m\alpha}$ where $(x_{\alpha})$ is the one dimensional space spanned by $x_{\alpha}$. On the one hand,  from the definition of $x_{\alpha}$, $\ad x_{\alpha}\lH\subset(x_{\alpha})$ and $\ad x_{\alpha}\lG_{m\alpha}\subset\lG_{(m+1)\alpha}$. On the other hand, $y\in\lG_{-\alpha}$ is defined by the relation $[x_{\alpha},y]=h_{\alpha}$, then  $\ad y\lH\subset\lG_{-\alpha}\subset\sum_{m<0}\lG_{m\alpha}$, $\ad y(x_{\alpha})\subset\lG_0=\lH$ and $\ad y\sum_{m<0}\lG_{m\alpha}=\sum_{m<0}\lG_{(m-1)\alpha}$. All this make $V$ invariant under $\ad x_{\alpha}$ and $\ad y$. 

Since $\ad h_{\alpha}=[\ad x_{\alpha},\ad y]$, the trace of $\ad h_{\alpha}$ is zero so that the invariance of $V$ gives
\[
\tr(\ad h_{\alpha}|_V)=0.
\]
By the definition of $x_{\alpha}$ particularised to $h\to h_{\alpha}$, we have $\tr(\ad h_{\alpha}|_{(x_{\alpha})})=\alpha(h_{\alpha})$. By the definition of $\lG_0$, for any $x\in\lH$ and $v\in\lG_0$, $\ad x$ is nilpotent on $v$. Taking $h_{\alpha}$ as $x$, we see that 
$(\ad h_{\alpha})h$ don't contain ``$h$-component''. Then $\tr(\ad h_{\alpha}|_{\lH})=0$. Finally the operator $(\ad h_{\alpha}-m\alpha(h_{\alpha}))$ is nilpotent on $\lG_{m\alpha}$, so that $\tr(\ad h_{\alpha}|_{\lG_{m\alpha}})=\tr( m\alpha(\alpha)|_{\lG_{m\alpha}} )=m\alpha(h_{\alpha})\dim\lG_{m\alpha}$. All this gives
\begin{equation}
\alpha(h_{\alpha})\left( 1+\sum_{m<0}m\dim\lG_{m\alpha}  \right)=0.
\end{equation}
As we saw that $\alpha(h_{\alpha})\neq 0$, we conclude that $\dim\lG_{m\alpha}=0$ for $m<-1$ and $\dim\lG_{-\alpha}=1$. This proves \ref{ite:six_cinq}. 

This also prove \ref{ite:six_quatre} in the particular case of \emph{integer} multiples. It is rather simple to get relations such that $0_{\alpha}=1$, $0^{\alpha}=1$, $\alpha_{\alpha}=2$, $(-\alpha)_{\alpha}=0$, and it is easy to check \ref{ite:six_trois} in the cases $\beta=-\alpha,0,\alpha$. Now we turn our attention to the case in which $\beta$ is not an integer multiple of $\alpha$. By \ref{ite:six_quatre} applied to $\alpha\to\beta+m\alpha$, we have $\dim\lG_{\beta+m\alpha}=1$ whenever $-\beta_{\alpha}\leq m\leq\beta^{\alpha}$.

From equation \eqref{eq:trace_U}, $\sum_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}( \beta(h_{\alpha})+m\alpha(h_{\alpha}) )=0$, then
\begin{equation}
(\beta_{\alpha}+\beta^{\alpha}+1)\beta(h_{\alpha})=(\sum_m m)\alpha(h_{\alpha})\\
                        =\left(
                    \frac{\beta_{\alpha}(\beta_{\alpha}+1)}{2}-\frac{(\beta^{\alpha}-1)\lbha}{2}
                        \right)\alpha(h_{\alpha}).
\end{equation}
This gives \ref{ite:six_trois}. Now we consider the formula of theorem \ref{tho:Killing_Cartan} in the case $x=y=h_{\alpha}$ and we use the fact that $B(h,h_{\alpha})=\alpha(h)$ in the case $h=h_{\alpha}$:
\begin{equation}
B(h_{\alpha},h_{\alpha})=\alpha(h_{\alpha})\\
            =\sum_{\gamma\in\Phi}\dim\lG_{\gamma}\gamma(h_{\alpha})^2\\
            =\sum_{\gamma\in\Phi}\gamma(h_{\alpha})^2.
\end{equation}
Since $\beta(h_{\alpha})=\frac{1}{2}(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, we find \ref{ite:six_deux}. In order to prove \ref{ite:six_quatre}, we consider $\beta=c\alpha$ for a $c\in\eC$. By \ref{ite:six_trois}, $2c\alpha(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, so that $c$ is an half integer: $c=p/2$ with $p\in\eZ$. If $c$ is non zero, we can interchange $\alpha$ and $\beta$ and see that $\alpha=c^{-1}\beta$ implies $c^{-1}=q/2$ with $q\in\eZ$. It is clear the $pq=4$. But we had already discussed the case of integer multiples of $\alpha$, so that we can suppose that $p$ is odd. The only odd $p$ such that $pq=3$ with $q\in\eZ$ are $p=1,-1$, which are two excluded cases: they are $\alpha=\pm 2\beta$ which lies in the case of integer multiples.

It remains to prove \ref{ite:six_six}. By definition of $\beta^{\alpha}$, the form $\beta+(\beta^{\alpha}+1)\alpha$ is not a root. But it remains possible that $\beta+(\beta^{\alpha}+2)\alpha$ is. We suppose that $k_1,\ldots,k_p$ are the $p$ positive integers such that $\beta+k_i\alpha\in\Phi$. We pose
\[
W=\bigoplus_{i=1}^p\lG_{\beta+k_i\alpha}.
\]
As usual we see that $W$ is stable under $\ad x_{\alpha}$ and $\ad y$ (because $k_i\geq\beta^{\alpha}+2$). The trace of $\ad g_{\alpha}$ on $W$ is zero, thus
\begin{equation}
0=\sum_{i=1}^p(\beta+k_i\alpha)(h_{\alpha}).
\end{equation}
By \ref{ite:six_trois}, we find
\[
p(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})=2(k_1+\ldots+k_p)>p(\beta^{\alpha}+1). 
\]
This is not possible because it would gives $-\beta^{\alpha}-\beta_{\alpha}>2$.
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Strings of roots}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Let $\alpha,\beta$ be two roots with respect to $\lH$ and suppose $\beta\neq 0$. We denote by $\alpha^{\beta}$ the largest integer $m$ such that $\alpha+m\beta$ is a root and by $\alpha_{\beta}$ the one such that $\alpha-m\beta$ is a root. Let $x\in\lG_{\alpha}$; since the Killing form is nondegenerate, there exists a $y\in\lG$ such that $B(x,y)\neq 0$. Using the root space decomposition \eqref{eq:decomp_racine} for $y$ and corollary \ref{cor:Bxy_zero}, $B(x,y)=B(x,y_{-\alpha})$ . Then \label{pg:root_ss}
\[
\forall x\in\lG_{\alpha},\exists y\in\lG_{-\alpha}\textrm{ such that } B(x,y)\neq 0.
\]
In particular if $\alpha$ is a root, $-\alpha$ is also a root and the restriction of $B$ to $\lH\times\lH$ is nondegenerate because $\lH=\lG_0$. So
\[
\forall\mu\in\lH^*,\exists!h_{\mu}\in\lH\textrm{ such that } \forall h\in\lG, B(h,h_{\mu})=\mu(h).
\]
This is a general result about nondegenerate (here we use the semi-simplicity assumption) bilinear forms on a vector space. If $B(x,y)=B_{ij}x^iy^j$ and $a(x)=a_ix^i$, then a vector $v$ such that $B(x,v)=a(x)$ exists, is unique and is given by coordinates $v^k=B^{ki}a_i$ where the matrix $(B^{ij})$ is the inverse of $(B_{ij})$.

We will sometimes use the following notation if $\alpha$ and $\beta$ are roots:
\[
(\alpha,\beta)=B( h_{\alpha},\hbb),\qquad |\alpha|^2=(\alpha,\alpha).
\]

By proposition \ref{tho:six_Cartan}, the roots come by pairs $(\alpha,-\alpha)$. For each of them, we choose $x_{\alpha}\in\lG_{\alpha}$. Our choice of $x_{-\alpha}$ is made as following. From discussion at page \pageref{pg:root_ss} we can find a $x_{-\alpha}\in\lG_{-\alpha}$ such that $B(x_{-\alpha},x_{\alpha})=1$. Note that this choice is unambigous: if we had chosen first $x_{-\alpha}\in\lG_{-\alpha}$, this construction would have given the same $x_{\alpha}$ than our starting point. Note also that $h_{-\alpha}=-h_{\alpha}$. These $ x_{\alpha}$ fulfil $[ x_{\alpha},\xbma]= h_{\alpha}$.

\begin{probleme}
    Here the notation \( \Delta\) does not follow our convention of subsection \ref{SubsecNotationRootsDel}.
\end{probleme}

Let $\Delta$ be the set of non zero roots. We define an antisymmetric map $\dpt{c}{\Delta\times \Delta}{\eC}$ as following. If $\alpha,\beta\in S$ are such that $\alpha+\beta\notin\Delta$, we pose $c(\alpha,\beta)=0$. If $\alpha+\beta\in\Delta$, 
\begin{equation}
[x_{\alpha},x_{\beta}]=c(\alpha,\beta)x_{\alpha+\beta}.
\end{equation}
It is easy to see that $c(\alpha,\beta)=-c(\beta,\alpha)$.

\begin{proposition}
If $\alpha,\beta,\alpha+\beta\in\Delta$, then

\begin{enumerate}
\item 
\[
c(-\alpha,\alpha+\beta)=c(\alpha+\beta,-\beta)=c(-\beta,-\alpha),
\]
\label{enuai}
\item\label{enuaii} If $\alpha,\beta,\gamma,\delta\in\Delta$ and $\alpha+\beta+\gamma+\delta=0$ wile $\delta$ is neither $-\alpha$, nor $-\beta$ nor $-\gamma$, then
\begin{equation}\label{eq:enuaii}
c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta)=0,
\end{equation}

\item if $\beta\neq\alpha\neq -\beta$, then
\[
c(\alpha,\beta)+c(-\alpha,-\beta)=c(\alpha,-\beta)c(-\alpha,\beta)-B(h_{\alpha},h_{\beta}),
\]
\label{enuaiii}
\item\label{enuaiv} if $\alpha+\beta\neq0$ then
\begin{equation}\label{eq:enuaiv}
2c(\alpha,\beta)c(-\alpha,-\beta)=\lbha(1+\lbba)\alpha(h_{\alpha}).
\end{equation}

\end{enumerate}
\label{prop:enua}
\end{proposition}

\begin{proof}
From our choice of $ x_{\alpha}$, we find that $B(\xbb,\xbmb)=B(\xbma, x_{\alpha})=B(x_{\alpha+\beta},\xbmab)=1$, but
\begin{equation}
\begin{split}
  B\big(c(-\alpha,\alpha+\beta)\xbb,\xbmb\big)&=B\big(\xbma,c(\alpha+\beta,-\beta) x_{\alpha}\big)\\
                        &=B\big(x_{\alpha+\beta},c(-\beta,-\alpha)\xbmamb \big).
\end{split}                            
\end{equation}
This proves \ref{enuai}. In order to prove \ref{enuaii}, suppose that
\begin{equation}\label{eq:amontrer}
c(\alpha,\beta)c(\gamma,\delta)=B\Big(  \big[[ x_{\alpha},\xbb],\xbg\big] ,\xbd  \Big)
\end{equation}
Then the Jacobi identity gives the result:
\begin{equation}
\begin{split}
0&=B\Big( \big[[ x_{\alpha},\xbb],\xbg\big],\xbd \Big)+B\Big(\big[[\xbb,\xbg], x_{\alpha}\big],\xbd\Big)+B\Big(\big[[\xbg, x_{\alpha}],\xbb\big] ,\xbd\Big)\\
&=c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta),
\end{split}
\end{equation}
Here, we used the hypothesis $-\gamma\neq\delta\neq -\beta$ by supposing that \eqref{eq:amontrer} still hold after permutation of $\alpha,\beta,\gamma$.
Now we show the \eqref{eq:amontrer} is true. The assumptions imply  $\alpha+\beta=-(\gamma+\delta)\neq 0$, then
\begin{equation}
\begin{split}
B\big(\big[[ x_{\alpha},\xbb],\xbg \big],\xbd\big)&=B\big( [ x_{\alpha},\xbb],[\xbg,\xbd]  \big)\\
&=c(\alpha,\beta)c(\gamma,\delta)B(x_{\alpha+\beta},x_{\gamma+\delta})\\
&=c(\alpha,\beta)c(\gamma,\delta).
\end{split}
\end{equation}
Now we turn our attention to \ref{enuaiii}. If $\alpha$ and $\beta$ fulfil the condition $\beta\neq\alpha\neq-\beta$, we can apply \ref{enuaii} on the quadruple $(\alpha,\beta,-\alpha,-\beta)$ to get $c(\alpha,\beta)c(-\alpha,-\beta)=
-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]   \big)$. 
If we replace $\beta$ by $-\beta$ and if we make the difference between the two expressions,
\begin{equation}
\begin{split}
c(\alpha,\beta)c(-\alpha,-\beta)&=-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]    \big)+B\big(   [ x_{\alpha},\xbmb],[\xbma,\xbb]    \big)\\
&=B\big( [ x_{\alpha},[\xbmb,\xbma]],\xbb \big)-B\big(
[\xbma,[ x_{\alpha},\xbmb]],\xbb \big)\\
&=-B\big( [\xbma, x_{\alpha}],[\xbmb,\xbb]  \big) \\
&=-B(h_{\alpha},h_{\beta}).
\end{split}
\end{equation}

In order to prove \ref{enuaiv}, we consider $\alpha+\beta\neq0$ and we pose
\[
d(\alpha,\beta)=c(\alpha,\beta)c(-\alpha,-\beta)-\frac{1}{2}\lbha(1+\lbba)\alpha(h_{\alpha}).
\]
Our aim is to prove that it is zero. We will do it by induction on $\lbha$. First $\lbha=0$ means that $\beta+\alpha=0$, so that $c(\alpha,\beta)=0$ and $d(\alpha,\beta)=0$. Now we suppose that $\lbha>0$ and that \ref{enuaiv} is yet checked for lower cases. Note that $\beta+\alpha\in \Delta$ and $(\beta+\alpha)+\alpha\neq 0$ because $-2\alpha$ is not a root. Then $\beta=2\alpha$ is not possible. From the fact that $(\beta+\alpha)^{\alpha}=\lbha-1$, we conclude $d(\alpha,\beta+\alpha)=0$. Then
\[
c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)=c(\alpha,-\alpha-\beta)c(-\alpha,\alpha+\beta)-B(h_{\alpha},h_{\alpha+\beta}).
\]
On the other hand, \ref{enuai} and the antisymmetry of $c$ give
\begin{subequations}
\begin{align}
c(-\alpha,\alpha+\beta)=c(-\beta,-\alpha)=-c(-\alpha,-\beta)\\
\intertext{and}
c(\alpha,-\alpha-\beta)=c(\beta,\alpha)=-c(\alpha,\beta)
\end{align}
\end{subequations}
With all this
\begin{equation}
\begin{split}
d(\alpha,\beta+\alpha)&=c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)-\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})\\
&=c(\alpha,\beta)c(-\alpha,-\beta)-k(\alpha,\beta)
\end{split}
\end{equation}
where $k(\alpha,\beta)=B(h_{\alpha},h_{\alpha+\beta})+\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})$. But $h_{\alpha+\beta}$ is definied in order to have $B(h,h_{\alpha+\beta})=(\alpha+\beta)(h)$ for any $h\in\lH$. Then using $2\beta(h_{\alpha})=(\lbba-\lbha)\alpha(h_{\alpha})$, we find $k(\alpha,\beta)=\frac{1}{2}\alpha(h_{\alpha})\lbha(1+\lbba)$.
\end{proof}

\begin{proposition}\label{prop:lHeR}\index{real!form!of a vector space}
Let 
\begin{equation}
\lHeR=\sum_{\alpha\in\Delta}\eR h_{\alpha}.
\end{equation}
Then :
\begin{enumerate}
    \item any root is real on $\lHeR$,
    \item the Killing form is real and strictly positive definite on $\lHeR$,
    \item $\lH=\lHeR\oplus i\lHeR$.
\end{enumerate}
\end{proposition}

The last item shows that $\lHeR$ is a real form of $\lH$. Remark also that $\lHeR$ can also be written as
\[
\lHeR=\{h\in\lH\tq \alpha(h)\in\eR\,\forall\alpha\in\Phi \}.
\]
\begin{proof}
Let $\beta\in\Delta$; we looks at $\beta( h_{\alpha})$. From \ref{ite:six_deux} of theorem \ref{tho:six_Cartan}, we know that $\alpha( h_{\alpha})$ is real and positive, and \ref{ite:six_trois} makes $\beta( h_{\alpha})$ real. From the formula $B( h_{\alpha},\hbb)=\sum_{\gamma\in\Delta}\gamma( h_{\alpha})\gamma(\hbb)$, the Killing form is real and positive definite on $\lHeR\times\lHeR$. If $B(h,h)=0$ for a certain $h\in\lHeR$, we find $\alpha(h)=0$ for all $\alpha\in\Delta$. Then any $x=x^{\alpha} x_{\alpha}\in\lG$ commutes with $h$ because
\[
[h,x]=\sum_{\alpha\in\Phi}a^{\alpha}(\ad h) x_{\alpha}=\sum_{\alpha}a^{\alpha}\alpha(h)=0.
\]
So $h$ is in the center of $\lG$ and so $h=0$ be cause $\lG$ is semisimple. Thus the Killing form is strictly positive definite on $\lHeR\times\lHeR$.

Now we are going to show that $\lH=\lHeR\oplus i\lHeR$. If $h\in\lHeR\cap i\lHeR$, it can be written as $h=ih'$ with $h,h'\in\lHeR$. Then
\[
0<B(h,h)=B(ih',ih')=-B(h',h')<0,
\]
so that $h=0$ because $B$ is nondegenerate. This shows that $\lHeR\cap i\lHeR=0$. It is clear that $\sum_{\alpha\in\Delta}\eC h_{\alpha}\subset\lH$; thus it remains to be proved that $\lH\subset\sum_{\alpha\in\Delta}\eC h_{\alpha}$. It it is not, we can build a linear function $\dpt{\lambda}{\lH}{\eC}$ which is not identically zero but which is zero on the subspace $\sum_{\alpha\in\Delta}\eC h_{\alpha}$. Then there exists (only one) $h_{\lambda}\in\lH$ such that $B(h,h_{\lambda})=\lambda(h)$ for every $h\in\lH$. In particular, $\alpha(h_{\lambda})=0$ for every $\alpha\in\Delta$ because $\alpha(h_{\lambda})=B( h_{\alpha},h_{\lambda})=\lambda( h_{\alpha})$. This implies that $h_{\lambda}=0$, so that $\lambda\equiv 0$.

\end{proof}

One interest in the third point of this proposition is that we are now able to see $\Delta$ as a subset of $\lHeR^*$ because the definition of $\alpha\in\Delta$ on $\lHeR$ only is sufficient to define $\alpha$ on the whole $\lH$. 

If $\{e_i\}$ is a basis of a vector space $V$, we say that $x=x^ie_i>y=y^ie_i$ if $x-y=a^ie_i$ and the first non zero $a^i$ is positive. This is the \defe{lexicographic order}{lexicographic ordering} on $V$. It is clear that it doesn't works on a complex vector space (because in this case we should first define $a^i>0$), but we can anyway get an order on $\Delta$ by seeing it as a subset of $\lHeR$.

\begin{lemma}
    If $\alpha-\beta$ are simple roots with $\alpha\neq\beta$, then $\beta-\alpha$ is not a root and $B( h_{\alpha},\hbb)\leq 0$.
\end{lemma}

\begin{proof}
    Define $\gamma=\beta-\alpha\in\Delta$ (and not $\Phi$ because $\alpha\neq\beta$). If $\gamma>0$, the fact that $\beta=\gamma+\alpha$ contradict the simplicity of $\beta$ while if $\gamma<0$, in the same way $\alpha=\beta-\alpha$ contradict the simplicity of $\alpha$.

    Since $\beta-\alpha$ is not a root, $\lbba=0$ and $\lbha\geq 0$ thus formula $2\beta( h_{\alpha})=(\lbba-\lbha)\alpha( h_{\alpha})$ gives
    \begin{equation}
        2B( h_{\alpha},\hbb)=\underbrace{(\lbba-\lbha)}_{\leq 0}B( h_{\alpha}, h_{\alpha}).
    \end{equation}
    Now proposition \ref{prop:lHeR} gives the result.
\end{proof}

\begin{lemma}
    The simple roots are linearly independent.
\end{lemma}

\begin{proof}
    In the definition of a simple root, we need an order notion on $\Delta$ which is then seen as a subset of $\lHeR$. But the roots are real thereon. Then the right notion of ``independence''{} for the simple root is the independence with respect to \emph{real} combinations.

    If one has a combination $c^i\alpha_i=0$ (sum over $i$) with at least one non zero among the $c^i$'s  by putting the negative $c^i$'s at right, one can write
    \[
        a^i\alpha_i=b^j\alpha_j
    \]
    with $a^i,b^j\geq 0$. Let us consider $\gamma=a^i\alpha_i$ and $h_{\gamma}$. For every $h\in\lH$, we have
    \[
        B(h,h\gamma)=\gamma(h)=a^i\alpha_i(h_{\gamma}).
    \]
    but $h_{\gamma}=a^jh_{\alpha}$, then
    \begin{equation}
        B(h_{\gamma},h\bgamma)=a^ia^j\alpha_i(h_{\alpha_j})
                    =a^ia^jB(h_{\alpha_i},h_{\alpha_i}).
    \end{equation}
    Since the $\alpha_i$ are all simple roots, the right hand side is negative, but proposition \ref{prop:lHeR} makes the left hand side positive. Thus $\gamma=0$.
\end{proof}

\begin{theorem}
    If $\{\alpha_1,\ldots,\alpha_r\}$ is the set of all the simple roots, then $\dim\lHeR=r$ and every $\beta\in\Phi$ can be decomposed as
    \[
        \beta=\sum_{i=1}^rn_i\alpha_i
    \]
    where the $n_i$ are integers either all positive either all negative.
\end{theorem}

\begin{proof}
    Let $\beta$ be a non simple positive root. Then it can be decomposed as $\beta=\gamma+\delta$ with $\gamma,\delta>0$.We can also separately decompose $\gamma$ and $\delta$ and continue so until we are left with simple roots. We have to see why the process stops. Since there are only a finite number of positive root, if the process does not stop, then the decomposition of (at least) one of the positive roots $\gamma$ contains $\gamma$ itself. So we have a situation $\gamma=\gamma+\alpha$ for a certain positive $\alpha$. This contradict the notion of order.

    In particular $\Span_{\eN}\{\alpha_i\}=\{\textrm{positive roots}\}$. Thus it is clear that
    \[
        \Span_{\eR}\{\alpha_i\}=\Phi.
    \]
\end{proof}


\begin{theorem}\label{tho:Phi_base}
The Cartan algebra of a complex semisimple Lie algebra is abelian and the dual is spanned by the roots: \( \Span\Phi=\lH^*\).
\end{theorem}

\begin{proof}
Let $\alpha$ be a non zero root; from the point \ref{prop:trois_poids:deux} of proposition \ref{prop:trois_poids}, there exists a $v\in\lG_{\alpha}$ such that for any $x\in\lH$, $[x,v]=\alpha(x)v$. Since $\dim\lG_{\alpha}=1$ it is in fact true for any $v\in\lG_{\alpha}$. In particular $\forall v\in\lG_{\alpha}$ and $h\in\lH$, we have $[h,x]=\alpha(h)x$.

Let $\lN\subset\lH$ be the set of elements which are annihilated by all the roots:
\begin{equation}
    \lN=\{ H\in\lH\tq\alpha(H)=0\,\forall \alpha\in\Phi \}.
\end{equation}
First remark that 
\begin{equation}\label{eq:GNz}
[\lG_{\alpha},\lN]=0
\end{equation}
because for $x\in\lG_{\alpha}$ and $h\in\lN\subset\lH$, we have $[h,x]=\alpha(h)x=0$. An other property of $\lN$ is
\begin{equation}\label{eq:HHN}
[\lH,\lH]\subset\lN.
\end{equation}
Indeed consider a root $\alpha$ and $x\in\lG_{\alpha}$. We have
\begin{equation}
\begin{split}
-\alpha([h,h'])x&=[x,[h,h']]
=[h,[h',x]]+[h',[x,h]]
=\alpha(h)[h',x]+\alpha(h')[x,h]\\
&=\alpha(h)\alpha(h')-\alpha(h')\alpha(h)
=0.
\end{split}
\end{equation}
If $x\in\lG$ is decomposed as $x=\sum_{\alpha\in\Phi}x_{\alpha}$ and if $n\in\lN$, then
\[
[x,n]=\sum_{\alpha}[x_{\alpha},n]=\sum_{\alpha}\alpha(n)x_{\alpha}=0.
\]
In particular, $\lN$ is an ideal\quext{\c Ca me semble quand m\^eme fort de prouver que c'est le centralisateur pour dire que c'est un id\'eal. D'autant plus que je pourais directement dire que $\lN$ est centralisateur dans un semisimple et donc nulle.}. Moreover, the fact that $\lN\subset\lH$ makes $\lN$ a \emph{nilpotent} ideal in the semisimple Lie algebra $\lG$. Then $\lN=0$. Equation \eqref{eq:GNz} makes $\lH$ abelian while equation \eqref{eq:HHN} says that no element of $\lH$ is anihilated by all the roots. This implies that $\Span\Phi=\lH^*$. To see it more precisely, if $\Phi$ don't span a certain (dual) basis element $e_i^*$ of $\lH^*$, then a basis of $\Span\Phi$ is at most $\{e_j\}_{j\neq i}$. Then it is clear that $\alpha(e_i)=0$ for any root $\alpha$.
\end{proof}


The following important result is the fact that a complex semisimple Lie algebra is determined by its root system.
\begin{theorem}
Let $\lG$ and $\lG'$ be two semisimple complex Lie algebras; $\lH$ and $\lH'$, Cartan subalgebras. We suppose that we have a bijection $\Phi\to\Phi'$, $\alpha\to\alpha'$ which preserve the root system:

\begin{itemize}
\item $\alpha'+\beta'=0$ if and only if $\alpha+\beta=0$,
\item $\alpha'+\beta'$ is not a root if and only if $\alpha+\beta$ is also not a root,
\item $(\alpha+\beta)'=\alpha'+\beta'$ whenever $\alpha+\beta$ is a root.
\end{itemize}
Then we have a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$ such that $\eta(\lH)=\lH'$ and $\alpha'\circ\eta|_{\lH}=\alpha$.
\end{theorem}

\begin{proof}
From the assumptions, $\lbha=(\beta')^{\alpha'}$ and $\lbba=(\beta')_{\alpha'}$ and the point \ref{ite:six_deux} of theorem \ref{tho:six_Cartan} makes $\alpha'(h_{\alpha'})=\alpha(h_{\alpha})$. The fourth point of the same theorem then gives 
\begin{equation}\label{eq:beta_h_beta}
\beta'(h_{}\alpha')=\beta(h_{\alpha}).
\end{equation}
Now we choose a maximally linearly independent set $(\alpha_1,\ldots,\alpha_R)$ of roots of $\lG$. Because of theorem \ref{tho:Phi_base}, this is a basis of $\lH^*$. For notational convenience, we put $h_r=h_{\alpha_r}$ and naturally, $h'_r=h_{\alpha'_r}$. It is easy to see that the set of $h_r$ is a basis of $\lH$. Indeed if $a^rh_r=0$ (with sum over $r$), then $B(h,a^r,h_r)=a^r\alpha_r(h)=0$ which implies that $a^r\alpha_r|_{\lH}=0$ but it is impossible because the $\alpha_r$ are free in $\lH^*$.
\begin{equation*}
\begin{split}
\{ \alpha_1,\ldots,\alpha_r \}&\textrm{ is a basis of $\lH^*$},\\
\{ h_1,\ldots,h_r \}&\textrm{ is a basis of $\lH$}.
\end{split}  
\end{equation*}
Then the matrix $(A_{ij})=\alpha_i(h_j)$ has non zero determinant. Since $\alpha'_i(h_j')=\alpha_i(h_j)$, the set $\{\alpha'_1,\ldots,\alpha'_r\}$ is free and $\{h'_1,\ldots,h'_r\}$ is a basis of $\lH'$.
\begin{equation*}
\begin{split}
\{ \alpha'_1,\ldots,\alpha'_r \}&\textrm{ is a basis of ${\lH'}^*$},\\
\{ h'_1,\ldots,h'_r \}&\textrm{ is a basis of $\lH'$}.
\end{split}  
\end{equation*}
Then can define an isomorphism $\dpt{\etalH}{\lH}{\lH'}$ by $\etalH(h_i)=h'_i$. If $x\in\lH$ is decomposed as $x=a^rh_r$, from equation \eqref{eq:beta_h_beta} we have $(\alpha'_i\circ\etalH)(a^rh_r)=a^r\alpha'(h'_r)=\alpha_i(h_r)$. Then
\[
\alpha'_i\circ\etalH=\alpha_i.
\]

Let $\alpha\in\Phi$; we can write $\alpha=c_i\alpha_i$ and $\alpha'=c'_i\alpha'_i$ (with a sum over $i$). We have
\begin{equation}
c_i\alpha_i(h_k)=\alpha(h_j)
=\alpha'(h_j)
=c'_i\alpha_i(h_j).
\end{equation}
As the determinant of $(\alpha_i(h_j))$ is non zero, this implies $c_i=c'_i$, so that 
\begin{equation}
\alpha'\circ\etalH=\alpha
\end{equation}
because $\alpha'\circ\etalH=c'_i(\alpha'_i\circ\etalH)=c_i\alpha_i=\alpha$. Now we ``just''{} have to extend $\etalH$ into a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$. As before for each $\alpha\in \Delta$ we choose $ x_{\alpha}\in\lG_{\alpha}$ such that $B( x_{\alpha},\xbma)=-1$ and $[\xbma, x_{\alpha}]= h_{\alpha}$. We naturally do the same for $x_{\alpha'}\in\lG'_{\alpha'}$. We also consider the function $c$ as before: $[ x_{\alpha},\xbb]=c(\alpha,\beta)x_{\alpha+\beta}$. Since $\lH=\lG_0$, these $ x_{\alpha}$ form a basis of $\lG\ominus\lH$ and $\eta$ can be defined by the date of $\eta( x_{\alpha})$. We set $\eta( x_{\alpha})=a_{\alpha}x_{\alpha'}$ (without sum).

The condition $\eta\big(  [ x_{\alpha},\xbb]=[\eta( x_{\alpha}),\eta(\xbb)]  \big)$ gives
\begin{subequations}
\begin{align}
\label{eq:ca_caa_a}
c(\alpha,\beta)a_{\alpha+\beta}&=c(\alpha',\beta')\aba\abb&\quad\text{if $\alpha+\beta\neq 0$}\\
\intertext{and}  
\label{eq:ca_caa_b}
\aba\abma &=1                        &\quad\forall\alpha\in\Phi.
\end{align}
\end{subequations}
These two conditions are necessary and also sufficient. Indeed there are three cases of $[x,y]$ to check: $x$, $y\in\lH$, one of these two is out of $\lH$ or $x,y$ are booth out of $\lH$. In the third case, using \eqref{eq:ca_caa_a},
\begin{equation}
\eta([ x_{\alpha},\xbb])=c(\alpha,\beta)a_{\alpha+\beta}x_{\alpha'+\beta'}
            =x(\alpha',\beta')\aba\abb x_{\alpha'+\beta'}
            =\aba\abb[x_{\alpha'},\abbp]
            =[\eta( x_{\alpha}),\eta(\xbb)].
\end{equation}
If $x$, $y\in\lH$, then from theorem \ref{tho:Phi_base}, $\eta([x,y])=0=[\eta(x),\eta(y)]$. Using the fact that $[h, x_{\alpha}]=\alpha(h) x_{\alpha}$, we find the third case:
\begin{equation}
\eta\big(  [\hbb, x_{\alpha}]  \big)=\eta\big(  \alpha( h_{\alpha}) x_{\alpha}  \big)
                =\eta\big(  \alpha'(h_{\beta'}) x_{\alpha}  \big)
                =\aba[h_{\beta'},x_{\alpha'}]
                =[\eta(h_{\beta}),\eta( x_{\alpha})].
\end{equation}
Now we are going to find some $\aba\in\eC$ such that
\begin{itemize}
\item $\aba\abma=1$ for any $\alpha$,
\item $c(\alpha,\beta)a_{\alpha+\beta}=c(\alpha',\beta')\aba\abb$ if $\alpha+\beta\neq 0$.  
\end{itemize}
We consider the lexicagraphic order \index{lexicographic ordering} on $\Phi$: this is the order on $\Phi$ seen as a subset of $\lHeR$ on which we put the lexicagraphic order. For a root $\alpha>0$, we will fix the coefficient $\aba$ by an induction with respect to the order and put $\abma=\aba^{-1}$. Let us consider $\rho>0$ and suppose that $\aba$ is already defined for $-\rho<\alpha<\rho$ in such a manner that $\aba\abma=1$ and $c(\alpha,\beta)\abab=c(\alpha',\beta')\aba\abb$ for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are stricly between $-\rho$ and $\rho$. We have to define $\abr$ in such a way that if $\abmr=\abr^{-1}$, the second condition holds for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are no zero roots between $-\rho$ and $\rho$.

If such a pair $(\alpha,\beta)$ doesn't exist, there are no problem to put $\abr=\abmr=1$. Let us suppose that such a pair exists: $\alpha+\beta=\rho$. Then $\lbha\neq 0$ and the point \ref{enuaiii} of proposition \ref{prop:enua} shows that $c(\alpha,\beta)\neq 0$; in the same way, $(\beta')^{\alpha'}=\lbha\neq 0$ implies $c(\alpha',\beta')\neq 0$. We define
\begin{subequations}\label{eq:def_abr}
\begin{align}
\abr&=c(\alpha,\beta)^{-1} c(\alpha',\beta')\aba\abb,\\
\abmr&=\abr^{-1}.
\end{align}   
\end{subequations}


Since the value of the right hand side of \eqref{eq:enuaiv} doesn't change under $\alpha\to\alpha'$ and $\beta\to\beta'$, it gives  $c(\alpha,\beta)c(-\alpha,-\beta)=c(\alpha',\beta')c(-\alpha',-\beta')$ and thus
\begin{equation}
\begin{split}
c(-\alpha,-\beta)\abmr&=c(-\alpha,-\beta)c(\alpha,\beta)c(\alpha',\beta')^{-1}\abma\abmb\\
&=c(\alpha',\beta')c(-\alpha',-\beta')c(\alpha',\beta')^{-1}\aba\abmb\\
&=c(-\alpha',-\beta')\abma\abmb.
\end{split}
\end{equation}
Thus the definition \eqref{eq:def_abr} fulfils the requirements for the pair $(\alpha,\beta)$. It should be shown whether that works as well with another pair $(\gamma,\delta)$ such that $-\rho\leq\gamma,\delta\leq\rho$ and $\gamma+\delta=\rho$. If this second pair is really different that $(\alpha,\beta)$, then $\delta$ is neither $\alpha$ nor $\beta$; it is allso clear that $\delta$ is not $-\gamma$. Then formula \eqref{eq:enuaii}  works with the quadruple $(-\alpha,-\beta,\gamma,\delta)$:
\begin{equation}\label{eq:c_un}
c(-\alpha,-\beta)c(\gamma,\delta)+c(-\beta,\gamma)c(-\alpha,\delta)+c(\gamma,-\alpha)c(-\beta,\delta)=0.
\end{equation}
If $\alpha<0$, the assumption $\alpha+\beta=\rho$ makes $\beta>\rho$, which is in contradiction with $-\rho\leq\beta\leq\rho$. Then $\alpha,\beta,\gamma,\delta>0$ and moreover, the difference of any two of them is strictly between $-\rho$ and $\rho$. Since $\delta-\alpha=-(\gamma-\beta)$, if $\gamma-\beta$ is a root, $\delta-\alpha$ is also a root and the induction hypothesis gives
\begin{subequations}\label{eq:c_deux_un}
\begin{align}
c(\gamma,-\beta)a_{\gamma-\beta}&=c(\gamma',-\beta')\abg\abmb,  \label{eq:c_deux_un_a}\\
c(-\alpha,\delta)a_{-\alpha+\delta}&=c(-\alpha',\delta')\abma\abd.\label{eq:c_deux_un_b}
\end{align}
\end{subequations}
If we take for the convention $a_{\mu}=1$ whenever $\mu$ is not a root, these relations still hold if $\gamma-\beta$ is not a root. In the same way,
\begin{subequations}\label{eq:c_deux_deux}
\begin{align}
c(\gamma,-\alpha)a_{\gamma-\alpha}&=c(\gamma',-\alpha')\abma\abg,\label{eq:c_deux_deux_a}\\
c(-\beta,\delta)a_{-\beta+\delta}&=c(-\beta',\delta')\abmb\abd.\label{eq:c_deux_deux_b}
\end{align}
\end{subequations}
As $\delta-\alpha=-(\gamma-\beta)$, we have $a_{\delta-\alpha}a_{\gamma-\beta}=1$ and in the same way, $a_{\gamma-\alpha}a_{\delta-\beta}=1$. Taking it into account and multiplicating \eqref{eq:c_deux_un_a} by \eqref{eq:c_deux_un_b} and \eqref{eq:c_deux_deux_a} by \eqref{eq:c_deux_deux_a}, we find:
\begin{subequations}\label{eq:c_deux_trois}
\begin{align}
c(-\beta,\gamma)c(-\alpha,\delta)&=c(-\beta',\gamma')c(-\alpha',\delta')\abma\abmb\abg\abd\label{eq:c_deux_trois_a}\\
c(\gamma,-\alpha)c(-\beta,\delta)&=c(\gamma',-\alpha')c(-\beta',\delta')\abma\abmb\abg\abd.
\end{align}
\end{subequations}
We can use it to rewrite equation \eqref{eq:c_un}. After multiplication by $\aba\abb\abmg\abmd$,
\begin{equation}
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd+c(-\beta',\gamma')c(-\alpha',\delta')+c(\gamma',-\alpha')c(-\beta',\delta')=0.
\end{equation}
But equation \eqref{eq:c_un} is also true for $(\alpha',\beta',\gamma',\delta')$ instead of $(\alpha,\beta,\gamma,\delta)$, so that the last two terms can be replaced by only one term to give
\[
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd-c(-\alpha',-\beta')c(\gamma',\delta')=0.
\]
Since the pair $(\alpha,\beta)$ fulfils $c(-\alpha,-\beta)a_{-\alpha-\beta}=c(-\alpha',-\beta')\abma\abmb$, using $\alpha+\beta=\gamma+\delta$, we find
\[
c(\gamma,\delta)a_{\gamma+\delta}=c(\gamma',\delta')\abg\abd.
\]

\end{proof}


\begin{corollary}
The elements $ x_{\alpha}\in\lG_{\alpha}$ can be chosen in order to satisfy

\begin{itemize}
\item $B( x_{\alpha},\xbma)=1,$
\item $[ x_{\alpha},\xbma]= h_{\alpha}$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}

\end{corollary}

These vectors $ x_{\alpha}\in\lG_{\alpha}$ are called \defe{root vectors}{root!vectors}.

\begin{proof}
We consider the isomorphism $\alpha\to\alpha$ from $\Phi$ to $\Phi$; by the theorem this induces an isomorphism $\dpt{\eta}{\lG}{\lG}$ given by some constants $c_{\alpha}$:
\[
\eta( x_{\alpha})=c_{-\alpha}\xbma
\]
without sum on $\alpha$, because of course $\eta( x_{\alpha})\in\lG_{-\alpha}$. We choose $a x_{\alpha}\in\eC$ in such a way that
\begin{subequations}
\begin{align}
a_{\alpha}^2&=-c_{-\alpha}\\
a_{\alpha} a_{-\alpha}&=1,
\end{align} 
\end{subequations} 
and then we put $y_{\alpha}=a_{\alpha} x_{\alpha}$. It is immediate that $B(y_{\alpha},y_{-\alpha})=1$, thus the redefinition $ x_{\alpha}\to y_{\alpha}$ doesn't change the obtained relations. Acting on $y_{\alpha}$, the isomorphism $\eta$ gives
\begin{equation}
\eta(y_{\alpha})=a_{\alpha} c_{-\alpha}\xbma
=-a_{-\alpha}\xbma
=-y_{-\alpha}.
\end{equation}

If $\alpha,\beta,\alpha+\beta\in\Delta$, we naturally define $c'(\alpha,\beta)$ by
\[
[y_{\alpha},y_{\beta}]=c'(\alpha,\beta)y_{\alpha,\beta}.
\]
Using the fact that $\eta$ is a Lie algebra automorphism of $\lG$ we have:
\begin{equation}
-c'(\alpha,\beta)y_{-(\alpha+\beta)}=\eta\big(  c'(\alpha,\beta)y_{\alpha+\beta}   \big)
            =[-y_{-\alpha},-y_{-\beta}]
            =c'(-\alpha,-\beta)y_{-(\alpha+\beta)}.
\end{equation}
\end{proof}

From now we always our $ x_{\alpha}$ in this way.

\begin{remark}
It is also possible to choice the $ x_{\alpha}$ in such a way that 

\begin{itemize}
\item $B( x_{\alpha},\xbma)=-1$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}
This is the choice of the reference \cite{Hochschild}.
\end{remark}


Here is a characterization for Cartan subalgebras of semisimple Lie algebras. This is sometimes taken as the \emph{definition} of a Cartan subalgebra in books devoted to semisimple Lie algebras (for example in \cite{Helgason}).
\begin{proposition}     \label{PropCartanMaxAnel}
    A subalgebra $\lH$ of a semisimple Lie algebra $\lG$ is a Cartan subalgebra if and only if
    \begin{itemize}
        \item $\lH$ is maximally abelian in $\lG$,
        \item the endomorphism $\ad h$ is semisimple\footnote{If the base field is \( \eC\), this means ``diagonalizable''.} for every $h\in\lH$.
    \end{itemize}
\end{proposition}

\begin{proof}
\subdem{Necessary condition} We know from theorem \ref{tho:Phi_base} that $\lH$ is abelian and from proposition \ref{prop:Cartan_max_nil} that it is maximally nilpotent. Then it is maximally abelian. On the other hand, let $h\in\lH$; the endomorphism $\ad h$ is diagonalisable with respect to the decomposition $\lG=\lH\bigoplus_{\alpha\in\Delta}\lH_{\alpha}$.

\subdem{Sufficient condition}
Firstly it is clear that a maximal abelian subalgebra is nilpotent and the $\ad h_i$ are simultaneously diagonalisable for the different $h_i\in\lH$. Let $\{x_1,\ldots,x_n\}$ be a basis of $\lG$ which diagonalise all the $\ad h_i$. In this basis, if $(\ad h)_{ii}=0$ for any $h\in\lH$, then $x_i\in\lH$: if it was not, $\lH\cup\{x_i\}$ would be abelian.

Let $x\in\lG$ such that $(\ad h)x\in\lH$ for every $h\in\lH$. Suppose that $x$ has a $x_i$-component with $x_i\notin\lH$. There is a $h\in\lH$ with $(\ad h)_{ii}\neq 0$. Then $(\ad h)x$ has a $x_i$-component and can't lies in $\lH$.

\end{proof}

This characterization of Cartan subalgebras is used to prove the existence of Cartan subalgebra for any complex semisimple Lie algebra.



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl: other results}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
Two immediate properties of the Weyl group are 

\begin{enumerate}
\item $W$ is a finite group of orthogonal transformations of $V$,
\item if $r$ is an orthogonal transformation of $V$, the $s_{r\alpha}=rs_{\alpha} r^{-1}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\subdem{First item} By definition of an abstract root system, $W$ leaves $\Delta$ invariant; since $V$ is spanned by $V$, it implies that $W$ also leaves $V$ invariant. From an easy computation, $(s_{\alpha}\varphi,s_{\alpha}\phi)=(\varphi,\phi)$. Since $\Delta$ is a finite set, there are only a finite number of common permutations of elements of $\Delta$ \emph{a fortiori} $W$ is finite.

\subdem{Second item}
It is easy to see that $s_{r\alpha}(r\varphi)=rs_{\alpha}\varphi$, then $s_{r\alpha}=r\circ s_{\alpha}\circ r^{-1}$.
\end{proof}


We introduce the \defe{root reflexion}{root!reflexion} $\dpt{s_{\alpha}}{\lHeR^*}{\lHeR^*}$ for $\alpha\in\Phi$ and $\varphi\in\lHeR^*$ by
\begin{equation}
s_{\alpha}(\varphi)=\varphi-\frac{2(\varphi,\alpha)}{|\alpha|^2}\alpha.
\end{equation}

\begin{proposition}
If $\alpha\in\Phi$, then $s_{\alpha}$ leaves $\Phi$ invariant.
\end{proposition}

\begin{proof}
If $\alpha$ or $\varphi$ is zero, then it is clear that $s_{\alpha}(\varphi)$ belongs to $\Phi$. Thus we can suppose that $\alpha\in\Delta$ and proof that $s_{\alpha}$ leaves $\Delta$ invariant. For, we use the theorem \ref{tho:six_Cartan} to find
\begin{equation}
  s_{\alpha}\beta=\beta-\frac{2(\beta,\alpha)}{|\alpha|^2}\alpha
               =\beta-(\lbba-\lbha)\alpha.
\end{equation}
If $\lbba-\lbha>0$, we are in a case $\beta-n\alpha$ with $\lbba-\lbha<\lbba$, so that $s_{\alpha}\beta$ is a root. The case $\lbha>\lbba$ is treated in the same way. It just remains to check that  if $\alpha,\beta\in\Delta$, then $s_{\alpha}\beta\neq0$. The problem is to show that the equation (with a given $\alpha$ in $\Delta$)
\begin{equation}\label{eq:beta_frZ_alpha}
   \beta=\frZ{\alpha}{\beta}\alpha
\end{equation}
has no solution in $\Delta$ (the indeterminate is $\beta$). The only nonzero multiples of $\beta$ which are roots are $\pm\beta$, then if we set $\beta=r\alpha$, equation \eqref{eq:beta_frZ_alpha} gives $r=\pm\frac{1}{2}$, which is impossible.

\end{proof}

\begin{proposition}
    The Weyl group permutes simply transitively the simple systems.
\end{proposition}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Longest element}
%---------------------------------------------------------------------------------------------------------------------------

Let \( w\in W\). The \defe{length}{length!in Weyl group}\nomenclature[G]{\( l(w)\)}{length in the Weyl group} of \( w\) is the smallest \( k\) such that \( w\) can be written as a composition of \( k\) reflexions \( s_{\alpha_i}\). That is the smallest \( k\) such that
\begin{equation}
    w=s_{\alpha_{i_1}}s_{\alpha_{i_2}}\ldots s_{\alpha_{i_k}}.
\end{equation}

\begin{lemma}
    If \( w\) and \( w'\) are elements of the Weyl group,
    \begin{enumerate}
        \item
            \( l(w)=l(w^{-1})\),
        \item
            \( l(w)=0\) if and only if \( w=\id\),
        \item
            \( l(ww')\leq l(w)+l(w')\),
        \item
            \( l(ww')\geq l(w)-l(w')\),
        \item
            \( l(w)-1\leq l(ws_{\alpha_i})\leq l(w)+1\).
    \end{enumerate}
\end{lemma}
Let \( n(w)\) be the number of positive simple roots that are send to a negative root:
\begin{equation}
    n(w)=Card\,\Pi\cap w^{-1}(-\Pi).
\end{equation}

\begin{proposition}
    Let \( \Delta\) be a system of simple roots and \( \Pi\) the associated positive system. The following conditions on an element \( w\) of the Weyl group are equivalent:
    \begin{enumerate}
        \item
            \( w\Pi=\Pi\);
        \item
            \( w\Delta=\Delta\);
        \item
            \( l(w)=0\);
        \item
            \( n(w)=0\);
        \item
            \( w=\id\).
    \end{enumerate}
\end{proposition}

For a proof see page 15 in \cite{HumphreysCoxeter}.

\begin{theorem}
    If \( w\) is an element of the Weyl group,
    \begin{equation}
        l(w)=n(w).
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl group and representations}
%---------------------------------------------------------------------------------------------------------------------------

This subsection comes from \cite{Cornwell}.

\begin{theorem}     \label{Thoirrepllamifffmor}
    There exists an irreducible representation of highest weight $\Lambda$ if and only if
    \begin{equation}
        \Lambda_{\alpha}=\frac{ 2(\Lambda,\alpha) }{ (\alpha,\alpha) }\in\eN
    \end{equation}
    for every simple root $\alpha$. Moreover, if $\xi$ is a highest weight vector and if $\alpha$ is a simple root, then
    \begin{equation}
        E_{-\alpha}^k\xi    
                \begin{cases}
                    \neq 0  &\text{if $k\leq\Lambda_{\alpha}$}\\
                    =0  &\text{if $k>\Lambda_{\alpha}$}.    
                \end{cases}
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

\begin{example} \label{ExHESKimc}
    We already studied the irreducible representations if \( \so(3)\) in the subsection \ref{subsecPJmtqrG}. The theorem \ref{Thoirrepllamifffmor} allows to determine them in a much more synthetic way.

    In the case of $\so(3)$, the Cartan subalgebra is one dimensional, and one has only one simple root: $\alpha=J_{12}^*$. If $\Lambda=aJ_{12}^*$, one has $(\Lambda,\alpha)=a$, and theorem \ref{Thoirrepllamifffmor} says that $\Lambda$ is highest weight of an irreducible representation if and only if $a\in \eN/2$.
\end{example}

\begin{theorem}     \label{ThoLOngestlowestrepres}
    If \( \Lambda\) is the highest weight of a representation and if \( w_0\) is the longest element of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{theorem}

\begin{probleme}
    It is still not clear for me how does the proof works. Questions to be answered:
    \begin{enumerate}
        \item
            existence, unicity
        \item
            \( w_0\) is the longest element of the Weyl group
        \item
            if \( \Lambda\) is the highest weight, then \( w_0\Lambda\) is the lowest.
    \end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Chevalley basis (deprecated)}
%---------------------------------------------------------------------------------------------------------------------------
See \cite{SSLA_Modave2005}.

Let $\Phi$\nomenclature{$\Phi$, $\Phi^+$}{Root system} be the finite set of roots of $\lG$. Then chose a positivity notion on $\lH^*$ and consider $\Phi^+$, the positive subset of $\Phi$. We also take $\Delta$\nomenclature{$\Delta$}{Basis of the roots}, a basis of the roots. An element of $\Phi^+$ is a \defe{simple root}{simple!root}\index{root!simple} if it cannot be written under the form of a sum of two elements of $\Phi^+$. Every positive root is a sum of simple roots. 

Let
\begin{equation}
    \{ \alpha_1,\ldots,\alpha_l \}
\end{equation}
be a basis of $\lH^*$ made of simple roots and
\begin{equation}
    \{ h_1,\ldots,h_l \},
\end{equation}
the dual basis. One can choose the $\alpha_i$ in such a way that $\{ h_1,\ldots,h_l \}$ is orthogonal with respect to the Killing form\quext{Why ?}. One consequence of that is that
\begin{equation}            \label{EqBhihalphaih}
    B(h_i,h)=\alpha_i(h)
\end{equation}
for every $h\in\lH$. Indeed, $h$ can be written, in the basis, as $h=h^jh_j$ where $h^j=B(h_j,h)$. Thus one has
\begin{equation}
    B(h_i,h)=h^i=h^j\delta_{ij}=\alpha_i(h^jh_j)=\alpha_i(h).
\end{equation}
We consider $\{ \alpha_1,\ldots,\alpha_m \}$, the positive roots (the roots $\alpha_1$,\ldots,$\alpha_l$ are some of them). One knows that $\lG_{\alpha_i}$ is one dimensional, so one take $e_i\in\lG_{\alpha_i}$ and $f_i\in\lG_{-\alpha_i}$ as basis of their respective spaces. If we denote by $\lN^+=\Span\{ e_1,\ldots, e_m \}$ and $\lN^-=\Span\{ f_1,\ldots,f_m \}$, we have the decomposition
\begin{equation}
    \lG=\lN^-\oplus\lH\oplus\lN^+.
\end{equation}


% The proposition about SL(2,R) was here. 198631779

It $\{ \alpha_i \}$ are the simple roots, we consider the following new basis for $\lH$:
\begin{equation}
    H_{\alpha_i}=\frac{ 2\alpha_i^* }{ (\alpha_i,\alpha_i) }
\end{equation}
where $\alpha_i^*$ is the dual of $\alpha_i$ with respect to the inner product on \( \lH^*\), this means
\begin{equation}
    \alpha_j(\alpha_i^*)=(\alpha_i,\alpha_j).
\end{equation}
Since \( \lH\) is abelian (proposition \ref{PropCartanMaxAnel}), we have
\begin{equation}
    [H_{\alpha_i},H_{\alpha_j}]=0.
\end{equation}
Each root is a combination of the simple roots. If $\beta=\sum_{i=1}^lk_i\alpha_i$, we generalise the definition of $H_{\alpha_i}$ to
\begin{equation}
    H_{\beta}=\frac{ 2\beta^* }{ (\beta,\beta) }=\sum_i k_i\frac{ (\alpha_i,\alpha_i) }{ (\beta,\beta) }H_{\alpha_i}.
\end{equation}
The element $H_{\beta}$ is the \defe{co-weight}{co-weight} associated with the weight $\beta$.

Using the inner product $(.,.)$, we have the decomposition $\beta=\sum_i(\beta,\alpha_i)\alpha_i$ of the roots. An immediate consequence is that 
\begin{equation}
    \beta(\alpha_i^*)=(\alpha_i,\beta).
\end{equation}
If $\beta$ is any root, we denote by $\beta_i$ the result of $\beta$ on $H_{\alpha_i}$:
\begin{equation}            \label{EqbetaialphaiH}
    \beta_i=\beta(H_{\alpha_i})=\frac{ 2(\alpha_i,\beta) }{ (\alpha_i,\alpha_i) }.
\end{equation}

\begin{theorem}[Chevalley basis]\index{Chevalley!basis}
    For each root $\beta$, one can found an eigenvector $E_{\beta}$ of $\ad(H_{\beta})$ such that
    \begin{equation}            \label{EqChevalleyBasis}
        \begin{aligned}[]
            [H_{\beta},H_{\gamma}]      &=  0\\
            [E_{\beta},E_{-\beta}]      &=  H_{\beta}\\
            [E_{\beta},E_{\gamma}]      &=
                                \begin{cases}
                                    \pm(p+1)E_{\beta+\gamma}    &\text{if $\beta+\gamma$ is a root}\\
                                    0               &\text{otherwise}\\
                                \end{cases}\\
            [H_{\beta},E_{\gamma}]      &=2\frac{ (\beta,\gamma) }{ (\beta,\beta) }E_{\gamma}
        \end{aligned}
    \end{equation}
    where $p$ is the biggest integer $j$ such that $\gamma+j\beta$ is a root. Moreover, if $\alpha_i$ and $\alpha_j$ are simple roots, the latter becomes
    \begin{equation}
        [H_{\alpha_i},E_{\pm\alpha_j}]=\pm A_{ij}E_{\pm\alpha_j}
    \end{equation}
    where $A$ is the Cartan matrix.
\end{theorem}

An important point to notice is that, for each positive root $\alpha$, the algebra generated by $\{ H_{\alpha},E_{\alpha},E_{-\alpha} \}$ is $\gsl(2)$. This is the reason why the representation theory of $\lG$ reduces to the representation theory of $\gsl(2)$.

\section{Real Lie algebras}
%++++++++++++++++++++++++++

\subsection{Real and complex vector spaces}
%//////////////////////////////////////////////

If $V$ is a real vector space, the \defe{complexification}{complexification!of a vector space} of $V$ is the vector space\nomenclature{$V\heC$}{Complexification of $V$}
\[
  V\heC:=V\otimes\beR\eC.
\]
If $\{v_i\}$ is a basis of $V$ on $\eR$, then $\{v_i\otimes 1\}$ is a basis of $V\heC$ on $C$. Then
\[
   \dim_{\eR}V=\dim_{\eC}V\heC.
\]

Let $W$ be a complex vector space. If one restrains the scalars to $\eR$, we find a real vector space denoted by $W\heR$\nomenclature{$W\heR$}{Restriction of a complex vector spaces to $\eR$}. If $\{w_j\}$ is a basis of $W$, then $\{w_j,iw_j\}$ is a basis of $W\heR$ and 
\[
  \dim\beC W=\frac{1}{2}\dim\beR W\heR.
\]
Note that $(V\heC)\heR=V\oplus iV$.

A real vector space $V$ is a \defe{real form}{real!form!of complex vector space} of a complex vector space $W$ if $W\heR=V\oplus iV$. If $V$ is a real form of $W$, the map $\dpt{\varphi}{V\heC}{V\heC}$ given by the identity on $V$ and the multiplication by $-1$ on $iV$ is the \defe{conjugation}{conjugation} of $V\heC$ with respect of the real form $V$.

\subsection{Real and complex Lie algebras}
%/////////////////////////////////////////

For notational convenience, if not otherwise mentioned, $\lG$ will denote a complex Lie algebra and $\lF$ a real one. If $\lF$ is a real Lie algebra and $\lF\heC=\lF\otimes\eC$, its complexification (as vector space), we endow $\lF\heC$ with a Lie algebra structure by defining
\[
  [ (X\otimes a),(Y\otimes b)  ]=[X,Y]\otimes ab.
\]
This is a bilinear extension of the Lie algebra bracket of $\lF$. It is rather easy to see that $[\lF,\lF]\heC=[\lF\heC,\lF\heC]$.

Now we turn our attention to the Killing form. Let $\lF$ be a real Lie algebra with a Killing form $B\blF$. A basis of $\lF$ is also a basis of $\lF\heC$. Then the matrix $B_{ij}=\tr( \ad X_i\circ\ad X_j )$ of the Killing form is the same for $\lF\heC$ than for $\lF$. In conclusion:
\[
   B_{\lF\heC}|_{\lF\times\lF}=B\blF.
\]

Let us study the inverse process: $\lG$ is a complex Lie algebra and $\lG\heR$ is the real Lie algebra obtained from $\lG$ by restriction of the scalars. If $\mB=\{v_j\}$ is a basis of $\lG$, $\mB'=\{v_j,iv_j\}$ is a one of $\lG\heR$. For a certain $X\in\lG$ we denote by $(c_{kl})$ the matrix of $\ad_{\lG}X$. Now we study the matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ by computing
\begin{equation}
(\ad_{\lG}X)v_i=c_{ik}v_k
               =\big[ \real(c_{ik})+i\imag(c_{ik}) \big]v_k
           =a_{ik}v_k+b_{ik}(iv_k)
\end{equation}
if $a=\real c$ and $b=\imag c$. Then the columns of $\ad_{\lG\heR}$ which correspond to the $v_i\in\mB'$'s are given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&\cdot\\
         b&\cdot
               \end{pmatrix}
\]
where the dots denote some entries to be find now:
\begin{equation}
(\ad_{\lG}X)(iv_i)=i\big(  a_{ik}v_k+b_{ik}(iv_k)  \big)\\
                  =a_{ik}(iv_k)-b_{ik}v_k,
\end{equation}
so that the complete matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ is given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&-b\\
         b&a
               \end{pmatrix}.
\]
So,
\[
\ad_{\lG\heR}X\circ\ad_{\lG\heR}X'=\begin{pmatrix}
                 aa'-bb'&\cdot\\
         \cdot&aa'-bb'
               \end{pmatrix}.
\]
Then $B(X,X')=2\tr(aa'-bb')$ while
\begin{equation}
  B(X,Y)=\tr\big(  (a+ib)(a'+ib')  \big)\\
        =\tr(aa'-bb')+i\tr(ab'+ba').
\end{equation}
Thus we have
\begin{equation}
     B_{\lG\heR}=2\real B_{\lG},
\end{equation}
so that $\lG\heR$ is semisimple if and only if $\lG$ is semisimple. 

A result about the group of inner automorphism which will be useful later:

\begin{lemma}\label{lem:Int_g_gR}
If $\lG$ is a complex semisimple Lie algebra, then $\Int\lG=\Int\lG\heR$.
\end{lemma}

\begin{proof}
If $\{X_i\}$ is a basis of $\lG$, then $\{X_j,iX_j\}$ is a basis of $\lG\heR$. We define $\dpt{\psi}{\ad\lG}{\ad\lG\heR}$ by
\[
   \psi(\ad(a^jX_j))=\ad(a^jX_j).
\]
It is clearly surjective. On the other hand, if $\ad(a^jX_j)\ad(b^kX_k)$ as elements of $\ad\lG\heR$, then they are equals as elements of $\ad\lG$. The discussion following equations \eqref{eq:schem_ad_int} finish the proof.
\end{proof}

\subsection{Split real form}
%//////////////////////////////

Let $\lG$ be a complex semisimple Lie algebra, $\lH$ a Cartan subalgebra, $\Phi$ the set roots, $\Delta$ the set of non zero roots and $B$, the Killing form. From property \eqref{eq:enuaiv} and the fact that $c(-\alpha,-\beta)=c(\alpha,\beta)$, we find $c(\alpha,\beta)^2=\frac{1}{2}\lbha(1+\lbba)|\alpha|^2$,
 so that $c(\alpha,\beta)^2\geq 0$ which gives $c(\alpha,\beta)\in\eR$. We can define
 
\[
   \lGeR=\lH_0\bigoplus_{\alpha\in\Phi}\eR x_{\alpha}.
\]
Remark that $\lG_{\alpha}$ has dimension one with respect to $\eC$, not $\eR$; then $\eR x_{\alpha}\neq\lG_{\alpha}$, but $\eC x_{\alpha}=\lG_{\alpha}$ and $\lG_{\alpha}=\eR x_{\alpha}\oplus i\eR x_{\alpha}$. Since it is clear that $\bigoplus_{\alpha\in\Delta}( \eR x_{\alpha}\oplus i\eR x_{\alpha} )=\bigoplus_{\alpha\in\Delta}\lG_{\alpha}$, the proposition \ref{prop:lHeR} gives
\begin{equation}
  \lG=\lGeR\oplus i\lGeR.
\end{equation}
Any real form of $\lG$ which contains the $\lHeR$ of a certain Cartan subalgebra $\lH$ of $\lG$ is said a \defe{split real form}{split!real form}. The construction shows that any complex semisimple Lie algebra admits a split real form.

\subsection{Compact real form}
%///////////////////////////////

A \defe{compact real form}{compact!real form}\index{compact!Lie algebra} of a complex Lie algebra is a real form which is compact as Lie algebra. Recall that a real Lie algebra is compact when its analytic group of inner automorphism is compact, see page \pageref{pg:compact_Lie}

\begin{theorem}
Any complex semisimple Lie algebra contains a compact real form.
\end{theorem}

\begin{proof}
Let $\lH$ be a Cartan algebra of the complex semisimple Lie algebra $\lG$ and $ x_{\alpha}$, some root vectors. We consider the space
\begin{equation}
 \lU_0=\underbrace{\sum_{\alpha\in\Phi}\eR ih_{\alpha}}_A+\underbrace{\sum_{\alpha\in\Phi}\eR( x_{\alpha}-\xbma)}_B+\underbrace{\sum_{\alpha\in\Phi}\eR i( x_{\alpha}+\xbma)}_C.
\end{equation}
Since $\lU_0\oplus i\lU_0$ contains all the $\eC h_{\alpha}$, $\lH\subset\lU_0\oplus i\lU_0$; it is also rather clear that $\lU_0$ is a real form of $\lG$ (as vector space), for example, $i\eR( x_{\alpha}-\xbma)+\eR i( x_{\alpha}+\xbma)=\eR i x_{\alpha}$. Now we have to check that $\lU_0$ is a real form of $\lG$ as Lie algebra, i.e. that $\lU_0$ is closed for the Lie bracket. This is a lot of computations:
\[
\begin{split}
[i h_{\alpha},i\hbb]               &=0,\\
[i h_{\alpha},( x_{\alpha}-\xbma)]        &=i(\alpha( h_{\alpha}) x_{\alpha}-(-\alpha)( h_{\alpha})\xbma)\\
                            &=i\alpha( h_{\alpha})( x_{\alpha}+\xbma)\in C,\\
[i h_{\alpha},i( x_{\alpha}+\xbma)]       &=-\alpha( h_{\alpha})( x_{\alpha}-\xbma)\in B,\\
[( x_{\alpha}-\xbma),(\xbb-\xbmb)] &=c(\alpha,\beta)( x_{\alpha+\beta}-x_{-(\alpha+\beta)} )\in B\\
                            &\quad -c(\alpha,\beta)(x_{\alpha-\beta}-x_{\beta-\alpha})\in B,\\
[( x_{\alpha}-\xbma),i(\xbb+\xbmb)]&=ic(\alpha,\beta)(x_{\alpha+\beta}+x_{-(\alpha+\beta)})\in C\\
                            &\quad +ic(\alpha,-\beta)(x_{\alpha-\beta)}+x_{-\alpha+\beta})\in C\\
[i h_{\alpha},(\xbb-\xbmb)]     &=i\beta( h_{\alpha})(\xbb-\xbmb)\in C\\
[i h_{\alpha},i(\xbb+\xbmb)]       &=-\beta( h_{\alpha})(\xbb-\xbmb)\in B\\
[i( x_{\alpha}+\xbma),i(\xbb+\xbmb)]&=-c(\alpha,\beta)(x_{\alpha+\beta}-x_{-(\alpha+\beta)})\\
                             &\quad -c'(\alpha,-\beta)(x_{\alpha-\beta}-x_{-\alpha+\beta}).
\end{split}
\]

From proposition \ref{prop:compact_Killing}, it just remains to prove that the Killing form of $\lU_0$ is strictly negative definite. We know that $B_{\lG}(\lG_{\alpha},\lG_{\beta})=0$ if $\alpha,\beta\in\Phi$ and $\alpha+\beta\neq 0$; then $A\perp B$ and $A\perp C$. It is a lot of computation to compute the Killing form; we know that $B$ is strictly positive definite on $\sum_{\alpha\in\Delta}\eR h_{\alpha}$ (and then strictly negative definite on $A$) a part this, the non zero elements are (recall that if $\alpha\neq 0$, $B( x_{\alpha}, x_{\alpha})=0$ from corollary \ref{cor:Bxy_zero})
\[
\begin{split}
  B( ( x_{\alpha}-\xbma),( x_{\alpha}-\xbma) )&=-2B( x_{\alpha},\xbma)=-2\\
  B(i( x_{\alpha}+\xbma),i( x_{\alpha},\xbma))&=-2.
\end{split}  
\]

What we have in the matrix of $B_{\lG}|_{\lU_0\times\lU_0}$ is a negative definite block (corresponding to $A$), $-2$ on the rest of the diagonal and zero anywhere else. Then it is well negative definite and $\lU_0$ is a compact real from of $\lG$.
\end{proof}

\subsection{Involutions}
%//////////////////////////

Let $\lG$ be a (real or complex) Lie algebra. An automorphism $\dpt{\sigma}{\lG}{\lG}$ which is not the identity such that $\sigma^2$ is the identity is a \defe{involution}{involutive!automorphism}. An involution $\dpt{\theta}{\lF}{\lF}$ of a \emph{real} semisimple Lie algebra $\lF$ such that the quadratic form $B_{\theta}$ defined by
\[
   B_{\theta}(X,Y):=-B(X,\theta Y)
\]
is positive definite is a \defe{Cartan involution}{Cartan!involution}.

\begin{proposition}
Let $\lG$ be a complex semisimple Lie algebra, $\lU_0$ a compact real form and $\tau$, the conjugation of $\lG$ with respect to $\lU_0$. Then $\tau$ is a Cartan involution of $\lG\heR$.
\label{prop:conj_invol}
\end{proposition}

\begin{proof}
From the assumptions, $\lG=\lU_0\oplus i\lU_0$, $\tau_{\lU_0}=id$ and $\tau_{i\lU_0}=-id$; then it is clear that $\tau_{\lG\heR}^2=id|_{\lG\heR}$. If $Z\in\lG$, we can decompose into $Z=X+iY$ with $X$, $Y\in\lU_0$. For $Z\neq 0$, we have
\begin{equation}
    B_{\lG}(Z,\tau Z)=B_{\lG}(X+iY,X-iY)
                     =B_{\lG}(X,X)+B_{\lG}(Y,Y)
             =B_{\lU_0}(X,X)+B_{\lU_0}(Y,Y)<0
\end{equation}
because $B$ restricts itself to $\lU_0$ which is compact. Then
\begin{equation}
  (B_{\lG\heR})_{\tau}(Z,Z')=B_{\lG\heR}(Z,\tau Z)
                            =-2\real B_{\lG}(Z,\tau Z')
\end{equation}
is positive definite because $(B_{\lG})_{\tau}$ is negative definite. Thus $\tau$ is a Cartan involution of $\lG\heR$.
\end{proof}

\begin{lemma}
If $\varphi$ and $\psi$ are involutions of a vector space $V$ (we denote by $V_{\psi^+}$ and $V_{\psi^-}$ the subspaces of $V$ for the eigenvalues $1$ and $-1$ of $\psi$ and similarly for $\varphi$), then
\[
[\varphi,\psi]=0\quad\text{iff}\quad \left\{   \begin{aligned}
                                                   V_{\varphi^+}&=(V_{\varphi^+}\cap V_{\psi^+})\oplus(V_{\varphi^+}\cap V_{\psi^-})\\
                           V_{\varphi^-}&=(V_{\varphi^-}\cap V_{\psi^+})\oplus(V_{\varphi^-}\cap V_{\psi^-}),
                                           \end{aligned}
                      \right.
\]
i.e. if and only if the decomposition of $V$ with respect to $\varphi$ is ``compatible''{} with the one with respect to $\psi$.
\label{lem:invol_compat}
\end{lemma}

\begin{proof}
\subdem{Direct sense}
Let us first see that $\varphi$ leaves the decomposition $V=V_{\psi^+}\oplus V_{\psi^-}$ invariant. If $x=x_{\psi^+}+x_{\psi^-}$,
\[
   \varphi(x_{\psi^+})=(\varphi\circ\psi)(x_{\psi^+})=(\psi\circ\varphi)(x_{\psi^+}).
\]
Then $\varphi(x_{\psi^+})\in V_{\psi^+}$, and the matrix of $\varphi$ is block-diagonal with respect to the decomposition given by $\psi$. Thus $V_{\psi^+}$ and $V_{\psi^-}$ split separately into two parts with respect to $\varphi$.

\subdem{Inverse sense} 
If $x\in V$, we can write $x=x_{++}+x_{+-}+x_{-+}+x_{--}$ where the first index refers to $\psi$ while the second one refers to $\psi$; for example, $x_{+-}\in V_{\psi^+}\cap V_{\varphi^-}$. The following computation is easy:
\begin{equation}
\begin{split}
(\varphi\circ\psi)(x)&=\varphi(x_{++}+x_{+-}-x_{-+}-x_{--})\\
                 &=x_{++}-x_{+-}-x_{-+}+x_{--}\\
         &=\psi(x_{++}-x_{+-}-x_{-+}-x_{--})\\
         &=(\psi\circ\varphi)(x).
\end{split}
\end{equation}
\end{proof}

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra, $\theta$ a Cartan involution on $\lF$ and $\sigma$, another involution (not specially Cartan). Then there exists a $\varphi\in\Int\lF$ such that $[\varphi\theta\varphi^{-1},\sigma]=0$ 
\label{tho:sigma_theta_un}
\end{theorem}

\begin{proof}
If $\theta$ is a Cartan involution, then $B_{\theta}$ is a scalar product on $\lF$. Let $\omega=\sigma\theta$. By using $\sigma^2=\theta^2=1$, $\theta=\theta^{-1}$ and the invariance property \ref{prop:auto_2} of the Killing form,
\begin{equation}
B(\omega X,\theta Y)=B(X,\omega^{-1}\theta Y)
                    =B(X,\theta\sigma\theta Y)
            =B(X,\theta\omega Y).
\end{equation}
Then $B_{\theta}(\omega X,Y)=B_{\theta}(X,\omega Y)$. This is a general property of scalar product that in this case, the matrix of $\omega$ is symmetric while the one of $\omega^2$ is positive definite. If we consider the classical scalar product whose matrix is $(\delta_{ij})$, the property is written as $A_{ij}v_jw_j=v_iA_{ij}w_j$ (with sum over $i$ and $j$); this implies the symmetry of $A$. To see that $A^2$ is positive definite, we compute (using the symmetry):
\[
   A_{ij}A_{jk}v_iv_k=v_iA_{ij}v_kA_{kj}=\sum_j(v_iA_{ij})^2>0.
\]
The next step is to see that there is an unique linear transformation $\dpt{A}{\lF}{\lF}$ such that $\omega^2=e^A$, and that for any $t\in\eR$, the transformation $e^{tA}$ is an automorphism of $\lF$.

We choose an orthonormal (with respect to the inner product $B_{\theta}$) basis $\{X_1,\ldots,X_n\}$  of $\lF$ in which $\omega$ is diagonal. In this basis, $\omega^2$ is also diagonal and has positive real numbers on the diagonal; then the existence and unicity of $A$ is clear. Now we take some notations:
\begin{subequations}
\begin{align}
  \omega(X_i)&=\lambda_iX_i\\
  \omega^2(X_i)&=e^{a_i}X_i,
\end{align}  
\end{subequations}
(no sum at all) where the $a_i$ are the diagonals elements of $A$. The structure constants are as usual defined by
\begin{equation}
   [X_i,X_j]=c_{ij}^kX_k.  
\end{equation}
Since $\sigma$ and $\theta$ are automorphisms, $\omega^2$ is also one. Then 
\[
\omega^2[X_i,X_j]=c_{ij}^k\omega^2(X_k)=c_{ij}^ke^{a_k}X_k
\]
can also be computed as
\[
   \omega^2[X_i,X_j]=[\omega^2X_i,\omega^2X_j]=e^{a_i}e^{a_j}c_{ij}^kX_k,
\]
so that $c_{ij}^ke^{a_k}=c_{ij}^ke^{a_i}e^{a_j}$, and then $\forall t\in\eR$,
\[
   c_{ij}^ke^{ta_k}=c_{ij}^ke^{ta_i}e^{ta_j},
\]
which proves that $e^{tA}$ is an automorphism of $\lF$. By lemma \ref{lem:autom_derr}, $A$ is thus a derivation of $\lF$. The semi-simplicity makes $\partial\lF=\ad\lF$, then $A\in\ad\lF$ and $e^{tA}\in\Int\lF$ because it clearly belongs to the identity component of $\Aut\lF$.

Now we can finish de proof by some computations. Remark that $\omega=e^{A/2}$ and $[e^{tA},\omega]=0$ because it can be seen as a common matrix commutator. Since $\omega^{-1}=\theta\sigma$, we have $\theta\omega^{-1}\theta=\sigma\theta$, or $\theta\omega^2\theta=\omega^2$ and
\begin{equation}\label{eq:eAth}
   e^{A}\theta=\theta e^{-A}.
\end{equation}
From this, one can deduce that $e^{tA}\theta=\theta e^{-tA}$. Indeed, as matricial identity, equation \eqref{eq:eAth} reads
\[
    (e^{A}\theta)_{ik}=(e^{A})_{ij}\theta_{jk}
                      =e^{a_i}\theta_{ik}
              =e^{-a_k}\theta_{ik}.
\]
Then for any $ik$ such that $\theta_{ik}\neq 0$, we find $e^{a_i}=e^{-a_k}$ and then also $e^{ta_i}=e^{-ta_k}$. Thus $(e^{tA}\theta)_{ik}=(e^{tA})_ij\theta_{jk}=e^{ta_i}\theta_{ik}=\theta_{ik}e^{-ta_k}=(\theta e^{-tA})_{ik}$. So we have
\begin{equation}
  e^{tA}\theta=\theta e^{-tA}.
\end{equation}
Now we consider $\varphi=e^{A/4}\in\Int\lF$ and $\theta_1=\varphi\theta\varphi^{-1}$. We find $\theta_1\sigma=e^{A/2}\omega^{-1}$ and $\sigma\theta^{-1}=e^{-A/2}\omega$. Since $\omega^2=A$, we have $e^{A/2}=e^{-A/2}\omega^2$ and thus $\theta_1\sigma=\sigma\theta_1$.

\end{proof}

\begin{corollary}
Any real Lie algebra has a Cartan involution.
\end{corollary}

\begin{proof}
Let $\lF$ be a real Lie algebra and $\lG$ be his complexification: $\lG=\lF\heC$. Let $\lU_0$ be a compact real form of $\lG$ and $\tau$ the induced involution (the conjugation) on $\lG$. By the proposition \ref{prop:conj_invol}, we know that $\tau$ is  a Cartan involution of $\lG\heR$. We also consider $\sigma$, the involution of $\lG$ with respect to the real form $\lF$. It is in particular an involution on the real Lie algebra $\lF$. Then one can find a $\varphi\in\Int\lG\heR$ such that $[\varphi\tau\varphi^{-1},\sigma]=0$ on $\lG\heR$. Let $\lU_1=\varphi\lU_0$ and $X\in\lU_1$. We can write $X=\varphi Y$ for a certain $Y\in\lU_0$. Then
\[
   \varphi\tau\varphi^{-1} X=\varphi\tau Y=\varphi Y=X,
\]
so that $\varphi\tau\varphi^{-1}=id|_{\lU_1}$. Note that $\lU_1$ is also a real compact form of $\lG$ because the Killing form is not affected by $\varphi$. Let $\tau_1$ be the involution of $\lG$ induced by $\lU_1$. We have
\[
   \tau_1|_{\lU_1}=\varphi\tau\varphi^{-1}_{\lU_1}=\id|_{\lU_1}.
\]
Since $\varphi$ is $\eC$-linear, we have in fact $\tau_1=\varphi\tau\varphi^{-1}$. Now we forget $\lU_0$ and we consider the compact real form $\lU_1$ with his involution $\tau_1$ of $\lG$ which satisfy $[\tau_1,\sigma]=0$ on $\lG\heR$ This relation holds also on $i\lG\heR$, then 
\[
   [\tau_1,\sigma]=0
\]
on $\lG=\lF\heC$. Let $X\in\lF$, i.e. $\sigma X=X$; it automatically fulfils 
\[
  \sigma\tau_1 X=\tau_1\sigma X=\tau_1 X,
\]
so that $\tau_1$ restrains to an involution on $\lF$ (because $\tau_1\lF\subset\lF$). Let $\theta=\tau_1|_{\lF}$. For $X$, $Y\in\lF$, we have
\begin{equation}
B_{\theta}(X,Y)=-B_{\lF}(X, \theta Y)
             =-B_{\lF}(X,\tau Y)
         =\frac{1}{2}(B_{\lG\heR})_{\tau_1}(X,Y),
\end{equation}
which shows that $\theta$ is a Cartan involution. The half factor on the last line comes from the fact that $\lG\heR=(\lF\heC)\heR=\lF\oplus i\lF$.

\end{proof}

\begin{corollary}
Any two Cartan involutions of a real semisimple Lie algebra are conjugate by an inner automorphism. \index{inner!automorphism}
\label{cor:Cartan_conj_inner}
\end{corollary}

\begin{proof}
Let $\sigma$ and $\sigma'$ be two Cartan involutions of $\lF$. We can find a $\varphi\in\inf\lF$ such that $[\varphi\sigma\varphi^{-1},\sigma']=0$. Thus it is sufficient to prove that any two Cartan involutions which commute are equals. So let us consider $\theta$ and $\theta'$, two Cartan involutions such that $[\theta,\theta']=0$. By lemma \ref{lem:invol_compat}, we know that the decompositions into $+1$ and $-1$  eigenspaces with respect to $\theta$ and $\theta'$ are compatibles. If we consider $X\in\lF$ such that $\theta X=X$ and $\theta' X=-1$ (it is always possible if $\theta\neq\theta'$), we have
\[
\begin{split}
  0<B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X)\\
  0<B_{\theta'}(X,X)=-B(X,\theta' X)=B(X,X)
\end{split}
\]
which is impossible.
\end{proof}

\begin{corollary}
Any two real compact form of a complex semisimple Lie algebra are conjugate by an inner automorphism.
\end{corollary}

\begin{proof}
We know that any real form of $\lG$ induces an involution (the conjugation) and that if the real form is compact, the involution is Cartan on $\lG\heR$. Let $\lU_0$ and $\lU_1$ be two compact real forms of $\lG$ and $\tau_0$, $\tau_1$ the associated involutions of $\lG$ (which are Cartan involutions of $\lG\heR$). For a suitable $\varphi\in\Int\lG\heR$, 
\[
   \tau_0=\varphi\tau_1\varphi^{-1}.
\]
The fact that $\Int\lG=\Int\lG\heR$ (lemma \ref{lem:Int_g_gR}) finish the proof.

\end{proof}
\subsection{Cartan decomposition}
%-------------------------------

Examples of Cartan and Iwasawa decomposition are given in sections \ref{SecToolSL}, \ref{SubSecIwaSOunn},\ref{subsecIwasawa_un}, \ref{SecSympleGp} and \ref{SecIwasldeuxC}. An example of how it works to prove isomorphism of Lie algebras is provided in subsection \ref{sssIsomsoslplussl}.

Let $\lF$ be a real semisimple Lie algebra. A vector space decomposition $\lF=\lK\oplus\lP$ is a \defe{Cartan decomposition}{Cartan!decomposition} if the Killing form is negative definite on $\lK$ and positive definite on $\lP$ and the following commutators hold:
\begin{equation}\label{eq:comm_Cartan}
   [\lK,\lK]\subseteq\lK,\quad[\lK,\lP]\subseteq\lP,\quad[\lP,\lP]\subseteq\lK.
\end{equation}
If $X\in\lK$ and $Y\in\lP$, we have $(\ad X\circ\ad Y)\lK\subseteq\lP$ and $(\ad X\circ\ad Y)\lP\subseteq\lK$, therefore $B_{\lF}(X,Y)=0$.

Let $\dpt{\theta}{\lF}{\lF}$ be a Cartan involution, $\lK$ its $+1$ eigenspace and $\lP$ his $-1$ one. It is easy to see that the relations \eqref{eq:comm_Cartan} are satisfied for the decomposition  $\lF=\lK\oplus\lP$. For example, for $X,X'\in\lK$, using the fact that $\theta$ is an automorphism, 
\[
   [X,X']=[\theta X,\theta X']=\theta[X,X'],
\]
which proves that $[\lK,\lK]\subseteq\lK$. Since $\theta$ is a Cartan involution, $B_{\theta}$ is positive definite. For $X\in\lK$,
\[
  B(X,X)=B(X,\theta X)=-B_{\theta}(X,X)
\]
proves that $B$ is negative definite on $\lK$; in the same way we find that $B$ is also positive definite on $\lP$. Then the Cartan involution gives rise to a Cartan decomposition. We are going to prove that any Cartan decomposition defines a Cartan involution.

Let us now do the converse. Let $\lF=\lK\oplus\lP$ be a Cartan decomposition of the real semisimple Lie algebra $\lF$. We define $\theta=\id|_{\lK}\oplus(-\id)|_{\lP}$. If $X,X'\in\lK$, the definition of a Cartan algebra makes $[X,X']\in\lK$ and so
\[
  \theta[X,X']=[X,X']=[\theta X,\theta X'],
\]
and so on, we prove that $\theta$ is an automorphism of $\mF$. It remains to prove that $B_{\theta}$ is positive definite. If $X\in\lK$,
\[
   B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X).
\]
Then $B_{\theta}$ is positive definite on $\lK$ because on this space, $B$ is negative definite by definition of a Cartan involution. The same trick shows that $B_{\theta}$ is also positive definite on $\lP$. We had seen that $\lP$ and $\lK$ where $B_{\theta}$-orthogonal spaces. Thus $B_{\theta}$ is positive definite and $\theta$ is a Cartan involution.

Let $\lF=\lK\oplus\lP$ be a Cartan decomposition. Then it is quite easy to see that $\lK\oplus i\lP$ is a compact real form of $\lG=(\lFeC)$.

\begin{proposition}
Let $\lL$ and $\lQ$ be the $+1$ and $-1$ eigenspaces of an involution $\sigma$. Then $\sigma$ is a Cartan involution if and only if $\lL\oplus i\lQ$ is  a compact real form of $\lFeC$.
\end{proposition}

\begin{proof}
First remark that $\lL\oplus i\lQ$ is always a real form of $\lFeC$. The direct sense is yet done. Then we suppose that $B_{\lFeC}$ is negative definite on $\lL\oplus i\lQ$ and we have to show that $\lL\oplus\lQ$ is a Cartan decomposition of $\lF$. The condition about the brackets on $\lL$ and $\lQ$ is clear from their definitions. If $X\in\lL$, $B(X,X)<0$ because $B$ is negative definite on $\lL$. If $Y\in\lQ$, $B(Y,Y)=-B(iY,iY)>0$ because $B$ is negative definite on $i\lQ$.
\end{proof}

\section{Root spaces in the real case}
%----------------------------------------

Let $\lF$ be a real semisimple Lie algebra with a Cartan involution $\theta$ and the corresponding Cartan decomposition $\lF=\lK\oplus\lP$. We consider $B$, a ``Killing like''{} form, i.e. $B$ is a symmetric nondegenerate invariant bilinear form on $\lF$ such that $B(X,Y)=B(\theta X,\theta Y)$ and $B_{\theta}:=-B(X,\theta X)$ is positive definite. Then $B$ is negative definite on the compact real form $\lK\oplus i\lP$. Indeed if $Y\in\lP$,
\begin{equation}
  B(iY,iY)=-B(\theta Y,\theta Y)
          =B(Y,\theta Y)
      =-B_{\theta}(Y,Y)<0.
\end{equation}
The case with $X\in\lK$ is similar. It is easy to see that $B_{\theta}$ is in fact a scalar product on $\lF$, so that we can define the orthogonality and the adjoint from $B_{\theta}$. If $\dpt{A}{\lF}{\lF}$ is an operator on $\lF$, his adjoint is the operator $A^*$ given by the formula
\[
   B_{\theta}(A X,Y)=B_{\theta}(X,A^*Y)
\]
for all $X$, $Y\in\lF$.

\begin{proposition}
With this definition, when $X\in\lF$, the adjoint operator of $\ad X$ is given by means of the Cartan involution:
\[ 
(\ad X)^*= \ad(\theta X). 
\]
\end{proposition}

\begin{proof}
This is a simple computation
\begin{equation}
B_{\theta}\big(  (\ad\theta X)Y,Z \big)=-B\big(  Y,[\theta X,\theta Y]  \big)
                                     =-B_{\theta}(Y,[X,Z])
                     =-B_{\theta}\big( (\ad X)^*Y,Z \big).
\end{equation}
\end{proof}

Let $\lA$ be a maximal abelian subalgebra of $\lP$ (the existence comes from the finiteness of the dimensions). If $H\in\lA$, the operator $\ad H$ is self-adjoint because
\begin{equation}
(\ad H)^*X=(-\ad\theta H)X
          =[H,X]
      =(\ad H)X,
\end{equation}
where we used the fact that $H\in\lP$.  For $\lambda\in\lA^*$, we define the space
\begin{equation}
  \lF_{\lambda}=\{ X\in\lF\tq\forall H\in\lA,\, (\ad H)X=\lambda(H)X\}.
\end{equation}
If $\lF_{\lambda}\neq 0$ and $\lambda\neq 0$, we say that $\lambda$ is a \defe{restricted root}{restricted root (real case)}\index{root!restricted (real case)} of $\lF$. We denote by $\Sigma$ the set of restricted roots of $\lF$. We may sometimes write $\Sigma_{\lF}$ if the Lie algebra is ambiguous.

The main properties of the real root spaces are given in the following proposition.

\begin{proposition}     \label{PropPropRacincesReelles}
The set $\Sigma$ of the restricted roots of a real semisimple Lie algebra $\lF$ has the following properties:
\label{prop:enuc}
\begin{enumerate}
\item\label{enuci} $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$,
\item\label{enucii} $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$,
\item\label{enuciii} $\theta\lF_{\lambda}=\lF_{-\lambda}$,
\item\label{enuciv} $\lambda\in\Sigma$ implies $-\lambda\in\Sigma$,
\item\label{enucv} $\lF_0=\lA\oplus\lM$ where $\lM=\mZ_{\lK}(\lA)$ and $\lA\perp\lM$.
\end{enumerate}
\end{proposition}

\begin{proof}
Proof of \ref{enuci}. The operators $\ad H$ with $H\in\lA$ form an abelian algebra of self-adjoint operators, then they are simultaneously diagonalisable. Let $\{X_i\}$ be a basis which realize this diagonalisation, and $\lF_i=\Span X_i$, so that $\lF=\oplus_i\lF_i$. We have $(\ad H)\lF_i=\lF_i$ and then $(\ad H)X_i=\lambda_i(H)X_i$ for a certain $\lambda_i\in\lA^*$. This shows that $\lF_i\subseteq\lF_{\lambda_i}$.\quext{pourquoi a n'implique pas que $\dim\lF_{\lambda_i}=1$ ? Rponse par Philippe : tu as oubli les valeurs propres nulles  dans ta base ce qui doit entrainer quelques modifs dans ton texte(par  ex.  $adH f_i = f_i$ pas toujours ) }

Proof of \ref{enucii}. Let $H\in\lA$, $X\in\lF_{\lambda}$ and $Y\in\lF_{\mu}$. We have
\begin{equation}
   (\ad H)[X,Y]=[[H,X],Y]+[X,[H,Y]]
               =\big(  \lambda(H)+\mu(H) \big) [X,Y].
\end{equation}

Proof of \ref{enuciii}. Using the fact that $\theta H=-H$ because $H\in\lP$,
\begin{equation}
  (\ad H)\theta X=\theta[\theta H,X]
                 =-\theta\lambda(H)X
         =-\lambda(H)(\theta X).
\end{equation}

Proof of \ref{enuciv}. It is a consequence of \ref{enuciii} because if $\lF_{\lambda}\neq 0$, then $\theta\lF_{_{\lambda}}\neq 0$.

Proof of \ref{enucv}. By \ref{enuciii}, $\theta\lF_0=\lF_0$, then $\lF_0=(\lK\cap\lF_0)\oplus(\lP\cap\lF_0)$. If $X\in\lF_0$, then it commutes with all the elements of $\lA$ and by the maximality property of $\lA$, provided that $X\in\lP$, it also must belongs to $\lA$. This fact makes $\lA=\lP\cap\lF_0$. Now,
\[
  \lM=\mZ_{\lK}(\lA)=\{X\in\lK\tq [X,\lA]=0\}=\lK\cap\lF_0.
\]
All this gives $\lF_0=\mZ_{\lK}(\lA)\oplus\lA$.
\end{proof}

We choose a positivity notion on $\lA^*$, we consider $\Sigma^+$, the set of restricted positive roots and we define\nomenclature{$\lN$}{Restricted roots}
\[
  \lN=\bigoplus_{\lambda\in\Sigma^+}\lF_{\lambda}.
\]

From finiteness of the dimension, there are only a finitely many forms $\lambda\in\lA^*$ such that $\lF_{\lambda}\neq 0$. Then, taking, more and more commutators in $\lN$, the formula $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$ shows that the result finish to fall into a $\lF_{\mu}=0$. On the other hand, since $\lA\subset\lF_0$, we have $[\lA,\lN]=\lN$. If $a_1,a_2\in\lA$ and $n_1,n_2\in\lN$,
\begin{equation}
   [a_1+n_1,a_2+n_2]=\underbrace{[a_1,a_2]}_{=0}+\underbrace{[a_1,n_2]}_{\in\lN}
                      \quad+\underbrace{[n_1,a_2]}_{\in\lN}+\underbrace{[n_1,n_2]}_{\in\lN},
\end{equation}
then $[\lA\oplus\lN,\lA\oplus\lN]=\lN$. This proves the three following important properties:

\begin{enumerate}
\item $\lN$ is nilpotent.
\item $\lA$ is abelian.
\item $\lA\oplus\lN$ is a solvable Lie subalgebra of $\lF$.
\end{enumerate}

\subsection{Iwasawa decomposition}
%----------------------------------

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra and $\lK$, $\lA$, $\lN$ as before. Then we have the following direct sum:
\begin{equation}
   \lF=\lK\oplus\lA\oplus\lN.
\end{equation}
\end{theorem}

This is the \defe{Iwasawa decomposition}{Iwasawa!decomposition}\index{decomposition!Iwasawa} for the real semisimple Lie algebra $\lF$.

\begin{proof}
We yet know the direct sum $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$. Roughly speaking, in $\lN$ we have only vectors of $\Sigma^+$, in $\theta\lN$, only of $\Sigma^-$ and in $\lA$, only in ``zero''. Then the sum $\lA\oplus\lN\oplus\theta\lN$ is direct.

Now we prove that the sum $\lK+\lA+\lN$ is also direct. It is clear that $\lA\cap\lN=0$ because $\lA\subseteq\lF_0$. Let $X\in\lK\cap(\lA\oplus\lN)$. Then $\theta X=X$. But $\theta X\in\lA\oplus\theta\lN$. Thus $X\in\lA\oplus\lN\cap\lA\oplus\lN$ which implies $X\in\lA$. All this makes $X\in\lP\oplus\lK$ and $X=0$.

Now we prove that $\lK\oplus\lA\oplus\lN=\lF$. An arbitrary $X\in\lF$ can be written as 
\[
   X=H+X_0+\sum_{\lambda\in\Sigma}X_{\lambda}
\]
where $H\in\lA$, $X_0\in\lM$ and $X_{\lambda}\in\lF_{\lambda}$. Now there are just some manipulations\ldots
\begin{equation}
  \sum_{\lambda\in\Sigma}X_{\lambda}=\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+X_{\lambda})
                                  =\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})
                  +\sum_{\lambda\in\Sigma^+}(X_{\lambda}+\theta X_{-\lambda}),
\end{equation}
but $\theta(X_{-\lambda}+\theta X_{-\lambda})=X_{-\lambda}+\theta X_{-\lambda}$, then $X_{-\lambda}+X_{-\lambda}\in\lK$. Moreover, $X_{\lambda}, \theta X_{-\lambda}\in\lF_{\lambda}$, then $X_{\lambda}-\theta X_{-\lambda}\in\lF_{\lambda}\subseteq\lN$. Then
\begin{equation}
  X=X_0+\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})+H+\sum_{\lambda\in\Sigma^+}(X_{\lambda}-\theta X_{-\lambda})
\end{equation}
where the two first term belong to $\lK$, $H\in\lA$ and the last term belongs to $\lN$.
\end{proof}

\begin{lemma}
There exists a basis $\{X_i\}$ of $\lF$ in which 

\begin{enumerate}
\item\label{enudi} The matrices of $\ad\lK$ are symmetric,
\item\label{enudii} The matrices of $\ad\lA$ are diagonal and real,
\item\label{enudiii} The matrices of $\ad\lN$ are upper triangular with zeros on the diagonal.
\end{enumerate}
\end{lemma}

\begin{proof}
We have the orthogonal decomposition $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$ given by proposition \ref{prop:enuc}. Let $\{X_i\}$ be an orthogonal basis of $\lF$ compatible with this decomposition and in such an order that $i<j$ implies $\lambda_i\geq\lambda_j$. From the orthogonality of the basis it follows that the matrix of $B_{\theta}$ is diagonal. Thus the adjoint is the transposition.

\ref{enudi} If $X\in\lK$, $(\ad X)^t=(\ad X)^*=-\ad\theta X=-\ad X$.

\ref{enudii} Each $X_i$ is a restricted root; then $(\ad H)X_i=\lambda_i(H)X_i$, then the diagonal of $\ad H$ is made of $\lambda_i(H)$ whose are real.

\ref{enudiii} If $Y_i\in\lF_{\lambda_i}$ with $\lambda_i\in\Sigma^+$, $(\ad Y_i)X_j$ has only components in $\lF_{\lambda_i+\lambda_j}$ with $\lambda_i+\lambda_j>\lambda_j$ because $\lambda_i\in\Sigma^+$.
\end{proof}


\begin{lemma}
Let $\lH$ be a subalgebra of the real semisimple Lie algebra $\lF$. Then $\lH$ is a Cartan subalgebra if and only if $\lHeC$ is Cartan in $\lFeC$.
\end{lemma}

\begin{proof}
\subdem{Direct sense} If $\lH$ is nilpotent in $\lF$, it is cleat that $\lHeC$ is nilpotent in $\lFeC$. We have to prove that $[x,\lHeC]\subseteq\lHeC$ implies $x\in\lHeC$. As set, $\lFeC=\mF\oplus i\lF$  (but not as vector space !), then we can write $x=a+ib$ with $a$, $b\in\lF$. The assumption makes that for any $h\in\lH$, there exists $h',h''\in\lH$ such that 
\[
   [a+ib,h]=h+ih''.
\]
This equation can be decomposed in $\lF$-part and $i\lF$-part: for any $h\in\lH$, there exists a $h'\in\lH$ such that $[a,h]=h'$,  and for any $h\in\lH$, there exists a $h''\in\lH$ such that $[b,h]=h''$. Thus $a$, $b\in\lH$ because $\lH$ is Cartan in $\lF$.

\subdem{Inverse sense} The assumption is that $[x,\lHeC]\subset\lHeC$ implies $x\in\lHeC$. In particular consider a $x\in\lH$ such that $[x,\lH]\subset\lH$. Then $x\in\lHeC$ because $[x,\lHeC]\subset\lHeC$. But $\lHeC\cap\lF=\lH$.
\end{proof}

In the complex case, the Cartan subalgebras all have same dimensions because they are maximal abelian.
