Source: \cite{UweLevy}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Lévy process}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let $G$ be a semigroup and $(\Omega,\mF,\eP)$ a probability space. Here $\mF$ is a $\sigma$-algebra and $\eP$ is a probability measure on $\Omega$.
\begin{definition}
    A \defe{Lévy process}{Lévy!process} is, for each $s$ and $t$ such that $0\leq s\leq t<\infty$, a map $X_{st}\colon (\Omega,\mF,\eP)\to (G,\mB)$ where $\mB$ is the Borel $\sigma$-algebra on $G$ such that
    \begin{enumerate}
        \item
            $\eP(X_{ss}=e)=1$,
        \item
            $X_{ss}=\lim_{t\searrow s}X_{st}$
        \item
            $X_{st}X_{tu}=X_{su}$,
        \item
            if $s_1\leq t_1\leq s_2\leq t_2\leq \ldots\leq t_n$, then the random variables $X_{s_1t_1}$, $X_{s_2t_2}$, \ldots, $X_{s_nt_n}$ are independent.
        \item
            $X_{s+h,t+h}=X_{st}$ in the sense that
            \begin{equation}
                \eP_{X_{st}}=\eP_{X_{s+h,t+h}}
            \end{equation}
            where $\eP_X(A)=\eP\big( X^{-1}(A) \big)$.
    \end{enumerate}
    If $G$ is a group, we add the condition $X_{st}=X_s^{-1}X_t$.
\end{definition}

The map $X\colon \Omega\to G$ gives rise to the map
\begin{equation}
    \begin{aligned}
        j_X\colon L^{\infty}(G,B)&\to L^{\infty}(\Omega,\mF,\eP) \\
        j_X(f)&=f\circ X 
    \end{aligned}
\end{equation}
Now we can look at the properties of a Lévy process on $j$ instead of $X$. Firstly, $j_X$ is a $*$-algebra homomorphism such that $j_X(1)=1$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Quantum probability space and stochastic processes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

A \defe{quantum probability space}{quantum!probability space} is a pair $(A,\Phi)$ is an unital $*$-algebra $A$ and a state $\Phi$ on $A$. A \defe{quantum random variable}{quantum!random variable} $j$ over a quantum probability space $(A,\Phi)$ on a $*$-algebra $B$ is an homomorphism $j\colon B\to A$ of $*$-algebras.

A \defe{quantum stochastic process}{quantum!stochastic process} is a family of quantum random variables $(j_j)_{t\in I}$. Its \defe{marginal distribution}{marginal!distribution}\index{distribution!marginal} is the set of maps
\begin{equation}
    \begin{aligned}
        \varphi_j\colon B&\to \eC \\
        \varphi_j&=\Phi\circ j_t. 
    \end{aligned}
\end{equation}

An \defe{operator process}{operator!process}\index{process!operator} if a family of elements $(X_t)_{t\in I}$ of a quantum probability space ($X_t\in A$). From an operator process and a choice of $b\in B$, we can define a stochastic process on $\eC\langle b,b^*\rangle$ by defining
\begin{equation}
    j_t(b)=X_t
\end{equation}
and extend it to $\eC\langle b,b^*\rangle$ as $*$-homomorphism.

On the other hand, if $\{ j_t\colon B\to A \}_{t\in I}$ is a stochastic process, we define an operator process by choosing $x\in B$ and defining
\begin{equation}
    X_t=j_t(x).
\end{equation}

\begin{definition}
    We say that maps $f_i\colon B\to A$ are \defe{tensor, or boson independent}{independent!with respect to a state} with respect to $\Phi$ if 
    \begin{enumerate}
        \item
            $\Phi\big( f_1(b_1)\cdots f_n(b_n) \big)=\Phi\big( f_1(b_1) \big)\ldots\Phi\big( f_n(b_n) \big)$ for every $b_i\in B$,
        \item
            the images commute : $[f_k(b_k),f_l(b_l)]=0$ for every $k\neq l$.
    \end{enumerate}
\end{definition}

If $j_1,j_2\colon B\to A$ are maps from $B$ to an algebra $A$, we define the \defe{convolution}{convolution} by
\begin{equation}
    j_1*j_2=m_A\circ(j_1\otimes j_2)\circ\Delta.
\end{equation}

Notice that, if $j_1$ and $j_2$ are homomorphisms, it is not guaranteed in general that the convolution $j_1*j_2$ is an homomorphism. In the case of independent variables, however, it is true: if $j_1$ and $j_2$ are independent random variables, then $j_2*j_2$ is still a random variable because
\begin{equation}
    \begin{aligned}[]
        (j_1*j_2)(ab)&=m_A\circ(j_1\otimes j_2)\big( a_{(1)}b_{(1)}\otimes a_{(2)}b_{(2)} \big)\\
        &=j_1(a_{(1)})j_1(b_{(1)})j_2(a_{(2)})j_2(b_{(2)})\\
        &=j_1(a_{(1)})j_2(a_{(2)})j_1(b_{(1)})j_2(b_{(2)})\\
        &=(j_1*j_2)(a)(j_1*j_2)(b).
    \end{aligned}
\end{equation}
where $a_{(1)}b_{(1)}$ stands for $(ab)_{(1)}$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Lévy process}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Let $(B,\Delta,\epsilon)$ be a bialgebra. A \defe{Lévy process}{Lévy!process} on $(B,\Delta)$ over the quantum probability space $(A,\Phi)$ is a quantum stochastic process $(j_{st})_{0\leq s\leq t}$ which satisfies the following requirements
    \begin{description}
        \item[Increment property] We have
            \begin{subequations}
                \begin{align}
                    j_{rs}*j_{st}&=j_{rt}       &\text{for all $0\leq r\leq s\leq t$}\\
                    j_{tt}(b)&=\epsilon(b)1_A   &\text{for all $0\leq t$, $b\in B$}
                \end{align}
            \end{subequations}
            
        \item[Independence of increments]
            for every $n\in\eN$ and $0\leq s_1\leq t_1\leq s_2\leq\ldots\leq s_n\leq t_n$, the maps $j_{s_1t_1}$, \ldots $j_{s_nt_n}$ are independent with respect to $\Phi$.
        \item[Stationarity of the increments]
            the distribution $\varphi_{st}=j_{st}\circ j_{st}$ depends only on the difference $t-s$, in other words we have $\varphi_{s+h,t+h}=\varphi_{st}$ for every $0\leq s\leq t$ and $h\geq 0$.
        \item[Weak continuity]
            $\lim_{t\searrow s}j_{st}(a)=j_{ss}(a)$ for every $a\in A$.
    \end{description}
\end{definition}

One say that the process $j_{st}\colon B\to (A,\Phi)$ and $k_{st}\colon B\to (A',\Phi')$ are \defe{equivalent}{equivalence!of Lévy process} if
\begin{equation}
    \Phi\big( j_{s_1t_1}(b_1)\ldots j_{s_nt_n}(b_n) \big)=\Phi'\big( k_{s_1t_1}(b_1)\ldots k_{s_nt_n}(b_n) \big),
\end{equation}
that is if all the expectation values that we could compute are equal.

If $(j_{st})$ is a Lévy process, we define 
\begin{equation}
    \varphi_{t-s}=\Phi\circ j_{st}
\end{equation}
for $0\leq s\leq t$. This is well defined from the stationarity of increments. We can as well write $\varphi_t=\Phi\circ j_{0t}$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Schürmann triple}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}
    Let $B$ be an involutive coalgebra and $(\varphi_t)_{t\geq 0}$, a convolution semigroup of linear functionals on $B$. If we define
    \begin{equation}
        L=\lim_{t\searrow 0} \frac{1}{ t }(\varphi_t-\epsilon),
    \end{equation}
    then the following are equivalents:
    \begin{enumerate}
        \item
            the maps $\varphi_{t}$ are states on $B$;
        \item
            the map $L\colon B\to \eC$ satisfies $L(1_B)=0$ and is hermitian and conditionally positive.
    \end{enumerate} 
\end{proposition}

\begin{lemma}
    If we define $\varphi_t=\varphi_{0t}=\Phi\circ j_{0t}$, we get a convolution semigroup.
\end{lemma}

\begin{proof}
    The first condition comes from
    \begin{equation}
        \varphi_0(b)=\Phi\big( j_{00}(b) \big)=\Phi\big( \epsilon(b)1_{A} \big)=\epsilon(b)\Phi(1_{A})=\epsilon(b)
    \end{equation}
    since $\Phi(1_{A})=1$ because $\Phi$ is a state on $A$.

    Using the fact that $j_{0,s+t}=j_{0s}*j_{s,s+t}$, we have
    \begin{equation}        \label{EqvpsptaUn}
        \varphi_{s+t}(a)=\varphi_{0,s+t}(a)=\Phi\big( (j_{0s}*j_{s,s+t})(a) \big).
    \end{equation}
    But, 
    \begin{equation}
        (j_{0s}*j_{s,s+t})(a)=m\circ(j_{0s}\otimes j_{s,s+t})(a_{(1)}\otimes a_{(2)})=j_{0s}(a_{(1)})j_{s,s+t}(a_{(2)}).
    \end{equation}
    Thus, since the intervals $0,s$ and $s,s+t$ are independent, the equation \eqref{EqvpsptaUn} becomes
    \begin{equation}
        \begin{aligned}[]
            \varphi_{s+t}(a)&=\Phi\big( j_{0s}(a_{(1)})j_{s,s+t}(a_{(2)}) \big)\\
            &=\Phi\big( j_{0s}(a_{(1)}) \big)\Phi\big( j_{s,s+t}(a_{2}) \big)\\
            &=\varphi_s(a_{(1)})\varphi_t(a_{(2)})\\
            &=(\varphi_s\otimes\varphi_t)\Delta a\\
            &=(\varphi_s*\varphi_t)(a).
        \end{aligned}
    \end{equation}  
    It remains to be justified that $\Phi\big( j_{s,s+t}(a) \big)=\varphi_t(a)$. This comes from the fact that $\varphi_{s+h,t+h}=\varphi_{st}$, so that $\varphi_{s,s+t}=\varphi_{0+s,s+t}=\varphi_{0,t}$.
\end{proof}

A linear functional $\omega$ on a bialgebra $B$ is \defe{conditionally positive}{positive!conditionally}\index{conditionally positive} if $\omega(a^*a)\geq 0$ for every $a$ such that $\epsilon(a)=0$. The functional is \defe{Hermitian}{Hermitian!functional} if $\omega(a^*)=\overline{ \omega(a) }$.

In our case, we know that the functionals $\varphi_t$ are in fact states. We define
\begin{equation}
    L=\lim_{t\searrow 0} \frac{1}{ t }(\varphi_t-\epsilon).
\end{equation}
This is conditionally positive because
\begin{equation}
    L(a^*a)=\lim_{t\searrow 0} \frac{1}{ t }\big( \varphi_t(a^*a)-\epsilon(a^*a) \big)=\lim_{t\searrow 0} \frac{ \varphi_t(a^*a) }{ t }\geq 0
\end{equation}
because $\varphi_t$ is a state and thus positive.

\begin{definition}
    Let $B$ be an unital $*$-algebra with an unital hermitian character $\epsilon\colon B\to \eC$. A \defe{Schürmann triple}{Schürmann triple} on $(B,\epsilon)$ is a triple $(\rho,\eta,L)$ with
    \begin{enumerate}
        \item
            $\rho\colon B\to \aL(D)$ is a $*$-representation of $B$ on a pre-Hilbert space $D$.
        \item
            The map $\eta\colon B\to D$ is linear and
            \begin{equation}
                \eta(ab)=\rho(a)\eta(b)+\eta(a)\epsilon(b),
            \end{equation}
            such a map is called a $\rho$-$\epsilon$-1-\defe{cocycle}{cocycle!of a Schürmann triple}.
        \item
            The map $L\colon B\to \eC$ is an hermitian linear functional whose $\epsilon-\epsilon-2-$coboundary is the map
            \begin{equation}
                (a,b)\mapsto-\langle \eta(a^*), \eta(b)\rangle ,
            \end{equation}
            that means that
            \begin{equation}        \label{EqCondFiffAssetaL}
                -\langle \eta(a^*), \eta(b)\rangle =(\partial L)(a,b)=\epsilon(a)L(b)-L(ab)+L(a)\epsilon(b)
            \end{equation}
            for every $a,b\in B$.
    \end{enumerate}
\end{definition}

\begin{corollary}       \label{ItemPropCorSchr}
    A Schürmann triple has the following immediate properties.
    \begin{enumerate}
        \item
            $\eta(1)=0$.
        \item
            $L(1)=0$.
        \item       \label{ItemPropCorSchriii}
            The condition \eqref{EqCondFiffAssetaL} is equivalent to ask
            \begin{equation}        \label{EqConsSimplAssetaL}
                L(ab)=-\langle \eta(a^*), \eta(b)\rangle 
            \end{equation}
            for every $a,b\in K_1$.
    \end{enumerate}
    
\end{corollary}

\begin{proof}
    \begin{enumerate}
        \item
            Using the cocycle property and the fact that $\rho(1)$ is the identity,
            \begin{equation}
                \eta(1\cdot 1)=\rho(1)\eta(1)+\eta(1)\epsilon(1)=2\eta(1).
            \end{equation}
        \item
            Now, writing the compatibility relation \eqref{EqCondFiffAssetaL} with $a=b=1$ and taking into account $\eta(1)=0$ we have
            \begin{equation}
                0=\epsilon(1)L(1)-L(1\cdot 1)+L(1)\epsilon(1)=L(1).
            \end{equation}
        \item
            If $a$ and $b$ belong to $K_1$, the relation \eqref{EqCondFiffAssetaL} reduces to \eqref{EqConsSimplAssetaL}. Now suppose that the condition \eqref{EqConsSimplAssetaL} holds for every $a$ and $b$ in $K_1$. Taking any $a,b\in B$ we consider $a-\epsilon(a)1$ and $b-\epsilon(b)1$ that are elements of $K_1$. Taking into account the fact that $L(1)=0$, we have
            \begin{equation}
                L(ab)-\epsilon(b)L(a)-\epsilon(a)L(b)=\langle \eta(a^*), \eta(b)\rangle .
            \end{equation}
    \end{enumerate}    
\end{proof}


Let $(\pi,\eta,L)$ be a Schürmann triple on the compact quantum group $\mA_q$ (see section \ref{SecGeneratorsonSUQn}). The map $\eta\colon \mA_q\to \oL(D) $ satisfies the cocycle condition
\begin{equation}
    \eta(ab)=\pi(a)\eta(b)+\eta(a)\epsilon(b).
\end{equation}

\begin{proposition}     \label{PropCocycleDeteretavjnmu}
    The cocycle $\eta$ is determined by its values on the elements $v_j^*$ with $j=1,\ldots,n-1$.
\end{proposition}

\begin{proof}
    Since $\epsilon(1)=1$, we have
    \begin{equation}
        \eta(1)=\pi(1)\eta(1)+\eta(1)\epsilon(1)=\big( \pi(1)+1 \big)\eta(1).
    \end{equation}
    Since the representation is nondegenerate we have $\pi(1)=\id$. The only possibility is thus $\eta(1)=0$.

    If $a$ and $b$ belong to $K_1$ we have $\eta(ab)=\pi(a)\eta(b)$. Since each element in $\mA_q$ is a polynomial in $u_{ij}$ and $u_{ij}^*$, we only have to fix the value of $\eta$ on these elements.

    First, $\eta(u_{jj}^*)=\eta(v_j^*)$ because $\eta(1)=0$. Using lemma \ref{lestmunijdiff}, taking the adjoint,
    \begin{equation}
        q\eta(u_{ll}^*u_{ij}^*)=\eta(u_{ij}^*u_{ll}^*).
    \end{equation}
    Using the cocycle property and the fact that $\epsilon(u_{ll}^*)=1$,
    \begin{equation}
        \big( q\pi(u_{ll}^*)-\id \big)\eta(u_{ij}^*)=\pi(u_{ij}^*)\eta(u_{ll}^*).
    \end{equation}
    The operator $q\pi(u_{li}^*)-\id$ is invertible because of lemma \ref{Propqpiideinve} and $q<1$. Thus, when $i\neq j$ and $i,j\leq n-1$, 
    \begin{equation}
        \eta(u_{ij}^*)=\big( q\pi(u_{li})^*-\id \big)^{-1}\pi(u_{ij}^*)\eta(u_{ll}^*).
    \end{equation}

    Let us now compute $\eta(v_j)$ for $j=1,\ldots,n-1$. Taking the relation \eqref{Equustrunsuqn},
    \begin{equation}
        \begin{aligned}[]
            0=\eta(1)&=\sum_{p=1}^n\eta(u_{jp}u_{jp}^*)\\
            &=\eta(u_{jj}u_{jj}^*)+\sum_{p\neq j}\eta(u_{jp}u_{jp}^*)\\
            &=\pi(u_{jj})\eta(u_{jj}^*)+\eta(u_{jj})\underbrace{\epsilon(u_{jj}^*)}_{=1}+\sum_{p\neq j}\eta(u_{jp}u_{jp}^*),
        \end{aligned}
    \end{equation}
    so that
    \begin{equation}        \label{Eqetaujjpiu}
        \eta(v_j)=\eta(u_{jj})=-\pi(u_{jj})\eta(u_{jj}^*)-\sum_p\pi(u_{jj})\eta(u_{jp}^*).
    \end{equation}
    Thus the vectors $\eta(u_{jj})$ and $\eta(v_j)$ are fixed for $j\leq n-1$.

    We are going to compute $\eta(u_{ij})$ with $i\neq j$ and $i,j\leq n-1$ using the lemma \ref{lestmaxjdiff}. We pose $k=\max(i,j)$. Since $\epsilon(u_{kk})=1$ and $\epsilon(u_{ij})=0$, the relation $\eta(u_{ij}u_{kk})=q\eta(u_{kk}u_{ij})$ leads to
    \begin{equation}
        \pi(u_{ij})\eta(u_{kk})=q\pi(u_{kk})\eta(u_{ij}).
    \end{equation}
    What we obtain is
    \begin{equation}
        \eta(u_{ij})=-\big( \id-q\pi(u_{kk}) \big)^{-1}\pi(u_{ij})\eta(u_{kk}).
    \end{equation}

    In order to compute the value of $\eta(v_n^*)$, we start from the definition \eqref{EqDefInvolutionSSUqn} of the involution. Taking into account the fact that in our case $D=1$,
    \begin{equation}
        u_{nn}^*=D^{nn}=\sum_{\sigma\colon \{ 1\ldots n-1 \}\to \{ 1,\ldots,n-1 \}}(-1)^{| \sigma |}u_{1\sigma(1)}\ldots u_{n-1,\sigma(n-1)}.
    \end{equation}
    Taking $\eta$ and using the cocycle property we have
    \begin{equation}
        \begin{aligned}[]
            \eta(v_n^*)&=\eta(u_{nn}^*)\\
            &=\sum_{\sigma(n-1)\neq n-1}(-1)^{| \sigma |}\pi\big( u_{1\sigma(1)}\ldots u_{n-2,\sigma(n-1)} \big)\eta(u_{n-1,\sigma(n-1)})\\
            &\quad+\sum_{\sigma(n-1)=n-1}(-1)^{| \sigma |}\pi\big( u_{1\sigma(1)}\ldots u_{n-2,\sigma(n-2)} \big)\eta(u_{n-1,n-1})\\
            &\quad+\sum_{\sigma(n-1)=n-1}(-1)^{| \sigma |}\eta\big( u_{1\sigma(1)}\ldots u_{n-2,\sigma(n-2)} \big)\underbrace{\epsilon(u_{n-1,n-1})}_{=1}
        \end{aligned}
    \end{equation}
    Applying many times the cocycle the term $\eta\big( u_{1\sigma(1)}\ldots u_{n-2,\sigma(n-2)} \big)$ decomposes into basic elements containing $\eta(u_{i\sigma(i)})$. This fixes $\eta(v_n^*)$.
   
    Taking the equation \eqref{Eqetaujjpiu} with $j=n$ we have 
    \begin{equation}
        \eta(v_n)=\eta(u_{nn})=-\pi(u_{nn})\eta(u_{nn}^*)-\sum_{p=1}^n\pi(u_{nn})\eta(u_{np}^*).
    \end{equation}
    That fixes $\eta(v_n)$. Now we consider the previously proved equation
    \begin{equation}
        \eta(u_{ij})=\big( q\pi(u_{kk})-\id \big)^{-1}\pi(u_{ij})\eta(v_k)
    \end{equation}
    with $i\neq j$ and $n=\max(i,j)$. Then
    \begin{equation}
        \eta(u_{ij})=\big( q\pi(u_{nn})-\id \big)^{-1}\pi(u_{ij})\eta(v_n).
    \end{equation}
    This fixes $\eta(u_{in})$ and $\eta(u_{nj})$.

\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{A representation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}     \label{PropReprezThetasuqn}
    The following is a one dimensional representation of $\SU_q(n)$ if $\sum_k\theta_k=1$:
    \begin{equation}
        \pi(u_{jk})= e^{i\theta_j}\delta_{jk}
    \end{equation}
\end{proposition}

\begin{proof}
    First we have
    \begin{equation}
        \pi(D)=\sum_{\sigma\in S(n)}(-q)^{| \sigma |} e^{i\theta_1}\delta_{1\sigma(1)}\ldots e^{i\theta_n}\delta_{n\sigma(n)}= e^{i(\theta_1+\cdots+\theta_n)}=1.
    \end{equation}
    Equations \eqref{SUBEquuijcondiv} all reduce to $0=0$ because of the Kronecker delta's and the condition on the indices.
\end{proof}

Since the condition $D=1$ imposes the sum of the $\theta$'s to be zero, we will write the representation $\pi$ by
\begin{equation}
    \epsilon_{\theta_1\ldots\theta_{n-1}}(u_{ij})= e^{i\theta_j}\delta_{jk}.
\end{equation}
Notice that the counit $\epsilon$ is $\epsilon_{0,\ldots,0}$. We define
\begin{equation}
    \epsilon'_k=\left.\frac{ \partial  }{ \partial \theta_k }\epsilon_{\theta_1\ldots\theta_{n-1}}\right|_{\theta_k=0}.
\end{equation}

As an example, in $\SU_q(3)$ we have
\begin{equation}
    \epsilon_{\theta_1\theta_2}=
    \begin{pmatrix}
        e^{i\theta_1}    &   0    &   0    \\
        0    &    e^{i\theta_2}    &   0    \\
        0    &   0    &    e^{-i(\theta_1+\theta_2)}
    \end{pmatrix}
\end{equation}
in the sense that $\epsilon_{\theta_1\theta_2}(u_{ik})=\big( \epsilon_{\theta_1\theta_2} \big)_{ik}$. Thus we have
\begin{equation}
    \begin{aligned}[]
        \epsilon'_{\theta_1}(u_{ik})&=\Dsdd{ \epsilon_{\theta_1\theta_2}(u_{ik}) }{\theta_1}{0}\\
        &=\left.\frac{ d }{ d\theta_1 }
        \begin{pmatrix}
            e^{i\theta_1}    &   0    &   0    \\
            0    &    e^{i\theta_2}    &   0    \\
            0    &   0    &    e^{-i(\theta_1+\theta_2)}
        \end{pmatrix}_{ik}\right|_{\theta_1=0}\\
        &=\begin{pmatrix}
            i    &   0    &   0    \\
            0    &   0    &   0    \\
            0    &   0    &   -i
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Higher order derivative are defined the same way:
\begin{equation}
    \epsilon''_{kl}=\left.\frac{ \partial  }{ \partial \theta_k }\frac{ \partial  }{ \partial \theta_l }\epsilon_{\theta_1\ldots\theta_{n-1}}\right|_{\substack{\theta_k=0\\\theta_l=0}}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ideals}
%---------------------------------------------------------------------------------------------------------------------------

Let $\mA_q$ be the Hopf $C^*$-subalgebra of $\SU_q(n)$ generated by $\{ 1,u_{ij} \}$, and a Schürmann triple $(\pi,\eta,L)$ where $\pi$ is a representation of $\mA_q$ on $\opB(\hH)$. We define the ideal
\begin{equation}
    K_1=\ker(\epsilon)\subset\mA_q.
\end{equation}
The ideal $K_1$ is generated by the elements of the form $u_{ij}$ ($i\neq j$) and $u_{ii}-1$. Then we define
\begin{equation}
    K_2=\Span\{ ab\tq a,b\in K_1 \}
\end{equation}
and more generally\nomenclature[Q]{$K_m$}{Ideal in $\SU_q(n)$}
\begin{equation}
    K_n=\Span\{ a_1\ldots a_n\tq a_i\in K_1 \}.
\end{equation}
We also define
\begin{equation}
    K_{\infty}=\bigcap_{m\geq 1}K_m.
\end{equation}


Each of these $K_m$ is a two-sided ideal since
\begin{equation}
    ba_1\ldots a_m=(ba_1)a_2\ldots a_m
\end{equation}
belongs to $K_m$ if $a_1\ldots a_m$ belongs to $K_m$.

We define 
\begin{equation}
    \begin{aligned}[]
        v_j&=u_{jj}-1\\
        d_j&=\frac{ v_j-v_j^* }{2i}
    \end{aligned}
\end{equation}
Since $\epsilon(1)=1$, the unit does not belong to $K_1$, but the elements $u_{ij}$, $u_{ij}^*$, $v_j$ and $d_j$ belong to $K_1$ when $i\neq j$.

\begin{lemma}       \label{Lemuijvkkuij}
    If $k=\max(i,j)$ we have
    \begin{equation}
        u_{ij}v_k-qv_ku_{ij}=(q-1)u_{ij}.
    \end{equation}
    
\end{lemma}

\begin{proof}
    This is a computation:
    \begin{equation}
        \begin{aligned}[]
            u_{ij}v_k&=u_{ij}u_{kk}-u_{ij}\\
            &=qu_{kk}u_{ij}-u_{ij}      &\text{because $k=\max(i,j)$}\\
            &=(qu_{kk}-1)u_{ij}\\
            &=(qu_{kk}-q)u_{ij}+(q-1)u_{ij}\\
            &=qv_ku_{ij}+(q-1)u_{ij}.
        \end{aligned}
    \end{equation}
    The proof is completed.
\end{proof}

\begin{lemma}       \label{Lesvjvjvuuuujjiiv}
    We have
    \begin{equation}
        v_jv_j^*=u_{jj}u_{jj}^*-v_j-v_j^*-1.
    \end{equation}
    
\end{lemma}

\begin{proof}
    This is a computation:
    \begin{equation}
        \begin{aligned}[]
            v_jv_j^*&=(u_{jj}-1)(u_{jj}^*-1)\\
            &=u_{jj}u_{jj}^*-u_{jj}-u_{jj}^*+1\\
            &=u_{jj}u_{jj}^^**-v_j-u_{jj}^*+1-1\\
            &=u_{jj}u_{jj}^*-v_j-v_j^*-1.
        \end{aligned}
    \end{equation}
    
\end{proof}


\begin{proposition}\label{PropuudansKKiii}
    We have
    \begin{enumerate}
        \item
            $u_{ij}\in K_{\infty}$ if $i\neq j$;
        \item       \label{ItemuudansKKii}
            $1-u_{jj}u_{jj}^*\in K_{\infty}$;
        \item\label{ItemuudansKKiii}
            $u_{ii}u_{jj}-u_{jj}u_{ii}\in K_{\infty}$;
        \item\label{ItemuudansKKiv}
            $u_{ii}u_{jj}^*-u_{jj}^*u_{ii}\in K_{\infty}$;
        \item
            $v_j+v_j^*\in K_2$;
        \item
            $\sum_k v_k\in K_2$;
        \item       \label{ItemPropuudansKKiiiavii}
            $\sum_{k=1}^nd_k\in K_2$.
    \end{enumerate}
    
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Since $\epsilon(u_{ij})=\delta_{ij}$, we have $u_{ij}\in K_1$ when $i\neq j$

            Now, using lemma \ref{Lemuijvkkuij} we have
            \begin{equation}
                u_{ij}=(q-1)^{-1}(u_{ij}v_k-qv_ku_{ij}).
            \end{equation}
        \item
            Since $\epsilon(u_{jj})=1$ we have $1-u_{jj}u_{jj}^*\in K_1$. Using formula \eqref{Equustrunsuqn}, we have
            \begin{equation}
                \begin{aligned}[]
                    1-u_{jj}u_{jj}^*&=\delta_{jj}-u_{jj}u_{jj}^*\\
                    &=\sum_ku_{jk}u_{jk}^*-u_{jj}u_{jj}^*\\
                    &=\sum_{k\neq j}u_{jk}u_{jk}^*.
                    \end{aligned}
            \end{equation}
            Since each of the terms in that sum belong to $K_{\infty}$, we have the result.
        \item
            The combination $u_{ii}u_{jj}-u_{jj}u_{ii}$ belongs to $\ker(\epsilon)=K_1$. Let us suppose $i<j$ (if not, consider change the sign). Using the relation \eqref{subEquuijcondiv}, we have
            \begin{equation}
                u_{ii}u_{jj}-u_{jj}u_{ii}=(q-q^{-1})u_{uj}u_{ji}\in K_{\infty}.
            \end{equation}
        \item
            Let's begin with $i=j$. We have
            \begin{equation}
                u_{ii}u_{ii}^*-u_{ii}^*u_{ii}=u_{ii}u_{ii}^*-1+1-u_{ii}^*u_{ii}\in K_{\infty}
            \end{equation}
            where we used the point \ref{ItemuudansKKii}.

            If $i\neq j$, we use the relation \eqref{eqREflsuusikl} which says that $u_{ii}u_{jj}^*=u_{jj}^*u_{ii}$, so that the combination we are looking at is zero.
        \item
            Lemma \ref{Lesvjvjvuuuujjiiv} shows that
            \begin{equation}        \label{Eqvvuiiujjvv}
                v_j+v_j^*=u_{jj}u_{jj}^*-1-v_jv_j^*
            \end{equation}
            The fact that $v_j\in K_1$ and item \ref{ItemuudansKKii} show that the right hand side belong to $K_2$.        
        \item
            Let us decompose the sum defining the determinant:
            \begin{equation}
                1=\sum_{\sigma\in S_n}(-q)^{| \sigma |}u_{1\sigma(1)}\ldots u_{n\sigma(n)}=u_{11}\ldots u_{nn}+\sum_{\sigma\neq \id}(-q)^{| \sigma |}u_{1\sigma(1)}\ldots u_{n\sigma(n)}.
            \end{equation}
            The last sum belongs to $K_{\infty}$ since there is at least one $k$ with $k\neq \sigma(k)$. Thus, replacing $u_{jj}$ by $v_j+1$,  we have
            \begin{equation}        \label{Eqinteunvuvuuvnldots}
                1=(v_1+1)\ldots (v_n+1)+k_{\infty}.
            \end{equation}
            The product can be written under the form
            \begin{equation}
                (v_1+1)\ldots(v_n+1)=\sum_{p=1}^n\sum_{i_1<\ldots<i_p}v_{i_1}\ldots v_{i_p}+1.
            \end{equation}
            In the latter sum, a part of the term $p=1$, each term belongs to $K_2$, so equation \eqref{Eqinteunvuvuuvnldots} reads
            \begin{equation}
                1=\sum_{i_1}v_{i_1}+k_2+k_{\infty},
            \end{equation}
            and we conclude that $\sum_iv_i=k_2+k_{\infty}\in K_2$.
        \item
            We know that $v_1^*+\cdots +v_n^*\in K_2$, so that
            \begin{equation}
                d_1+\cdots+ d_n=\frac{1}{ 2i }\big( (v_1+\ldots v_n)-(v_1^*+\cdots +v_n^*) \big)\in K_2.
            \end{equation}
    \end{enumerate}
    
\end{proof}

\begin{proposition}
    We have
    \begin{equation}
        \mA_q/K_{\infty}\simeq C(\eT^{n-1})
    \end{equation}
    
\end{proposition}

\begin{proof}
    Since $u_{ij}$ belongs to $K_{\infty}$ when $i\neq j$, the algebra $\mA_q/K_{\infty}$ is generated by the elements $u_{jj}$ and $u_{jj}^*$ as well as $1$. That algebra is commutative from points \ref{ItemuudansKKiii} and \ref{ItemuudansKKiv} of proposition \ref{PropuudansKKiii}.

    We consider the following map\footnote{Here $\Delta(\mA_q/K_{\infty})$ stands for the structure space of $\mA_q/K_{\infty}$, see definition \ref{DefStructureSpaceDel}.}:
    \begin{equation}
        \begin{aligned}
            \varphi\colon \eT^{n-1}&\to \Delta(\mA_q/K_{\infty}) \\
            t_1,\ldots t_{n-1}&\mapsto \omega_{t_1,\ldots t_{n-1}} 
        \end{aligned}
    \end{equation}
    where $\omega_t\colon \mA_q/K_{\infty}\to \eC$ ($t\in\eT^{n-1}$) is defined by
    \begin{equation}
        \omega_t(u_{kk})=\begin{cases}
            e^{it_k}    &   \text{if $k\neq n$}\\
            e^{-i(t_1+\cdots+t_{n-1})}  &    \text{if $k=n$}
        \end{cases}
    \end{equation}
    and $\omega_t(1)=1$. Notice that the value of $\omega_t$ on $u_{nn}$ is imposed by the fact that $\omega_t(1-u_{11}\ldots u_{nn})=0$ and the fact that $\omega_t$ has to be multiplicative.

    The map $\varphi$ is injective because the value of $\varphi(t_1,\ldots,t_{n-1})$ on the element $u_{kk}$ fixes the values of $t_k$.

    It is also surjective because $1-u_{jj}u_{jj}^*$ belongs to $K_{\infty}$; thus each $\omega$ in $\Delta(\mA_q/K_{\infty})$ satisfies $\omega(u_{kk}u_{kk}^*)=1$ and
    \begin{equation}
        \omega(u_{kk})\omega(u_{kk})^*=1,
    \end{equation}
    so that there exists a unique $t_k\in\mathopen[ 0 , 2\pi [$ such that $\omega(u_{kk})= e^{it_k}$.

    Now we conclude the proof using the Gelfand theorem \ref{thoGelfand} which states that for every $C^*$-algebra $\cA$, we have $\cA\simeq C\big( \Delta(\cA) \big)$. Indeed, we just proved that 
    \begin{equation}
        \eT^{n-1}\simeq\Delta(\mA_q/K_{\infty}).
    \end{equation}
    Thus we have
    \begin{equation}
        C(\eT^{n-1})\simeq C\big( \Delta(\mA_q/K_{\infty}) \big)\simeq\mA_q/K_{\infty}.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Decomposition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    Every element $x\in\cA_q$ decomposes into
    \begin{equation}
        x=c_01+\sum_{j=1}^{n-1}c_jd_j+\sum_{1\leq i\leq j\leq n}x_{ij}v_iv_j^*+k_3
    \end{equation}
    for some $c_0,c_j,c_{ij}\in \eC$ and $k_3\in K_3$.
\end{proposition}

\begin{proof}
    First, $x$ is a complex polynomial in the variables $u_{ij}$ and $u_{ij}^*$. If we replace $u_{jj}$ by $v_j+1$, we have a polynomial
    \begin{equation}
        x=p(v_j,v_j^*,u_{ij},u_{ij}^*)
    \end{equation}
    with $i\neq j$. We know that $u_{ij}\in K_{\infty}$ (proposition \ref{PropuudansKKiii}), so that, modulo elements of $K_3$, we can neglect the terms with $u_{ij}$ ($i\neq j$). It remains a polynomial in the variables $v_j$ and $v_j^*$. The terms of order $m$ belong to $K_m$, so that we only have to consider the first two terms. What we have is then a polynomial of degree $2$ in $v_j$ and $v_j^*$ :
    \begin{equation}        \label{EqxSumvvdgammaUn}
        x=c_01+\sum_{j=1}^n(\alpha_jv_j+\beta_jv_j^*)+\sum_{i,j=1}^n\sum_{\epsilon_1,\epsilon_2\in\{ 1,* \}}\gamma_{ij}^{\epsilon_1,\epsilon_2}v_i^{\epsilon_1}v_j^{\epsilon_2}+k_3.
    \end{equation}
    Here the sum over $\epsilon_i$ means that we have the terms $v_iv_j$, $v_iv_j^*$, $v_i^*v_j$ and $v_i^*v_j^*$.

    The first degree terms are of the form
    \begin{equation}
        \alpha_j v_j+\beta_jv^*_j=\tilde c(\alpha,\beta)\frac{ v_j+v_j^* }{ 2 }+c(\alpha,\beta)\frac{ v_j-v_j^* }{ 2i }.
    \end{equation}
    Since
    \begin{equation}
        d_n=d_1+\cdots+d_{n-1}+k_2,
    \end{equation}
    up to changing the coefficients of the $d_k$'s ($j\leq n-1$) and in the higher order terms, we can reduce the sum to $\sum_{j=1}^{n-1}d_j$.


    So let's say that
    \begin{equation}
        \sum_{j=1}^n c_jd_j\in K_1/K_2.
    \end{equation}
    From element \ref{ItemPropuudansKKiiiavii} of proposition \ref{PropuudansKKiii}, we know that there exists an element $k_2\in K_2$ such that
    \begin{equation}
        d_n=k_2-d_1-\ldots -d_{n-1},
    \end{equation}
    so that in $K_1/K_2$ we have $d_n=-\sum_{j=1}^{n-1}d_j$ and a basis of $K_1/K_2$ is
    \begin{equation}
        B_1=\{ d_1,\ldots,d_{j-1} \}.
    \end{equation}
    \begin{probleme}
        Ok, this is a generating part for $K_1/K_2$. Why is it free ? In what sense ?
    \end{probleme}
    
    The decomposition \eqref{EqxSumvvdgammaUn} reads now
    \begin{equation}        \label{EqxSumvvdgammaDeux}
        x=c_01+\sum_{j=1}^{n-1}c_jd_j+\sum_{j=1}^n\tilde c_j(v_j+v_j^*)+\sum_{i,j=1}^n\sum_{\epsilon_1,\epsilon_2}\gamma_{ij}^{\epsilon_1,\epsilon_2}v_i^{\epsilon_1}v_j^{\epsilon_2}+k_3.
    \end{equation}
    Using relation \eqref{Eqvvuiiujjvv},
    \begin{equation}
        v_j+v_j^*=v_jv_j^*+\underbrace{1-u_{jj}u_{jj}^*}_{\in K_{\infty}},
    \end{equation}
    We can replace $\sum_{j=1}^n\tilde c_j(c_j+v_j^*)$ by $\sum_{j=1}^n\tilde c_jv_jv_j^*$.

    Let us prove that modulo elements in $K_3$, the elements
    \begin{equation}
        \begin{aligned}[]
            v_iv_j,&&v_i^*v_j,&&v_i^*v_j^*
        \end{aligned}
    \end{equation}
    can be written as combinations of $v_iv_j^*$. Since $v_i+v_i^*\in K_2$, we have
    \begin{equation}
        (v_i+v_i^*)v_j\in K_3,
    \end{equation}
    so that $v_iv_j=v_i^*v_j+k_3$. Making the same with $(v_i+v_i^*)v_j^*$ and $v_i(v_j+v_j^*)$, we have
    \begin{equation}
        \sum_{\epsilon_1,\epsilon_2}\gamma_{ij}^{\epsilon_1,\epsilon_2}v_i^{\epsilon_1}v_j^{\epsilon_2}=\sum_{i,j=1}^nc_{ij}v_iv_j^*+k_3.
    \end{equation}
    We can continue the simplification because $v_iv_j^*=v_j^*v_i$ in $K_2/K_3$. Indeed
    \begin{equation}
        v_iv_j^*-v_j^*v_i=(u_{ii}-1)(u_{jj}^*-1)-(u_{jj}^*-1)(u_{ii}-1)=u_{ii}u_{jj}^*-u_{jj}^*u_{ii}\in K_{\infty}.
    \end{equation}
    Thus we have for example:
    \begin{equation}
        v_2v_1^*=v_1^*v_2=v_1v_2^*,
    \end{equation}
    and (up to redefinition of $c_{ij}$),
    \begin{equation}
        \sum_{i,j=1}^nc_{ij}v_iv_j^*=\sum_{1\leq i\leq j\leq n}c_{ij}v_iv_j^*+k_3.
    \end{equation}
    Since $v^*_1+\cdots+v^*_n\in K_2$, we have
    \begin{equation}
        v_iv_1^*+\cdots+v_iv^*_{n-1}+v_iv^*_n\in K_3,
    \end{equation}
    so that the term $v_iv^*_n$ is a combination of the terms $v_iv_j$ with $j<n$. Finally the sum \eqref{EqxSumvvdgammaDeux} reduces to
    \begin{equation}
        x=c_01+\sum_{j=1}^{n-1}c_jd_j+\sum_{1\leq i\leq j\leq n-1}c_{ij}v_iv_j^*+k_3.
    \end{equation}
\end{proof}



    The left hand side belongs to $K_1$ while the first term in the right hand side belongs to $K_2$ (see proposition \ref{PropuudansKKiii}).

    \begin{probleme}
        Why does it prove that $d_j\in K_1/K_2$ ?
        \begin{enumerate}
            \item
                $K_m$ is not a vector space. So when we write $K_i/K_{i+1}$, do we mean the \emph{set difference} or the class with respect to
                \begin{equation}
                    x\sim x+k_{i+1}
                \end{equation}
                or
                \begin{equation}
                    x\sim xk_{i+1}\quad ?
                \end{equation}
            \item
                Why $K_1/K_2$ could not be empty ?
            \item
                Why $\alpha v_j+\beta v_j^*$ does \emph{not} belong to $K_2$ ?
        \end{enumerate}
        
    \end{probleme}

The decomposition leads us to decompose a basis of $\suqA_q$ into
\begin{equation}
    B=\{ 1 \}\cup B_1\cup B_2\cup B_3
\end{equation}
where 
\begin{equation}
    \begin{aligned}[]
        B_1&=\{ d_i\tq 1\leq i\leq n-1 \}\\
        B_2&=\{ v_iv_j^*\tq 1\leq i\leq j\leq n-1 \}
    \end{aligned}
\end{equation}
and $B_3$ is a basis of whatever remains. We have $B_1\subset K_1$, $B_2\subset K_2$ and $B_3$ can be chosen as a subset of $K_3$.


We have the homomorphisms $\epsilon_{\theta}\colon \SU_q(n)\to \eC$ and we define the derivatives
\begin{equation}
    \begin{aligned}[]
        \epsilon'_k=\frac{ \partial  }{ \partial \theta_k }\epsilon_{\theta}|_{\theta_k=0}\\
        \epsilon''_{kl}=\frac{ \partial  }{ \partial \theta_l }\frac{ \partial  }{ \partial \theta_k }\epsilon_{\theta}|_{\theta_k=\theta_l=0}
    \end{aligned}
\end{equation}

\begin{proposition}
    The functionals $\epsilon$, $\epsilon'_{k}$ and $\epsilon''_{kl}$ with $1\leq k\leq j\leq n$ separate the points $1,d_1,\ldots,d_{n-1}$, $v_iv_j^*$ with $1\leq i\leq j\leq n-1$.
\end{proposition}

\begin{proof}
    We have to prove that for each $x$ in the enumerated elements, there exist a functional $\omega$ in the list such that $\omega(x)\neq 0$ and $\omega(y)\neq 0$ for $y\neq x$ where $x,y\in\{ 1,d_i,v_iv_j^* \}_{1\leq i\leq j\leq n-1}$.

    Let us begin with $x=1$. For him, the functional is $\omega=\epsilon$. We have $\epsilon(1)=1$ and $\epsilon(d_i)=\epsilon(v_iv_j^*)=0$. Let us pass to $x=d_j$. We prove that $\omega=\epsilon'_k$ with $k\neq j$ works. We have
    \begin{equation}
        \epsilon_{\theta}(d_j)=\frac{1}{ 2i }\big( \epsilon_{\theta}(u_{jj})-\epsilon_{\theta}(u_{jj}^*) \big)=\frac{1}{ 2i }\big(  e^{i\theta_j}- e^{-i\theta_j} \big).
    \end{equation}
    If $k\neq j$, we have $\epsilon'_k(d_j)=0$ while, if $j=k$, we have
    \begin{equation}
        \epsilon'_j(d_j)=\frac{1}{ 2i }\frac{ \partial  }{ \partial \theta_j }\big(  e^{i\theta_j}- e^{-i\theta_j} \big)_{\theta_j=0}=1.
    \end{equation}
    In summary,
    \begin{equation}
        \epsilon'(d_j)=\delta_{kj}.
    \end{equation}
    In order to see that $\epsilon'_k$ separates $d_k$, we still have to prove that $\epsilon'_k(v_iv_j^*)=0$ for every $i$ and $j$.

    \begin{probleme}
        It seems to me that the functional we are looking at are $\epsilon$ and its derivatives. We are not working with arbitrary $\theta_1,\ldots,\theta_{n-1}$. For arbitrary $\theta$, we have
        \begin{equation}
            \epsilon'_k(ab)=\frac{ \partial  }{ \partial \theta_k }\big( \epsilon_{\theta}(a)\epsilon_{\theta}(b) \big)=\epsilon'_k(a)\epsilon_{\theta}(b)+\epsilon_{\theta}(a)\epsilon'_k(b).
        \end{equation}
    \end{probleme}
\end{proof}

We consider the functionals
\begin{equation}
    a\mapsto\epsilon(\theta,a)=\epsilon_{\theta}(a)
\end{equation}
and the derivatives
\begin{equation}
    \begin{aligned}[]
        \epsilon'_l(\theta,a)&=\frac{ \partial  }{ \partial \theta_l }\big( \epsilon(\theta,a) \big)\\
        \epsilon''_{kl}(\theta,a)&=\frac{ \partial  }{ \partial \theta_k }\frac{ \partial  }{ \partial \theta_l }\big( \epsilon(\theta,a) \big)\\
    \end{aligned}
\end{equation}
We have the Leibnitz rules
\begin{equation}
    \begin{aligned}[]
        \epsilon'_l(\theta,ab)&=\frac{ \partial  }{ \partial \theta_l }\big( \epsilon(\theta,a)\epsilon(\theta,b) \big)\\
        &=\epsilon'_l(\theta,a)\epsilon(\theta,b)+\epsilon(\theta,a)\epsilon_l'(\theta,b),
    \end{aligned}
\end{equation}
and
\begin{equation}        \label{EqLeibnitzepsppkl}
    \begin{aligned}[]
        \epsilon''_{kl}(\theta,ab)&=\epsilon''_{kl}(\theta,a)\epsilon(\theta,b)+\epsilon'_l(\theta,a)\epsilon'_k(\theta,b)\\
        &\quad+\epsilon'_k(\theta,a)\epsilon'_l(\theta,b)+\epsilon(\theta,a)\epsilon''_{kl}(\theta,b).
    \end{aligned}
\end{equation}
We are interested in computing them on the basics elements $1$, $d_j$, $v_iv_j^*$ that are generating $\SU_q(n)$ modulo $K_3$. Computations show that
\begin{equation}
    \begin{aligned}[]
        \epsilon(\theta,1)&=1\\
        \epsilon(\theta,d_j)&=\frac{ 1 }{2}\big(  e^{i\theta_j}- e^{-i\theta_j} \big)&&\epsilon(0,d_j)=0\\
        \epsilon(\theta,v_j)&= e^{i\theta_j}-1&&\epsilon(0,v_j)=0\\
        \epsilon(\theta,v_iv_j^*)&=( e^{i\theta_i}-1)( e^{-i\theta_j}-1)&&\epsilon(0,v_iv_j^*)=0.
    \end{aligned}
\end{equation}
Taking the derivative,
\begin{equation}
    \begin{aligned}[]
        \epsilon'_l(\theta,1)&=0\\
        \epsilon'_l(\theta,d_j)&=\frac{ 1 }{2}i\delta_{lj}( e^{i\theta_l}+ e^{-i\theta_l})                             &&\epsilon'_l(0,d_j)=i\delta_{lj}\\
        \epsilon'_l(\theta,v_j)&=i\delta_{kl} e^{i\theta_j}                                                           &&\epsilon'_l(0,v_j)=i\delta_{lj}\\
        \epsilon'_l(\theta,v_iv_j^*)&=\delta_{ki} e^{i\theta_i}( e^{-i\theta_j}-1)+( e^{i\theta_i}-1)\delta_{kj} e^{-i\theta_j}     &&\epsilon'_l(0,v_iv_j^*)=0.
    \end{aligned}
\end{equation}

\begin{probleme}
    Pourquoi dans $\epsilon'_l(\theta,d_j)$, y'a pas un $i$ qui descent à cause de la dérivée ?
\end{probleme}

Taking once again the derivative,
\begin{equation}
    \begin{aligned}[]
        \epsilon''_{kl}(\theta,1)&=0\\
        \epsilon''_{kl}(\theta,d_j)&=\frac{ 1 }{2}\delta_{lj}\delta_{kl}( e^{i\theta_l}- e^{-i\theta_j})&&\epsilon''_{kl}(0,d_j)=0\\
        \epsilon''_{kl}(\theta,v_j)&=\delta_{lj}\delta_{kj} e^{i\theta_j}&&\epsilon''_{kl}(0,v_j)=\delta_{lj}\delta_{kj}
    \end{aligned}
\end{equation}
and
\begin{subequations}
    \begin{align}
        \epsilon''_{kl}(\theta,v_iv_j^*)&
        =-\delta_{ki}\delta_{li} e^{i\theta_i}( e^{-i\theta_j}-1)
        +\delta_{li}\delta_{kj} e^{i\theta_i} e^{-i\theta_j}\\
        &\quad-\delta_{kj}\delta_{lj} e^{-i\theta_j} ( e^{i\theta_i}-1)
        +\delta_{lj}\delta_{ki} e^{-i\theta_j} e^{i\theta_i}        \label{SubEqepsppsurvvs}
    \end{align}
\end{subequations}


From these results, we deduce the following proposition.
\begin{proposition}
    The functional family
    \begin{equation}
        \begin{aligned}[]
            a&\mapsto\epsilon(a)\\
            a&\mapsto\epsilon'_l(a)\\
            a&\mapsto\epsilon''_{kl}(a)\\
        \end{aligned}
    \end{equation}
    with $1\leq k\leq l\leq n-1$ separates the points $1$, $d_j$, $v_jv_j^*$ with $1\leq i\leq j\leq n-1$.
\end{proposition}

\begin{proof}
    For each point $x\in\{ 1,d_j,v_iv_j^* \}$, we have to find a functional $\omega$ in the given family such that $\omega(y)\neq 0$ if and only if $y=x$.

    For $x=1$, the functional $\epsilon$ makes the work:
    \begin{equation}
        \begin{aligned}[]
            \epsilon(1)\neq 0,&&\epsilon(d_j)=0,&&\epsilon(v_iv_j^*)=0.
        \end{aligned}
    \end{equation}
    The functional $\epsilon'_l$ separates $d_l$, indeed
    \begin{equation}
        \begin{aligned}[]
            \epsilon'_l(0,1)=0,&&\epsilon'_l(0,d_j)=\delta_{lj},&&\epsilon'_l(0,v_iv_j^*)=0.
        \end{aligned}
    \end{equation}
    And finally the functional $\epsilon''_{kl}$ separates $v_kv_l^*$ because
    \begin{equation}
        \begin{aligned}[]
            \epsilon''_kl(0,1)=0,&&\epsilon''_kl(0,d_j)=0,&&\epsilon''_kl(0,v_iv_j^*)=\delta_{li}\delta_{kj}+\delta_{ki}\delta_{kj}.
        \end{aligned}
    \end{equation}
    The last is nonzero if and only if $k=i$ and $l=j$.
\end{proof}


\begin{proposition}     \label{PropDecompxczepsApp}
    The decomposition
    \begin{equation}
        x=c_01+\sum_{j=1}^{n-1}c_jd_j+\sum_{1\leq i\leq j\leq n-1}c_{ij}v_iv_j^*+k_3
    \end{equation}
    is unique and $c_0=\epsilon(x)$, $c_j=\epsilon'_j(x)$ and $c_{ij}=\epsilon''_{ij}(x)$.
\end{proposition}

\begin{proof}
    Applying $\epsilon$ to $x$ we get $\epsilon(x)=c_0$. We also have
    \begin{equation}
        \epsilon'_l(x)=\sum_{j=1}^{n-1}c_j\underbrace{\epsilon'_l(d_j)}_{\delta_{lj}}=c_l
    \end{equation}
    and 
    \begin{equation}
        \epsilon''_{kl}(x)=\sum_{i\leq i\leq j\leq n-1}c_{ij}\delta_{lj}\delta_{kj}=c_{kl}.
    \end{equation}
    Then the element $k_3$ is unique as the difference
    \begin{equation}
        k_3=x-c_01-\sum c_jd_j-\sum c_{ij}v_iv_j^*.
    \end{equation}
\end{proof}

We define the map
\begin{equation}
    \begin{aligned}
        P\colon \mA_q&\to K_2 \ \\
        x&\mapsto x-\epsilon(x)1-\sum_{j=1}^{n-1}\epsilon'_j(x)d_j. 
    \end{aligned}
\end{equation}
This map satisfies because
\begin{equation}
    \begin{aligned}[]
        \epsilon\big( P(x) \big)&=\epsilon(x)-\epsilon(x)=0\\
        \epsilon'_k\big( P(x) \big)&=\epsilon'_k(x)-\sum\epsilon'_j(x)\epsilon'_k(d_j)=\epsilon'_k(x)-\epsilon'_k(x)=0,
    \end{aligned}
\end{equation}
so that
\begin{equation}
    P\big( P(x) \big)=P(x).
\end{equation}



%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Gaussian process}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We follow \cite{UweLevy}.

\begin{proposition}
    If $L$ is conditionally positive and hermitian, then the following conditions are equivalent:
    \begin{enumerate}
        \item
            $\eta=0$
        \item
            $L|_{K_2}=0$
        \item
            $L$ is a $\epsilon$-derivation, that means
            \begin{equation}
                L(ab)=\epsilon(a)L(b)+L(a)\epsilon(b)
            \end{equation}
            for every $ab,\in\mA$.
        \item
            The states $\varphi_t$ are homomorphisms : $\varphi_t(ab)=\varphi_t(a)\varphi_t(b)$ for every $a,b\in\mA$ and $t\geq 0$.
    \end{enumerate}
\end{proposition}
A Schürmann triple with such a $L$ is a \defe{drift}{drift!process}.

\begin{proposition}     \label{PropProcessusGaussien}
    Let $L$ be an hermitian conditionally positive linear functional on $\mA$. The following conditions are then equivalent:
    \begin{enumerate}
        \item
            $L|_{K_3}=0$;
        \item
            $\eta|_{K_2}=0$;
        \item
            $L(b^*b)=0$ for every $b\in K_2$;
        \item
            $\eta(ab)=\epsilon(a)\eta(b)+\eta(a)\epsilon(b)$ for every $a,b\in\mA$;
        \item
            $\pi(a)=\epsilon(a)1$ for every $a\in\mA$.
    \end{enumerate}
\end{proposition}
We say that the triple $(\pi,\eta,L)$ with a $L$ satisfying the above conditions describes a \defe{Gaussian}{Gaussian!Schürmann triple}.

\begin{proposition}
    A generator of a Gaussian process satisfies
    \begin{equation}
        L=\sum\alpha_k\epsilon'_k+\sum B_{ij}\epsilon''_{ij}
    \end{equation}
    where the $\epsilon_k'$ and $\epsilon''_{ij}$ are the derivatives of the representation defined in proposition \ref{PropReprezThetasuqn}.
\end{proposition}

We suppose now that $\pi$ is not Gaussian (that is $\pi$ is not $\epsilon 1$), but
\begin{equation}
    \pi=\pi_1\oplus\pi_2
\end{equation}
where $\pi_1$ is Gaussian.

\begin{probleme}
    Il est dit que l'espace sur lequel $\pi$ agit est 
    \begin{equation}
        H_1=\bigcap_{\alpha\in K_1}\ker\pi(a).
    \end{equation}
    Cependant par définition de $K_1$, $\epsilon(a)=0$ dès que $a\in K_1$, donc $\pi_1(a)=0$.

    Comment ça marche ?
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Poisson process}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropDefPoissonnL}
    Let $L\colon \mA\to \eC$ be a linear hermitian conditionally positive functional. Then the following conditions are equivalent:
    \begin{enumerate}
        \item
            there exists a state $\varphi\colon \mA\to \eC$ and a real $\lambda>0$ such that
            \begin{equation}
                L(b)=\lambda\big( \varphi(b)-\epsilon(b) \big)
            \end{equation}
            for every $b\in\mA$;
        \item
            there exists a Schürmann triple $(\pi,\eta,L)$ such that the cocycle is trivial, that is there exists a $h\in H$ such that
            \begin{equation}
                \eta(b)=\big( \pi(b)-\epsilon(b) \big)h.
            \end{equation}
    \end{enumerate}
\end{proposition}
A generator $L$ satisfying these properties is a \defe{Poisson generator}{Poisson!generator}. The map $\eta$ is the \defe{coboundary}{coboundary} of $h$.

\begin{probleme}
    Il faut expliquer cette histoire d'opérateurs $\mO$.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Gaussian cocycle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Let $\pi$ be a representation of $\mA$ on an Hilbert space $H$. We define
    \begin{equation}
        H_{\epsilon}=\bigcap_{a\in K_1}\ker\pi(a).
    \end{equation}
    The restriction of $\pi$ on $H_{\epsilon}$ is the \defe{Gaussian part}{Gaussian!part of a representation} of $\pi$.
\end{definition}
In order to see that this definition makes sense, we have to prove that $\pi(x)f\in H_{\epsilon}$ whenever $x\in\mA$ and $f\in H_{\epsilon}$. If $a\in K_1$, we have $\pi(a)\pi(x)f=\pi(ax)f=0$ because $K_1$ is an ideal.

\begin{lemma}       \label{LempiepsHepsUni}
    The representation respects the decomposition \(H=H_{\epsilon}\oplus H_{\epsilon}^{\perp}\). Namely,
    \begin{enumerate}
        \item
            The representation acts as the counit on its Gaussian part:
            \begin{equation}
                \pi(a)|_{H_{\epsilon}}=\epsilon(a)\id|_{H_{\epsilon}}.
            \end{equation}
        \item
            We have \(\pi(\suqA_q)H_{\epsilon}^{\perp}\subset H_{\epsilon}^{\perp}\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}
        \item
            Let $f\in H_{\epsilon}$. We use proposition \ref{PropDecompxczepsApp} taking into account the fact that $\pi(d_j)f=\pi(v_j^*)f=\pi(k_3)f=0$. It remains $\pi(x)f=c_0f=\epsilon(x)f$.
        \item
            Let \(h\in H_{\epsilon}^{\perp}\). For every \(v\in H_{\epsilon}\) we have \(\langle v, h\rangle =0\). If \(x\in\suqA_q\), we have
            \begin{equation}
                \langle v, \pi(x)h\rangle =\langle \pi(x^*)v, h\rangle =0
            \end{equation}
            since \(\pi(x^*)v=\epsilon(x^*)v\in H_{\epsilon}\).
    \end{enumerate}
\end{proof}

\begin{proposition}
    Let $w=\sum_{j=1}^nv_j^*v_j$. Then
    \begin{equation}
        H_{\epsilon}=\ker\pi(w)=\bigcap_{j=1}^n\ker\pi(v_j).
    \end{equation}
\end{proposition}

\begin{proof}
    Let $f\in H_{\epsilon}$. Since $\pi(v_j)f=0$ for every $j$, taking the sum we have $\pi(w)f=0$. Thus \( H_{\epsilon}\subset\ker\pi(w)\). Let now $f\in\ker\pi(w)$. We have
    \begin{equation}
        \begin{aligned}[]
            0&=\langle f, \pi(w)f\rangle \\
            &=\sum_{j=1}^n\langle \pi(v_j)f, \pi(v_j)f\rangle \\
            &=\sum_j\| \pi(v_j)f \|^2.
        \end{aligned}
    \end{equation}
    Thus for every $j$ we have $\| \pi(v_j)f \|=0$ and $f\in\ker\pi(v_j)$. That proves the inclusion
    \begin{equation}        \label{Eqkerpiwinclucapkervj}
        \ker\pi(w)\subset\bigcap_{j=1}^n\ker\pi(v_j).
    \end{equation}
    
    Let $f\in\ker\pi(v_j)$ and \( i<j\). We have $\pi(u_{jj})f=f$ and from \eqref{subEquuijcondii} we know that
    \begin{equation}
        \pi(u_{ij})f=\pi(u_{ij})\pi(u_{jj})f=q\pi(u_{jj})\pi(u_{ij})f,
    \end{equation}
    in other words,
    \begin{equation}
        \big[ \id-q\pi(u_{jj}) \big]\pi(u_{ij})f=0.
    \end{equation}
    By invertibility of the operator $\id-q\pi(u_{jj})$ is invertible (corollary \ref{CorOpOdquijInverti}), the latter implies $\pi(u_{ij})f=0$ and $f\in\bigcap_{i<j}\ker\pi(u_{ij})$. Thus we have
    \begin{equation}        \label{Eqketpivjsubipyjketpiuij}
        \ker\pi(v_j)\subset\bigcap_{i<j}\ker\pi(u_{ij}).
    \end{equation}
    Combining with \eqref{Eqkerpiwinclucapkervj}, we have
    \begin{equation}        \label{EqIntSubSintkerkera}
        \ker\pi(w)\subset\bigcap_{1\leq i<j\leq n}\ker\pi(u_{ij}).
    \end{equation}
    
    Let us now prove that
    \begin{equation}
        \bigcap_{j=1}^n\ker\pi(v_j)\subset\bigcap_{1\leq j<i\leq n}\ker\pi(u_{ij}).
    \end{equation}
    For that consider $f\in H$ such that $\pi(v_j)f=0$ for every $j=1,\cdots,n$ and apply $\pi$ to the relation \eqref{Equustrunsuqn}. For each $i$ we have
    \begin{equation}
        \id=\sum_{s=1}^n\pi(u_{si}^*)\pi(u_{si}).
    \end{equation}
    Applying to $f$ and cutting the sum in three parts,
    \begin{equation}
        f=\sum_{s<i}\pi(u_{si}^*)\pi(u_{si})f+\pi(u_{ii}^*)\pi(u_{ii})f+\sum_{s>i}\pi(u_{si}^*)\pi(u_{si})f.
    \end{equation}
    The inclusion \eqref{Eqketpivjsubipyjketpiuij} shows that $\pi(u_{si})f=0$ when $s<i$; in the same time $\pi(u_{ii})f=f$. We are thus left with
    \begin{equation}
        0=\sum_{s>i}\pi(u_{si}^*)\pi(u_{si})f
    \end{equation}
    That implies in particular that 
    \begin{equation}
        0=\sum_{s>i}\langle f, \pi(u_{si})^*\pi(u_{si})f\rangle =\sum_{s>i}\| \pi(u_{si})f \|,
    \end{equation}
    and that $\pi(u_{si})f=0$. What we just proved is that
    \begin{equation}
        \bigcap_{j=1}^n\ker\pi(v_j)\subset\bigcap_{1\leq j<i\leq n}\ker\pi(u_{ij}).
    \end{equation}
    Combining with \eqref{EqIntSubSintkerkera}, we have
    \begin{equation}
        \bigcap_{j=1}^n\ker\pi(v_j)\subset\bigcap_{i\neq j}\ker\pi(u_{ij}).
    \end{equation}
    Since $K_1$ is made of $u_{ij}$ ($i\neq j$) and $v_j$, we also have 
    \begin{equation}        \label{EqInclkerpivjHesp}
        \bigcap_{j=1}^n\ker\pi(v_j)\subset H_{\epsilon}
    \end{equation}
    and then $\ker \pi(w)\subset H_{\epsilon}$ because of \eqref{Eqkerpiwinclucapkervj}. The inclusion \eqref{EqInclkerpivjHesp} also shows that $\bigcap_{j=1}^n\ker\pi(v_j)=H_{\epsilon}$. This concludes the proof.
\end{proof}

\begin{proposition}
    Let $h_1,\ldots,h_n\in H$. If $(\pi,\eta,L)$ is a Gaussian triple and if $\eta$ satisfies $\eta(v_j^*)=h_j$ for $j=1,\ldots,n-1$, then $\eta$ reads
    \begin{equation}        \label{EqDefEtaCocyGaussU}
        \begin{aligned}
            \eta\colon \suqA_q&\to H \\
            a&\mapsto \sum_{k=1}^{n-1}i\epsilon_k'(a)h_k.
        \end{aligned}
    \end{equation}
\end{proposition}

\begin{proof}

    The first point to be shown is that the given $\eta$ is a Gaussian $\pi$-$\epsilon$-1-cocycle. For that we have
    \begin{equation}
        \begin{aligned}[]
            \eta(ab)&=\sum_{k=1}^{n-1}i\Big( \epsilon_k'(a)\epsilon(b)+\epsilon(a)\epsilon_k'(b) \Big)h_k\\
            &=\eta(a)\epsilon(b)+\epsilon(a)\eta(b).
        \end{aligned}
    \end{equation}
    Since $\pi$ is part of a Gaussian triple, it satisfies $\epsilon(a)=\pi(a)$. Thus we obtain the cocycle condition $\eta(ab)=\pi(a)\eta(b)+\eta(a)\epsilon(b)$. The fact for a cocycle to be Gaussian is $\eta|_{K_2}=0$ (proposition \ref{PropProcessusGaussien}). Here, since $\epsilon'_k(ab)=\epsilon'_k(a)\epsilon(b)+\epsilon(a)\epsilon'_k(b)$, we have
    \begin{equation}
        \eta(ab)=0
    \end{equation}
    whenever $a$ and $b$ belong to $K_1$. We proved that the map $\eta$ of equation \eqref{EqDefEtaCocyGaussU} is a Gaussian cocycle.

    Since $v_j=u_{jj}-1$, we have
    \begin{equation}
        \begin{aligned}[]
            \epsilon'_k(0,v_j^*&)=\frac{ \partial  }{ \partial \theta_k }\big[ \epsilon(\theta,v_j^*) \big]_{\theta=0}\\
            &=\frac{ \partial  }{ \partial \theta_k }\big[ \epsilon(\theta,u_{jj}^*)-\epsilon(\theta,1) \big]_{\theta=0}\\
            &=\frac{ \partial  }{ \partial \theta_k }\big[  e^{-i\theta_j} \big]\\
            &=-i\delta_{kj},
        \end{aligned}
    \end{equation}
    so that
    \begin{equation}
        \eta(v_j^*)=\sum_{k=1}^{n-1}i(-i)\delta_{kj}h_k=h_j.
    \end{equation}
    Since proposition \ref{PropCocycleDeteretavjnmu} states that a cocycle is determined by its values on the elements $v_j^*$ ($j=1,\ldots n-1$), the formula \eqref{EqDefEtaCocyGaussU} determines the unique cocycle such that $\eta(v_j^*)=h_j$.
\end{proof}


\begin{proposition}     \label{PropGaussCocycle}
    Every Gaussian cocycle reads
    \begin{equation}
        \eta(a)=\sum_{k=1}^{n-1}i\epsilon_k'(a)h_k
    \end{equation}
    with $h_k=\eta(v_j^*)$.
\end{proposition}

\begin{proof}
    For each $x\in\suqA_q$ we have the decomposition
    \begin{equation}
        x=\epsilon(x)1+\sum_{j=1}^{n-1}\epsilon_j'(x)d_j+k_2.
    \end{equation}
    Applying $\eta$ and taking into account $\eta(1)=\eta(k_2)=0$ (the second equality is because $\eta$ is Gaussian), we have
    \begin{equation}
        \eta(x)=\sum_{k=1}^{n-1}\epsilon_k'(x)\eta(d_k).
    \end{equation}
    It remains to be proven that $\eta(d_j)=i\eta(v_j^*)$. We have
    \begin{equation}
        \begin{aligned}[]
            \eta(v_j^*)=\eta(u_{jj}^*)&=\sum_k^{n-1}\epsilon_k'(u_{jj}^*)\eta(d_k)\\
            &=-\sum_{k=1}^{n-1}i\delta_{jk}\eta(d_k)\\
            &=-i\eta(d_j).
        \end{aligned}
    \end{equation}
    \begin{probleme}
        Ici, Anna fait un truc plus compliqué.
    \end{probleme}
    
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Gaussian generator}
%---------------------------------------------------------------------------------------------------------------------------

Following proposition \ref{PropDefPoissonnL}, a Gaussian generator has to vanish on $K_3$ and being hermitian, conditionally positive.

Let us begin to investigate linear functional $\psi$ vanishing on $K_3$. Since an element of $\suqA_q$ decomposes into
\begin{equation}
    x=\epsilon(x)1+\sum_{j=1}^{n-1}\epsilon_j'(x)d_j+\sum_{1\leq i\leq j\leq n-1}\epsilon_{ij}'(x)v_iv_j^*+k_3,
\end{equation}
the functional $\psi$ is determined by its values on the elements $1$, $d_j$ ($j=1,\ldots, n-1$) and $v_iv_j^*$ ($1\leq i\leq j\leq n-1$). But the functionals $\epsilon$, $\epsilon_k'$ and $\epsilon_{kl}''$ are separating these points, so
\begin{equation}
    \psi=\alpha_0\epsilon+\sum_{k=1}^{n-1}\alpha_k\epsilon_k'+\sum_{k,l}^{n-1}\beta_{kl}\epsilon_{kl}''.
\end{equation}
Since $\epsilon_{kl}''$ is symmetric with respect to $k,l$, we can choose $\beta_{kl}=\beta_lk$.

\begin{proposition}     \label{Propproppsidefposhermconsdpos}
    Let $\psi$ be of the form
    \begin{equation}        \label{Eqproppsidefposhermconsdpos}
        \psi=\alpha_0\epsilon+\sum_{k=1}^{n-1}\alpha_k\epsilon_k'+\sum_{k,l}^{n-1}\beta_{kl}\epsilon_{kl}''
    \end{equation}
    and denote by $B$ the matrix made of the numbers $\beta_{ij}$. Then
    \begin{enumerate}
        \item
            $\psi(1)=0$ if and only if $\alpha_0=0$.
        \item
            The functional $\psi$ is hermitian if and only if $\alpha_i\in\eR$ and $\beta_{ij}\in\eR$.

            \begin{probleme}
                Et $\alpha_0$ aussi doit être réel pour que $\psi$ soit hermitienne ?
            \end{probleme}
            
        \item
            If the matrix $B$ is positive defined, the functional $\psi$ is conditionally positive.
        \item
            If $\psi$ is conditionally positive and hermitian, then $B$ is positive defined.

    \end{enumerate}
    
    
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Since $\epsilon_k'(1)=\epsilon_{kl}''(1)=0$ and $\epsilon(1)=1$, we have $\psi(1)=\alpha_0$.
        \item
            Let us begin with the elements of $B_1$. If $a$ belongs to the span of $B_1$,
            \begin{equation}
                \psi(a^*)=\sum_{k=1}^{n-1}\alpha_k\epsilon_k'(a^*)=\sum_{k=1}^{n-1}\alpha_k\overline{ \epsilon_k'(a) },
            \end{equation}
            so $\psi(a^*)=\overline{ \psi(a) }$ if and only if $\alpha_k\in\eR$.

            \begin{probleme}
                Il me semble qu'il faut aussi prendre $\alpha_0\in\eR$.
            \end{probleme}

            For elements in $B_2$, we use the formula \eqref{SubEqepsppsurvvs}:
            \begin{equation}
                \begin{aligned}[]
                    \psi\big( (v_kv_l^*)^* \big)&=\sum_{i,j=1}^{n-1}B_{ij}\epsilon_{ij}''(v_lv_k^*)\\
                    &=\sum_{i,j}B_{ij}(\delta_{jl}\delta_{ik}+\delta_{jk}\delta_{il})\\
                    &=B_{kl}+B_{lk}\\
                    &=2B_{kl}
                \end{aligned}
            \end{equation}
            because $B$ is symmetric. On the other hand we have similarly 
            \begin{equation}
                \psi(v_kv_l^*)=2B_{kl},
            \end{equation}
            so that $B_{kl}$ has to be real when $\psi$ is Hermitian.
            
        \item
            Let $a\in K_1$. Using Leibnitz rule we have
            \begin{equation}
                \begin{aligned}[]
                    \epsilon(a^*a)&=0\\
                    \epsilon_i'(a^*a)&=0\\
                    \epsilon_{ij}''(a^*a)&=\epsilon_j'(a^*)\epsilon'_i(a)+\epsilon'_i(a^*)\epsilon'_i(a).
                \end{aligned}
            \end{equation}
            Thus
            \begin{equation}        \label{eqpsiaasBij}
                \psi(a^*a)=2\sum_{ij}B_{ij}\epsilon'_i(a^*)\epsilon'_j(a)=2\sum_{ij}B_{ij}\overline{ \epsilon_i'(a) }\epsilon'_j(a).
            \end{equation}
            Let us notice that $\overline{ \epsilon_i'(a) }\epsilon'_j(a)$ is a general product $x_ix_j$ with $x\in\eR^{n-1}$ because, if $a=\sum_kx_kd_k$, 
            \begin{equation}
                \epsilon_i(a)=\sum_kx_k\epsilon'_i(d_k)=ix_i,
            \end{equation}
            so that $\overline{ \epsilon'_i(a) }\epsilon'_j(a)=(-i)ix_ix_j=x_ix_j$. By definition if $B$ is positive defined, the right hand side of equation \eqref{eqpsiaasBij} is positive and $\psi$ is conditionally positive.

        \item
            If $\psi$ is Hermitian, we already see that $B_{ij}$ are real. Now positivity of
            \begin{equation}
                \sum_{ij}B_{ij}\overline{ \epsilon'_i(a) }\epsilon'_j(a)
            \end{equation}
            shows that $B$ is positive defined.

            \begin{probleme}
                Que signifie une matrice «définie positive» ?
            \end{probleme}
                        
    \end{enumerate}
    
\end{proof}

\begin{corollary}
    If $\psi$ is a Gaussian generator, there exist reals numbers $\alpha_1,\cdots,\alpha_{n-1}$ and a symmetric positive defined matrix $B$ such that
    \begin{equation}
        \psi=\sum_{k=1}^{n-1}\alpha_k\epsilon'_k+\sum_{i,j=1}^{n-1}B_{ij}\epsilon_{ij}''.
    \end{equation}
\end{corollary}

\begin{proof}
    By definition, a Gaussian generator is Hermitian, conditionally positive and vanishes on $K_3$. We already know that it is of the form
    \begin{equation}        \label{EqFormgenpsiGauss}
        \psi=\alpha_0\epsilon+\sum_{k=1}^{n-1}\alpha_k\epsilon'_k+\sum_{k,l}^{n-1}B_{kl}\epsilon_{kl}''.
    \end{equation}
    If $\psi$ is Hermitian, $\alpha_i=0$ and $B_{ij}\in \eR$. If $\psi$ is Hermitian and conditionally positive, then $B$ is positive definite.

    \begin{probleme}
        Pourquoi $\alpha_0=0$ ? C'est à dire : pourquoi $\psi(1)=0$ ?
    \end{probleme}
    
\end{proof}

\begin{proposition}
    consider the Gaussian cocycle (proposition \ref{PropGaussCocycle})
    \begin{equation}
        \eta(a)=\sum_{k=1}^{n-1}i\epsilon_k'(a)h_k.
    \end{equation}
    \begin{enumerate}
        \item
            If the numbers $\langle h_k, h_l\rangle $ are real, then the formula
            \begin{equation}
                \psi=\frac{ 1 }{2}\sum_{k,l=1}^{n-1}\langle h_k, h_l\rangle \epsilon_{kl}''
            \end{equation}
            defines a conditionally positive Hermitian functional associated with the cocycle $\eta$.

        \item
            If one of $\langle h_k, h_l\rangle $ is not real, then there are no Hermitian functional associated with $\eta$.
    \end{enumerate}
    
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            We denote $\alpha_{kl}=\langle h_k, h_l\rangle $. If these numbers are real, we have $\alpha_{kl}=\alpha_{lk}$ since $\langle h_k, h_l\rangle =\overline{ \langle h_l, h_k\rangle  }$. From corollary \ref{ItemPropCorSchr}\ref{ItemPropCorSchriii}, the functional $\psi$ is associated with $\eta$ if
            \begin{equation}
                -\langle \eta(a^*), \eta(b)\rangle =\epsilon(a)\psi(b)-\psi(ab)+\psi(a)\epsilon(b).
            \end{equation}
            Using the Leibnitz rule \eqref{EqLeibnitzepsppkl} (reduced by the fact that $a$ and $b$ belong to $K_1$), we have
            \begin{equation}
                \begin{aligned}[]
                    \psi(ab)&=\frac{ 1 }{2}\sum_{kl}\alpha_{kl}\epsilon_{kl}''(ab)\\
                    &=\frac{ 1 }{2}\alpha_{kl}\big( \epsilon'_l(a)\epsilon'_k(b)+\epsilon'_k(a)\epsilon'_l(b) \big)\\
                    &=\sum_{kl}\langle h_k, h_l\rangle \epsilon'_k(a)\epsilon'_l(b)\\
                    &=\sum_{kl}\langle \overline{ \epsilon'_k(a)h_k }, \epsilon'_l(b)h_l\rangle \\
                    &=\langle \eta(a^*), \eta(b)\rangle .
                \end{aligned}
            \end{equation}
            The functional $\psi$ is hermitian because it is of form \eqref{Eqproppsidefposhermconsdpos} with $\alpha_i=0$ and $\alpha_{kl}\in\eR$. Moreover the matrix $B_{kl}=\langle h_k, h_l\rangle $ is symmetric and real and defines a Gaussian cocycle, so that the functional $\psi$ is Hermitian.

        \item
            The number $\langle h_k, h_l\rangle $ decomposes in real and imaginary parts as
            \begin{equation}
                \langle h_k, h_l\rangle =\beta_{kl}+id_{kl}
            \end{equation}
            with
            \begin{equation}
                \begin{aligned}[]
                    \beta_{kl}&=\frac{ 1 }{2}\big( \langle h_k, h_l\rangle +\langle h_l, h_k\rangle  \big)\\
                    d_{kl}&=\frac{ -i }{2}\big( \langle h_k, h_l\rangle -\langle h_l, h_k\rangle  \big)\\
                \end{aligned}
            \end{equation}
            If $\psi$ is Hermitian, for $a$ and $b$ in $K_1$ we have
            \begin{equation}
                \begin{aligned}[]
                    \psi(ab)&=\langle \eta(a^*), \eta(b)\rangle \\
                    &=\sum_{kl}\langle \epsilon'_k(a^*)h_k, \epsilon'_l(b)h_l\rangle \\
                    &=\sum_{kl}\overline{ \epsilon'_k(a^*) }\epsilon'_l(b)\langle h_k, h_l\rangle \\
                    &=\sum_{kl}\epsilon'_k(a) \epsilon'_l(b)\langle h_k, h_l\rangle 
                \end{aligned}
            \end{equation}
            Computing that way $\psi(v_iv_j)$ and $\psi(v_j^*v_i^*)$ and taking into account the formula $\epsilon'_k(v_i)=i\delta_{ki}$, we found that $\psi$ is only Hermitian if
            \begin{equation}
                \langle h_i, h_j\rangle =\langle h_j, h_i\rangle ,
            \end{equation}
            which means that $\langle h_i, h_j\rangle $ is real.
    \end{enumerate}
\end{proof}

Let \(\pi\) be the representation of \(\suqA_q\) on \(H\) and
\begin{equation}
    H=H_{\epsilon}\oplus H_{\epsilon}^{\perp}
\end{equation}
be the decomposition of \(H\) into Gaussian and non-Gaussian parts. The Schürmann triple is completed by \(\psi\colon \suqA_q\to \eC\) and \(\eta\colon \suqA_q\to H\). We consider the corresponding decomposition of \(\eta\):
\begin{equation}
    \eta=\eta^{\epsilon}\oplus\eta^{\perp}
\end{equation}
where \(\eta^{\epsilon}=\pr_{H_{\epsilon}}\circ\eta\) and \(\eta^{\perp}=\pr_{H_{\epsilon}^{\perp}}\circ\eta\). Let us denote by \(h^{\epsilon}\) the projection of \(h\) on \(H_{\epsilon}\). We want to see under which conditions \(\eta\) is a cocycle associated with a generator.

A Gaussian generator reads
\begin{equation}
    \eta(a)=\sum_{k=1}^{n-1}i\epsilon'_k(a)h_k
\end{equation}
where \(h_i=\eta(v_i^*)\). A condition to have a generator is to have \(\langle h_i, h_j\rangle \in\eR\). Now, the Gaussian part of the cocycle reads
\begin{equation}
    \eta^{\epsilon}(a)=\sum_ki\epsilon'_k(a)\pr_{\epsilon}h_k.
\end{equation}
It will accept a generator if and only if
\begin{equation}
    \langle \pr_{\epsilon}h_i, \pr_{\epsilon}h_j\rangle 
\end{equation}
is real.

On the other hand, we can check that \(\eta^{\epsilon}\) is a cocycle for the representation \(\pi\), i.e.
\begin{equation}
    \eta^{\epsilon}(ab)=\pi(a)\eta^{\epsilon}(b)+\eta^{\epsilon}(a)\epsilon(b),
\end{equation}
or
\begin{equation}
    \pr_{\epsilon}\eta(ab)=\pi(a)\pr_{\epsilon}\eta(b)+\pr_{\epsilon}\eta(a)\epsilon(b).
\end{equation}
From lemma \ref{LempiepsHepsUni} we have \(\pi(a)\eta^{\epsilon}(b)=\epsilon(a)\eta^{\epsilon}(b)\). We write the second term under the form
\begin{equation}
    \pr_{\epsilon}\Big( \pi(a)\big( \eta^{\epsilon}(b)+\eta^{\perp}(b) \big) \Big)=\epsilon(a)\eta^{\epsilon}(b)+\pr_{\epsilon}\pi(a)\eta^{\perp}.
\end{equation}
The last term vanishes by lemma \ref{LempiepsHepsUni}.
