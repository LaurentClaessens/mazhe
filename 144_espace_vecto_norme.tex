% This is part of Mes notes de mathématique
% Copyright (c) 2008-2016
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminants}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecGYzHWs}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Formes multilinéaires alternées}
%---------------------------------------------------------------------------------------------------------------------------

%  Lire http://www.les-mathematiques.net/phorum/read.php?2,302266

\begin{definition}\index{déterminant!forme linéaire alternée}
    Soit \( E\), un \( \eK\)-espace vectoriel. Une forme linéaire \defe{alternée}{forme linéaire!alternée}\index{alternée!forme linéaire} sur \( E\) est une application linéaire \( f\colon E\to \eK\) telle que \( f(v_1,\ldots, v_k)=0\) dès que \( v_i=v_j\) pour certains \( i\neq j\).
\end{definition}

\begin{lemma}   \label{LemHiHNey}
    Une forme linéaire alternée est antisymétrique. Si \( \eK\) est de caractéristique différente de \( 2\), alors une forme antisymétrique est alternée.
\end{lemma}

\begin{proof}
    Soit \( f\) une forme alternée; quitte à fixer toutes les autres variables, nous pouvons travailler avec une \( 2\)-forme et simplement montrer que \( f(x,y)=-f(y,x)\). Pour ce faire nous écrivons
    \begin{equation}
        0=f(x+y,x+y)=f(x,x)+f(x,y)+f(y,x)+f(y,y)=f(x,y)+f(y,x).
    \end{equation}
    
    Pour la réciproque, si \( f\) est antisymétrique, alors \( f(x,x)=-f(x,x)\). Cela montre que \( f(x,x)=0\) lorsque \( \eK\) est de caractéristique différente de deux.
\end{proof}

\begin{proposition}[\cite{GQolaof}] \label{ProprbjihK}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension \( n\), où la caractéristique de \( \eK\) n'est pas deux. L'espace des \( n\)-formes multilinéaires alternées sur \( E\) est de \( \eK\)-dimension \( 1\).
\end{proposition}
\index{groupe!permutation}
\index{groupe!et géométrie}
\index{espace!vectoriel!dimension}
\index{rang}
\index{déterminant}
\index{dimension!\( n\)-formes multilinéaires alternées}

\begin{proof}
    Soit \( \{ e_i \}\), une base de \( E\) et \( f\colon E\to \eK\) une \( n\)-forme linéaire alternée, puis \( (v_1,\ldots, v_n)\) des vecteurs de \( E\). Nous pouvons les écrire dans la base
    \begin{equation}
        v_j=\sum_{i=1}^n\alpha_{ij}e_i
    \end{equation}
    et alors exprimer \( f\) par
    \begin{subequations}
        \begin{align}
            f(v_1,\ldots, v_n)&=f\big( \sum_{i_1=1}^n\alpha_{1i_1}e_{i_1},\ldots, \sum_{i_n=1}^n\alpha_{ni_n}e_{i_n} \big)\\
            &=\sum_{i,j}\alpha_{1i_1}\ldots \alpha_{ni_n}f(e_{i_1},\ldots, e_{i_n}).
        \end{align}
    \end{subequations}
    Étant donné que \( f\) est alternée, les seuls termes de la somme sont ceux dont les \( i_k\) sont tous différents, c'est à dire ceux où \( \{ i_1,\ldots, i_n \}=\{ 1,\ldots, n \}\). Il y a donc un terme par élément du groupe des permutations \( S_n\) et
    \begin{equation}
        f(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}f(e_{\sigma(1)},\ldots, e_{\sigma(n)}).
    \end{equation}
    En utilisant encore une fois le fait que la forme \( f\) soit alternée, \( f=f(e_1,\ldots, e_n)\Pi\) où
    \begin{equation}
        \Pi(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}.
    \end{equation}
    Pour rappel, la donnée des \( v_i\) est dans les nombres \( \alpha_{ij}\).
    
    L'espace des \( n\)-formes alternées est donc \emph{au plus} de dimension \( 1\). Pour montrer qu'il est exactement de dimension \( 1\), il faut et suffit de prouver que \( \Pi\) est alternée. Par le lemme \ref{LemHiHNey}, il suffit de prouver que cette forme est antisymétrique\footnote{C'est ici que joue l'hypothèse sur la caractéristique de \( \eK\).}. 

    Soient donc \( v_1,\ldots, v_n\) tels que \( v_i=v_j\). En posant \( \tau=(1i)\) et \( \tau'=(2j)\) et en sommant sur \( \sigma\tau\tau'\) au lieu de \( \sigma\), nous pouvons supposer que \( i=1\) et \( j=2\). Montrons que \( \Pi(v,v,v_3,\ldots, v_n)=0\) en tenant compte que \( \alpha_{i1}=\alpha_{i2}\) :
    \begin{subequations}
        \begin{align}
            \Pi(v,v,v_3,\ldots, v_n)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n}\\
            &=\sum_{\sigma\in S_n}\epsilon(\sigma\tau)\alpha_{\sigma\tau(1)1}\alpha_{\sigma\tau(2)2}\alpha_{\sigma\tau(3)3}\ldots \alpha_{\sigma\tau(n)n}&\text{où \( \tau=(12)\)}\\
            &=-\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n} \\
            &=-\Pi(v,v,v_3,\ldots, v_n).
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}   \label{LemcDOTzM}
    Soit \( \eK\) un corps fini autre que \( \eF_2\)\quext{Je ne comprends pas très bien à quel moment joue cette hypothèse.}, soit un groupe abélien \( M\) et un morphisme \( \varphi\colon \GL(n,\eK)\to M\). Alors il existe un unique morphisme \( \delta\colon \eK^*\to M\) tel que \( \varphi=\delta\circ\det\).
\end{lemma}

\begin{proof}
    D'abord le groupe dérivé de \( \GL(n,\eK)\) est \( \SL(n,\eK)\) parce que les éléments de \( D\big( \GL(n,\eK) \big)\) sont de la forme \( ghg^{-1}h^{-1}\) dont le déterminant est \( 1\).
    
    De plus le groupe \( \SL(n,\eK)\) est normal dans \( \GL(n,\eK)\). Par conséquent \( \GL(n,\eK)/\SL(n,\eK)\) est un groupe et nous pouvons définir l'application relevée
    \begin{equation}
        \tilde \varphi\colon \frac{ \GL(n,\eK) }{ \SL(n,\eK) }\to M
    \end{equation}
    vérifiant \( \varphi=\tilde \varphi\circ\pi\) où \( \pi\) est la projection. 

    Nous pouvons faire la même chose avec l'application
    \begin{equation}
        \det\colon \GL(n,\eK)\to \eK^*
    \end{equation}
    qui est un morphisme de groupes dont le noyau est \( \SL(n,\eK)\). Cela nous donne une application
    \begin{equation}
        \tilde \det\colon \frac{ \GL(n,\eK) }{ \SL(n,\eK) }\to \eK^*
    \end{equation}
    telle que \( \det=\tilde \det\circ\pi\). Cette application \( \tilde \det\) est un isomorphisme. En effet elle est surjective parce que le déterminant l'est et elle est injective parce que son noyau est précisément ce par quoi on prend le quotient. Par conséquent \( \tilde \det \) possède un inverse et nous pouvons écrire
    \begin{equation}
        \varphi=\tilde \varphi\circ\tilde \det^{-1}\circ\tilde \det\circ\pi.
    \end{equation}
    État donné que \( \tilde \det\circ\pi=\det\), nous avons alors \( \varphi=\delta\circ\det\) avec \( \delta=\tilde \varphi\circ\tilde \det^{-1}\). Ceci conclut la partie existence de la preuve.

    En ce qui concerne l'unicité, nous considérons \( \delta'\colon \eK^*\to M\) telle que \( \varphi=\delta'\circ\det\). Pour tout \( u\in \GL(n,\eK)\) nous avons \( \delta'(\det(u))=\varphi(u)=\delta(\det(u))\). L'application \( \det\) étant surjective depuis \( \GL(n,\eK)\) vers \( \eK^*\), nous avons \( \delta'=\delta\).
\end{proof}

\begin{theorem}
    Soit \( p\geq 3\) un nombre premier et \( V\), un \( \eF_p\)-espace vectoriel de dimension finie \( n\). Pour tout \( u\in\GL(V)\) nous avons
    \begin{equation}
        \epsilon(u)=\left(\frac{\det(u)}{p}\right).
    \end{equation}
\end{theorem}
Ici \( \epsilon\) est la signature de \( u \) vue comme une permutation des éléments de \( \eF_p\).

\begin{proof}
    Commençons par prouver que
    \begin{equation}
        \epsilon\colon \GL(V)\to \{ -1,1 \}.
    \end{equation}
    est un morphisme. Si nous notons \( \bar u\in S(V)\) l'élément du groupe symétrique correspondant à la matrice \( u\in \GL(V)\), alors nous avons \( \overline{ uv }=\bar u\circ\bar v\), et la signature étant un homomorphisme (proposition \ref{ProphIuJrC}), 
    \begin{equation}
        \epsilon(uv)=\epsilon(\bar u\circ\bar v)=\epsilon(\bar u)\epsilon(\bar v).
    \end{equation}
    Par ailleurs \( \{ -1,1 \}\) est abélien, donc le lemme \ref{LemcDOTzM} s'applique et nous pouvons considérer un morphisme \( \delta\colon \eF_p^*\to \{ -1,1 \}\) tel que \( \epsilon=\delta\circ\det\).

    Nous allons utiliser le lemme \ref{Lemoabzrn} pour montrer que \( \delta\) est le symbole de Legendre. Pour cela il nous faudrait trouver un \( x\in \eF_p^*\) tel que \( \delta(x)=-1\). Étant donné que \( \det\) est surjective, nous cherchons ce \( x\) sous la forme \( x=\det(u)\). Par conséquent nous aurions
    \begin{equation}
        \delta(x)=(\delta\circ\det)(u)=\epsilon(u),
    \end{equation}
    et notre problème revient à trouver une matrice \( u\in\GL(V)\) dont la permutation associée soit de signature \( -1\).

    Soit \( n=\dim V\); en conséquence de la proposition \ref{PropHfrNCB}\ref{ItemiEFRTg}, l'espace \( \eE_q=\eF_{p^n}\) est un \( \eF_p\)-espace vectoriel de dimension \( n\) et est donc isomorphe en tant qu'espace vectoriel à \( V\). Étant donné que \( \eF_q\) est un corps fini, nous savons que \( \eF_q^*\) est un groupe cyclique à \( q-1\) éléments. Soit \( y\), un générateur de \( \eF_q^*\) et l'application
    \begin{equation}
        \begin{aligned}
            \beta\colon \eF_q&\to \eF_q \\
            x&\mapsto yx. 
        \end{aligned}
    \end{equation}
    Cela est manifestement \( \eF_p\)-linéaire (ici \( y\) et \( x\) sont des classes de polynômes et \( \eF_p\) est le corps des coefficients). L'application \( \beta\) fixe zéro et à part zéro, agit comme le cycle
    \begin{equation}
        (1,y,y^2,\ldots, y^{q-2}).
    \end{equation}
    Nous savons qu'un cycle de longueur \( n\) est de signature \( (-1)^{n+1}\). Ici le cycle est de longueur \( q-1\) qui est pair (parce que \( p\geq 3\)) et par conséquent, l'application \( \beta\) est de signature \( -1\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Déterminant d'une famille de vecteurs}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons un corps \( \eK\) et l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\).

\begin{definition}[Déterminant d'une famille de vecteurs\cite{MathAgreg}]\label{DEFooODDFooSNahPb}
    Le \defe{déterminant}{déterminant!d'une famille de vecteurs} \( (v_1,\ldots, v_n)\) dans la base \( B\) est l'élément de \( \eK\)
    \begin{equation}
        \sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne^*_{\sigma(i)}(v_i)
    \end{equation}
    où la somme porte sur le groupe symétrique, \( \epsilon(\sigma)\) est la signature de la permutation \( \sigma\) et \( e_k^*\) est le dual de \( e_k\).

    Nous le notons \( \det_{(e_1,\ldots, e_n)}(v_1,\ldots, v_n)\).
\end{definition}

\begin{lemma}[\cite{MathAgreg}]     \label{LemJMWCooELZuho}
    Les propriétés du déterminant. Soit \( B\) une base de \( E\).
    \begin{enumerate}
        \item
            L'application \( \det_B\colon E^n\to \eK\) est \( n\)-linéaire et alternée.
        \item
            Pour toute base, \( \det_B(B)=1\).
        \item
            Le déterminant ne change pas si on remplace un vecteur par une combinaison linéaire des autres :
            \begin{equation}
                \det_B(v_1,\ldots, v_n)=\det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big).
            \end{equation}
        \item
            Si on permute les vecteurs,
            \begin{equation}
                \det_B(v_1,\ldots, v_n)=\epsilon(\sigma)\det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)}).
            \end{equation}
        \item
            Si \( B'\) est une autre base :
            \begin{equation}        \label{EqAWICooBLTTOY}
                \det_B=\det_B(B')\det_{B'}
            \end{equation}
        \item
            Nous avons aussi la formule \( \det_{B}(B')\det_{B'}(B)=1\).
        \item\label{ItemDWFLooDUePAf}
            Les vecteurs \( \{ v_1,\ldots, v_n \}\) forment une base si et seulement si \( \det_B(v_1,\ldots, v_n)\neq 0\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
            En posant \( v_1=x_1+\lambda x_2\) nous avons
            \begin{subequations}
                \begin{align}
                    \det_B(x_1+\lambda x_2,v_2,\ldots, v_n)&=\sum_{\sigma}\epsilon(\sigma)\prod_{i=1}^ne^*_{\sigma(i)}(v_i)\\
                    &=\sum_{\sigma}\epsilon(\sigma)\Big( e^*_{\sigma(1)}(x_1+\lambda x_2) \Big)\prod_{i=2}^ne^*_{\sigma(i)}(v_i).
                \end{align}
            \end{subequations}
            À partir de là, la linéarité de \( e^*_{\sigma(1)}\) montre que \( \det_B\) est linéaire en son premier argument. Pour les autres argument, le même calcul tient.

            En ce qui concerne le fait d'être alternée, permuter \( v_k\) et \( v_l\) revient à calculer \( \det_B( v_{\sigma_{kl}(1)},\ldots, v_{\sigma_{kl}(n)} )\), c'est à dire changer la somme \( \sum_{\sigma}\) en \( \sum_{\sigma\circ\sigma_{kl}}\). Cela ajoute \( 1\) à \( \epsilon(\sigma)\) vu que l'on ajoute une permutation.
        \item
            Nous avons
            \begin{equation}
                \det_B(B)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^n\underbrace{e_{\sigma(i)}^*(e_i)}_{=\delta_{\sigma(i),i}}.
            \end{equation}
            Si \( \sigma\) n'est pas l'identité, le produit contient forcément un facteur nul. Il ne reste de la somme que \( \sigma=\id\) et le résultat est \( 1\).
        \item
            Vu que \( \det_B\) est linéaire en tous ses arguments, 
            \begin{equation}
                \det_B\big( v_1+\sum_{s=2}^na_sv_s,v_2,\ldots, v_n \big)=\det_B(v_1,\ldots, v_n)+\sum_{s=2}^na_s\det_B(v_s,v_2,\ldots, v_n).
            \end{equation}
            Chacun des termes de la somme est nul parce qu'il y a répétition de \( v_s\) parmi les arguments alors que la forme est alternée.
        \item
            Nous devons calculer \( \det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)})\), et pour y voir plus clair nous posons \( w_i=v_{\sigma(i)}\). Alors :
            \begin{subequations}
                \begin{align}
                    \det_B(v_{\sigma(1)},\ldots, v_{\sigma(n)})&=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(w_i)\\
                    &=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_{\sigma(i)})\\
                    &=\sum_{\sigma'}\epsilon(\sigma')\prod_{i=1}^ne^*_{\sigma^{-1}\sigma'(i)}(v_i)\\
                    &=\sum_{\sigma'}\epsilon(\sigma\sigma')\prod_{i=1}^ne^*_{\sigma'(i)}(v_i)\\
                    &=\epsilon(\sigma)\det_B(v_1,\ldots, v_n).
                \end{align}
            \end{subequations}
            Justifications : nous avons d'abord modifié l'ordre des éléments du produit et ensuite l'ordre des éléments de la somme. Nous avons ensuite utilisé le fait que \( \epsilon\colon S_n\to \{ 0,1 \}\) était un morphisme de groupe (proposition \ref{ProphIuJrC}).
        \item
            Étant donné que l'espace des formes multilinéaires alternées est de dimension \( 1\), il existe un \( \lambda\in \eK\) tel que \( \det_B=\lambda\det_{B'}\). Appliquons cela à \( B'\) :
            \begin{equation}
                \det_B(B')=\lambda\det_{B'}(B'),
            \end{equation}
            donc \( \lambda=\det_B(B')\).
        \item
            Il suffit d'appliquer l'égalité précédente à \( B\) en nous souvenant que \( \det_B(B)=1\).
        \item
            Si \( B'=\{ v_1,\ldots, v_n \}\) est une base alors \( \det_B(B')\neq 0\), sinon il n'est pas possible d'avoir \( \det_B(B')\det_{B'}(B)=1\).

            À l'inverse, si \( B'\) n'est pas une base, c'est que \( \{ v_1,\ldots, v_n \}\) est liée par le théorème \ref{ThoMGQZooIgrXjy}\ref{ItemHIVAooPnTlsBi}. Il y a donc moyen de remplacer un des vecteurs par une combinaison linéaire des autres. Le déterminant s'annule alors.
    \end{enumerate}
\end{proof}

D'après la proposition \ref{ProprbjihK}, il existe une unique forme \( n\)-linéaire alternée égale à \( 1\) sur \( B\), et c'est \( \det_B\colon E^n\to \eK\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Déterminant d'un endomorphisme}
%---------------------------------------------------------------------------------------------------------------------------

L'interprétation géométrique du déterminant en termes d'aires et de volumes est donnée après la théorème \ref{ThoBVIJooMkifod}.

\begin{lemma}
    Si \( f\colon E\to E\) est un endomorphisme, si \( B\) et \( B'\) sont deux bases, alors \( \det_B\big( f(B) \big)=\det_{B'}\big( f(B') \big)  \).
\end{lemma}

\begin{proof}
    L'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon E^n&\to \eK \\
            v_1,\ldots, v_n&\mapsto \det_B\big( f(v_1),\ldots, f(v_n) \big) 
        \end{aligned}
    \end{equation}
    est \( n\)-linéaire et alternée; il existe donc \( \lambda\in \eK\) tel que \( \varphi=\lambda\det_B\). En appliquant cela à \( B\) :
    \begin{equation}
        \det_B\big( f(B) \big)=\lambda \det_B(B)=\lambda.
    \end{equation}
    Nous avons donc déjà prouvé que \( \lambda=\det_B\big( f(B) \big)\), c'est à dire
    \begin{equation}
        \det_B\big( f(v) \big)=\det_B\big( f(B) \big)\det_B(v).
    \end{equation}
    
    Nous allons maintenant introduire \( B'\) là où il y a du \( v\) en utilisant les formules \eqref{EqAWICooBLTTOY} :
    \begin{subequations}
        \begin{align}
            \det_B\big( f(v) \big)&=\det_B(B')\det_{B'}\big( f(v) \big)\\
            \det_B(v)=\det_B(B')\det_{B'}(v).
        \end{align}
    \end{subequations}
    Nous obtenons
    \begin{equation}
        \det_{B'}\big( f(v) \big)=\det_B\big( f(B) \big)\det_{B'}(v).
    \end{equation}
    Et on applique cela à \( v=B'\) :
    \begin{equation}
        \det_{B'}\big( f(B') \big)=\det_B\big( f(B) \big)\underbrace{\det_{B'}(B')}_{=1}.
    \end{equation}
\end{proof}

Cette proposition nous permet de définir le déterminant d'un endomorphisme de la façon suivante sans préciser la base.
\begin{definition}[\cite{MathAgreg}]        \label{DefCOZEooGhRfxA}
    Si \( f\colon E\to E\) est un endomorphisme, le \defe{déterminant}{déterminant!d'un endomorphisme} de \( f\) est
    \begin{equation}
        \det(f)=\det_B\big( f(B) \big)
    \end{equation}
\end{definition}

\begin{proposition}     \label{PropYQNMooZjlYlA}
    Principales propriétés géométriques du déterminant d'un endomorphisme.
    \begin{enumerate}
        \item   \label{ItemUPLNooYZMRJy}
            Si \( f\) et \( g\)  sont des endomorphismes, alors \( \det(f\circ g)=\det(f)\det(g)\).
        \item       \label{ITEMooNZNLooODdXeH}
            L'endomorphisme \( f\) est un automorphisme\footnote{Endomorphisme inversible, définition \ref{DEFooOAOGooKuJSup}.} si et seulement si \( \det(f)\neq 0\).\index{déterminant!et inversibilité}
        \item   \label{ITEMooZMVXooLGjvCy}
            Si \( \det(f)\neq 0\) alors \( \det(f^{-1})=\det(f)^{-1}\).
        \item       \label{ItemooPJVYooYSwqaE}
            L'application \( \det\colon \GL(E)\to \eK\setminus\{ 0 \}\) est un morphisme de groupe.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
            Nous considérons l'application
            \begin{equation}
                \begin{aligned}
                    \varphi\colon E^n&\to \eK \\
                    v&\mapsto \det_B\big( f(v) \big). 
                \end{aligned}
            \end{equation}
            Comme d'habitude nous avons \( \varphi(v)=\lambda\det_B(v)\). En appliquant à \( B\) et en nous souvenant que \( \det_B(B)=1\) nous avons
                $\det_B\big( f(B) \big)=\lambda$. Autrement dit :
                \begin{equation}
                    \lambda=\det(f).
                \end{equation}
            Calculons à présent \( \varphi\big( g(B) \big)\) : d'une part,
            \begin{equation}
                \varphi\big( g(B) \big)=\det_B\big( (f\circ g)(B) \big)
            \end{equation}
            et d'autre part,
            \begin{equation}
                \varphi\big( g(B) \big)=\lambda\det_B\big( g(B) \big)=\lambda\det(g)
            \end{equation}
            En égalisant et en reprenant la la valeur déjà trouvée de \( \lambda\),
            \begin{equation}
                \det\big(f\circ g)(B) \big)=\det(f)\det(g),
            \end{equation}
            ce qu'il fallait.
        \item
            Supposons que \( f\) soit un automorphisme. Alors si \( B\) est une base, \( f(B) \) est une base. Par conséquent \( \det(f)=\det_B\big( f(B) \big)\neq 0\) parce que \( f(B)\) est une base (lemme \ref{LemJMWCooELZuho}\ref{ItemDWFLooDUePAf}).

            Réciproquement, supposons que \( \det(f)\neq 0\). Alors si \( B\) est une base quelconque nous avons \( \det_B\big( f(B) \big)\neq 0\), ce qui est uniquement possible lorsque \( f(B)\) est une base. L'application \( f\) transforme donc toute base en une base et est alors un automorphisme d'espace vectoriel.
        \item
            Vu que le déterminant de l'identité est \( 1\) et que \( f\) est inversible, \( 1=\det(f\circ f^{-1})=\det(f)\det(f^{-1})\).
    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Vandermonde}
%---------------------------------------------------------------------------------------------------------------------------

Liens internes sur l'utilisation du déterminant de Vandermonde :
\begin{enumerate}
    \item
        Pour prouver que \( \tr(u^p)=0\) pour tout \( p\) si et seulement si \( u\) est nilpotente (lemme \ref{LemzgNOjY}).
    \item
        Pour prouver qu'un endomorphisme possédant \( \dim(E)\) valeurs propres distinctes est cyclique (proposition \ref{PropooQALUooTluDif}).
\end{enumerate}

\begin{proposition}[\cite{fJhCTE}]  \label{PropnuUvtj}
    Le \defe{déterminant de Vandermonde}{déterminant!Vandermonde}\index{Vandermonde (déterminant)} est le polynôme en \( n\) variables donné par
    \begin{equation}
        V(T_1,\ldots, T_n)=\det\begin{pmatrix}
             1   &   1    &   \ldots    &   1    \\
             T_1   &   T_2    &   \ldots    &   T_n    \\
             \vdots   &   \ddots    &   \ddots    &   \vdots    \\ 
             T_1^{n-1}   &   T_2^{n-1}    &   \ldots    &   T_n^{n-1}     
         \end{pmatrix}=\prod_{1\leq i<j\leq n}(T_j-T_i).
    \end{equation}
    Notez que l'inégalité du milieu est stricte (sinon d'ailleurs l'expression serait nulle).
\end{proposition}

\begin{proof}
    Nous considérons le polynôme
    \begin{equation}
        f(X)=V(T_1,\ldots, T_{n-1},X)\in \big( \eK[T_1,\ldots, T_{n-1}] \big)[X].
    \end{equation}
    C'est un polynôme de degré au plus \( n-1\) en \( X\) et il s'annule aux points \( T_1,\ldots, T_{n-1}\). Par conséquent il existe \( \alpha\in \eK[T_1,\ldots, T_{n-1}]\) tel que
    \begin{equation}    \label{EqeVxRwO}
        f=\alpha(X-T_{n-1})\ldots(X-T_1).
    \end{equation}
    Nous trouvons \( \alpha\) en écrivant \( f(0)\). D'une part la formule \eqref{EqeVxRwO} nous donne
    \begin{equation}    \label{EqblwWMj}
        f(0)=\alpha(-1)^{n-1}T_1\ldots T_{n-1}.
    \end{equation}
    D'autre par la définition donne
    \begin{subequations}
        \begin{align}
            f(0)&=\det\begin{pmatrix}
                 1   &   \cdots    &   1    &   1    \\
                 T_1      &       &   T_{n-1}    &   0    \\
                 \vdots   &       &   \vdots    &   \vdots    \\ 
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}    &   0     
             \end{pmatrix}\\
             &=(-1)^{n-1}\det\begin{pmatrix}
                 T_1   &   \ldots    &   T_{n-1}    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \ldots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}\det\begin{pmatrix}
                 1   &   \cdots    &   1    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}V(T_1,\ldots, T_{n-1})
        \end{align}
    \end{subequations}
    En égalisant avec \eqref{EqblwWMj}, nous trouvons \( \alpha=V(T_1,\ldots, T_{n-1})\), et donc
    \begin{equation}
        f=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(X-T_j)
    \end{equation}
    Enfin, une récurrence montre que
    \begin{subequations}
        \begin{align}
            V(T_1,\ldots, T_n)&=f(T_n)\\
            &=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(T_n-T_j)\\
            &=\prod_{k\leq n}\prod_{j\leq k-1}(T_k-T_j)\\
            &=\prod_{1\leq j<k\leq n}(T_i-T_j).
        \end{align}
    \end{subequations}
\end{proof}

\begin{example}
    Le déterminant de Vandermonde (proposition \ref{PropnuUvtj}) est alterné, semi-symétrique et non symétrique. Le fait qu'il soit alterné est le fait qu'il soit un déterminant. Étant donné qu'il est alterné, il est semi-symétrique parce que sur \( A_n\), nous avons \( \epsilon=1\). Étant donné qu'il est alterné, il change de signe sous l'action des éléments impairs de \( S_n\) et n'est donc pas symétrique.
\end{example}

\begin{proposition}\index{action de groupe} \label{PropUDqXax}
    Un polynôme semi-symétrique \( f\in \eK[T_1,\ldots, T_n]\) se décompose de façon unique en
    \begin{equation}
        f=P+VQ
    \end{equation}
    où \( P\) et \( Q\) sont deux polynômes symétriques.
\end{proposition}
\index{groupe!permutation}
\index{polynôme!symétrique}

\begin{proof}

    Nous commençons par prouver l'unicité en montrant que si \( f=PVQ\) avec \( P\) et \( Q\) symétrique, alors \( P\) et \( Q\) sont donnés par des formules explicites en termes de \( f\).


    Si \( \sigma_1\) et \( \sigma_2\) sont deux permutations impaires de \( \{ 1,\ldots, n \}\), alors \( \sigma_1\cdot f=\sigma_2\cdot f\) parce que l'élément \( \sigma_2^{-1}\sigma_1\) est pair (proposition \ref{ProphIuJrC}), de telle sorte que \( \sigma_2^{-1}\sigma_1\cdot f=f\). Nous posons donc \( g=\tau\cdot f\) où \( \tau\) est une permutation impaire quelconque -- par exemple une transposition.

    Vu que \( V\) est alternée et que \( \tau\) est une transposition nous avons
    \begin{equation}
        g=\tau\cdot f=P-VQ.
    \end{equation}
    Donc \( f+g=2P\) et \( f-g=2VQ\). Cela donne \( P\) et \( Q\) en terme de \( f\) et \( g\), et donc l'unicité.

    Attention : cela ne donne pas un moyen de prouver l'existence parce que rien ne prouve pour l'instant que \( f-g\) peut effectivement être écrit sous la forme \( VQ\), c'est à dire que \( f-g\) soit divisible par \( V\). C'est cela que nous allons nous atteler à démontrer maintenant.

    Nous commençons par prouver que \( f+g\) est symétrique et \( f-g\) alterné. Si \( \sigma\) est une transposition,
    \begin{equation}
        \sigma\cdot(f+g)=\sigma\cdot f+\sigma\tau\cdot f=g+f
    \end{equation}
    parce que \( \sigma\tau\) est pair. De la même façon,
    \begin{equation}
        \sigma\cdot(f-g)=g-f=\epsilon(\sigma)(f-g).
    \end{equation}
    Dans les deux cas nous concluons en utilisant le fait que toute permutation est un produit de transpositions (proposition \ref{PropPWIJbu}) et que \( \epsilon\) est un homomorphisme.

    Soient maintenant deux entiers \( h<k\) dans \( \{ 1,\ldots, n \}\) et l'anneau
    \begin{equation}
        \big( \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \big)[T_k].
    \end{equation}
    Cet anneau contient le polynôme \( T_k-T_h\) où \( T_k\) est la variable et \( T_h\) est un coefficient. Nous faisons la division euclidienne de \( f-g\) par  \( T_k-T_h\) parce que nous avons dans l'idée de faire arriver le déterminant de Vandermonde et donc le produit de toutes les différences \( T_k-T_h\) :
    \begin{equation}    \label{EqSHdgrG}
        f-g=(T_k-T_h)q+r
    \end{equation}
    où \( \deg_{T_k}r<1\), c'est à dire que \( r\) ne dépends pas de \( T_k\). Nous revoyons maintenant l'égalité \eqref{EqSHdgrG} dans \( \eK[T_1,\ldots, T_n]\) et nous y appliquons la transposition \( \tau_{kh}\). Nous savons que \( \tau_{kh}(f-g)=-(f-g)\) et \( \tau_{kh}(T_k-T_h)=-(T_k-T_h)\), et donc
    \begin{equation}    \label{EqVOhjKB}
        -(f-g)=-(T_k-T_h)\tau_{kh}\cdot   q+\tau_{kh}\cdot r
    \end{equation}
    où \(\tau_{kh}\cdot r\) ne dépend pas de \( T_h\). Nous appliquons à \eqref{EqVOhjKB} l'application
    \begin{equation}
        \begin{aligned}
            t\alpha\colon \eK[T_1,\ldots, T_n]&\to \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \\
            \alpha(PT_1,\ldots, \hat T_k,\ldots, T_n)&=P(T_1,\ldots, T_h,\ldots, T_n). 
        \end{aligned}
    \end{equation}
    Cette application vérifie \( \alpha\big( \tau_{kh}\cdot r \big)=\alpha(r)\) et nous avons
    \begin{equation}
        -\alpha(f-g)=\alpha(r).
    \end{equation}
    Puis en appliquant \( \alpha\) à la relation \( f-g=(T_k-T_h)q+r\), nous trouvons
    \begin{equation}
        \alpha(f-g)=\alpha(r),
    \end{equation}
    et par conséquent \( \alpha(r)=0\). Ici nous utilisons l'hypothèse de caractéristique différente de deux. Dire que \( \alpha(r)=0\), c'est dire que \( r\) est divisible par \( T_k-T_h\), mais \( r\) étant de degré zéro en \( T_k\), nous avons \( r=0\). Par conséquent \( T_k-T_h\) divise \( f-g\) pour tout \( h<k\), et nous pouvons définir un polynôme \( Q\) par
    \begin{equation}    \label{EqrnbgdA}
        f-g=2Q\prod_{h<k}\prod_{k\leq n}(T_k-T_h)=2Q(T_1,\ldots, T_n)V(T_1,\ldots, T_n),
    \end{equation}
    où nous avons utilisé la formule du déterminant de Vandermonde de la proposition \ref{PropnuUvtj}.

    Étant donné que \( f+g\) est un polynôme symétrique, nous allons aussi poser \( f+g=2P\) avec \( P\) symétrique.

    Montrons à présent que \( Q\) est un polynôme symétrique. Soit \( \sigma\in S_n\); vu que nous savons déjà que \( f-g\) est alternée, nous avons
    \begin{equation}    \label{EqpSPEyq}
        \sigma\cdot (f-g)=\epsilon(\sigma)(f-g)=\epsilon(\sigma)2QV,
    \end{equation}
    Mais en appliquant \( \sigma\) à l'équation \eqref{EqrnbgdA},
    \begin{subequations}
        \begin{align}
            \sigma\cdot (f-g)&=2(\sigma\cdot V)(T_1,\ldots, ,T_n)(\sigma\cdot Q)(T_1,\ldots,T_n)\\
            &=2\epsilon(\sigma)V(T_1,\ldots, T_n)(\sigma\cdot Q)(T_1,\ldots, T_n).
        \end{align}
    \end{subequations}
    En égalisant avec \eqref{EqpSPEyq} et en se souvenant que l'anneau \( \eK[T_1,\ldots, T_n]\) était intègre (théorème \ref{ThoBUEDrJ}), nous simplifions par \( 2\epsilon(\sigma)V\) pour obtenir
    \begin{equation}
        Q=\sigma\cdot Q,
    \end{equation}
    c'est à dire que \( Q\) est symétrique.

    Au final nous avons \( f+q=2P\) et \( f-g=2VQ\) avec \( P\) et \( Q\) symétriques. En faisant la somme,
    \begin{equation}
        f=P+VQ.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Gram}
%---------------------------------------------------------------------------------------------------------------------------

Si \( x_1,\ldots, x_r\) sont des vecteurs d'un espace vectoriel, alors le \defe{déterminant de Gram}{déterminant!Gram}\index{Gram (déterminant)} est le déterminant
\begin{equation}
    G(x_1,\ldots, x_r)=\det\big( \langle x_i, x_j\rangle  \big).
\end{equation}
Notons que la matrice est une matrice symétrique.

\begin{proposition}\label{PropMsZhIK}
    Si \( F\) est un sous-espace vectoriel de base \( \{ x_1,\ldots, x_n \}\) et si \( x\) est un vecteur, alors le déterminant de Gram est un moyen de calculer la distance entre \( x\) et \( F\) par 
    \begin{equation}
        d(x,F)^2=\frac{ G(x,x_1,\ldots, x_n)}{ G(x_1,\ldots, x_n) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Cauchy}
%---------------------------------------------------------------------------------------------------------------------------

Soient des nombres \( a_i\) et \( b_i\) (\( i=1,\ldots, n\)) tels que \( a_i+b_j\neq 0\) pour tout couple \( (i,j)\). Le \defe{déterminant de Cauchy}{déterminant!de Cauchy}\index{Cauchy!déterminant} est 
\begin{equation}
    D_n=\det\left( \frac{1}{ a_i+b_j } \right).
\end{equation}

\begin{proposition}[\cite{RollandRobertjyYDzY}] \label{ProptoDYKA}
    Le déterminant de Cauchy est donné par la formule
    \begin{equation}
        D_n=\frac{ \prod_{i<j}(a_j-a_i)\prod_{i<j}(b_j-b_i) }{ \prod_{ij}(a_i+b_j) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSQBJfr}

La définition est pompée de \wikipedia{fr}{Matrice_de_Sylvester}{wikipédia}. Soient \( P\) et \( Q\) deux polynômes non nuls, de degrés respectifs \( m\) et \( n\) :
\begin{subequations}
    \begin{align}
        P(x)=p_0+p_1x+\ldots +p_nx^n\\
        Q(x)=q_0+q_1x+\ldots +q_mx^m.
    \end{align}
\end{subequations}
La \defe{matrice de Sylvester}{matrice!de Sylvester}\index{Sylvester (matrice)} associée à \( P\) et \( Q\) est la matrice carrée \( m+n\times m+n\) définie ainsi :
\begin{enumerate}
    \item
la première ligne est formée des coefficients de \( P\), suivis de 0 :
\begin{equation}
\begin{pmatrix} p_n & p_{n-1} & \cdots & p_1 & p_0 & 0 & \cdots & 0 \end{pmatrix} ;
\end{equation}
\item la seconde ligne s'obtient à partir de la première par permutation circulaire vers la droite ;
\item les $(m-2)$ lignes suivantes s'obtiennent en répétant la même opération ;
\item la ligne $(m+1)$ est formée des coefficients de \( Q\), suivis de 0 :
    \begin{equation}
    \begin{pmatrix} q_m & q_{m-1} & \cdots & q_1 & q_0 & 0 & \cdots & 0 \end{pmatrix} ;
    \end{equation}
    \item les $(m-1)$ lignes suivantes sont formées par des permutations circulaires.
\end{enumerate}

Ainsi dans le cas $n=4$ et $m=3$, la matrice obtenue est
\begin{equation}    \label{EqPEgtle}
S_{p,q}=\begin{pmatrix} 
p_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0 \\
0 & p_4 & p_3 & p_2 & p_1 & p_0 & 0 \\
0 & 0 & p_4 & p_3 & p_2 & p_1 & p_0 \\
q_3 & q_2 & q_1 & q_0 & 0 & 0 & 0 \\
0 & q_3 & q_2 & q_1 & q_0 & 0 & 0 \\
0 & 0 & q_3 & q_2 & q_1 & q_0 & 0 \\
0 & 0 & 0 & q_3 & q_2 & q_1 & q_0 \\
\end{pmatrix}.
\end{equation}
Le déterminant de la matrice de Sylvester associée à \( P\) et \( Q\) est appelé le \defe{résultant}{résultant} de \( P\) et \( Q\) et noté \( \res(P,Q)\)\nomenclature[A]{\( \res(P,Q)\)}{résultat des polynômes \( P\) et \( Q\)}.

Attention : si \( P\) est de degré \( n\) et \( Q\) de degré \( m\), il y a \( m\) lignes pour \( P\) et \( n\) pour \( Q\) dans le déterminant du résultant (et non le contraire).

\begin{lemma}[\cite{QQuRUzA}]       \label{LemBFrhgnA}
    Si \( P\) et \( Q\) sont deux polynômes de degrés \( n\) et \( m\) à coefficients dans l'anneau \( \eA\), alors pour tout \( \lambda\in \eA\),
    \begin{subequations}
        \begin{align}
            \res(\lambda P,Q)&=\lambda^m\res(P,Q)\\
            \res(P,\lambda Q)&=\lambda^n\res(P,Q).
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Cela est simplement un comptage de nombre de lignes. Il y a \( m\) lignes contenant les coefficients de \( P\); donc prendre \( \lambda P\) revient à multiplier \( m\) lignes dans un déterminant et donc le multiplier par \( \lambda^m\).
\end{proof}

L'équation de Bézout \eqref{EqkbbzAi}\index{théorème!Bézout!utilisation} peut être traitée avec une matrice de Sylvester. Soient \( P\) et \( Q\), deux polynômes donnés et à résoudre l'équation 
\begin{equation}    \label{EqSsyXOo}
    xP+yQ=0
\end{equation}
par rapport aux polynômes inconnus \( x\) et \( y\) dont les degrés sont \( \deg(x)<\deg(Q)\) et \( \deg(y)<\deg(P)\). Si nous notons \( \tilde x\) et \( \tilde y\) la liste des coefficients de \( x\) et \( y\) (dans l'ordre décroissant de degré), nous pouvons récrire l'équation \eqref{EqSsyXOo} sous la forme
\begin{equation}
    S_{PQ}^t\begin{pmatrix}
        \tilde x    \\ 
        \tilde y    
    \end{pmatrix}=0.
\end{equation}
Pour s'en convaincre, écrivons pour les polynômes de l'exemple \eqref{EqPEgtle} :
\begin{equation}
    \begin{pmatrix}
        p_4    &   0    &   0    &   q_3    &   0    &   0    &   0\\ 
        p_3    &   p_4    &   0    &   q_2    &   q_3    &   0    &   0\\ 
        p_2    &   p_3    &   p_4    &   q_1    &   q_2    &   q_3    &   0\\ 
        p_1    &   p_2    &   p_3    &   q_0    &   q_1    &   q_2    &   q_3\\ 
        p_0    &   p_1    &   p_2    &   0    &   q_0    &   q_1    &   q_2\\ 
        0    &   p_0    &   p_1    &   0    &   0    &   q_0    &   q_1\\ 
        0    &   0    &   p_0    &   0    &   0    &   0    &   q_0\\    
    \end{pmatrix}\begin{pmatrix}
        x_2    \\ 
        x_1  \\ 
        x_0  \\    
        y_3   \\ 
        y_2    \\ 
        y_1    \\ 
        y_0    
    \end{pmatrix}=
    \begin{pmatrix}
        x_2p_4+y_2q_3    \\ 
        p_3x_2+p_4x_1+q_2y_3+q_3y_2  \\ 
          \\    
           \\ 
        \vdots    \\ 
            \\ 
            
    \end{pmatrix}
\end{equation}
Nous voyons que sur la ligne numéro \( k\) (en partant du bas et en numérotant de à partir de zéro) nous avons les produits \( p_ix_j\) et \( q_iy_j\) avec \( i+j=k\). La colonne de droite représente donc bien les coefficients du polynôme \( xP+yQ\).


\begin{proposition} \label{PropAPxzcUl}
    Le résultant de deux polynômes est non nul si et seulement si les deux polynômes sont premiers entre eux.
\end{proposition}
\index{déterminant!résultant}

Un polynôme \( P\) a une racine double en \( a\) si et seulement si \( P\) et \( P'\) ont \( a\) comme racine commune, ce qui revient à dire que \( P\) et \( P'\) ne sont pas premiers entre eux. 

Une application importante de ces résultats sera le théorème de Rothstein-Trager \ref{ThoXJFatfu} sur l'intégration de fractions rationnelles.

\begin{example}
    Si nous prenons \( P=aX^2+bX+c\) et \( P'=2aX+b\) alors la taille de la matrice de Sylvester sera \( 2+1=3\) et
    \begin{equation}
        S_{P,P'}=\begin{pmatrix}
              a  &   b    &   c    \\
            2a    &   b    &   0    \\
            0    &   2a    &   b
        \end{pmatrix}.
    \end{equation}
    Le résultant est alors
    \begin{equation}
        \res(P,P')=-a(b^2-4ac).
    \end{equation}
    Donc un polynôme du second degré a une racine double si et seulement si \( b^2-4ac=0\). Cela est un résultat connu depuis longtemps mais qui fait toujours plaisir à revoir.
\end{example}

La matrice de Sylvester permet aussi de récrire l'équation de Bézout pour les polynômes; voir le théorème \ref{ThoBezoutOuGmLB} et la discussion qui s'ensuit.

Une proposition importante du résultant est qu'il peut s'exprimer à l'aide des racines des polynômes.
\begin{proposition} \label{PropNDBOGNx}
    Si
    \begin{subequations}
        \begin{align}
        P(X)&=a_p\prod_{i=1}^p(X-\alpha_i)\\
        Q(X)&=b_q\prod_{j=1}^q(X-\beta_i)
        \end{align}
    \end{subequations}
    alors nous avons les expressions suivantes pour le résultant :
    \begin{equation}        \label{EqCFUumjx}
        \res(P,Q)=a_p^qb_q^p\prod_{i=1}^p\prod_{j=1}^q(\beta_j-\alpha_i)=b_q^p\prod_{j=1}^qP(\beta_j)=(-1)^{pq}a_p^q\prod_{i=1}^pQ(\alpha_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Si \( P\) et \( Q\) ne sont pas premiers entre eux, d'une part la proposition \ref{PropAPxzcUl} nous dit que \( \res(P,Q)=0\) et d'autre part, \( P\) et \( Q\) ont un facteur irréductible en commun, ce qui  signifie que nous devons avoir un des \( X-\alpha_i\) égal à un des \( X-\beta_j\). Autrement dit, nous avons \( \alpha_i=\beta_j\) pour un couple \( (i,j)\). Par conséquent tous les membres de l'équation \eqref{EqCFUumjx} sont nuls.

    Nous supposons donc que \( P\) et \( Q\) sont premiers entre eux. Nous commençons par supposer que les polynômes \( P\) et \( Q\) sont unitaires, c'est à dire que \( a_p=b_q=1\). Nous considérons alors l'anneau
    \begin{equation}
        \eA=\eZ[\alpha_1,\ldots, \alpha_p,\beta_1,\ldots, \beta_q].
    \end{equation}
    Dans cet anneau, l'élément \( \beta_j-\alpha_i\) est irréductible (tout comme \( X-Y\) est irréductible dans \( \eZ[X,Y]\)). Le résultant \( R=\res(P,Q)\) est un élément de \( \eA\) parce que tous leurs coefficients peuvent être exprimés à l'aide des \( \alpha_i\) et des \( \beta_j\). Dans \( \eA\), l'élément \( \beta_j-\alpha_i\) divise \( R\). En effet lorsque \( \beta_j=\alpha_i\), le déterminant définissant le résultant est nul, ce qui signifie que \( \beta_j-\alpha_i\) est un facteur irréductible de \( R\).

    Par conséquent il existe un polynôme \( T\in \eA\) tel que
    \begin{equation}
        R=\lambda(\alpha_1,\ldots, \beta_q)\prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i).
    \end{equation}
    Comptons les degrés. Pour donner une idée de ce calcul de degré, voici comment se présente, au niveau des dimensions, le déterminant :
    \begin{equation}  \label{EqJCaATOH}
    \xymatrix{%
        \ar@{<->}[rrr]^{p+1}&&&& \ar@{<->}[r]^{q-1}  &\\
        a_p\ar@{.}[rrd] &a_{p-1}\ar@{.}[rr]  &  & a_0\ar@{.}[rrd] & 0\ar@{.}[r]&0&\ar@{<->}[d]^q \\
        0\ar@{.}[r]&0&a_p\ar@{.}[rr]&&a_1&a_0&\\
        \ar@{<->}[rrrrr]_{p+q}&&&&&&
       }
    \end{equation}
    si les \( a_i\) sont les coefficients de \( P\). Mais chacun des \( a_i\) est de degré \( 1\) en les \( \alpha_i\), donc le déterminant dans son ensemble est de degré \( q\) en les \( \alpha_i\), parce que \( R\) contient \( q\) lignes telles que \eqref{EqJCaATOH}. Le même raisonnement montre que \( R\) est de degré \( p\) en les \( \beta_j\). Par ailleurs le polynôme \( \prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i)\) est de degré \( p\) en les \( \beta_j\) et \( q\) en les \( \alpha_i\). Nous en déduisons que \( T\) doit être un polynôme ne dépendant pas de \( \alpha_i\) ou de \( \beta_j\).

    Nous pouvons donc calculer la valeur de \( T\) en choisissant un cas particulier. Avec \( P(X)=X^p\) et \( Q(X)=X^q+1\), il est vite vu que \( R(P,Q)=1\) et donc que \( T=1\).

    Si les polynômes \( P\) et \( Q\) ne sont pas unitaires, le lemme \ref{LemBFrhgnA} nous permet de conclure. 

\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Kronecker}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons \( K_n\) l'ensemble des polynômes de \( \eZ[X]\)
\begin{enumerate}
    \item
        unitaires de degré \( n\),
    \item
        dont les racines dans \( \eC\) sont de modules plus petits ou égaux à \( 1\),
    \item
        et qui ne sont pas divisés par \( X\).
\end{enumerate}
Un tel polynôme s'écrit sous la forme
\begin{equation}
    P=X^n+\sum_{k=0}^{n-1}a_kX^k.
\end{equation}

\begin{theorem}[Kronecker\cite{KXjFWKA}]    \label{ThoOWMNAVp}
    Les racines des éléments de \( K_n\) sont des racines de l'unité.
\end{theorem}
\index{théorème!Kronecker}
\index{polynôme!à plusieurs indéterminées}
\index{résultant!utilisation}
\index{polynôme!symétrique}

\begin{proof}
    Vu que \( \eC\) est algébriquement clos 
    nous pouvons considérer les racines \( \alpha_1,\ldots, \alpha_n\) de \( P\) dans \( \eC\). Nous les considérons avec leurs multiplicités.
%TODO : lorsqu'on aura démontré que \eC est algébriquement clos, il faudra le référentier ici.

    Soit \( R=X^n+\sum_{k=0}^{n-1}b_kX^k\) un élément de \( K_n\) dont nous notons \( \beta_1,\ldots, \beta_n\) les racines dans \( \eC\). Les relations coefficients-racines stipulent que
    \begin{equation}
        b_k=\sum_{1\leq i_1<\ldots <i_{n-k}\leq n}\prod_{j=1}^{n-k}\beta_{i_j}.
    \end{equation}
    En prenant le module et en se souvenant que \( | \beta_{l} |\leq 1\) pour tout \( l\), nous trouvons que
    \begin{equation}
        | b_k |\leq\binom{ n }{ n-k }.
    \end{equation}
    Mais comme \( b_k\in \eZ\), nous avons
    \begin{equation}
        b_k\in\big\{    -\binom{ n }{ n-k },-\binom{ n }{ n-k }+1,\ldots, 0,\cdots,\binom{ n }{ n-k }   \big\}
    \end{equation}
    qui est de cardinal \( \binom{ n }{ n-k }+1\). Nous avons donc
    \begin{equation}
        \Card(K_n)\leq\prod_{k=0}^{n-1}\big( 1+\binom{ n }{ n-k } \big)<\infty.
    \end{equation}
    La conclusion jusqu'ici est que \( K_n\) est un ensemble fini.

    Pour chaque \( k\in \eN^*\) nous considérons les polynômes
    \begin{subequations}
        \begin{align}
            P_k&=\prod_{i=1}^n(X-\alpha_i^k)\\
            Q_k&=X^k-Y\in \eZ[X,Y],
        \end{align}
    \end{subequations}
    et puis nous considérons le résultant \( R_k=\res_X(P,Q_k)\in \eZ[Y]\) :
    \begin{equation}
        R_k=\res_X(P,Q_k)=
        \begin{pmatrix}
            1&a_{n-1}&\cdots&a_0&0&\cdots&0&0&0\\  
            0&1&a_{n-1}&\cdots&a_0&0&\cdots&0&0\\  
            \vdots&\ddots&\ddots&\ddots&&\ddots&\\  
            0&\cdots&0&1&a_{n-1}&\cdots&a_0&0&0\\
            0&\cdots&0&0&1&a_{n-1}&\cdots&a_0&0\\
            0&\cdots&0&0&0&1&a_{n-1}&\cdots&a_0\\
        \\
                    1&0&\ldots&0&-Y&0&\ldots&0&0\\
                    0&1&0&\ldots&0&-Y&0&\ldots&0\\
                &&\ddots&&&\ddots&\ddots\\
                    0&\cdots&0&1&0&\cdots&0&-Y&0\\
                    0&0&\cdots&0&1&0&\cdots&0&-Y
        \end{pmatrix}
    \end{equation}
    Cela est un polynôme en \( Y\) dont le terme de plus haut degré est \( (-1)^nY^n\). Les petites formules de la proposition \ref{PropNDBOGNx} nous permettent d'exprimer \( R_k(Y)\) en termes des racines de \( P\) :
    \begin{equation}
        R_k(Y)=\prod_{i=1}^nQ_k(\alpha_i)=\prod_{i=1}^n(\alpha_i^k-Y)=(-1)^n\prod_{i=1}^n(Y-\alpha_i^k)=(-1)^nP_k(Y).
    \end{equation}
    Vu que \( P\in K_n\) nous savons que les \( \alpha_i\) ne sont pas tous nuls; donc \( P_k\in K_n\). Cependant nous avons vu que \( K_n\) est un ensemble fini; donc parmi les \( P_k\), il y a des doublons (et pas un peu)\quext{Ici dans \cite{KXjFWKA}, il déduit qu'on a un \( k\) tel que \( P_k=P_1=P\). Mois je vois pourquoi on a un \( k\) et un \( l\) tels que \( P_k=P_l\), mais pourquoi on peut en trouver un spécialement égal au premier ? Une réponse à cette question permettrait de solidement réduire la lourdeur de la suite de la preuve.}. Nous regardons même l'ensemble des \( P_{2^n}\) dans lequel nous pouvons en trouver deux les mêmes. Soit \( l>k\) tels que \( P_{2^k}=P_{2^l}\). Si \( \alpha\) est racine de \( P_{2^k}\), alors il est de la forme \( \alpha=\beta^{2^k}\) pour une certaine racines \( \beta\) de \( P\). Par conséquent
    \begin{equation}    \label{EqBEgJtzm}
        \alpha^{2^l/2^k}=\alpha^{2^{l-k}}
    \end{equation}
    est racine de \( P_{2^l}\). Notons que dans cette expression il n'y a pas de problèmes de définition d'exposant fractionnaire dans \( \eC\) parce que \( l>k\). Vu que \eqref{EqBEgJtzm} est racine de \( P_{2^l}\), il est aussi racine de \( P_{2^k}\). Donc
    \begin{equation}
        \big( \alpha^{2^{l-k}} \big)^{2^{l-k}}=\alpha^{2^{2(l-k)}}
    \end{equation}
    est racine de \( P_{2^l}\) et donc de \( P_{2^k}\). Au final nous savons que tous les nombres de la forme \( \alpha^{2^{n(l-k)}}\) sont racines de \( P_{2^k}\). Mais comme \( P_{2^k}\) a un nombre fini de racines, nous pouvons en trouver deux égales. Si nous avons
    \begin{equation}
        \alpha^{2^{n(l-k)}}=\alpha^{2^{m(l-k)}}
    \end{equation}
    pour certains entiers \( m>n\), alors
    \begin{equation}
        \alpha^{2^{n(l-k)}-2^{m(l-k)}}=1,
    \end{equation}
    ce qui prouver que \( \alpha\) est une racine de l'unité. Nous avons donc prouvé que toutes les racines de \( P_{2^k}\) sont des racines de l'unité et donc que les racines de \( P\) sont racines de l'unité.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Directions conservées}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Nous savons qu'une application \emph{linéaire} $A\colon \eR^3\to \eR^3$ est complètement définie par la donnée de son action sur les trois vecteurs de base, c'est à dire par la donnée de
\begin{equation}
	\begin{aligned}[]
		Ae_1,&&Ae_2&&\text{et}&&Ae_3.
	\end{aligned}
\end{equation}
Nous allons former la matrice de $A$ en mettant simplement les vecteurs $Ae_1$, $Ae_2$ et $Ae_3$ en colonne. Donc la matrice
\begin{equation}		\label{EqExempleALin}
	A=\begin{pmatrix}
		3	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	1	&	0
	\end{pmatrix}
\end{equation}
signifie que l'application linéaire $A$ envoie le vecteur $e_1$ sur $\begin{pmatrix}
	3	\\ 
	0	\\ 
	0	
\end{pmatrix}$, le vecteur $e_2$ sur $\begin{pmatrix}
	0	\\ 
	0	\\ 
	1	
\end{pmatrix}$ et le vecteur $e_3$ sur $\begin{pmatrix}
	0	\\ 
	1	\\ 
	0	
\end{pmatrix}$.
Pour savoir comment $A$ agit sur n'importe quel vecteur, on applique la règle de produit vecteur$\times$matrice :
\begin{equation}
	\begin{pmatrix}
		1	&	2	&	3	\\
		4	&	5	&	6	\\
		7	&	8	&	9
	\end{pmatrix}\begin{pmatrix}
		x	\\ 
		y	\\ 
		z	
	\end{pmatrix}=
	\begin{pmatrix}
		x+2y+3z	\\ 
		4x+5y+6z	\\ 
		7x+8y+9z	
	\end{pmatrix}.
\end{equation}

Une chose intéressante est de savoir quelles sont les directions invariantes de la transformation linéaire. Par exemple, on peut lire sur la matrice \eqref{EqExempleALin} que la direction $\begin{pmatrix}
	1	\\ 
	0	\\ 
	0	
\end{pmatrix}$ est invariante : elle est simplement multipliée par $3$. Dans cette direction, la transformation est juste une dilatation. Afin de savoir si $v$ est un vecteur d'une direction conservée, il faut voir si il existe un nombre $\lambda$ tel que $Av=\lambda v$, c'est à dire voir si $v$ est simplement dilaté.

L'équation $Av=\lambda v$ se récrit $(A-\lambda\mtu)v=0$, c'est à dire qu'il faut résoudre l'équation
\begin{equation}
	(A-\lambda\mtu)\begin{pmatrix}
		x	\\ 
		y	\\ 
		z	
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\ 
		0	\\ 
		0	
	\end{pmatrix}.
\end{equation}
Nous savons qu'une telle équation ne peut avoir de solutions que si $\det(A-\lambda\mtu)=0$. La première étape est donc de trouver les $\lambda$ qui vérifient cette condition.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice orthogonale}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Une matrice \( U\) est \defe{orthogonale}{matrice!orthogonale}\index{orthogonal!matrice} si \( UU^t=\mtu\). Le \defe{groupe orthogonal}{groupe!orthogonal} noté \( \gO(n)\) est l'ensemble des matrices orthogonales \( n\times n\).
\end{definition}

\begin{proposition}     \label{PropKBCXooOuEZcS}
    À propos de matrices orthogonales.
    \begin{enumerate}
        \item
            L'ensemble des matrice réelles orthogonales forme un groupe noté \( \gO(n,\eR)\)\nomenclature[B]{\( \gO(n,\eR)\)}{le groupe des matrices orthogonales}.
        \item
            Si \( A\) est une matrice orthogonale, alors \( \det(A)=\pm 1\).
        \item       \label{ITEMooOWMBooHUatNb}
            Le groupe \( \gO(n)\) est le groupe des isométries de \( \eR^n\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Si \( A\) et \( B\) sont orthogonales, alors
    \begin{equation}
        (AB)(AB)^t=ABB^tA^t=A\mtu A^t=\mtu.
    \end{equation}
    Vu que \( \mtu\) est orthogonale, nous avons bien un groupe.

    En ce qui concerne le déterminant, \( AA^t=\mtu\) donne \( \det(A)\det(A^t)=1\) ou encore \( \det(A)^2=1\). D'où le fait que \( \det(A)=\pm 1\).

    D'autre part si \( A\) est une isométrie de \( \eR^n\) alors pour tout \( x,y\in \eR^n\) nous avons \( \langle Ax, Ay\rangle =\langle x, y\rangle \). En particulier,
    \begin{equation}
        \langle A^tAx, y\rangle =\langle x, y\rangle 
    \end{equation}
    pour tout \( x,y\in \eR^n\). En prenant \( y=e_i\) nous trouvons
    \begin{equation}
        (A^tAx)_i=x_i,
    \end{equation}
    ce qui signifie que pour tout \( x\), \( A^tAx=x\), ce qui signifie que \( A^tA\) est l'identité.

    Réciproquement si \( A^tA\) est l'identité nous avons
    \begin{equation}
        \langle a, y\rangle =\langle A^tAx, y\rangle =\langle Ax, Ay\rangle ,
    \end{equation}
    ce qui prouve que \( A\) est une isométrie.
\end{proof}

Le sous-groupe des matrices orthogonales de déterminant \( 1\) est le groupe \defe{spécial orthogonal}{groupe!spécial orthogonal} noté \( \SO(n)\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Comment trouver la matrice d'une symétrie donnée ?}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMtrSym}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à un plan}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Comment trouver par exemple la matrice $A$ qui donne la symétrie autour du plan $z=0$ ? La définition d'une telle symétrie est que les vecteurs du plan $z=0$ ne bougent pas, tandis que les vecteurs perpendiculaires changent de signe. Ces informations vont permettre de trouver comment $A$ agit sur une base de $\eR^3$. En effet :
\begin{enumerate}

	\item
		Le vecteur $\begin{pmatrix}
			1	\\ 
			0	\\ 
			0	
		\end{pmatrix}$ est dans le plan $z=0$, donc il ne bouge pas,

	\item
		le vecteur $\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix}$ est également dans le plan, donc il ne bouge pas non plus,

	\item
		et le vecteur $\begin{pmatrix}
			0	\\ 
			0	\\ 
			1	
		\end{pmatrix}$ est perpendiculaire au plan $z=0$, donc il va changer de signe.

\end{enumerate}
Cela nous donne directement les valeurs de $A$ sur la base canonique et nous permet d'écrire 
\begin{equation}
	A=\begin{pmatrix}
		1	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	0	&	-1
	\end{pmatrix}.
\end{equation}
Pour écrire cela, nous avons juste mit en colonne les images des vecteurs de base. Les deux premiers n'ont pas changé et le troisième a changé.

Et si maintenant on donne un plan moins facile que $z=0$ ? Le principe reste le même : il faudra trouver deux vecteurs qui sont dans le plan (et dire qu'ils ne bougent pas), et puis un vecteur qui est perpendiculaire au plan\footnote{Pour le trouver, penser au produit vectoriel.}, et dire qu'il change de signe.

Voyons ce qu'il en est pour le plan $x=-z$. Il faut trouver deux vecteurs linéairement indépendants dans ce plan. Prenons par exemple
\begin{equation}		\label{EqffudE}
	\begin{aligned}[]
		f_1&=\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix},&f_2&=\begin{pmatrix}
			1	\\ 
			0	\\ 
			-1	
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons 
\begin{equation}
	\begin{aligned}[]
		Af_1&=f_1\\
		Af_2&=f_2.
	\end{aligned}
\end{equation}
Afin de trouver un vecteur perpendiculaire au plan, calculons le produit vectoriel :
\begin{equation}
	f_3=f_1\times f_2=\begin{vmatrix}
		e_1	&	e_2	&	e_3	\\
		0	&	1	&	0	\\
		1	&	0	&	-1
	\end{vmatrix}=-e_1-e_3=\begin{pmatrix}
		-1	\\ 
		0	\\ 
		-1	
	\end{pmatrix}.
\end{equation}
Nous avons 
\begin{equation}
	Af_3=-f_3.
\end{equation}
Afin de trouver la matrice $A$, il faut trouver $Ae_1$, $Ae_2$ et $Ae_3$. Pour ce faire, il faut d'abord écrire $\{ e_1,e_2,e_3 \}$ en fonction de $\{ f_1,f_2,f_3 \}$. La première des équation \eqref{EqffudE} dit que 
\begin{equation}
	f_1=e_2.
\end{equation}
Ensuite, nous avons
\begin{equation}
	\begin{aligned}[]
		f_2&=e_1-e_3\\
		f_3&=-e_1-e_3.
	\end{aligned}
\end{equation}
La somme de ces deux équations donne $-2e_3=f_2+f_3$, c'est à dire
\begin{equation}
	e_3=-\frac{ f_2+f_3 }{ 2 }
\end{equation}
Et enfin, nous avons
\begin{equation}
	e_1=\frac{ f_2-f_3 }{ 2 }.
\end{equation}

Maintenant nous pouvons calculer les images de $e_1$, $e_2$ et $e_3$ en faisant
\begin{equation}
	\begin{aligned}[]
		Ae_1&=\frac{ Af_2-Af_3 }{ 2 }=\frac{1 }{2}\begin{pmatrix}
			0	\\ 
			0	\\ 
			-2	
		\end{pmatrix}=\begin{pmatrix}
			0	\\ 
			0	\\ 
			-1	
		\end{pmatrix},\\
		Ae_2&=Af_1=f_1=\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix},\\
		Ae_3&=-\frac{ f_2-f_3 }{ 2 }=-\frac{ 1 }{2}\begin{pmatrix}
			2	\\ 
			0	\\ 
			0	
		\end{pmatrix}=\begin{pmatrix}
			-1	\\ 
			0	\\ 
			0	
		\end{pmatrix}.
	\end{aligned}
\end{equation}
La matrice $A$ s'écrit maintenant en mettant les trois images trouvées en colonnes :
\begin{equation}
	A=\begin{pmatrix}
		0	&	0	&	-1	\\
		0	&	1	&	0	\\
		-1	&	0	&	0
	\end{pmatrix}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à une droite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le principe est exactement le même : il faut trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Ensuite il faudra exprimer $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$ et $f_3$.

Le seul problème est de trouver les trois vecteurs $f_i$. Le premier est tout trouvé : c'est n'importe quel vecteur sur la droite. Pour les deux autres, il faut un peu ruser parce qu'il faut impérativement qu'ils soient perpendiculaire à la droite. Pour trouver $f_2$, on peut écrire
\begin{equation}
	f_2=\begin{pmatrix}
		1	\\ 
		0	\\ 
		x	
	\end{pmatrix},
\end{equation}
et puis fixer le $x$ pour que le produit scalaire de $f_2$ avec $f_1$ soit nul. Si il n'y a pas moyen (genre si $f_1$ a sa troisième composante nulle), essayer avec $\begin{pmatrix}
	x	\\ 
	1	\\ 
	0	
\end{pmatrix}$. Une fois que $f_2$ est trouvé (il y a des milliards de choix possibles), trouver $f_3$ est super facile : prendre le produit vectoriel entre $f_1$ et $f_2$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{En résumé}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
La marche à suivre est

\begin{enumerate}

	\item
		Trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Typiquement : des vecteurs qui sont sur l'axe ou le plan de symétrie, et puis des perpendiculaires. Pour la perpendiculaire, penser au produit scalaire et au produit vectoriel.

	\item
		Exprimer la base canonique $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$, $f_3$.

	\item
		Trouver $Ae_1$, $Ae_2$ et $Ae_3$ en utilisant leur expression en termes des $f_i$, et le fait que l'on connaisse l'action de $A$ sur les $f_i$.

	\item
		La matrice s'obtient en mettant les images des $e_i$ en colonnes.
\end{enumerate}
